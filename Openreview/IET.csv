title,abstract
a lightweight multiscale smoke segmentation algorithm based on improved deeplabv3+,"Fires not only cause devastating consequences for human life and property, but also lead to soil erosion in forests. Therefore, it is necessary to design a novel algorithm that can quickly monitor smoke from fires. Most existing smoke segmentation methods do not consider the segmentation accuracy of algorithms under limited computational resources. To address this research gap, this paper proposes a lightweight smoke segmentation algorithm based on DeepLabV3+ that achieves fast inference speed and high accuracy for different sizes smoke. To reduce the number of parameters, the feature extraction network of the DeeplabV3+ algorithm is replaced by MobileNetV2, which enhances the extraction ability of the algorithm in segment smoke images. Then, the Convolutional Block Attention Module (CBAM) is added to the encoder part to enhance the perception of the algorithm for small smoke and effectively alleviates smoke mis‐segmentation. Furthermore, a newly designed loss function is used in the network. The experimental results show that the proposed method has improved by 1.27% in Smoke IoU and 1.21% in mPA compared with other methods. The weight size has been reduced to 10.76% of the original DeepLabV3+, and the inference time is only 33.71ms. Therefore, it is a more suitable early fire detection algorithm for resource‐constrained environments."
feature ensemble network for medical image segmentation with multi‐scale atrous transformer,"Recent years have witnessed notable advancements in medical image segmentation through deep convolutional neural networks. However, a notable limitation lies in the local operation of convolution, which hinders the ability to fully exploit global semantic information. To overcome the challenges prevalent in medical image segmentation, the feature ensemble network with multi‐scale atrous transformer is proposed. At the core of the approach lies the multi‐scale contextual integration module, which is based on the multi‐scale atrous transformer and facilitates contextual integration of multi‐level features. To extract discriminative fine‐grained features of the target region, a hybrid attention mechanism that synergistically combines spatial and channel attention, thereby sharpening the model's focus on crucial target information within high‐level features, is incorporated. Additionally, the channel‐aware feature reconstruction module is introduced as an innovative component engineered to tackle feature similarity issues across different categories. This module performs feature reconstruction based on channel perception, effectively widening the feature gap between categories and enhancing the segmentation capability. It is worth mentioning that our approach surpasses the state‐of‐the‐art method using three benchmark datasets in medical image segmentation."
tmnio:triplet merged network with involution operators for improved few-shot image classification,"Few‐shot learning enables machines to learn efficiently from limited labelled data. However, existing few‐shot learning methods may perform poorly when there is a lack of sufficient samples, and may encounter problems such as domain shift or overfitting when applied to new domains or tasks. To address the issues of poor fitting and insufficient generalization ability in new domains, a new method called triplet merged network with involution operators (TMNIO) is proposed. This method employs dual encoders that extract common and distinctive features from the prototype network, thereby enhancing the model's feature extraction capability. To further improve this ability, the traditional convolutional kernels are replaced with involution operators, which not only reduce the parameter count but also enlarge the receptive field to better extract local feature information. Additionally, this method employs a two‐stage training strategy, where triplet loss is used in the first stage to train the model and enhance its robustness and generalization ability. Extensive experiments on the miniImageNet, Omniglot, and Caltech‐UCSD Birds‐200 (CUB) datasets have shown that our proposed method achieved significant improvements in both training speed and accuracy, particularly on the miniImageNet dataset, where it achieved an outstanding 10% performance improvement."
a novel efficient wildlife detecting method with lightweight deployment on uavs based on yolov7,"Efficient animal detection is essential for biodiversity protection. Unmanned aerial vehicles (UAVs) have been widely used because of their low costs and minimal environmental intrusion. However, using UAVs for practical animal detection poses two challenges: (a) the UAV's fly highly to avoid disturbing animals, resulting in small object detection problems; (b) the limited processing power of UAVs makes large state‐of‐the‐art (SOTA) methods (e.g., You Only Look Once V7, YOLOv7) difficult to deploy. This work proposes the WILD‐YOLO based on YOLOv7 to deal with the two problems. To detect small objects, WILD‐YOLO improves upon YOLOv7 by adding a small object detection head in the head part. To enable real‐time animal detection in field environments with UAVs, the lighten FasterNet and GhostNet have been used to significantly reduce the model size.  Compared to YOLOv7, WILD‐YOLO significantly reduces the number of parameters, making it suitable for lightweight deployment on UAVs. Additionally, comparisons with other lightweight models such as YOLOv7‐tiny, YOLOv5‐s, YOLOv4‐s and MobilenetV2 on the datasets are conducted. The experimental results demonstrate that this proposed WILD‐YOLO method outperforms other approaches and has great potential for effective detection of wildlife in complex environments encountered by UAVs."
fruit fast tracking and recognition of apple picking robot based on improved yolov5,"The article proposes a real‐time apple picking method based on an improved YOLOv5. This method accurately recognizes different apple targets on fruit trees for robots and helps them adjust their position to avoid obstructions during fruit picking. Firstly, the original BottleneckCSP module in the YOLOv5 backbone network is enhanced to extract deeper features from images while maintaining lightweight. Secondly, the ECA module is embedded into the improved backbone network to better extract features of different apple targets. Lastly, the initial anchor frame size of the network is adjusted to avoid recognizing apples in distant planting rows. The results demonstrate that this improved model achieves high accuracy rates and recall rates for recognizing various types of apple picking methods with an average recognition time of 0.025s per image. Compared with other models tested on six types of apple picking methods, including the original YOLOv5 model as well as YOLOv3 and EfficientDet‐D0 algorithms, our improved model shows significant improvements in mAP by 1.95%, 17.6%, and 12.7% respectively. This method provides technical support for robots' picking hands to actively avoid obstructions caused by branches during fruit harvesting, effectively reducing apple loss."
afcn: an attention-directed feature-fusion convnext network for low-voltage apparatus assembly quality inspection,"In the production of low‐voltage apparatus, assembly quality inspection is of great relevance for ensuring the final quality of the entire product. With the continuous improvement of production efficiency and people's requirements for production quality, traditional manual inspection methods can no longer meet the quality inspection requirements. In this paper, an Attention‐guided Feature‐fusion ConvNeXt Network (AFCN) for the automated visual inspection is proposed. By embedding the attention mechanism of the Coordinate Attention block into the residual channel of the ConvNeXt block, the position‐aware information and features of the low‐voltage apparatus images can be effectively captured to locate the quality problems. Then, an improved attention feature fusion module is adopted to merge the output features at different stages, which introduces a 3D non‐parameter attention SimAM block and adapts output accordingly. Therefore, this model can capture the key information of the feature map in a coordinated way in terms of channel and position, fully integrating multiscale features and obtaining contour texture information and semantic information of the low‐voltage apparatus. Experiments show the proposed approach can effectively classify defective and normal products."
cross-modal fusion encoder via graph neural network for referring image segmentation,"Referring image segmentation identifies the object masks from images with the guidance of input natural language expressions. Nowadays, many remarkable cross‐modal decoder are devoted to this task. But there are mainly two key challenges in these models. One is that these models usually lack to extract fine‐grained boundary information and gradient information of images. The other is that these models usually lack to explore language associations among image pixels. In this work, a Multi‐scale Gradient balanced Central Difference Convolution (MG‐CDC) and a Graph convolutional network‐based Language and Image Fusion (GLIF) for cross‐modal encoder, called Graph‐RefSeg, are designed. Specifically, in the shallow layer of the encoder, the MG‐CDC captures comprehensive fine‐grained image features. It could enhance the perception of target boundaries and provide effective guidance for deeper encoding layers. In each encoder layer, the GLIF is used for cross‐modal fusion. It could explore the correlation of every pixel and its corresponding language vectors by a graph neural network. Since the encoder achieves robust cross‐modal alignment and context mining, a light‐weight decoder could be used for segmentation prediction. Extensive experiments show that the proposed Graph‐RefSeg outperforms the state‐of‐the‐art methods on three public datasets. Code and models will be made publicly available at https://github.com/ZYQ111/Graph_refseg."
an effective screening of covid‐19 pneumonia by employing chest x‐ray segmentation and attention‐based ensembled classification,"Quick and accurate diagnosis of COVID‐19 is crucial in preventing its transmission. Chest X‐ray (CXR) imaging is often used for diagnosis, however, even experienced radiologists may misinterpret the results, necessitating computer‐aided diagnosis. Deep learning has yielded favourable results previously, but overfitting, excessive variance, and generalization errors may occur due to noise and limited datasets. Ensemble learning can improve predictions by using robust techniques. Therefore, this study, proposes two‐fold strategy that combines advanced and robust algorithms, including DenseNet201, EfficientNetB7, and Xception, to achieve faster and more accurate COVID‐19 detection. Segmented lung images were generated from CXR images using the residual U‐Net model, and two attention‐based ensemble neural networks were used for classification. The COVID‐19 radiography dataset was used to evaluate the proposed approach, which achieved an accuracy of 98.21%, 93.4%, and 89.06% for two, three, and four classes respectively which outperformed previous studies by a significant margin considering COVID, viral pneumonia, and lung opacity simultaneously. Despite the similarity in CXR images of COVID, pneumonia, and lung opacity, the proposed approach achieved 89.06% accuracy, demonstrating its ability to recognize distinguishable features. The developed algorithm is expected to have applications in clinics for diagnosing different diseases using X‐ray images."
a multidimensional fusion image stereo matching algorithm,"In response to the low matching accuracy of stereo matching algorithms in image regions with specular reflection, this paper proposes a multidimensional fusion stereo matching algorithm named MFANet. The algorithm embeds a multispectral attention module into the residual feature extraction network, utilizing two‐dimensional discrete cosine transforms to extract frequency features. In the pyramid pooling module, a coordinated attention mechanism is introduced to capture relevant positional information. In the cost aggregation part, the MFANet algorithm incorporates a three‐dimensional attention mechanism, focusing on the more important semantic information in high‐level features. By combining detailed information from low‐level features, semantic information from high‐level features, and contextual information, the algorithm generates features that are more conducive to disparity prediction. The MFANet algorithm is evaluated on three standard datasets (SceneFlow, KITTI2015, and KITTI2012). Experimental results demonstrate its robustness against specular reflection interference, accurate prediction of disparities in specular reflection pathological regions, and promising application prospects."
msfa: multi-stage feature aggregation network for multi-label image recognition,"Multi‐label image recognition (MLR) is a significant branch of image classification that aims to assign multiple categorical labels to each input. Previous research has focused on enhancing the learning of category‐related regional features. However, the potential impact of multi‐scale distributions in intra‐ and inter‐category targets on MLR tends to be neglected. Besides, semantic consistency for categories is restricted to be considered on single‐scale features, resulting in suboptimal feature extraction. To address the limitations of above, a Multi‐stage Feature Aggregation (MSFA) network is proposed. In MSFA, a novel local feature extraction method is suggested to progressively extract category‐related high‐resolution local features in both spatial and channel dimensions. Subsequently, local and global features are fused without additional up‐ and down‐sampling to enrich the scale diversity of the features while incorporating refined class‐specific information. Furthermore, a hierarchical prediction scheme for MLR is proposed, which generates classification confidence corresponding to different scales under hierarchical loss supervision. Consequently, the final output of the network comes from the joint prediction by the classifiers on multi‐scale features, ensuring a stronger feature extraction capability. The extensive experiments have been carried on VOC and MS‐COCO datasets, and the superiority of MSFA over existing mainstream methods has been verified."
a joint image super-resolution network for multiple degradations removal via complementary transformer and convolutional neural network,"While recent years have witnessed the unprecedented success of deep convolutional neural networks (CNNs) and vision transformers in single‐image super‐resolution (SISR), the degradation assumptions are simple and usually bicubic downsampling. Thus, their performances will drop dramatically when the actual degradation does not match this assumption, and they lack the capability to handle multiple degradations (e.g. Gaussian noise, bicubic downsizing, and salt & pepper noise). To address the issues, in this paper, the authors propose a joint SR model (JIRSR) that can effectively handle multiple degradations in a single model. Specifically, the authors build the parallel Transformer and CNN branches that complement each other through bidirectional feature fusion. Moreover, the authors also adopt a random permutation of different kinds of noise and resizing operations to build the training datasets. Extensive experiments on classical SR, denoising, and multiple degradation removal demonstrate that the authors’ JIRSR achieves state‐of‐the‐art (SOTA) performance on public benchmarks. Concretely, the authors’ JIRSR outperforms the second‐best model by 0.23 to 0.74 dB for multiple degradations removal and is 0.20 to 0.36 dB higher than the SOTA methods on the Urban100 dataset under the ×4 SR task."
spatio‐temporal attention modules in orientation‐magnitude‐response guided multi‐stream cnns for human action recognition,"This paper introduces a new descriptor called orientation‐magnitude response maps as a single 2D image to effectively explore motion patterns. Moreover, boosted multi‐stream CNN‐based model with various attention modules is designed for human action recognition. The model incorporates a convolutional self‐attention autoencoder to represent compressed and high‐level motion features. Sequential convolutional self‐attention modules are used to exploit the implicit relationships within motion patterns. Furthermore, 2D discrete wavelet transform is employed to decompose RGB frames into discriminative coefficients, providing supplementary spatial information related to the actors actions. A spatial attention block, implemented through the weighted inception module in a CNN‐based structure, is designed to weigh the multi‐scale neighbours of various image patches. Moreover, local and global body pose features are combined by extracting informative joints based on geometry features and joint trajectories in 3D space. To provide the importance of specific channels in pose descriptors, a multi‐scale channel attention module is proposed. For each data modality, a boosted CNN‐based model is designed, and the action predictions from different streams are seamlessly integrated. The effectiveness of the proposed model is evaluated across multiple datasets, including HMDB51, UTD‐MHAD, and MSR‐daily activity, showcasing its potential in the field of action recognition."
attention-generative adversarial networks for simulating rain field,"The synthesis of rain fields is essential in multiple research fields and applications, including Single‐image Derain. However, there is a lack of research on simulated rain fields, and the existing rain field generation models struggle to capture complex spatial distributions and generate truly random rain fields. To address this, the authors propose a generative adversarial networks‐based rain field generation network, which consists of a generator, a discriminator, and a feature extraction block that can produce realistic and complex rain fields. The authors’ experiments demonstrate that this method achieves an average Frechet Inception Distance score of 0.035, and user studies indicate that the generated rain distribution looks naturally."
factor annealing decoupling compositional training method for imbalanced hyperspectral image classification,"Due to differences in the quantity and size of observed targets, hyperspectral images are characterized by class imbalance. The standard deep learning classification model training scheme optimizes the overall classification error, which may lead to performance imbalance between classes in hyperspectral image classification frameworks. Therefore, a novel factor annealing decoupling compositional training method is proposed in this paper. Without requiring resampling or reweighting, it implicitly modulates the training process, so standard models can sufficiently learn the representation of the minority classes and further be trained as robust classifiers. Specifically, the label‐distribution‐aware margin loss is combined with the error‐rate‐based cross‐entropy loss via combination factor, which considers both imbalanced data representation learning and classifier overall performance. Then, a factor annealing optimization training scheme is designed to adjust the combination factor, which solves the stage division problem of two‐stage decoupling learning. Experimental results on two hyperspectral image datasets demonstrate that, as compared with other competing approaches, the proposed method can continuously and stably optimize the model parameters, achieving improvements in class average metrics and difficult classes without affecting overall classification performance."
a siamese network optimization using genetic algorithm for brain diseases,"The complex nature of human brain tissues is important in ensuring accurate diagnosis to save human lives. Research on early detection of brain diseases has gained significant prominence within medical intelligence using highly complex model architectures with only a single label that cannot be verified. This paper introduces an innovative approach to Siamese Network Genetic Algorithm (SN‐GA) leveraging Siamese contrastive learning for classifying brain images across diverse diseases. Our core architecture is a Bi‐Convolutional Neural Network (Bi‐CNN) optimized by a genetic algorithm to enhance brain image classification. Specifically, five widely recognized transfer learning‐based architectures, namely AlexNet, Efficient‐B0, VGG‐16, ResNet‐50, and Inception‐v3, have been incorporated to evaluate the effectiveness of the proposed SN‐GA system. The performance of these models has been rigorously analyzed and compared using two distinct datasets: Brain tumors and Alzheimer's datasets. The experimental results robustly affirm the efficacy of the proposed Siamese model, yielding exceptional levels of accuracy, precision, and recall, all peaking at 97%. These findings underscore the potential and resilience of the optimized Siamese network in the context of brain disease classification, emphasizing its significance in advancing the field of medical imaging and diagnosis, with implications for early intervention and patient care."
a new network model for multiple object detection for autonomous vehicle detection in mining environment,"Considering the challenges of low multi‐object detection accuracy and difficulty in identifying small targets caused by challenging environmental conditions including irregular lighting patterns and ambient noise levels in the mining environment with autonomous electric locomotives. A new network model based on SOD−YOLOv5s−4L has been proposed to detect multi‐objects for autonomous electric locomotives in underground coal mines. Improvements have been applied in YOLOv5s to construct the SOD−YOLOv5s−4L model, by introducing the SIoU loss function to address the mismatch between real and predicted bounding box directions, facilitating the model to learn target position information more efficiently. This research introduces a decoupled head to enhance feature fusion and improve the positioning precision of the network model, enabling rapid capture of multi‐scale target features. Furthermore, the detection capability of the model has been increased by introducing the small target detection layer which is developed by increasing the number of detection layers from three to four. The experimental results on multiple object detection dataset show that the proposed model achieves significant improvement in mean average precision (mAP) of almost 98% for various types of targets and an average precision (AP) of nearly 99% for small targets on the other hand it achieves 5.19% (mAP) and 9.79% (AP) compared to the YOLOv5s model. Furthermore, comparative analysis with other models like YOLOv7 and YOLOv8 shows that the proposed model has superior performance in terms of object detection."
semantic segmentation of remote sensing images based on dual‐channel attention mechanism,"Due to the inadequate utilization of data correlation and complementarity in the feature extraction process of multimodal remote sensing images, the paper proposes a deep learning semantic segmentation algorithm based on the Dual Channel Attention Mechanism (DCAM). This algorithm uses U‐Net as the backbone, combining the RGB remote sensing image as one input channel with the Convolutional Block Attention Module to extract colour space features. Simultaneously, it utilizes near‐infrared (NIR) as another input channel with the Self‐Attention Module (SAM) to extract shape space features. Finally, by concatenating the multi‐scale attention features of the RGB remote sensing image channel and the NIR remote sensing image channel, it achieves the correlation and complementarity of contextual features between the two modal remote sensing images. Experimental results on the GID‐15 dataset demonstrate that the DCAM algorithm significantly improves the segmentation accuracy, edge segmentation quality, and object segmentation integrity for various types of targets compared to current mainstream segmentation methods."
improving the generalization of image denoising via structure‐preserved mlp‐based denoiser and generative diffusion prior,"Image denoising aims to remove noise from images and improve the quality of images. However, most image denoising methods heavily rely on pairwise training strategies and strict prior knowledge about image structure or noise distribution. While these methods exhibit significant results when handling known types of noise, their generalization performance diminishes when confronted with images containing unknown noise distributions. To address this issue, a two‐stage approach is introduced for enhancing the generalizability of image denoising. The proposed method does not rely on a large amount of paired data or prior knowledge of the noise type and level. Instead, it constructs a denoising pipeline with improved generalizability through an MLP‐based denoiser and generative diffusion prior. Specifically, in the first stage, an initial denoised image is predicted with a structure resembling that of the underlying clean image by introducing an MLP‐based U‐shaped denoising network aided by an implicit structural prior. In the second stage, the generalizability and quality of the denoiser are further enhanced by conditioning the result obtained from the previous stage on the pretrained denoising diffusion null‐space model. Extensive experimentation on multiple datasets demonstrates that this method exhibits better denoising performance and generalizability than other image denoising methods."
time-attentive fusion network: an efficient model for online detection of action start,"Online detection of action start is a significant and challenging task that requires prompt identification of action start positions and corresponding categories within streaming videos. This task presents challenges due to data imbalance, similarity in boundary content, and real‐time detection requirements. Here, a novel Time‐Attentive Fusion Network is introduced to address the requirements of improved action detection accuracy and operational efficiency. The time‐attentive fusion module is proposed, which consists of long‐term memory attention and the fusion feature learning mechanism, to improve spatial‐temporal feature learning. The temporal memory attention mechanism captures more effective temporal dependencies by employing weighted linear attention. The fusion feature learning mechanism facilitates the incorporation of current moment action information with historical data, thus enhancing the representation. The proposed method exhibits linear complexity and parallelism, enabling rapid training and inference speed. This method is evaluated on two challenging datasets: THUMOS’14 and ActivityNet v1.3. The experimental results demonstrate that the proposed method significantly outperforms existing state‐of‐the‐art methods in terms of both detection accuracy and inference speed."
an algorithm for detecting dense small objects in aerial photography based on coordinate position attention module,"To address the challenges of detecting a large number of objects and a high proportion of small objects in aerial drone imagery, an aerial dense small object detection algorithm called coordinate position attention module you only look once (CPAM‐YOLO) is proposed based on the coordinate position attention module (CPAM). In the backbone network of CPAM‐YOLO, a CPAM is proposed and embedded that decomposes channel attention into two 1D feature encoding processes, and selectively combines the features of each position through the weighted sum of all position features. Finally, features are aggregated along two spatial directions, increasing the effective information utilization of input feature positions and channels. The backbone network, feature enhancement network, and detection heads have been optimized to improve detection accuracy while ensuring a lightweight detection network. Using lightweight backbone networks to significantly reduce the number of parameters while using high‐resolution feature enhancement networks to retain more semantic and detailed features. The algorithm's performance was evaluated using the publicly available VisDrone2019 dataset. Compared to the baseline network YOLOv5l, CPAM‐YOLO achieved a 4.5% improvement in mAP0.5 and a 3.2% improvement in mAP0.95. These experimental results demonstrate the strong practicality of the CPAM‐YOLO object detection network for detecting dense small objects in aerial image."
unsupervised hyperspectral images classification using hypergraph convolutional extreme learning machines,"Aiming at the problem that traditional methods are difficult to fully utilize the rich spectral information in hyperspectral images (HSI) and fail to capture the complex higher‐order relations in hyperspectral data, which leads to limited classification performance extreme learning machine and fails to further improve the classification accuracy of HSIs, the authors propose the hypergraph convolutional extreme learning machine (HGCELM) method. The method not only inherits all the advantages of extreme learning machine (ELM), but also embeds hypergraph convolution for feature selection, which is capable of handling higher‐order relations. This enables HGCELM to capture more complex relationships between nodes and provide richer representation capabilities. At the same time, the training speed advantage of ELM is retained, thus speeding up the model training process. Experimental results show that the proposed algorithm achieves better accuracy compared to other clustering algorithms."
t-skeleton: accurate scene text detection via instance-aware skeleton embedding,"Existing segmentation‐based methods have made considerable progress in arbitrarily shaped text detection due to the advantage of dealing with shape variation. However, there still exist challenges to detecting accurate text instances with dense layouts, inaccurate annotations, and complex backgrounds. Many recent works have focused on improving arbitrary boundary prediction, but it may be difficult to accurately distinguish each instance of dense layouts because their boundary pixels may be mistakenly classified to produce inaccurate results (i.e., adhesive texts) with inaccurate annotation and complex backgrounds. Considering the local and long‐range dependencies, this paper proposes an efficient text detector, namely T‐Skeleton, to obtain more reliable segmentation detections. In the spirit of object skeletonization, we introduce the text instance skeleton highlighting the semantically significant structure (similar to the skeleton of a fish) to explicitly capture the long‐range dependencies of text instances. The key idea of T‐Skeleton is to calibrate the coarse text proposals by embedding text instance skeletons to separate crowd texts accurately and robustly. We further design a channel attention module to enlarge the performance margin between T‐Skeleton and the segmentation baseline. Experimental results on four publicly available datasets show the superiority of T‐Skeleton in handling long and curved texts."
research on surface defect detection model of steel strip based on mffa-yolov5,"The surface quality of steel strip is a critical indicator of the quality of hot‐rolled strip, so accurate inspection of its surface is essential. However, the complex texture of steel strip surface defects makes the detection challenging. Here, a multi‐scale feature fusion and attention based YOLOv5 (MFFA‐YOLOv5) model is proposed. Specifically, the bottom layer features are up‐sampled and fused not only with the middle layer features, but also with the top layer features, so that the model better captures the surface texture information of the steel strip. Secondly, an improved attention mechanism module is introduced to deal with the global and local information of the steel strip surface by introducing down‐sampling and up‐sampling paths based on Convolutional Block Attention Module (CBAM). Meanwhile, a self‐attention mechanism path is added to improve the capability of feature representation. Experimental results on the NEU‐DET dataset show that the MFFA‐YOLOv5 model significantly outperforms other state‐of‐the‐art methods."
maritime distress target detection algorithm based on yolov5s‐efoe network,"Traditional maritime search and rescue methods mainly rely on manual operation, which takes a long time to identify and results in poor search and rescue results. This paper applies computer vision technology to the field of maritime distress object detection and proposes an improved object detection algorithm YOLOv5s‐EFOE based on the YOLOv5s algorithm. Firstly, the authors change the detection head of the YOLOv5s algorithm to use a mixed channel strategy to build a more efficient decoupling head, reducing the number of 3 × 3 convolutional layers in the middle of the decoupling head to only one. The width of the head is scaled by the width multipliers of Backbone and Neck. Secondly, in conjunction with the SimOTA matching algorithm, the positive samples of the target to be tested are dynamically allocated to improve the recognition ability of different targets. Finally, considering the low pixel value of the maritime distress target from the perspective of unmanned aerial vehicles (UAV), the loss function CIoU in the original YOLOv5s is changed to EIoU. EIoU not only considers the distance and aspect ratio of the centre point, but also considers the true difference between the width and height of the prediction box and the real box, which improves the prediction accuracy of the anchor box. Experiments are conducted on a subset of the public dataset SeaDronesSee. The of the YOLOv5s‐EFOE model proposed in this paper reached 75.9%, reached 79.9%, and reached 44.5%. These indicators are superior to the original YOLOv5s model, YOLOv7 series models, and YOLOv8 series models. Compared with the original model, the YOLOv5s‐EFOE model has increased the by 5.4%, by 5.6%, and by 4.6%, improving the difficult to detect and missed detection situations. This model can be deployed on UAVs and can effectively identify maritime distress target, achieving the purpose of search and rescue."
robust image watermarking using ant colony optimization and fast generic radial harmonic fourier moment calculation,"In open networks, the geometric deformation and common image processing are common image manipulation modes, which pose a great challenge in robust watermarking. To improve the robustness of GRHFM‐based watermarking, a watermarking algorithm based on the fast GRHFM calculation method and ant colony optimization (ACO) based fractional parameter selection method is proposed. With the similarity between the discrete Fourier transform and GRHFM moment calculation, the algorithm utilizes fast Fourier transform to improve the GRHFM calculation accuracy and speed. To select the optimal GRHFM fractional parameter for watermarking algorithm, ACO is introduced to the fractional parameter adaptive selection method. Here, the fast Fourier transform‐based calculation method is combined with the adaptive parameter selection method to maximize the invisibility and robustness of watermarking. The experimental results indicate that the algorithm achieves higher robustness with the same payload and invisibility compared with other existing watermarking methods."
skeleton extraction of hard-pen regular script based on stroke characterization and ambiguous zone detection,"The Intelligent Evaluation System for Calligraphy Characters (IESCC) is used for teaching calligraphy, and users can learn calligraphy through the modifications given by the system. Chinese character skeleton extraction is an important step in the intelligent evaluation algorithm of calligraphic characters. The skeletons of Chinese characters extracted by traditional refinement algorithms are prone to redundant branches and deformed skeletons, which can lead to skeleton extraction results that do not conform to the topology of the original character. In this study, the focus lies on hard‐pen regular script, and skeleton repair and extraction are performed for these characters. According to the writing characteristics of regular script, the redundant burs are removed and the deformation zone of the thinned skeleton is detected, and then the idea of first splitting is used, then restructuring, to propose a skeleton extraction algorithm based on stroke characterization and ambiguous zone detection for hard‐pen regular script, referred to as SCAD. First, a thinning algorithm is used to extract the skeleton of Chinese characters and remove redundant pixels. By analyzing the stroke characteristics of regular script, the burrs are classified and different conditions are set to detect and remove the burrs. Then the ambiguous zones are detected according to the different kinds of junction points. Then, curvature, stroke width and direction deviation are used to analyze the continuity of stroke segments, and the decision function is used to classify the stroke segments. Finally, the stroke segments with optimal pairings were compensated by interpolation according to the direction trend. This concludes the skeleton extraction. Skeleton extraction is performed on 1000 sample characters, and the SCAD algorithm can extract the skeleton of Chinese characters with an accuracy of up to 98.37%. It is proved that the SCAD method proposed here is a practical and effective method to extract the skeleton of hard‐pen regular script."
topology‐aware anatomical segmentation of the circle of willis: hunet unveils the vascular network,"This research investigates the Circle of Willis, a critical vascular structure vital for cerebral blood supply. A modified novel dual‐pathway multi‐scale hierarchical upsampling network (HUNet) is presented, tailored explicitly for accurate segmentation of Circle of Willis anatomical components from medical imaging data. Evaluating both the multi‐label magnetic resonance angiography region of interest and the multi‐label magnetic resonance angiography whole brain‐case datasets, HUNet consistently outperforms the convolutional U‐net model, demonstrating superior capabilities and achieving higher accuracy across various classes. Additionally, the HUNet model achieves an exceptional dice similarity coefficient of 98.61 and 97.95, along with intersection over union scores of 73.32 and 85.76 in both the multi‐label magnetic resonance angiography region of interest and the multi‐label magnetic resonance angiography whole brain‐case datasets, respectively. These metrics highlight HUNet's exceptional performance in achieving precise and accurate segmentation of anatomical structures within the Circle of Willis, underscoring its robustness in medical image segmentation tasks. Visual representations substantiate HUNet's efficacy in delineating Circle of Willis structures, offering comprehensive insights into its superior performance."
a two-stage substation equipment classification method based on dual-scale attention,"Accurate classification of substation equipment images remains challenging due to various factors such as unexpected illumination, viewing angles, scale variations, shadows, surface contaminants, and different elements sharing similar appearances. This paper presents a novel two‐stage substation equipment classification method based on dual‐scale attention. Leveraging the region proposal technique from Faster‐regions with CNN features (RCNN), the input images are initially decomposed into multiple scales to capture latent features. A dual‐scale attention module is introduced to enhance the precision of feature extraction. Furthermore, a two‐stage network is proposed to address the challenge of classifying closely similar substation equipment. A multi‐layer perceptron performs a coarse classification to categorize the equipment into broad categories. Then, a lightweight classifier is employed for fine‐grained subclassification, further distinguishing equipment within the same broad category. To mitigate the issue of limited training data, a specialized dataset is collected and annotated for the substation equipment classification. Experimental results demonstrate that the proposed method achieves remarkable accuracy, recall, and F1‐score surpassing 0.91, outperforming mainstream approaches in terms of recall and F1 scores. Ablation experiments further validate the significant contributions of both the dual‐scale attention and the two‐stage classification module in improving the overall performance of the classification network."
dynamic estimator selection for double-bit-range estimation in vvc cabac entropy coding,"CABAC is the only entropy coding used in Versatile Video Coding (VVC). This is achieved through multiple estimators approach that provide more accurate predictions by considering different estimated probability results, but CABAC coding requires higher complexity and bit‐range accuracy than other approaches. Therefore, there is more potential to refine the performance from the perspective of bit allocation and architecture design. In this paper, a selection method is proposed to determine which estimator is recommended to dynamically perform the current entropy coding. Taking advantage of the double‐bit‐range architecture, the bits contained in the different estimators are also rearranged based on Most Probable Symbol () determination and Least Probable Symbol () considerations. Experimental reports in the work report that the coding time can be reduced using the proposed method and there is a slight gain in Peak Signal‐to‐Noise Ratio (PSNR) while saving some Rate‐distortion (RD) performance in bitrate."
yolo‐uod: an underwater small object detector via improved efficient layer aggregation network,"Accurate detection of underwater objects is a key indicator technology to effectively enhance the field of marine development and application, and is of great importance to various fields including marine military defense and seafood aquaculture. Efficient and rapid detection of underwater targets is a crucial technological challenge in this field. To meet the challenges posed by these issues, this study applies the convolutional omni‐efficient layer aggregation network (CO‐ELAN) module to the detector backbone to improve the ability of the network structure to acquire underwater objects from image information. The module improves the feature representation of gradient branching through a multi‐dimensional dynamic convolution and attention mechanism. In terms of loss calculation, the optimized normalized Wasserstein distance approach is used to predict the box distribution probabilistic modelling method to determine comparable distances to the ground box and obtain better samples of small target labels. Here, an underwater image enhancement algorithm based on white balance and underwater blur fusion is used to obtain clear images that enable improved detector performance. After the verification experiment on the URPC2018 dataset, it is found that the detector has better underwater detection ability compared with other detectors in the complex underwater environment. The proposed method achieves a 2.4% improvement over the YOLOv7 baseline model, while reducing computation costs by 5%."
aodimp-tir: anti-occlusion thermal infrared targets tracker based on superdimp,"To address the issue of tracking drift and failures in thermal infrared (TIR) tracking tasks caused by target occlusion, this study proposes an anti‐occlusion TIR target tracker named AODiMP‐TIR. This approach involves an anti‐occlusion strategy that relies on target occlusion status determination and trajectory prediction. This enables the prediction of the target's current position when it is identified as occluded, ensuring swift recapture upon reappearance. A criterion is introduced for occlusion status determination based on the classification response map of SuperDiMP. Additionally, a trajectory mapping module designed to decouple target motion from camera motion is presented, enhancing the precision of trajectory prediction. Comparative experiments with other state‐of‐the‐art trackers are conducted on the large‐scale high‐diversity thermal infrared object tracking benchmark (LSOTB‐TIR), LSOTB‐TIR100, and thermal infrared pedestrian tracking benchmark (PTB‐TIR) datasets. The results indicate that the AODiMP‐TIR performs well across all three datasets, particularly exhibiting outstanding performance in occlusion sequences. Furthermore, ablation study experiments confirm the effectiveness of the anti‐occlusion strategy, occlusion determination criterion and trajectory mapping module."
new method of colour image encryption using triple chaotic maps,"A new image encryption algorithm based on the triple chaotic maps is proposed to deal with the issues of inadequate security and low encryption efficiency. Coloured images consist of three linked channels used in the scheme. This method uses different keys to break the correlations between adjacent pixels in each channel. The triple chaotic maps are Lorenz, 2D‐Logistic, and Henon. First, the plain image is split into RGB channels to encrypt each channel separately. Second, the triple chaotic maps generate two groups of keys. The first group of keys performs a pixel permutation, resulting in scrambled channels used as input for the following step. Finally, the second group of keys is used to diffuse the scrambled channels independently, resulting in diffused channels, which are then merged to obtain a cipher image. The triple chaotic maps of different orders generate the cipher image with great unpredictability and security. The security is evaluated using various measures. The results demonstrated a high level of security attained by successfully encrypting coloured images. Recent encryption algorithms are compared in terms of entropy, correlation coefficients, and attack robustness. The proposed method provided outstanding security and outperformed existing image encryption algorithms."
vga-net: vessel graph based attentional u-net for retinal vessel segmentation,"Segmentation is crucial in diagnosing retinal diseases by accurately identifiying retinal vessels. This paper addresses the complexity of segmenting retinal vessels, highlighting the need for precise analysis of blood vessel structures. Despite the progress made by convolutional neural networsks (CNNs) in image segmentation, their limitations in capturing the global structure of retinal vsessels and maintaining segmentation continuity present challenges. To tackle these issues, our proposed network integrates graph convolutional networks (GCNs) and attention mechansims. This allows the model to consider pixel relationships and learn vessel graphical structures, significantly improving segmentation accuracy. Additionally, the attentional feature fusion module, including pixel‐wise and channel‐wise attention mechansims within the U‐Net architecture, refines the model's focus on relevant features. This paper emphasizes the importance of continuty preservation, ensuring an accurate representation of pixel‐level information and structural details during sefmentation. Therefore, our method performs as an effective solution to overcome challenges in retinal vessel segmentation. The proposed method outperformed the state‐of‐the‐art approaches on DRIVE (Digital Retinal Images for Vessel Extraction) and STARE (Structed Analysis of the Retina) datasets with accuracies of 0.12% and 0.14%, respecttively. Importantly, our proposed approach excelled in delineating slender and diminutive blood vessels, crucial for diagnosing vascular‐related diseases. Implementation is accessible on https://github.com/CVLab‐SHUT/VGA‐Net."
research on adaptive object detection via improved hsa‐yolov5 for raspberry maturity detection,"In the field of machine vision, target detection models have experienced rapid development and have been practically applied in various domains. In agriculture, target detection models are commonly used to identify various types of fruits. However, when it comes to recognizing berries, such as raspberries, the fruits nearing ripeness exhibit highly similar colours, posing a challenge for existing target detection models to accurately identify raspberries in this stage. Addressing this issue, a raspberry detection method called HSA‐YOLOv5 (HSV self‐adaption YOLOv5) is proposed. This method detects immature, nearly ripe, and ripe raspberries. The approach involves transforming the RGB colour space of the original dataset images into an improved HSV colour space. By adjusting corresponding parameters and enhancing the contrast of similar colours while retaining the maximum features of the original image, the method strengthens data features. Adaptive selection of HSV parameters is performed based on data captured under different weather conditions, applying homogeneous preprocessing to the dataset. The improved model is compared with the original YOLOv5 model using a self‐constructed dataset. Experimental results demonstrate that the improved model achieves a mean average precision (mAP) of 0.97, a 6.42 percentage point increase compared to the baseline YOLOv5 model. In terms of immature, nearly ripe, and ripe raspberries, there are improvements of 6, 4, and 7 percentage points, respectively, validating the effectiveness of the proposed model."
self-attention residual network-based spatial super-resolution synthesis for time-varying volumetric data,"In the field of scientific visualization, the upscaling of time‐varying volume is meaningful. It can be used in in situ visualization to help scientists overcome the limitations of I/O speed and storage capacity when analysing and visualizing large‐scale, time‐varying simulation data. This paper proposes self‐attention residual network‐based spatial super‐resolution (SARN‐SSR), a spatial super‐resolution model based on self‐attention residual networks that can generate time‐varying data with temporal coherence. SARN‐SSR consists of two components: a generator and a discriminator. The generator takes the low‐resolution volume sequences as the input and gives the corresponding high‐resolution volume sequences as the output. The discriminator takes both synthesized and real high‐resolution volume sequence as the input and gives a matrix to predict the realness as the output. To verify the validity of SARN‐SSR, four sets of time‐varying volume datasets are applied from scientific simulation. In addition, SARN‐SSR is compared on these datasets, both qualitatively and quantitatively, with two deep learning‐based techniques and one traditional technique. The experimental results show that by using this method, the closest time‐varying data to the ground truth can be obtained."
attention-based prohibited item detection in x-ray images during security checking,"This paper focuses on the intelligent detection of prohibited items in X‐ray images during the security checking process. An intelligent semantic segmentation model of prohibited items in X‐ray images is proposed based on the attention‐based object localization method. Based on the pre‐trained CNN classification framework, the attention mechanism can map the high‐layer semantic information of objects into the input space, while generating energy saliency maps to locate the prohibited items. In order to make the obtained attention maps discriminative, the lateral and contrastive inhibition strategies are introduced and combined together which can highlight the responses of activated neurons. Under the guidance of attention responses, two traditional image segmentation algorithms are employed to achieve the semantic segmentation results for the prohibited items detection in X‐ray images. The proposed semantic segmentation model relies on weakly supervised learning mechanism, and only depends on the category labels of prohibited items, which greatly avoids the work cost of data semantic annotation. The experimental results based on the public SIXray baseline and the self‐built X‐ray image database demonstrate the proposed method can achieve about 65% IoU localization precise averagely. In addition, comparison experiments were carried out with the state‐of‐the‐arts and ablation experiments to verify the effectiveness of the proposed model."
tanrscolour: transformer‐based medical image colourization with content and structure preservation,"Medical image colouring techniques enable to colourize grey‐scale medical images for assisting doctors in diagnosis. Benefiting from the non‐linear fitting ability of deep neural network, deep medical image colouring techniques have achieved remarkable results. However, existing methods are still facing content and structure feature leakage, unrealistic colouring and poor scale invariability. Thus, this paper, proposes a Transformer‐based medical image colouring algorithm with long‐term dependency to avoid feature leakage of coloured images. To be specific, this method employs two different Transformer encoders to generate and encode feature sequences for grey‐scale medical images and real human colour slice images, respectively. Then, a novel multi‐layer Transformer decoder is used to stylize grey‐scale map image features based on the real physical colour feature sequences. For colouring images at different scales, we implement content‐ aware positional encoding with scale invariance and propose style‐aware positional encoding strategy to take realistic and physical colour prior into account. Extensive experimental results indicate our method has achieved better colourization effects than recent state‐of‐the‐art medical image colourization methods."
research on image scaling algorithm for granular image detection,"In order to improve the accuracy of granular image detection during image scaling, an image scaling algorithm combining the interpolation algorithm of protective features and Kalman filtering of neurons is proposed. Firstly, the interpolation algorithm of protective features is de‐designed according to the phenomenon of sudden change of edge grey values, and the difference and trend of grey values in horizontal and vertical directions are used to obtain the pre‐scaled image with maximum contrast. Then the grey value data input of the pre‐scaled image is introduced into the Kalman filter of neurons, and the filtering process is optimized by using the black‐box thinking of neurons, and the pre‐scaled image is smoothed. Finally, the scale transformation is implemented to obtain the final grey value to complete the final scaling, so that the image has both visual experience and features. Comparison with mainstream image scaling algorithms shows that the proposed algorithm can effectively overcome the degradation of granularity image detection accuracy due to image scaling, with point acutance value (PAV) improved by 10%–41%, and the granularity detection error caused by image scaling algorithms reduced from about 0.7% to about 0.1%, with a relative improvement of 85%."
diverse branch feature refinement network for efficient multi-scale super-resolution,"Despite the existence of various super‐resolution (SR) methods, most of them focus on designing models for specific upscaling factors rather than fully exploiting inter‐scale correlation to improve efficiency. In contrast, multi‐scale SR methods can effectively reduce the redundancy of network parameters by aggregating the feature extraction processes corresponding to multiple scales into a unified process. The aim of this study is to enhance the compactness and efficiency of the SR model. Thus, an efficient multi‐scale SR method called the diverse branch feature refinement network (DBFRN) is proposed. By decoupling the training process and inference process based on the idea of structural re‐parameterization, multi‐branch topology is adopted to enrich multi‐scale learning and merge branches to achieve efficient inference with equivalent effects. Specifically, two re‐parameterization strategies are designed and two corresponding feature refinement blocks for different feature levels in multi‐scale SR network. Extensive experiments demonstrate that the proposed multi‐scale SR method is effective and efficient, and it can outperform advanced single‐scale methods in terms of quantity and quality."
fskt-ge: feature maps similarity knowledge transfer for low-resolution gaze estimation,"The limited of texture details information in low‐resolution facial or eye images presents a challenge for gaze estimation. To address this, FSKT‐GE (feature maps similarity knowledge transfer for low‐resolution gaze estimation) is proposed, a gaze estimation framework consisting of both a high resolution (HR) network and low resolution (LR) network with the identical structure. Rather than mere feature imitation, this issue is addressed by assessing the cosine similarity of feature layers, emphasizing the distribution similarity between the HR and LR networks. This enables the LR network to acquire richer knowledge. This framework utilizes a combination loss function, incorporating cosine similarity measurement, soft loss based on probability distribution difference and gaze direction output, along with a hard loss from the LR network output layer. This approach on low‐resolution datasets derived from Gaze360 and RT‐Gene datasets is validated, demonstrating excellent performance in low‐resolution gaze estimation. Evaluations on low‐resolution images obtained through 2×, 4×, and 8× down‐sampling are conducted on two datasets. On the Gaze360 dataset, the lowest mean angular errors of 10.97°, 11.22°, and 13.61° were achieved, while on the RT‐Gene dataset, the lowest mean angular errors of 6.73°, 6.83°, and 7.75° were obtained."
unsupervised remote sensing image thin cloud removal method based on contrastive learning,"Cloud removal algorithm is a crucial step of remote sensing image preprocessing. The current mainstream remote sensing image cloud removal algorithms are implemented based on deep learning, and most of them are supervised. A large number of data pairs are required for training to achieve cloud removal. However, real with/without cloud image pairs datasets are difficult to obtain in the real world, and the models obtained by training on synthetic datasets often need to generalize better to natural scenes. And the existing unsupervised thin cloud removal methods based on Cycle‐GAN framework with considerable model complexity and unstable training are not an excellent solution to the problem of lack of paired datasets. Based on this, in this paper, the authors propose an unsupervised remote sensing image thin cloud removal method based on contrastive learning—GAN‐UD. It is a network consisting of a frequency‐spatial attention generator and a discriminator. In addition, the authors introduce local contrastive loss and global content loss to constrain the content of the generated images to ensure that the generated cloud‐free images are consistent with the input cloud images in terms of image content. Experimental results show that the proposed method in this paper can still effectively remove thin clouds from remote sensing images without paired training datasets, outperforms current unsupervised cloud removal methods, and achieves comparable performance to supervised methods."
daf‐retinex: preserve the image detailed features and restore the reflected image,"Currently, deep learning methods for low‐light image enhancement tasks mainly focus on the illumination of images, while neglecting the problems of image noise and feature loss. To address this issue, this paper proposes a novel low‐light image enhancement network called DAF‐Retinex, based on the Retinex‐Net. To address the issue of image noise, different from traditional image denoise methods, this paper utilizes a fully convolutional neural network to denoise the reflection component, additionally, a denoising loss function is introduced to suppress noise. For preserving image details and extracting features, this paper creatively introduces self‐calibrated convolutions into low‐light image enhancement tasks, furthermore, a feature augmented attention block consisting of feature‐guided attention (FGA) is designed for feature learning to effectively enhance image illumination and extract image detail features. Experimental results demonstrate that the proposed algorithm in this paper effectively removes image noise and extracts detailed features, resulting in visually improved outcomes. On public datasets, the average improvement in objective evaluation metrics of image quality such as PSNR, SSIM, and NIQE are 1.13%, 4.12%, and 1.28%, respectively."
research on a multi‐scale degradation fusion network in all‐in‐one image restoration,"Image restoration aims to recover high‐quality clean images from degraded low‐quality ones. Deep learning‐based approaches have been a focal point in the field of image restoration. However, most methods focus solely on a single type of degradation and may not extend well to real‐world scenarios with unknown degradation. For this purpose, the present study introduces an all‐in‐one image restoration approach by designing a multi‐scale feature fusion UNet structure (MdfUNet). In summary, the proposed method exhibits two significant advantages. For starters, it implicitly fuses degradation information across multiple scales, enabling the network to extract rich hierarchical features and enhancing its generalization ability towards unknown degradations. Secondly, MdfUnet possesses strong image reconstruction capabilities. It utilizes a simple non‐linear feature optimizer to enhance skip connections, providing rich feature representations for the image reconstruction process, and ultimately generating high‐quality restored images. Extensive experimental results show the proposed method outperforms multiple baselines on deraining, dehazing, and denoising datasets."
automatic measurement of slug flow processes from in-line videos,"Slug flow processes draw much attention in chemical and pharmaceutical manufacturing thanks to their ability to eliminate downtime costs and batch‐to‐batch variation. Using in‐line video can monitor the current process status and improve the stability of the slug flow. However, there are no effective image processing methods aiming at such videos from chemical experiments due to limited training sample size and the variety of experimental settings. The paper proposes a training‐free method to automatically detect and measure the slugs from in‐line videos. Multiple image features are fused to identify the slug shapes under various lighting conditions, a six‐point model is fitted to achieve better volume estimation, and a consistency score is estimated to quantify the detection uncertainty. The proposed algorithm achieves similar results to manual labeling for various types of slug flow processes, improving the accuracy of slug column estimation by a large margin compared to the existing automatic methods. Finally, using it to monitor the slug flow process's stability and the change of the slug volume in an injection point are demonstrated. The method not only shows that handcrafted image features have the capability for detection and segmentation from chemical experiment images but also paves the road for a training‐based algorithm for the task."
anomaly-background separation and particle swarm optimization based band selection for hyperspectral anomaly detection,"As one of the dimensionality reduction techniques of hyperspectral image (HSI), band selection (BS) does not change the spectral characteristics and physical meaning of HSIs, which is beneficial to the identification and analysis of surface objects. Recently, many BS methods for target detection have achieved promising results by making full use of the priori spectral features of the target to be detected. Conversely, anomaly detection separates the anomaly based solely on the statistical distribution difference between anomaly and background without any prior information. Therefore, the development of BS for anomaly detection has lagged far behind that of BS for target detection. To this end, this paper proposes a novel BS algorithm dedicated to anomaly detection tasks, named anomaly‐background separation and particle swarm optimization (PSO)‐based BS. Specifically, an anomaly‐background separation framework (ABSF) is established to predetermine a priori knowledge of anomaly distribution. Then, three band prioritization criteria are constructed with the anomaly‐background constraints generated by ABSF. Finally, PSO is used to find the optimal subset of bands in the solution space. The experiments on two real datasets demonstrate that the proposed method yields better detection results and greater stability compared to other BS methods discussed in this paper."
probabilistic 3d motion model for object tracking in aerial applications,"Visual object tracking, crucial in aerial applications such as surveillance, cinematography, and chasing, faces challenges despite AI advancements. Current solutions lack full reliability, leading to common tracking failures in the presence of fast motions or long‐term occlusions of the subject. To tackle this issue, a 3D motion model is proposed that employs camera/vehicle states to locate a subject in the inertial coordinates. Next, a probability distribution is generated over future trajectories and they are sampled using a Monte Carlo technique to provide search regions that are fed into an online appearance learning process. This 3D motion model incorporates machine‐learning approaches for direct range estimation from monocular images. The model adapts computationally by adjusting search areas based on tracking confidence. It is integrated into DiMP, an online and deep learning‐based appearance model. The resulting tracker is evaluated on the VIOT dataset with sequences of both images and camera states, achieving a 68.9% tracking precision compared to DiMP's 49.7%. This approach demonstrates increased tracking duration, improved recovery after occlusions, and faster motions. Additionally, this strategy outperforms random searches by about 3.0%."
a feature-enhanced hybrid attention network for traffic sign recognition in real scenes,"Currently, traffic sign recognition techniques have been brought into the assistive driving of automobiles. However, small traffic sign recognition in real scenes is still a challenging task due to the class imbalance issue and the size limit of the traffic signs. To address the above issues, a feature‐enhanced hybrid attention network is proposed based on YOLOv5s for a small, fast, and accurate traffic sign detector. First, a series of online data augmentation strategies are designed in the preprocessing module for the model training. Second, the hybrid channel and spatial attention module CSAM are integrated into the backbone for a better feature extraction ability. Third, the channel attention module CAM is used in the detection head for a more efficient feature fusion ability. To validate the approach, extensive experiments are conducted based on the Tsinghua‐Tencent 100K dataset. It is found that the novel method achieves state‐of‐the‐art performance with only negligible increases in the model parameter and computational overhead. Specifically, the , parameters, and FLOPs are 85.8%, 7.13 M, and 16.1 G, respectively."
adf‐net: attention‐guided deep feature decomposition network for infrared and visible image fusion,"To effectively enhance the ability to acquire information by making full use of the complementary features of infrared and visible images, the widely used image fusion algorithm is faced with challenges such as information loss and image blurring. In response to this issue, the authors propose a dual‐branch deep hierarchical fusion network (ADF‐Net) guided by an attention mechanism. Initially, the attention convolution module extracts the shallow features of the image. Subsequently, a dual‐branch deep decomposition feature extractor is introduced, where in the transformer encoder block (TEB) employs remote attention to process low‐frequency global features, while the CNN encoder block (CEB) extracts high‐frequency local information. Ultimately, the global fusion layer based on TEB and the local fusion layer based on CEB produce the fused image through the encoder. Multiple experiments demonstrate that ADF‐Net excels in various aspects by utilizing two‐stage training and an appropriate loss function for training and testing."
a novel automatic annotation method for whole slide pathological images combined clustering and edge detection technique,"Pixel‐level labeling of regions of interest in an image is a key step in building a labeled training dataset for supervised deep learning networks of images. However, traditional manual labeling of cancerous regions in digital pathological images by doctors is time‐consuming and inefficient. To address this issue, this paper proposes an automatic labeling method for whole slide images, which combines clustering and edge detection techniques. The proposed method utilizes the multi‐level feature fusion model and the Long‐Short Term Memory network to discriminate the cancerous nature of the whole slide images, thereby improving the classification accuracy of the whole slide images. Subsequently, the automatic labeling of cancerous regions is achieved by integrating a density‐based clustering algorithm and an edge point extraction algorithm, both based on the discriminated results of the cancerous properties of whole slide images. The experimental results demonstrate the effectiveness of the proposed method, which offers an efficient and accurate solution to the challenging task of cancerous region labeling in digital pathological images."
microalgae detection based on improved yolov5,"Accurate detection of algae in microscopic image plays a crucial role in water quality monitoring. However, the existing object detection methods still face challenges in accurately detecting different categories of algae in microscopic image. In order to improve the accuracy, an improved YOLOv5s model is proposed for microalgae detection, by combining a Receptive Field Enhancement (RFE) module, the Wise‐IoU v3 dynamic non‐monotonic focal loss function, and a Dynamic head (Dyhead), which is termed receptive field enhancement wise‐IoU dyhead (RWD)‐you only look once (YOLO). Firstly, to detect microalgae of various scales, the Bottleneck in the C3 module of YOLOv5s is replaced with a more reasonable RFE module. Secondly, Wise‐IoU v3 is applied to enhance detection accuracy by assigning varying weights between high‐quality and low‐quality images. Finally, Dyhead is introduced to enhance the representation capacity of the detection head by integrating three attention mechanisms: scale awareness, spatial awareness, and task awareness. The proposed RWD‐YOLO model significantly enhances the accuracy of algae detection in microscopic image. Specifically, the experimental results on the microalgae dataset show that the RWD‐YOLO achieves an mAP@0.5 of 93.2% and an mAP@0.5:0.95 of 65.1%. Compared to the original YOLOv5s, mAP@0.5 and mAP@0.5:0.95 are improved by 3.7% and 5.7%, respectively."
brake disc positioning and defect detection method based on improved canny operator,"Firstly, the Canny operator combined with circle fitting is used to position the working surface. Secondly, the median filter and the Laplacian operator are selected for preprocessing. Due to uneven illumination, surface texture, or environmental dust, the extracted scratches exhibit fragmentation, and traditional morphological operations cannot handle scratches well, so an improved Canny operator combined with probabilistic Hough transform is proposed for scratch connection. A bidirectional connection is introduced to optimize the edge detection process of the Canny operator, and a parallel algorithm is introduced to shorten the detection time. Finally, the standard Hough transform needs to traverse the entire parameter space, and the calculation is complicated, and the probability Hough transform is selected. The experimental results show that compared with the traditional Canny algorithm, the precision, recall, and F1‐score of the improved Canny algorithm are increased by 3.2%, 13.2%, and 6.0%, respectively. After adding parallel processing, the average detection time was reduced by 45 ms. Finally, the accuracy of scratches extracted using this algorithm reached 97.96%, the leak rate, and mistake rate of this study are only 3.33% and 2.00%, which provides a more accurate and efficient theoretical support for the research on brake disc scratch detection."
vpdet: refined region cnn for hair follicle recognition in arbitrary angle,"In response to the growing demand for hair‐loss treatments, this study introduces the vector proposal detector (VPDet), a groundbreaking solution in hair transplant robotics. VPDet, distinct from traditional approaches, addresses the complex challenges of hair follicle detection, notably the variability in hair growth orientations and the intricacies of hair clustering. The method innovatively leverages the linear nature of hair, spanning a full 360‐degree orientation spectrum. The VPDet framework, a novel two‐stage object detection system, incorporates the vector proposal network and vector align blocks. These elements are crucial in transforming conventional anchor boxes into anchor vectors, thereby generating reference vectors across various scales and angles. The vector align block, a key innovation, uniquely combines vector and adjacent feature data to align features through shared maps. The extensive experiments, conducted on the FDU_HairFollicleDataset and an extended dataset, exhibit a remarkable enhancement in model performance, with a 51.3% increase in precision and a 20.8% boost in F1 score. The results not only demonstrate VPDet's superior capability in hair follicle recognition but also its potential in posture recognition for vector‐characteristic objects. This approach represents a significant advancement in both the field of hair transplant robotics and vector‐based object detection."
color subspace exploring for natural image matting,"Deep neural networks have seen a surge of successful methods in natural image matting. However, the overlap of foreground and background color distributions in an image is still troubling in matting. It is observed that the three color channels contain different contrast information of an image: some color channels may provide clearer contrast information for separating the foreground from the image, while the foreground and background color distributions in other channels may heavily overlap, resulting in blurred foreground‐background boundaries. Motivated by this observation, the Color Subspace Exploring Network (CSEMat) is proposed to extract the foreground object from an image by exploring high‐contrast appearance information in individual color spaces. Specifically, a 4‐branch encoder is constructed, with one branch for the RGB image and three branches for subdividing the color space. Each color channel is individually processed by a sub‐encoder. Additionally, the trimap‐based color information aggregation module (CIA) is introduced to integrate the feature maps from the independent sub‐encoders, facilitating the transfer of optimized features to the decoder. Extensive experiments demonstrate that the proposed CSEMat achieves favorable performance on publicly available matting datasets."
adaptively hybrid fractal image coding,"An adaptively hybrid method was proposed to improve the performance of fractal coding methods. First, it is found that the range blocks with large variances (RBLVs) play a crucial role in degrading decoded images, and the effect of the remaining range blocks with small variances (RBSVs) can be ignored. Then, an adaptive method was proposed to divide the range blocks into the above two categories: RBLVs and RBSVs. Second, RBLVs were designed to be encoded in an extended domain block pool (EDBP). Then, better block‐matching effect can be obtained, which will result in better decoded image quality. Further, the no‐search fractal encoding method is adopted for RBSVs to achieve faster encoding speed and fewer bits per pixel (bpp). Finally, four fractal coding methods were adopted to assess the performance of the proposed method. Experimental results show that compared with the previous methods, the PSNR quality of decoded images in the proposed method can be improved by about 0.15–0.4 dB, about 20%–35% of the total computations in encoding process can be saved, and about 0.2 bpp can be saved. Moreover, under the same decoding time, the proposed method can achieve comparable or smaller deviations regarding the decoded image."
weakly supervised instance segmentation via peak mining and filtering,"Learning the full extent of pixel‐level instance response in a weakly supervised manner remains unsatisfactory. Peak response maps (PRMs) localizes the discriminative object regions but cannot provide complete instance information, suffering from incomplete segmentation and unreliable mask prediction by noisy proposal retrieval. This work tackles this challenging problem by mining diverse class peak responses that include more discriminative and complete object regions and retrieving more reliable proposals from noisy segment proposal galleries. First, the existing method is enhanced with two more classification branches, thus contributing to more diverse and abundant instance regions from peak response maps. The mined class peak responses from two of the branches are then merged to generate more complete peak response maps by a clustering approach in their deep feature space. Then, instance segmentation masks are retrieved from a noisy object segment proposal gallery with class confidence, which is calculated by a normal classifier to obtain cleaner mask prediction. Finally, the pseudo‐supervision can be used to train an instance segmentation network in a fully supervised manner. Experiments on the PASCAL VOC 2012 dataset and COCO dataset show that the approach works effectively and outperforms other counterparts by a margin of more than 6 %, 4%, and 3% with the mean average precision (mAP) at IoU threshold of 0.25, 0.5 and 0.75, respectively."
3d position and pose measurement based on coded light field,"High‐precision relative position and attitude measurement technology has a wide range of applications in aerospace and industrial production. Currently, the commonly used method for measurement is based on visual cooperative signs. However, its accuracy significantly decreases as the distance increases. Therefore, a relative positioning system is designed based on light field spatial coding and visual recognition. The projector emits spatially encoded structured light within the coverage of the light field, while the receiving end captures, identifies, measures the code, and calculates its pose in the light field coordinate system. Compared with the traditional measurement method, the measurement accuracy of this system does not decrease greatly with the increase in distance, the measurement distance can be adjusted in real‐time and does not depend on an external light source. By changing the projection pattern with different resolutions without changing hardware systems, it can adjust effective measurement distance accordingly. Theoretical and experimental results show that the proposed method can maintain measurement accuracy with the increase in distance."
a lightweight waxberry fruit detection model based on yolov5,"In order to solve the safety and efficiency problems in the picking process of Waxberry, the slow speed and low precision of high‐density Waxberry target detection under a complex background were studied. A lightweight Waxberry target detection algorithm based on YOLOv5 is studied. In this study, C3‐Faster1 and C3‐Faster2 modules are proposed, which are located in the backbone and neck of the network: C3‐Faster1 can improve the model speed with a simple structure; C3‐Faster2 integrates the context attention mechanism and transform module based on C3‐Faster1 to make the network pay attention to the information of Waxberry image context and expand the channel receptive field. A new pyramid module, SPPFCSPC, is proposed to expand the sensing field and improve the accuracy of boundary detection. It also combines the Coordinate Attention (CA) and Dyhead dynamic detection head to suppress useless information and enhance the detection ability of small targets. Compared to YOLOv4, YOLOv7, and YOLOv8, mean accuracy percentage (mAP) improved by 5.7%, 9.4%, 8.3%. Compared to the base YOLOv5 model, mAP has improved from 86.5% to 91.9%, running on 2 GB Jeston nano, and the improved model is 5.03 frames per second (FPS) faster than YOLOv5. Experiments show that the designed module is more effective in Waxberry detection tasks."
nhd-yolo: improved yolov8 using optimized neck and head for product surface defect detection with data augmentation,"Surface defect detection is an essential task for ensuring the quality of products. Many excellent object detectors have been employed to detect surface defects in resent years, which has achieved outstanding success. To further improve the detection performance, a defect detector based on state‐of‐the‐art YOLOv8, named improved YOLOv8 by neck, head and data (NHD‐YOLO), is proposed. Specifically, YOLOv8 from three crucial aspects including neck, head and data is improved. First, a shortcut feature pyramid network is designed to effectively fuse features from backbone by improving the information transmission. Then, an adaptive decoupled head is proposed to alleviate the feature spatial misalignment between the classification and regression tasks. Finally, to enhance the training on small objects, a data augmentation method named selective small object copy and paste is proposed. Extensive experiments are conducted on three real‐world datasets: detection dataset from Northeastern University (NEU‐DET), printed circuit boards from Peking University (PKU‐Market‐PCB) and common objects in context (COCO). According to the results, NHD‐YOLO achieves the highest detection accuracy and exhibits outstanding inference speed and generalisation performance."
absolute velocity estimation of uavs based on phase correlation and monocular vision in unknown gnss‐denied environments,"This paper proposes a novel approach for absolute velocity estimation of unmanned aerial vehicles in unknown and unmapped GNSS‐denied environments. The proposed method leverages the advantages of Fourier‐based image phase correlation and utilizes off‐the‐shelf onboard sensors, including a downward‐facing monocular camera, an inertial sensor, and a sonar. The non‐matching tracking approach is particularly appealing, offering accurate estimation while remaining robust against frequency‐dependent noise, significant intensity variations, and time‐varying illumination disturbances. In the proposed method, the first step involves computing global pixel motion from consecutive images using phase correlation, which utilizes the shift property of the Fourier transform. This pixel motion estimation serves as the basis for creating a closed‐loop solution for absolute velocity estimation. To further enhance accuracy, a Kalman filter is implemented to fuse all available data and provide a reliable velocity estimate. Validation of the proposed visual‐inertial technique is conducted through simulation experiments using AirSim and real‐world flight tests. The results demonstrate the practicality and effectiveness of the approach across a range of challenging scenarios."
a person re-identification method for sports event scenes incorporating textual information mining,"Person re‐identification represents a pivotal sub‐problem in image retrieval, boasting broad application prospects in fields such as intelligent security and video surveillance. However, most existing person re‐identification methods predominantly focus solely on visual features pertaining to the person targets, thereby disregarding some supporting information closely related to the scene context. In the context of athlete re‐identification during sports event scenes, the athlete bib number is fully considered, an important clue that can provide different athletes' identities, and the traditional visual features of the person and high‐level semantic information of the bib number text are fused. A multi‐source information mutual gain mechanism is designed to improve the accuracy of the person re‐identification task. In the existing only publicly available marathon bib number dataset RBNR, the recognition accuracy of this method is significantly superior to that of the existing person re‐identification method. In addition, this paper constructs and publishes an athlete re‐identification dataset (HNNU‐ReID8000) for mainstream sports events, and the mean average precision (mAP) value of this method reaches 96.1% on this dataset, significantly ahead of existing state‐of‐the‐art person re‐identification methods. The code and the HNNU‐ReID8000 dataset will be released at https://github.com/yanbin‐zhu/zyb_person‐reid."
ssa-unet: whole brain segmentation by u-net with squeeze-and-excitation block and self-attention block from the 2.5d slice image,"Whole brain segmentation from magnetic resonance images (MRI) is crucial in diagnosing brain diseases and analyzing neuroimaging data. Despite advances through deep learning, challenges such as uneven gray distribution and the presence of artifacts still present hurdles in medical image processing. These limitations are often a result of insufficient spatial contextual information and lack of attention to important regions within existing models. To address these issues, this paper presents SSA‐UNet (Squeeze‐and‐Excitation and Self‐Attention UNet), a uniquely designed deep convolutional neural network that integrates spatial constraints by converting three consecutive 2D MRI slices into a single 2.5D image. This facilitates capturing inter‐slice dependencies effectively. Additionally, the newly formulated SSA block, which sequentially incorporates channel attention and Self‐Attention mechanisms, is placed before the decoders in the conventional U‐Net architecture. This enables the network to automatically weight different feature maps and focus more effectively on regions requiring precise segmentation. Rigorous evaluations on LPBA40 and IBSR18 datasets substantiate the remarkable improvements in accuracy and stability achieved by SSA‐UNet. Results indicate Dice coefficients of 98.38% and 97.47%, specificity of 99.69% and 99.57%, and sensitivity of 98.5% and 97.98% for the respective datasets. Compared to other existing models, SSA‐UNet shows significant improvements on both the LPBA40 and IBSR18 datasets. On the LPBA40 dataset, SSA‐UNet's Dice coefficient improved by 0.33% compared to the sub‐optimal model, while on the IBSR18 dataset, the improvement reached 1.78%. These empirical findings demonstrate SSA‐UNet's heightened capability in addressing the long‐standing challenges in MRI‐based whole‐brain segmentation."
fusing angular features for skeleton-based action recognition using multi-stream graph convolution network,"Distinguishing similar actions has been a challenging challenge in skeleton‐based action recognition. Since the joint coordinates in these actions are similar, it is difficult to accomplish the recognition task using traditional joint features. To address this issue, the use of angle features to capture subtle nuances in various body parts, along with a critical angle enhancement module that assigns weights to different angle feature representations for a given action are proposed, highlighting the critical angle feature representation. The approach is evaluated using a three‐stream ensemble method on three large action recognition datasets, NTU‐RGB+D, NTU‐RGB+D 120, and Kinetics‐400. The experimental results demonstrate that incorporating angular information can effectively complement joint and skeletal features, leading to improved recognition of similar actions and enhanced model performance and robustness."
rrer: a refined registration method based on contrast minimum for event and rgb cameras,"The precise perception of the surrounding environment in traffic scenes is an important part of an intelligent transportation system. The event camera could provide complementary information to traditional frame‐based cameras, such as high dynamic range, and high time resolution, in the perception of traffic targets. To improve the precision and reliability of perception as well as facilitate lots of RGB camera‐based studies introduced to event cameras directly, a refined registration method for event‐based cameras and RGB cameras on the basis of pixel‐level region segmentation is proposed, to provide a fusion method at pixel level. A total of eight sequences and a dataset containing 260 typical traffic scenes are contained in the experiment dataset, both selected from DSEC, a traffic event‐based dataset. The registered event image shows a better spatial consistency with RGB images visually. Compared to the baseline, the evaluation indicators, such as the performance of the contrast, the proportion of overlapping pixels, and average registration accuracy have been improved. In the traffic object segmentation task, the average boundary displacement error of our method has decreased and the max decline value has reached 79.665%, compared to the boundary displacement error between ground truth and baseline. These results indicate prospective applications in the perception of intelligent transportation systems combined with event and RGB cameras. The traffic dataset with pixel‐level semantic annotations will be provided soon."
modelling appearance variations in expressive and neutral face image for automatic facial expression recognition,"In automatic facial expression recognition (AFER) systems, modelling the spatio‐temporal feature information in a specific manner, coalescing, and its effective utilization is challenging. The state‐of‐the‐art studies have examined integrating multiple features to enhance the recognition rate of AFER systems. However, the feature variations between expressive and neutral face images are not fully explored to identify the expression class. The proposed research presents an innovative approach to AFER by modelling appearance variations in both expressive and neutral face images. The prominent contributions of the work are developing a novel and hybrid feature space by integrating the discriminative feature distribution derived from expressive and neutral face images; preserving the highly discriminative latent feature distribution using autoencoders. Local binary pattern (LBP) and histogram of oriented gradients (HOG) are the feature descriptors employed to derive the discriminative texture and shape information, respectively. The component‐based approach is employed, wherein the features are derived from the salient facial regions instead of the whole face. The three‐stage stacked deep convolutional autoencoder (SDCA) and multi‐class support vector machine (MSVM) are employed to address dimensionality reduction and classification, respectively. The efficacy of the proposed model is substantiated by empirical findings, which establish its superiority in terms of accuracy in AFER tasks on widely recognized benchmark datasets."
am-mulfsnet: a fast semantic segmentation network combining attention mechanism and multi-branch,"In order to balance accuracy and real‐time performance in semantic segmentation, this paper proposes a real‐time semantic segmentation algorithm model based on attention mechanism and multi‐branch feature fusion using Fast convolutional neural network model (Fast‐SCNN). In this method, the spatial detail feature enhancement branch is introduced to enhance spatial detail features firstly. Then, through rational design of fusion module, the feature information of each branch is optimized to achieve better fusion of deep and shallow features. At the end of the feature fusion module, an adaptive feature enhancement focus module is introduced to capture the interdependence between remote pixels. The experimental results show that the proposed algorithm achieves 71.55% segmentation accuracy on Cityscapes dataset, the reasoning speed FPS is 97.6 frames/s, and the number of parameters is 1.39 M, which verifies the effectiveness of the network model constructed by the algorithm. Code is available at https://github.com/ccchhheeennn/model."
an infrared and visible image fusion network based on multi-scale feature cascades and non-local attention,"In recent years, research on infrared and visible image fusion has mainly focused on deep learning‐based approaches, particularly deep neural networks with auto‐encoder architectures. However, these approaches suffer from problems such as insufficient feature extraction capability and inefficient fusion strategies. Therefore, this paper introduces a novel image fusion network to address the limitations of infrared and visible image fusion networks with auto‐encoder architectures. In the designed network, the encoder employs a multi‐branch cascade structure, and these convolution branches with different kernel sizes provide the encoder with an adaptive receptive field to extract multi‐scale features. In addition, the fusion layer incorporates a non‐local attention module that is inspired by the self‐attention mechanism. With its global receptive field, this module is used to build a non‐local attention fusion network, which works together with the ‐norm spatial fusion strategy to extract, split, filter, and fuse global and local features. Comparative experiments on the TNO and MSRS datasets demonstrate that the proposed method outperforms other state‐of‐the‐art fusion approaches."
light field imaging technology for virtual reality content creation: a review,"The light field (LF) imaging technique can capture 3D scene information in 4D by recording both 2D intensity and 2D direction of incoming light rays. Due to this capability, LF has shown a great interest in virtual reality (VR) and augmented reality (AR) for enhanced immersion, improved depth perception and reconstruction of realistic 3D environments. This paper presents a comprehensive review of LF imaging technology and other approaches used for VR content creation. The applications of LF technology beyond VR and AR are also discussed. The challenges and limitations of other approaches for VR content creation are examined. State‐of‐the‐art research has focused on how VR experiences benefit from LF technology and identified the challenges to creating comfortable, immersive and realistic VR content such as (1) image size and resolution, (2) processing speed, (3) precise calibration and (4) depth reconstruction. Recommendations that can be considered for creating immersive VR content are provided to enhance user experience. These recommendations aim to contribute to developing more comfortable and realistic VR content, extending the potential applications of LF imaging technology in diverse fields."
noise2variance: dual networks with variance constraint for self‐supervised real‐world image denoising,"Image denoising aims to restore a clean image from a noisy image. Traditional methods utilizing convolutional neural networks (CNN) for denoising are trained using pairs of noisy and clean images to comprehend the transformation from a noisy image to a clean one. However, the acquisition of such image pairs in real‐world scenarios presents a challenge. Hence, numerous self‐supervised denoising techniques have been developed that do not require clean images for training. This study demonstrates that a straightforward loss design, concentrating on variance, can effectively train a standard CNN denoiser in a self‐supervised fashion. A novel theoretical framework is introduced for training a basic CNN denoising model using three constraints: mean, variance, and augmentation. The variance constraint is crucial as it prevents the trained model from converging to trivial solutions such as identity or zero mapping. This theory provides valuable insights for the development of new self‐supervised denoising methods. Furthermore, a method that applies this theory to proposed dual networks is developed, which consist of two standard CNN models predicting both the clean image and the noise. This approach enhances model capacity during training while minimizing computational costs during inference. This method exemplifies the implementation of the variance constraint and introduces a data constraint for dual networks. Notably, the proposed method only assumes the presence of additive white noise, irrespective of the noise distribution. This minimal assumption enhances the model's robustness against noise with complex or unknown distributions in real‐world distorted images. Experimental results indicate that the proposed Noise2Variance method exhibits commendable performance on peak signal noise ratio and structural similarity metrics compared to existing self‐supervised denoising techniques. Visual comparison of results further substantiates the efficacy of the proposed method. A comparison of model complexity reveals that the method is efficient among the compared CNN‐based techniques."
hiding image into image with hybrid attention mechanism based on gans,"Image steganography is the art of concealing secret information within images to prevent detection. In deep‐learning‐based image steganography, a common practice is to fuse the secret image with the cover image to directly generate the stego image. However, not all features are equally critical for data hiding, and some insignificant ones may lead to the appearance of residual artifacts in the stego image. In this article, a novel network architecture for image steganography with hybrid attention mechanism based on generative adversarial network is introduced. This model consists of three subnetworks: a generator for generate stego images, an extractor for extracting the secret images, and a discriminator to simulate the detection process, which aids the generator in producing more realistic stego images. A specific hybrid attention mechanism (HAM) module is designed that effectively fuses information across channel and spatial domains, facilitating adaptive feature refinement within deep image representations. The experimental results suggest that the HAM module not only enhances the image quality during both the steganography and extraction processes but also improves the model's undetectability. Stego images are mixed with varying levels of noise in the training process, which can further improve robustness. Finally, it is verified that the model outperforms current steganography approaches on three datasets and exhibits good undetectability."
ma‐resunet: multi‐attention optic cup and optic disc segmentation based on improved u‐net,"Glaucoma poses a significant threat to vision, capable of causing irreversible damage and, in severe instances, leading to permanent blindness. Accurate optic cup (OC) and optic disc (OD) segmentation are essential in glaucoma screening. In this study, a novel OC and OD segmentation approach is proposed. Based on U‐Net, it is optimized by introducing cardinality dimensions. Moreover, attention gates are implemented to reinforce salient features while suppressing irrelevant information. Additionally, a convolutional block attention module (CBAM) is integrated into the decoder segment. This fusion hones in on effective information in both channel and spatial dimensions. Meanwhile, an image processing procedure is proposed for image normalization and enhancement. All of these increase the accuracy of the model. This model is evaluated on the ORIGA and REFUGE datasets, demonstrating the model's superiority in OC and OD segmentation compared to the state‐of‐the‐art methods. Additionally, after the proposed image processing, cup‐to‐disc ratio (CDR) prediction on a batch of 155 in‐house fundus images yields an absolute CDR error of 0.099, which is reduced by 0.04 compared to the case where only conventional processing was performed."
multi‐target detection and tracking of shallow marine organisms based on improved yolo v5 and deepsort,"In order to solve the related problems of detection and tracking of shallow marine organisms, this paper designs a YOLO v5 multi‐target detection and tracking algorithm with attention mechanism. When working underwater, the authors usually encounter many difficulties. Different luminosity and complex coral background usually affect the detection of marine organisms. At the same time, the unrestricted movement of marine organisms, the ability to hide behind rocks and algae, and their mutual occlusion while swimming pose additional challenges to this task. Considering the characteristics of shallow marine organisms activity environment, the attention mechanism is added to the feature extraction network of YOLO v5 to reduce redundant information and improve the detection accuracy of shallow marine organism targets in complex environment. The improved algorithm improves the average detection accuracy of marine organisms target detection by 3.2%. Aiming at the problem of shallow marine organisms target tracking, a shallow marine organisms multi‐target tracking algorithm based on improved Deep Simple Online And Realtime Tracking (SORT) is designed. The improved YOLO v5 algorithm is used to replace Faster R‐CNN (Faster Region‐Convolutional Neural Networks) as the detector of DeepSORT tracking algorithm, and the cascade matching strategy is adopted to solve the problem that the target cannot be tracked continuously when it is occluded for a long time. The experimental results show that the shallow marine organisms multi‐target tracking algorithm based on improved DeepSORT reduces the number of id transformation of marine biological target tracking in shallow sea environment, and improves the accuracy of shallow marine organisms multi‐target tracking."
lpnet: a remote sensing scene classification method based on large kernel convolution and parameter fusion,"Remote sensing scene images contain numerous feature targets with unrelated semantic information, so how to extract to the local key information and semantic features of the image becomes the key to achieving accurate classification. Existing Convolutional Neural Networks (CNNs) mostly concentrate on the global representation of an image and lose the shallow features. To overcome these issues, this paper proposes LPNet for remote sensing scene image classification. First, LPNet employs LKConv to extract the semantic features in the image, while using standard convolution to extract local key information in the image. Additionally, the LPNet applies a shortcut residual concatenation branch to reuse features. Then, parameter fusion combines parameters from previous branches, improving the capacity of the model to obtain a more comprehensive and rich feature representation of the image. Finally, considering the relationship between the classification ability of the model and the depth of feature extraction, the Feature Mixture (FM) Block is used to deepen the model for feature extraction. Comparative experiments on four publicly available datasets show that LPNet provides comparable results to other state‐of‐the‐art methods. The effectiveness of LPNet is further demonstrated by visualizing the effective receptive fields (ERFs)."
video object segmentation via couple streams and feature memory,"In recent years, most video segmentation methods use deep CNN to process the input image, but they did not fully mine the rich intermediate predictions in spatio‐temporal space. And, the segmentation challenges such as occlusion, severe deformation and illumination have not been well solved so far. To alleviate these problems, this paper focuses on constructing multi module network structures that represent multi semantics and proposes a video object segmentation network via coupled‐stream architecture with feature memory mechanism. This network first extracts high‐level semantic features, edge features, long‐term and short‐term stable depth features of the target, and then decode them into the segmentation mask of target. In addition, negative skeleton inhibition and frame interpolation are used to prevent the interference of similar objects and motion blur, respectively. The method has a low GPU memory usage, regardless of the number of object in video. And performs 86.5%and 62.4% in J&F measure on DAVIS 2016 and DAVIS 2017 validation set, without fine‐tuning and online training."
pre-trained low-light image enhancement transformer,"Low‐light image enhancement is a longstanding challenge in low‐level vision, as images captured in low‐light conditions often suffer from significant aesthetic quality flaws. Recent methods based on deep neural networks have made impressive progress in this area. In contrast to mainstream convolutional neural network (CNN)‐based methods, an effective solution inspired by the transformer, which has shown impressive performance in various tasks, is proposed. This solution is centred around two key components. The first is an image synthesis pipeline, and the second is a powerful transformer‐based pre‐trained model, known as the low‐light image enhancement transformer (LIET). The image synthesis pipeline includes illumination simulation and realistic noise simulation, enabling the generation of more life‐like low‐light images to overcome the issue of data scarcity. LIET combines streamlined CNN‐based encoder‐decoders with a transformer body, efficiently extracting global and local contextual features at a relatively low computational cost. The extensive experiments show that this approach is highly competitive with current state‐of‐the‐art methods. The codes have been released and are available at LIET."
optimized dehazing algorithm based on dark channel prior with gabor filter and multiscale minimum filter,"Traditional dehazing algorithms have long been limited in their ability to remove fog effectively, especially in preserving details. This study proposes an improved dark channel prior dehazing algorithm designed to overcome the limitations of traditional algorithms by refining the transmission map and estimating atmospheric light intensity. The algorithm fully exploits the anti‐haze characteristics of the Gabor filter to extract multi‐directional texture features. By fusing these features to form a guidance map for guided filtering, it effectively reduces the blurring caused by guided filtering on the image edges, thereby producing a more accurate transmission map. Simultaneously, an enhanced atmospheric light successfully reduces interference from white objects in the image. The experiment phase utilized the publicly available RESIDE dataset for validation. The algorithm achieved a Peak Signal‐to‐Noise Ratio (PSNR) of 22.4 and a Structural Similarity Index (SSIM) of 0.88. These metrics indicate the algorithm's superior dehazing capabilities."
local feature‐based video captioning with multiple classifier and caru‐attention,"Video captioning aims to identify multiple objects and their behaviours in a video event and generate captions for the current scene. This task aims to generate a detailed description of the current video in real‐time using natural language, which requires deep learning to analyze and determine the relationships between interesting objects in the frame sequence. In practice, existing methods typically involve detecting objects in the frame sequence and then generating captions based on features extracted through object coverage locations. Therefore, the results of caption generation are highly dependent on the performance of object detection and identification. This work proposes an advanced video captioning approach that works in adaptively and effectively addresses the interdependence between event proposals and captions. Additionally, an attention‐based multimodel framework is introduced to capture the main context from the frame and sound in the video scene. Also, an intermediate model is presented to collect the hidden states captured from the input sequence, which performs to extract the main features and implicitly produce multiple event proposals. For caption prediction, the proposed method employs the CARU layer with attention consideration as the primary RNN layer for decoding. Experimental results showed that the proposed work achieves improvements compared to the baseline method and also better performance compared to other state‐of‐the‐art models on the ActivityNet dataset, presenting competitive results in the tasks of video captioning."
few-shot object detection based on global context and implicit knowledge decoupled head,"The acquisition cycle of remote sensing images is slow, and the labelling process encounters challenges, which have become prominent with the rapid development of remote sensing image object detection research. Therefore, this article provides a way to make the model better capture the diversity and contextual relationships in the data, and solve this problem by more than just data augmentation. Specifically, this method is a few‐shot object detection method for remote sensing based on global context combined with implicit knowledge decoupled head (GC‐IKDH). This method first uses a segmentation strategy to convert high‐resolution images into low‐resolution images and expands the sample size through a generative model. Secondly, GC attention is introduced to generate a GC vector by weighting and averaging the information of each position in the input sequence, which helps the model better understand the semantics of the input sequence. Finally, an IKDH is added to improve the model head, which is used to learn specific features in the data so that the model can better handle the diversity in the data. Experimental results show that GC attention and IKDH boosting provide a good performance boost to the baseline model. Compared with other few‐shot samples, this method achieves state‐of‐the‐art performance under different shot settings and highly competitive results on two benchmark datasets (NWPU VHR‐10 and DIOR)."
feature comparison residuals for foreign fibre classification model,"Various types of foreign fibres may be mixed in the planting, transportation, and production processes of cotton, which not only cause equipment to be out of control, but also leads to a decrease in the quality of cotton textile products and economic losses. The machine vision based detection method for cotton foreign fibres is widely used. Based on existing related research, we construct a classification dataset for cotton foreign fibres in practical scenarios, named the CF2113‐10 dataset. The authors design a basic foreign fibre classification network called CottonNet that balances performance and efficiency. The classification accuracy on the validation set reached 94.2%. In order to enhance the high‐level feature extraction ability, this paper improves the feature fusion method of residual networks and proposes CottonNet‐Res, which improves the classification accuracy to 95.1%. Finally, a classification model based on feature difference fitting, CottonNet‐Fusion, is proposed to address the classification problem of foreign fibre images sampled in complex environments. The classification accuracy of foreign fibre images sampled in ordinary scenes has improved to 97.4%, while the images sampled in complex environments maintain an accuracy of 90.3%."
video stabilization based on low-rank constraint and trajectory optimization,"Video stabilization plays a pivotal role in enhancing video quality by eliminating unwanted jitter in shaky videos. This paper introduces a novel video stabilization algorithm that leverages low‐rank constraint and trajectory optimization to effectively eliminate undesirable motion and generate stabilized videos. In the proposed algorithm, a low‐rank constraint regularization term is incorporated to enhance the smoothness of motion trajectories. Additionally, a predictive path smoothness term is integrated to ensure the consistency of motion across neighbouring frames. To address the problem of excessive cropping resulting from aggressive smoothing, a flexible local window strategy that emphasizes local motion relationships within the trajectories is introduced. The experimental results show that, compared to other excellent video stabilization algorithms, the proposed algorithm improves the stability metric by approximately 2.3%. Furthermore, in the stabilized videos generated by the algorithm, an approximate improvement of 2.18 dB in average image temporal fidelity and a 5.7% increase in average structural similarity between adjacent frames are achieved. The code that implements the proposed method is publicly accessible at https://github.com/CZS0319/VS_Low‐Rank_Constraint_Trajectory_Optimization."
"sehrnet: a lightweight, high‐resolution network for aircraft keypoint detection","Current research on apron conflict detection is often limited to the interaction between the aircraft as a whole and other objects, making it difficult to accomplish targeted identification of vulnerable and high‐cost aircraft components. However, the implementation of detailed aircraft identification is of great significance to enhance the safety of airport surface operations. Based on the excellent performance of High‐Resolution Network (HRNet) in keypoint detection, a lightweight end‐to‐end keypoint detection network, namely Squeeze and Excitation High‐Resolution Network (SEHRNet), is proposed in this paper to solve the problems of HRNet's slower computation and more redundancy. First, the errors arising from coordinate transformations in the coding and decoding process are solved by an improved feature map coding and decoding process. Second, replace the BasicBlock in HRNet with the Depthwise separable convolutions based on the Squeeze‐and‐Excitation Networks, which drastically cuts the computational cost of the network. Third, the improved Bottleneck module is used to further enhance the capability of feature extraction. Experimental results prove that, based on the aircraft keypoint detection dataset, the SEHRNet proposed in this paper shows stronger applicability compared to the current mainstream networks. Compared with the original HRNet, the improved network has higher accuracy, faster speed, and lighter model for aircraft keypoint detection."
chinese image captioning with fusion encoder and visual keyword search,"Automatic generation of image captions is essentially a cross‐modal conversion from image to text. Owing to the differences in linguistic characteristics between Chinese and English, quite a few Chinese image captioning methods have recently been proposed. Nevertheless, the existing Chinese image captioning models usually lack attention to local details of images or tend to produce general descriptions. To address these challenges, a Chinese image captioning method is proposed that incorporates fusion encoder, visual keyword search, and reinforcement learning. The fusion encoder can simultaneously extract local and global features of the input image to enrich the semantic information in the decoding stage, visual keyword search can pursue potential visual words associated with the image content, and the reinforcement learning mechanism can optimize the evaluation metric CIDEr at sentence level to promote the lexical diversity of image description. The results of extensive experiments demonstrate that the proposed model outperforms the state‐of‐the‐art models and delivers expressive and informative Chinese image captions."
"inception-yolo: computational cost and accuracy improvement of the yolov5 model based on employing modified csp, sppf, and inception modules","The demand for less complex and more accurate architectures has always been a priority since the broad usage of computer vision in everyday life, like auto‐drive cars, portable applications, augmented reality systems, medical image analysis etc. There are a lot of methods that have been developed to improve the accuracy and complexity of object detection, like the generations of R‐CNNs and YOLOs. However, these methods are not the most efficient architectures, and there is always room to improve. In this study, the 5th version of YOLO is employed and the improved architecture, Inception‐YOLO, is presented. The model significantly outperforms the SOTA YOLO family. Specifically, the improvements can be summarised as follows: impressive improvement of floating point operations (FLOPs) and number of parameters, as well as improvement in accuracy compared to the models with fewer FLOPs. All our presented approaches, like the optimized inception module, proposed structures for CSP and SPPF, and the improved loss function used in this research, work together to incrementally improve detection results, accuracy, demanded memory, and FLOPs simultaneously. For a glimpse of performance, the Inception‐YOLO‐S model hits 38.7% AP with 5.9M parameters and 11.5 BFLOPs and outperforms YOLOv5‐S with 37.4% AP, 7.2M parameters, and 16.5 BFLOPs."
adir: advanced domain‐invariant representation via decoupling learning and information bottleneck,"The discrepancy in data distribution between training and testing scenarios, as well as the inductive bias of convolutional neural networks towards image styles, reduces the model's generalization ability. Many unsupervised domain generalization methods based on feature decoupling suffer from an initial neglect of explicit decoupling of content and style features, resulting in content features that still contain considerable redundant information, thereby restricting improvements in generalization capability. To tackle this problem, this paper optimizes the learning process of domain‐invariant (content) features into an information compression issue, minimizing redundancy in content features. Furthermore, to enhance decoupled learning, this paper introduces innovative cross‐domain loss functions and image reconstruction modules that explicitly decouple and merge content and style across different domains. Extensive experiments demonstrate the method's significant enhancements over recent cutting‐edge approaches."
a novel nam-based image segmentation using hierarchical density-based spatial clustering,"This paper proposes a new method for hierarchical image segmentation based on the nonsymetry and anti‐packing pattern representation model (NAM) and the hierarchical density‐based spatial clustering of application with noise (HDBSCAN). The proposed framework consists of two phases. In the first phase, a super‐pixel generation algorithm base on NAM is proposed. In the second phase, instead of defining an affinity matrix to merge similar regions using spatial clustering, the distance matrix defined by different region features is directly fitted into an HDBSCAN clustering module in order to merge similar regions efficiently. Similar adjacent regions can be merged into larger ones progressively and form a segmentation dendrogram for image segmentation with the clustering module. The experiments show that the proposed algorithm has a comparable or even better performance compared to the state‐of‐the‐art hierarchical image segmentation algorithms while having much less time and memory consumption."
salient object detection in egocentric videos,"In the realm of video salient object detection (VSOD), the majority of research has traditionally been centered on third‐person perspective videos. However, this focus overlooks the unique requirements of certain first‐person tasks, such as autonomous driving or robot vision. To bridge this gap, a novel dataset and a camera‐based VSOD model, CaMSD, specifically designed for egocentric videos, is introduced. First, the SalEgo dataset, comprising 17,400 fully annotated frames for video salient object detection, is presented. Second, a computational model that incorporates a camera movement module is proposed, designed to emulate the patterns observed when humans view videos. Additionally, to achieve precise segmentation of a single salient object during switches between salient objects, as opposed to simultaneously segmenting two objects, a saliency enhancement module based on the Squeeze and Excitation Block is incorporated. Experimental results show that the approach outperforms other state‐of‐the‐art methods in egocentric video salient object detection tasks. Dataset and codes can be found at https://github.com/hzhang1999/SalEgo."
rflse: joint radiomics feature‐enhanced level‐set segmentation for low‐contrast spect/ct tumour images,"Doctors typically use non‐contrast‐enhanced computed tomography (NCECT) in the treatment of kidney cancer to map kidney and tumour structural information to functional imaging single‐photon emission computed tomography, which is then used to assess patient kidney function and predict postoperative recovery. However, the assessment of kidney function and formulation of surgical plans is constrained by the low contrast of tumours in NCECT, which hinders the acquisition of accurate tumour boundaries. Therefore, this study designed a radiomics feature‐enhanced level‐set evolution (RFLSE) to precisely segment small‐sample low‐contrast kidney tumours. Integration of high‐dimensional radiomics features into the level‐set energy function enhances the edge detection capability of low‐contrast kidney tumours. The use of sensitive radiomics features to control the regional term parameters achieves adaptive adjustment of the curve evolution amplitude, improving the level‐set segmentation process. The experimental data used low‐contrast, limited‐sample tumours provided by hospitals, as well as the public datasets BUSI18 and KiTS19. Comparative results with advanced energy functionals and deep learning models demonstrate the precision and robustness of RFLSE segmentation. Additionally, the application value of RFLSE in assisting doctors with accurately marking tumours and generating high‐quality pseudo‐labels for deep learning datasets is demonstrated."
a transformer‐based lightweight method for multiple‐object tracking,"At present, the multi‐object tracking method based on transformer generally uses its powerful self‐attention mechanism and global modelling ability to improve the accuracy of object tracking. However, most existing methods excessively rely on hardware devices, leading to an inconsistency between accuracy and speed in practical applications. Therefore, a lightweight transformer joint position awareness algorithm is proposed to solve the above problems. Firstly, a joint attention module to enhance the ShuffleNet V2 network is proposed. This module comprises the spatio‐temporal pyramid module and the convolutional block attention module. The spatio‐temporal pyramid module fuses multi‐scale features to capture information on different spatial and temporal scales. The convolutional block attention module aggregates channel and spatial dimension information to enhance the representation ability of the model. Then, a position encoding generator module and a dynamic template update strategy are proposed to solve the occlusion. Group convolution is adopted in the input sequence through position encoding generator module, with each convolution group responsible for handling the relative positional relationships of a specific range. In order to improve the reliability of the template, dynamic template update strategy is used to update the template at the appropriate time. The effectiveness of the approach is validated on the MOT16, MOT17, and MOT20 datasets."
flexible thin parts multi‐target positioning method of multi‐level feature fusion,"In new energy battery manufacturing, machine vision is widely used in automated assembly scenarios for key parts. To improve the accuracy and real‐time multi‐target positioning recognition of flexible thin parts, this paper proposes a multi‐level feature fusion template matching algorithm based on the Gaussian pyramid. Firstly, the algorithm constructs a Gaussian pyramid by multi‐scale image construction. Secondly, considering the image features of each layer of the pyramid, this paper uses the grey‐based Fast Normalized Matching algorithm to obtain coarse positioning coordinates on the upper layer, and the improved Linemod‐2D algorithm is applied to the bottom layer image to get accurate positioning coordinates. Finally, the positioning coordinates returned from each layer are fused to obtain the final positioning coordinate. The experimental results show that the proposed algorithm achieves excellent performance in nickel plate positioning and recognition. It exhibits satisfactory performance in nickel sheet localization and recognition. In terms of angular error, repeat accuracy, and matching speed, it competes favourably with Halcon, VisionMaster, and SCISmart. Its positioning error closely approximates that of Halcon, effectively meeting the practical production demands for high‐speed feeding and high‐precision positioning."
dmcvs: decomposed motion compensation-based video stabilization,"With the popularity of handheld devices, video stabilization is becoming increasingly important. In previous studies, many methods have been proposed to stabilize shaky videos. However, these methods fail to balance between image content integrity and stability. Some methods sacrifice image content for better stability. Other methods ignore the subtle jitters, which leads to poor stability. This work innovatively proposes a video stabilization method based on decomposed motion compensation. First, a grid‐based motion statistics method is adopted for motion estimation, which obtains more accurate motion vectors according to matched likelihood estimates. Then, the motion compensation is inherently decomposed into two parts: linear motion compensation and auxiliary motion compensation. Linear motion compensation removes complex jitter by constructing linear path constraints to obtain a more stable camera path. Auxiliary motion compensation uses a moving average filter to remove the high‐frequency jitter as a supplement and preserve more image content. The two components are combined with individual weights to derive the final transform matrix and warp the original frames. Experimental results show that our method outperforms the previous methods on NUS and DeepStab datasets qualitatively and quantitatively."
feature-enhanced representation with transformers for multi-view stereo,"Most existing multi‐view stereo (MVS) methods fail to consider global context information in the stage of feature extraction and cost aggregation. As transformers have shown remarkable performance on various vision tasks due to their ability to perceive global contextual information, this paper proposes a transformer‐based feature enhancement network (TF‐MVSNet) to facilitate feature representation learning by combining local features (both 2D and 3D) with long‐range contextual information. To reduce memory consumption of feature matching, the cross‐attention mechanism is leveraged to efficiently construct 3D cost volumes under the epipolar constraint. Additionally, a colour‐guided network is designed to refine depth maps at a coarse stage, hence reducing incorrect depth predictions at a fine stage. Extensive experiments were performed on the DTU dataset and Tanks and Temples (T&T) benchmark and results are reported."
you only label once: a self-adaptive clustering-based method for source-free active domain adaptation,"With the growing significance of data privacy protection, Source‐Free Domain Adaptation (SFDA) has gained attention as a research topic that aims to transfer knowledge from a labeled source domain to an unlabeled target domain without accessing source data. However, the absence of source data often leads to model collapse or restricts the performance improvements of SFDA methods, as there is insufficient true‐labeled knowledge for each category. To tackle this, Source‐Free Active Domain Adaptation (SFADA) has emerged as a new task that aims to improve SFDA by selecting a small set of informative target samples labeled by experts. Nevertheless, existing SFADA methods impose a significant burden on human labelers, requiring them to continuously label a substantial number of samples throughout the training period. In this paper, a novel approach is proposed to alleviate the labeling burden in SFADA by only necessitating the labeling of an extremely small number of samples on a one‐time basis. Moreover, considering the inherent sparsity of these selected samples in the target domain, a Self‐adaptive Clustering‐based Active Learning (SCAL) method is proposed that propagates the labels of selected samples to other datapoints within the same cluster. To further enhance the accuracy of SCAL, a self‐adaptive scale search method is devised that automatically determines the optimal clustering scale, using the entropy of the entire target dataset as a guiding criterion. The experimental evaluation presents compelling evidence of our method's supremacy. Specifically, it outstrips previous SFDA methods, delivering state‐of‐the‐art (SOTA) results on standard benchmarks. Remarkably, it accomplishes this with less than 0.5% annotation cost, in stark contrast to the approximate 5% required by earlier techniques. The approach thus not only sets new performance benchmarks but also offers a markedly more practical and cost‐effective solution for SFADA, making it an attractive choice for real‐world applications where labeling resources are limited."
multi‐scale feature aggregation network for single‐image dehazing,"Transformer possesses a broader perceptual scope, while the Convolutional Neural Network (CNN) excels at capturing local information. In this paper, the authors propose the Multi‐Sclale Feature Aggregation Network (MSFA‐Net) for single‐image dehazing which is fused with the advantages of Transformer and CNN. Our MSFA‐Net is based on the encoder–decoder structure, and there are four main innovations. Firstly, the authors make some improvements to the original Swin Transformer to make it more effective for dehazing tasks, and the authors name it Spatial Information Aggregation Transformer (SIAT). The authors place the SIAT in both encoder and decoder of MSFA‐Net for feature extraction. The authors propose an upsampling module called Efficient Spatial Resolution Recovery (ESRR) which is placed in the decoder part. Compared to commonly used transposed convolutions, the authors’ ESRR module has fewer computational cost. Considering that the haze distribution is always uneven and the information from each channel is different, the authors introduce the Dynamic Multi‐Attention (DMA) module to provide pixel‐wise weights and channel‐wise weights for input features. The authors place the DMA module between the encoder and decoder parts. As the network depth increases, the spatial structural information from the high‐resolution layer tends to degrade. To deal with the problem, the authors propose the Multi‐Scale Feature Fusion (MSFF) module to recover missing spatial structural information. The authors place the MSFF module in both the encoder and decoder parts. Extensive experimental results show that the authors’ proposed dehazing network achieves state‐of‐the‐art dehazing performance with relatively low computational cost."
progressive prediction: video anomaly detection via multi‐grained prediction,"Video Anomaly Detection (VAD) has been an active research field for several decades. However, most existing approaches merely extract a single type of feature from videos and define a single paradigm to indicate the extent of abnormalities. A coarse‐to‐fine three‐level prediction is built by integrating different levels of spatio‐temporal representations, better highlighting the difference between normal and abnormal behaviors. First, an object‐level trajectory prediction is proposed to model human historical position using a graph transformer network. Subsequently, skeleton‐level prediction is achieved by incorporating the positional information from the trajectory prediction. More importantly, based on the predicted skeleton, a skeleton‐guided pixel‐level region prediction is performed. A novel Skeleton Conditioned Generative Adversarial Network (SCGAN) is designed to explore the correlation between skeleton‐level and pixel‐level motion prediction. Benefiting from SCGAN, the prediction of human regions is contributed by both coarse‐grained and fine‐grained motion features. This three‐level prediction, namely Progressive Prediction Video Anomaly Detection (P3VAD), enlarges the prediction error on irregular motion patterns. Besides, a pixel‐level analysis method is proposed to achieve Background‐bias Elimination (BE) and denoise the predicted region. Experimental results validate the effectiveness of P3VAD on the four benchmark datasets (ShanghaiTech, CUHK Avenue, IITB‐Corridor, and ADOC)."
tsd‐yolo: small traffic sign detection based on improved yolo v8,"Traffic sign detection is critical for autonomous driving technology. However, accurately detecting traffic signs in complex traffic environments remains challenge despite the widespread use of one‐stage detection algorithms known for their real‐time processing capabilities. In this paper, the authors propose a traffic sign detection method based on YOLO v8. Specifically, this study introduces the Space‐to‐Depth (SPD) module to address missed detections caused by multi‐scale variations of traffic signs in traffic scenes. The SPD module compresses spatial information into depth channels, expanding the receptive field and enhancing the detection capabilities for objects of varying sizes. Furthermore, to address missed detections caused by complex backgrounds such as trees, this paper employs the Select Kernel attention mechanism. This mechanism enables the model to dynamically adjust its focus and more effectively concentrate on key features. Additionally, considering the uneven distribution of training data, the authors adopted the WIoUv3 loss function, which optimizes loss calculation through a weighted approach, thereby improving the model's detection performance across various sizes and frequencies of instances. The proposed methods were validated on the CCTSDB and TT100K datasets. Experimental results demonstrate that the authors’ method achieves substantial improvements of 3.2% and 5.1% on the mAP50 metric compared to YOLOv8s, while maintaining high detection speed, significantly enhancing the overall performance of the detection system. The code for this paper is located at https://github.com/dusongjie/TSD‐YOLO‐Small‐Traffic‐Sign‐Detection‐Based‐on‐Improved‐YOLO‐v8"
cantonese sentence dataset for lip‐reading,"Lip‐reading deciphers speech by observing lip movements without relying on audio data. The rapid advancements in deep learning have significantly improved lip‐reading for both English and Chinese; however, research on dialects such as Cantonese remains scarce. Consequently, most Chinese lip‐reading datasets focus on Mandarin, with only a few addressing Cantonese. To bridge this gap, a sentence‐level Cantonese lip‐reading dataset, designated as Cantonese lip‐reading sentences are introduced, comprising over 500 unique speakers and more than 30,000 samples. To ensure alignment with real‐world scenarios, no restrictions are imposed on factors such as gender, age, posture, lighting conditions, or speech rate. A comprehensive description of the pipeline employed is provided for collecting and constructing the dataset and introduce an innovative visual frontend, 3D‐visual attention net. This frontend combines the advantages of convolution and self‐attention mechanisms to extract fine‐grained lip region features. These features are subsequently input into the conformer backend for temporal sequence modelling, achieving comparable performance on Chinese Mandarin lip reading dataset, lip reading sentences 2, lip reading sentences 3, and Cantonese lip‐reading sentences datasets. Benchmark tests on Cantonese lip‐reading sentences demonstrate the challenges it poses, providing a novel research foundation for dialect lip‐reading and fostering the advancement of Cantonese lip‐reading tasks."
image super‐resolution reconstruction based on implicit image functions,"Image super‐resolution (SR) reconstruction is a key technique for improving image quality and details. Conventional methods are frequently limited by interpolation, filtering, or statistical approaches; thus, they are unable to reconstruct high‐quality continuously enlarged images with detailed information. This study proposes an image SR reconstruction network model, called LALNet, based on implicit image functions and residual multilayered perceptron (RAMLP) with an attention mechanism. Through the implicit image function and RAMLP + attention, high‐quality SR reconstruction with continuous scale factors is achieved, and LALNets can run on embedded edge computing platforms. This method exhibits the following advantages: lightweight network structure reduces computing requirements, introduction of implicit image functions and RAMLP improves reconstruction quality, and attention mechanism suppresses artefacts and distortions. Experimental results show that LALNet outperforms traditional and other deep learning methods in terms of reconstruction performance and computational efficiency. This research provides new ideas and methods for the further development of the field of image SR reconstruction."
ai‐generated video steganography based on semantic segmentation,"Traditional video steganography methods primarily rely on modifying concealed spaces for embedding, thereby exhibiting a certain degree of security and embedding capacity. Nevertheless, these methods do not fully capitalize on the rich semantic information inherent in videos, limiting their overall effectiveness. In this paper, an AI‐generated video steganography scheme based on semantic segmentation is proposed. The mapping relationship between secret and semantic information is established by using a semantic segmentation model. The secret information can be converted into semantic labels by semantic histograms or pixels means, and semantic labels containing secret information are obtained and input into the video‐to‐video model to drive the generation of stego videos. After receiving the stego video, the receiver extracts the secret information using a pre‐defined specific embedding mode, including the methods of sub‐block partitioning and embedding capacity per frame. The experimental results show that the stego video has good visual quality, security, and robustness against various noise attacks."
3sg: three-stage guidance for indoor human action recognition,"Inference using skeleton to steer RGB videos is applicable to fine‐grained activities in indoor human action recognition (IHAR). However, existing methods that explore only spatial alignment are prone to bias, resulting in limited performance. The authors propose a Three‐stage Guidance (3sG) framework, leveraging skeleton knowledge to promote RGB in three stages. First, a soft shading image is proposed for alleviating background noise in videos, allowing the network to directly focus more on the motion region. Second, the authors propose to extract RGB frames of interest to reduce the computational effort. Furthermore, to explore more fully the complementary information between skeletons and RGB, the skeleton is coupled to the frame representation in a different spatial–temporal sharing pattern. Third, the global skeleton and skeleton‐guided RGB features are fed into the shared classifiers, which approximate the logit distributions of the two to enhance the performance in RGB unimodal. Finally, a fusion strategy that utilizes two learnable parameters to adaptively integrate the skeleton with the RGB is proposed. 3sG outperforms the state‐of‐the‐art results on the Toyota Smarthome dataset while it is more efficient than similar methods on the NTU RGB+D dataset."
head pose estimation with particle swarm optimization‐based contrastive learning and multimodal entangled gcn,"Head pose estimation is an especially challenging task due to the complexity nonlinear mapping from 2D feature space to 3D pose space. To address the above issue, this paper presents a novel and efficient head pose estimation framework based on particle swarm optimized contrastive learning and multimodal entangled graph convolution network. Firstly, a new network, the region and difference‐aware feature pyramid network (RD‐FPN), is proposed for 2D keypoints detection to alleviate the background interference and enhance the feature expressiveness. Then, particle swarm optimized contrastive learning is constructed to alternatively match 2D and 3D keypoints, which takes the multimodal keypoints matching accuracy as the optimization objective, while considering the similarity of cross‐modal positive and negative sample pairs from contrastive learning as a local contrastive constraint. Finally, multimodal entangled graph convolution network is designed to enhance the ability of establishing geometric relationships between keypoints and head pose angles based on second‐order bilinear attention, in which point‐edge attention is introduced to improve the representation of geometric features between multimodal keypoints. Compared with other methods, the average error of our method is reduced by 8.23%, indicating the accuracy, generalization, and efficiency of our method on the 300W‐LP, AFLW2000, BIWI datasets."
a pyramid gaussian pooling based cnn and transformer hybrid network for smoke segmentation,"Visual smoke semantic segmentation is a challenging task due to semi‐transparency, variable shapes, and complex textures of smoke. To improve segmentation performance, a convolutional neural network and transformer hybrid network are proposed based on pyramid Gaussian pooling (PGP) for smoke segmentation. In order to utilize low‐pass filtering to suppress noise, a PGP method is designed. Then, the output of PGP is reshaped to construct a set of visual tokens for transformers, thus a PGP‐transformer module is presented to make full use of the self‐attention mechanism. Finally, the PGP‐transformer module is inserted into the U‐shaped architecture with skip connections. A large number of experiments have proved that the method is significantly superior to existing state‐of‐the‐art algorithms on virtual and real smoke datasets, and ablation experiments have also verified the effectiveness of the proposed modules."
real-world image deblurring using data synthesis and feature complementary network,"Many learning‐based approaches to image deblurring have received increasing attention in recent years. However, the models trained on existing synthetic datasets do not generalize well to real‐world blur, resulting in undesirable artifacts and residual blur. This work attempts to address this problem from two aspects: training data synthesis and network architecture. To narrow the domain gap between synthetic and real domains, a realistic blur synthesis pipeline to generate high‐quality blurred data is proposed. Since the blur is non‐uniform and has different scales and degrees, a parallel feature complementary module to fully exploit the local and non‐local information, which improves the feature representation and helps the network to perceive the non‐uniform blur, is developed. In addition, a spatial Fourier reconstruction block to facilitate correct detail recovery in the spatial and Fourier domains is introduced. Based on these two designs, an effective encoder–decoder network for deblurring is designed. Extensive experiments demonstrate the validity and superiority of the proposed blur synthesis method and deblurring network. In particular, the proposed deblurring network can achieve superior or comparable performance to Restormer, while saving 70% of network parameters and 53% of floating point operations (FLOPs)."
cross-modal knowledge learning with scene text for fine-grained image classification,"Scene text in natural images carries additional semantic information to aid in image classification. Existing methods lack full consideration of the deep understanding of the text and the visual text relationship, which results in the difficult to judge the semantic accuracy and the relevance of the visual text. This paper proposes image classification based on Cross modal Knowledge Learning of Scene Text (CKLST) method. CKLST consists of three stages: cross‐modal scene text recognition, text semantic enhancement, and visual‐text feature alignment. In the first stage, multi‐attention is used to extract features layer by layer, and a self‐mask‐based iterative correction strategy is utilized to improve the scene text recognition accuracy. In the second stage, knowledge features are extracted using external knowledge and are fused with text features to enhance text semantic information. In the third stage, CKLST realizes visual‐text feature alignment across attention mechanisms with a similarity matrix, thus the correlation between images and text can be captured to improve the accuracy of the image classification tasks. On Con‐Text dataset, Crowd Activity dataset, Drink Bottle dataset, and Synth Text dataset, CKLST can perform significantly better than other baselines on fine‐grained image classification, with improvements of 3.54%, 5.37%, 3.28%, and 2.81% over the best baseline in mAP, respectively."
gdb‐yolov5s: improved yolo‐based model for ship detection in sar images,"In recent years, deep learning methods were good solutions for object detection in synthetic aperture radar (SAR) images. However, the problems of complex scenarios, large object scale differences and imperfect fine‐grained classification in ship detection were intractable. In response, an improved model GDB‐YOLOv5s (Improved YOLOv5s model incorporating global attention mechanism (GAM), DCN‐v2 and BiFusion) is designed. This model introduces deformable convolution networks (DCN‐v2) into the Backbone to enhance the adaptability of the receptive field. It replaces the original Neck's PANet structure with a BiFusion structure to better fuse the extracted multiscale features. Additionally, it integrates GAM into the network to reduce information loss and improve global feature interaction. Experiments were conducted on single‐class dataset SSDD and multi‐class dataset SRSSD‐V1.0. The results show that the GDB‐YOLOv5s model improves mean average precision scores (mAP) significantly, outperforming the original YOLOv5s model and other traditional methods. GDB‐YOLOv5s also improves Precision‐score (P) and Recall‐score (R) for fine‐grained classification to some extent, thereby reducing false alarms and missed detections. It has been proved that the improved model is relatively effective."
b‐splines image approximation using resampled chordal parameterization,"Image processing often requires filtering, which can be effectively performed by using B‐spline surface approximation. This article presents a fast method of approximating data in rectangular (image) form using such surfaces. The method considers dynamic changes of data by representing rows and columns in chordal parameterized form. To improve performance, after parametrization, lines are uniformly resampled using linear interpolation. It allows using the same basis functions with uniform parameterization for approximation of all processed lines. After approximation, reconstruction of original parameterization is required, but its computational complexity is also linear."
point cloud registration method for indoor depth sensor acquisition system based on dual graph computation with irregular shape factors,"The registration performance determines the widespread indoor application of 3D models acquired by depth sensors. Many advanced registration methods lack comprehensive feature aggregation and poor generalization capabilities, which improves the mismatching ratio. Here, a dual graph network is proposed by incorporating irregular shape factors to make point cloud features more expressive. At first, we transform point cloud sets into the stellar graph within the local neighbourhood of each point. The deep feature and shape factor of each point are combined in the directional‐connected irregular projection space. Subsequently, the combined features are modelled as the second graph. By the attention mechanism computation, feature information is continuously aggregated with intra‐graph and inter‐graph. Finally, a loss function is utilized to confirm point correspondence and perform the registration through singular value decomposition. Extensive experiments validate that the proposed point cloud registration method achieves state‐of‐the‐art performance."
macn: a cascade defect detection for complex background based on mixture attention mechanism,"Defect detection in complex background is a critical issue. To address this issue, this paper proposes the mixture attention mechanism cascade network, in which the new channel attention network is linked with the spatial attention network to create an effective mixed attention network that takes advantage of their respective advantages, adaptively suppresses background noise, and highlights defect features. To ensure the efficiency and effectiveness of effective mixed attention network, the new channel attention network splices the output features of the global average pooling layer and the global maximum pooling layer and then sends the spliced features into a shared network, which is a one‐dimensional convolutional network, and uses cross‐channel interaction for fusion. Furthermore, in order to provide more discriminative feature representation, the authors extract the intermediate features of the region proposal network and input them into effective mixed attention network. Finally, the cascade head is used to refine the predicted bounding box to achieve high‐quality defect location. To demonstrate the superiority and usefulness of this method, it is compared to the latest method using widely used PCB and NEU data sets. A large number of trials demonstrate that this strategy outperforms other methods for detecting defects in complicated backgrounds."
sihnet: a safe image hiding method with less information leaking,"Image hiding is a task that hides secret images into cover images. The purposes of image hiding are to ensure the secret images are invisible to the human and the secret images can be recovered. The current state‐of‐the‐art steganography methods run the risk of secret information leakage. A safe image hiding network (SIHNet) is presented to reduce the leakage of secret information. Based on some phenomena of image hiding methods which use invertible neural network, a reversible secret image processing (SIP) module is proposed to make the secret images suitable for hiding and make the stego images leak less secret information. Besides, a reversible lost information hiding (LIH) module is used to hide the lost information into the cover images, thus the method can recover the secret images better than the method that uses random noise to replace the lost information. Experimental results show that SIHNet outperforms other state‐of‐the‐art methods on the PSNR and SSIM values of the recovered secret images and the stego images. Besides, residual images of other state‐of‐the‐art methods all contain information about secret images while residual images of SIHNet leak almost no secret information. Thus the method can prevent the listener of transmission channel from obtaining the information of the secret image through the residual image, which means SIHNet performs better in security than other state‐of‐the‐art methods."
global to multi-scale local architecture with hardwired cnn for 1-ms tomato defect detection,"A 1 millisecond (1‐ms) vision system that guarantees high efficiency and timely response for tomato defect detection is essential for factory automation. Because of various defect appearances, recently many existing researches focus on CNN based defect detection, but few of them attempt to reach high processing speed to adapt to the factorial assembly line. This paper proposes a global to multi‐scale local based parallel architecture with hardwired CNN for tomato defect detection. This architecture breaks down image‐wise detection into pixel‐wise localization and block‐wise classification. The pixel‐wise localization utilizes tomato‐aware information as constraints for localization performance. The block‐wise classification uses a fully pipelined network structure to obtain the classification result for each block as the pixel stream moves through the network. The classification network has a six‐layer lightweight network structure with quantization for hardwired type implementation on FPGA. The experiment results show that the proposed architecture processes 1000 FPS images with 0.9476 ms/frame delay. And for detection performance, this architecture keeps at 80.18%, only 1.31% lower than ResNet50 based detection system."
design and implementation in an altera's cyclone iv ep4ce6e22c8 fpga board of a fast and robust cipher using combined 1d maps,"This paper proposes an image encryption algorithm based on combined 1D chaotic maps. First, a permutation technique was applied. It was then reorganized into 1D matrices along the rows and columns respectively, which were then shuffled by computing the substituted position indices to obtain the scrambled image. Subsequently, a method of confusion of the scrambled image was used through another generated data map, combined with random sub‐matrices for diffusion, then resulting in an encrypted image. Finally, the proposed cryptosystem was implemented in a single kernel platform developed using the Nios II Software Build Tools processor for Eclipse. A hardware architecture was designed using the Qsys‐built tool which is available in the Quartus II 13.0sp1 environment. The developed single‐core system was implemented using the Cyclone IV EP4CE6E22C8. Robustness evaluation of the cryptosystem was performed through security analysis tests such as histogram analysis, correlation coefficient, differential analysis, and key space analysis to prove that it is of good quality, efficient, fast, and successfully resisting brute force attacks. The hardware performance analysis was also carried out. Then the cryptosystem is compared with those in the literature both in the hardware and security performance aspects."
retinex decomposition based low‐light image enhancement by integrating swin transformer and u‐net‐like architecture,"Low‐light images are captured in environments with minimal lighting, such as nighttime or underwater conditions. These images often suffer from issues like low brightness, poor contrast, lack of detail, and overall darkness, significantly impairing human visual perception and subsequent high‐level visual tasks. Enhancing low‐light images holds great practical significance. Among the various existing methods for Low‐Light Image Enhancement (LLIE), those based on the Retinex theory have gained significant attention. However, despite considerable efforts in prior research, the challenge of Retinex decomposition remains unresolved. In this study, an LLIE network based on the Retinex theory is proposed, which addresses these challenges by integrating attention mechanisms and a U‐Net‐like architecture. The proposed model comprises three modules: the Decomposition module (DECM), the Reflectance Recovery module (REFM), and the Illumination Enhancement module (ILEM). Its objective is to decompose low‐light images based on the Retinex theory and enhance the decomposed reflectance and illumination maps using attention mechanisms and a U‐Net‐like architecture. We conducted extensive experiments on several widely used public datasets. The qualitative results demonstrate that the approach produces enhanced images with superior visual quality compared to the existing methods on all test datasets, especially for some extremely dark images. Furthermore, the quantitative evaluation results based on metrics PSNR, SSIM, LPIPS, BRISQUE, and MUSIQ show the proposed model achieves superior performance, with PSNR and BRISQUE significantly outperforming the baseline approaches, where (PSNR, mean BRISQUE) values of the proposed method and the second best results are (17.14, 17.72) and (16.44, 19.65). Additionally, further experimental results such as ablation studies indicate the effectiveness of the proposed model."
a lightweight tomato leaf disease identification method based on shared‐twin neural networks,"Automatic detection of tomato leaf spot disease is essential for control and loss reduction. Traditional algorithms face challenges such as large amount of data, multiple training and heavy computation. In this study, a lightweight shared Siamese neural network method was proposed for tomato leaf disease identification, which is suitable for resource‐limited environments. Experiments on Plant‐Village, Taiwan and Taiwan ++ datasets show that the accuracy fluctuates very little even when trained with only 60% of the data, which confirms the effectiveness of the proposed method in the small data environment. In addition, compared with the mainstream algorithms, it improves the accuracy by up to 35.3%on Plant‐Village and two Taiwan datasets respectively. The experimental results also show that the proposed method still performs well when the data is imbalanced and the sample size is small."
a 4d spontaneous micro‐expression database: establishment and evaluation,"Micro‐expressions are spontaneous and unconscious facial movements that reveal individuals’ genuine inner emotions. They hold significant potential in various psychological testing fields. As the face is a 3D deformation object, the emergence of facial expression leads to spatial deformation of the face. However, existing databases primarily offer 2D video sequences, limiting descriptions of 3D spatial information related to micro‐expressions. Here, a new micro‐expression database is proposed, which contains 2D image sequences and corresponding 3D point cloud sequences. These samples were classified using both an objective method based on the facial action coding system and a non‐objective emotion classification method that considers video contents and participants’ self‐reports. A variety of feature extraction techniques are applied to 2D data, including traditional algorithms and deep learning methods. Additionally, a novel local curvature‐based algorithm is developed to extract 3D spatio‐temporal deformation features from the 3D data. The authors evaluated the classification accuracies of these two features individually and their fusion results under leave‐one‐subject‐out (LOSO) and tenfold cross‐validation. The results demonstrate that fusing 3D features with 2D features results in improved recognition performance compared to using 2D features alone."
a hybrid u-shaped and transformer network for change detection in high-resolution remote sensing images,"Deep convolutional neural networks based remote sensing change detection has recently shown significant performance improvement. However, small region changes and global‐local features in high‐resolution remote sensing images are not fully explored. This paper introduces a hybrid U‐shaped and transformer network for change detection in high‐resolution remote sensing images. Specifically, a UNet++‐based backbone to facilitate feature learning across different scales. In addition, we introduce a transformer‐based feature fusion module for extracting long‐range dependencies, which can enhance the representation ability of the network. Furthermore, the introduced efficient channel attention mechanism can efficiently calibrate the feature representation and concentrate on more important feature information. Thanks to the above designs, the proposed method enjoys a strong ability to extract local and global features for remote sensing change detection. Extensive experimental results on different remote sensing images show that our method can achieve superior performance in comparison with state‐of‐the‐art change detection methods."
a survey of feature matching methods,"Feature matching plays a crucial role in computer vision, with applications in visual localization, simultaneous localization and mapping (SLAM), image stitching, and more. It establishes correspondences between sets of feature points from multiple images, enabling various tasks. Over the years, feature matching has witnessed significant development, with an increasing number of methods being applied. However, different methods exhibit different degrees of applicability in different scenarios and requirements due to their different rationales. To cope with these issues, a comprehensive analysis and comparison of matching methods are essential. Existing reviews often lack coverage of deep learning models and focus more on feature detection and description, neglecting the matching process. This survey investigates feature detection, description, and matching techniques within the feature‐based image‐matching pipeline. Representative methods, their mechanisms, and application scenarios are also briefly introduced. In addition, comprehensive evaluations of classical and state‐of‐the‐art methods are conducted through extensive experiments on representative datasets. Particularly, matching‐based applications are compared to fully demonstrate the advantages of the methods. Lastly, this survey highlights current problems and development directions in matching methods, serving as a reference for researchers in the field."
iris image retrieval using partial matching of image blocks,"The problem of human identification through recognition of patterns in iris images captured in unconstrained environments results in image artefacts such as image occlusion and specular reflection, in which iris tissue is observable to extremely low content. To overcome this problem, this paper presents a novel method for iris image retrieval and recognition based on partial pattern matching. The main contribution of the proposed method relies on an image partitioning schema in which the iris is divided into non‐overlapping blocks with varying dimensions, facilitating the identification and removal of image regions impaired by eyelids, eyelashes, and specular reflections. In fact, the blocks that contain artefacts are completely ignored and those blocks are preserved that include useful patterns for identification. In addition, a multi‐feature similarity followed by a score fusion technique is employed for ranking the retrieval results. The remarkable results of the classification stage include an accuracy of 100%, 98.75%, and 99.94% on three benchmark databases, including UPOL, UBIRIS.V2, and CASIA‐Iris‐Interval.v3, respectively. Additionally, in the retrieval stage, the proposed method achieves a precision of 100% on all three benchmark databases, a recall of 83.33%, and a F1‐measure of 90.91 on the CASIA‐Iris‐Interval.v3 dataset."
super-resolution reconstruction based on generative adversarial networks with dual branch half instance normalization,"This paper proposes a super‐resolution reconstruction model, SRPGANto improve the visual quality of images based on generative adversarial networks (GANs) by improving the network structures of the generator and the discriminator. In the generator, a dual branch residual block is designed instead of the residual block, including a branch with an attention mechanism and a branch without an attention mechanism, to extract more differentiated features. Normalization methods are explored to avoid unstable training and bath normalization artifacts and use a half instance normalization layer that is more suitable for underlying visual problems compared with traditional batch normalization. In the discriminator, PatchGAN is applied instead of typical GAN to improve the generation of local texture by discriminating each patch rather than the global image. The experimental results on the public datasets demonstrate that the proposed SRPGAN can achieve excellent quantitative evaluation while improving the visual quality of reconstructed images."
hgsnet: a hypergraph network for subtle lesions segmentation in medical imaging,"Lesion segmentation is a fundamental task in medical image processing, often facing the challenge of subtle lesions. It is important to detect these lesions, even though they can be difficult to identify. Convolutional neural networks, an effective method in medical image processing, often ignore the relationship between lesions, leading to topological errors during training. To tackle topological errors, move is made from pixel‐level to hypergraph representations. Hypergraphs can model lesions as vertices connected by hyperedges, capturing the topology between lesions. This paper introduces a novel dynamic hypergraph learning strategy called DHLS. DHLS allows for the dynamic construction of hypergraphs contingent upon input vertex variations. A hypergraph global‐aware segmentation network, termed HGSNet, is further proposed. HGSNet can capture the key high‐order structure information, which is able to enhance global topology expression. Additionally, a composite loss function is introduced. The function emphasizes the global aspect and the boundary of segmentation regions. The experimental setup compared HGSNet with other advanced models on medical image datasets from various organs. The results demonstrate that HGSNet outperforms other models and achieves state‐of‐the‐art performance on three public datasets."
mspan: multi-scale pyramid attention network for efficient skin cancer lesion segmentation,"Skin cancer is common and deadly, needs to be detected and treated properly. Deep learning algorithms like UNet have shown potential results in medical imaging. Such approaches still struggle to capture fine‐grained details and scale differences in skin lesions‐based occlusions' appearance, size etc. This research proposes a redesign UNet, the Multi‐Scale Pyramid Attention Network (MSPAN), to improve skin cancer lesion segmentation. The input data is processed at numerous scales with varied receptive fields. This enhances the network's ability to identify lesion locations by capturing local and global context. Attention approaches also help the network to suppress noise by focusing on informative features. We have evaluated MSPAN model on the publicly available ISIC2018 benchmark dataset for skin lesion segmentation. The method surpasses traditional UNet and other current methods in accuracy and effectiveness. The model also has a post‐processing to estimate lesion area for fast inference, making it suitable for extensive screening. Redesigned UNet with the Multi‐Scale Pyramid Attention Network improves skin cancer lesion segmentation. The model's ability to collect fine‐grained information and handle occlusions allows for more accurate skin cancer diagnosis and treatment. The MSPAN design can improve computer‐aided diagnosis systems and help dermatologists make precise clinical decisions."
a metaheuristic image cryptosystem using improved parallel model and many‐objective optimization,"Metaheuristic is one of the techniques to improve the security of image encryption. However, existing metaheuristic image cryptosystems based on metaheuristic may have convergence difficulties during the optimization process, which cause insecurity and slow convergence. Besides, the time cost of the parallel execution model applied to metaheuristic image cryptosystems is not low enough. Therefore, a parallel many‐objective optimized key generation framework is proposed. Firstly, if four or more security indicators of cryptosystem, which are the results of security test, need to be optimized, the many‐objective optimization algorithm is employed to the proposed framework. With method adjusts the chaotic system parameters as the optimization key, which effectively avoid the convergence difficulty of the encryption key. Secondly, a master‐slave parallel model is improved to metaheuristic encryption. The model allocates the most time‐consuming fitness calculation work to slave nodes, which makes the modified model more reasonable and thus reduces the encryption time. To evaluate the performance of the proposed framework, a specific encryption scheme is constructed, that utilizes a 2D quadric map and many‐objective optimization algorithm based on dominance and decomposition (MOEA/DD) to optimize five security indicators. Experimental results reveal that this scheme has good security performances and less parallel encryption time."
a lightweight underwater fish image semantic segmentation model based on u‐net,"Semantic segmentation of underwater fish images is vital for monitoring fish stocks, assessing marine resources, and sustaining fisheries. To tackle challenges such as low segmentation accuracy, inadequate real‐time performance, and imprecise location segmentation in current methods, a novel lightweight U‐Net model is proposed. The proposed model acquires more segmentation details by applying a multiple‐input approach at the first four encoder levels. To achieve both lightweight and high accuracy, a multi‐scale residual structure (MRS) module is proposed to reduce parameters and compensate for the accuracy loss caused by the reduction of channels. To improve segmentation accuracy, a multi‐scale skip connection (MSC) structure is further proposed, and the convolution block attention mechanism (CBAM) is introduced at the end of each decoder level for weight adjustment. Experimental results demonstrate a notable reduction in model volume, parameters, and floating‐point operations by 94.20%, 94.39%, and 51.52% respectively, compared to the original model. The proposed model achieves a high mean intersection over union (mIOU) of 94.44%, mean pixel accuracy (mPA) of 97.03%, and a frame rate of 43.62 frames per second (FPS). With its high precision and minimal parameters, the model strikes a balance between accuracy and speed, making it particularly suitable for underwater image segmentation."
um-gan: underground mine gan for underground mine low-light image enhancement,"In recent years, low‐light image enhancement has become increasingly active. However, in underground mine environments, acquiring high‐quality images is still challenging due to low light, low contrast, and occlusion. To address this problem, this study proposes a low‐light image enhancement method for underground mine based on generative adversarial networks (UM‐GAN), which aims to take full advantage of the ability of GAN to achieve the restoration of details, the reduction of noise, and the improvement of overall image quality. The model proposed in this paper is divided into three main parts. Initially, a generator network adopting an encoder–decoder structure is developed. Subsequently, a novel strategy is introduced to merge information by utilizing inverted greyscale images and low‐light images as inputs. Further image quality enhancement is achieved by incorporating a noise reduction module employing the diffusion model. To ascertain the efficacy of the UM‐GAN method, evaluations are conducted on diverse real‐world and synthetic datasets, juxtaposing the approach against superior methods. Through qualitative and quantitative comparative experiments, the method showcases noteworthy advancements through qualitative and quantitative comparative experiments, substantiating its effectiveness. This research provides new ideas and methods for overcoming image quality problems in underground mine environments and contributes to the development of underground mine image processing."
enhanced classification performance through gaugan‐based data augmentation for tomato leaf images,"This study investigated a data augmentation method for plant disease classification and early diagnosis based on a generative adversarial neural network (GAN). In the development of classification models using deep learning, data imbalance is a primary factor that reduces classification performance. To address this issue, tomato disease images from the public dataset PlantVillage were used to evaluate the performance of the GauGAN algorithm. The images generated by the proposed GauGAN model were used to train a MobileNet‐based classification model and compared with methods trained with conventional data augmentation techniques and cut‐mix and mix‐up algorithms. The experimental results demonstrate that based on F1‐scores, GauGAN‐based data augmentation outperformed conventional methods by more than 10%. In addition, after the model was retrained on data collected in the field, it efficiently generated various disease images. The evaluation results from those images also revealed a data augmentation effect of about 10% compared with traditional augmentation techniques."
hab-net: hierarchical asymmetric convolution and boundary enhancement network for brain tumor segmentation,"Brain tumour segmentation (BTS) is crucial for diagnosis and treatment planning by delineating tumour boundaries and subregions in multi‐modality bio‐imaging data. Several BTS models have been proposed to address specific technical challenges encountered in this field. However, accurately capturing intricate tumour structures and boundaries remains a difficult task. To overcome this challenge, HAB‐Net, a model that combines the strengths of convolutional neural networks and transformer architectures, is presented. HAB‐Net incorporates a custom‐designed hierarchical and pseudo‐convolutional module called hierarchical asymmetric convolutions (HAC). In the encoder, a coordinate attention is included to extract feature maps. Additionally, swin transformer, which has a self‐attention mechanism, is integrated to effectively capture long‐range relationships. Moreover, the decoder is enhanced with a boundary attention module (BAM) to improve boundary information and overall segmentation performance. Extensive evaluations conducted on the BraTS2018 and BraTS2021 datasets demonstrate significant improvements in segmentation accuracy for tumour regions."
maritime vessel classification based on a dual network combining efficientnet with a hybrid network mpanet,"Ship classification is an important technique for enhancing maritime management and security. Visible and infrared sensors are generally employed to deal with the challenging problem and improve classification performance. Herein, a two‐branch feature fusion neural network structure is proposed to classify the visible and infrared maritime vessel images simultaneously. Specifically, in this two‐branch neural network, one branch is based on a deep convolutional neural network that is used to extract the visible image features, while the other is a hybrid network structure that is a multi‐scale patch embedding network called MPANet. The sub‐network MPANet can extract fine‐ and coarse‐grained features, in which the pooling operation instead of the multi‐head attention mechanism is utilized to reduce memory consumption. When there are infrared images, it is used to extract the infrared image features, otherwise, this branch is also utilized to extract visible image features. Therefore, this dual network is suitable with or without infrared images. The experimental results on the visible and infrared spectrums (VAIS) dataset demonstrate that the introduced network achieves state‐of‐the‐art ship classification performance on visible images and paired visible and infrared ship images."
a review of optic disc and optic cup segmentation based on fundus images,"Optic disc (OD) and optic cup (OC) segmentation is an important task in ophthalmic medicine and is crucial for aiding glaucoma screening. With the development of smart healthcare and the increase of large datasets, there is an increasing number of research efforts targeting OD and OC segmentation, making it particularly important to provide a systematic review of the latest advances in the field. This paper presents a systematic review of commonly used datasets, evaluation metrics, and related research results in the field of OD and OC segmentation. The advantages and disadvantages of segmentation techniques based on traditional and deep learning methods are comparatively analysed. In addition, this study emphasizes the importance of OD and OC segmentation efforts in smart healthcare. Despite the technological advances, the lack of generalization capability is still a major obstacle limiting its clinical application. To address this issue, this study explores unsupervised domain adaptation methods to enhance the generalization performance of segmentation techniques and provide new strategies for clinical diagnosis. Finally, this paper discusses the challenges and future research directions faced by OD and OC segmentation when applied in the medical field to help readers comprehensively grasp the research dynamics in this area."
mdsk‐net: multi‐scale dynamic segmentation kernel network for renal tumour endoscopic image segmentation,"Automatic segmentation of renal tumours during renal cell carcinoma surgery can help doctors accurately locate the tumour region, protect the tissues and organs around the kidneys, enhance surgical efficiency, and reduce the possibility of leakage and misdiagnosis. However, since general polyp endoscopic image segmentation models have many problems when facing the task of renal tumour segmentation, there needs to be more research on the segmentation of endoscopic images of renal tumours. This paper proposes a multi‐scale dynamic segmentation kernel network for endoscopic image segmentation of kidney tumours. First, a spatial receptive field module is proposed to augment the feature information and improve the performance of the whole network. Second, an enhanced cross‐attention module is offered to attenuate the effect of a high‐similarity segmentation background. Finally, a multi‐scale dynamic segmentation kernel module is introduced to gradually refine the segmentation results from small to large sizes to obtain more accurate tumour boundaries. Extensive experiments on the established kidney tumour endoscopic dataset and publicly available endoscopic datasets show that this method exhibits enhanced performance and generalization capabilities compared to existing techniques. On this renal tumour dataset, MDSK‐Net achieved excellent results of 94.1% and 90.1% on mDice and mIoU."
change detection in sar image based on weighted difference image generation and optimized random forest,"Synthetic aperture radar (SAR) image change detection suffers from poor quality of the difference image and low detection accuracy. Hence, this paper proposes a SAR image change detection method based on a fused difference image and an optimized random forest scheme, termed LRN‐SSARF. Specifically, a fusion operator difference image LRN is proposed, which is generated using a weighted fusion of log‐ratio (LR), ratio (R), and normalized ratio (NoR). This difference image generation method reduces noise's influence. Then, the Otsu algorithm is applied to segment the difference image and select the training samples. The training samples are input into the random forest (RF) model optimised by the sparrow search algorithm (SSA) for training and classification. Finally, the region link is uesd to refine the detection results and generate the final result. The change detection results of six real SAR image scenes highlight that the proposed algorithm has a high detection accuracy, and affords appealing integrity and detailed information about the change regions. Specially, the detection accuracy advantage of the Bangladesh dataset is larger, with the accuracy and Kappa coefficient reaching 98.04% and 92.00%, much higher than the competitor methods."
mfe‐mvsnet: multi‐scale feature enhancement multi‐view stereo with bi‐directional connections,"Recent advancements in deep learning have significantly improved performance in the multi‐view stereo (MVS) domain, yet achieving a balance between reconstruction efficiency and quality remains challenging for learning‐based MVS methods. To address this, we introduce MFE‐MVSNet, designed for more effective and precise depth estimation. Our model incorporates a pyramid feature extraction network, featuring efficient multi‐scale attention and multi‐scale feature enhancement modules. These components capture pixel‐level pairwise relationships and semantic features with long‐range contextual information, enhancing feature representation. Additionally, we propose a lightweight 3D UNet regularization network based on depthwise separable convolutions to reduce computational costs. This network employs bi‐directional skip connections, establishing a fluid relationship between encoders and decoders and enabling cyclic reuse of building blocks without adding learnable parameters. By integrating these methods, MFE‐MVSNet effectively balances reconstruction quality and efficiency. Extensive qualitative and quantitative experiments on the DTU dataset validate our model's competitiveness, demonstrating approximately 33% and 12% relative improvements in overall score compared to MVSNet and CasMVSNet, respectively. Compared to other MVS networks, our approach more effectively balances reconstruction quality with efficiency."
an adaptive neuro-fuzzy inference system optimized by genetic algorithm for brain tumour detection in magnetic resonance images,"An adaptive neuro‐fuzzy inference system is presented which is optimized by a genetic algorithm to classify normal and abnormal brain tumours. The classifier is fast and simple, named genetic algorithm‐adaptive neuro‐fuzzy inference system, and the determined learning rules minimize its error and improve its accuracy. The presented system follows five steps including preprocessing, morphological operation, feature extraction, feature selection, and classification. Morphological operators segment the abnormal regions and calculate the tumour area. The statistical features and the grey‐level co‐occurrence matrix are employed for feature extraction. Magnetic resonance images are considered and 12 statistical features are extracted, then the genetic algorithm‐based selection technique helps to select features and reduce the extracted features and improves the accuracy and decision time. So, the high dimensionality and the computational complexity of the adaptive neuro‐fuzzy inference system are reduced, and the classifier decides more efficiently. The input data are the figshare brain tumour dataset with 670 abnormal and 670 normal magnetic resonance images, and the classifier requires 10.788 s for classification. The efficient performance of the genetic algorithm‐adaptive neuro‐fuzzy inference system is confirmed by the accuracy of 99.85%, sensitivity of 99.7%, specificity of 100%, precision of 100%, and mean square error of 0.0027."
interpretable vision transformer based on prototype parts for covid-19 detection,"Over the past few years, the COVID‐19 virus has had a significant impact on the physical and mental health of people around the world. Therefore, in order to effectively distinguish COVID‐19 patients, many deep learning efforts have used chest medical images to detect COVID‐19. As with model accuracy, interpretability is also important in the work related to human health. This work introduces an interpretable vision transformer that uses the prototype method for the detection of positive patients with COVID‐19. The model can learn the prototype features of each category based on the structural characteristics of ViT. The predictions of the model are obtained by comparing all the features of the prototype in the designed prototype block. The proposed model was applied to two chest X‐ray datasets and one chest CT dataset, achieving classification performance of 99.3%, 96.8%, and 98.5% respectively. Moreover, the prototype method can significantly improve the interpretability of the model. The decisions of the model can be interpreted based on prototype parts. In the prototype block, the entire inference process of the model can be shown and the predictions of the model can be demonstrated to be meaningful through the visualization of the prototype features."
network architecture for single image super‐resolution: a comprehensive review and comparison,"Single image super‐resolution (SISR) is a promising research direction in computer vision and image processing for improving the visual perception of low‐quality images. In recent years, deep learning algorithms have driven tremendous development in SR, and SR methods based on various network architectures have significantly improved the quality of reconstructed images. Although there has been a large amount of reviews focusing on SISR, few studies have focused specifically on network architectures for SISR. This paper aims to provide a systematic overview of the design ideas of SISR using multiple architectures, including Convolutional Neural Networks (CNN), Generative Adversarial Networks (GAN), Transformer, and Diffusion model. In addition, an experimental analysis and comparison of state‐of‐the‐art SR algorithms have been performed on publicly available quantitative and qualitative datasets. Finally, some future directions are discussed that may help other community researchers."
real-time defect detection method based on yolo-gss at the edge end of a transmission line,"Combining edge devices with intelligent inspection for transmission lines can fulfill the demand for real‐time defect detection in the field. However, there has been limited research on algorithms suitable for edge devices with low computational power and memory, and the existing research primarily focuses on CPU optimization. To address these issues, this paper proposes a real‐time defect detection method for transmission line endpoints based on YOLO‐GSS (YOLOv8 with Mosaic‐9, G‐GhostNet, S‐FPN, and Spatial Intersection over Union (SIoU) modifications). First, the authors improve the input of the YOLOv8 network using Mosaic‐9 to increase the number of input features in the training phase and enhance algorithm robustness. Next, the authors introduce G‐GhostNet and S‐FPN to enhance the backbone and neck sections while improving inference speed and accuracy. Finally, the authors modify the Complete Intersection over Union loss function of YOLOv8 using SIoU to further improve the detection accuracy. Experimental results demonstrate that compared to the original YOLOv8, the proposed method achieves a 5x increase in inference speed on Nvidia Jetson NX edge devices and a 7.7% improvement in accuracy, meeting the real‐time defect detection requirements for transmission line field inspections."
ttmri: multislice texture transformer network for undersampled mri reconstruction,"Magnetic resonance imaging (MRI) is a non‐interposition imaging technique that provides rich anatomical and physiological information. Yet it is limited by the long imaging time. Recently, deep neural networks have shown potential to significantly accelerate MRI. However, most of these approaches ignore the correlation between adjacent slices in MRI image sequences. In addition, the existing deep learning‐based methods for MRI are mainly based on convolutional neural networks (CNNs). They fail to capture long‐distance dependencies due to the small receptive field. Inspired by the feature similarity in adjacent slices and impressive performance of Transformer for exploiting the long‐distance dependencies, a novel multislice texture transformer network is presented for undersampled MRI reconstruction (TTMRI). Specifically, the proposed TTMRI is consisted of four modules, namely the texture extraction, correlation calculation, texture transfer and texture synthesis. It takes three adjacent slices as inputs, in which the middle one is the target image to be reconstructed, and the other two are auxiliary images. The multiscale features are extracted by the texture extraction module and their inter‐dependencies are calculated by the correlation calculation module, respectively. Then the relevant features are transferred by the texture transfer module and fused by the texture synthesis module. By considering inter‐slice correlations and leveraging the Transformer architecture, the joint feature learning across target and adjacent slices are encouraged. Moreover, TTMRI can be stacked with multiple layers to recover more texture information at different levels. Extensive experiments demonstrate that the proposed TTMRI outperforms other state‐of‐the‐art methods in both quantitative and qualitative evaluationsions."
an image-based runway detection method for fixed-wing aircraft based on deep neural network,"Visual information is important in final approach and landing phases for an approaching aircraft, it presents supplementary source for navigation system, and provides backup guidance when radio navigation fails, or even supports a complete vision‐based landing. Relative position and attitude can be solved from the runway features in the image. Traditional runway detection methods have high latency and low accuracy, which is unable to satisfy the requirements for a safe landing. This paper proposes a real‐time runway detection model, efficient runway feature extractor (ERFE), based on deep convolutional neural network, generating semantic segmentation and feature lines output. In order to evaluate the model's effectiveness, a benchmark is proposed to calculate the actual error between predicted feature line and ground truth one. A novel runway dataset which is based on pictures from Microsoft Flight Simulator 2020 (FS2020), is also proposed in this paper to train and test the model. The dataset will be released at https://www.kaggle.com/datasets/relufrank/fs2020‐runway‐dataset. ERFE shows excellent performance in FS2020 dataset, it gives satisfactory results even for real runway images excluded from our dataset."
improvement of ship target detection algorithm for yolov7-tiny,"In addressing the challenge of ships being prone to occlusion in multi‐target situations during ship target detection, leading to missed and false detections, this paper proposes an enhanced ship detection algorithm for YOLOv7‐tiny. The proposed method incorporates several key modifications. Firstly, it introduces the Convolutional Block Attention Module in the Backbone section of the original model, emphasizing position information while attending to channel features to enhance the network's ability to extract crucial information. Secondly, it replaces standard convolution with GSConv convolution in the Neck section, preserving detailed information and reducing computational load. Subsequently, the lightweight operator Content‐Aware ReAssembly of Features is employed to replace the original nearest‐neighbour interpolation, mitigating the loss of feature information during the up‐sampling process. Finally, the localization loss function, SIOU Loss, is utilized to calculate loss, expedite training convergence, and enhance detection accuracy. The research results indicate that the precision of the improved model is 91.2%, mAP@0.5 is 94.5%, and the F1‐score is 90.7%. These values are 3.7%, 5.5%, and 4.2% higher than those of the original YOLOv7‐tiny model, respectively. The improved model effectively enhances detection accuracy. Additionally, the improved model achieves an FPS of 145.4, meeting real‐time requirements."
multi-exposure embeddings for graph learning: towards high dynamic range image saliency prediction,"Identifying saliency in high dynamic range (HDR) images is a fundamentally important issue in HDR imaging, and plays critical roles towards comprehensive scene understanding. Most of existing studies leverage hand‐crafted features for HDR image saliency prediction, lacking the capabilities of fully exploiting the characteristics of HDR image (i.e. wider luminance range and richer colour gamut). Here, systematical studies are carried out on HDR image saliency prediction by proposing a new framework to single out the contributions from multi‐exposure images. Specifically, inspired by the mechanism of HDR imaging, the method first utilizes graph neural networks to model the relations among multi‐exposure images and the tone‐mapped image obtained from an HDR image, enabling more discriminative saliency‐related feature representations. Subsequently, the saliency features driven by global semantic knowledge are aggregated from the tone‐mapped image through enhancing global context‐aware semantic information. Finally, a fusion module is designed to integrate saliency‐oriented feature representations originated from multi‐exposure images and the tone‐mapped image, producing the saliency maps of HDR images. Moreover, a new challenging HDR eye fixation database (HDR‐EYEFix) is created, expecting to further contribute the research on HDR image saliency prediction. Experiment results show that the method obtains superior performance compared to the state‐of‐the‐art methods."
infrared multi-target detection and tracking in dense urban traffic scenes,"Infrared object detection and tracking in dense urban traffic remain a challenge due to factors such as low contrast, small intra‐class differences, and frequent false positives and negatives. To overcome these, the authors introduce YOLO‐IR, an algorithm based on the enhanced YOLOv8s, and YOLO‐DeepOC‐IR, a comprehensive infrared multi‐object tracking method for urban traffic, integrating both detection and tracking. During preprocessing, three infrared image enhancement techniques, local contrast multi‐scale enhancement, non‐local means, and contrast limited adaptive histogram equalization, are applied for better reliability in dense scenes. To further improve the performance, the original YOLOv8s backbone is replaced with MobileVITv3 to enhance detection accuracy and robustness. This infrared feature extraction module, incorporated into the detector, combines canny edge detection, Gabor filtering, and open operation layers, significantly boosting object detection in infrared imagery. The tracker's feature processing capabilities are improved using the learned arrangements of three patch codes descriptor and locality‐sensitive hashing for feature extraction and matching. Experimental results on FLIR ADAS v2 and InfiRay datasets indicate superior performance of this method, achieving 78.6% mAP and 151.1 FPS in detection, and up to 80.8% moving object tracking accuracy, 78.6% identification F1 score, and 62.1% higher order tracking accuracy in multi‐object tracking."
a flare removal network for night vision perception: resistant to the interference of complex light,"The high‐precision visual perception results are easily affected by the lens flare issue when the image sensor is facing to strong light. The existing flare removal methods have poor robustness when confronted with flare interference caused by complex nighttime lighting, which has to preserve natural light source information. A simulated dataset for the removal of night flares is created to solve the problem of collecting complete paired training data, and night flare removal network (NFR‐Net) is proposed to remove the interference caused by various light disturbances at night. The light source extraction module is introduced to retain light source information realistically and effectively in night vision scenes. Extensive experimental results demonstrate that the proposed method is superior to the existing related methods in the various complex night vision scenes. The proposed NFR‐Net can enhance visual perception of nighttime images significantly and improve the performance of night vision tasks."
tim-net: a multi-label classification network for tcm tongue images fusing global-local features,"Combining the extracted tongue features with other medical indicators can effectively judge the diseases of patients. The previous work usually only analyzes a certain feature of the tongue body and is unable to extract multiple features simultaneously. In this study, a multi‐label classification network named TIM‐Net is proposed, which integrates global and local features to achieve multi‐label intelligent diagnosis of Chinese medicine tongue images. First, a feature extraction network based on ResNet is proposed to capture the features of tongue images more sufficiently. Then, a multi‐label classification algorithm fusing global and local features is proposed, and targeted screening operations are carried out on the class‐related feature maps based on global confidence. In addition, a logical masking algorithm is proposed to ensure that the local features can only correct the feature labels they represent, and do not interfere with other feature labels. The classification accuracy is further improved by using local feature confidence and correcting the global classification results. Finally, the experimental results indicate that the classification accuracy of the tongue images is gradually improved through optimizing the feature extraction network and fusing local features, and it exceeds other state‐of‐the‐art multi‐label classification networks."
yolov5s maritime distress target detection method based on swin transformer,"In recent years, the task of maritime emergency rescue has increased, while the cost of time for traditional methods of search and rescue is pretty long with poor effect subject to the constraints of the complex circumstances around the sea, the effective conditions, and the support capability. This paper applies deep learning and proposes a YOLOv5s‐SwinDS algorithm for target detection in distress at sea. Firstly, the backbone network of the YOLOv5s algorithm is replaced by swin transformer, and a multi‐level feature fusion module is introduced to enhance the feature expression ability for maritime targets. Secondly, deformable convolutional networks v2 (DCNv2) is used instead of traditional convolution to improve the recognition capability for irregular targets when the neck network features are output. Finally, the CIoU loss function is replaced with SIoU to reduce the redundant box effectively while accelerating the convergence and regression of the predicted box. Experimenting on the publicly dataset SeaDronesSee, the , , and of YOLOv5s‐SwinDS model are 87.9%, 75.8%, 79.1% and 42.9%, respectively, which get higher results than the original YOLOv5s model, the YOLOv7 series of models, and the YOLOv8 series of models. The experiments verifies that the algorithm has good performance in detecting maritime distress targets."
lightweight fruit detection algorithms for low‐power computing devices,"A lightweight fruit detection algorithm is important to ensure real‐time detection on low‐power computing devices while maintaining detection accuracy. In addition, the fruit detection algorithm is also faced with some environmental factors. To solve these challenges, lightweight detection algorithms termed YOLO‐Lite, YOLO‐Liter and YOLO‐Litest were developed based on the YOLOv5 framework. The compared mean average precision (mAP) detection revealed that YOLO‐Lite at 0.86 is 2%, 4%, 5%, 7%, and 16% more than YOLO‐Liter and YOLOv5n at 0.84 each, YOLOv4‐tiny at 0.82, YOLO‐Liter at 0.81, YOLO‐MobileNet at 0.79, and YOLO‐ShuffleNet at 0.70, respectively, but not for YOLOv8n at 0.87. On the Computer platform, except for YOLOv4‐tiny at 178.6 frames per second (FPS), the speed of YOLO‐Litest at 158.7 FPS is faster than YOLO‐Liter at 129.9 FPS, YOLO‐Lite at 120.5 FPS, YOLO‐ShuffleNet at 119.0 FPS, YOLOv8n at 116 FPS, YOLOv5n at 111.1 FPS, and YOLO‐MobileNet at 89.3 FPS. Using Jetson Nano, the 32.3 FPS of YOLO‐Litest is faster than other algorithms, but not YOLOv4‐tiny's 34.1 FPS. On the Raspberry Pi 4B, YOLO‐Litest with 4.69 FPS, outperformed other algorithms. The choices for an accurate and faster detection algorithm are YOLO‐Lite and YOLO‐Litest respectively, while YOLO‐Liter maintains a balance between them."
cross‐scale resolution consistent network for salient object detection,"The salient object detection task tries to simulate the human visual system for most eye‐catching objects or regions detection. However, due to the complexity of the visual mechanisms, current methods will suffer from severe performance degradation, leading to inconsistent prediction results for the same regions, when directly adopting a model trained on a fixed resolution to evaluate at other different resolutions. Considering that consistency in predictions is essential for salient object detection, a cross‐scale resolution consistent salient object detection method, called RCNet, is proposed. Specifically, to enhance the model's capacity for generalization across images of varying resolutions and make the model implicitly learn the scale invariance, a multi‐resolution data enhancement module is constructed to generate images with arbitrary resolutions for the same scene. Moreover, to accomplish better multi‐level feature fusion, a cross‐scale fusion module is developed to fuse high‐level semantic features and low‐level detail features. Additionally, to explicitly learn the scale invariance of the salient scores, a hybrid salient consistency loss is formulated on salient object detection with different resolutions. Comprehensive evaluations on five benchmark datasets show that RCNet achieves a highly competitive result."
no-reference image quality assessment via a dual-branch residual network,"The No‐Reference Image Quality Assessment (NR‐IQA) method can predict quality scores of distorted images without the reference image. However, due to the variability in both color and structure of images, existing NR‐IQA methods struggle to accurately predict quality scores of distorted images. Therefore, an NR‐IQA method based on a Dual‐Branch Residual Network (DBRIQA) for evaluating the quality scores of color‐distorted images is proposed. First, guided filtering is applied to the hue images in the HSV space to extract distortion information from the image's color. Then, due to significant differences in distorted images, enhancements are made to the traditional residual blocks, forming a feature extraction module that captures multi‐scale features from the image. In order to capture the global relationships in the distorted image, a Global‐Level Attention Block (GLAB) is introduced, facilitating the interaction of information among the extracted features. Experiments were conducted across four publicly available IQA datasets, including LIVE, CSIQ, TID2008, and TID2013. The experimental results demonstrate that the proposed method exhibits strong performance and generalization capabilities in predicting image quality compared to peer methods."
stereo matching from monocular images using feature consistency,"Synthetic images facilitate stereo matching. However, synthetic images may suffer from image distortion, domain bias, and stereo mismatch, which would significantly restrict the widespread use of stereo matching models in the real world. The first goal in this paper is to synthesize real‐looking images for minimizing the domain bias between the synthesized and real images. For this purpose, sharpened disparity maps are produced from a mono real image. Then, stereo image pairs are synthesized using these imperfect disparity maps and the single real image in the proposed pipeline. Although the synthesized images are as realistic as possible, the domain styles of the synthesized images are always very different from the real images. Thus, the second goal is to enhance the domain generalization ability of the stereo matching network. For that, the feature extraction layer is replaced with a teacher–student model. Then, a constraint of binocular contrast features is imposed on the output of the model. When tested on the KITTI, ETH3D, and Middlebury datasets, the accuracy of the method outperforms traditional methods by at least 30%. Experiments demonstrate that the approaches are general and can be conveniently embedded into existing stereo networks."
