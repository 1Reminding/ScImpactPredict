id,title,TNCSI,abstract,OA,authors_title
03915660-c7ee-4993-a48f-d10f3c262408,"Privacy in Large Language Models: Attacks, Defenses and Future Directions",0.850771,"The advancement of large language models (LLMs) has significantly enhanced
the ability to effectively tackle various downstream NLP tasks and unify these
tasks into generative pipelines. On the one hand, powerful language models,
trained on massive textual data, have brought unparalleled accessibility and
usability for both models and users. On the other hand, unrestricted access to
these models can also introduce potential malicious and unintentional privacy
risks. Despite ongoing efforts to address the safety and privacy concerns
associated with LLMs, the problem remains unresolved. In this paper, we provide
a comprehensive analysis of the current privacy attacks targeting LLMs and
categorize them according to the adversary's assumed capabilities to shed light
on the potential vulnerabilities present in LLMs. Then, we present a detailed
overview of prominent defense strategies that have been developed to counter
these privacy attacks. Beyond existing works, we identify upcoming privacy
concerns as LLMs evolve. Lastly, we point out several potential avenues for
future exploration.",None,-1
0dff7b7b-cc27-4c58-a6ac-568f87d52e62,Planning with Sequence Models through Iterative Energy Minimization,0.125204,"Recent works have shown that sequence modeling can be effectively used to
train reinforcement learning (RL) policies. However, the success of applying
existing sequence models to planning, in which we wish to obtain a trajectory
of actions to reach some goal, is less straightforward. The typical
autoregressive generation procedures of sequence models preclude sequential
refinement of earlier steps, which limits the effectiveness of a predicted
plan. In this paper, we suggest an approach towards integrating planning with
sequence models based on the idea of iterative energy minimization, and
illustrate how such a procedure leads to improved RL performance across
different tasks. We train a masked language model to capture an implicit energy
function over trajectories of actions, and formulate planning as finding a
trajectory of actions with minimum energy. We illustrate how this procedure
enables improved performance over recent approaches across BabyAI and Atari
environments. We further demonstrate unique benefits of our iterative
optimization procedure, involving new task generalization, test-time
constraints adaptation, and the ability to compose plans together. Project
website: https://hychen-naza.github.io/projects/LEAP",None,-1
d9b05e03-8fb8-49b3-ba69-60a2b04a1e4a,Therbligs in Action: Video Understanding through Motion Primitives,0.523283,"In this paper we introduce a rule-based, compositional, and hierarchical
modeling of action using Therbligs as our atoms. Introducing these atoms
provides us with a consistent, expressive, contact-centered representation of
action. Over the atoms we introduce a differentiable method of rule-based
reasoning to regularize for logical consistency. Our approach is complementary
to other approaches in that the Therblig-based representations produced by our
architecture augment rather than replace existing architectures'
representations. We release the first Therblig-centered annotations over two
popular video datasets - EPIC Kitchens 100 and 50-Salads. We also broadly
demonstrate benefits to adopting Therblig representations through evaluation on
the following tasks: action segmentation, action anticipation, and action
recognition - observing an average 10.5\%/7.53\%/6.5\% relative improvement,
respectively, over EPIC Kitchens and an average 8.9\%/6.63\%/4.8\% relative
improvement, respectively, over 50 Salads. Code and data will be made publicly
available.",None,-1
46aff764-9b3e-47c8-93f8-da7e51b920a0,Foreground Object Search by Distilling Composite Image Feature,0.565502,"Foreground object search (FOS) aims to find compatible foreground objects for
a given background image, producing realistic composite image. We observe that
competitive retrieval performance could be achieved by using a discriminator to
predict the compatibility of composite image, but this approach has
unaffordable time cost. To this end, we propose a novel FOS method via
distilling composite feature (DiscoFOS). Specifically, the abovementioned
discriminator serves as teacher network. The student network employs two
encoders to extract foreground feature and background feature. Their
interaction output is enforced to match the composite image feature from the
teacher network. Additionally, previous works did not release their datasets,
so we contribute two datasets for FOS task: S-FOSD dataset with synthetic
composite images and R-FOSD dataset with real composite images. Extensive
experiments on our two datasets demonstrate the superiority of the proposed
method over previous approaches. The dataset and code are available at
https://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD.",None,-1
3112a03b-3b5c-49bb-8973-fa027d679bb5,"Mephisto: A Framework for Portable, Reproducible, and Iterative Crowdsourcing",0.7561,"We introduce Mephisto, a framework to make crowdsourcing for research more
reproducible, transparent, and collaborative. Mephisto provides abstractions
that cover a broad set of task designs and data collection workflows, and
provides a simple user experience to make best-practices easy defaults. In this
whitepaper we discuss the current state of data collection and annotation in ML
research, establish the motivation for building a shared framework to enable
researchers to create and open-source data collection and annotation tools as
part of their publication, and outline a set of suggested requirements for a
system to facilitate these goals. We then step through our resolution in
Mephisto, explaining the abstractions we use, our design decisions around the
user experience, and share implementation details and where they align with the
original motivations. We also discuss current limitations, as well as future
work towards continuing to deliver on the framework's initial goals. Mephisto
is available as an open source project, and its documentation can be found at
www.mephisto.ai.",None,-1
1cadf93a-5c63-4d1b-9b91-08c907431244,Evaluating the Effectiveness of Natural Language Inference for Hate Speech Detection in Languages with Limited Labeled Data,0.209793,"Most research on hate speech detection has focused on English where a
sizeable amount of labeled training data is available. However, to expand hate
speech detection into more languages, approaches that require minimal training
data are needed. In this paper, we test whether natural language inference
(NLI) models which perform well in zero- and few-shot settings can benefit hate
speech detection performance in scenarios where only a limited amount of
labeled data is available in the target language. Our evaluation on five
languages demonstrates large performance improvements of NLI fine-tuning over
direct fine-tuning in the target language. However, the effectiveness of
previous work that proposed intermediate fine-tuning on English data is hard to
match. Only in settings where the English training data does not match the test
domain, can our customised NLI-formulation outperform intermediate fine-tuning
on English. Based on our extensive experiments, we propose a set of
recommendations for hate speech detection in languages where minimal labeled
training data is available.",None,-1
ded85fc3-b467-4ca0-a013-b67504a5674d,Examining Autoexposure for Challenging Scenes,0.864665,"Autoexposure (AE) is a critical step applied by camera systems to ensure
properly exposed images. While current AE algorithms are effective in well-lit
environments with constant illumination, these algorithms still struggle in
environments with bright light sources or scenes with abrupt changes in
lighting. A significant hurdle in developing new AE algorithms for challenging
environments, especially those with time-varying lighting, is the lack of
suitable image datasets. To address this issue, we have captured a new 4D
exposure dataset that provides a large solution space (i.e., shutter speed
range from (1/500 to 15 seconds) over a temporal sequence with moving objects,
bright lights, and varying lighting. In addition, we have designed a software
platform to allow AE algorithms to be used in a plug-and-play manner with the
dataset. Our dataset and associate platform enable repeatable evaluation of
different AE algorithms and provide a much-needed starting point to develop
better AE methods. We examine several existing AE strategies using our dataset
and show that most users prefer a simple saliency method for challenging
lighting conditions.",None,-1
64585e76-3e1d-4b5b-8831-5e04d4fde741,ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation,0.278232,"We address a weakly-supervised low-shot instance segmentation, an
annotation-efficient training method to deal with novel classes effectively.
Since it is an under-explored problem, we first investigate the difficulty of
the problem and identify the performance bottleneck by conducting systematic
analyses of model components and individual sub-tasks with a simple baseline
model. Based on the analyses, we propose ENInst with sub-task enhancement
methods: instance-wise mask refinement for enhancing pixel localization quality
and novel classifier composition for improving classification accuracy. Our
proposed method lifts the overall performance by enhancing the performance of
each sub-task. We demonstrate that our ENInst is 7.5 times more efficient in
achieving comparable performance to the existing fully-supervised few-shot
models and even outperforms them at times.",None,-1
852056ea-361b-4df3-b84b-7c7719cf5a8e,ThoughtSource: A central hub for large language model reasoning data,0.452033,"Large language models (LLMs) such as GPT-4 have recently demonstrated
impressive results across a wide range of tasks. LLMs are still limited,
however, in that they frequently fail at complex reasoning, their reasoning
processes are opaque, they are prone to 'hallucinate' facts, and there are
concerns about their underlying biases. Letting models verbalize reasoning
steps as natural language, a technique known as chain-of-thought prompting, has
recently been proposed as a way to address some of these issues. Here we
present ThoughtSource, a meta-dataset and software library for chain-of-thought
(CoT) reasoning. The goal of ThoughtSource is to improve future artificial
intelligence systems by facilitating qualitative understanding of CoTs,
enabling empirical evaluations, and providing training data. This first release
of ThoughtSource integrates seven scientific/medical, three general-domain and
five math word question answering datasets.",None,-1
2785f09a-cc11-403c-bd5a-d307a6ffe795,An Efficient Self-Supervised Cross-View Training For Sentence Embedding,0.122416,"Self-supervised sentence representation learning is the task of constructing
an embedding space for sentences without relying on human annotation efforts.
One straightforward approach is to finetune a pretrained language model (PLM)
with a representation learning method such as contrastive learning. While this
approach achieves impressive performance on larger PLMs, the performance
rapidly degrades as the number of parameters decreases. In this paper, we
propose a framework called Self-supervised Cross-View Training (SCT) to narrow
the performance gap between large and small PLMs. To evaluate the effectiveness
of SCT, we compare it to 5 baseline and state-of-the-art competitors on seven
Semantic Textual Similarity (STS) benchmarks using 5 PLMs with the number of
parameters ranging from 4M to 340M. The experimental results show that STC
outperforms the competitors for PLMs with less than 100M parameters in 18 of 21
cases.",None,-1
51ee54de-d753-4d36-8f59-14d3bed669b1,Preference Ranking Optimization for Human Alignment,0.995773,"Large language models (LLMs) often contain misleading content, emphasizing
the need to align them with human values to ensure secure AI systems.
Reinforcement learning from human feedback (RLHF) has been employed to achieve
this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits
complexity, instability, and sensitivity to hyperparameters in contrast to SFT.
(2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise
contrast, thus lacking contrasts from a macro perspective. In this paper, we
propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to
directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast
to accommodate preference rankings of any length. By iteratively contrasting
candidates, PRO instructs the LLM to prioritize the best response while
progressively ranking the rest responses. In this manner, PRO effectively
transforms human alignment into aligning the probability ranking of n responses
generated by LLM with the preference ranking of humans towards these responses.
Experiments have shown that PRO outperforms baseline algorithms, achieving
comparable results to ChatGPT and human responses through automatic-based,
reward-based, GPT-4, and human evaluations.",None,-1
6226d20c-a926-4797-ac30-9dc2628a7b03,TEQ: Trainable Equivalent Transformation for Quantization of LLMs,0.142942,"As large language models (LLMs) become more prevalent, there is a growing
need for new and improved quantization methods that can meet the
computationalast layer demands of these modern architectures while maintaining
the accuracy. In this paper, we present TEQ, a trainable equivalent
transformation that preserves the FP32 precision of the model output while
taking advantage of low-precision quantization, especially 3 and 4 bits
weight-only quantization. The training process is lightweight, requiring only
1K steps and fewer than 0.1 percent of the original model's trainable
parameters. Furthermore, the transformation does not add any computational
overhead during inference. Our results are on-par with the state-of-the-art
(SOTA) methods on typical LLMs. Our approach can be combined with other methods
to achieve even better performance. The code is available at
https://github.com/intel/neural-compressor.",None,-1
39c02478-24c2-4b79-87c1-d0036af71e8a,Ultra-High Resolution Segmentation with Ultra-Rich Context: A Novel Benchmark,0.912546,"With the increasing interest and rapid development of methods for Ultra-High
Resolution (UHR) segmentation, a large-scale benchmark covering a wide range of
scenes with full fine-grained dense annotations is urgently needed to
facilitate the field. To this end, the URUR dataset is introduced, in the
meaning of Ultra-High Resolution dataset with Ultra-Rich Context. As the name
suggests, URUR contains amounts of images with high enough resolution (3,008
images of size 5,120x5,120), a wide range of complex scenes (from 63 cities),
rich-enough context (1 million instances with 8 categories) and fine-grained
annotations (about 80 billion manually annotated pixels), which is far superior
to all the existing UHR datasets including DeepGlobe, Inria Aerial, UDD, etc..
Moreover, we also propose WSDNet, a more efficient and effective framework for
UHR segmentation especially with ultra-rich context. Specifically, multi-level
Discrete Wavelet Transform (DWT) is naturally integrated to release computation
burden while preserve more spatial details, along with a Wavelet Smooth Loss
(WSL) to reconstruct original structured context and texture with a smooth
constrain. Experiments on several UHR datasets demonstrate its state-of-the-art
performance. The dataset is available at https://github.com/jankyee/URUR.",None,-1
3349ecc6-c213-4b86-a5cc-4e75a0c3b563,"A Robust Multilabel Method Integrating Rule-based Transparent Model, Soft Label Correlation Learning and Label Noise Resistance",0.0284751,"Model transparency, label correlation learning and the robust-ness to label
noise are crucial for multilabel learning. However, few existing methods study
these three characteristics simultaneously. To address this challenge, we
propose the robust multilabel Takagi-Sugeno-Kang fuzzy system (R-MLTSK-FS) with
three mechanisms. First, we design a soft label learning mechanism to reduce
the effect of label noise by explicitly measuring the interactions between
labels, which is also the basis of the other two mechanisms. Second, the
rule-based TSK FS is used as the base model to efficiently model the inference
relationship be-tween features and soft labels in a more transparent way than
many existing multilabel models. Third, to further improve the performance of
multilabel learning, we build a correlation enhancement learning mechanism
based on the soft label space and the fuzzy feature space. Extensive
experiments are conducted to demonstrate the superiority of the proposed
method.",None,-1
280d2ea4-4092-4c16-a8e6-b5353e56b050,Markov Decision Processes with Time-Varying Geometric Discounting,0.185081,"Canonical models of Markov decision processes (MDPs) usually consider
geometric discounting based on a constant discount factor. While this standard
modeling approach has led to many elegant results, some recent studies indicate
the necessity of modeling time-varying discounting in certain applications.
This paper studies a model of infinite-horizon MDPs with time-varying discount
factors. We take a game-theoretic perspective -- whereby each time step is
treated as an independent decision maker with their own (fixed) discount factor
-- and we study the subgame perfect equilibrium (SPE) of the resulting game as
well as the related algorithmic problems. We present a constructive proof of
the existence of an SPE and demonstrate the EXPTIME-hardness of computing an
SPE. We also turn to the approximate notion of $\epsilon$-SPE and show that an
$\epsilon$-SPE exists under milder assumptions. An algorithm is presented to
compute an $\epsilon$-SPE, of which an upper bound of the time complexity, as a
function of the convergence property of the time-varying discount factor, is
provided.",None,-1
150bb673-4bfc-4b46-9783-58d693aee3c6,RGB-T Tracking via Multi-Modal Mutual Prompt Learning,0.310338,"Object tracking based on the fusion of visible and thermal im-ages, known as
RGB-T tracking, has gained increasing atten-tion from researchers in recent
years. How to achieve a more comprehensive fusion of information from the two
modalities with fewer computational costs has been a problem that re-searchers
have been exploring. Recently, with the rise of prompt learning in computer
vision, we can better transfer knowledge from visual large models to downstream
tasks. Considering the strong complementarity between visible and thermal
modalities, we propose a tracking architecture based on mutual prompt learning
between the two modalities. We also design a lightweight prompter that
incorporates attention mechanisms in two dimensions to transfer information
from one modality to the other with lower computational costs, embedding it
into each layer of the backbone. Extensive ex-periments have demonstrated that
our proposed tracking ar-chitecture is effective and efficient, achieving
state-of-the-art performance while maintaining high running speeds.",None,-1
3d137da2-4122-4d12-b8a8-2fc5c3f96786,"Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search",0.501855,"In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual,
zero-shot event extraction system and accompanying user interface for event
visualization & search. Using only English training data, ISI-Clear makes
global events available on-demand, processing user-supplied text in 100
languages ranging from Afrikaans to Yiddish. We provide multiple event-centric
views of extracted events, including both a graphical representation and a
document-level summary. We also integrate existing cross-lingual search
algorithms with event extraction capabilities to provide cross-lingual
event-centric search, allowing English-speaking users to search over events
automatically extracted from a corpus of non-English documents, using either
English natural language queries (e.g. cholera outbreaks in Iran) or structured
queries (e.g. find all events of type Disease-Outbreak with agent cholera and
location Iran).",None,-1
1f5a976c-e524-42bf-8c2e-ec754a7615cb,Uncertainty-Aware AB3DMOT by Variational 3D Object Detection,0.112084,"Autonomous driving needs to rely on high-quality 3D object detection to
ensure safe navigation in the world. Uncertainty estimation is an effective
tool to provide statistically accurate predictions, while the associated
detection uncertainty can be used to implement a more safe navigation protocol
or include the user in the loop. In this paper, we propose a Variational Neural
Network-based TANet 3D object detector to generate 3D object detections with
uncertainty and introduce these detections to an uncertainty-aware AB3DMOT
tracker. This is done by applying a linear transformation to the estimated
uncertainty matrix, which is subsequently used as a measurement noise for the
adopted Kalman filter. We implement two ways to estimate output uncertainty,
i.e., internally, by computing the variance of the CNN outputs and then
propagating the uncertainty through the post-processing, and externally, by
associating the final predictions of different samples and computing the
covariance of each predicted box. In experiments, we show that the external
uncertainty estimation leads to better results, outperforming both internal
uncertainty estimation and classical tracking approaches. Furthermore, we
propose a method to initialize the Variational 3D object detector with a
pretrained TANet model, which leads to the best performing models.",None,-1
2a097229-15d7-4a8b-ab63-261e4d3f44d5,WiCE: Real-World Entailment for Claims in Wikipedia,0.868313,"Textual entailment models are increasingly applied in settings like
fact-checking, presupposition verification in question answering, or summary
evaluation. However, these represent a significant domain shift from existing
entailment datasets, and models underperform as a result. We propose WiCE, a
new fine-grained textual entailment dataset built on natural claim and evidence
pairs extracted from Wikipedia. In addition to standard claim-level entailment,
WiCE provides entailment judgments over sub-sentence units of the claim, and a
minimal subset of evidence sentences that support each subclaim. To support
this, we propose an automatic claim decomposition strategy using GPT-3.5 which
we show is also effective at improving entailment models' performance on
multiple datasets at test time. Finally, we show that real claims in our
dataset involve challenging verification and retrieval problems that existing
models fail to address.",None,-1
d1092b3c-bd69-4383-bcbd-87493effe708,BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text,0.930517,"Real-world NLP applications often deal with nonstandard text (e.g.,
dialectal, informal, or misspelled text). However, language models like BERT
deteriorate in the face of dialect variation or noise. How do we push BERT's
modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it
is designed for specializing a model to a task and does not seem to bring about
the deeper, more pervasive changes needed to adapt a model to nonstandard
language. In this paper, we introduce the novel idea of sandwiching BERT's
encoder stack between additional encoder layers trained to perform masked
language modeling on noisy text. We find that our approach, paired with recent
work on including character-level noise in fine-tuning data, can promote
zero-shot transfer to dialectal text, as well as reduce the distance in the
embedding space between words and their noisy counterparts.",None,-1
b21da23c-f8ef-4b17-ac5c-47579f1e8eae,Improving Link Prediction in Social Networks Using Local and Global Features: A Clustering-based Approach,0.185646,"Link prediction problem has increasingly become prominent in many domains
such as social network analyses, bioinformatics experiments, transportation
networks, criminal investigations and so forth. A variety of techniques has
been developed for link prediction problem, categorized into 1) similarity
based approaches which study a set of features to extract similar nodes; 2)
learning based approaches which extract patterns from the input data; 3)
probabilistic statistical approaches which optimize a set of parameters to
establish a model which can best compute formation probability. However,
existing literatures lack approaches which utilize strength of each approach by
integrating them to achieve a much more productive one. To tackle the link
prediction problem, we propose an approach based on the combination of first
and second group methods; the existing studied works use just one of these
categories. Our two-phase developed method firstly determines new features
related to the position and dynamic behavior of nodes, which enforce the
approach more efficiency compared to approaches using mere measures. Then, a
subspace clustering algorithm is applied to group social objects based on the
computed similarity measures which differentiate the strength of clusters;
basically, the usage of local and global indices and the clustering information
plays an imperative role in our link prediction process. Some extensive
experiments held on real datasets including Facebook, Brightkite and HepTh
indicate good performances of our proposal method. Besides, we have
experimentally verified our approach with some previous techniques in the area
to prove the supremacy of ours.",None,-1
c3a233b8-6cdc-4f37-b318-7563381bdc9e,Probabilistic Generative Modeling for Procedural Roundabout Generation for Developing Countries,0.414849,"Due to limited resources and fast economic growth, designing optimal
transportation road networks with traffic simulation and validation in a
cost-effective manner is vital for developing countries, where extensive manual
testing is expensive and often infeasible. Current rule-based road design
generators lack diversity, a key feature for design robustness. Generative Flow
Networks (GFlowNets) learn stochastic policies to sample from an unnormalized
reward distribution, thus generating high-quality solutions while preserving
their diversity. In this work, we formulate the problem of linking incident
roads to the circular junction of a roundabout by a Markov decision process,
and we leverage GFlowNets as the Junction-Art road generator. We compare our
method with related methods and our empirical results show that our method
achieves better diversity while preserving a high validity score.",None,-1
0c969d78-af73-49c8-b13b-b581da52ab96,Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency,0.05428,"With growing capabilities of large language models, prompting them has become
the dominant way to access them. This has motivated the development of
strategies for automatically selecting effective language prompts. In this
paper, we introduce prompt flatness, a new metric to quantify the expected
utility of a language prompt. This metric is inspired by flatness
regularization in statistical learning that quantifies the robustness of the
model towards its parameter perturbations. We provide theoretical foundations
for this metric and its relationship with other prompt selection metrics,
providing a comprehensive understanding of existing methods. Empirically, we
show that combining prompt flatness with existing metrics improves both
performance and sample efficiency. Our metric outperforms the previous prompt
selection metrics with an average increase of 5% in accuracy and 10% in Pearson
correlation across 6 classification benchmarks.",None,-1
28d900e9-0a55-48b1-b46d-d89e29f56a2b,Delving into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling,0.382962,"Normalizing flows (NFs) provide a powerful tool to construct an expressive
distribution by a sequence of trackable transformations of a base distribution
and form a probabilistic model of underlying data. Rotation, as an important
quantity in computer vision, graphics, and robotics, can exhibit many
ambiguities when occlusion and symmetry occur and thus demands such
probabilistic models. Though much progress has been made for NFs in Euclidean
space, there are no effective normalizing flows without discontinuity or
many-to-one mapping tailored for SO(3) manifold. Given the unique non-Euclidean
properties of the rotation manifold, adapting the existing NFs to SO(3)
manifold is non-trivial. In this paper, we propose a novel normalizing flow on
SO(3) by combining a Mobius transformation-based coupling layer and a
quaternion affine transformation. With our proposed rotation normalizing flows,
one can not only effectively express arbitrary distributions on SO(3), but also
conditionally build the target distribution given input observations. Extensive
experiments show that our rotation normalizing flows significantly outperform
the baselines on both unconditional and conditional tasks.",None,-1
d522ebba-4d74-4cad-99e2-63c0f12b3cc7,Anatomy-Driven Pathology Detection on Chest X-rays,0.924691,"Pathology detection and delineation enables the automatic interpretation of
medical scans such as chest X-rays while providing a high level of
explainability to support radiologists in making informed decisions. However,
annotating pathology bounding boxes is a time-consuming task such that large
public datasets for this purpose are scarce. Current approaches thus use weakly
supervised object detection to learn the (rough) localization of pathologies
from image-level annotations, which is however limited in performance due to
the lack of bounding box supervision. We therefore propose anatomy-driven
pathology detection (ADPD), which uses easy-to-annotate bounding boxes of
anatomical regions as proxies for pathologies. We study two training
approaches: supervised training using anatomy-level pathology labels and
multiple instance learning (MIL) with image-level pathology labels. Our results
show that our anatomy-level training approach outperforms weakly supervised
methods and fully supervised detection with limited training samples, and our
MIL approach is competitive with both baseline approaches, therefore
demonstrating the potential of our approach.",None,-1
7cfc220a-df4f-4fd1-9e8a-785606028692,Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment,0.109551,"Human communication often involves information gaps between the
interlocutors. For example, in an educational dialogue, a student often
provides an answer that is incomplete, and there is a gap between this answer
and the perfect one expected by the teacher. Successful dialogue then hinges on
the teacher asking about this gap in an effective manner, thus creating a rich
and interactive educational experience. We focus on the problem of generating
such gap-focused questions (GFQs) automatically. We define the task, highlight
key desired aspects of a good GFQ, and propose a model that satisfies these.
Finally, we provide an evaluation by human annotators of our generated
questions compared against human generated ones, demonstrating competitive
performance.",None,-1
b73fa342-aae7-47c6-997f-8f731fb54ecf,In-Context Impersonation Reveals Large Language Models' Strengths and Biases,0.424797,"In everyday conversations, humans can take on different roles and adapt their
vocabulary to their chosen roles. We explore whether LLMs can take on, that is
impersonate, different roles when they generate text in-context. We ask LLMs to
assume different personas before solving vision and language tasks. We do this
by prefixing the prompt with a persona that is associated either with a social
identity or domain expertise. In a multi-armed bandit task, we find that LLMs
pretending to be children of different ages recover human-like developmental
stages of exploration. In a language-based reasoning task, we find that LLMs
impersonating domain experts perform better than LLMs impersonating non-domain
experts. Finally, we test whether LLMs' impersonations are complementary to
visual information when describing different categories. We find that
impersonation can improve performance: an LLM prompted to be a bird expert
describes birds better than one prompted to be a car expert. However,
impersonation can also uncover LLMs' biases: an LLM prompted to be a man
describes cars better than one prompted to be a woman. These findings
demonstrate that LLMs are capable of taking on diverse roles and that this
in-context impersonation can be used to uncover their hidden strengths and
biases.",None,-1
f60ec9f7-8cdb-4717-b8ad-c9371a23891a,Integrating Temporality and Causality into Acyclic Argumentation Frameworks using a Transition System,0.0241943,"In the context of abstract argumentation, we present the benefits of
considering temporality, i.e. the order in which arguments are enunciated, as
well as causality. We propose a formal method to rewrite the concepts of
acyclic abstract argumentation frameworks into an action language, that allows
us to model the evolution of the world, and to establish causal relationships
between the enunciation of arguments and their consequences, whether direct or
indirect. An Answer Set Programming implementation is also proposed, as well as
perspectives towards explanations.",None,-1
21be650d-f2f2-4282-9d95-faedbf1f1f8b,Car-Studio: Learning Car Radiance Fields from Single-View and Endless In-the-wild Images,0.0822696,"Compositional neural scene graph studies have shown that radiance fields can
be an efficient tool in an editable autonomous driving simulator. However,
previous studies learned within a sequence of autonomous driving datasets,
resulting in unsatisfactory blurring when rotating the car in the simulator. In
this letter, we propose a pipeline for learning unconstrained images and
building a dataset from processed images. To meet the requirements of the
simulator, which demands that the vehicle maintain clarity when the perspective
changes and that the contour remains sharp from the background to avoid
artifacts when editing, we design a radiation field of the vehicle, a crucial
part of the urban scene foreground. Through experiments, we demonstrate that
our model achieves competitive performance compared to baselines. Using the
datasets built from in-the-wild images, our method gradually presents a
controllable appearance editing function. We will release the dataset and code
on https://lty2226262.github.io/car-studio/ to facilitate further research in
the field.",None,-1
17ceb794-dcfc-4727-a0ee-6611cdc29420,Exploring Partial Knowledge Base Inference in Biomedical Entity Linking,0.437878,"Biomedical entity linking (EL) consists of named entity recognition (NER) and
named entity disambiguation (NED). EL models are trained on corpora labeled by
a predefined KB. However, it is a common scenario that only entities within a
subset of the KB are precious to stakeholders. We name this scenario partial
knowledge base inference: training an EL model with one KB and inferring on the
part of it without further training. In this work, we give a detailed
definition and evaluation procedures for this practically valuable but
significantly understudied scenario and evaluate methods from three
representative EL paradigms. We construct partial KB inference benchmarks and
witness a catastrophic degradation in EL performance due to dramatically
precision drop. Our findings reveal these EL paradigms can not correctly handle
unlinkable mentions (NIL), so they are not robust to partial KB inference. We
also propose two simple-and-effective redemption methods to combat the NIL
issue with little computational overhead. Codes are released at
https://github.com/Yuanhy1997/PartialKB-EL.",None,-1
daab5080-a3c4-46f3-afe1-06edf0904244,Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention,0.385735,"Large transformer models have achieved state-of-the-art results in numerous
natural language processing tasks. Among the pivotal components of the
transformer architecture, the attention mechanism plays a crucial role in
capturing token interactions within sequences through the utilization of
softmax function.
  Conversely, linear attention presents a more computationally efficient
alternative by approximating the softmax operation with linear complexity.
However, it exhibits substantial performance degradation when compared to the
traditional softmax attention mechanism.
  In this paper, we bridge the gap in our theoretical understanding of the
reasons behind the practical performance gap between softmax and linear
attention. By conducting a comprehensive comparative analysis of these two
attention mechanisms, we shed light on the underlying reasons for why softmax
attention outperforms linear attention in most scenarios.",None,-1
22a60b51-6859-4ee7-8e7a-27225a194758,AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks,0.0944174,"Sparse training is emerging as a promising avenue for reducing the
computational cost of training neural networks. Several recent studies have
proposed pruning methods using learnable thresholds to efficiently explore the
non-uniform distribution of sparsity inherent within the models. In this paper,
we propose Gradient Annealing (GA), where gradients of masked weights are
scaled down in a non-linear manner. GA provides an elegant trade-off between
sparsity and accuracy without the need for additional sparsity-inducing
regularization. We integrated GA with the latest learnable pruning methods to
create an automated sparse training algorithm called AutoSparse, which achieves
better accuracy and/or training/inference FLOPS reduction than existing
learnable pruning methods for sparse ResNet50 and MobileNetV1 on ImageNet-1K:
AutoSparse achieves (2x, 7x) reduction in (training,inference) FLOPS for
ResNet50 on ImageNet at 80% sparsity. Finally, AutoSparse outperforms
sparse-to-sparse SotA method MEST (uniform sparsity) for 80% sparse ResNet50
with similar accuracy, where MEST uses 12% more training FLOPS and 50% more
inference FLOPS.",None,-1
1957652e-96fd-4d5d-8736-73533fe060e0,Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning,0.310007,"Recent works on instruction tuning (IT) have achieved great performance with
zero-shot generalizability to unseen tasks. With additional context (e.g., task
definition, examples) provided to models for fine-tuning, they achieved much
higher performance than untuned models. Despite impressive performance gains,
what models learn from IT remains understudied. In this work, we analyze how
models utilize instructions during IT by comparing model training with altered
vs. original instructions. Specifically, we create simplified task definitions
by removing all semantic components and only leaving the output space
information, and delusive examples that contain incorrect input-output mapping.
Our experiments show that models trained on simplified task definition or
delusive examples can achieve comparable performance to the ones trained on the
original instructions and examples. Furthermore, we introduce a random baseline
to perform zeroshot classification tasks, and find it achieves similar
performance (42.6% exact-match) as IT does (43% exact-match) in low resource
setting, while both methods outperform naive T5 significantly (30% per
exact-match). Our analysis provides evidence that the impressive performance
gain of current IT models can come from picking up superficial patterns, such
as learning the output format and guessing. Our study highlights the urgent
need for more reliable IT methods and evaluation.",None,-1
436027b0-0332-469a-a3a0-adfceed4853f,Is More Always Better? The Effects of Personal Characteristics and Level of Detail on the Perception of Explanations in a Recommender System,0.879326,"Despite the acknowledgment that the perception of explanations may vary
considerably between end-users, explainable recommender systems (RS) have
traditionally followed a one-size-fits-all model, whereby the same explanation
level of detail is provided to each user, without taking into consideration
individual user's context, i.e., goals and personal characteristics. To fill
this research gap, we aim in this paper at a shift from a one-size-fits-all to
a personalized approach to explainable recommendation by giving users agency in
deciding which explanation they would like to see. We developed a transparent
Recommendation and Interest Modeling Application (RIMA) that provides on-demand
personalized explanations of the recommendations, with three levels of detail
(basic, intermediate, advanced) to meet the demands of different types of
end-users. We conducted a within-subject study (N=31) to investigate the
relationship between user's personal characteristics and the explanation level
of detail, and the effects of these two variables on the perception of the
explainable RS with regard to different explanation goals. Our results show
that the perception of explainable RS with different levels of detail is
affected to different degrees by the explanation goal and user type.
Consequently, we suggested some theoretical and design guidelines to support
the systematic design of explanatory interfaces in RS tailored to the user's
context.",None,-1
96275ac6-3164-417e-b587-f4af4d2fae5f,Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation,0.410868,"Few-shot question answering (QA) aims at precisely discovering answers to a
set of questions from context passages while only a few training samples are
available. Although existing studies have made some progress and can usually
achieve proper results, they suffer from understanding deep semantics for
reasoning out the questions. In this paper, we develop Gotta, a Generative
prOmpT-based daTa Augmentation framework to mitigate the challenge above.
Inspired by the human reasoning process, we propose to integrate the cloze task
to enhance few-shot QA learning. Following the recent success of prompt-tuning,
we present the cloze task in the same format as the main QA task, allowing the
model to learn both tasks seamlessly together to fully take advantage of the
power of prompt-tuning. Extensive experiments on widely used benchmarks
demonstrate that Gotta consistently outperforms competitive baselines,
validating the effectiveness of our proposed prompt-tuning-based cloze task,
which not only fine-tunes language models but also learns to guide reasoning in
QA tasks. Further analysis shows that the prompt-based loss incorporates the
auxiliary task better than the multi-task loss, highlighting the strength of
prompt-tuning on the few-shot QA task.",None,-1
3a8ad368-2bb2-4bd2-9f8d-4632f2d9b635,Vision Transformer Adapters for Generalizable Multitask Learning,0.245987,"We introduce the first multitasking vision transformer adapters that learn
generalizable task affinities which can be applied to novel tasks and domains.
Integrated into an off-the-shelf vision transformer backbone, our adapters can
simultaneously solve multiple dense vision tasks in a parameter-efficient
manner, unlike existing multitasking transformers that are parametrically
expensive. In contrast to concurrent methods, we do not require retraining or
fine-tuning whenever a new task or domain is added. We introduce a task-adapted
attention mechanism within our adapter framework that combines gradient-based
task similarities with attention-based ones. The learned task affinities
generalize to the following settings: zero-shot task transfer, unsupervised
domain adaptation, and generalization without fine-tuning to novel domains. We
demonstrate that our approach outperforms not only the existing convolutional
neural network-based multitasking methods but also the vision transformer-based
ones. Our project page is at \url{https://ivrl.github.io/VTAGML}.",None,-1
24b8d3bb-a8ff-483b-a458-d864020b8438,Identification of Novel Classes for Improving Few-Shot Object Detection,0.504927,"Conventional training of deep neural networks requires a large number of the
annotated image which is a laborious and time-consuming task, particularly for
rare objects. Few-shot object detection (FSOD) methods offer a remedy by
realizing robust object detection using only a few training samples per class.
An unexplored challenge for FSOD is that instances from unlabeled novel classes
that do not belong to the fixed set of training classes appear in the
background. These objects behave similarly to label noise, leading to FSOD
performance degradation. We develop a semi-supervised algorithm to detect and
then utilize these unlabeled novel objects as positive samples during training
to improve FSOD performance. Specifically, we propose a hierarchical ternary
classification region proposal network (HTRPN) to localize the potential
unlabeled novel objects and assign them new objectness labels. Our improved
hierarchical sampling strategy for the region proposal network (RPN) also
boosts the perception ability of the object detection model for large objects.
Our experimental results indicate that our method is effective and outperforms
the existing state-of-the-art (SOTA) FSOD methods.",None,-1
17882a95-3b11-4b10-be08-4d610c8df7e5,Where are We in Event-centric Emotion Analysis? Bridging Emotion Role Labeling and Appraisal-based Approaches,0.751249,"The term emotion analysis in text subsumes various natural language
processing tasks which have in common the goal to enable computers to
understand emotions. Most popular is emotion classification in which one or
multiple emotions are assigned to a predefined textual unit. While such setting
is appropriate for identifying the reader's or author's emotion, emotion role
labeling adds the perspective of mentioned entities and extracts text spans
that correspond to the emotion cause. The underlying emotion theories agree on
one important point; that an emotion is caused by some internal or external
event and comprises several subcomponents, including the subjective feeling and
a cognitive evaluation. We therefore argue that emotions and events are related
in two ways. (1) Emotions are events; and this perspective is the fundament in
natural language processing for emotion role labeling. (2) Emotions are caused
by events; a perspective that is made explicit with research how to incorporate
psychological appraisal theories in NLP models to interpret events. These two
research directions, role labeling and (event-focused) emotion classification,
have by and large been tackled separately. In this paper, we contextualize both
perspectives and discuss open research questions.",None,-1
419292c6-b9b6-4c30-a730-a7ca61f18f2e,Tools for Landscape Analysis of Optimisation Problems in Procedural Content Generation for Games,0.122696,"The term Procedural Content Generation (PCG) refers to the (semi-)automatic
generation of game content by algorithmic means, and its methods are becoming
increasingly popular in game-oriented research and industry. A special class of
these methods, which is commonly known as search-based PCG, treats the given
task as an optimisation problem. Such problems are predominantly tackled by
evolutionary algorithms.
  We will demonstrate in this paper that obtaining more information about the
defined optimisation problem can substantially improve our understanding of how
to approach the generation of content. To do so, we present and discuss three
efficient analysis tools, namely diagonal walks, the estimation of high-level
properties, as well as problem similarity measures. We discuss the purpose of
each of the considered methods in the context of PCG and provide guidelines for
the interpretation of the results received. This way we aim to provide methods
for the comparison of PCG approaches and eventually, increase the quality and
practicality of generated content in industry.",None,-1
c664c994-cd7b-46d2-a966-079f4121c793,Weighted First Order Model Counting with Directed Acyclic Graph Axioms,0.142891,"Statistical Relational Learning (SRL) integrates First-Order Logic (FOL) and
probability theory for learning and inference over relational data.
Probabilistic inference and learning in many SRL models can be reduced to
Weighted First Order Model Counting (WFOMC). However, WFOMC is known to be
intractable ($\mathrm{\#P_1-}$ complete). Hence, logical fragments that admit
polynomial time WFOMC are of significant interest. Such fragments are called
domain liftable. Recent line of works have shown the two-variable fragment of
FOL, extended with counting quantifiers ($\mathrm{C^2}$) to be domain-liftable.
However, many properties of real-world data can not be modelled in
$\mathrm{C^2}$. In fact many ubiquitous properties of real-world data are
inexressible in FOL. Acyclicity is one such property, found in citation
networks, genealogy data, temporal data e.t.c. In this paper we aim to address
this problem by investigating the domain liftability of directed acyclicity
constraints. We show that the fragment $\mathrm{C^2}$ with a Directed Acyclic
Graph (DAG) axiom, i.e., a predicate in the language is axiomatized to
represent a DAG, is domain-liftable. We present a method based on principle of
inclusion-exclusion for WFOMC of $\mathrm{C^2}$ formulas extended with DAG
axioms.",None,-1
ff2b3268-777d-4c93-985d-4adaa9bf9ad6,TACO: Topics in Algorithmic COde generation dataset,0.291125,"We introduce TACO, an open-source, large-scale code generation dataset, with
a focus on the optics of algorithms, designed to provide a more challenging
training dataset and evaluation benchmark in the field of code generation
models. TACO includes competition-level programming questions that are more
challenging, to enhance or evaluate problem understanding and reasoning
abilities in real-world programming scenarios. There are 25433 and 1000 coding
problems in training and test set, as well as up to 1.55 million diverse
solution answers. Moreover, each TACO problem includes several fine-grained
labels such as task topics, algorithms, programming skills, and difficulty
levels, providing a more precise reference for the training and evaluation of
code generation models. The dataset and evaluation scripts are available on
Hugging Face Hub (https://huggingface.co/datasets/BAAI/TACO) and Github
(https://github.com/FlagOpen/TACO).",None,-1
43674f7c-99b4-4851-a834-a99275c8933c,STNet: Spatial and Temporal feature fusion network for change detection in remote sensing images,0.360282,"As an important task in remote sensing image analysis, remote sensing change
detection (RSCD) aims to identify changes of interest in a region from
spatially co-registered multi-temporal remote sensing images, so as to monitor
the local development. Existing RSCD methods usually formulate RSCD as a binary
classification task, representing changes of interest by merely feature
concatenation or feature subtraction and recovering the spatial details via
densely connected change representations, whose performances need further
improvement. In this paper, we propose STNet, a RSCD network based on spatial
and temporal feature fusions. Specifically, we design a temporal feature fusion
(TFF) module to combine bi-temporal features using a cross-temporal gating
mechanism for emphasizing changes of interest; a spatial feature fusion module
is deployed to capture fine-grained information using a cross-scale attention
mechanism for recovering the spatial details of change representations.
Experimental results on three benchmark datasets for RSCD demonstrate that the
proposed method achieves the state-of-the-art performance. Code is available at
https://github.com/xwmaxwma/rschange.",None,-1
627d24a3-f017-4c2a-bccf-f5355171587e,Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation,0.977306,"Generative AI and large language models hold great promise in enhancing
programming education by automatically generating individualized feedback for
students. We investigate the role of generative AI models in providing human
tutor-style programming hints to help students resolve errors in their buggy
programs. Recent works have benchmarked state-of-the-art models for various
feedback generation scenarios; however, their overall quality is still inferior
to human tutors and not yet ready for real-world deployment. In this paper, we
seek to push the limits of generative AI models toward providing high-quality
programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a
first step, our technique leverages GPT-4 as a ``tutor'' model to generate
hints -- it boosts the generative quality by using symbolic information of
failing test cases and fixes in prompts. As a next step, our technique
leverages GPT-3.5, a weaker model, as a ``student'' model to further validate
the hint quality -- it performs an automatic quality validation by simulating
the potential utility of providing this feedback. We show the efficacy of our
technique via extensive evaluation using three real-world datasets of Python
programs covering a variety of concepts ranging from basic algorithms to
regular expressions and data analysis using pandas library.",None,-1
dea81070-bfeb-4ce7-87f1-79d794a51c15,Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health,0.949664,"Pretrained language models have been used in various natural language
processing applications. In the mental health domain, domain-specific language
models are pretrained and released, which facilitates the early detection of
mental health conditions. Social posts, e.g., on Reddit, are usually long
documents. However, there are no domain-specific pretrained models for
long-sequence modeling in the mental health domain. This paper conducts
domain-specific continued pretraining to capture the long context for mental
health. Specifically, we train and release MentalXLNet and MentalLongformer
based on XLNet and Longformer. We evaluate the mental health classification
performance and the long-range ability of these two domain-specific pretrained
models. Our models are released in HuggingFace.",None,-1
72d5da8a-98fe-4660-8d62-9f990e8ece19,MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction,0.218383,"Existing methods proposed for hand reconstruction tasks usually parameterize
a generic 3D hand model or predict hand mesh positions directly. The parametric
representations consisting of hand shapes and rotational poses are more stable,
while the non-parametric methods can predict more accurate mesh positions. In
this paper, we propose to reconstruct meshes and estimate MANO parameters of
two hands from a single RGB image simultaneously to utilize the merits of two
kinds of hand representations. To fulfill this target, we propose novel
Mesh-Mano interaction blocks (MMIBs), which take mesh vertices positions and
MANO parameters as two kinds of query tokens. MMIB consists of one graph
residual block to aggregate local information and two transformer encoders to
model long-range dependencies. The transformer encoders are equipped with
different asymmetric attention masks to model the intra-hand and inter-hand
attention, respectively. Moreover, we introduce the mesh alignment refinement
module to further enhance the mesh-image alignment. Extensive experiments on
the InterHand2.6M benchmark demonstrate promising results over the
state-of-the-art hand reconstruction methods.",None,-1
5e158563-8e6b-4626-907f-a8bb2cfbd0e8,Chebyshev Particles,0.515897,"Markov chain Monte Carlo (MCMC) provides a feasible method for inferring
Hidden Markov models, however, it is often computationally prohibitive,
especially constrained by the curse of dimensionality, as the Monte Carlo
sampler traverses randomly taking small steps within uncertain regions in the
parameter space. We are the first to consider the posterior distribution of the
objective as a mapping of samples in an infinite-dimensional Euclidean space
where deterministic submanifolds are embedded and propose a new criterion by
maximizing the weighted Riesz polarization quantity, to discretize rectifiable
submanifolds via pairwise interaction. We study the characteristics of
Chebyshev particles and embed them into sequential MCMC, a novel sampler with a
high acceptance ratio that proposes only a few evaluations. We have achieved
high performance from the experiments for parameter inference in a linear
Gaussian state-space model with synthetic data and a non-linear stochastic
volatility model with real-world data.",None,-1
8d80204c-26a8-42f5-bae0-534d842ee4be,DomainDrop: Suppressing Domain-Sensitive Channels for Domain Generalization,0.476516,"Deep Neural Networks have exhibited considerable success in various visual
tasks. However, when applied to unseen test datasets, state-of-the-art models
often suffer performance degradation due to domain shifts. In this paper, we
introduce a novel approach for domain generalization from a novel perspective
of enhancing the robustness of channels in feature maps to domain shifts. We
observe that models trained on source domains contain a substantial number of
channels that exhibit unstable activations across different domains, which are
inclined to capture domain-specific features and behave abnormally when exposed
to unseen target domains. To address the issue, we propose a DomainDrop
framework to continuously enhance the channel robustness to domain shifts,
where a domain discriminator is used to identify and drop unstable channels in
feature maps of each network layer during forward propagation. We theoretically
prove that our framework could effectively lower the generalization bound.
Extensive experiments on several benchmarks indicate that our framework
achieves state-of-the-art performance compared to other competing methods. Our
code is available at https://github.com/lingeringlight/DomainDrop.",None,-1
423e6777-4065-4c1a-be4c-b4ccbb442b39,Unsupervised Speech Recognition with N-Skipgram and Positional Unigram Matching,0.210983,"Training unsupervised speech recognition systems presents challenges due to
GAN-associated instability, misalignment between speech and text, and
significant memory demands. To tackle these challenges, we introduce a novel
ASR system, ESPUM. This system harnesses the power of lower-order N-skipgrams
(up to N=3) combined with positional unigram statistics gathered from a small
batch of samples. Evaluated on the TIMIT benchmark, our model showcases
competitive performance in ASR and phoneme segmentation tasks. Access our
publicly available code at https://github.com/lwang114/GraphUnsupASR.",None,-1
0d460f42-2492-43b7-8c68-dcd9c920a054,Human Preference Score: Better Aligning Text-to-Image Models with Human Preference,0.521627,"Recent years have witnessed a rapid growth of deep generative models, with
text-to-image models gaining significant attention from the public. However,
existing models often generate images that do not align well with human
preferences, such as awkward combinations of limbs and facial expressions. To
address this issue, we collect a dataset of human choices on generated images
from the Stable Foundation Discord channel. Our experiments demonstrate that
current evaluation metrics for generative models do not correlate well with
human choices. Thus, we train a human preference classifier with the collected
dataset and derive a Human Preference Score (HPS) based on the classifier.
Using HPS, we propose a simple yet effective method to adapt Stable Diffusion
to better align with human preferences. Our experiments show that HPS
outperforms CLIP in predicting human choices and has good generalization
capability toward images generated from other models. By tuning Stable
Diffusion with the guidance of HPS, the adapted model is able to generate
images that are more preferred by human users. The project page is available
here: https://tgxs002.github.io/align_sd_web/ .",None,-1
21e54c28-af36-4b2c-a175-0e6fbf7034ed,ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus,0.831193,"We introduce \`{I}r\`{o}y\`{i}nSpeech, a new corpus influenced by the desire
to increase the amount of high quality, contemporary Yor\`{u}b\'{a} speech
data, which can be used for both Text-to-Speech (TTS) and Automatic Speech
Recognition (ASR) tasks. We curated about 23000 text sentences from news and
creative writing domains with the open license CC-BY-4.0. To encourage a
participatory approach to data creation, we provide 5000 curated sentences to
the Mozilla Common Voice platform to crowd-source the recording and validation
of Yor\`{u}b\'{a} speech data. In total, we created about 42 hours of speech
data recorded by 80 volunteers in-house, and 6 hours of validated recordings on
Mozilla Common Voice platform. Our TTS evaluation suggests that a
high-fidelity, general domain, single-speaker Yor\`{u}b\'{a} voice is possible
with as little as 5 hours of speech. Similarly, for ASR we obtained a baseline
word error rate (WER) of 23.8.",None,-1
4aa1ea26-a43c-42c0-8188-c3c0724f4721,Development and Whole-Body Validation of Personalizable Female and Male Pedestrian SAFER Human Body Models,0.218601,"Vulnerable road users are overrepresented in the worldwide number of
road-traffic injury victims. Developing biofidelic male and female pedestrian
HBMs representing a range of anthropometries is imperative to follow through
with the efforts to increase road safety and propose intervention strategies.
In this study, a 50th percentile male and female pedestrian of the SAFER HBM
was developed via a newly developed image registration-based mesh morphing
framework for subject personalization. The HBM and its accompanied
personalization framework were evaluated by means of a set of cadaver
experiments, where subjects were struck laterally by a generic sedan buck. In
the simulated whole-body pedestrian collisions, the personalized HBMs
demonstrate a good capability of reproducing the trajectories and head
kinematics observed in lateral impacts. The presented pedestrian HBMs and
personalization framework provide robust means to thoroughly and accurately
reconstruct and evaluate pedestrian-to-vehicle collisions.",None,-1
d3f043a4-9d3e-4e4d-b847-b5ffe3875aee,Overcoming Generic Knowledge Loss with Selective Parameter Update,0.170661,"Foundation models encompass an extensive knowledge base and offer remarkable
transferability. However, this knowledge becomes outdated or insufficient over
time. The challenge lies in continuously updating foundation models to
accommodate novel information while retaining their original capabilities.
Leveraging the fact that foundation models have initial knowledge on various
tasks and domains, we propose a novel approach that, instead of updating all
parameters equally, localizes the updates to a sparse set of parameters
relevant to the task being learned. We strike a balance between efficiency and
new task performance, while maintaining the transferability and
generalizability of foundation models. We extensively evaluate our method on
foundational vision-language models with a diverse spectrum of continual
learning tasks. Our method achieves improvements on the accuracy of the newly
learned tasks up to 7% while preserving the pretraining knowledge with a
negligible decrease of 0.9% on a representative control set accuracy.",None,-1
f6544405-280e-4c0f-b9d0-a37178213e15,Laplacian ICP for Progressive Registration of 3D Human Head Meshes,0.399427,"We present a progressive 3D registration framework that is a highly-efficient
variant of classical non-rigid Iterative Closest Points (N-ICP). Since it uses
the Laplace-Beltrami operator for deformation regularisation, we view the
overall process as Laplacian ICP (L-ICP). This exploits a `small deformation
per iteration' assumption and is progressively coarse-to-fine, employing an
increasingly flexible deformation model, an increasing number of correspondence
sets, and increasingly sophisticated correspondence estimation. Correspondence
matching is only permitted within predefined vertex subsets derived from
domain-specific feature extractors. Additionally, we present a new benchmark
and a pair of evaluation metrics for 3D non-rigid registration, based on
annotation transfer. We use this to evaluate our framework on a
publicly-available dataset of 3D human head scans (Headspace). The method is
robust and only requires a small fraction of the computation time compared to
the most popular classical approach, yet has comparable registration
performance.",None,-1
73dc9175-14cd-43a0-91a7-5063264a9e7c,Open-Set Face Identification on Few-Shot Gallery by Fine-Tuning,0.177517,"In this paper, we focus on addressing the open-set face identification
problem on a few-shot gallery by fine-tuning. The problem assumes a realistic
scenario for face identification, where only a small number of face images is
given for enrollment and any unknown identity must be rejected during
identification. We observe that face recognition models pretrained on a large
dataset and naively fine-tuned models perform poorly for this task. Motivated
by this issue, we propose an effective fine-tuning scheme with classifier
weight imprinting and exclusive BatchNorm layer tuning. For further improvement
of rejection accuracy on unknown identities, we propose a novel matcher called
Neighborhood Aware Cosine (NAC) that computes similarity based on neighborhood
information. We validate the effectiveness of the proposed schemes thoroughly
on large-scale face benchmarks across different convolutional neural network
architectures. The source code for this project is available at:
https://github.com/1ho0jin1/OSFI-by-FineTuning",None,-1
dce5e920-b46f-4f03-bdd9-851eaf70ffb7,Emergence and Function of Abstract Representations in Self-Supervised Transformers,0.214725,"Human intelligence relies in part on our brains' ability to create abstract
mental models that succinctly capture the hidden blueprint of our reality. Such
abstract world models notably allow us to rapidly navigate novel situations by
generalizing prior knowledge, a trait deep learning systems have historically
struggled to replicate. However, the recent shift from supervised to
self-supervised objectives, combined with expressive transformer-based
architectures, have yielded powerful foundation models that appear to learn
versatile representations that can support a wide range of downstream tasks.
This promising development raises the intriguing possibility of such models
developing in silico abstract world models. We test this hypothesis by studying
the inner workings of small-scale transformers trained to reconstruct partially
masked visual scenes generated from a simple blueprint. We show that the
network develops intermediate abstract representations, or abstractions, that
encode all semantic features of the dataset. These abstractions manifest as
low-dimensional manifolds where the embeddings of semantically related tokens
transiently converge, thus allowing for the generalization of downstream
computations. Using precise manipulation experiments, we demonstrate that
abstractions are central to the network's decision-making process. Our research
also suggests that these abstractions are compositionally structured,
exhibiting features like contextual independence and part-whole relationships
that mirror the compositional nature of the dataset. Finally, we introduce a
Language-Enhanced Architecture (LEA) designed to encourage the network to
articulate its computations. We find that LEA develops an abstraction-centric
language that can be easily interpreted, allowing us to more readily access and
steer the network's decision-making process.",None,-1
bfc0f17c-6ae8-4c05-903f-9c0112caf569,Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes,0.0875915,"Natural Language Processing (NLP) methods have been broadly applied to
clinical tasks. Machine learning and deep learning approaches have been used to
improve the performance of clinical NLP. However, these approaches require
sufficiently large datasets for training, and trained models have been shown to
transfer poorly across sites. These issues have led to the promotion of data
collection and integration across different institutions for accurate and
portable models. However, this can introduce a form of bias called confounding
by provenance. When source-specific data distributions differ at deployment,
this may harm model performance. To address this issue, we evaluate the utility
of backdoor adjustment for text classification in a multi-site dataset of
clinical notes annotated for mentions of substance abuse. Using an evaluation
framework devised to measure robustness to distributional shifts, we assess the
utility of backdoor adjustment. Our results indicate that backdoor adjustment
can effectively mitigate for confounding shift.",None,-1
5b5b5f6e-2d7e-4c9c-8a08-19e646d94bac,Poisoning Language Models During Instruction Tuning,0.791408,"Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on
datasets that contain user-submitted examples, e.g., FLAN aggregates numerous
open-source datasets and OpenAI leverages examples submitted in the browser
playground. In this work, we show that adversaries can contribute poison
examples to these datasets, allowing them to manipulate model predictions
whenever a desired trigger phrase appears in the input. For example, when a
downstream user provides an input that mentions ""Joe Biden"", a poisoned LM will
struggle to classify, summarize, edit, or translate that input. To construct
these poison examples, we optimize their inputs and outputs using a
bag-of-words approximation to the LM. We evaluate our method on open-source
instruction-tuned LMs. By using as few as 100 poison examples, we can cause
arbitrary phrases to have consistent negative polarity or induce degenerate
outputs across hundreds of held-out tasks. Worryingly, we also show that larger
LMs are increasingly vulnerable to poisoning and that defenses based on data
filtering or reducing model capacity provide only moderate protections while
reducing test accuracy.",None,-1
0976db13-164d-4cca-8097-33fbb67590f5,"Wireless Channel Charting: Theory, Practice, and Applications",0.623191,"Channel charting is a recently proposed framework that applies dimensionality
reduction to channel state information (CSI) in wireless systems with the goal
of associating a pseudo-position to each mobile user in a low-dimensional
space: the channel chart. Channel charting summarizes the entire CSI dataset in
a self-supervised manner, which opens up a range of applications that are tied
to user location. In this article, we introduce the theoretical underpinnings
of channel charting and present an overview of recent algorithmic developments
and experimental results obtained in the field. We furthermore discuss concrete
application examples of channel charting to network- and user-related
applications, and we provide a perspective on future developments and
challenges as well as the role of channel charting in next-generation wireless
networks.",None,-1
1c634899-5f95-4685-8cc2-763842d34053,Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models,0.0318173,"The wide applicability and adaptability of generative large language models
(LLMs) has enabled their rapid adoption. While the pre-trained models can
perform many tasks, such models are often fine-tuned to improve their
performance on various downstream applications. However, this leads to issues
over violation of model licenses, model theft, and copyright infringement.
Moreover, recent advances show that generative technology is capable of
producing harmful content which exacerbates the problems of accountability
within model supply chains. Thus, we need a method to investigate how a model
was trained or a piece of text was generated and what their pre-trained base
model was. In this paper we take the first step to address this open problem by
tracing back the origin of a given fine-tuned LLM to its corresponding
pre-trained base model. We consider different knowledge levels and attribution
strategies, and find that we can correctly trace back 8 out of the 10 fine
tuned models with our best method.",None,-1
df6f9b6e-72d8-412d-9617-f982d3262cc8,VISION Datasets: A Benchmark for Vision-based InduStrial InspectiON,0.957586,"Despite progress in vision-based inspection algorithms, real-world industrial
challenges -- specifically in data availability, quality, and complex
production requirements -- often remain under-addressed. We introduce the
VISION Datasets, a diverse collection of 14 industrial inspection datasets,
uniquely poised to meet these challenges. Unlike previous datasets, VISION
brings versatility to defect detection, offering annotation masks across all
splits and catering to various detection methodologies. Our datasets also
feature instance-segmentation annotation, enabling precise defect
identification. With a total of 18k images encompassing 44 defect types, VISION
strives to mirror a wide range of real-world production scenarios. By
supporting two ongoing challenge competitions on the VISION Datasets, we hope
to foster further advancements in vision-based industrial inspection.",None,-1
9d73ccac-2f48-46f0-b9d9-e12bae95eb3d,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,0.768886,"Recently, deep learning models have shown the potential to predict breast
cancer risk and enable targeted screening strategies, but current models do not
consider the change in the breast over time. In this paper, we present a new
method, PRIME+, for breast cancer risk prediction that leverages prior
mammograms using a transformer decoder, outperforming a state-of-the-art risk
prediction method that only uses mammograms from a single time point. We
validate our approach on a dataset with 16,113 exams and further demonstrate
that it effectively captures patterns of changes from prior mammograms, such as
changes in breast density, resulting in improved short-term and long-term
breast cancer risk prediction. Experimental results show that our model
achieves a statistically significant improvement in performance over the
state-of-the-art based model, with a C-index increase from 0.68 to 0.73 (p <
0.05) on held-out test sets.",None,-1
e3c4e4be-0280-4bbc-a727-bde8176de5dd,DomainAdaptor: A Novel Approach to Test-time Adaptation,0.661406,"To deal with the domain shift between training and test samples, current
methods have primarily focused on learning generalizable features during
training and ignore the specificity of unseen samples that are also critical
during the test. In this paper, we investigate a more challenging task that
aims to adapt a trained CNN model to unseen domains during the test. To
maximumly mine the information in the test data, we propose a unified method
called DomainAdaptor for the test-time adaptation, which consists of an
AdaMixBN module and a Generalized Entropy Minimization (GEM) loss.
Specifically, AdaMixBN addresses the domain shift by adaptively fusing training
and test statistics in the normalization layer via a dynamic mixture
coefficient and a statistic transformation operation. To further enhance the
adaptation ability of AdaMixBN, we design a GEM loss that extends the Entropy
Minimization loss to better exploit the information in the test data. Extensive
experiments show that DomainAdaptor consistently outperforms the
state-of-the-art methods on four benchmarks. Furthermore, our method brings
more remarkable improvement against existing methods on the few-data unseen
domain. The code is available at https://github.com/koncle/DomainAdaptor.",None,-1
5ff73c35-7f94-41a8-be12-74e6000157be,Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data,0.76178,"Large language models (LLMs) provide a new way to build chatbots by accepting
natural language prompts. Yet, it is unclear how to design prompts to power
chatbots to carry on naturalistic conversations while pursuing a given goal,
such as collecting self-report data from users. We explore what design factors
of prompts can help steer chatbots to talk naturally and collect data reliably.
To this aim, we formulated four prompt designs with different structures and
personas. Through an online study (N = 48) where participants conversed with
chatbots driven by different designs of prompts, we assessed how prompt designs
and conversation topics affected the conversation flows and users' perceptions
of chatbots. Our chatbots covered 79% of the desired information slots during
conversations, and the designs of prompts and topics significantly influenced
the conversation flows and the data collection performance. We discuss the
opportunities and challenges of building chatbots with LLMs.",None,-1
669ae066-7aa6-4e58-93ce-9ee57d6f9ac3,Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm,0.283653,"Given the large-scale data and the high annotation cost,
pretraining-finetuning becomes a popular paradigm in multiple computer vision
tasks. Previous research has covered both the unsupervised pretraining and
supervised finetuning in this paradigm, while little attention is paid to
exploiting the annotation budget for finetuning. To fill in this gap, we
formally define this new active finetuning task focusing on the selection of
samples for annotation in the pretraining-finetuning paradigm. We propose a
novel method called ActiveFT for active finetuning task to select a subset of
data distributing similarly with the entire unlabeled pool and maintaining
enough diversity by optimizing a parametric model in the continuous space. We
prove that the Earth Mover's distance between the distributions of the selected
subset and the entire data pool is also reduced in this process. Extensive
experiments show the leading performance and high efficiency of ActiveFT
superior to baselines on both image classification and semantic segmentation.
Our code is released at https://github.com/yichen928/ActiveFT.",None,-1
189c8bdb-a60b-4ec3-9d3f-b72e20cd8f64,Evaluation of OpenAI Codex for HPC Parallel Programming Models Kernel Generation,0.993216,"We evaluate AI-assisted generative capabilities on fundamental numerical
kernels in high-performance computing (HPC), including AXPY, GEMV, GEMM, SpMV,
Jacobi Stencil, and CG. We test the generated kernel codes for a variety of
language-supported programming models, including (1) C++ (e.g., OpenMP
[including offload], OpenACC, Kokkos, SyCL, CUDA, and HIP), (2) Fortran (e.g.,
OpenMP [including offload] and OpenACC), (3) Python (e.g., numba, Numba, cuPy,
and pyCUDA), and (4) Julia (e.g., Threads, CUDA.jl, AMDGPU.jl, and
KernelAbstractions.jl). We use the GitHub Copilot capabilities powered by
OpenAI Codex available in Visual Studio Code as of April 2023 to generate a
vast amount of implementations given simple <kernel> + <programming model> +
<optional hints> prompt variants. To quantify and compare the results, we
propose a proficiency metric around the initial 10 suggestions given for each
prompt. Results suggest that the OpenAI Codex outputs for C++ correlate with
the adoption and maturity of programming models. For example, OpenMP and CUDA
score really high, whereas HIP is still lacking. We found that prompts from
either a targeted language such as Fortran or the more general-purpose Python
can benefit from adding code keywords, while Julia prompts perform acceptably
well for its mature programming models (e.g., Threads and CUDA.jl). We expect
for these benchmarks to provide a point of reference for each programming
model's community. Overall, understanding the convergence of large language
models, AI, and HPC is crucial due to its rapidly evolving nature and how it is
redefining human-computer interactions.",None,-1
933fb5c7-54de-432d-8c00-5bfdb98273e1,PUPS: Point Cloud Unified Panoptic Segmentation,0.805893,"Point cloud panoptic segmentation is a challenging task that seeks a holistic
solution for both semantic and instance segmentation to predict groupings of
coherent points. Previous approaches treat semantic and instance segmentation
as surrogate tasks, and they either use clustering methods or bounding boxes to
gather instance groupings with costly computation and hand-crafted designs in
the instance segmentation task. In this paper, we propose a simple but
effective point cloud unified panoptic segmentation (PUPS) framework, which use
a set of point-level classifiers to directly predict semantic and instance
groupings in an end-to-end manner. To realize PUPS, we introduce bipartite
matching to our training pipeline so that our classifiers are able to
exclusively predict groupings of instances, getting rid of hand-crafted
designs, e.g. anchors and Non-Maximum Suppression (NMS). In order to achieve
better grouping results, we utilize a transformer decoder to iteratively refine
the point classifiers and develop a context-aware CutMix augmentation to
overcome the class imbalance problem. As a result, PUPS achieves 1st place on
the leader board of SemanticKITTI panoptic segmentation task and
state-of-the-art results on nuScenes.",None,-1
dfc216fa-6459-457d-910d-7831cdad68cd,Everyone Deserves A Reward: Learning Customized Human Preferences,0.750768,"Reward models (RMs) are essential for aligning large language models (LLMs)
with human preferences to improve interaction quality. However, the real world
is pluralistic, which leads to diversified human preferences with respect to
different religions, politics, cultures, etc. Moreover, each individual can
have their unique preferences on various topics. Neglecting the diversity of
human preferences, current human feedback aligning methods only consider a
general reward model, which is below satisfaction for customized or
personalized application scenarios. To explore customized preference learning,
we collect a domain-specific preference (DSP) dataset, which includes preferred
responses for each given query from four practical domains. Besides, from the
perspective of data efficiency, we propose a three-stage customized RM learning
scheme, then empirically verify its effectiveness on both general preference
datasets and our DSP set. Furthermore, we test multiple training and data
strategies on the three learning stages. We find several ways to better
preserve the general preferring ability while training the customized RMs,
especially general preference enrichment, and customized preference imitation
learning. The DSP dataset and code are available at
https://github.com/Linear95/DSP.",None,-1
3668d4e0-eabb-4d20-a24e-f654367e42d4,Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study,0.101113,"This paper investigates the potential usage of large text-to-image (LTI)
models for the automated diagnosis of a few skin conditions with rarity or a
serious lack of annotated datasets. As the input to the LTI model, we provide
the targeted instantiation of a generic but succinct prompt structure designed
upon careful observations of the conditional narratives from the standard
medical textbooks. In this regard, we pave the path to utilizing accessible
textbook descriptions for automated diagnosis of conditions with data scarcity
through the lens of LTI models. Experiments show the efficacy of the proposed
framework, including much better localization of the infected regions.
Moreover, it has the immense possibility for generalization across the medical
sub-domains, not only to mitigate the data scarcity issue but also to debias
automated diagnostics from the all-pervasive racial biases.",None,-1
c4a96aac-cae3-4db6-a5a0-fc73b8379eca,A Novel Deep Reinforcement Learning-based Approach for Enhancing Spectral Efficiency of IRS-assisted Wireless Systems,0.164745,"This letter investigates an intelligent reflecting surfaces (IRS)-enhanced
network from spectral efficiency enhancement point of view for downlink
multi-user (MU) multi-input-single-output systems (MISO). In contrast to
previous works which mainly focused on alternative optimization methods, we
investigate the non-convex joint optimization problem of the active transmit
beamforming matrix at the base station together with the passive phase shift
matrix at the IRS by utilizing two deep reinforcement learning frameworks, i.
e., deep deterministic policy gradient (DDPG) and twin delayed DDPG (TD3).
Simulation results reveal that the neural networks in the latter scheme perform
generally more satisfactorily in various situations.",None,-1
c8892f42-c192-463b-98c7-a03d04ac3fac,TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision,0.44145,"In this paper, we investigate an open research task of generating
controllable 3D textured shapes from the given textual descriptions. Previous
works either require ground truth caption labeling or extensive optimization
time. To resolve these issues, we present a novel framework, TAPS3D, to train a
text-guided 3D shape generator with pseudo captions. Specifically, based on
rendered 2D images, we retrieve relevant words from the CLIP vocabulary and
construct pseudo captions using templates. Our constructed captions provide
high-level semantic supervision for generated 3D shapes. Further, in order to
produce fine-grained textures and increase geometry diversity, we propose to
adopt low-level image regularization to enable fake-rendered images to align
with the real ones. During the inference phase, our proposed model can generate
3D textured shapes from the given text without any additional optimization. We
conduct extensive experiments to analyze each of our proposed components and
show the efficacy of our framework in generating high-fidelity 3D textured and
text-relevant shapes.",None,-1
d807ef5f-3676-4863-810f-24ccb67fbe12,Can LLMs facilitate interpretation of pre-trained language models?,0.544927,"Work done to uncover the knowledge encoded within pre-trained language models
rely on annotated corpora or human-in-the-loop methods. However, these
approaches are limited in terms of scalability and the scope of interpretation.
We propose using a large language model, ChatGPT, as an annotator to enable
fine-grained interpretation analysis of pre-trained language models. We
discover latent concepts within pre-trained language models by applying
agglomerative hierarchical clustering over contextualized representations and
then annotate these concepts using ChatGPT. Our findings demonstrate that
ChatGPT produces accurate and semantically richer annotations compared to
human-annotated concepts. Additionally, we showcase how GPT-based annotations
empower interpretation analysis methodologies of which we demonstrate two:
probing frameworks and neuron interpretation. To facilitate further exploration
and experimentation in the field, we make available a substantial ConceptNet
dataset (TCN) comprising 39,000 annotated concepts.",None,-1
b5365a04-9430-4ab8-92c0-bd0ae43ab2b8,"The Ontology for Agents, Systems and Integration of Services: OASIS version 2",0.560412,"Semantic representation is a key enabler for several application domains, and
the multi-agent systems realm makes no exception. Among the methods for
semantically representing agents, one has been essentially achieved by taking a
behaviouristic vision, through which one can describe how they operate and
engage with their peers. The approach essentially aims at defining the
operational capabilities of agents through the mental states related with the
achievement of tasks. The OASIS ontology -- An Ontology for Agent, Systems, and
Integration of Services, presented in 2019 -- pursues the behaviouristic
approach to deliver a semantic representation system and a communication
protocol for agents and their commitments. This paper reports on the main
modeling choices concerning the representation of agents in OASIS 2, the latest
major upgrade of OASIS, and the achievement reached by the ontology since it
was first introduced, in particular in the context of ontologies for
blockchains.",None,-1
846d8c3d-5485-4ef2-b016-5de35a779f0b,CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models,0.00840268,"Parameter-efficient tuning (PET) has been widely explored in recent years
because it tunes much fewer parameters (PET modules) than full-parameter
fine-tuning (FT) while still stimulating sufficient knowledge from large
language models (LLMs) for downstream tasks. Moreover, when PET is employed to
serve multiple tasks, different task-specific PET modules can be built on a
frozen LLM, avoiding redundant LLM deployments. Although PET significantly
reduces the cost of tuning and deploying LLMs, its inference still suffers from
the computational bottleneck of LLMs. To address the above issue, we propose an
effective PET framework based on compressed LLMs, named ""CPET"". In CPET, we
evaluate the impact of mainstream LLM compression techniques on PET performance
and then introduce knowledge inheritance and recovery strategies to restore the
knowledge loss caused by these compression techniques. Our experimental results
demonstrate that, owing to the restoring strategies of CPET, collaborating
task-specific PET modules with a compressed LLM can achieve comparable
performance to collaborating PET modules with the original version of the
compressed LLM and outperform directly applying vanilla PET methods to the
compressed LLM.",None,-1
540a720f-ab49-4ceb-9266-2f7a5eb61c63,Software Vulnerability Prediction Knowledge Transferring Between Programming Languages,0.215,"Developing automated and smart software vulnerability detection models has
been receiving great attention from both research and development communities.
One of the biggest challenges in this area is the lack of code samples for all
different programming languages. In this study, we address this issue by
proposing a transfer learning technique to leverage available datasets and
generate a model to detect common vulnerabilities in different programming
languages. We use C source code samples to train a Convolutional Neural Network
(CNN) model, then, we use Java source code samples to adopt and evaluate the
learned model. We use code samples from two benchmark datasets: NIST Software
Assurance Reference Dataset (SARD) and Draper VDISC dataset. The results show
that proposed model detects vulnerabilities in both C and Java codes with
average recall of 72\%. Additionally, we employ explainable AI to investigate
how much each feature contributes to the knowledge transfer mechanisms between
C and Java in the proposed model.",None,-1
3ce56841-f102-4dca-b546-ea90bb1ec02d,Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation,0.225168,"Data contamination in model evaluation is getting increasingly prevalent as
the massive training corpora of large language models often unintentionally
include benchmark samples. Therefore, contamination analysis has became an
inevitable part of reliable model evaluation. However, existing method of
contamination analysis requires the access of the entire training data which is
often confidential for recent models. This prevent the community to rigorously
audit these models and conduct accurate assessment of their capability. In this
paper, we propose a novel method to quantify contamination without the access
of the full training set, that measure the extent of contamination with
perplexity. Our analysis provides evidence of significant memorisation of
recent foundation models in popular reading comprehension, summarisation
benchmarks, while multiple choice appears less contaminated.",None,-1
27eb526a-3904-42dd-b62d-15c154153e6d,CSED: A Chinese Semantic Error Diagnosis Corpus,0.876224,"Recently, much Chinese text error correction work has focused on Chinese
Spelling Check (CSC) and Chinese Grammatical Error Diagnosis (CGED). In
contrast, little attention has been paid to the complicated problem of Chinese
Semantic Error Diagnosis (CSED), which lacks relevant datasets. The study of
semantic errors is important because they are very common and may lead to
syntactic irregularities or even problems of comprehension. To investigate
this, we build the CSED corpus, which includes two datasets. The one is for the
CSED-Recognition (CSED-R) task. The other is for the CSED-Correction (CSED-C)
task. Our annotation guarantees high-quality data through quality assurance
mechanisms. Our experiments show that powerful pre-trained models perform
poorly on this corpus. We also find that the CSED task is challenging, as
evidenced by the fact that even humans receive a low score. This paper proposes
syntax-aware models to specifically adapt to the CSED task. The experimental
results show that the introduction of the syntax-aware approach is meaningful.",None,-1
1fe7c249-5417-41c2-83cb-19c216b66c23,TransCAR: Transformer-based Camera-And-Radar Fusion for 3D Object Detection,0.620433,"Despite radar's popularity in the automotive industry, for fusion-based 3D
object detection, most existing works focus on LiDAR and camera fusion. In this
paper, we propose TransCAR, a Transformer-based Camera-And-Radar fusion
solution for 3D object detection. Our TransCAR consists of two modules. The
first module learns 2D features from surround-view camera images and then uses
a sparse set of 3D object queries to index into these 2D features. The
vision-updated queries then interact with each other via transformer
self-attention layer. The second module learns radar features from multiple
radar scans and then applies transformer decoder to learn the interactions
between radar features and vision-updated queries. The cross-attention layer
within the transformer decoder can adaptively learn the soft-association
between the radar features and vision-updated queries instead of
hard-association based on sensor calibration only. Finally, our model estimates
a bounding box per query using set-to-set Hungarian loss, which enables the
method to avoid non-maximum suppression. TransCAR improves the velocity
estimation using the radar scans without temporal information. The superior
experimental results of our TransCAR on the challenging nuScenes datasets
illustrate that our TransCAR outperforms state-of-the-art Camera-Radar
fusion-based 3D object detection approaches.",None,-1
1a076b3b-6af7-4371-832e-b5eb40cb4851,Multi-Scale Prototypical Transformer for Whole Slide Image Classification,0.837679,"Whole slide image (WSI) classification is an essential task in computational
pathology. Despite the recent advances in multiple instance learning (MIL) for
WSI classification, accurate classification of WSIs remains challenging due to
the extreme imbalance between the positive and negative instances in bags, and
the complicated pre-processing to fuse multi-scale information of WSI. To this
end, we propose a novel multi-scale prototypical Transformer (MSPT) for WSI
classification, which includes a prototypical Transformer (PT) module and a
multi-scale feature fusion module (MFFM). The PT is developed to reduce
redundant instances in bags by integrating prototypical learning into the
Transformer architecture. It substitutes all instances with cluster prototypes,
which are then re-calibrated through the self-attention mechanism of the
Trans-former. Thereafter, an MFFM is proposed to fuse the clustered prototypes
of different scales, which employs MLP-Mixer to enhance the information
communication between prototypes. The experimental results on two public WSI
datasets demonstrate that the proposed MSPT outperforms all the compared
algorithms, suggesting its potential applications.",None,-1
148e0ec8-abd9-48fa-9dd4-c42715835753,Adaptive Texture Filtering for Single-Domain Generalized Segmentation,0.0500268,"Domain generalization in semantic segmentation aims to alleviate the
performance degradation on unseen domains through learning domain-invariant
features. Existing methods diversify images in the source domain by adding
complex or even abnormal textures to reduce the sensitivity to domain specific
features. However, these approaches depend heavily on the richness of the
texture bank, and training them can be time-consuming. In contrast to importing
textures arbitrarily or augmenting styles randomly, we focus on the single
source domain itself to achieve generalization. In this paper, we present a
novel adaptive texture filtering mechanism to suppress the influence of texture
without using augmentation, thus eliminating the interference of
domain-specific features. Further, we design a hierarchical guidance
generalization network equipped with structure-guided enhancement modules,
which purpose is to learn the domain-invariant generalized knowledge. Extensive
experiments together with ablation studies on widely-used datasets are
conducted to verify the effectiveness of the proposed model, and reveal its
superiority over other state-of-the-art alternatives.",None,-1
7d39e70c-ed16-4bc2-a3b9-9039b4d97fcc,Toward Computationally Efficient Inverse Reinforcement Learning via Reward Shaping,0.119637,"Inverse reinforcement learning (IRL) is computationally challenging, with
common approaches requiring the solution of multiple reinforcement learning
(RL) sub-problems. This work motivates the use of potential-based reward
shaping to reduce the computational burden of each RL sub-problem. This work
serves as a proof-of-concept and we hope will inspire future developments
towards computationally efficient IRL.",None,-1
b66f8a23-a223-4694-9c5a-6593538ccd7f,SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples,0.092815,"Detecting negatives (such as non-entailment relationships, unanswerable
questions, and false claims) is an important and challenging aspect of many
natural language understanding tasks. Though manually collecting challenging
negative examples can help models detect them, it is both costly and
domain-specific. In this work, we propose Self-labeled Counterfactuals for
Extrapolating to Negative Examples (SCENE), an automatic method for
synthesizing training data that greatly improves models' ability to detect
challenging negative examples. In contrast with standard data augmentation,
which synthesizes new examples for existing labels, SCENE can synthesize
negative examples zero-shot from only positive ones. Given a positive example,
SCENE perturbs it with a mask infilling model, then determines whether the
resulting example is negative based on a self-training heuristic. With access
to only answerable training examples, SCENE can close 69.6% of the performance
gap on SQuAD 2.0, a dataset where half of the evaluation examples are
unanswerable, compared to a model trained on SQuAD 2.0. Our method also extends
to boolean question answering and recognizing textual entailment, and improves
generalization from SQuAD to ACE-whQA, an out-of-domain extractive QA
benchmark.",None,-1
0e6d3c86-561c-4bb2-b591-f827b3e31b60,A Transformer-based Approach for Arabic Offline Handwritten Text Recognition,0.852553,"Handwriting recognition is a challenging and critical problem in the fields
of pattern recognition and machine learning, with applications spanning a wide
range of domains. In this paper, we focus on the specific issue of recognizing
offline Arabic handwritten text. Existing approaches typically utilize a
combination of convolutional neural networks for image feature extraction and
recurrent neural networks for temporal modeling, with connectionist temporal
classification used for text generation. However, these methods suffer from a
lack of parallelization due to the sequential nature of recurrent neural
networks. Furthermore, these models cannot account for linguistic rules,
necessitating the use of an external language model in the post-processing
stage to boost accuracy. To overcome these issues, we introduce two alternative
architectures, namely the Transformer Transducer and the standard
sequence-to-sequence Transformer, and compare their performance in terms of
accuracy and speed. Our approach can model language dependencies and relies
only on the attention mechanism, thereby making it more parallelizable and less
complex. We employ pre-trained Transformers for both image understanding and
language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that
our proposed method outperforms the current state-of-the-art approaches for
recognizing offline Arabic handwritten text.",None,-1
c5ec1e35-ed48-44d4-a74b-bb389f1f611c,Multi-Modal Mutual Attention and Iterative Interaction for Referring Image Segmentation,0.755187,"We address the problem of referring image segmentation that aims to generate
a mask for the object specified by a natural language expression. Many recent
works utilize Transformer to extract features for the target object by
aggregating the attended visual regions. However, the generic attention
mechanism in Transformer only uses the language input for attention weight
calculation, which does not explicitly fuse language features in its output.
Thus, its output feature is dominated by vision information, which limits the
model to comprehensively understand the multi-modal information, and brings
uncertainty for the subsequent mask decoder to extract the output mask. To
address this issue, we propose Multi-Modal Mutual Attention ($\mathrm{M^3Att}$)
and Multi-Modal Mutual Decoder ($\mathrm{M^3Dec}$) that better fuse information
from the two input modalities. Based on {$\mathrm{M^3Dec}$}, we further propose
Iterative Multi-modal Interaction ($\mathrm{IMI}$) to allow continuous and
in-depth interactions between language and vision features. Furthermore, we
introduce Language Feature Reconstruction ($\mathrm{LFR}$) to prevent the
language information from being lost or distorted in the extracted feature.
Extensive experiments show that our proposed approach significantly improves
the baseline and outperforms state-of-the-art referring image segmentation
methods on RefCOCO series datasets consistently.",None,-1
1b5aab62-d1ab-4b00-a834-fb937c32a1f8,GAM : Gradient Attention Module of Optimization for Point Clouds Analysis,0.646464,"In point cloud analysis tasks, the existing local feature aggregation
descriptors (LFAD) are unable to fully utilize information in the neighborhood
of central points. Previous methods rely solely on Euclidean distance to
constrain the local aggregation process, which can be easily affected by
abnormal points and cannot adequately fit with the original geometry of the
point cloud. We believe that fine-grained geometric information (FGGI) is
significant for the aggregation of local features. Therefore, we propose a
gradient-based local attention module, termed as Gradient Attention Module
(GAM), to address the aforementioned problem. Our proposed GAM simplifies the
process that extracts gradient information in the neighborhood and uses the
Zenith Angle matrix and Azimuth Angle matrix as explicit representation, which
accelerates the module by 35X. Comprehensive experiments were conducted on five
benchmark datasets to demonstrate the effectiveness and generalization
capability of the proposed GAM for 3D point cloud analysis. Especially on S3DIS
dataset, GAM achieves the best performance among current point-based models
with mIoU/OA/mAcc of 74.4%/90.6%/83.2%, respectively.",None,-1
a78b98b9-f242-47ef-92b3-bef4326c1f18,MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning,0.719917,"This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing
Detection. We used a multi-label contrastive loss for fine-tuning large
pre-trained language models in a multi-lingual setting, achieving very
competitive results: our system was ranked first on the official test set and
on the official shared task leaderboard for five of the six languages for which
we had training data and for which we could perform fine-tuning. Here, we
describe our experimental setup, as well as various ablation studies. The code
of our system is available at https://github.com/QishengL/SemEval2023",None,-1
fce1f812-5b1c-47e0-8b10-beda292909b5,Generative AI-empowered Effective Physical-Virtual Synchronization in the Vehicular Metaverse,0.505611,"Metaverse seamlessly blends the physical world and virtual space via
ubiquitous communication and computing infrastructure. In transportation
systems, the vehicular Metaverse can provide a fully-immersive and hyperreal
traveling experience (e.g., via augmented reality head-up displays, AR-HUDs) to
drivers and users in autonomous vehicles (AVs) via roadside units (RSUs).
However, provisioning real-time and immersive services necessitates effective
physical-virtual synchronization between physical and virtual entities, i.e.,
AVs and Metaverse AR recommenders (MARs). In this paper, we propose a
generative AI-empowered physical-virtual synchronization framework for the
vehicular Metaverse. In physical-to-virtual synchronization, digital twin (DT)
tasks generated by AVs are offloaded for execution in RSU with future route
generation. In virtual-to-physical synchronization, MARs customize diverse and
personal AR recommendations via generative AI models based on user preferences.
Furthermore, we propose a multi-task enhanced auction-based mechanism to match
and price AVs and MARs for RSUs to provision real-time and effective services.
Finally, property analysis and experimental results demonstrate that the
proposed mechanism is strategy-proof and adverse-selection free while
increasing social surplus by 50%.",None,-1
e7fd5928-e3cb-4714-82b2-0f465ed16f58,Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-identification,0.998303,"For the visible-infrared person re-identification (VIReID) task, one of the
major challenges is the modality gaps between visible (VIS) and infrared (IR)
images. However, the training samples are usually limited, while the modality
gaps are too large, which leads that the existing methods cannot effectively
mine diverse cross-modality clues. To handle this limitation, we propose a
novel augmentation network in the embedding space, called diverse embedding
expansion network (DEEN). The proposed DEEN can effectively generate diverse
embeddings to learn the informative feature representations and reduce the
modality discrepancy between the VIS and IR images. Moreover, the VIReID model
may be seriously affected by drastic illumination changes, while all the
existing VIReID datasets are captured under sufficient illumination without
significant light changes. Thus, we provide a low-light cross-modality (LLCM)
dataset, which contains 46,767 bounding boxes of 1,064 identities captured by 9
RGB/IR cameras. Extensive experiments on the SYSU-MM01, RegDB and LLCM datasets
show the superiority of the proposed DEEN over several other state-of-the-art
methods. The code and dataset are released at: https://github.com/ZYK100/LLCM",None,-1
8262fbbd-7eb3-4a38-b7a3-4649a4d5c90c,Unbiased organism-agnostic and highly sensitive signal peptide predictor with deep protein language model,0.572113,"Signal peptide (SP) is a short peptide located in the N-terminus of proteins.
It is essential to target and transfer transmembrane and secreted proteins to
correct positions. Compared with traditional experimental methods to identify
signal peptides, computational methods are faster and more efficient, which are
more practical for analyzing thousands or even millions of protein sequences,
especially for metagenomic data. Here we present Unbiased Organism-agnostic
Signal Peptide Network (USPNet), a signal peptide classification and cleavage
site prediction deep learning method that takes advantage of protein language
models. We propose to apply label distribution-aware margin loss to handle data
imbalance problems and use evolutionary information of protein to enrich
representation and overcome species information dependence.",None,-1
f54a44fd-ce32-4d14-af9d-a278b8dcfa36,3D View Prediction Models of the Dorsal Visual Stream,0.676852,"Deep neural network representations align well with brain activity in the
ventral visual stream. However, the primate visual system has a distinct dorsal
processing stream with different functional properties. To test if a model
trained to perceive 3D scene geometry aligns better with neural responses in
dorsal visual areas, we trained a self-supervised geometry-aware recurrent
neural network (GRNN) to predict novel camera views using a 3D feature memory.
We compared GRNN to self-supervised baseline models that have been shown to
align well with ventral regions using the large-scale fMRI Natural Scenes
Dataset (NSD). We found that while the baseline models accounted better for
ventral brain regions, GRNN accounted for a greater proportion of variance in
dorsal brain regions. Our findings demonstrate the potential for using
task-relevant models to probe representational differences across visual
streams.",None,-1
7d0674aa-1c14-41b9-9533-5f3ad44dc2fc,PDPU: An Open-Source Posit Dot-Product Unit for Deep Learning Applications,0.439626,"Posit has been a promising alternative to the IEEE-754 floating point format
for deep learning applications due to its better trade-off between dynamic
range and accuracy. However, hardware implementation of posit arithmetic
requires further exploration, especially for the dot-product operations
dominated in deep neural networks (DNNs). It has been implemented by either the
combination of multipliers and an adder tree or cascaded fused multiply-add
units, leading to poor computational efficiency and excessive hardware
overhead. To address this issue, we propose an open-source posit dot-product
unit, namely PDPU, that facilitates resource-efficient and high-throughput
dot-product hardware implementation. PDPU not only features the fused and
mixed-precision architecture that eliminates redundant latency and hardware
resources, but also has a fine-grained 6-stage pipeline, improving
computational efficiency. A configurable PDPU generator is further developed to
meet the diverse needs of various DNNs for computational accuracy. Experimental
results evaluated under the 28nm CMOS process show that PDPU reduces area,
latency, and power by up to 43%, 64%, and 70%, respectively, compared to the
existing implementations. Hence, PDPU has great potential as the computing core
of posit-based accelerators for deep learning applications.",None,-1
b79d9c0c-fb04-4168-bf24-21cea2d59073,Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks,0.556798,"Large Language Models (LLMs) evaluation is a patchy and inconsistent
landscape, and it is becoming clear that the quality of automatic evaluation
metrics is not keeping up with the pace of development of generative models. We
aim to improve the understanding of current models' performance by providing a
preliminary and hybrid evaluation on a range of open and closed-source
generative LLMs on three NLP benchmarks: text summarisation, text
simplification and grammatical error correction (GEC), using both automatic and
human evaluation. We also explore the potential of the recently released GPT-4
to act as an evaluator. We find that ChatGPT consistently outperforms many
other popular models according to human reviewers on the majority of metrics,
while scoring much more poorly when using classic automatic evaluation metrics.
We also find that human reviewers rate the gold reference as much worse than
the best models' outputs, indicating the poor quality of many popular
benchmarks. Finally, we find that GPT-4 is capable of ranking models' outputs
in a way which aligns reasonably closely to human judgement despite
task-specific variations, with a lower alignment in the GEC task.",None,-1
6a9999a3-b42b-4ed4-94b7-902daaf13341,Detect Any Deepfakes: Segment Anything Meets Face Forgery Detection and Localization,0.563211,"The rapid advancements in computer vision have stimulated remarkable progress
in face forgery techniques, capturing the dedicated attention of researchers
committed to detecting forgeries and precisely localizing manipulated areas.
Nonetheless, with limited fine-grained pixel-wise supervision labels, deepfake
detection models perform unsatisfactorily on precise forgery detection and
localization. To address this challenge, we introduce the well-trained vision
segmentation foundation model, i.e., Segment Anything Model (SAM) in face
forgery detection and localization. Based on SAM, we propose the Detect Any
Deepfakes (DADF) framework with the Multiscale Adapter, which can capture
short- and long-range forgery contexts for efficient fine-tuning. Moreover, to
better identify forged traces and augment the model's sensitivity towards
forgery regions, Reconstruction Guided Attention (RGA) module is proposed. The
proposed framework seamlessly integrates end-to-end forgery localization and
detection optimization. Extensive experiments on three benchmark datasets
demonstrate the superiority of our approach for both forgery detection and
localization. The codes will be released soon at
https://github.com/laiyingxin2/DADF.",None,-1
e0217a79-4614-4cdb-9e94-18921dcb82cd,Advancing Referring Expression Segmentation Beyond Single Image,0.794147,"Referring Expression Segmentation (RES) is a widely explored multi-modal
task, which endeavors to segment the pre-existing object within a single image
with a given linguistic expression. However, in broader real-world scenarios,
it is not always possible to determine if the described object exists in a
specific image. Typically, we have a collection of images, some of which may
contain the described objects. The current RES setting curbs its practicality
in such situations. To overcome this limitation, we propose a more realistic
and general setting, named Group-wise Referring Expression Segmentation (GRES),
which expands RES to a collection of related images, allowing the described
objects to be present in a subset of input images. To support this new setting,
we introduce an elaborately compiled dataset named Grouped Referring Dataset
(GRD), containing complete group-wise annotations of target objects described
by given expressions. We also present a baseline method named Grouped Referring
Segmenter (GRSer), which explicitly captures the language-vision and
intra-group vision-vision interactions to achieve state-of-the-art results on
the proposed GRES and related tasks, such as Co-Salient Object Detection and
RES. Our dataset and codes will be publicly released in
https://github.com/yixuan730/group-res.",None,-1
b687a32c-6686-49d4-be6f-a30e04e2d023,AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing,0.710102,"The intensive care unit (ICU) is a specialized hospital space where
critically ill patients receive intensive care and monitoring. Comprehensive
monitoring is imperative in assessing patients conditions, in particular
acuity, and ultimately the quality of care. However, the extent of patient
monitoring in the ICU is limited due to time constraints and the workload on
healthcare providers. Currently, visual assessments for acuity, including fine
details such as facial expressions, posture, and mobility, are sporadically
captured, or not captured at all. These manual observations are subjective to
the individual, prone to documentation errors, and overburden care providers
with the additional workload. Artificial Intelligence (AI) enabled systems has
the potential to augment the patient visual monitoring and assessment due to
their exceptional learning capabilities. Such systems require robust annotated
data to train. To this end, we have developed pervasive sensing and data
processing system which collects data from multiple modalities depth images,
color RGB images, accelerometry, electromyography, sound pressure, and light
levels in ICU for developing intelligent monitoring systems for continuous and
granular acuity, delirium risk, pain, and mobility assessment. This paper
presents the Intelligent Intensive Care Unit (I2CU) system architecture we
developed for real-time patient monitoring and visual assessment.",None,-1
78b46fcf-81c3-4b97-b8b8-95cf511f7fda,Contextual Affinity Distillation for Image Anomaly Detection,0.552884,"Previous works on unsupervised industrial anomaly detection mainly focus on
local structural anomalies such as cracks and color contamination. While
achieving significantly high detection performance on this kind of anomaly,
they are faced with logical anomalies that violate the long-range dependencies
such as a normal object placed in the wrong position. In this paper, based on
previous knowledge distillation works, we propose to use two students (local
and global) to better mimic the teacher's behavior. The local student, which is
used in previous studies mainly focuses on structural anomaly detection while
the global student pays attention to logical anomalies. To further encourage
the global student's learning to capture long-range dependencies, we design the
global context condensing block (GCCB) and propose a contextual affinity loss
for the student training and anomaly scoring. Experimental results show the
proposed method doesn't need cumbersome training techniques and achieves a new
state-of-the-art performance on the MVTec LOCO AD dataset.",None,-1
f2bd9de5-1980-4de2-abff-78b9a0b568f2,Company classification using zero-shot learning,0.0639039,"In recent years, natural language processing (NLP) has become increasingly
important in a variety of business applications, including sentiment analysis,
text classification, and named entity recognition. In this paper, we propose an
approach for company classification using NLP and zero-shot learning. Our
method utilizes pre-trained transformer models to extract features from company
descriptions, and then applies zero-shot learning to classify companies into
relevant categories without the need for specific training data for each
category. We evaluate our approach on a dataset obtained through the Wharton
Research Data Services (WRDS), which comprises textual descriptions of publicly
traded companies. We demonstrate that the approach can streamline the process
of company classification, thereby reducing the time and resources required in
traditional approaches such as the Global Industry Classification Standard
(GICS). The results show that this method has potential for automation of
company classification, making it a promising avenue for future research in
this area.",None,-1
5b9b754c-4ec1-4e97-bb9a-a62f12603a34,LineFormer: Rethinking Line Chart Data Extraction as Instance Segmentation,0.121466,"Data extraction from line-chart images is an essential component of the
automated document understanding process, as line charts are a ubiquitous data
visualization format. However, the amount of visual and structural variations
in multi-line graphs makes them particularly challenging for automated parsing.
Existing works, however, are not robust to all these variations, either taking
an all-chart unified approach or relying on auxiliary information such as
legends for line data extraction. In this work, we propose LineFormer, a robust
approach to line data extraction using instance segmentation. We achieve
state-of-the-art performance on several benchmark synthetic and real chart
datasets. Our implementation is available at
https://github.com/TheJaeLal/LineFormer .",None,-1
1a42a218-9cfc-4f46-b66b-2033ff227b62,Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models,0.0154914,"This project focuses on enhancing open-source large language models through
instruction-tuning and providing comprehensive evaluations of their
performance. We explore how various training data factors, such as quantity,
quality, and linguistic distribution, influence the performance of
instruction-tuned models trained on publicly accessible high-quality
instruction datasets for both English and Chinese languages. Our goal is to
supplement evaluation with quantitative analyses, providing valuable insights
for the continued advancement of open-source chat models. Our model, data, and
code are publicly available for others to use and build upon.",None,-1
b5b48c61-38a7-4189-8694-94a9df5945f1,Tuning computer vision models with task rewards,0.522967,"Misalignment between model predictions and intended usage can be detrimental
for the deployment of computer vision models. The issue is exacerbated when the
task involves complex structured outputs, as it becomes harder to design
procedures which address this misalignment. In natural language processing,
this is often addressed using reinforcement learning techniques that align
models with a task reward. We adopt this approach and show its surprising
effectiveness across multiple computer vision tasks, such as object detection,
panoptic segmentation, colorization and image captioning. We believe this
approach has the potential to be widely useful for better aligning models with
a diverse range of computer vision tasks.",None,-1
29ec6f64-b098-46d8-8b1c-3d8b2f20089f,TopicGPT: A Prompt-based Topic Modeling Framework,0.817646,"Topic modeling is a well-established technique for exploring text corpora.
Conventional topic models (e.g., LDA) represent topics as bags of words that
often require ""reading the tea leaves"" to interpret; additionally, they offer
users minimal control over the formatting and specificity of resulting topics.
To tackle these issues, we introduce TopicGPT, a prompt-based framework that
uses large language models (LLMs) to uncover latent topics in a text
collection. TopicGPT produces topics that align better with human
categorizations compared to competing methods: it achieves a harmonic mean
purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for
the strongest baseline. Its topics are also interpretable, dispensing with
ambiguous bags of words in favor of topics with natural language labels and
associated free-form descriptions. Moreover, the framework is highly adaptable,
allowing users to specify constraints and modify topics without the need for
model retraining. By streamlining access to high-quality and interpretable
topics, TopicGPT represents a compelling, human-centered approach to topic
modeling.",None,-1
521a6657-5deb-44e7-9d19-b8e2d7e9d4d4,AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation,0.851422,"We present All-Pairs Multi-Field Transforms (AMT), a new network architecture
for video frame interpolation. It is based on two essential designs. First, we
build bidirectional correlation volumes for all pairs of pixels, and use the
predicted bilateral flows to retrieve correlations for updating both flows and
the interpolated content feature. Second, we derive multiple groups of
fine-grained flow fields from one pair of updated coarse flows for performing
backward warping on the input frames separately. Combining these two designs
enables us to generate promising task-oriented flows and reduce the
difficulties in modeling large motions and handling occluded areas during frame
interpolation. These qualities promote our model to achieve state-of-the-art
performance on various benchmarks with high efficiency. Moreover, our
convolution-based model competes favorably compared to Transformer-based models
in terms of accuracy and efficiency. Our code is available at
https://github.com/MCG-NKU/AMT.",None,-1
0dd4a5b2-d5a9-4d95-8b59-bc104b6ab009,A Reference-less Quality Metric for Automatic Speech Recognition via Contrastive-Learning of a Multi-Language Model with Self-Supervision,0.43102,"The common standard for quality evaluation of automatic speech recognition
(ASR) systems is reference-based metrics such as the Word Error Rate (WER),
computed using manual ground-truth transcriptions that are time-consuming and
expensive to obtain. This work proposes a multi-language referenceless quality
metric, which allows comparing the performance of different ASR models on a
speech dataset without ground truth transcriptions. To estimate the quality of
ASR hypotheses, a pre-trained language model (LM) is fine-tuned with
contrastive learning in a self-supervised learning manner. In experiments
conducted on several unseen test datasets consisting of outputs from top
commercial ASR engines in various languages, the proposed referenceless metric
obtains a much higher correlation with WER scores and their ranks than the
perplexity metric from the state-of-art multi-lingual LM in all experiments,
and also reduces WER by more than $7\%$ when used for ensembling hypotheses.
The fine-tuned model and experiments are made available for the
reproducibility: https://github.com/aixplain/NoRefER",None,-1
1c88f356-2d2e-4d37-a203-0521f26ddb38,Automated Heterogeneous Low-Bit Quantization of Multi-Model Deep Learning Inference Pipeline,0.0404605,"Multiple Deep Neural Networks (DNNs) integrated into single Deep Learning
(DL) inference pipelines e.g. Multi-Task Learning (MTL) or Ensemble Learning
(EL), etc., albeit very accurate, pose challenges for edge deployment. In these
systems, models vary in their quantization tolerance and resource demands,
requiring meticulous tuning for accuracy-latency balance. This paper introduces
an automated heterogeneous quantization approach for DL inference pipelines
with multiple DNNs.",None,-1
c0a831d0-cb64-47fe-8424-acce6b1b6e0c,Prompt-Guided Transformers for End-to-End Open-Vocabulary Object Detection,0.230365,"Prompt-OVD is an efficient and effective framework for open-vocabulary object
detection that utilizes class embeddings from CLIP as prompts, guiding the
Transformer decoder to detect objects in both base and novel classes.
Additionally, our novel RoI-based masked attention and RoI pruning techniques
help leverage the zero-shot classification ability of the Vision
Transformer-based CLIP, resulting in improved detection performance at minimal
computational cost. Our experiments on the OV-COCO and OVLVIS datasets
demonstrate that Prompt-OVD achieves an impressive 21.2 times faster inference
speed than the first end-to-end open-vocabulary detection method (OV-DETR),
while also achieving higher APs than four two-stage-based methods operating
within similar inference time ranges. Code will be made available soon.",None,-1
5d75ba91-88c5-41d5-9666-fd09783b81a5,Controlled Diversity with Preference : Towards Learning a Diverse Set of Desired Skills,0.057652,"Autonomously learning diverse behaviors without an extrinsic reward signal
has been a problem of interest in reinforcement learning. However, the nature
of learning in such mechanisms is unconstrained, often resulting in the
accumulation of several unusable, unsafe or misaligned skills. In order to
avoid such issues and ensure the discovery of safe and human-aligned skills, it
is necessary to incorporate humans into the unsupervised training process,
which remains a largely unexplored research area. In this work, we propose
Controlled Diversity with Preference (CDP), a novel, collaborative human-guided
mechanism for an agent to learn a set of skills that is diverse as well as
desirable. The key principle is to restrict the discovery of skills to those
regions that are deemed to be desirable as per a preference model trained using
human preference labels on trajectory pairs. We evaluate our approach on 2D
navigation and Mujoco environments and demonstrate the ability to discover
diverse, yet desirable skills.",None,-1
088bdfe2-17cf-4b61-b4b7-99710147192f,EfficientRep:An Efficient Repvgg-style ConvNets with Hardware-aware Neural Network Design,0.406039,"We present a hardware-efficient architecture of convolutional neural network,
which has a repvgg-like architecture. Flops or parameters are traditional
metrics to evaluate the efficiency of networks which are not sensitive to
hardware including computing ability and memory bandwidth. Thus, how to design
a neural network to efficiently use the computing ability and memory bandwidth
of hardware is a critical problem. This paper proposes a method how to design
hardware-aware neural network. Based on this method, we designed EfficientRep
series convolutional networks, which are high-computation hardware(e.g. GPU)
friendly and applied in YOLOv6 object detection framework. YOLOv6 has published
YOLOv6N/YOLOv6S/YOLOv6M/YOLOv6L models in v1 and v2 versions.",None,-1
0172d745-650b-4c2d-a39d-66a50a45a8bb,Using Simple Incentives to Improve Two-Sided Fairness in Ridesharing Systems,0.0762015,"State-of-the-art order dispatching algorithms for ridesharing batch passenger
requests and allocate them to a fleet of vehicles in a centralized manner,
optimizing over the estimated values of each passenger-vehicle matching using
integer linear programming (ILP). Using good estimates of future values, such
ILP-based approaches are able to significantly increase the service rates
(percentage of requests served) for a fixed fleet of vehicles. However, such
approaches that focus solely on maximizing efficiency can lead to disparities
for both drivers (e.g., income inequality) and passengers (e.g., inequality of
service for different groups). Existing approaches that consider fairness only
do it for naive assignment policies, require extensive training, or look at
only single-sided fairness. We propose a simple incentive-based fairness scheme
that can be implemented online as a part of this ILP formulation that allows us
to improve fairness over a variety of fairness metrics. Deriving from a lens of
variance minimization, we describe how these fairness incentives can be
formulated for two distinct use cases for passenger groups and driver fairness.
We show that under mild conditions, our approach can guarantee an improvement
in the chosen metric for the worst-off individual. We also show empirically
that our Simple Incentives approach significantly outperforms prior art,
despite requiring no retraining; indeed, it often leads to a large improvement
over the state-of-the-art fairness-aware approach in both overall service rate
and fairness.",None,-1
364455d5-5f96-4454-b3aa-ea646bd8c2c8,Turning a CLIP Model into a Scene Text Detector,0.512344,"The recent large-scale Contrastive Language-Image Pretraining (CLIP) model
has shown great potential in various downstream tasks via leveraging the
pretrained vision and language knowledge. Scene text, which contains rich
textual and visual information, has an inherent connection with a model like
CLIP. Recently, pretraining approaches based on vision language models have
made effective progresses in the field of text detection. In contrast to these
works, this paper proposes a new method, termed TCM, focusing on Turning the
CLIP Model directly for text detection without pretraining process. We
demonstrate the advantages of the proposed TCM as follows: (1) The underlying
principle of our framework can be applied to improve existing scene text
detector. (2) It facilitates the few-shot training capability of existing
methods, e.g., by using 10% of labeled data, we significantly improve the
performance of the baseline method with an average of 22% in terms of the
F-measure on 4 benchmarks. (3) By turning the CLIP model into existing scene
text detection methods, we further achieve promising domain adaptation ability.
The code will be publicly released at https://github.com/wenwenyu/TCM.",None,-1
fb71780a-4521-45ae-812a-3db016eff8a3,A Strategy-Oriented Bayesian Soft Actor-Critic Model,0.163176,"Adopting reasonable strategies is challenging but crucial for an intelligent
agent with limited resources working in hazardous, unstructured, and dynamic
environments to improve the system's utility, decrease the overall cost, and
increase mission success probability. This paper proposes a novel hierarchical
strategy decomposition approach based on the Bayesian chain rule to separate an
intricate policy into several simple sub-policies and organize their
relationships as Bayesian strategy networks (BSN). We integrate this approach
into the state-of-the-art DRL method -- soft actor-critic (SAC) and build the
corresponding Bayesian soft actor-critic (BSAC) model by organizing several
sub-policies as a joint policy. We compare the proposed BSAC method with the
SAC and other state-of-the-art approaches such as TD3, DDPG, and PPO on the
standard continuous control benchmarks -- Hopper-v2, Walker2d-v2, and
Humanoid-v2 -- in MuJoCo with the OpenAI Gym environment. The results
demonstrate that the promising potential of the BSAC method significantly
improves training efficiency.",None,-1
0590cfde-f171-4c71-9264-bd6d87a06258,Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations,0.941993,"Large pre-trained language models have exhibited unprecedented capabilities
in producing high-quality text via prompting techniques. This fact introduces
new possibilities for data collection and annotation, particularly in
situations where such data is scarce, complex to gather, expensive, or even
sensitive. In this paper, we explore the potential of these models to generate
and annotate goal-oriented dialogues, and conduct an in-depth analysis to
evaluate their quality. Our experiments employ ChatGPT, and encompass three
categories of goal-oriented dialogues (task-oriented, collaborative, and
explanatory), two generation modes (interactive and one-shot), and two
languages (English and Italian). Based on extensive human-based evaluations, we
demonstrate that the quality of generated dialogues and annotations is on par
with those generated by humans.",None,-1
49bced9a-37c2-4aa4-bda6-e56a0213288d,Rolling Horizon based Temporal Decomposition for the Offline Pickup and Delivery Problem with Time Windows,0.236943,"The offline pickup and delivery problem with time windows (PDPTW) is a
classical combinatorial optimization problem in the transportation community,
which has proven to be very challenging computationally. Due to the complexity
of the problem, practical problem instances can be solved only via heuristics,
which trade-off solution quality for computational tractability. Among the
various heuristics, a common strategy is problem decomposition, that is, the
reduction of a large-scale problem into a collection of smaller sub-problems,
with spatial and temporal decompositions being two natural approaches. While
spatial decomposition has been successful in certain settings, effective
temporal decomposition has been challenging due to the difficulty of stitching
together the sub-problem solutions across the decomposition boundaries. In this
work, we introduce a novel temporal decomposition scheme for solving a class of
PDPTWs that have narrow time windows, for which it is able to provide both fast
and high-quality solutions. We utilize techniques that have been popularized
recently in the context of online dial-a-ride problems along with the general
idea of rolling horizon optimization. To the best of our knowledge, this is the
first attempt to solve offline PDPTWs using such an approach. To show the
performance and scalability of our framework, we use the optimization of
paratransit services as a motivating example. We compare our results with an
offline heuristic algorithm using Google OR-Tools. In smaller problem
instances, the baseline approach is as competitive as our framework. However,
in larger problem instances, our framework is more scalable and can provide
good solutions to problem instances of varying degrees of difficulty, while the
baseline algorithm often fails to find a feasible solution within comparable
compute times.",None,-1
badb04e5-f3c3-404a-aa76-0b675f07564e,Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships,0.318484,"Understanding interpersonal communication requires, in part, understanding
the social context and norms in which a message is said. However, current
methods for identifying offensive content in such communication largely operate
independent of context, with only a few approaches considering community norms
or prior conversation as context. Here, we introduce a new approach to
identifying inappropriate communication by explicitly modeling the social
relationship between the individuals. We introduce a new dataset of
contextually-situated judgments of appropriateness and show that large language
models can readily incorporate relationship information to accurately identify
appropriateness in a given context. Using data from online conversations and
movie dialogues, we provide insight into how the relationships themselves
function as implicit norms and quantify the degree to which context-sensitivity
is needed in different conversation settings. Further, we also demonstrate that
contextual-appropriateness judgments are predictive of other social factors
expressed in language such as condescension and politeness.",None,-1
3facc911-241f-4d16-a6d2-dba3beb99460,DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields,0.248984,"Advances in neural fields are enabling high-fidelity capture of the shape and
appearance of dynamic 3D scenes. However, their capabilities lag behind those
offered by conventional representations such as 2D videos because of
algorithmic challenges and the lack of large-scale multi-view real-world
datasets. We address the dataset limitation with DiVa-360, a real-world 360
dynamic visual dataset that contains synchronized high-resolution and
long-duration multi-view video sequences of table-scale scenes captured using a
customized low-cost system with 53 cameras. It contains 21 object-centric
sequences categorized by different motion types, 25 intricate hand-object
interaction sequences, and 8 long-duration sequences for a total of 17.4 M
image frames. In addition, we provide foreground-background segmentation masks,
synchronized audio, and text descriptions. We benchmark the state-of-the-art
dynamic neural field methods on DiVa-360 and provide insights about existing
methods and future challenges on long-duration neural field capture.",None,-1
42f13c6a-70c6-4dc5-b9a9-63882d7e4066,Large Language Models Cannot Self-Correct Reasoning Yet,0.86057,"Large Language Models (LLMs) have emerged as a groundbreaking technology with
their unparalleled text generation capabilities across various applications.
Nevertheless, concerns persist regarding the accuracy and appropriateness of
their generated content. A contemporary methodology, self-correction, has been
proposed as a remedy to these issues. Building upon this premise, this paper
critically examines the role and efficacy of self-correction within LLMs,
shedding light on its true potential and limitations. Central to our
investigation is the notion of intrinsic self-correction, whereby an LLM
attempts to correct its initial responses based solely on its inherent
capabilities, without the crutch of external feedback. In the context of
reasoning, our research indicates that LLMs struggle to self-correct their
responses without external feedback, and at times, their performance even
degrades after self-correction. Drawing from these insights, we offer
suggestions for future research and practical applications in this field.",None,-1
7fe2167f-c3a6-4981-9bcf-177bbbac33cb,DFormer: Diffusion-guided Transformer for Universal Image Segmentation,0.318751,"This paper introduces an approach, named DFormer, for universal image
segmentation. The proposed DFormer views universal image segmentation task as a
denoising process using a diffusion model. DFormer first adds various levels of
Gaussian noise to ground-truth masks, and then learns a model to predict
denoising masks from corrupted masks. Specifically, we take deep pixel-level
features along with the noisy masks as inputs to generate mask features and
attention masks, employing diffusion-based decoder to perform mask prediction
gradually. At inference, our DFormer directly predicts the masks and
corresponding categories from a set of randomly-generated masks. Extensive
experiments reveal the merits of our proposed contributions on different image
segmentation tasks: panoptic segmentation, instance segmentation, and semantic
segmentation. Our DFormer outperforms the recent diffusion-based panoptic
segmentation method Pix2Seq-D with a gain of 3.6% on MS COCO val2017 set.
Further, DFormer achieves promising semantic segmentation performance
outperforming the recent diffusion-based method by 2.2% on ADE20K val set. Our
source code and models will be publicly on https://github.com/cp3wan/DFormer",None,-1
1e8dc07a-89da-4991-b7d4-b0c83b13e87b,Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games,0.993761,"In this study, we explore the application of Large Language Models (LLMs) in
\textit{Jubensha}, a Chinese detective role-playing game and a novel area in
Artificial Intelligence (AI) driven gaming. We introduce the first dataset
specifically for Jubensha, including character scripts and game rules, to
foster AI agent development in this complex narrative environment. Our work
also presents a unique multi-agent interaction framework using LLMs, allowing
AI agents to autonomously engage in this game. To evaluate the gaming
performance of these AI agents, we developed novel methods measuring their
mastery of case information and reasoning skills. Furthermore, we incorporated
the latest advancements in in-context learning to improve the agents'
performance in information gathering, murderer identification, and logical
reasoning. The experimental results validate the effectiveness of our proposed
methods. This work aims to offer a novel perspective on understanding LLM
capabilities and establish a new benchmark for evaluating large language
model-based agents.",None,-1
ded32d16-ea2b-48e0-bfbb-6b7920f134e7,End-to-End 3D Dense Captioning with Vote2Cap-DETR,0.948967,"3D dense captioning aims to generate multiple captions localized with their
associated object regions. Existing methods follow a sophisticated
``detect-then-describe'' pipeline equipped with numerous hand-crafted
components. However, these hand-crafted components would yield suboptimal
performance given cluttered object spatial and class distributions among
different scenes. In this paper, we propose a simple-yet-effective transformer
framework Vote2Cap-DETR based on recent popular \textbf{DE}tection
\textbf{TR}ansformer (DETR). Compared with prior arts, our framework has
several appealing advantages: 1) Without resorting to numerous hand-crafted
components, our method is based on a full transformer encoder-decoder
architecture with a learnable vote query driven object decoder, and a caption
decoder that produces the dense captions in a set-prediction manner. 2) In
contrast to the two-stage scheme, our method can perform detection and
captioning in one-stage. 3) Without bells and whistles, extensive experiments
on two commonly used datasets, ScanRefer and Nr3D, demonstrate that our
Vote2Cap-DETR surpasses current state-of-the-arts by 11.13\% and 7.11\% in
CIDEr@0.5IoU, respectively. Codes will be released soon.",None,-1
518db053-5b8d-4c27-a102-ac25eb013b1d,Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking,0.1496,"Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring
and annotating task-oriented dialogues, which can be time-consuming and costly.
However, DST extends beyond simple slot-filling and requires effective updating
strategies for tracking dialogue state as conversations progress. In this
paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to
introduce additional intricate updating strategies in zero-shot DST. Our
approach reformulates the DST task by leveraging powerful Large Language Models
(LLMs) and translating the original dialogue text to JSON through semantic
parsing as an intermediate state. We also design a novel framework that
includes more modules to ensure the effectiveness of updating strategies in the
text-to-JSON process. Experimental results demonstrate that our approach
outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant
improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to
existing ICL methods. Our code has been released.",None,-1
f6cacd5f-8500-4dbb-a2e0-1ffccf683b65,HyperSparse Neural Networks: Shifting Exploration to Exploitation through Adaptive Regularization,0.145137,"Sparse neural networks are a key factor in developing resource-efficient
machine learning applications. We propose the novel and powerful sparse
learning method Adaptive Regularized Training (ART) to compress dense into
sparse networks. Instead of the commonly used binary mask during training to
reduce the number of model weights, we inherently shrink weights close to zero
in an iterative manner with increasing weight regularization. Our method
compresses the pre-trained model knowledge into the weights of highest
magnitude. Therefore, we introduce a novel regularization loss named
HyperSparse that exploits the highest weights while conserving the ability of
weight exploration. Extensive experiments on CIFAR and TinyImageNet show that
our method leads to notable performance gains compared to other sparsification
methods, especially in extremely high sparsity regimes up to 99.8 percent model
sparsity. Additional investigations provide new insights into the patterns that
are encoded in weights with high magnitudes.",None,-1
0f1806d0-3386-4ec6-81bb-efd4ccba1533,TemporalMaxer: Maximize Temporal Context with only Max Pooling for Temporal Action Localization,0.889456,"Temporal Action Localization (TAL) is a challenging task in video
understanding that aims to identify and localize actions within a video
sequence. Recent studies have emphasized the importance of applying long-term
temporal context modeling (TCM) blocks to the extracted video clip features
such as employing complex self-attention mechanisms. In this paper, we present
the simplest method ever to address this task and argue that the extracted
video clip features are already informative to achieve outstanding performance
without sophisticated architectures. To this end, we introduce TemporalMaxer,
which minimizes long-term temporal context modeling while maximizing
information from the extracted video clip features with a basic,
parameter-free, and local region operating max-pooling block. Picking out only
the most critical information for adjacent and local clip embeddings, this
block results in a more efficient TAL model. We demonstrate that TemporalMaxer
outperforms other state-of-the-art methods that utilize long-term TCM such as
self-attention on various TAL datasets while requiring significantly fewer
parameters and computational resources. The code for our approach is publicly
available at https://github.com/TuanTNG/TemporalMaxer",None,-1
ccfcdd86-ddb2-45a6-ad02-d63a80a9307c,A Diffusion Model for Event Skeleton Generation,0.522847,"Event skeleton generation, aiming to induce an event schema skeleton graph
with abstracted event nodes and their temporal relations from a set of event
instance graphs, is a critical step in the temporal complex event schema
induction task. Existing methods effectively address this task from a graph
generation perspective but suffer from noise-sensitive and error accumulation,
e.g., the inability to correct errors while generating schema. We, therefore,
propose a novel Diffusion Event Graph Model~(DEGM) to address these issues. Our
DEGM is the first workable diffusion model for event skeleton generation, where
the embedding and rounding techniques with a custom edge-based loss are
introduced to transform a discrete event graph into learnable latent
representation. Furthermore, we propose a denoising training process to
maintain the model's robustness. Consequently, DEGM derives the final schema,
where error correction is guaranteed by iteratively refining the latent
representation during the schema generation process. Experimental results on
three IED bombing datasets demonstrate that our DEGM achieves better results
than other state-of-the-art baselines. Our code and data are available at
https://github.com/zhufq00/EventSkeletonGeneration.",None,-1
4afe136f-0f50-462d-973b-6d4e3770eb37,Dual Aggregation Transformer for Image Super-Resolution,0.999533,"Transformer has recently gained considerable popularity in low-level vision
tasks, including image super-resolution (SR). These networks utilize
self-attention along different dimensions, spatial or channel, and achieve
impressive performance. This inspires us to combine the two dimensions in
Transformer for a more powerful representation capability. Based on the above
idea, we propose a novel Transformer model, Dual Aggregation Transformer (DAT),
for image SR. Our DAT aggregates features across spatial and channel
dimensions, in the inter-block and intra-block dual manner. Specifically, we
alternately apply spatial and channel self-attention in consecutive Transformer
blocks. The alternate strategy enables DAT to capture the global context and
realize inter-block feature aggregation. Furthermore, we propose the adaptive
interaction module (AIM) and the spatial-gate feed-forward network (SGFN) to
achieve intra-block feature aggregation. AIM complements two self-attention
mechanisms from corresponding dimensions. Meanwhile, SGFN introduces additional
non-linear spatial information in the feed-forward network. Extensive
experiments show that our DAT surpasses current methods. Code and models are
obtainable at https://github.com/zhengchen1999/DAT.",None,-1
5818da8c-8ffa-46b0-823c-113c9b2a7dd2,Evaluating Online Bandit Exploration In Large-Scale Recommender System,0.442494,"Bandit learning has been an increasingly popular design choice for
recommender system. Despite the strong interest in bandit learning from the
community, there remains multiple bottlenecks that prevent many bandit learning
approaches from productionalization. One major bottleneck is how to test the
effectiveness of bandit algorithm with fairness and without data leakage.
Different from supervised learning algorithms, bandit learning algorithms
emphasize greatly on the data collection process through their explorative
nature. Such explorative behavior may induce unfair evaluation in a classic A/B
test setting. In this work, we apply upper confidence bound (UCB) to our large
scale short video recommender system and present a test framework for the
production bandit learning life-cycle with a new set of metrics. Extensive
experiment results show that our experiment design is able to fairly evaluate
the performance of bandit learning in the recommender system.",None,-1
2c0079a6-131e-47c1-9f6f-008a147f3bd6,Robust Multi-Agent Pickup and Delivery with Delays,0.620802,"Multi-Agent Pickup and Delivery (MAPD) is the problem of computing
collision-free paths for a group of agents such that they can safely reach
delivery locations from pickup ones. These locations are provided at runtime,
making MAPD a combination between classical Multi-Agent Path Finding (MAPF) and
online task assignment. Current algorithms for MAPD do not consider many of the
practical issues encountered in real applications: real agents often do not
follow the planned paths perfectly, and may be subject to delays and failures.
In this paper, we study the problem of MAPD with delays, and we present two
solution approaches that provide robustness guarantees by planning paths that
limit the effects of imperfect execution. In particular, we introduce two
algorithms, k-TP and p-TP, both based on a decentralized algorithm typically
used to solve MAPD, Token Passing (TP), which offer deterministic and
probabilistic guarantees, respectively. Experimentally, we compare our
algorithms against a version of TP enriched with online replanning. k-TP and
p-TP provide robust solutions, significantly reducing the number of replans
caused by delays, with little or no increase in solution cost and running time.",None,-1
61287b64-52e0-4dc5-bcfc-ceeecb41c1b4,Theater Aid System for the Visually Impaired Through Transfer Learning of Spatio-Temporal Graph Convolution Networks,0.0457459,"The aim of this research is to recognize human actions performed on stage to
aid visually impaired and blind individuals. To achieve this, we have created a
theatre human action recognition system that uses skeleton data captured by
depth image as input. We collected new samples of human actions in a theatre
environment, and then tested the transfer learning technique with three
pre-trained Spatio-Temporal Graph Convolution Networks for skeleton-based human
action recognition: the spatio-temporal graph convolution network, the
two-stream adaptive graph convolution network, and the multi-scale disentangled
unified graph convolution network. We selected the NTU-RGBD human action
benchmark as the source domain and used our collected dataset as the target
domain. We analyzed the transferability of the pre-trained models and proposed
two configurations to apply and adapt the transfer learning technique to the
diversity between the source and target domains. The use of transfer learning
helped to improve the performance of the human action system within the context
of theatre. The results indicate that Spatio-Temporal Graph Convolution
Networks is positively transferred, and there was an improvement in performance
compared to the baseline without transfer learning.",None,-1
35f8cfb5-b2ae-4e0b-ae91-b49bf840e68a,How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation,0.140094,"We release MMSMR, a Massively Multi-System MultiReference dataset to enable
future work on metrics and evaluation for dialog. Automatic metrics for
dialogue evaluation should be robust proxies for human judgments; however, the
verification of robustness is currently far from satisfactory. To quantify the
robustness correlation and understand what is necessary in a test set, we
create and release an 8-reference dialog dataset by extending single-reference
evaluation sets and introduce this new language learning conversation dataset.
We then train 1750 systems and evaluate them on our novel test set and the
DailyDialog dataset. We release the novel test set, and model hyper parameters,
inference outputs, and metric scores for each system on a variety of datasets.",None,-1
3eee4107-bb92-44b2-8804-fc88ae4bc4f6,DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion,0.57696,"We propose a new formulation of temporal action detection (TAD) with
denoising diffusion, DiffTAD in short. Taking as input random temporal
proposals, it can yield action proposals accurately given an untrimmed long
video. This presents a generative modeling perspective, against previous
discriminative learning manners. This capability is achieved by first diffusing
the ground-truth proposals to random ones (i.e., the forward/noising process)
and then learning to reverse the noising process (i.e., the backward/denoising
process). Concretely, we establish the denoising process in the Transformer
decoder (e.g., DETR) by introducing a temporal location query design with
faster convergence in training. We further propose a cross-step selective
conditioning algorithm for inference acceleration. Extensive evaluations on
ActivityNet and THUMOS show that our DiffTAD achieves top performance compared
to previous art alternatives. The code will be made available at
https://github.com/sauradip/DiffusionTAD.",None,-1
216b752b-7c1e-4d98-a199-fb15ecedcc83,VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View,0.963772,"Incremental decision making in real-world environments is one of the most
challenging tasks in embodied artificial intelligence. One particularly
demanding scenario is Vision and Language Navigation~(VLN) which requires
visual and natural language understanding as well as spatial and temporal
reasoning capabilities. The embodied agent needs to ground its understanding of
navigation instructions in observations of a real-world environment like Street
View. Despite the impressive results of LLMs in other research areas, it is an
ongoing problem of how to best connect them with an interactive visual
environment. In this work, we propose VELMA, an embodied LLM agent that uses a
verbalization of the trajectory and of visual environment observations as
contextual prompt for the next action. Visual information is verbalized by a
pipeline that extracts landmarks from the human written navigation instructions
and uses CLIP to determine their visibility in the current panorama view. We
show that VELMA is able to successfully follow navigation instructions in
Street View with only two in-context examples. We further finetune the LLM
agent on a few thousand examples and achieve 25%-30% relative improvement in
task completion over the previous state-of-the-art for two datasets.",None,-1
93d46a3f-b051-4bd0-bfd0-e41dff17f114,2-D SSM: A General Spatial Layer for Visual Transformers,0.622787,"A central objective in computer vision is to design models with appropriate
2-D inductive bias. Desiderata for 2D inductive bias include two-dimensional
position awareness, dynamic spatial locality, and translation and permutation
invariance. To address these goals, we leverage an expressive variation of the
multidimensional State Space Model (SSM). Our approach introduces efficient
parameterization, accelerated computation, and a suitable normalization scheme.
Empirically, we observe that incorporating our layer at the beginning of each
transformer block of Vision Transformers (ViT) significantly enhances
performance for multiple ViT backbones and across datasets. The new layer is
effective even with a negligible amount of additional parameters and inference
time. Ablation studies and visualizations demonstrate that the layer has a
strong 2-D inductive bias. For example, vision transformers equipped with our
layer exhibit effective performance even without positional encoding",None,-1
0f34d6f2-4e42-4de4-ac14-ad6987a253d4,Chasing Consistency in Text-to-3D Generation from a Single Image,0.500005,"Text-to-3D generation from a single-view image is a popular but challenging
task in 3D vision. Although numerous methods have been proposed, existing works
still suffer from the inconsistency issues, including 1) semantic
inconsistency, 2) geometric inconsistency, and 3) saturation inconsistency,
resulting in distorted, overfitted, and over-saturated generations. In light of
the above issues, we present Consist3D, a three-stage framework Chasing for
semantic-, geometric-, and saturation-Consistent Text-to-3D generation from a
single image, in which the first two stages aim to learn parameterized
consistency tokens, and the last stage is for optimization. Specifically, the
semantic encoding stage learns a token independent of views and estimations,
promoting semantic consistency and robustness. Meanwhile, the geometric
encoding stage learns another token with comprehensive geometry and
reconstruction constraints under novel-view estimations, reducing overfitting
and encouraging geometric consistency. Finally, the optimization stage benefits
from the semantic and geometric tokens, allowing a low classifier-free guidance
scale and therefore preventing oversaturation. Experimental results demonstrate
that Consist3D produces more consistent, faithful, and photo-realistic 3D
assets compared to previous state-of-the-art methods. Furthermore, Consist3D
also allows background and object editing through text prompts.",None,-1
83c1b25c-f397-42f5-a3b2-bad4f56811cf,Rigorously Assessing Natural Language Explanations of Neurons,0.840006,"Natural language is an appealing medium for explaining how large language
models process and store information, but evaluating the faithfulness of such
explanations is challenging. To help address this, we develop two modes of
evaluation for natural language explanations that claim individual neurons
represent a concept in a text input. In the observational mode, we evaluate
claims that a neuron $a$ activates on all and only input strings that refer to
a concept picked out by the proposed explanation $E$. In the intervention mode,
we construe $E$ as a claim that the neuron $a$ is a causal mediator of the
concept denoted by $E$. We apply our framework to the GPT-4-generated
explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the
most confident explanations have high error rates and little to no causal
efficacy. We close the paper by critically assessing whether natural language
is a good choice for explanations and whether neurons are the best level of
analysis.",None,-1
40cded90-19cf-44e5-90be-bbcd0e7d3870,Towards Cognitive Bots: Architectural Research Challenges,0.194119,"Software bots operating in multiple virtual digital platforms must understand
the platforms' affordances and behave like human users. Platform affordances or
features differ from one application platform to another or through a life
cycle, requiring such bots to be adaptable. Moreover, bots in such platforms
could cooperate with humans or other software agents for work or to learn
specific behavior patterns. However, present-day bots, particularly chatbots,
other than language processing and prediction, are far from reaching a human
user's behavior level within complex business information systems. They lack
the cognitive capabilities to sense and act in such virtual environments,
rendering their development a challenge to artificial general intelligence
research. In this study, we problematize and investigate assumptions in
conceptualizing software bot architecture by directing attention to significant
architectural research challenges in developing cognitive bots endowed with
complex behavior for operation on information systems. As an outlook, we
propose alternate architectural assumptions to consider in future bot design
and bot development frameworks.",None,-1
8e2fd36f-c9ad-463a-9f78-2d30decca4b9,Visual Hindsight Self-Imitation Learning for Interactive Navigation,0.235551,"Interactive visual navigation tasks, which involve following instructions to
reach and interact with specific targets, are challenging not only because
successful experiences are very rare but also because the complex visual inputs
require a substantial number of samples. Previous methods for these tasks often
rely on intricately designed dense rewards or the use of expensive expert data
for imitation learning. To tackle these challenges, we propose a novel
approach, Visual Hindsight Self-Imitation Learning (VHS) for enhancing sample
efficiency through hindsight goal re-labeling and self-imitation. We also
introduce a prototypical goal embedding method derived from experienced goal
observations, that is particularly effective in vision-based and partially
observable environments. This embedding technique allows the agent to visually
reinterpret its unsuccessful attempts, enabling vision-based goal re-labeling
and self-imitation from enhanced successful experiences. Experimental results
show that VHS outperforms existing techniques in interactive visual navigation
tasks, confirming its superior performance and sample efficiency.",None,-1
6014002f-4dbf-47c1-883a-13f9b7c7ccaf,Zero-touch realization of Pervasive Artificial Intelligence-as-a-service in 6G networks,0.813266,"The vision of the upcoming 6G technologies, characterized by ultra-dense
network, low latency, and fast data rate is to support Pervasive AI (PAI) using
zero-touch solutions enabling self-X (e.g., self-configuration,
self-monitoring, and self-healing) services. However, the research on 6G is
still in its infancy, and only the first steps have been taken to conceptualize
its design, investigate its implementation, and plan for use cases. Toward this
end, academia and industry communities have gradually shifted from theoretical
studies of AI distribution to real-world deployment and standardization. Still,
designing an end-to-end framework that systematizes the AI distribution by
allowing easier access to the service using a third-party application assisted
by a zero-touch service provisioning has not been well explored. In this
context, we introduce a novel platform architecture to deploy a zero-touch
PAI-as-a-Service (PAIaaS) in 6G networks supported by a blockchain-based smart
system. This platform aims to standardize the pervasive AI at all levels of the
architecture and unify the interfaces in order to facilitate the service
deployment across application and infrastructure domains, relieve the users
worries about cost, security, and resource allocation, and at the same time,
respect the 6G stringent performance requirements. As a proof of concept, we
present a Federated Learning-as-a-service use case where we evaluate the
ability of our proposed system to self-optimize and self-adapt to the dynamics
of 6G networks in addition to minimizing the users' perceived costs.",None,-1
2ec4e85d-3029-4b05-bac0-cf6c00a3aa2a,Syntactically Robust Training on Partially-Observed Data for Open Information Extraction,0.516583,"Open Information Extraction models have shown promising results with
sufficient supervision. However, these models face a fundamental challenge that
the syntactic distribution of training data is partially observable in
comparison to the real world. In this paper, we propose a syntactically robust
training framework that enables models to be trained on a syntactic-abundant
distribution based on diverse paraphrase generation. To tackle the intrinsic
problem of knowledge deformation of paraphrasing, two algorithms based on
semantic similarity matching and syntactic tree walking are used to restore the
expressionally transformed knowledge. The training framework can be generally
applied to other syntactic partial observable domains. Based on the proposed
framework, we build a new evaluation set called CaRB-AutoPara, a syntactically
diverse dataset consistent with the real-world setting for validating the
robustness of the models. Experiments including a thorough analysis show that
the performance of the model degrades with the increase of the difference in
syntactic distribution, while our framework gives a robust boundary. The source
code is publicly available at https://github.com/qijimrc/RobustOIE.",None,-1
b8d50bb1-73ba-4880-8bcd-148cebc498cd,SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition,0.477285,"Multilingual automatic speech recognition (ASR) systems have garnered
attention for their potential to extend language coverage globally. While
self-supervised learning (SSL) models, like MMS, have demonstrated their
effectiveness in multilingual ASR, it is worth noting that various layers'
representations potentially contain distinct information that has not been
fully leveraged. In this study, we propose a novel method that leverages
self-supervised hierarchical representations (SSHR) to fine-tune the MMS model.
We first analyze the different layers of MMS and show that the middle layers
capture language-related information, and the high layers encode
content-related information, which gradually decreases in the final layers.
Then, we extract a language-related frame from correlated middle layers and
guide specific language extraction through self-attention mechanisms.
Additionally, we steer the model toward acquiring more content-related
information in the final layers using our proposed Cross-CTC. We evaluate SSHR
on two multilingual datasets, Common Voice and ML-SUPERB, and the experimental
results demonstrate that our method achieves state-of-the-art performance.",None,-1
ea0788e0-eb06-4914-a0f4-fc285097964a,Retentive Network: A Successor to Transformer for Large Language Models,0.674462,"In this work, we propose Retentive Network (RetNet) as a foundation
architecture for large language models, simultaneously achieving training
parallelism, low-cost inference, and good performance. We theoretically derive
the connection between recurrence and attention. Then we propose the retention
mechanism for sequence modeling, which supports three computation paradigms,
i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel
representation allows for training parallelism. The recurrent representation
enables low-cost $O(1)$ inference, which improves decoding throughput, latency,
and GPU memory without sacrificing performance. The chunkwise recurrent
representation facilitates efficient long-sequence modeling with linear
complexity, where each chunk is encoded parallelly while recurrently
summarizing the chunks. Experimental results on language modeling show that
RetNet achieves favorable scaling results, parallel training, low-cost
deployment, and efficient inference. The intriguing properties make RetNet a
strong successor to Transformer for large language models. Code will be
available at https://aka.ms/retnet.",None,-1
80ce33b3-ac87-4344-b8a4-aaf009e53442,Advancing Medical Imaging with Language Models: A Journey from N-grams to ChatGPT,0.883215,"In this paper, we aimed to provide a review and tutorial for researchers in
the field of medical imaging using language models to improve their tasks at
hand. We began by providing an overview of the history and concepts of language
models, with a special focus on large language models. We then reviewed the
current literature on how language models are being used to improve medical
imaging, emphasizing different applications such as image captioning, report
generation, report classification, finding extraction, visual question
answering, interpretable diagnosis, and more for various modalities and organs.
The ChatGPT was specially highlighted for researchers to explore more potential
applications. We covered the potential benefits of accurate and efficient
language models for medical imaging analysis, including improving clinical
workflow efficiency, reducing diagnostic errors, and assisting healthcare
professionals in providing timely and accurate diagnoses. Overall, our goal was
to bridge the gap between language models and medical imaging and inspire new
ideas and innovations in this exciting area of research. We hope that this
review paper will serve as a useful resource for researchers in this field and
encourage further exploration of the possibilities of language models in
medical imaging.",None,-1
6a187201-07ee-4f8c-b3ba-99ea67232e66,CCpdf: Building a High Quality Corpus for Visually Rich Documents from Web Crawl Data,0.389912,"In recent years, the field of document understanding has progressed a lot. A
significant part of this progress has been possible thanks to the use of
language models pretrained on large amounts of documents. However, pretraining
corpora used in the domain of document understanding are single domain,
monolingual, or nonpublic. Our goal in this paper is to propose an efficient
pipeline for creating a big-scale, diverse, multilingual corpus of PDF files
from all over the Internet using Common Crawl, as PDF files are the most
canonical types of documents as considered in document understanding. We
analysed extensively all of the steps of the pipeline and proposed a solution
which is a trade-off between data quality and processing time. We also share a
CCpdf corpus in a form or an index of PDF files along with a script for
downloading them, which produces a collection useful for language model
pretraining. The dataset and tools published with this paper offer researchers
the opportunity to develop even better multilingual language models.",None,-1
233a5778-3894-4257-8551-c018bcaa07fb,Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,0.999393,"There is a growing interest in applying pre-trained large language models
(LLMs) to planning problems. However, methods that use LLMs directly as
planners are currently impractical due to several factors, including limited
correctness of plans, strong reliance on feedback from interactions with
simulators or even the actual environment, and the inefficiency in utilizing
human feedback. In this work, we introduce a novel alternative paradigm that
constructs an explicit world (domain) model in planning domain definition
language (PDDL) and then uses it to plan with sound domain-independent
planners. To address the fact that LLMs may not generate a fully functional
PDDL model initially, we employ LLMs as an interface between PDDL and sources
of corrective feedback, such as PDDL validators and humans. For users who lack
a background in PDDL, we show that LLMs can translate PDDL into natural
language and effectively encode corrective feedback back to the underlying
domain model. Our framework not only enjoys the correctness guarantee offered
by the external planners but also reduces human involvement by allowing users
to correct domain models at the beginning, rather than inspecting and
correcting (through interactive prompting) every generated plan as in previous
work. On two IPC domains and a Household domain that is more complicated than
commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be
leveraged to produce high-quality PDDL models for over 40 actions, and the
corrected PDDL models are then used to successfully solve 48 challenging
planning tasks. Resources, including the source code, are released at:
https://guansuns.github.io/pages/llm-dm.",None,-1
6cd98528-4445-41f2-b898-ad7bf43aa1a1,Scaling Sentence Embeddings with Large Language Models,0.982438,"Large language models (LLMs) have recently garnered significant interest.
With in-context learning, LLMs achieve impressive results in various natural
language tasks. However, the application of LLMs to sentence embeddings remains
an area of ongoing research. In this work, we propose an in-context
learning-based method aimed at improving sentence embeddings performance. Our
approach involves adapting the previous prompt-based representation method for
autoregressive models, constructing a demonstration set that enables LLMs to
perform in-context learning, and scaling up the LLMs to different model sizes.
Through extensive experiments, in-context learning enables LLMs to generate
high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve
performance comparable to current contrastive learning methods. By scaling
model size, we find scaling to more than tens of billion parameters harms the
performance on semantic textual similarity (STS) tasks. However, the largest
model outperforms other counterparts and achieves the new state-of-the-art
result on transfer tasks. We also fine-tune LLMs with current contrastive
learning approach, and the 2.7B OPT model, incorporating our prompt-based
method, surpasses the performance of 4.8B ST5, achieving the new
state-of-the-art results on STS tasks. Our code is available at
https://github.com/kongds/scaling_sentemb.",None,-1
723ee770-0d85-4168-b419-9fad1960e430,Attentive Graph Enhanced Region Representation Learning,0.523219,"Representing urban regions accurately and comprehensively is essential for
various urban planning and analysis tasks. Recently, with the expansion of the
city, modeling long-range spatial dependencies with multiple data sources plays
an important role in urban region representation. In this paper, we propose the
Attentive Graph Enhanced Region Representation Learning (ATGRL) model, which
aims to capture comprehensive dependencies from multiple graphs and learn rich
semantic representations of urban regions. Specifically, we propose a
graph-enhanced learning module to construct regional graphs by incorporating
mobility flow patterns, point of interests (POIs) functions, and check-in
semantics with noise filtering. Then, we present a multi-graph aggregation
module to capture both local and global spatial dependencies between regions by
integrating information from multiple graphs. In addition, we design a
dual-stage fusion module to facilitate information sharing between different
views and efficiently fuse multi-view representations for urban region
embedding using an improved linear attention mechanism. Finally, extensive
experiments on real-world datasets for three downstream tasks demonstrate the
superior performance of our model compared to state-of-the-art methods.",None,-1
686f9626-2866-4631-89e2-03675ab9be0a,"KBody: Towards general, robust, and aligned monocular whole-body estimation",0.387632,"KBody is a method for fitting a low-dimensional body model to an image. It
follows a predict-and-optimize approach, relying on data-driven model estimates
for the constraints that will be used to solve for the body's parameters.
Acknowledging the importance of high quality correspondences, it leverages
``virtual joints"" to improve fitting performance, disentangles the optimization
between the pose and shape parameters, and integrates asymmetric distance
fields to strike a balance in terms of pose and shape capturing capacity, as
well as pixel alignment. We also show that generative model inversion offers a
strong appearance prior that can be used to complete partial human images and
used as a building block for generalized and robust monocular body fitting.
Project page: https://zokin.github.io/KBody.",None,-1
844a9b74-5385-49e2-b81b-917b4cf50048,Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters,0.79919,"Identifying logical errors in complex, incomplete or even contradictory and
overall heterogeneous data like students' experimentation protocols is
challenging. Recognizing the limitations of current evaluation methods, we
investigate the potential of Large Language Models (LLMs) for automatically
identifying student errors and streamlining teacher assessments. Our aim is to
provide a foundation for productive, personalized feedback. Using a dataset of
65 student protocols, an Artificial Intelligence (AI) system based on the
GPT-3.5 and GPT-4 series was developed and tested against human raters. Our
results indicate varying levels of accuracy in error detection between the AI
system and human raters. The AI system can accurately identify many fundamental
student errors, for instance, the AI system identifies when a student is
focusing the hypothesis not on the dependent variable but solely on an expected
observation (acc. = 0.90), when a student modifies the trials in an ongoing
investigation (acc. = 1), and whether a student is conducting valid test trials
(acc. = 0.82) reliably. The identification of other, usually more complex
errors, like whether a student conducts a valid control trial (acc. = .60),
poses a greater challenge. This research explores not only the utility of AI in
educational settings, but also contributes to the understanding of the
capabilities of LLMs in error detection in inquiry-based learning like
experimentation.",None,-1
51390cab-2b87-4694-941c-f16d7cf4f0e7,LEGO-Prover: Neural Theorem Proving with Growing Libraries,0.999444,"Despite the success of large language models (LLMs), the task of theorem
proving still remains one of the hardest reasoning tasks that is far from being
fully solved. Prior methods using language models have demonstrated promising
results, but they still struggle to prove even middle school level theorems.
One common limitation of these methods is that they assume a fixed theorem
library during the whole theorem proving process. However, as we all know,
creating new useful theorems or even new theories is not only helpful but
crucial and necessary for advancing mathematics and proving harder and deeper
results. In this work, we present LEGO-Prover, which employs a growing skill
library containing verified lemmas as skills to augment the capability of LLMs
used in theorem proving. By constructing the proof modularly, LEGO-Prover
enables LLMs to utilize existing skills retrieved from the library and to
create new skills during the proving process. These skills are further evolved
(by prompting an LLM) to enrich the library on another scale. Modular and
reusable skills are constantly added to the library to enable tackling
increasingly intricate mathematical problems. Moreover, the learned library
further bridges the gap between human proofs and formal proofs by making it
easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass
rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%).
During the proving process, LEGO-Prover also manages to generate over 20,000
skills (theorems/lemmas) and adds them to the growing library. Our ablation
study indicates that these newly added skills are indeed helpful for proving
theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We
also release our code and all the generated skills.",None,-1
1a35fc2a-c2d3-4670-b39a-97e81a707c78,WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models,0.853202,"The open road poses many challenges to autonomous perception, including poor
visibility from extreme weather conditions. Models trained on good-weather
datasets frequently fail at detection in these out-of-distribution settings. To
aid adversarial robustness in perception, we introduce WEDGE (WEather images by
DALL-E GEneration): a synthetic dataset generated with a vision-language
generative model via prompting. WEDGE consists of 3360 images in 16 extreme
weather conditions manually annotated with 16513 bounding boxes, supporting
research in the tasks of weather classification and 2D object detection. We
have analyzed WEDGE from research standpoints, verifying its effectiveness for
extreme-weather autonomous perception. We establish baseline performance for
classification and detection with 53.87% test accuracy and 45.41 mAP. Most
importantly, WEDGE can be used to fine-tune state-of-the-art detectors,
improving SOTA performance on real-world weather benchmarks (such as DAWN) by
4.48 AP for well-generated classes like trucks. WEDGE has been collected under
OpenAI's terms of use and is released for public use under the CC BY-NC-SA 4.0
license. The repository for this work and dataset is available at
https://infernolia.github.io/WEDGE.",None,-1
024e0c15-1c53-4b4b-8039-d11a09bb1e23,Contextual Vision Transformers for Robust Representation Learning,0.393507,"We introduce Contextual Vision Transformers (ContextViT), a method designed
to generate robust image representations for datasets experiencing shifts in
latent factors across various groups. Derived from the concept of in-context
learning, ContextViT incorporates an additional context token to encapsulate
group-specific information. This integration allows the model to adjust the
image representation in accordance with the group-specific context.
Specifically, for a given input image, ContextViT maps images with identical
group membership into this context token, which is appended to the input image
tokens. Additionally, we introduce a context inference network to predict such
tokens on-the-fly, given a batch of samples from the group. This enables
ContextViT to adapt to new testing distributions during inference time. We
demonstrate the efficacy of ContextViT across a wide range of applications. In
supervised fine-tuning, we show that augmenting pre-trained ViTs with our
proposed context conditioning mechanism results in consistent improvements in
out-of-distribution generalization on iWildCam and FMoW. We also investigate
self-supervised representation learning with ContextViT. Our experiments on the
Camelyon17 pathology imaging benchmark and the JUMP-CP microscopy imaging
benchmark demonstrate that ContextViT excels in learning stable image
featurizations amidst distribution shift, consistently outperforming its ViT
counterpart.",None,-1
d036d2a1-f729-4062-9a0c-bf3af50429a4,Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation,0.33404,"Large pre-trained language models achieve impressive results across many
tasks. However, recent works point out that pre-trained language models may
memorize a considerable fraction of their training data, leading to the privacy
risk of information leakage. In this paper, we propose a method named Ethicist
for targeted training data extraction through loss smoothed soft prompting and
calibrated confidence estimation, investigating how to recover the suffix in
the training data when given a prefix. To elicit memorization in the attacked
model, we tune soft prompt embeddings while keeping the model fixed. We further
propose a smoothing loss that smooths the loss distribution of the suffix
tokens to make it easier to sample the correct suffix. In order to select the
most probable suffix from a collection of sampled suffixes and estimate the
prediction confidence, we propose a calibrated confidence estimation method,
which normalizes the confidence of the generated suffixes with a local
estimation. We show that Ethicist significantly improves the extraction
performance on a recently proposed public benchmark. We also investigate
several factors influencing the data extraction performance, including decoding
strategy, model scale, prefix length, and suffix length. Our code is available
at https://github.com/thu-coai/Targeted-Data-Extraction.",None,-1
41807d98-e3fa-443d-a883-5e636934e6b2,DDRF: Denoising Diffusion Model for Remote Sensing Image Fusion,0.770817,"Denosing diffusion model, as a generative model, has received a lot of
attention in the field of image generation recently, thanks to its powerful
generation capability. However, diffusion models have not yet received
sufficient research in the field of image fusion. In this article, we introduce
diffusion model to the image fusion field, treating the image fusion task as
image-to-image translation and designing two different conditional injection
modulation modules (i.e., style transfer modulation and wavelet modulation) to
inject coarse-grained style information and fine-grained high-frequency and
low-frequency information into the diffusion UNet, thereby generating fused
images. In addition, we also discussed the residual learning and the selection
of training objectives of the diffusion model in the image fusion task.
Extensive experimental results based on quantitative and qualitative
assessments compared with benchmarks demonstrates state-of-the-art results and
good generalization performance in image fusion tasks. Finally, it is hoped
that our method can inspire other works and gain insight into this field to
better apply the diffusion model to image fusion tasks. Code shall be released
for better reproducibility.",None,-1
1e760575-1898-4055-8ca3-853b7003431b,EdgeYOLO: An Edge-Real-Time Object Detector,0.474289,"This paper proposes an efficient, low-complexity and anchor-free object
detector based on the state-of-the-art YOLO framework, which can be implemented
in real time on edge computing platforms. We develop an enhanced data
augmentation method to effectively suppress overfitting during training, and
design a hybrid random loss function to improve the detection accuracy of small
objects. Inspired by FCOS, a lighter and more efficient decoupled head is
proposed, and its inference speed can be improved with little loss of
precision. Our baseline model can reach the accuracy of 50.6% AP50:95 and 69.8%
AP50 in MS COCO2017 dataset, 26.4% AP50:95 and 44.8% AP50 in VisDrone2019-DET
dataset, and it meets real-time requirements (FPS>=30) on edge-computing device
Nvidia Jetson AGX Xavier. We also designed lighter models with less parameters
for edge computing devices with lower computing power, which also show better
performances. Our source code, hyper-parameters and model weights are all
available at https://github.com/LSH9832/edgeyolo.",None,-1
b7d1fa1c-983b-4def-9d0c-1cc9081adf13,Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts,0.308938,"Reasoning methods, best exemplified by the well-known Chain-of-Thought (CoT),
empower the reasoning abilities of Large Language Models (LLMs) by eliciting
them to solve complex tasks in a step-by-step manner. Although they are
achieving significant success, the ability to deliver multi-step reasoning
remains limited to English because of the imbalance in the distribution of
pre-training data, which makes other languages a barrier. In this paper, we
propose Cross-lingual Tree-of-Thoughts (Cross-ToT), a method for aligning
Cross-lingual CoT reasoning across languages. The proposed method, through a
self-consistent cross-lingual prompting mechanism inspired by the
Tree-of-Thoughts approach, provides multi-step reasoning paths in different
languages that, during the steps, lead to the final solution. Experimental
evaluations show that our method significantly outperforms existing prompting
methods by reducing the number of interactions and achieving state-of-the-art
performance.",None,-1
6f73d2e5-ef7c-4ecf-87e1-2a503dec8e84,Divide and Prompt: Chain of Thought Prompting for Text-to-SQL,0.533647,"Chain-of-thought (CoT) prompting combined with large language models (LLMs)
have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a
critical semantic parsing task that converts natural language questions into
SQL statements, involving a complex reasoning process. However, there is little
work about using CoT prompting to activate LLM's reasoning capabilities on
Text-to-SQL tasks. In this work, we propose a new paradigm for prompting
Text-to-SQL tasks, called Divide-and-Prompt, which first divides the task into
subtasks, and then approach each subtask through CoT. We present 3
prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments
show that these prompts guide LLMs to generate Text-to-SQL with higher
execution accuracy.",None,-1
dd30c008-179b-41d8-8651-fee19cd1b472,Neural Spectro-polarimetric Fields,0.85507,"Modeling the spatial radiance distribution of light rays in a scene has been
extensively explored for applications, including view synthesis. Spectrum and
polarization, the wave properties of light, are often neglected due to their
integration into three RGB spectral bands and their non-perceptibility to human
vision. However, these properties are known to encompass substantial material
and geometric information about a scene. Here, we propose to model
spectro-polarimetric fields, the spatial Stokes-vector distribution of any
light ray at an arbitrary wavelength. We present Neural Spectro-polarimetric
Fields (NeSpoF), a neural representation that models the physically-valid
Stokes vector at given continuous variables of position, direction, and
wavelength. NeSpoF manages inherently noisy raw measurements, showcases memory
efficiency, and preserves physically vital signals - factors that are crucial
for representing the high-dimensional signal of a spectro-polarimetric field.
To validate NeSpoF, we introduce the first multi-view
hyperspectral-polarimetric image dataset, comprised of both synthetic and
real-world scenes. These were captured using our compact
hyperspectral-polarimetric imaging system, which has been calibrated for
robustness against system imperfections. We demonstrate the capabilities of
NeSpoF on diverse scenes.",None,-1
e09d4fe9-31e6-45c0-8c39-8227fefa1a4c,An Architecture for Deploying Reinforcement Learning in Industrial Environments,0.127867,"Industry 4.0 is driven by demands like shorter time-to-market, mass
customization of products, and batch size one production. Reinforcement
Learning (RL), a machine learning paradigm shown to possess a great potential
in improving and surpassing human level performance in numerous complex tasks,
allows coping with the mentioned demands. In this paper, we present an OPC UA
based Operational Technology (OT)-aware RL architecture, which extends the
standard RL setting, combining it with the setting of digital twins. Moreover,
we define an OPC UA information model allowing for a generalized plug-and-play
like approach for exchanging the RL agent used. In conclusion, we demonstrate
and evaluate the architecture, by creating a proof of concept. By means of
solving a toy example, we show that this architecture can be used to determine
the optimal policy using a real control system.",None,-1
c5063379-9489-4201-a395-e626940b2d34,Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning,0.341735,"Existing Knowledge Base Question Answering (KBQA) architectures are hungry
for annotated data, which make them costly and time-consuming to deploy. We
introduce the problem of few-shot transfer learning for KBQA, where the target
domain offers only a few labeled examples, but a large labeled training dataset
is available in a source domain. We propose a novel KBQA architecture called
FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers,
re-ranks using an LLM and uses this as input for LLM few-shot in-context
learning to generate logical forms. These are further refined using
execution-guided feedback. Experiments over multiple source-target KBQA pairs
of varying complexity show that FuSIC-KBQA significantly outperforms
adaptations of SoTA KBQA models for this setting. Additional experiments show
that FuSIC-KBQA also outperforms SoTA KBQA models in the in-domain setting when
training data is limited.",None,-1
cc04d121-ec2e-4462-b5aa-8e2daa53eee1,FlowIBR: Leveraging Pre-Training for Efficient Neural Image-Based Rendering of Dynamic Scenes,0.599058,"We introduce FlowIBR, a novel approach for efficient monocular novel view
synthesis of dynamic scenes. Existing techniques already show impressive
rendering quality but tend to focus on optimization within a single scene
without leveraging prior knowledge, resulting in long optimization times per
scene. FlowIBR circumvents this limitation by integrating a neural image-based
rendering method, pre-trained on a large corpus of widely available static
scenes, with a per-scene optimized scene flow field. Utilizing this flow field,
we bend the camera rays to counteract the scene dynamics, thereby presenting
the dynamic scene as if it were static to the rendering network. The proposed
method reduces per-scene optimization time by an order of magnitude, achieving
comparable rendering quality to existing methods -- all on a single
consumer-grade GPU.",None,-1
a2c585e0-540b-4cdc-9f15-cb3fe0fd684e,DReg-NeRF: Deep Registration for Neural Radiance Fields,0.436607,"Although Neural Radiance Fields (NeRF) is popular in the computer vision
community recently, registering multiple NeRFs has yet to gain much attention.
Unlike the existing work, NeRF2NeRF, which is based on traditional optimization
methods and needs human annotated keypoints, we propose DReg-NeRF to solve the
NeRF registration problem on object-centric scenes without human intervention.
After training NeRF models, our DReg-NeRF first extracts features from the
occupancy grid in NeRF. Subsequently, our DReg-NeRF utilizes a transformer
architecture with self-attention and cross-attention layers to learn the
relations between pairwise NeRF blocks. In contrast to state-of-the-art (SOTA)
point cloud registration methods, the decoupled correspondences are supervised
by surface fields without any ground truth overlapping labels. We construct a
novel view synthesis dataset with 1,700+ 3D objects obtained from Objaverse to
train our network. When evaluated on the test set, our proposed method beats
the SOTA point cloud registration methods by a large margin, with a mean
$\text{RPE}=9.67^{\circ}$ and a mean $\text{RTE}=0.038$.
  Our code is available at https://github.com/AIBluefisher/DReg-NeRF.",None,-1
54eeb0fb-0efe-4c18-93cf-c89783ea3afd,The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions,0.333486,"Recent progress in Large Language Models (LLMs) has produced models that
exhibit remarkable performance across a variety of NLP tasks. However, it
remains unclear whether the existing focus of NLP research accurately captures
the genuine requirements of human users. This paper provides a comprehensive
analysis of the divergence between current NLP research and the needs of
real-world NLP applications via a large-scale collection of user-GPT
conversations. We analyze a large-scale collection of real user queries to GPT.
We compare these queries against existing NLP benchmark tasks and identify a
significant gap between the tasks that users frequently request from LLMs and
the tasks that are commonly studied in academic research. For example, we find
that tasks such as ``design'' and ``planning'' are prevalent in user
interactions but are largely neglected or different from traditional NLP
benchmarks. We investigate these overlooked tasks, dissect the practical
challenges they pose, and provide insights toward a roadmap to make LLMs better
aligned with user needs.",None,-1
cef3e81c-ad83-4504-85ff-e4940bd13abe,ModelScope Text-to-Video Technical Report,0.996967,"This paper introduces ModelScopeT2V, a text-to-video synthesis model that
evolves from a text-to-image synthesis model (i.e., Stable Diffusion).
ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame
generation and smooth movement transitions. The model could adapt to varying
frame numbers during training and inference, rendering it suitable for both
image-text and video-text datasets. ModelScopeT2V brings together three
components (i.e., VQGAN, a text encoder, and a denoising UNet), totally
comprising 1.7 billion parameters, in which 0.5 billion parameters are
dedicated to temporal capabilities. The model demonstrates superior performance
over state-of-the-art methods across three evaluation metrics. The code and an
online demo are available at
\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.",None,-1
4f495207-777d-4cdc-92d3-779e71f9aa57,OVO: Open-Vocabulary Occupancy,0.527836,"Semantic occupancy prediction aims to infer dense geometry and semantics of
surroundings for an autonomous agent to operate safely in the 3D environment.
Existing occupancy prediction methods are almost entirely trained on
human-annotated volumetric data. Although of high quality, the generation of
such 3D annotations is laborious and costly, restricting them to a few specific
object categories in the training dataset. To address this limitation, this
paper proposes Open Vocabulary Occupancy (OVO), a novel approach that allows
semantic occupancy prediction of arbitrary classes but without the need for 3D
annotations during training. Keys to our approach are (1) knowledge
distillation from a pre-trained 2D open-vocabulary segmentation model to the 3D
occupancy network, and (2) pixel-voxel filtering for high-quality training data
generation. The resulting framework is simple, compact, and compatible with
most state-of-the-art semantic occupancy prediction models. On NYUv2 and
SemanticKITTI datasets, OVO achieves competitive performance compared to
supervised semantic occupancy prediction approaches. Furthermore, we conduct
extensive analyses and ablation studies to offer insights into the design of
the proposed framework. Our code is publicly available at
https://github.com/dzcgaara/OVO.",None,-1
f7850965-402e-4ce9-8250-2d505c337918,EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding,0.379022,"With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI),
large-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed.
However, most current research is built on resources derived from third-person
video action recognition. This inherent domain gap between first- and
third-person action videos, which have not been adequately addressed before,
makes current Ego-HOI suboptimal. This paper rethinks and proposes a new
framework as an infrastructure to advance Ego-HOI recognition by Probing,
Curation and Adaption (EgoPCA). We contribute comprehensive pre-train sets,
balanced test sets and a new baseline, which are complete with a
training-finetuning strategy. With our new framework, we not only achieve
state-of-the-art performance on Ego-HOI benchmarks but also build several new
and effective mechanisms and settings to advance further research. We believe
our data and the findings will pave a new way for Ego-HOI understanding. Code
and data are available at https://mvig-rhos.com/ego_pca",None,-1
5ee6e792-b5c1-4bc4-b66e-653dc66c4728,A Template Is All You Meme,0.654088,"Memes are a modern form of communication and meme templates possess a base
semantics that is customizable by whomever posts it on social media. Machine
learning systems struggle with memes, which is likely due to such systems
having insufficient context to understand memes, as there is more to memes than
the obvious image and text. Here, to aid understanding of memes, we release a
knowledge base of memes and information found on www.knowyourmeme.com, which we
call the Know Your Meme Knowledge Base (KYMKB), composed of more than 54,000
images. The KYMKB includes popular meme templates, examples of each template,
and detailed information about the template. We hypothesize that meme templates
can be used to inject models with the context missing from previous approaches.
To test our hypothesis, we create a non-parametric majority-based classifier,
which we call Template-Label Counter (TLC). We find TLC more effective than or
competitive with fine-tuned baselines. To demonstrate the power of meme
templates and the value of both our knowledge base and method, we conduct
thorough classification experiments and exploratory data analysis in the
context of five meme analysis tasks.",None,-1
45dad6c7-cf35-4fd4-8855-68a72e925204,A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI),0.808963,"Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.",None,-1
fbb2feee-18dd-432c-855b-af24c486ee41,Towards Reliable Rare Category Analysis on Graphs via Individual Calibration,0.387298,"Rare categories abound in a number of real-world networks and play a pivotal
role in a variety of high-stakes applications, including financial fraud
detection, network intrusion detection, and rare disease diagnosis. Rare
category analysis (RCA) refers to the task of detecting, characterizing, and
comprehending the behaviors of minority classes in a highly-imbalanced data
distribution. While the vast majority of existing work on RCA has focused on
improving the prediction performance, a few fundamental research questions
heretofore have received little attention and are less explored: How confident
or uncertain is a prediction model in rare category analysis? How can we
quantify the uncertainty in the learning process and enable reliable rare
category analysis?
  To answer these questions, we start by investigating miscalibration in
existing RCA methods. Empirical results reveal that state-of-the-art RCA
methods are mainly over-confident in predicting minority classes and
under-confident in predicting majority classes. Motivated by the observation,
we propose a novel individual calibration framework, named CALIRARE, for
alleviating the unique challenges of RCA, thus enabling reliable rare category
analysis. In particular, to quantify the uncertainties in RCA, we develop a
node-level uncertainty quantification algorithm to model the overlapping
support regions with high uncertainty; to handle the rarity of minority classes
in miscalibration calculation, we generalize the distribution-based calibration
metric to the instance level and propose the first individual calibration
measurement on graphs named Expected Individual Calibration Error (EICE). We
perform extensive experimental evaluations on real-world datasets, including
rare category characterization and model calibration tasks, which demonstrate
the significance of our proposed framework.",None,-1
56ed26e3-623c-49da-bd03-d2f4faca19bf,TADA! Text to Animatable Digital Avatars,1.0,"We introduce TADA, a simple-yet-effective approach that takes textual
descriptions and produces expressive 3D avatars with high-quality geometry and
lifelike textures, that can be animated and rendered with traditional graphics
pipelines. Existing text-based character generation methods are limited in
terms of geometry and texture quality, and cannot be realistically animated due
to inconsistent alignment between the geometry and the texture, particularly in
the face region. To overcome these limitations, TADA leverages the synergy of a
2D diffusion model and an animatable parametric body model. Specifically, we
derive an optimizable high-resolution body model from SMPL-X with 3D
displacements and a texture map, and use hierarchical rendering with score
distillation sampling (SDS) to create high-quality, detailed, holistic 3D
avatars from text. To ensure alignment between the geometry and texture, we
render normals and RGB images of the generated character and exploit their
latent embeddings in the SDS training process. We further introduce various
expression parameters to deform the generated character during training,
ensuring that the semantics of our generated character remain consistent with
the original SMPL-X model, resulting in an animatable character. Comprehensive
evaluations demonstrate that TADA significantly surpasses existing approaches
on both qualitative and quantitative measures. TADA enables creation of
large-scale digital character assets that are ready for animation and
rendering, while also being easily editable through natural language. The code
will be public for research purposes.",None,-1
f7512f67-211a-4c4d-9c57-3c4c443ac661,Dual Associated Encoder for Face Restoration,0.175146,"Restoring facial details from low-quality (LQ) images has remained a
challenging problem due to its ill-posedness induced by various degradations in
the wild. The existing codebook prior mitigates the ill-posedness by leveraging
an autoencoder and learned codebook of high-quality (HQ) features, achieving
remarkable quality. However, existing approaches in this paradigm frequently
depend on a single encoder pre-trained on HQ data for restoring HQ images,
disregarding the domain gap between LQ and HQ images. As a result, the encoding
of LQ inputs may be insufficient, resulting in suboptimal performance. To
tackle this problem, we propose a novel dual-branch framework named DAEFR. Our
method introduces an auxiliary LQ branch that extracts crucial information from
the LQ inputs. Additionally, we incorporate association training to promote
effective synergy between the two branches, enhancing code prediction and
output quality. We evaluate the effectiveness of DAEFR on both synthetic and
real-world datasets, demonstrating its superior performance in restoring facial
details. Project page: https://liagm.github.io/DAEFR/",None,-1
218693f4-0294-42ba-990c-b48d1a9274b2,A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness,0.0973849,"Cracks play a crucial role in assessing the safety and durability of
manufactured buildings. However, the long and sharp topological features and
complex background of cracks make the task of crack segmentation extremely
challenging. In this paper, we propose a novel convolutional-transformer
network based on encoder-decoder architecture to solve this challenge.
Particularly, we designed a Dilated Residual Block (DRB) and a Boundary
Awareness Module (BAM). The DRB pays attention to the local detail of cracks
and adjusts the feature dimension for other blocks as needed. And the BAM
learns the boundary features from the dilated crack label. Furthermore, the DRB
is combined with a lightweight transformer that captures global information to
serve as an effective encoder. Experimental results show that the proposed
network performs better than state-of-the-art algorithms on two typical
datasets. Datasets, code, and trained models are available for research at
https://github.com/HqiTao/CT-crackseg.",None,-1
e3e54e61-4122-4b5f-bda5-b3a33d8fc455,TryOnDiffusion: A Tale of Two UNets,0.859621,"Given two images depicting a person and a garment worn by another person, our
goal is to generate a visualization of how the garment might look on the input
person. A key challenge is to synthesize a photorealistic detail-preserving
visualization of the garment, while warping the garment to accommodate a
significant body pose and shape change across the subjects. Previous methods
either focus on garment detail preservation without effective pose and shape
variation, or allow try-on with the desired shape and pose but lack garment
details. In this paper, we propose a diffusion-based architecture that unifies
two UNets (referred to as Parallel-UNet), which allows us to preserve garment
details and warp the garment for significant pose and body change in a single
network. The key ideas behind Parallel-UNet include: 1) garment is warped
implicitly via a cross attention mechanism, 2) garment warp and person blend
happen as part of a unified process as opposed to a sequence of two separate
tasks. Experimental results indicate that TryOnDiffusion achieves
state-of-the-art performance both qualitatively and quantitatively.",None,-1
6427d9db-6a1b-4699-b150-1f49a375cdcf,Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,0.80379,"Intracerebral hemorrhage (ICH) is the second most common and deadliest form
of stroke. Despite medical advances, predicting treat ment outcomes for ICH
remains a challenge. This paper proposes a novel prognostic model that utilizes
both imaging and tabular data to predict treatment outcome for ICH. Our model
is trained on observational data collected from non-randomized controlled
trials, providing reliable predictions of treatment success. Specifically, we
propose to employ a variational autoencoder model to generate a low-dimensional
prognostic score, which can effectively address the selection bias resulting
from the non-randomized controlled trials. Importantly, we develop a
variational distributions combination module that combines the information from
imaging data, non-imaging clinical data, and treatment assignment to accurately
generate the prognostic score. We conducted extensive experiments on a
real-world clinical dataset of intracerebral hemorrhage. Our proposed method
demonstrates a substantial improvement in treatment outcome prediction compared
to existing state-of-the-art approaches. Code is available at
https://github.com/med-air/TOP-GPM",None,-1
4d731077-5035-444b-b02c-aac7432e5ac6,Enhancing ResNet Image Classification Performance by using Parameterized Hypercomplex Multiplication,0.142535,"Recently, many deep networks have introduced hypercomplex and related
calculations into their architectures. In regard to convolutional networks for
classification, these enhancements have been applied to the convolution
operations in the frontend to enhance accuracy and/or reduce the parameter
requirements while maintaining accuracy. Although these enhancements have been
applied to the convolutional frontend, it has not been studied whether adding
hypercomplex calculations improves performance when applied to the densely
connected backend. This paper studies ResNet architectures and incorporates
parameterized hypercomplex multiplication (PHM) into the backend of residual,
quaternion, and vectormap convolutional neural networks to assess the effect.
We show that PHM does improve classification accuracy performance on several
image datasets, including small, low-resolution CIFAR 10/100 and large
high-resolution ImageNet and ASL, and can achieve state-of-the-art accuracy for
hypercomplex networks.",None,-1
86519ba7-ff50-4ccf-94b7-d0132df651df,Denoising Diffusion for 3D Hand Pose Estimation from Images,0.424571,"Hand pose estimation from a single image has many applications. However,
approaches to full 3D body pose estimation are typically trained on day-to-day
activities or actions. As such, detailed hand-to-hand interactions are poorly
represented, especially during motion. We see this in the failure cases of
techniques such as OpenPose or MediaPipe. However, accurate hand pose
estimation is crucial for many applications where the global body motion is
less important than accurate hand pose estimation.
  This paper addresses the problem of 3D hand pose estimation from monocular
images or sequences. We present a novel end-to-end framework for 3D hand
regression that employs diffusion models that have shown excellent ability to
capture the distribution of data for generative purposes. Moreover, we enforce
kinematic constraints to ensure realistic poses are generated by incorporating
an explicit forward kinematic layer as part of the network. The proposed model
provides state-of-the-art performance when lifting a 2D single-hand image to
3D. However, when sequence data is available, we add a Transformer module over
a temporal window of consecutive frames to refine the results, overcoming
jittering and further increasing accuracy.
  The method is quantitatively and qualitatively evaluated showing
state-of-the-art robustness, generalization, and accuracy on several different
datasets.",None,-1
67bfca9c-e8ec-4d43-81b5-51f502c0e409,Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems,0.945103,"When learning task-oriented dialogue (ToD) agents, reinforcement learning
(RL) techniques can naturally be utilized to train dialogue strategies to
achieve user-specific goals. Prior works mainly focus on adopting advanced RL
techniques to train the ToD agents, while the design of the reward function is
not well studied. This paper aims at answering the question of how to
efficiently learn and leverage a reward function for training end-to-end (E2E)
ToD agents. Specifically, we introduce two generalized objectives for
reward-function learning, inspired by the classical learning-to-rank
literature. Further, we utilize the learned reward function to guide the
training of the E2E ToD agent. With the proposed techniques, we achieve
competitive results on the E2E response-generation task on the Multiwoz 2.0
dataset. Source code and checkpoints are publicly released at
https://github.com/Shentao-YANG/Fantastic_Reward_ICLR2023.",None,-1
45f70977-33cc-4179-a08b-539c5cdd1c97,Human Trajectory Forecasting with Explainable Behavioral Uncertainty,0.419619,"Human trajectory forecasting helps to understand and predict human behaviors,
enabling applications from social robots to self-driving cars, and therefore
has been heavily investigated. Most existing methods can be divided into
model-free and model-based methods. Model-free methods offer superior
prediction accuracy but lack explainability, while model-based methods provide
explainability but cannot predict well. Combining both methodologies, we
propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM,
where a behavior SDE model is combined with Bayesian neural networks (BNNs).
While the NNs provide superior predictive power, the SDE offers strong
explainability with quantifiable uncertainty in behavior and observation. We
show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy,
compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to
drastically different scenes with different environments and crowd densities (~
20 times higher than the testing data). Finally, BNSP-SFM can provide
predictions with confidence to better explain potential causes of behaviors.
The code will be released upon acceptance.",None,-1
8161bfae-097a-4c8a-80f1-0f7f96e88c2a,"General Policies, Subgoal Structure, and Planning Width",0.0761464,"It has been observed that many classical planning domains with atomic goals
can be solved by means of a simple polynomial exploration procedure, called IW,
that runs in time exponential in the problem width, which in these cases is
bounded and small. Yet, while the notion of width has become part of
state-of-the-art planning algorithms such as BFWS, there is no good explanation
for why so many benchmark domains have bounded width when atomic goals are
considered. In this work, we address this question by relating bounded width
with the existence of general optimal policies that in each planning instance
are represented by tuples of atoms of bounded size. We also define the notions
of (explicit) serializations and serialized width that have a broader scope as
many domains have a bounded serialized width but no bounded width. Such
problems are solved non-optimally in polynomial time by a suitable variant of
the Serialized IW algorithm. Finally, the language of general policies and the
semantics of serializations are combined to yield a simple, meaningful, and
expressive language for specifying serializations in compact form in the form
of sketches, which can be used for encoding domain control knowledge by hand or
for learning it from small examples. Sketches express general problem
decompositions in terms of subgoals, and sketches of bounded width express
problem decompositions that can be solved in polynomial time.",None,-1
7ebd676f-7359-498c-b348-0b4f152a6a89,Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction,0.686121,"Knowledge graph construction (KGC) is a multifaceted undertaking involving
the extraction of entities, relations, and events. Traditionally, large
language models (LLMs) have been viewed as solitary task-solving agents in this
complex landscape. However, this paper challenges this paradigm by introducing
a novel framework, CooperKGC. Departing from the conventional approach,
CooperKGC establishes a collaborative processing network, assembling a KGC
collaboration team capable of concurrently addressing entity, relation, and
event extraction tasks. Our experiments unequivocally demonstrate that
fostering collaboration and information interaction among diverse agents within
CooperKGC yields superior results compared to individual cognitive processes
operating in isolation. Importantly, our findings reveal that the collaboration
facilitated by CooperKGC enhances knowledge selection, correction, and
aggregation capabilities across multiple rounds of interactions.",None,-1
3dd646fb-9f7e-4e31-bdca-10193b4cfdb8,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,0.377661,"During the past decade, the Unmanned-Aerial-Vehicles (UAVs) have attracted
increasing attention due to their flexible, extensive, and dynamic
space-sensing capabilities. The volume of video captured by UAVs is
exponentially growing along with the increased bitrate generated by the
advancement of the sensors mounted on UAVs, bringing new challenges for
on-device UAV storage and air-ground data transmission. Most existing video
compression schemes were designed for natural scenes without consideration of
specific texture and view characteristics of UAV videos. In this work, we first
contribute a detailed analysis of the current state of the field of UAV video
coding. Then we propose to establish a novel task for learned UAV video coding
and construct a comprehensive and systematic benchmark for such a task, present
a thorough review of high quality UAV video datasets and benchmarks, and
contribute extensive rate-distortion efficiency comparison of learned and
conventional codecs after. Finally, we discuss the challenges of encoding UAV
videos. It is expected that the benchmark will accelerate the research and
development in video coding on drone platforms.",None,-1
7fd4de29-e6f4-498a-a357-494f1568020b,"Sea Ice Extraction via Remote Sensed Imagery: Algorithms, Datasets, Applications and Challenges",0.777271,"The deep learning, which is a dominating technique in artificial
intelligence, has completely changed the image understanding over the past
decade. As a consequence, the sea ice extraction (SIE) problem has reached a
new era. We present a comprehensive review of four important aspects of SIE,
including algorithms, datasets, applications, and the future trends. Our review
focuses on researches published from 2016 to the present, with a specific focus
on deep learning-based approaches in the last five years. We divided all
relegated algorithms into 3 categories, including classical image segmentation
approach, machine learning-based approach and deep learning-based methods. We
reviewed the accessible ice datasets including SAR-based datasets, the
optical-based datasets and others. The applications are presented in 4 aspects
including climate research, navigation, geographic information systems (GIS)
production and others. It also provides insightful observations and inspiring
future research directions.",None,-1
740d9031-1297-4b8a-9baf-a039aa842aad,Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems,0.748593,"In this article, a benchmark for real-world bin packing problems is proposed.
This dataset consists of 12 instances of varying levels of complexity regarding
size (with the number of packages ranging from 38 to 53) and user-defined
requirements. In fact, several real-world-oriented restrictions were taken into
account to build these instances: i) item and bin dimensions, ii) weight
restrictions, iii) affinities among package categories iv) preferences for
package ordering and v) load balancing. Besides the data, we also offer an own
developed Python script for the dataset generation, coined Q4RealBPP-DataGen.
The benchmark was initially proposed to evaluate the performance of quantum
solvers. Therefore, the characteristics of this set of instances were designed
according to the current limitations of quantum devices. Additionally, the
dataset generator is included to allow the construction of general-purpose
benchmarks. The data introduced in this article provides a baseline that will
encourage quantum computing researchers to work on real-world bin packing
problems.",None,-1
5e6c34cd-9b06-452f-9566-1389fc70ec4c,Automatic Answerability Evaluation for Question Generation,0.126658,"Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed
for natural language generation (NLG) tasks, are based on measuring the n-gram
overlap between the generated and reference text. These simple metrics may be
insufficient for more complex tasks, such as question generation (QG), which
requires generating questions that are answerable by the reference answers.
Developing a more sophisticated automatic evaluation metric, thus, remains an
urgent problem in QG research. This work proposes PMAN (Prompting-based Metric
on ANswerability), a novel automatic evaluation metric to assess whether the
generated questions are answerable by the reference answers for the QG tasks.
Extensive experiments demonstrate that its evaluation results are reliable and
align with human evaluations. We further apply our metric to evaluate the
performance of QG models, which shows that our metric complements conventional
metrics. Our implementation of a GPT-based QG model achieves state-of-the-art
performance in generating answerable questions.",None,-1
f50c414a-922d-4668-9ee6-27eac91d4839,Revisiting Large Language Models as Zero-shot Relation Extractors,0.882441,"Relation extraction (RE) consistently involves a certain degree of labeled or
unlabeled data even if under zero-shot setting. Recent studies have shown that
large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt, which provides the possibility of extracting
relations from text without any data and parameter tuning. This work focuses on
the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.
On the one hand, we analyze the drawbacks of existing RE prompts and attempt to
incorporate recent prompt techniques such as chain-of-thought (CoT) to improve
zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a
simple prompt recursively using LLMs to transform RE inputs to the effective
question answering (QA) format. On the other hand, we conduct comprehensive
experiments on various benchmarks and settings to investigate the capabilities
of LLMs on zero-shot RE. Specifically, we have the following findings: (i)
\textsc{SumAsk} consistently and significantly improves LLMs performance on
different model sizes, benchmarks and settings; (ii) Zero-shot prompting with
ChatGPT achieves competitive or superior results compared with zero-shot and
fully supervised methods; (iii) LLMs deliver promising performance in
extracting overlapping relations; (iv) The performance varies greatly regarding
different relations. Different from small language models, LLMs are effective
in handling challenge none-of-the-above (NoTA) relation.",None,-1
474ff3bd-81c3-407c-a20c-48d672c516bd,Affordance Grounding from Demonstration Video to Target Image,0.574253,"Humans excel at learning from expert demonstrations and solving their own
problems. To equip intelligent robots and assistants, such as AR glasses, with
this ability, it is essential to ground human hand interactions (i.e.,
affordances) from demonstration videos and apply them to a target image like a
user's AR glass view. The video-to-image affordance grounding task is
challenging due to (1) the need to predict fine-grained affordances, and (2)
the limited training data, which inadequately covers video-image discrepancies
and negatively impacts grounding. To tackle them, we propose Affordance
Transformer (Afformer), which has a fine-grained transformer-based decoder that
gradually refines affordance grounding. Moreover, we introduce Mask Affordance
Hand (MaskAHand), a self-supervised pre-training technique for synthesizing
video-image data and simulating context changes, enhancing affordance grounding
across video-image discrepancies. Afformer with MaskAHand pre-training achieves
state-of-the-art performance on multiple benchmarks, including a substantial
37% improvement on the OPRA dataset. Code is made available at
https://github.com/showlab/afformer.",None,-1
98eb63f4-6aa9-4f90-b3b6-621850c84e26,Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers,0.970653,"Abstracts derived from biomedical literature possess distinct domain-specific
characteristics, including specialised writing styles and biomedical
terminologies, which necessitate a deep understanding of the related
literature. As a result, existing language models struggle to generate
technical summaries that are on par with those produced by biomedical experts,
given the absence of domain-specific background knowledge. This paper aims to
enhance the performance of language models in biomedical abstractive
summarisation by aggregating knowledge from external papers cited within the
source article. We propose a novel attention-based citation aggregation model
that integrates domain-specific knowledge from citation papers, allowing neural
networks to generate summaries by leveraging both the paper content and
relevant knowledge from citation papers. Furthermore, we construct and release
a large-scale biomedical summarisation dataset that serves as a foundation for
our research. Extensive experiments demonstrate that our model outperforms
state-of-the-art approaches and achieves substantial improvements in
abstractive biomedical text summarisation.",None,-1
2eb9eb8f-36de-418f-a9fc-932dda7450aa,Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation,0.388172,"Using a vocabulary that is shared across languages is common practice in
Multilingual Neural Machine Translation (MNMT). In addition to its simple
design, shared tokens play an important role in positive knowledge transfer,
assuming that shared tokens refer to similar meanings across languages.
However, when word overlap is small, especially due to different writing
systems, transfer is inhibited. In this paper, we define word-level information
transfer pathways via word equivalence classes and rely on graph networks to
fuse word embeddings across languages. Our experiments demonstrate the
advantages of our approach: 1) embeddings of words with similar meanings are
better aligned across languages, 2) our method achieves consistent BLEU
improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less
than 1.0\% additional trainable parameters are required with a limited increase
in computational costs, while inference time remains identical to the baseline.
We release the codebase to the community.",None,-1
4e00127b-6c93-4b15-8876-0443df6a13d3,The Big Data Myth: Using Diffusion Models for Dataset Generation to Train Deep Detection Models,0.415661,"Despite the notable accomplishments of deep object detection models, a major
challenge that persists is the requirement for extensive amounts of training
data. The process of procuring such real-world data is a laborious undertaking,
which has prompted researchers to explore new avenues of research, such as
synthetic data generation techniques. This study presents a framework for the
generation of synthetic datasets by fine-tuning pretrained stable diffusion
models. The synthetic datasets are then manually annotated and employed for
training various object detection models. These detectors are evaluated on a
real-world test set of 331 images and compared against a baseline model that
was trained on real-world images. The results of this study reveal that the
object detection models trained on synthetic data perform similarly to the
baseline model. In the context of apple detection in orchards, the average
precision deviation with the baseline ranges from 0.09 to 0.12. This study
illustrates the potential of synthetic data generation techniques as a viable
alternative to the collection of extensive training data for the training of
deep models.",None,-1
2aecbe1b-2734-4f04-b485-8710a6e19aef,MRecGen: Multimodal Appropriate Reaction Generator,0.815906,"Verbal and non-verbal human reaction generation is a challenging task, as
different reactions could be appropriate for responding to the same behaviour.
This paper proposes the first multiple and multimodal (verbal and nonverbal)
appropriate human reaction generation framework that can generate appropriate
and realistic human-style reactions (displayed in the form of synchronised
text, audio and video streams) in response to an input user behaviour. This
novel technique can be applied to various human-computer interaction scenarios
by generating appropriate virtual agent/robot behaviours. Our demo is available
at \url{https://github.com/SSYSteve/MRecGen}.",None,-1
d05fff8a-67ca-47ec-956c-3f070d66fd85,Discourse Structures Guided Fine-grained Propaganda Identification,0.375043,"Propaganda is a form of deceptive narratives that instigate or mislead the
public, usually with a political purpose. In this paper, we aim to identify
propaganda in political news at two fine-grained levels: sentence-level and
token-level. We observe that propaganda content is more likely to be embedded
in sentences that attribute causality or assert contrast to nearby sentences,
as well as seen in opinionated evaluation, speculation and discussions of
future expectation. Hence, we propose to incorporate both local and global
discourse structures for propaganda discovery and construct two teacher models
for identifying PDTB-style discourse relations between nearby sentences and
common discourse roles of sentences in a news article respectively. We further
devise two methods to incorporate the two types of discourse structures for
propaganda identification by either using teacher predicted probabilities as
additional features or soliciting guidance in a knowledge distillation
framework. Experiments on the benchmark dataset demonstrate that leveraging
guidance from discourse structures can significantly improve both precision and
recall of propaganda content identification.",None,-1
cfe03d8b-5651-49a0-b173-8ed0a146aa8b,Class Attention Transfer Based Knowledge Distillation,0.846371,"Previous knowledge distillation methods have shown their impressive
performance on model compression tasks, however, it is hard to explain how the
knowledge they transferred helps to improve the performance of the student
network. In this work, we focus on proposing a knowledge distillation method
that has both high interpretability and competitive performance. We first
revisit the structure of mainstream CNN models and reveal that possessing the
capacity of identifying class discriminative regions of input is critical for
CNN to perform classification. Furthermore, we demonstrate that this capacity
can be obtained and enhanced by transferring class activation maps. Based on
our findings, we propose class attention transfer based knowledge distillation
(CAT-KD). Different from previous KD methods, we explore and present several
properties of the knowledge transferred by our method, which not only improve
the interpretability of CAT-KD but also contribute to a better understanding of
CNN. While having high interpretability, CAT-KD achieves state-of-the-art
performance on multiple benchmarks. Code is available at:
https://github.com/GzyAftermath/CAT-KD.",None,-1
33251d88-aa35-424f-8444-6b3a30fb9dd9,Sketch-an-Anchor: Sub-epoch Fast Model Adaptation for Zero-shot Sketch-based Image Retrieval,0.0514094,"Sketch-an-Anchor is a novel method to train state-of-the-art Zero-shot
Sketch-based Image Retrieval (ZSSBIR) models in under an epoch. Most studies
break down the problem of ZSSBIR into two parts: domain alignment between
images and sketches, inherited from SBIR, and generalization to unseen data,
inherent to the zero-shot protocol. We argue one of these problems can be
considerably simplified and re-frame the ZSSBIR problem around the
already-stellar yet underexplored Zero-shot Image-based Retrieval performance
of off-the-shelf models. Our fast-converging model keeps the single-domain
performance while learning to extract similar representations from sketches. To
this end we introduce our Semantic Anchors -- guiding embeddings learned from
word-based semantic spaces and features from off-the-shelf models -- and
combine them with our novel Anchored Contrastive Loss. Empirical evidence shows
we can achieve state-of-the-art performance on all benchmark datasets while
training for 100x less iterations than other methods.",None,-1
276288c6-51a9-41b3-b4d9-87ec4358bff2,Tied-Augment: Controlling Representation Similarity Improves Data Augmentation,0.194263,"Data augmentation methods have played an important role in the recent advance
of deep learning models, and have become an indispensable component of
state-of-the-art models in semi-supervised, self-supervised, and supervised
training for vision. Despite incurring no additional latency at test time, data
augmentation often requires more epochs of training to be effective. For
example, even the simple flips-and-crops augmentation requires training for
more than 5 epochs to improve performance, whereas RandAugment requires more
than 90 epochs. We propose a general framework called Tied-Augment, which
improves the efficacy of data augmentation in a wide range of applications by
adding a simple term to the loss that can control the similarity of
representations under distortions. Tied-Augment can improve state-of-the-art
methods from data augmentation (e.g. RandAugment, mixup), optimization (e.g.
SAM), and semi-supervised learning (e.g. FixMatch). For example,
Tied-RandAugment can outperform RandAugment by 2.0% on ImageNet. Notably, using
Tied-Augment, data augmentation can be made to improve generalization even when
training for a few epochs and when fine-tuning. We open source our code at
https://github.com/ekurtulus/tied-augment/tree/main.",None,-1
6344456f-89a0-497f-8a7a-d861a031a039,PRODIGy: a PROfile-based DIalogue Generation dataset,0.0406199,"Providing dialogue agents with a profile representation can improve their
consistency and coherence, leading to better conversations. However, current
profile-based dialogue datasets for training such agents contain either
explicit profile representations that are simple and dialogue-specific, or
implicit representations that are difficult to collect. In this work, we
propose a unified framework in which we bring together both standard and more
sophisticated profile representations by creating a new resource where each
dialogue is aligned with all possible speaker representations such as
communication style, biographies, and personality. This framework allows to
test several baselines built using generative language models with several
profile configurations. The automatic evaluation shows that profile-based
models have better generalisation capabilities than models trained on dialogues
only, both in-domain and cross-domain settings. These results are consistent
for fine-tuned models and instruction-based LLMs. Additionally, human
evaluation demonstrates a clear preference for generations consistent with both
profile and context. Finally, to account for possible privacy concerns, all
experiments are done under two configurations: inter-character and
intra-character. In the former, the LM stores the information about the
character in its internal representation, while in the latter, the LM does not
retain any personal information but uses it only at inference time.",None,-1
679ca2ec-bbfb-464a-b11b-7aa0c1f4a807,An Explainable Collaborative Dialogue System using a Theory of Mind,0.0787908,"Eva is a neuro-symbolic domain-independent multimodal collaborative dialogue
system that takes seriously that the purpose of task-oriented dialogue is to
assist the user. To do this, the system collaborates by inferring their
intentions and plans, detects obstacles to success, finds plans to overcome
them or to achieve higher-level goals, and plans its actions, including speech
acts, to help users accomplish those goals. In doing so, the system maintains
and reasons with its own declaratively-specified beliefs, goals and intentions,
and explicitly reasons about those of its user. Because Eva can track different
users' mental states, it can engage multiple agents in multi-party dialogues.
Reasoning is accomplished with a modal Horn-clause meta-interpreter that
enables computable inference within the subset of logic implemented. The system
employs both hierarchical and backward-chaining planning, operating over a rich
modal logic-based knowledge and action representation. The planning and
reasoning subsystems obey the principles of persistent goals and intentions
including: 1) The formation and decomposition of intentions to perform complex
actions, 2) the conditions under which persistent goals and intentions can be
given up, and 3) persistent goal and intention revision using the relativizing
formulas that are created during the planning process. The system treats its
speech acts just like its other actions. This general approach enables Eva to
plan a variety of speech acts, including requests, informs, questions,
confirmations, offers, acceptances, and emotive expressions. Because the
dialogue engine is a planner, as the dialogue proceeds, the system can flexibly
generate, execute, and potentially repair its plans using physical, digital,
and speech actions. Importantly, Eva can explain its utterances because it has
created a plan that caused it to utter them.",None,-1
dd49e829-4c07-4cfe-8d9e-f7e66f26d063,Dual-Alignment Pre-training for Cross-lingual Sentence Embedding,0.808211,"Recent studies have shown that dual encoder models trained with the
sentence-level translation ranking task are effective methods for cross-lingual
sentence embedding. However, our research indicates that token-level alignment
is also crucial in multilingual scenarios, which has not been fully explored
previously. Based on our findings, we propose a dual-alignment pre-training
(DAP) framework for cross-lingual sentence embedding that incorporates both
sentence-level and token-level alignment. To achieve this, we introduce a novel
representation translation learning (RTL) task, where the model learns to use
one-side contextualized token representation to reconstruct its translation
counterpart. This reconstruction objective encourages the model to embed
translation information into the token representation. Compared to other
token-level alignment methods such as translation language modeling, RTL is
more suitable for dual encoder architectures and is computationally efficient.
Extensive experiments on three sentence-level cross-lingual benchmarks
demonstrate that our approach can significantly improve sentence embedding. Our
code is available at https://github.com/ChillingDream/DAP.",None,-1
11874e53-3dda-426c-aa80-e1d152090a06,4DHumanOutfit: a multi-subject 4D dataset of human motion sequences in varying outfits exhibiting large displacements,0.355813,"This work presents 4DHumanOutfit, a new dataset of densely sampled
spatio-temporal 4D human motion data of different actors, outfits and motions.
The dataset is designed to contain different actors wearing different outfits
while performing different motions in each outfit. In this way, the dataset can
be seen as a cube of data containing 4D motion sequences along 3 axes with
identity, outfit and motion. This rich dataset has numerous potential
applications for the processing and creation of digital humans, e.g. augmented
reality, avatar creation and virtual try on. 4DHumanOutfit is released for
research purposes at https://kinovis.inria.fr/4dhumanoutfit/. In addition to
image data and 4D reconstructions, the dataset includes reference solutions for
each axis. We present independent baselines along each axis that demonstrate
the value of these reference solutions for evaluation tasks.",None,-1
8378eb07-b7fa-4b4a-b9d4-dbb34d4a8850,On the Importance of Signer Overlap for Sign Language Detection,0.109127,"Sign language detection, identifying if someone is signing or not, is
becoming crucially important for its applications in remote conferencing
software and for selecting useful sign data for training sign language
recognition or translation tasks. We argue that the current benchmark data sets
for sign language detection estimate overly positive results that do not
generalize well due to signer overlap between train and test partitions. We
quantify this with a detailed analysis of the effect of signer overlap on
current sign detection benchmark data sets. Comparing accuracy with and without
overlap on the DGS corpus and Signing in the Wild, we observed a relative
decrease in accuracy of 4.17% and 6.27%, respectively. Furthermore, we propose
new data set partitions that are free of overlap and allow for more realistic
performance assessment. We hope this work will contribute to improving the
accuracy and generalization of sign language detection systems.",None,-1
80896675-26fd-4d5e-a0f0-cb3c076155cd,A Frustratingly Easy Plug-and-Play Detection-and-Reasoning Module for Chinese Spelling Check,0.94389,"In recent years, Chinese Spelling Check (CSC) has been greatly improved by
designing task-specific pre-training methods or introducing auxiliary tasks,
which mostly solve this task in an end-to-end fashion. In this paper, we
propose to decompose the CSC workflow into detection, reasoning, and searching
subtasks so that the rich external knowledge about the Chinese language can be
leveraged more directly and efficiently. Specifically, we design a
plug-and-play detection-and-reasoning module that is compatible with existing
SOTA non-autoregressive CSC models to further boost their performance. We find
that the detection-and-reasoning module trained for one model can also benefit
other models. We also study the primary interpretability provided by the task
decomposition. Extensive experiments and detailed analyses demonstrate the
effectiveness and competitiveness of the proposed module.",None,-1
f48d87d9-adad-4def-ac54-add49ff90c75,INO at Factify 2: Structure Coherence based Multi-Modal Fact Verification,0.586227,"This paper describes our approach to the multi-modal fact verification
(FACTIFY) challenge at AAAI2023. In recent years, with the widespread use of
social media, fake news can spread rapidly and negatively impact social
security. Automatic claim verification becomes more and more crucial to combat
fake news. In fact verification involving multiple modal data, there should be
a structural coherence between claim and document. Therefore, we proposed a
structure coherence-based multi-modal fact verification scheme to classify fake
news. Our structure coherence includes the following four aspects: sentence
length, vocabulary similarity, semantic similarity, and image similarity.
Specifically, CLIP and Sentence BERT are combined to extract text features, and
ResNet50 is used to extract image features. In addition, we also extract the
length of the text as well as the lexical similarity. Then the features were
concatenated and passed through the random forest classifier. Finally, our
weighted average F1 score has reached 0.8079, achieving 2nd place in FACTIFY2.",None,-1
4a70d449-37ec-4548-9b6e-8ac812851d82,Batch Prompting: Efficient Inference with Large Language Model APIs,0.359726,"Performing inference on large volumes of samples with large language models
(LLMs) can be computationally and financially costly in industry and real-world
use. We propose batch prompting, a simple yet effective prompting approach that
enables the LLM to run inference in batches, instead of one sample at a time.
Our method reduces both token and time costs while retaining downstream
performance. We theoretically demonstrate that under a few-shot in-context
learning setting, the inference costs decrease almost inverse linearly with the
number of samples in each batch. We extensively validate the effectiveness of
batch prompting on ten datasets across commonsense QA, arithmetic reasoning,
and NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch)
reduces the LLM (Codex) inference token and time costs while achieving better
or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5
and GPT-4, we show the benefits of batch prompting also hold. Further analysis
shows that the number of samples in each batch and the complexity of tasks
affect its performance. Moreover, batch prompting can be applied across
different reasoning methods using LLMs. Our code can be found at the site
https://github.com/xlang-ai/batch-prompting.",None,-1
1a84e851-e127-41f5-b7e7-5b1198e627da,OpenMix: Exploring Outlier Samples for Misclassification Detection,0.395229,"Reliable confidence estimation for deep neural classifiers is a challenging
yet fundamental requirement in high-stakes applications. Unfortunately, modern
deep neural networks are often overconfident for their erroneous predictions.
In this work, we exploit the easily available outlier samples, i.e., unlabeled
samples coming from non-target classes, for helping detect misclassification
errors. Particularly, we find that the well-known Outlier Exposure, which is
powerful in detecting out-of-distribution (OOD) samples from unknown classes,
does not provide any gain in identifying misclassification errors. Based on
these observations, we propose a novel method called OpenMix, which
incorporates open-world knowledge by learning to reject uncertain
pseudo-samples generated via outlier transformation. OpenMix significantly
improves confidence reliability under various scenarios, establishing a strong
and unified framework for detecting both misclassified samples from known
classes and OOD samples from unknown classes. The code is publicly available at
https://github.com/Impression2805/OpenMix.",None,-1
06377f8b-a474-4e06-b9b7-b411f6d8d71a,Local Implicit Normalizing Flow for Arbitrary-Scale Image Super-Resolution,0.656719,"Flow-based methods have demonstrated promising results in addressing the
ill-posed nature of super-resolution (SR) by learning the distribution of
high-resolution (HR) images with the normalizing flow. However, these methods
can only perform a predefined fixed-scale SR, limiting their potential in
real-world applications. Meanwhile, arbitrary-scale SR has gained more
attention and achieved great progress. Nonetheless, previous arbitrary-scale SR
methods ignore the ill-posed problem and train the model with per-pixel L1
loss, leading to blurry SR outputs. In this work, we propose ""Local Implicit
Normalizing Flow"" (LINF) as a unified solution to the above problems. LINF
models the distribution of texture details under different scaling factors with
normalizing flow. Thus, LINF can generate photo-realistic HR images with rich
texture details in arbitrary scale factors. We evaluate LINF with extensive
experiments and show that LINF achieves the state-of-the-art perceptual quality
compared with prior arbitrary-scale SR methods.",None,-1
a3d5423c-87f1-49ef-a73f-7aea5d1bb56b,Bridging the Gap between Structural and Semantic Similarity in Diverse Planning,0.235938,"Diverse planning is the problem of finding multiple plans for a given problem
specification, which is at the core of many real-world applications. For
example, diverse planning is a critical piece for the efficiency of plan
recognition systems when dealing with noisy and missing observations. Providing
diverse solutions can also benefit situations where constraints are too
expensive or impossible to model. Current diverse planners operate by
generating multiple plans and then applying a selection procedure to extract
diverse solutions using a similarity metric. Generally, current similarity
metrics only consider the structural properties of the given plans. We argue
that this approach is a limitation that sometimes prevents such metrics from
capturing why two plans differ. In this work, we propose two new
domain-independent metrics which are able to capture relevant information on
the difference between two given plans from a domain-dependent viewpoint. We
showcase their utility in various situations where the currently used metrics
fail to capture the similarity between plans, failing to capture some
structural symmetries.",None,-1
39dd6415-30fa-4bb7-b40b-cefe0113345a,Beyond AUROC & co. for evaluating out-of-distribution detection performance,0.496121,"While there has been a growing research interest in developing
out-of-distribution (OOD) detection methods, there has been comparably little
discussion around how these methods should be evaluated. Given their relevance
for safe(r) AI, it is important to examine whether the basis for comparing OOD
detection methods is consistent with practical needs. In this work, we take a
closer look at the go-to metrics for evaluating OOD detection, and question the
approach of exclusively reducing OOD detection to a binary classification task
with little consideration for the detection threshold. We illustrate the
limitations of current metrics (AUROC & its friends) and propose a new metric -
Area Under the Threshold Curve (AUTC), which explicitly penalizes poor
separation between ID and OOD samples. Scripts and data are available at
https://github.com/glhr/beyond-auroc",None,-1
36ab12a9-abd5-445e-9059-0dd95ace1107,Prompt Engineering a Prompt Engineer,0.637058,"Prompt engineering is a challenging yet crucial task for optimizing the
performance of large language models on customized tasks. It requires complex
reasoning to examine the model's errors, hypothesize what is missing or
misleading in the current prompt, and communicate the task with clarity. While
recent works indicate that large language models can be meta-prompted to
perform automatic prompt engineering, we argue that their potential is limited
due to insufficient guidance for complex reasoning in the meta-prompt. We fill
this gap by infusing into the meta-prompt three key components: detailed
descriptions, context specification, and a step-by-step reasoning template. The
resulting method, named PE2, showcases remarkable versatility across diverse
language tasks. It finds prompts that outperform ""let's think step by step"" by
6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on
counterfactual tasks by 6.9%. Further, we show that PE2 can make targeted
prompt edits, rectify erroneous prompts, and induce multi-step plans for
complex tasks.",None,-1
58dd213b-7d01-4846-95dc-770d3f34739c,Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations,0.2913,"Computer simulations offer a robust toolset for exploring complex systems
across various disciplines. A particularly impactful approach within this realm
is Agent-Based Modeling (ABM), which harnesses the interactions of individual
agents to emulate intricate system dynamics. ABM's strength lies in its
bottom-up methodology, illuminating emergent phenomena by modeling the
behaviors of individual components of a system. Yet, ABM has its own set of
challenges, notably its struggle with modeling natural language instructions
and common sense in mathematical equations or rules. This paper seeks to
transcend these boundaries by integrating Large Language Models (LLMs) like GPT
into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based
Modeling (SABM). Building upon the concept of smart agents -- entities
characterized by their intelligence, adaptability, and computation ability --
we explore in the direction of utilizing LLM-powered agents to simulate
real-world scenarios with increased nuance and realism. In this comprehensive
exploration, we elucidate the state of the art of ABM, introduce SABM's
potential and methodology, and present three case studies (source codes
available at https://github.com/Roihn/SABM), demonstrating the SABM methodology
and validating its effectiveness in modeling real-world systems. Furthermore,
we cast a vision towards several aspects of the future of SABM, anticipating a
broader horizon for its applications. Through this endeavor, we aspire to
redefine the boundaries of computer simulations, enabling a more profound
understanding of complex systems.",None,-1
acc923e0-6f74-4222-ac8a-4824cd51ec9f,Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation,0.618403,"Simultaneous speech translation is an essential communication task difficult
for humans whereby a translation is generated concurrently with oncoming speech
inputs. For such a streaming task, transformers using block processing to break
an input sequence into segments have achieved state-of-the-art performance at a
reduced cost. Current methods to allow information to propagate across
segments, including left context and memory banks, have faltered as they are
both insufficient representations and unnecessarily expensive to compute. In
this paper, we propose an Implicit Memory Transformer that implicitly retains
memory through a new left context method, removing the need to explicitly
represent memory with memory banks. We generate the left context from the
attention output of the previous segment and include it in the keys and values
of the current segment's attention calculation. Experiments on the MuST-C
dataset show that the Implicit Memory Transformer provides a substantial
speedup on the encoder forward pass with nearly identical translation quality
when compared with the state-of-the-art approach that employs both left context
and memory banks.",None,-1
e0ccc4c7-48e3-4553-af99-3cfe95f76ed3,Conformal Nucleus Sampling,0.234533,"Language models generate text based on successively sampling the next word. A
decoding procedure based on nucleus (top-$p$) sampling chooses from the
smallest possible set of words whose cumulative probability exceeds the
probability $p$. In this work, we assess whether a top-$p$ set is indeed
aligned with its probabilistic meaning in various linguistic contexts. We
employ conformal prediction, a calibration procedure that focuses on the
construction of minimal prediction sets according to a desired confidence
level, to calibrate the parameter $p$ as a function of the entropy of the next
word distribution. We find that OPT models are overconfident, and that
calibration shows a moderate inverse scaling with model size.",None,-1
5fb25f68-6e8f-4338-9ffd-6aeb926ffe9b,Collision Avoidance Detour for Multi-Agent Trajectory Forecasting,0.430642,"We present our approach, Collision Avoidance Detour (CAD), which won the 3rd
place award in the 2023 Waymo Open Dataset Challenge - Sim Agents, held at the
2023 CVPR Workshop on Autonomous Driving. To satisfy the motion prediction
factorization requirement, we partition all the valid objects into three
mutually exclusive sets: Autonomous Driving Vehicle (ADV),
World-tracks-to-predict, and World-others. We use different motion models to
forecast their future trajectories independently. Furthermore, we also apply
collision avoidance detour resampling, additive Gaussian noise, and
velocity-based heading estimation to improve the realism of our simulation
result.",None,-1
5a31d39d-bcb7-4f0b-8e3b-226aba9fd3c9,Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances,0.411361,"To achieve state-of-the-art performance, one still needs to train NER models
on large-scale, high-quality annotated data, an asset that is both costly and
time-intensive to accumulate. In contrast, real-world applications often resort
to massive low-quality labeled data through non-expert annotators via
crowdsourcing and external knowledge bases via distant supervision as a
cost-effective alternative. However, these annotation methods result in noisy
labels, which in turn lead to a notable decline in performance. Hence, we
propose to denoise the noisy NER data with guidance from a small set of clean
instances. Along with the main NER model we train a discriminator model and use
its outputs to recalibrate the sample weights. The discriminator is capable of
detecting both span and category errors with different discriminative prompts.
Results on public crowdsourcing and distant supervision datasets show that the
proposed method can consistently improve performance with a small guidance set.",None,-1
39602d73-f903-49b2-a77a-2dc23354acd7,Insights into Classifying and Mitigating LLMs' Hallucinations,0.129063,"The widespread adoption of large language models (LLMs) across diverse AI
applications is proof of the outstanding achievements obtained in several
tasks, such as text mining, text generation, and question answering. However,
LLMs are not exempt from drawbacks. One of the most concerning aspects regards
the emerging problematic phenomena known as ""Hallucinations"". They manifest in
text generation systems, particularly in question-answering systems reliant on
LLMs, potentially resulting in false or misleading information propagation.
This paper delves into the underlying causes of AI hallucination and elucidates
its significance in artificial intelligence. In particular, Hallucination
classification is tackled over several tasks (Machine Translation, Question and
Answer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and
Visual Question Answer). Additionally, we explore potential strategies to
mitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our
research addresses this critical issue within the HeReFaNMi (Health-Related
Fake News Mitigation) project, generously supported by NGI Search, dedicated to
combating Health-Related Fake News dissemination on the Internet. This
endeavour represents a concerted effort to safeguard the integrity of
information dissemination in an age of evolving AI technologies.",None,-1
a086024a-d248-40d9-91ea-6f9ce4917ace,"Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild",0.333586,"Recently, reinforcement learning has become a promising and polular solution
for robot legged locomotion. Compared to model-based control, reinforcement
learning based controllers can achieve better robustness against uncertainties
of environments through sim-to-real learning. However, the corresponding
learned gaits are in general overly conservative and unatural. In this paper,
we propose a new framework for learning robust, agile and natural legged
locomotion skills over challenging terrain. We incorporate an adversarial
training branch based on real animal locomotion data upon a teacher-student
training pipeline for robust sim-to-real transfer. Empirical results on both
simulation and real world of a quadruped robot demonstrate that our proposed
algorithm enables robustly traversing challenging terrains such as stairs,
rocky ground and slippery floor with only proprioceptive perception. Meanwhile,
the gaits are more agile, natural, and energy efficient compared to the
baselines. Both qualitative and quantitative results are presented in this
paper.",None,-1
ed01fb4e-eea4-4686-b844-bf17ea9fc679,RoboBEV: Towards Robust Bird's Eye View Perception under Corruptions,0.922749,"The recent advances in camera-based bird's eye view (BEV) representation
exhibit great potential for in-vehicle 3D perception. Despite the substantial
progress achieved on standard benchmarks, the robustness of BEV algorithms has
not been thoroughly examined, which is critical for safe operations. To bridge
this gap, we introduce RoboBEV, a comprehensive benchmark suite that
encompasses eight distinct corruptions, including Bright, Dark, Fog, Snow,
Motion Blur, Color Quant, Camera Crash, and Frame Lost. Based on it, we
undertake extensive evaluations across a wide range of BEV-based models to
understand their resilience and reliability. Our findings indicate a strong
correlation between absolute performance on in-distribution and
out-of-distribution datasets. Nonetheless, there are considerable variations in
relative performance across different approaches. Our experiments further
demonstrate that pre-training and depth-free BEV transformation has the
potential to enhance out-of-distribution robustness. Additionally, utilizing
long and rich temporal information largely helps with robustness. Our findings
provide valuable insights for designing future BEV models that can achieve both
accuracy and robustness in real-world deployments.",None,-1
c60a3518-662a-42e2-9621-1e6f55f72560,Revisiting Event-based Video Frame Interpolation,0.295783,"Dynamic vision sensors or event cameras provide rich complementary
information for video frame interpolation. Existing state-of-the-art methods
follow the paradigm of combining both synthesis-based and warping networks.
However, few of those methods fully respect the intrinsic characteristics of
events streams. Given that event cameras only encode intensity changes and
polarity rather than color intensities, estimating optical flow from events is
arguably more difficult than from RGB information. We therefore propose to
incorporate RGB information in an event-guided optical flow refinement
strategy. Moreover, in light of the quasi-continuous nature of the time signals
provided by event cameras, we propose a divide-and-conquer strategy in which
event-based intermediate frame synthesis happens incrementally in multiple
simplified stages rather than in a single, long stage. Extensive experiments on
both synthetic and real-world datasets show that these modifications lead to
more reliable and realistic intermediate frame results than previous video
frame interpolation methods. Our findings underline that a careful
consideration of event characteristics such as high temporal density and
elevated noise benefits interpolation accuracy.",None,-1
97553cf7-7e7a-4b4d-9268-af0ab25ba819,Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs,0.40485,"Despite advancements in LLMs, knowledge-based reasoning remains a
longstanding issue due to the fragility of knowledge recall and inference.
Existing methods primarily encourage LLMs to autonomously plan and solve
problems or to extensively sample reasoning chains without addressing the
conceptual and inferential fallacies. Attempting to alleviate inferential
fallacies and drawing inspiration from multi-agent collaboration, we present a
framework to increase faithfulness and causality for knowledge-based reasoning.
Specifically, we propose to employ multiple intelligent agents (i.e., reasoners
and an evaluator) to work collaboratively in a reasoning-and-consensus paradigm
for elevated reasoning faithfulness. The reasoners focus on providing solutions
with human-like causality to solve open-domain problems. On the other hand, the
\textit{evaluator} agent scrutinizes if a solution is deducible from a
non-causal perspective and if it still holds when challenged by a
counterfactual candidate. According to the extensive and comprehensive
evaluations on a variety of knowledge reasoning tasks (e.g., science question
answering and commonsense reasoning), our framework outperforms all compared
state-of-the-art approaches by large margins.",None,-1
96568ecb-f5d5-41da-910b-11ff0d8b4108,The Obscure Limitation of Modular Multilingual Language Models,0.44278,"We expose the limitation of modular multilingual language models (MLMs) in
multilingual inference scenarios with unknown languages. Existing evaluations
of modular MLMs exclude the involvement of language identification (LID)
modules, which obscures the performance of real-case multilingual scenarios of
modular MLMs. In this work, we showcase the effect of adding LID on the
multilingual evaluation of modular MLMs and provide discussions for closing the
performance gap of caused by the pipelined approach of LID and modular MLMs.",None,-1
703c0341-cb93-464e-a684-cc5ef0151ab6,Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints,0.522158,"In safety-critical robotic tasks, potential failures must be reduced, and
multiple constraints must be met, such as avoiding collisions, limiting energy
consumption, and maintaining balance. Thus, applying safe reinforcement
learning (RL) in such robotic tasks requires to handle multiple constraints and
use risk-averse constraints rather than risk-neutral constraints. To this end,
we propose a trust region-based safe RL algorithm for multiple constraints
called a safe distributional actor-critic (SDAC). Our main contributions are as
follows: 1) introducing a gradient integration method to manage infeasibility
issues in multi-constrained problems, ensuring theoretical convergence, and 2)
developing a TD($\lambda$) target distribution to estimate risk-averse
constraints with low biases. We evaluate SDAC through extensive experiments
involving multi- and single-constrained robotic tasks. While maintaining high
scores, SDAC shows 1.93 times fewer steps to satisfy all constraints in
multi-constrained tasks and 1.78 times fewer constraint violations in
single-constrained tasks compared to safe RL baselines. Code is available at:
https://github.com/rllab-snu/Safe-Distributional-Actor-Critic.",None,-1
e3691610-d54d-4d48-a684-ed9a0123556b,DiffuSum: Generation Enhanced Extractive Summarization with Diffusion,0.998015,"Extractive summarization aims to form a summary by directly extracting
sentences from the source document. Existing works mostly formulate it as a
sequence labeling problem by making individual sentence label predictions. This
paper proposes DiffuSum, a novel paradigm for extractive summarization, by
directly generating the desired summary sentence representations with diffusion
models and extracting sentences based on sentence representation matching. In
addition, DiffuSum jointly optimizes a contrastive sentence encoder with a
matching loss for sentence representation alignment and a multi-class
contrastive loss for representation diversity. Experimental results show that
DiffuSum achieves the new state-of-the-art extractive results on CNN/DailyMail
with ROUGE scores of $44.83/22.56/40.56$. Experiments on the other two datasets
with different summary lengths also demonstrate the effectiveness of DiffuSum.
The strong performance of our framework shows the great potential of adapting
generative models for extractive summarization. To encourage more following
work in the future, we have released our codes at
\url{https://github.com/hpzhang94/DiffuSum}",None,-1
89926a8e-d9b1-46f4-9eec-875aa72fe099,Principal-Agent Reward Shaping in MDPs,0.342994,"Principal-agent problems arise when one party acts on behalf of another,
leading to conflicts of interest. The economic literature has extensively
studied principal-agent problems, and recent work has extended this to more
complex scenarios such as Markov Decision Processes (MDPs). In this paper, we
further explore this line of research by investigating how reward shaping under
budget constraints can improve the principal's utility. We study a two-player
Stackelberg game where the principal and the agent have different reward
functions, and the agent chooses an MDP policy for both players. The principal
offers an additional reward to the agent, and the agent picks their policy
selfishly to maximize their reward, which is the sum of the original and the
offered reward. Our results establish the NP-hardness of the problem and offer
polynomial approximation algorithms for two classes of instances: Stochastic
trees and deterministic decision processes with a finite horizon.",None,-1
5a120acf-fbee-443b-925e-298528f0ef1c,Compositional Probabilistic and Causal Inference using Tractable Circuit Models,0.461829,"Probabilistic circuits (PCs) are a class of tractable probabilistic models,
which admit efficient inference routines depending on their structural
properties. In this paper, we introduce md-vtrees, a novel structural
formulation of (marginal) determinism in structured decomposable PCs, which
generalizes previously proposed classes such as probabilistic sentential
decision diagrams. Crucially, we show how mdvtrees can be used to derive
tractability conditions and efficient algorithms for advanced inference queries
expressed as arbitrary compositions of basic probabilistic operations, such as
marginalization, multiplication and reciprocals, in a sound and generalizable
manner. In particular, we derive the first polytime algorithms for causal
inference queries such as backdoor adjustment on PCs. As a practical
instantiation of the framework, we propose MDNets, a novel PC architecture
using md-vtrees, and empirically demonstrate their application to causal
inference.",None,-1
f9a9a1c3-a06f-4b57-b512-ec38e06e7de1,NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images,0.896954,"We study the problem of reconstructing 3D feature curves of an object from a
set of calibrated multi-view images. To do so, we learn a neural implicit field
representing the density distribution of 3D edges which we refer to as Neural
Edge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based
rendering loss where a 2D edge map is rendered at a given view and is compared
to the ground-truth edge map extracted from the image of that view. The
rendering-based differentiable optimization of NEF fully exploits 2D edge
detection, without needing a supervision of 3D edges, a 3D geometric operator
or cross-view edge correspondence. Several technical designs are devised to
ensure learning a range-limited and view-independent NEF for robust edge
extraction. The final parametric 3D curves are extracted from NEF with an
iterative optimization method. On our benchmark with synthetic data, we
demonstrate that NEF outperforms existing state-of-the-art methods on all
metrics. Project page: https://yunfan1202.github.io/NEF/.",None,-1
67846cba-1158-4743-8bbd-4aad5057ab01,Leveraging Auxiliary Domain Parallel Data in Intermediate Task Fine-tuning for Low-resource Translation,0.198025,"NMT systems trained on Pre-trained Multilingual Sequence-Sequence (PMSS)
models flounder when sufficient amounts of parallel data is not available for
fine-tuning. This specifically holds for languages missing/under-represented in
these models. The problem gets aggravated when the data comes from different
domains. In this paper, we show that intermediate-task fine-tuning (ITFT) of
PMSS models is extremely beneficial for domain-specific NMT, especially when
target domain data is limited/unavailable and the considered languages are
missing or under-represented in the PMSS model. We quantify the domain-specific
results variations using a domain-divergence test, and show that ITFT can
mitigate the impact of domain divergence to some extent.",None,-1
79b164a3-4b8f-4116-848b-5e6456917862,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,0.742348,"Predicting the pose of objects from a single image is an important but
difficult computer vision problem. Methods that predict a single point estimate
do not predict the pose of objects with symmetries well and cannot represent
uncertainty. Alternatively, some works predict a distribution over orientations
in $\mathrm{SO}(3)$. However, training such models can be computation- and
sample-inefficient. Instead, we propose a novel mapping of features from the
image domain to the 3D rotation manifold. Our method then leverages
$\mathrm{SO}(3)$ equivariant layers, which are more sample efficient, and
outputs a distribution over rotations that can be sampled at arbitrary
resolution. We demonstrate the effectiveness of our method at object
orientation prediction, and achieve state-of-the-art performance on the popular
PASCAL3D+ dataset. Moreover, we show that our method can model complex object
symmetries, without any modifications to the parameters or loss function. Code
is available at https://dmklee.github.io/image2sphere.",None,-1
c0fda8e9-4c2b-4b56-9506-1b08e17089d8,Randomized Adversarial Style Perturbations for Domain Generalization,0.0796691,"We propose a novel domain generalization technique, referred to as Randomized
Adversarial Style Perturbation (RASP), which is motivated by the observation
that the characteristics of each domain are captured by the feature statistics
corresponding to style. The proposed algorithm perturbs the style of a feature
in an adversarial direction towards a randomly selected class, and makes the
model learn against being misled by the unexpected styles observed in unseen
target domains. While RASP is effective to handle domain shifts, its naive
integration into the training procedure might degrade the capability of
learning knowledge from source domains because it has no restriction on the
perturbations of representations. This challenge is alleviated by Normalized
Feature Mixup (NFM), which facilitates the learning of the original features
while achieving robustness to perturbed representations via their mixup during
training. We evaluate the proposed algorithm via extensive experiments on
various benchmarks and show that our approach improves domain generalization
performance, especially in large-scale benchmarks.",None,-1
efeb2ce9-8f50-47ff-af1e-9bed64f94293,Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models,0.133321,"Large Language Models (LLMs) have achieved remarkable success in reasoning
tasks with the development of prompting methods. However, existing prompting
approaches cannot reuse insights of solving similar problems and suffer from
accumulated errors in multi-step reasoning, since they prompt LLMs to reason
\textit{from scratch}. To address these issues, we propose
\textbf{\textit{Thought Propagation} (TP)}, which explores the analogous
problems and leverages their solutions to enhance the complex reasoning ability
of LLMs. These analogous problems are related to the input one, with reusable
solutions and problem-solving strategies. Thus, it is promising to propagate
insights of solving previous analogous problems to inspire new problem-solving.
To achieve this, TP first prompts LLMs to propose and solve a set of analogous
problems that are related to the input one. Then, TP reuses the results of
analogous problems to directly yield a new solution or derive a
knowledge-intensive plan for execution to amend the initial solution obtained
from scratch. TP is compatible with existing prompting approaches, allowing
plug-and-play generalization and enhancement in a wide range of tasks without
much labor in task-specific prompt engineering. Experiments across three
challenging tasks demonstrate TP enjoys a substantial improvement over the
baselines by an average of 12\% absolute increase in finding the optimal
solutions in Shortest-path Reasoning, 13\% improvement of human preference in
Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent
Planning.",None,-1
1998e226-af56-4d50-89f2-096a6dc0a73c,Edge-aware Consistent Stereo Video Depth Estimation,0.269646,"Video depth estimation is crucial in various applications, such as scene
reconstruction and augmented reality. In contrast to the naive method of
estimating depths from images, a more sophisticated approach uses temporal
information, thereby eliminating flickering and geometrical inconsistencies. We
propose a consistent method for dense video depth estimation; however, unlike
the existing monocular methods, ours relates to stereo videos. This technique
overcomes the limitations arising from the monocular input. As a benefit of
using stereo inputs, a left-right consistency loss is introduced to improve the
performance. Besides, we use SLAM-based camera pose estimation in the process.
To address the problem of depth blurriness during test-time training (TTT), we
present an edge-preserving loss function that improves the visibility of fine
details while preserving geometrical consistency. We show that our edge-aware
stereo video model can accurately estimate the dense depth maps.",None,-1
08ef63ea-d2b0-4668-9642-0b59edf04cdc,LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models,0.25709,"Large language models (LLMs) have been applied in various applications due to
their astonishing capabilities. With advancements in technologies such as
chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed
to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of
tokens. To accelerate model inference and reduce cost, this paper presents
LLMLingua, a coarse-to-fine prompt compression method that involves a budget
controller to maintain semantic integrity under high compression ratios, a
token-level iterative compression algorithm to better model the interdependence
between compressed contents, and an instruction tuning based method for
distribution alignment between language models. We conduct experiments and
analysis over four datasets from different scenarios, i.e., GSM8K, BBH,
ShareGPT, and Arxiv-March23; showing that the proposed approach yields
state-of-the-art performance and allows for up to 20x compression with little
performance loss. Our code is available at https://aka.ms/LLMLingua.",None,-1
eaf18c06-503a-4a68-92d0-3d2a61231c8d,GDDS: Pulmonary Bronchioles Segmentation with Group Deep Dense Supervision,0.734749,"Airway segmentation, especially bronchioles segmentation, is an important but
challenging task because distal bronchus are sparsely distributed and of a fine
scale. Existing neural networks usually exploit sparse topology to learn the
connectivity of bronchioles and inefficient shallow features to capture such
high-frequency information, leading to the breakage or missed detection of
individual thin branches. To address these problems, we contribute a new
bronchial segmentation method based on Group Deep Dense Supervision (GDDS) that
emphasizes fine-scale bronchioles segmentation in a simple-but-effective
manner. First, Deep Dense Supervision (DDS) is proposed by constructing local
dense topology skillfully and implementing dense topological learning on a
specific shallow feature layer. GDDS further empowers the shallow features with
better perception ability to detect bronchioles, even the ones that are not
easily discernible to the naked eye. Extensive experiments on the BAS benchmark
dataset have shown that our method promotes the network to have a high
sensitivity in capturing fine-scale branches and outperforms state-of-the-art
methods by a large margin (+12.8 % in BD and +8.8 % in TD) while only
introducing a small number of extra parameters.",None,-1
28033602-1634-4430-864a-2f730c15d2b7,Digital Twin Applications in Urban Logistics: An Overview,0.650761,"Urban traffic attributed to commercial and industrial transportation is
observed to largely affect living standards in cities due to external effects
pertaining to pollution and congestion. In order to counter this, smart cities
deploy technological tools to achieve sustainability. Such tools include
Digital Twins (DT)s which are virtual replicas of real-life physical systems.
Research suggests that DTs can be very beneficial in how they control a
physical system by constantly optimizing its performance. The concept has been
extensively studied in other technology-driven industries like manufacturing.
However, little work has been done with regards to their application in urban
logistics. In this paper, we seek to provide a framework by which DTs could be
easily adapted to urban logistics networks. To do this, we provide a
characterization of key factors in urban logistics for dynamic decision-making.
We also survey previous research on DT applications in urban logistics as we
found that a holistic overview is lacking. Using this knowledge in combination
with the characterization, we produce a conceptual model that describes the
ontology, learning capabilities and optimization prowess of an urban logistics
digital twin through its quantitative models. We finish off with a discussion
on potential research benefits and limitations based on previous research and
our practical experience.",None,-1
889b7024-bae6-4b51-a589-dd5300b3da7b,ColonMapper: topological mapping and localization for colonoscopy,0.0883493,"We propose a topological mapping and localization system able to operate on
real human colonoscopies, despite significant shape and illumination changes.
The map is a graph where each node codes a colon location by a set of real
images, while edges represent traversability between nodes. For close-in-time
images, where scene changes are minor, place recognition can be successfully
managed with the recent transformers-based local feature matching algorithms.
However, under long-term changes -- such as different colonoscopies of the same
patient -- feature-based matching fails. To address this, we train on real
colonoscopies a deep global descriptor achieving high recall with significant
changes in the scene. The addition of a Bayesian filter boosts the accuracy of
long-term place recognition, enabling relocalization in a previously built map.
Our experiments show that ColonMapper is able to autonomously build a map and
localize against it in two important use cases: localization within the same
colonoscopy or within different colonoscopies of the same patient. Code will be
available upon acceptance.",None,-1
f65c5823-c106-4512-811a-c3057e6c5df2,Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning,0.541887,"Adapting to regularities of the environment is critical for biological
organisms to anticipate events and plan. A prominent example is the circadian
rhythm corresponding to the internalization by organisms of the $24$-hour
period of the Earth's rotation. In this work, we study the emergence of
circadian-like rhythms in deep reinforcement learning agents. In particular, we
deployed agents in an environment with a reliable periodic variation while
solving a foraging task. We systematically characterize the agent's behavior
during learning and demonstrate the emergence of a rhythm that is endogenous
and entrainable. Interestingly, the internal rhythm adapts to shifts in the
phase of the environmental signal without any re-training. Furthermore, we show
via bifurcation and phase response curve analyses how artificial neurons
develop dynamics to support the internalization of the environmental rhythm.
From a dynamical systems view, we demonstrate that the adaptation proceeds by
the emergence of a stable periodic orbit in the neuron dynamics with a phase
response that allows an optimal phase synchronisation between the agent's
dynamics and the environmental rhythm.",None,-1
c0e30eb6-01bf-4142-8f3c-81bfb54bf970,Improving Contrastive Learning of Sentence Embeddings from AI Feedback,0.73365,"Contrastive learning has become a popular approach in natural language
processing, particularly for the learning of sentence embeddings. However, the
discrete nature of natural language makes it difficult to ensure the quality of
positive and negative sample pairs generated through data augmentation methods.
Although supervised contrastive learning can produce more accurate sample pairs
with human feedback labels, it still lacks fine-grained training signals. In
this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of
sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our
method utilizes AI feedback from large pre-trained language models (LLMs) to
construct sample pairs with fine-grained sample similarity scores to improve
contrastive learning. Besides, we combine human feedback and AI feedback to
provide better supervision signals for supervised contrastive learning of
sentence embeddings. Experimental results show that our method achieves
state-of-the-art performance on several semantic textual similarity (STS) and
transfer learning tasks compared to other unsupervised and supervised
contrastive learning methods.",None,-1
32ee415e-e440-489f-adb4-8f49185d1008,"Exploring the Consistency, Quality and Challenges in Manual and Automated Coding of Free-text Diagnoses from Hospital Outpatient Letters",0.327337,"Coding of unstructured clinical free-text to produce interoperable structured
data is essential to improve direct care, support clinical communication and to
enable clinical research.However, manual clinical coding is difficult and time
consuming, which motivates the development and use of natural language
processing for automated coding. This work evaluates the quality and
consistency of both manual and automated clinical coding of diagnoses from
hospital outpatient letters. Using 100 randomly selected letters, two human
clinicians performed coding of diagnosis lists to SNOMED CT. Automated coding
was also performed using IMO's Concept Tagger. A gold standard was constructed
by a panel of clinicians from a subset of the annotated diagnoses. This was
used to evaluate the quality and consistency of both manual and automated
coding via (1) a distance-based metric, treating SNOMED CT as a graph, and (2)
a qualitative metric agreed upon by the panel of clinicians. Correlation
between the two metrics was also evaluated. Comparing human and
computer-generated codes to the gold standard, the results indicate that humans
slightly out-performed automated coding, while both performed notably better
when there was only a single diagnosis contained in the free-text description.
Automated coding was considered acceptable by the panel of clinicians in
approximately 90% of cases.",None,-1
31c7b081-2191-4157-8872-8c6015e46bd3,Predictive Heterogeneity: Measures and Applications,0.0716784,"As an intrinsic and fundamental property of big data, data heterogeneity
exists in a variety of real-world applications, such as precision medicine,
autonomous driving, financial applications, etc. For machine learning
algorithms, the ignorance of data heterogeneity will greatly hurt the
generalization performance and the algorithmic fairness, since the prediction
mechanisms among different sub-populations are likely to differ from each
other. In this work, we focus on the data heterogeneity that affects the
prediction of machine learning models, and firstly propose the \emph{usable
predictive heterogeneity}, which takes into account the model capacity and
computational constraints. We prove that it can be reliably estimated from
finite data with probably approximately correct (PAC) bounds. Additionally, we
design a bi-level optimization algorithm to explore the usable predictive
heterogeneity from data. Empirically, the explored heterogeneity provides
insights for sub-population divisions in income prediction, crop yield
prediction and image classification tasks, and leveraging such heterogeneity
benefits the out-of-distribution generalization performance.",None,-1
77a5a5b3-c134-45e5-aec9-ea45e2b68d77,Motion-Scenario Decoupling for Rat-Aware Video Position Prediction: Strategy and Benchmark,0.340759,"Recently significant progress has been made in human action recognition and
behavior prediction using deep learning techniques, leading to improved
vision-based semantic understanding. However, there is still a lack of
high-quality motion datasets for small bio-robotics, which presents more
challenging scenarios for long-term movement prediction and behavior control
based on third-person observation. In this study, we introduce RatPose, a
bio-robot motion prediction dataset constructed by considering the influence
factors of individuals and environments based on predefined annotation rules.
To enhance the robustness of motion prediction against these factors, we
propose a Dual-stream Motion-Scenario Decoupling (\textit{DMSD}) framework that
effectively separates scenario-oriented and motion-oriented features and
designs a scenario contrast loss and motion clustering loss for overall
training. With such distinctive architecture, the dual-branch feature flow
information is interacted and compensated in a decomposition-then-fusion
manner. Moreover, we demonstrate significant performance improvements of the
proposed \textit{DMSD} framework on different difficulty-level tasks. We also
implement long-term discretized trajectory prediction tasks to verify the
generalization ability of the proposed dataset.",None,-1
dbc7764b-8b8a-4c16-a5bd-38590d1f8495,CCLAP: Controllable Chinese Landscape Painting Generation via Latent Diffusion Model,0.750882,"With the development of deep generative models, recent years have seen great
success of Chinese landscape painting generation. However, few works focus on
controllable Chinese landscape painting generation due to the lack of data and
limited modeling capabilities. In this work, we propose a controllable Chinese
landscape painting generation method named CCLAP, which can generate painting
with specific content and style based on Latent Diffusion Model. Specifically,
it consists of two cascaded modules, i.e., content generator and style
aggregator. The content generator module guarantees the content of generated
paintings specific to the input text. While the style aggregator module is to
generate paintings of a style corresponding to a reference image. Moreover, a
new dataset of Chinese landscape paintings named CLAP is collected for
comprehensive evaluation. Both the qualitative and quantitative results
demonstrate that our method achieves state-of-the-art performance, especially
in artfully-composed and artistic conception. Codes are available at
https://github.com/Robin-WZQ/CCLAP.",None,-1
094c0ea8-8cb3-4cca-937d-2b870d5d2815,Continual Learning with Scaled Gradient Projection,0.428807,"In neural networks, continual learning results in gradient interference among
sequential tasks, leading to catastrophic forgetting of old tasks while
learning new ones. This issue is addressed in recent methods by storing the
important gradient spaces for old tasks and updating the model orthogonally
during new tasks. However, such restrictive orthogonal gradient updates hamper
the learning capability of the new tasks resulting in sub-optimal performance.
To improve new learning while minimizing forgetting, in this paper we propose a
Scaled Gradient Projection (SGP) method, where we combine the orthogonal
gradient projections with scaled gradient steps along the important gradient
spaces for the past tasks. The degree of gradient scaling along these spaces
depends on the importance of the bases spanning them. We propose an efficient
method for computing and accumulating importance of these bases using the
singular value decomposition of the input representations for each task. We
conduct extensive experiments ranging from continual image classification to
reinforcement learning tasks and report better performance with less training
overhead than the state-of-the-art approaches.",None,-1
318cb870-d93d-4a53-bb05-25906548ef91,A recommender for the management of chronic pain in patients undergoing spinal cord stimulation,0.426041,"Spinal cord stimulation (SCS) is a therapeutic approach used for the
management of chronic pain. It involves the delivery of electrical impulses to
the spinal cord via an implanted device, which when given suitable stimulus
parameters can mask or block pain signals. Selection of optimal stimulation
parameters usually happens in the clinic under the care of a provider whereas
at-home SCS optimization is managed by the patient. In this paper, we propose a
recommender system for the management of pain in chronic pain patients
undergoing SCS. In particular, we use a contextual multi-armed bandit (CMAB)
approach to develop a system that recommends SCS settings to patients with the
aim of improving their condition. These recommendations, sent directly to
patients though a digital health ecosystem, combined with a patient monitoring
system closes the therapeutic loop around a chronic pain patient over their
entire patient journey. We evaluated the system in a cohort of SCS-implanted
ENVISION study subjects (Clinicaltrials.gov ID: NCT03240588) using a
combination of quality of life metrics and Patient States (PS), a novel measure
of holistic outcomes. SCS recommendations provided statistically significant
improvement in clinical outcomes (pain and/or QoL) in 85\% of all subjects
(N=21). Among subjects in moderate PS (N=7) prior to receiving recommendations,
100\% showed statistically significant improvements and 5/7 had improved PS
dwell time. This analysis suggests SCS patients may benefit from SCS
recommendations, resulting in additional clinical improvement on top of
benefits already received from SCS therapy.",None,-1
2dddb45d-3fbf-4a0a-aeec-572e2fb7e2b8,RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition,0.542516,"Emotion recognition in conversation (ERC) has received increasing attention
from researchers due to its wide range of applications.As conversation has a
natural graph structure,numerous approaches used to model ERC based on graph
convolutional networks (GCNs) have yielded significant results.However,the
aggregation approach of traditional GCNs suffers from the node information
redundancy problem,leading to node discriminant information
loss.Additionally,single-layer GCNs lack the capacity to capture long-range
contextual information from the graph. Furthermore,the majority of approaches
are based on textual modality or stitching together different modalities,
resulting in a weak ability to capture interactions between modalities. To
address these problems, we present the relational bilevel aggregation graph
convolutional network (RBA-GCN), which consists of three modules: the graph
generation module (GGM), similarity-based cluster building module (SCBM) and
bilevel aggregation module (BiAM). First, GGM constructs a novel graph to
reduce the redundancy of target node information.Then,SCBM calculates the node
similarity in the target node and its structural neighborhood, where noisy
information with low similarity is filtered out to preserve the discriminant
information of the node. Meanwhile, BiAM is a novel aggregation method that can
preserve the information of nodes during the aggregation process. This module
can construct the interaction between different modalities and capture
long-range contextual information based on similarity clusters. On both the
IEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a
2.17$\sim$5.21\% improvement over that of the most advanced method.Our code is
available at https://github.com/luftmenscher/RBA-GCN and our article with the
same name has been published in IEEE/ACM Transactions on Audio,Speech,and
Language Processing,vol.31,2023",None,-1
2af44fda-5574-4701-9e90-6b5d8f4f1946,RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models,0.432753,"In this paper, we investigate the in-context learning ability of
retrieval-augmented encoder-decoder language models. We first conduct a
comprehensive analysis of existing models and identify their limitations in
in-context learning, primarily due to a mismatch between pretraining and
inference, as well as a restricted context length. To address these issues, we
propose RAVEN, a model that combines retrieval-augmented masked language
modeling and prefix language modeling. We further introduce Fusion-in-Context
Learning to enhance the few-shot performance by enabling the model to leverage
more in-context examples without requiring additional training. Through
extensive experiments, we demonstrate that our simple yet effective design
significantly improves performance, achieving results comparable to the most
advanced language models in certain scenarios, despite having substantially
fewer parameters. Our work underscores the potential of retrieval-augmented
encoder-decoder language models for in-context learning and encourages further
research in this direction.",None,-1
1f325a82-7255-48ee-8c11-5a9b8f67630e,"Data-driven intelligent computational design for products: Method, techniques, and applications",0.326283,"Data-driven intelligent computational design (DICD) is a research hotspot
emerged under the context of fast-developing artificial intelligence. It
emphasizes on utilizing deep learning algorithms to extract and represent the
design features hidden in historical or fabricated design process data, and
then learn the combination and mapping patterns of these design features for
the purposes of design solution retrieval, generation, optimization,
evaluation, etc. Due to its capability of automatically and efficiently
generating design solutions and thus supporting human-in-the-loop intelligent
and innovative design activities, DICD has drawn the attentions from both
academic and industrial fields. However, as an emerging research subject, there
are still many unexplored issues that limit the development and application of
DICD, such as specific dataset building, engineering design related feature
engineering, systematic methods and techniques for DICD implementation in the
entire product design process, etc. In this regard, a systematic and operable
road map for DICD implementation from full-process perspective is established,
including a general workflow for DICD project planning, an overall framework
for DICD project implementation, the computing mechanisms for DICD
implementation, key enabling technologies for detailed DICD implementation, and
three application scenarios of DICD. The road map reveals the common mechanisms
and calculation principles of existing DICD researches, and thus it can provide
systematic guidance for the possible DICD applications that have not been
explored.",None,-1
b4de9704-ae8f-4483-8bae-81f6dcd813a8,MITFAS: Mutual Information based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition,0.521802,"We present a novel approach for action recognition in UAV videos. Our
formulation is designed to handle occlusion and viewpoint changes caused by the
movement of a UAV. We use the concept of mutual information to compute and
align the regions corresponding to human action or motion in the temporal
domain. This enables our recognition model to learn from the key features
associated with the motion. We also propose a novel frame sampling method that
uses joint mutual information to acquire the most informative frame sequence in
UAV videos. We have integrated our approach with X3D and evaluated the
performance on multiple datasets. In practice, we achieve 18.9% improvement in
Top-1 accuracy over current state-of-the-art methods on UAV-Human(Li et al.,
2021), 7.3% improvement on Drone-Action(Perera et al., 2019), and 7.16%
improvement on NEC Drones(Choi et al., 2020).",None,-1
512491da-e707-4bf9-b383-46b88b361168,Deception Abilities Emerged in Large Language Models,0.463849,"Large language models (LLMs) are currently at the forefront of intertwining
artificial intelligence (AI) systems with human communication and everyday
life. Thus, aligning them with human values is of great importance. However,
given the steady increase in reasoning abilities, future LLMs are under
suspicion of becoming able to deceive human operators and utilizing this
ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to
possess a conceptual understanding of deception strategies. This study reveals
that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were
non-existent in earlier LLMs. We conduct a series of experiments showing that
state-of-the-art LLMs are able to understand and induce false beliefs in other
agents, that their performance in complex deception scenarios can be amplified
utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in
LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown
machine behavior in LLMs, our study contributes to the nascent field of machine
psychology.",None,-1
1543ccb6-8f09-44de-bc15-a95238f02142,Random Padding Data Augmentation,0.0605246,"The convolutional neural network (CNN) learns the same object in different
positions in images, which can improve the recognition accuracy of the model.
An implication of this is that CNN may know where the object is. The usefulness
of the features' spatial information in CNNs has not been well investigated. In
this paper, we found that the model's learning of features' position
information hindered the learning of the features' relationship. Therefore, we
introduced Random Padding, a new type of padding method for training CNNs that
impairs the architecture's capacity to learn position information by adding
zero-padding randomly to half of the border of feature maps. Random Padding is
parameter-free, simple to construct, and compatible with the majority of
CNN-based recognition models. This technique is also complementary to data
augmentations such as random cropping, rotation, flipping and erasing, and
consistently improves the performance of image classification over strong
baselines.",None,-1
8b95ba62-95a6-4c5b-9de8-d7e0f37b71a2,Redrawing attendance boundaries to promote racial and ethnic diversity in elementary schools,0.486412,"Most US school districts draw ""attendance boundaries"" to define catchment
areas that assign students to schools near their homes, often recapitulating
neighborhood demographic segregation in schools. Focusing on elementary
schools, we ask: how much might we reduce school segregation by redrawing
attendance boundaries? Combining parent preference data with methods from
combinatorial optimization, we simulate alternative boundaries for 98 US school
districts serving over 3 million elementary-aged students, minimizing
White/non-White segregation while mitigating changes to travel times and school
sizes. Across districts, we observe a median 14% relative decrease in
segregation, which we estimate would require approximately 20\% of students to
switch schools and, surprisingly, a slight reduction in travel times. We
release a public dashboard depicting these alternative boundaries
(https://www.schooldiversity.org/) and invite both school boards and their
constituents to evaluate their viability. Our results show the possibility of
greater integration without significant disruptions for families.",None,-1
0e186ba1-9632-4009-9d85-fa1a0e3ff02f,Towards Explainable TOPSIS: Visual Insights into the Effects of Weights and Aggregations on Rankings,0.545257,"Multi-Criteria Decision Analysis (MCDA) is extensively used across diverse
industries to assess and rank alternatives. Among numerous MCDA methods
developed to solve real-world ranking problems, TOPSIS remains one of the most
popular choices in many application areas. TOPSIS calculates distances between
the considered alternatives and two predefined ones, namely the ideal and the
anti-ideal, and creates a ranking of the alternatives according to a chosen
aggregation of these distances. However, the interpretation of the inner
workings of TOPSIS is difficult, especially when the number of criteria is
large. To this end, recent research has shown that TOPSIS aggregations can be
expressed using the means (M) and standard deviations (SD) of alternatives,
creating MSD-space, a tool for visualizing and explaining aggregations. Even
though MSD-space is highly useful, it assumes equally important criteria,
making it less applicable to real-world ranking problems. In this paper, we
generalize the concept of MSD-space to weighted criteria by introducing the
concept of WMSD-space defined by what is referred to as weight-scaled means and
standard deviations. We demonstrate that TOPSIS and similar distance-based
aggregation methods can be successfully illustrated in a plane and interpreted
even when the criteria are weighted, regardless of their number. The proposed
WMSD-space offers a practical method for explaining TOPSIS rankings in
real-world decision problems.",None,-1
8adaa9e3-91d7-4819-932a-e801787e0234,Asymptotic Convergence and Performance of Multi-Agent Q-Learning Dynamics,0.806154,"Achieving convergence of multiple learning agents in general $N$-player games
is imperative for the development of safe and reliable machine learning (ML)
algorithms and their application to autonomous systems. Yet it is known that,
outside the bounds of simple two-player games, convergence cannot be taken for
granted.
  To make progress in resolving this problem, we study the dynamics of smooth
Q-Learning, a popular reinforcement learning algorithm which quantifies the
tendency for learning agents to explore their state space or exploit their
payoffs. We show a sufficient condition on the rate of exploration such that
the Q-Learning dynamics is guaranteed to converge to a unique equilibrium in
any game. We connect this result to games for which Q-Learning is known to
converge with arbitrary exploration rates, including weighted Potential games
and weighted zero sum polymatrix games.
  Finally, we examine the performance of the Q-Learning dynamic as measured by
the Time Averaged Social Welfare, and comparing this with the Social Welfare
achieved by the equilibrium. We provide a sufficient condition whereby the
Q-Learning dynamic will outperform the equilibrium even if the dynamics do not
converge.",None,-1
e2e81b56-5fcf-41b0-8cef-2ebeb282b4b1,Improving Deep Policy Gradients with Value Function Search,0.234072,"Deep Policy Gradient (PG) algorithms employ value networks to drive the
learning of parameterized policies and reduce the variance of the gradient
estimates. However, value function approximation gets stuck in local optima and
struggles to fit the actual return, limiting the variance reduction efficacy
and leading policies to sub-optimal performance. This paper focuses on
improving value approximation and analyzing the effects on Deep PG primitives
such as value prediction, variance reduction, and correlation of gradient
estimates with the true gradient. To this end, we introduce a Value Function
Search that employs a population of perturbed value networks to search for a
better approximation. Our framework does not require additional environment
interactions, gradient computations, or ensembles, providing a computationally
inexpensive approach to enhance the supervised learning task on which value
networks train. Crucially, we show that improving Deep PG primitives results in
improved sample efficiency and policies with higher returns using common
continuous control benchmark domains.",None,-1
14158b11-54d8-4822-949f-629fa04b5a50,Improving the Generalizability of Collaborative Dialogue Analysis with Multi-Feature Embeddings,0.111908,"Conflict prediction in communication is integral to the design of virtual
agents that support successful teamwork by providing timely assistance. The aim
of our research is to analyze discourse to predict collaboration success.
Unfortunately, resource scarcity is a problem that teamwork researchers
commonly face since it is hard to gather a large number of training examples.
To alleviate this problem, this paper introduces a multi-feature embedding
(MFeEmb) that improves the generalizability of conflict prediction models
trained on dialogue sequences. MFeEmb leverages textual, structural, and
semantic information from the dialogues by incorporating lexical, dialogue
acts, and sentiment features. The use of dialogue acts and sentiment features
reduces performance loss from natural distribution shifts caused mainly by
changes in vocabulary.
  This paper demonstrates the performance of MFeEmb on domain adaptation
problems in which the model is trained on discourse from one task domain and
applied to predict team performance in a different domain. The generalizability
of MFeEmb is quantified using the similarity measure proposed by Bontonou et
al. (2021). Our results show that MFeEmb serves as an excellent domain-agnostic
representation for meta-pretraining a few-shot model on collaborative
multiparty dialogues.",None,-1
7ceb3c6b-e6e3-4554-99c7-e20c1ea65549,Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention,0.22262,"BIQA (Blind Image Quality Assessment) is an important field of study that
evaluates images automatically. Although significant progress has been made,
blind image quality assessment remains a difficult task since images vary in
content and distortions. Most algorithms generate quality without emphasizing
the important region of interest. In order to solve this, a multi-stream
spatial and channel attention-based algorithm is being proposed. This algorithm
generates more accurate predictions with a high correlation to human perceptual
assessment by combining hybrid features from two different backbones, followed
by spatial and channel attention to provide high weights to the region of
interest. Four legacy image quality assessment datasets are used to validate
the effectiveness of our proposed approach. Authentic and synthetic distortion
image databases are used to demonstrate the effectiveness of the proposed
method, and we show that it has excellent generalization properties with a
particular focus on the perceptual foreground information.",None,-1
8afcbf8a-7209-4770-9fd6-8f3d17de9dc2,Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias,0.58271,"Large language models (LLMs) have demonstrated their potential in social
science research by emulating human perceptions and behaviors, a concept
referred to as algorithmic fidelity. This study assesses the algorithmic
fidelity and bias of LLMs by utilizing two nationally representative climate
change surveys. The LLMs were conditioned on demographics and/or psychological
covariates to simulate survey responses. The findings indicate that LLMs can
effectively capture presidential voting behaviors but encounter challenges in
accurately representing global warming perspectives when relevant covariates
are not included. GPT-4 exhibits improved performance when conditioned on both
demographics and covariates. However, disparities emerge in LLM estimations of
the views of certain groups, with LLMs tending to underestimate worry about
global warming among Black Americans. While highlighting the potential of LLMs
to aid social science research, these results underscore the importance of
meticulous conditioning, model selection, survey question format, and bias
assessment when employing LLMs for survey simulation. Further investigation
into prompt engineering and algorithm auditing is essential to harness the
power of LLMs while addressing their inherent limitations.",None,-1
3fd2cd28-a675-4967-a4fb-79f252b94923,Algebraic Positional Encodings,0.0808471,"We introduce a novel positional encoding strategy for Transformer-style
models, addressing the shortcomings of existing, often ad hoc, approaches. Our
framework provides a flexible mapping from the algebraic specification of a
domain to an interpretation as orthogonal operators. This design preserves the
algebraic characteristics of the source domain, ensuring that the model upholds
the desired structural properties. Our scheme can accommodate various
structures, including sequences, grids and trees, as well as their
compositions. We conduct a series of experiments to demonstrate the practical
applicability of our approach. Results suggest performance on par with or
surpassing the current state-of-the-art, without hyperparameter optimizations
or ``task search'' of any kind. Code will be made available at
\url{github.com/konstantinosKokos/UnitaryPE}.",None,-1
c61cf47d-4c3f-4872-8072-a86c934f107f,Amodal Intra-class Instance Segmentation: Synthetic Datasets and Benchmark,0.542788,"Images of realistic scenes often contain intra-class objects that are heavily
occluded from each other, making the amodal perception task that requires
parsing the occluded parts of the objects challenging. Although important for
downstream tasks such as robotic grasping systems, the lack of large-scale
amodal datasets with detailed annotations makes it difficult to model
intra-class occlusions explicitly. This paper introduces two new amodal
datasets for image amodal completion tasks, which contain a total of over 267K
images of intra-class occlusion scenarios, annotated with multiple masks,
amodal bounding boxes, dual order relations and full appearance for instances
and background. We also present a point-supervised scheme with layer priors for
amodal instance segmentation specifically designed for intra-class occlusion
scenarios. Experiments show that our weakly supervised approach outperforms the
SOTA fully supervised methods, while our layer priors design exhibits
remarkable performance improvements in the case of intra-class occlusion in
both synthetic and real images.",None,-1
8e0d1e97-d368-49e1-aae0-6e268320bc90,Probability-based Global Cross-modal Upsampling for Pansharpening,0.712391,"Pansharpening is an essential preprocessing step for remote sensing image
processing. Although deep learning (DL) approaches performed well on this task,
current upsampling methods used in these approaches only utilize the local
information of each pixel in the low-resolution multispectral (LRMS) image
while neglecting to exploit its global information as well as the cross-modal
information of the guiding panchromatic (PAN) image, which limits their
performance improvement. To address this issue, this paper develops a novel
probability-based global cross-modal upsampling (PGCU) method for
pan-sharpening. Precisely, we first formulate the PGCU method from a
probabilistic perspective and then design an efficient network module to
implement it by fully utilizing the information mentioned above while
simultaneously considering the channel specificity. The PGCU module consists of
three blocks, i.e., information extraction (IE), distribution and expectation
estimation (DEE), and fine adjustment (FA). Extensive experiments verify the
superiority of the PGCU method compared with other popular upsampling methods.
Additionally, experiments also show that the PGCU module can help improve the
performance of existing SOTA deep learning pansharpening methods. The codes are
available at https://github.com/Zeyu-Zhu/PGCU.",None,-1
dcdb1a58-cb1b-41ba-801e-74188d3b1f46,No that's not what I meant: Handling Third Position Repair in Conversational Question Answering,0.0334763,"The ability to handle miscommunication is crucial to robust and faithful
conversational AI. People usually deal with miscommunication immediately as
they detect it, using highly systematic interactional mechanisms called repair.
One important type of repair is Third Position Repair (TPR) whereby a speaker
is initially misunderstood but then corrects the misunderstanding as it becomes
apparent after the addressee's erroneous response. Here, we collect and
publicly release Repair-QA, the first large dataset of TPRs in a conversational
question answering (QA) setting. The data is comprised of the TPR turns,
corresponding dialogue contexts, and candidate repairs of the original turn for
execution of TPRs. We demonstrate the usefulness of the data by training and
evaluating strong baseline models for executing TPRs. For stand-alone TPR
execution, we perform both automatic and human evaluations on a fine-tuned T5
model, as well as OpenAI's GPT-3 LLMs. Additionally, we extrinsically evaluate
the LLMs' TPR processing capabilities in the downstream conversational QA task.
The results indicate poor out-of-the-box performance on TPR's by the GPT-3
models, which then significantly improves when exposed to Repair-QA.",None,-1
b0c623db-e04e-4946-8a20-842fea707bf2,A Comparison of Speech Data Augmentation Methods Using S3PRL Toolkit,0.301043,"Data augmentations are known to improve robustness in speech-processing
tasks. In this study, we summarize and compare different data augmentation
strategies using S3PRL toolkit. We explore how HuBERT and wav2vec perform using
different augmentation techniques (SpecAugment, Gaussian Noise, Speed
Perturbation) for Phoneme Recognition (PR) and Automatic Speech Recognition
(ASR) tasks. We evaluate model performance in terms of phoneme error rate (PER)
and word error rate (WER). From the experiments, we observed that SpecAugment
slightly improves the performance of HuBERT and wav2vec on the original
dataset. Also, we show that models trained using the Gaussian Noise and Speed
Perturbation dataset are more robust when tested with augmented test sets.",None,-1
17b06114-3742-4327-b437-c9f50df062b4,Smart Home Environment Modelled with a Multi-Agent System,0.183222,"A smart home can be considered a place of residence that enables the
management of appliances and systems to help with day-to-day life by automated
technology. In the current paper is described a prototype that simulates a
context-aware environment, developed in a designed smart home. The smart home
environment has been simulated using three agents and five locations in a
house. The context-aware agents behave based on predefined rules designed for
daily activities. Our proposal aims to reduce operational cost of running
devices. In the future, monitors of health aspects belonging to home residents
will sustain their healthy life daily.",None,-1
f137c195-9134-4465-937f-c7b3b5beae5e,Creating a silver standard for patent simplification,0.170864,"Patents are legal documents that aim at protecting inventions on the one hand
and at making technical knowledge circulate on the other. Their complex style
-- a mix of legal, technical, and extremely vague language -- makes their
content hard to access for humans and machines and poses substantial challenges
to the information retrieval community. This paper proposes an approach to
automatically simplify patent text through rephrasing. Since no in-domain
parallel simplification data exist, we propose a method to automatically
generate a large-scale silver standard for patent sentences. To obtain
candidates, we use a general-domain paraphrasing system; however, the process
is error-prone and difficult to control. Thus, we pair it with proper filters
and construct a cleaner corpus that can successfully be used to train a
simplification system. Human evaluation of the synthetic silver corpus shows
that it is considered grammatical, adequate, and contains simple sentences.",None,-1
7dd10d4c-0ea0-4138-bedc-9c09f6842e4b,Progressive Channel-Shrinking Network,0.213788,"Currently, salience-based channel pruning makes continuous breakthroughs in
network compression. In the realization, the salience mechanism is used as a
metric of channel salience to guide pruning. Therefore, salience-based channel
pruning can dynamically adjust the channel width at run-time, which provides a
flexible pruning scheme. However, there are two problems emerging: a gating
function is often needed to truncate the specific salience entries to zero,
which destabilizes the forward propagation; dynamic architecture brings more
cost for indexing in inference which bottlenecks the inference speed. In this
paper, we propose a Progressive Channel-Shrinking (PCS) method to compress the
selected salience entries at run-time instead of roughly approximating them to
zero. We also propose a Running Shrinking Policy to provide a testing-static
pruning scheme that can reduce the memory access cost for filter indexing. We
evaluate our method on ImageNet and CIFAR10 datasets over two prevalent
networks: ResNet and VGG, and demonstrate that our PCS outperforms all
baselines and achieves state-of-the-art in terms of compression-performance
tradeoff. Moreover, we observe a significant and practical acceleration of
inference.",None,-1
f049a8b3-0904-43d6-b82a-f6772505375b,Region-Aware Portrait Retouching with Sparse Interactive Guidance,0.277925,"Portrait retouching aims to improve the aesthetic quality of input portrait
photos and especially requires human-region priority. The deep learning-based
methods largely elevate the retouching efficiency and provide promising
retouched results. However, existing portrait retouching methods focus on
automatic retouching, which treats all human-regions equally and ignores users'
preferences for specific individuals, thus suffering from limited flexibility
in interactive scenarios. In this work, we emphasize the importance of users'
intents and explore the interactive portrait retouching task. Specifically, we
propose a region-aware retouching framework with two branches: an automatic
branch and an interactive branch. The automatic branch involves an
encoding-decoding process, which searches region candidates and performs
automatic region-aware retouching without user guidance. The interactive branch
encodes sparse user guidance into a priority condition vector and modulates
latent features with a region selection module to further emphasize the
user-specified regions. Experimental results show that our interactive branch
effectively captures users' intents and generalizes well to unseen scenes with
sparse user guidance, while our automatic branch also outperforms the
state-of-the-art retouching methods due to improved region-awareness.",None,-1
11ef1ce2-b554-44e0-bafc-397ec439d489,Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders,0.136236,"In collaboration with Postpartum Support International (PSI), a non-profit
organization dedicated to supporting caregivers with postpartum mood and
anxiety disorders, we developed three chatbots to provide context-specific
empathetic support to postpartum caregivers, leveraging both rule-based and
generative models. We present and evaluate the performance of our chatbots
using both machine-based metrics and human-based questionnaires. Overall, our
rule-based model achieves the best performance, with outputs that are close to
ground truth reference and contain the highest levels of empathy. Human users
prefer the rule-based chatbot over the generative chatbot for its
context-specific and human-like replies. Our generative chatbot also produced
empathetic responses and was described by human users as engaging. However,
limitations in the training dataset often result in confusing or nonsensical
responses. We conclude by discussing practical benefits of rule-based vs.
generative models for supporting individuals with mental health challenges. In
light of the recent surge of ChatGPT and BARD, we also discuss the
possibilities and pitfalls of large language models for digital mental
healthcare.",None,-1
e0e43930-ade5-4e05-a4e2-d9895b3f762d,SGLang: Efficient Execution of Structured Language Model Programs,0.880764,"Large language models (LLMs) are increasingly used for complex tasks that
require multiple generation calls, advanced prompting techniques, control flow,
and structured inputs/outputs. However, efficient systems are lacking for
programming and executing these applications. We introduce SGLang, a system for
efficient execution of complex language model programs. SGLang consists of a
frontend language and a runtime. The frontend simplifies programming with
primitives for generation and parallelism control. The runtime accelerates
execution with novel optimizations like RadixAttention for KV cache reuse and
compressed finite state machines for faster structured output decoding.
Experiments show that SGLang achieves up to 6.4x higher throughput compared to
state-of-the-art inference systems on various large language and multi-modal
models on tasks including agent control, logical reasoning, few-shot learning
benchmarks, JSON decoding, retrieval-augmented generation pipelines, and
multi-turn chat. The code is publicly available at
https://github.com/sgl-project/sglang",None,-1
b52d15f8-6a56-47c7-a0f6-cbc9b876e660,Can Language Models Employ the Socratic Method? Experiments with Code Debugging,0.802069,"When employing the Socratic method of teaching, instructors guide students
toward solving a problem on their own rather than providing the solution
directly. While this strategy can substantially improve learning outcomes, it
is usually time-consuming and cognitively demanding. Automated Socratic
conversational agents can augment human instruction and provide the necessary
scale, however their development is hampered by the lack of suitable data for
training and evaluation. In this paper, we introduce a manually created dataset
of multi-turn Socratic advice that is aimed at helping a novice programmer fix
buggy solutions to simple computational problems. The dataset is then used for
benchmarking the Socratic debugging abilities of a number of language models,
ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5
to zero-shot and chain of thought prompting of the much larger GPT-4. The code
and datasets are made freely available for research at the link below.
https://github.com/taisazero/socratic-debugging-benchmark",None,-1
7cc35a0d-5120-4bab-acd6-bac873eb079d,DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars,0.904779,"We present DINAR, an approach for creating realistic rigged fullbody avatars
from single RGB images. Similarly to previous works, our method uses neural
textures combined with the SMPL-X body model to achieve photo-realistic quality
of avatars while keeping them easy to animate and fast to infer. To restore the
texture, we use a latent diffusion model and show how such model can be trained
in the neural texture space. The use of the diffusion model allows us to
realistically reconstruct large unseen regions such as the back of a person
given the frontal view. The models in our pipeline are trained using 2D images
and videos only. In the experiments, our approach achieves state-of-the-art
rendering quality and good generalization to new poses and viewpoints. In
particular, the approach improves state-of-the-art on the SnapshotPeople public
benchmark.",None,-1
a39a0063-758e-45ac-bf0b-8b5c95fd5a84,IDA: Informed Domain Adaptive Semantic Segmentation,0.449823,"Mixup-based data augmentation has been validated to be a critical stage in
the self-training framework for unsupervised domain adaptive semantic
segmentation (UDA-SS), which aims to transfer knowledge from a well-annotated
(source) domain to an unlabeled (target) domain. Existing self-training methods
usually adopt the popular region-based mixup techniques with a random sampling
strategy, which unfortunately ignores the dynamic evolution of different
semantics across various domains as training proceeds. To improve the UDA-SS
performance, we propose an Informed Domain Adaptation (IDA) model, a
self-training framework that mixes the data based on class-level segmentation
performance, which aims to emphasize small-region semantics during mixup. In
our IDA model, the class-level performance is tracked by an expected confidence
score (ECS). We then use a dynamic schedule to determine the mixing ratio for
data in different domains. Extensive experimental results reveal that our
proposed method is able to outperform the state-of-the-art UDA-SS method by a
margin of 1.1 mIoU in the adaptation of GTA-V to Cityscapes and of 0.9 mIoU in
the adaptation of SYNTHIA to Cityscapes.",None,-1
e85e2f55-d372-48f7-b5b0-ed9f8adf61d8,An Empirical Study on the Transferability of Transformer Modules in Parameter-Efficient Fine-Tuning,0.0327163,"Parameter-efficient fine-tuning approaches have recently garnered a lot of
attention. Having considerably lower number of trainable weights, these methods
can bring about scalability and computational effectiveness. In this paper, we
look for optimal sub-networks and investigate the capability of different
transformer modules in transferring knowledge from a pre-trained model to a
downstream task. Our empirical results suggest that every transformer module in
BERT can act as a winning ticket: fine-tuning each specific module while
keeping the rest of the network frozen can lead to comparable performance to
the full fine-tuning. Among different modules, LayerNorms exhibit the best
capacity for knowledge transfer with limited trainable weights, to the extent
that, with only 0.003% of all parameters in the layer-wise analysis, they show
acceptable performance on various target tasks. On the reasons behind their
effectiveness, we argue that their notable performance could be attributed to
their high-magnitude weights compared to that of the other modules in the
pre-trained BERT.",None,-1
341a20c7-1db8-41bc-a799-c8e5702d294b,VEGETA: Vertically-Integrated Extensions for Sparse/Dense GEMM Tile Acceleration on CPUs,0.522479,"Deep Learning (DL) acceleration support in CPUs has recently gained a lot of
traction, with several companies (Arm, Intel, IBM) announcing products with
specialized matrix engines accessible via GEMM instructions. CPUs are pervasive
and need to handle diverse requirements across DL workloads running in
edge/HPC/cloud platforms. Therefore, as DL workloads embrace sparsity to reduce
the computations and memory size of models, it is also imperative for CPUs to
add support for sparsity to avoid under-utilization of the dense matrix engine
and inefficient usage of the caches and registers. This work presents VEGETA, a
set of ISA and microarchitecture extensions over dense matrix engines to
support flexible structured sparsity for CPUs, enabling programmable support
for diverse DL models with varying degrees of sparsity. Compared to the
state-of-the-art (SOTA) dense matrix engine in CPUs, a VEGETA engine provides
1.09x, 2.20x, 3.74x, and 3.28x speed-ups when running 4:4 (dense), 2:4, 1:4,
and unstructured (95%) sparse DNN layers.",None,-1
ba4e31c0-66c0-4787-ac1c-ef1fa2391ab3,Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking,0.47278,"Discovering entity mentions that are out of a Knowledge Base (KB) from texts
plays a critical role in KB maintenance, but has not yet been fully explored.
The current methods are mostly limited to the simple threshold-based approach
and feature-based classification, and the datasets for evaluation are
relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL)
method which can identify mentions that do not have corresponding KB entities
by matching them to a special NIL entity. To better utilize BERT, we propose
new techniques including NIL entity representation and classification, with
synonym enhancement. We also apply KB Pruning and Versioning strategies to
automatically construct out-of-KB datasets from common in-KB EL datasets.
Results on five datasets of clinical notes, biomedical publications, and
Wikipedia articles in various domains show the advantages of BLINKout over
existing methods to identify out-of-KB mentions for the medical ontologies,
UMLS, SNOMED CT, and the general KB, WikiData.",None,-1
75065f06-6ba7-405a-92ff-76a6ce7a3246,SEPAL: Spatial Gene Expression Prediction from Local Graphs,0.219333,"Spatial transcriptomics is an emerging technology that aligns histopathology
images with spatially resolved gene expression profiling. It holds the
potential for understanding many diseases but faces significant bottlenecks
such as specialized equipment and domain expertise. In this work, we present
SEPAL, a new model for predicting genetic profiles from visual tissue
appearance. Our method exploits the biological biases of the problem by
directly supervising relative differences with respect to mean expression, and
leverages local visual context at every coordinate to make predictions using a
graph neural network. This approach closes the gap between complete locality
and complete globality in current methods. In addition, we propose a novel
benchmark that aims to better define the task by following current best
practices in transcriptomics and restricting the prediction variables to only
those with clear spatial patterns. Our extensive evaluation in two different
human breast cancer datasets indicates that SEPAL outperforms previous
state-of-the-art methods and other mechanisms of including spatial context.",None,-1
5ebaa540-8dc7-496f-83c9-372dd1a2ae95,FFT-based Dynamic Token Mixer for Vision,0.0393262,"Multi-head-self-attention (MHSA)-equipped models have achieved notable
performance in computer vision. Their computational complexity is proportional
to quadratic numbers of pixels in input feature maps, resulting in slow
processing, especially when dealing with high-resolution images. New types of
token-mixer are proposed as an alternative to MHSA to circumvent this problem:
an FFT-based token-mixer involves global operations similar to MHSA but with
lower computational complexity. However, despite its attractive properties, the
FFT-based token-mixer has not been carefully examined in terms of its
compatibility with the rapidly evolving MetaFormer architecture. Here, we
propose a novel token-mixer called Dynamic Filter and novel image recognition
models, DFFormer and CDFFormer, to close the gaps above. The results of image
classification and downstream tasks, analysis, and visualization show that our
models are helpful. Notably, their throughput and memory efficiency when
dealing with high-resolution image recognition is remarkable. Our results
indicate that Dynamic Filter is one of the token-mixer options that should be
seriously considered. The code is available at
https://github.com/okojoalg/dfformer",None,-1
b50a3cc6-25a9-4bb1-9146-17b48be416ba,Parallel Reasoning Network for Human-Object Interaction Detection,0.384693,"Human-Object Interaction (HOI) detection aims to learn how human interacts
with surrounding objects. Previous HOI detection frameworks simultaneously
detect human, objects and their corresponding interactions by using a
predictor. Using only one shared predictor cannot differentiate the attentive
field of instance-level prediction and relation-level prediction. To solve this
problem, we propose a new transformer-based method named Parallel Reasoning
Network(PR-Net), which constructs two independent predictors for instance-level
localization and relation-level understanding. The former predictor
concentrates on instance-level localization by perceiving instances' extremity
regions. The latter broadens the scope of relation region to reach a better
relation-level semantic understanding. Extensive experiments and analysis on
HICO-DET benchmark exhibit that our PR-Net effectively alleviated this problem.
Our PR-Net has achieved competitive results on HICO-DET and V-COCO benchmarks.",None,-1
28ee54e5-2c66-4fdc-8a3f-f82fe129a226,Translating SUMO-K to Higher-Order Set Theory,0.289069,"We describe a translation from a fragment of SUMO (SUMO-K) into higher-order
set theory. The translation provides a formal semantics for portions of SUMO
which are beyond first-order and which have previously only had an informal
interpretation. It also for the first time embeds a large common-sense ontology
into a very secure interactive theorem proving system. We further extend our
previous work in finding contradictions in SUMO from first order constructs to
include a portion of SUMO's higher order constructs. Finally, using the
translation, we can create problems that can be proven using higher-order
interactive and automated theorem provers. This is tested in several systems
and can be used to form a corpus of higher-order common-sense reasoning
problems.",None,-1
f76fbd64-2591-41ad-986b-dbcccaf01630,Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation,0.374643,"We study the problem of few-shot physically-aware articulated mesh
generation. By observing an articulated object dataset containing only a few
examples, we wish to learn a model that can generate diverse meshes with high
visual fidelity and physical validity. Previous mesh generative models either
have difficulties in depicting a diverse data space from only a few examples or
fail to ensure physical validity of their samples. Regarding the above
challenges, we propose two key innovations, including 1) a hierarchical mesh
deformation-based generative model based upon the divide-and-conquer philosophy
to alleviate the few-shot challenge by borrowing transferrable deformation
patterns from large scale rigid meshes and 2) a physics-aware deformation
correction scheme to encourage physically plausible generations. We conduct
extensive experiments on 6 articulated categories to demonstrate the
superiority of our method in generating articulated meshes with better
diversity, higher visual fidelity, and better physical validity over previous
methods in the few-shot setting. Further, we validate solid contributions of
our two innovations in the ablation study. Project page with code is available
at https://meowuu7.github.io/few-arti-obj-gen.",None,-1
5246d809-b33e-4d58-b0bb-1141502b167e,Mind the Gap! Bridging Explainable Artificial Intelligence and Human Understanding with Luhmann's Functional Theory of Communication,0.311715,"Over the past decade explainable artificial intelligence has evolved from a
predominantly technical discipline into a field that is deeply intertwined with
social sciences. Insights such as human preference for contrastive -- more
precisely, counterfactual -- explanations have played a major role in this
transition, inspiring and guiding the research in computer science. Other
observations, while equally important, have received much less attention. The
desire of human explainees to communicate with artificial intelligence
explainers through a dialogue-like interaction has been mostly neglected by the
community. This poses many challenges for the effectiveness and widespread
adoption of such technologies as delivering a single explanation optimised
according to some predefined objectives may fail to engender understanding in
its recipients and satisfy their unique needs given the diversity of human
knowledge and intention. Using insights elaborated by Niklas Luhmann and, more
recently, Elena Esposito we apply social systems theory to highlight challenges
in explainable artificial intelligence and offer a path forward, striving to
reinvigorate the technical research in this direction. This paper aims to
demonstrate the potential of systems theoretical approaches to communication in
understanding problems and limitations of explainable artificial intelligence.",None,-1
4145ff07-ffac-4f39-88ec-0470268844fa,UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering,0.784189,"In this technical report, we present our solution, named UniOCC, for the
Vision-Centric 3D occupancy prediction track in the nuScenes Open Dataset
Challenge at CVPR 2023. Existing methods for occupancy prediction primarily
focus on optimizing projected features on 3D volume space using 3D occupancy
labels. However, the generation process of these labels is complex and
expensive (relying on 3D semantic annotations), and limited by voxel
resolution, they cannot provide fine-grained spatial semantics. To address this
limitation, we propose a novel Unifying Occupancy (UniOcc) prediction method,
explicitly imposing spatial geometry constraint and complementing fine-grained
semantic supervision through volume ray rendering. Our method significantly
enhances model performance and demonstrates promising potential in reducing
human annotation costs. Given the laborious nature of annotating 3D occupancy,
we further introduce a Depth-aware Teacher Student (DTS) framework to enhance
prediction accuracy using unlabeled data. Our solution achieves 51.27\% mIoU on
the official leaderboard with single model, placing 3rd in this challenge.",None,-1
b407a70d-7929-4120-9a8f-7a5a80efc948,Reflective Linguistic Programming (RLP): A Stepping Stone in Socially-Aware AGI (SocialAGI),0.529254,"This paper presents Reflective Linguistic Programming (RLP), a unique
approach to conversational AI that emphasizes self-awareness and strategic
planning. RLP encourages models to introspect on their own predefined
personality traits, emotional responses to incoming messages, and planned
strategies, enabling contextually rich, coherent, and engaging interactions. A
striking illustration of RLP's potential involves a toy example, an AI persona
with an adversarial orientation, a demon named `Bogus' inspired by the
children's fairy tale Hansel & Gretel. Bogus exhibits sophisticated behaviors,
such as strategic deception and sensitivity to user discomfort, that
spontaneously arise from the model's introspection and strategic planning.
These behaviors are not pre-programmed or prompted, but emerge as a result of
the model's advanced cognitive modeling. The potential applications of RLP in
socially-aware AGI (Social AGI) are vast, from nuanced negotiations and mental
health support systems to the creation of diverse and dynamic AI personas. Our
exploration of deception serves as a stepping stone towards a new frontier in
AGI, one filled with opportunities for advanced cognitive modeling and the
creation of truly human `digital souls'.",None,-1
ec004937-7c47-43d7-93cd-603736092ae1,Prediction-Powered Inference,0.871716,"Prediction-powered inference is a framework for performing valid statistical
inference when an experimental dataset is supplemented with predictions from a
machine-learning system. The framework yields simple algorithms for computing
provably valid confidence intervals for quantities such as means, quantiles,
and linear and logistic regression coefficients, without making any assumptions
on the machine-learning algorithm that supplies the predictions. Furthermore,
more accurate predictions translate to smaller confidence intervals.
Prediction-powered inference could enable researchers to draw valid and more
data-efficient conclusions using machine learning. The benefits of
prediction-powered inference are demonstrated with datasets from proteomics,
astronomy, genomics, remote sensing, census analysis, and ecology.",None,-1
5e4e35a4-199e-41b9-b990-a255c76c5ce4,A Comprehensive Evaluation Study on Risk Level Classification of Melanoma by Computer Vision on ISIC 2016-2020 Datasets,0.0263236,"Skin cancer is the most common type of cancer. Specifically, melanoma is the
cause of 75% of skin cancer deaths, although it is the least common skin
cancer. Better detection of melanoma could have a positive impact on millions
of people. The ISIC archive contains the largest publicly available collection
of dermatoscopic images of skin lesions. In this research, we investigate the
efficacy of applying advanced deep learning techniques in computer vision to
identify melanoma in images of skin lesions. Through reviewing previous
methods, including pre-trained models, deep-learning classifiers, transfer
learning, etc., we demonstrate the applicability of the popular deep learning
methods on critical clinical problems such as identifying melanoma. Finally, we
proposed a processing flow with a validation AUC greater than 94% and a
sensitivity greater than 90% on ISIC 2016 - 2020 datasets.",None,-1
71f3c9e0-b876-4e2e-9d37-5fc248f13461,Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?,0.203153,"An important milestone for AI is the development of algorithms that can
produce drawings that are indistinguishable from those of humans. Here, we
adapt the 'diversity vs. recognizability' scoring framework from Boutin et al,
2022 and find that one-shot diffusion models have indeed started to close the
gap between humans and machines. However, using a finer-grained measure of the
originality of individual samples, we show that strengthening the guidance of
diffusion models helps improve the humanness of their drawings, but they still
fall short of approximating the originality and recognizability of human
drawings. Comparing human category diagnostic features, collected through an
online psychophysics experiment, against those derived from diffusion models
reveals that humans rely on fewer and more localized features. Overall, our
study suggests that diffusion models have significantly helped improve the
quality of machine-generated drawings; however, a gap between humans and
machines remains -- in part explainable by discrepancies in visual strategies.",None,-1
f03d2369-bd4c-4927-9649-26132dd9b485,Crosslingual Retrieval Augmented In-context Learning for Bangla,0.625468,"The promise of Large Language Models (LLMs) in Natural Language Processing
has often been overshadowed by their limited performance in low-resource
languages such as Bangla. To address this, our paper presents a pioneering
approach that utilizes cross-lingual retrieval augmented in-context learning.
By strategically sourcing semantically similar prompts from high-resource
language, we enable multilingual pretrained language models (MPLMs), especially
the generative model BLOOMZ, to successfully boost performance on Bangla tasks.
Our extensive evaluation highlights that the cross-lingual retrieval augmented
prompts bring steady improvements to MPLMs over the zero-shot performance.",None,-1
a620f597-cbea-4cdc-9cc5-ef40faf5edf0,Grandma Karl is 27 years old -- research agenda for pseudonymization of research data,0.825978,"Accessibility of research data is critical for advances in many research
fields, but textual data often cannot be shared due to the personal and
sensitive information which it contains, e.g names or political opinions.
General Data Protection Regulation (GDPR) suggests pseudonymization as a
solution to secure open access to research data, but we need to learn more
about pseudonymization as an approach before adopting it for manipulation of
research data. This paper outlines a research agenda within pseudonymization,
namely need of studies into the effects of pseudonymization on unstructured
data in relation to e.g. readability and language assessment, as well as the
effectiveness of pseudonymization as a way of protecting writer identity, while
also exploring different ways of developing context-sensitive algorithms for
detection, labelling and replacement of personal information in unstructured
data. The recently granted project on pseudonymization Grandma Karl is 27 years
old addresses exactly those challenges.",None,-1
48a9ee20-3414-4ace-8f8a-7744adf94bd7,Mind the Backbone: Minimizing Backbone Distortion for Robust Object Detection,0.0367524,"Building object detectors that are robust to domain shifts is critical for
real-world applications. Prior approaches fine-tune a pre-trained backbone and
risk overfitting it to in-distribution (ID) data and distorting features useful
for out-of-distribution (OOD) generalization. We propose to use Relative
Gradient Norm (RGN) as a way to measure the vulnerability of a backbone to
feature distortion, and show that high RGN is indeed correlated with lower OOD
performance. Our analysis of RGN yields interesting findings: some backbones
lose OOD robustness during fine-tuning, but others gain robustness because
their architecture prevents the parameters from changing too much from the
initial model. Given these findings, we present recipes to boost OOD robustness
for both types of backbones. Specifically, we investigate regularization and
architectural choices for minimizing gradient updates so as to prevent the
tuned backbone from losing generalizable features. Our proposed techniques
complement each other and show substantial improvements over baselines on
diverse architectures and datasets. Code is available at
https://github.com/VisionLearningGroup/mind_back.",None,-1
4318b605-04ec-4cd0-a7af-8114386d1a9a,Diving Deep into Modes of Fact Hallucinations in Dialogue Systems,0.525736,"Knowledge Graph(KG) grounded conversations often use large pre-trained models
and usually suffer from fact hallucination. Frequently entities with no
references in knowledge sources and conversation history are introduced into
responses, thus hindering the flow of the conversation -- existing work attempt
to overcome this issue by tweaking the training procedure or using a multi-step
refining method. However, minimal effort is put into constructing an
entity-level hallucination detection system, which would provide fine-grained
signals that control fallacious content while generating responses. As a first
step to address this issue, we dive deep to identify various modes of
hallucination in KG-grounded chatbots through human feedback analysis.
Secondly, we propose a series of perturbation strategies to create a synthetic
dataset named FADE (FActual Dialogue Hallucination DEtection Dataset). Finally,
we conduct comprehensive data analyses and create multiple baseline models for
hallucination detection to compare against human-verified data and already
established benchmarks.",None,-1
6fc64f83-6466-4a47-9cd7-d82ed23c57d8,SemEval-2023 Task 10: Explainable Detection of Online Sexism,0.999994,"Online sexism is a widespread and harmful phenomenon. Automated tools can
assist the detection of sexism at scale. Binary detection, however, disregards
the diversity of sexist content, and fails to provide clear explanations for
why something is sexist. To address this issue, we introduce SemEval Task 10 on
the Explainable Detection of Online Sexism (EDOS). We make three main
contributions: i) a novel hierarchical taxonomy of sexist content, which
includes granular vectors of sexism to aid explainability; ii) a new dataset of
20,000 social media comments with fine-grained labels, along with larger
unlabelled datasets for model adaptation; and iii) baseline models as well as
an analysis of the methods, results and errors for participant submissions to
our task.",None,-1
f219bddb-228c-4666-a76d-8499150e3596,Learning Road Scene-level Representations via Semantic Region Prediction,0.124315,"In this work, we tackle two vital tasks in automated driving systems, i.e.,
driver intent prediction and risk object identification from egocentric images.
Mainly, we investigate the question: what would be good road scene-level
representations for these two tasks? We contend that a scene-level
representation must capture higher-level semantic and geometric representations
of traffic scenes around ego-vehicle while performing actions to their
destinations. To this end, we introduce the representation of semantic regions,
which are areas where ego-vehicles visit while taking an afforded action (e.g.,
left-turn at 4-way intersections). We propose to learn scene-level
representations via a novel semantic region prediction task and an automatic
semantic region labeling algorithm. Extensive evaluations are conducted on the
HDD and nuScenes datasets, and the learned representations lead to
state-of-the-art performance for driver intention prediction and risk object
identification.",None,-1
dfd1501e-09d7-4700-abdf-69c61fd4d6e9,Have it your way: Individualized Privacy Assignment for DP-SGD,0.714111,"When training a machine learning model with differential privacy, one sets a
privacy budget. This budget represents a maximal privacy violation that any
user is willing to face by contributing their data to the training set. We
argue that this approach is limited because different users may have different
privacy expectations. Thus, setting a uniform privacy budget across all points
may be overly conservative for some users or, conversely, not sufficiently
protective for others. In this paper, we capture these preferences through
individualized privacy budgets. To demonstrate their practicality, we introduce
a variant of Differentially Private Stochastic Gradient Descent (DP-SGD) which
supports such individualized budgets. DP-SGD is the canonical approach to
training models with differential privacy. We modify its data sampling and
gradient noising mechanisms to arrive at our approach, which we call
Individualized DP-SGD (IDP-SGD). Because IDP-SGD provides privacy guarantees
tailored to the preferences of individual users and their data points, we find
it empirically improves privacy-utility trade-offs.",None,-1
2e281e2d-c737-422d-b48b-a5dc4d1d78ad,ContrastNet: A Contrastive Learning Framework for Few-Shot Text Classification,0.781883,"Few-shot text classification has recently been promoted by the meta-learning
paradigm which aims to identify target classes with knowledge transferred from
source classes with sets of small tasks named episodes. Despite their success,
existing works building their meta-learner based on Prototypical Networks are
unsatisfactory in learning discriminative text representations between similar
classes, which may lead to contradictions during label prediction. In addition,
the tasklevel and instance-level overfitting problems in few-shot text
classification caused by a few training examples are not sufficiently tackled.
In this work, we propose a contrastive learning framework named ContrastNet to
tackle both discriminative representation and overfitting problems in few-shot
text classification. ContrastNet learns to pull closer text representations
belonging to the same class and push away text representations belonging to
different classes, while simultaneously introducing unsupervised contrastive
regularization at both task-level and instance-level to prevent overfitting.
Experiments on 8 few-shot text classification datasets show that ContrastNet
outperforms the current state-of-the-art models.",None,-1
aeb13161-2dea-4a44-9c4c-45d29d8f3260,Robustness and Generalizability of Deepfake Detection: A Study with Diffusion Models,0.454549,"The rise of deepfake images, especially of well-known personalities, poses a
serious threat to the dissemination of authentic information. To tackle this,
we present a thorough investigation into how deepfakes are produced and how
they can be identified. The cornerstone of our research is a rich collection of
artificial celebrity faces, titled DeepFakeFace (DFF). We crafted the DFF
dataset using advanced diffusion models and have shared it with the community
through online platforms. This data serves as a robust foundation to train and
test algorithms designed to spot deepfakes. We carried out a thorough review of
the DFF dataset and suggest two evaluation methods to gauge the strength and
adaptability of deepfake recognition tools. The first method tests whether an
algorithm trained on one type of fake images can recognize those produced by
other methods. The second evaluates the algorithm's performance with imperfect
images, like those that are blurry, of low quality, or compressed. Given varied
results across deepfake methods and image changes, our findings stress the need
for better deepfake detectors. Our DFF dataset and tests aim to boost the
development of more effective tools against deepfakes.",None,-1
bc097e55-e0ee-4e00-9147-bd92738cb7ba,SemHint-MD: Learning from Noisy Semantic Labels for Self-Supervised Monocular Depth Estimation,0.186522,"Without ground truth supervision, self-supervised depth estimation can be
trapped in a local minimum due to the gradient-locality issue of the
photometric loss. In this paper, we present a framework to enhance depth by
leveraging semantic segmentation to guide the network to jump out of the local
minimum. Prior works have proposed to share encoders between these two tasks or
explicitly align them based on priors like the consistency between edges in the
depth and segmentation maps. Yet, these methods usually require ground truth or
high-quality pseudo labels, which may not be easily accessible in real-world
applications. In contrast, we investigate self-supervised depth estimation
along with a segmentation branch that is supervised with noisy labels provided
by models pre-trained with limited data. We extend parameter sharing from the
encoder to the decoder and study the influence of different numbers of shared
decoder parameters on model performance. Also, we propose to use cross-task
information to refine current depth and segmentation predictions to generate
pseudo-depth and semantic labels for training. The advantages of the proposed
method are demonstrated through extensive experiments on the KITTI benchmark
and a downstream task for endoscopic tissue deformation tracking.",None,-1
276857af-1b29-4894-a81e-84cb306f2aaf,"Philosophical Foundations of GeoAI: Exploring Sustainability, Diversity, and Bias in GeoAI and Spatial Data Science",0.726076,"This chapter presents some of the fundamental assumptions and principles that
could form the philosophical foundation of GeoAI and spatial data science.
Instead of reviewing the well-established characteristics of spatial data
(analysis), including interaction, neighborhoods, and autocorrelation, the
chapter highlights themes such as sustainability, bias in training data,
diversity in schema knowledge, and the (potential lack of) neutrality of GeoAI
systems from a unifying ethical perspective. Reflecting on our profession's
ethical implications will assist us in conducting potentially disruptive
research more responsibly, identifying pitfalls in designing, training, and
deploying GeoAI-based systems, and developing a shared understanding of the
benefits but also potential dangers of artificial intelligence and machine
learning research across academic fields, all while sharing our unique
(geo)spatial perspective with others.",None,-1
4e996435-debb-4178-8a3b-17666efb17bf,On Evaluating and Mitigating Gender Biases in Multilingual Settings,0.707932,"While understanding and removing gender biases in language models has been a
long-standing problem in Natural Language Processing, prior research work has
primarily been limited to English. In this work, we investigate some of the
challenges with evaluating and mitigating biases in multilingual settings which
stem from a lack of existing benchmarks and resources for bias evaluation
beyond English especially for non-western context. In this paper, we first
create a benchmark for evaluating gender biases in pre-trained masked language
models by extending DisCo to different Indian languages using human
annotations. We extend various debiasing methods to work beyond English and
evaluate their effectiveness for SOTA massively multilingual models on our
proposed metric. Overall, our work highlights the challenges that arise while
studying social biases in multilingual settings and provides resources as well
as mitigation techniques to take a step toward scaling to more languages.",None,-1
67a768c8-b9df-4fd1-9484-efdf2c89d7d7,AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web,0.81599,"Existing datasets for automated fact-checking have substantial limitations,
such as relying on artificial claims, lacking annotations for evidence and
intermediate reasoning, or including evidence published after the claim. In
this paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims
covering fact-checks by 50 different organizations. Each claim is annotated
with question-answer pairs supported by evidence available online, as well as
textual justifications explaining how the evidence combines to produce a
verdict. Through a multi-round annotation process, we avoid common pitfalls
including context dependence, evidence insufficiency, and temporal leakage, and
reach a substantial inter-annotator agreement of $\kappa=0.619$ on verdicts. We
develop a baseline as well as an evaluation scheme for verifying claims through
several question-answering steps against the open web.",None,-1
bc92fa02-5ecd-4f5f-b465-2a7d9962a2b7,NeRF-Supervised Deep Stereo,0.572812,"We introduce a novel framework for training deep stereo networks effortlessly
and without any ground-truth. By leveraging state-of-the-art neural rendering
solutions, we generate stereo training data from image sequences collected with
a single handheld camera. On top of them, a NeRF-supervised training procedure
is carried out, from which we exploit rendered stereo triplets to compensate
for occlusions and depth maps as proxy labels. This results in stereo networks
capable of predicting sharp and detailed disparity maps. Experimental results
show that models trained under this regime yield a 30-40% improvement over
existing self-supervised methods on the challenging Middlebury dataset, filling
the gap to supervised models and, most times, outperforming them at zero-shot
generalization.",None,-1
0020f708-ff02-48d6-8ca2-472ee2f951dc,Vertical Symbolic Regression,0.0811882,"Automating scientific discovery has been a grand goal of Artificial
Intelligence (AI) and will bring tremendous societal impact. Learning symbolic
expressions from experimental data is a vital step in AI-driven scientific
discovery. Despite exciting progress, most endeavors have focused on the
horizontal discovery paths, i.e., they directly search for the best expression
in the full hypothesis space involving all the independent variables.
Horizontal paths are challenging due to the exponentially large hypothesis
space involving all the independent variables. We propose Vertical Symbolic
Regression (VSR) to expedite symbolic regression. The VSR starts by fitting
simple expressions involving a few independent variables under controlled
experiments where the remaining variables are held constant. It then extends
the expressions learned in previous rounds by adding new independent variables
and using new control variable experiments allowing these variables to vary.
The first few steps in vertical discovery are significantly cheaper than the
horizontal path, as their search is in reduced hypothesis spaces involving a
small set of variables. As a consequence, vertical discovery has the potential
to supercharge state-of-the-art symbolic regression approaches in handling
complex equations with many contributing factors. Theoretically, we show that
the search space of VSR can be exponentially smaller than that of horizontal
approaches when learning a class of expressions. Experimentally, VSR
outperforms several baselines in learning symbolic expressions involving many
independent variables.",None,-1
2de89f6d-4dfd-487f-8cb6-57e077eb6f09,Future Aware Pricing and Matching for Sustainable On-demand Ride Pooling,0.655846,"The popularity of on-demand ride pooling is owing to the benefits offered to
customers (lower prices), taxi drivers (higher revenue), environment (lower
carbon footprint due to fewer vehicles) and aggregation companies like Uber
(higher revenue). To achieve these benefits, two key interlinked challenges
have to be solved effectively: (a) pricing -- setting prices to customer
requests for taxis; and (b) matching -- assignment of customers (that accepted
the prices) to taxis/cars. Traditionally, both these challenges have been
studied individually and using myopic approaches (considering only current
requests), without considering the impact of current matching on addressing
future requests. In this paper, we develop a novel framework that handles the
pricing and matching problems together, while also considering the future
impact of the pricing and matching decisions. In our experimental results on a
real-world taxi dataset, we demonstrate that our framework can significantly
improve revenue (up to 17% and on average 6.4%) in a sustainable manner by
reducing the number of vehicles (up to 14% and on average 10.6%) required to
obtain a given fixed revenue and the overall distance travelled by vehicles (up
to 11.1% and on average 3.7%). That is to say, we are able to provide an ideal
win-win scenario for all stakeholders (customers, drivers, aggregator,
environment) involved by obtaining higher revenue for customers, drivers,
aggregator (ride pooling company) while being good for the environment (due to
fewer number of vehicles on the road and lesser fuel consumed).",None,-1
17725416-2576-42d6-84f6-be3f559bf958,A Systematic Analysis of Vocabulary and BPE Settings for Optimal Fine-tuning of NMT: A Case Study of In-domain Translation,0.575362,"The effectiveness of Neural Machine Translation (NMT) models largely depends
on the vocabulary used at training; small vocabularies can lead to
out-of-vocabulary problems -- large ones, to memory issues. Subword (SW)
tokenization has been successfully employed to mitigate these issues. The
choice of vocabulary and SW tokenization has a significant impact on both
training and fine-tuning an NMT model. Fine-tuning is a common practice in
optimizing an MT model with respect to new data. However, new data potentially
introduces new words (or tokens), which, if not taken into consideration, may
lead to suboptimal performance. In addition, the distribution of tokens in the
new data can differ from the distribution of the original data. As such, the
original SW tokenization model could be less suitable for the new data. Through
a systematic empirical evaluation, in this work we compare different strategies
for SW tokenization and vocabulary generation with the ultimate goal to uncover
an optimal setting for fine-tuning a domain-specific model. Furthermore, we
developed several (in-domain) models, the best of which achieves 6 BLEU points
improvement over the baseline.",None,-1
d13c2278-08eb-4c91-9100-dfd13caadf0d,Householder Projector for Unsupervised Latent Semantics Discovery,0.733369,"Generative Adversarial Networks (GANs), especially the recent style-based
generators (StyleGANs), have versatile semantics in the structured latent
space. Latent semantics discovery methods emerge to move around the latent code
such that only one factor varies during the traversal. Recently, an
unsupervised method proposed a promising direction to directly use the
eigenvectors of the projection matrix that maps latent codes to features as the
interpretable directions. However, one overlooked fact is that the projection
matrix is non-orthogonal and the number of eigenvectors is too large. The
non-orthogonality would entangle semantic attributes in the top few
eigenvectors, and the large dimensionality might result in meaningless
variations among the directions even if the matrix is orthogonal. To avoid
these issues, we propose Householder Projector, a flexible and general low-rank
orthogonal matrix representation based on Householder transformations, to
parameterize the projection matrix. The orthogonality guarantees that the
eigenvectors correspond to disentangled interpretable semantics, while the
low-rank property encourages that each identified direction has meaningful
variations. We integrate our projector into pre-trained StyleGAN2/StyleGAN3 and
evaluate the models on several benchmarks. Within only $1\%$ of the original
training steps for fine-tuning, our projector helps StyleGANs to discover more
disentangled and precise semantic attributes without sacrificing image
fidelity.",None,-1
3e3f918f-f277-4708-b3bb-a1153d88a8dd,Exploring Methods for Building Dialects-Mandarin Code-Mixing Corpora: A Case Study in Taiwanese Hokkien,0.0687832,"In natural language processing (NLP), code-mixing (CM) is a challenging task,
especially when the mixed languages include dialects. In Southeast Asian
countries such as Singapore, Indonesia, and Malaysia, Hokkien-Mandarin is the
most widespread code-mixed language pair among Chinese immigrants, and it is
also common in Taiwan. However, dialects such as Hokkien often have a scarcity
of resources and the lack of an official writing system, limiting the
development of dialect CM research. In this paper, we propose a method to
construct a Hokkien-Mandarin CM dataset to mitigate the limitation, overcome
the morphological issue under the Sino-Tibetan language family, and offer an
efficient Hokkien word segmentation method through a linguistics-based toolkit.
Furthermore, we use our proposed dataset and employ transfer learning to train
the XLM (cross-lingual language model) for translation tasks. To fit the
code-mixing scenario, we adapt XLM slightly. We found that by using linguistic
knowledge, rules, and language tags, the model produces good results on CM data
translation while maintaining monolingual translation quality.",None,-1
c280c954-afbb-4f67-865d-a498736cded4,Measuring axiomatic soundness of counterfactual image models,0.907701,"We present a general framework for evaluating image counterfactuals. The
power and flexibility of deep generative models make them valuable tools for
learning mechanisms in structural causal models. However, their flexibility
makes counterfactual identifiability impossible in the general case. Motivated
by these issues, we revisit Pearl's axiomatic definition of counterfactuals to
determine the necessary constraints of any counterfactual inference model:
composition, reversibility, and effectiveness. We frame counterfactuals as
functions of an input variable, its parents, and counterfactual parents and use
the axiomatic constraints to restrict the set of functions that could represent
the counterfactual, thus deriving distance metrics between the approximate and
ideal functions. We demonstrate how these metrics can be used to compare and
choose between different approximate counterfactual inference models and to
provide insight into a model's shortcomings and trade-offs.",None,-1
4dd07f57-4d48-4c7f-8288-2aba3afdba6b,End-to-end Manipulator Calligraphy Planning via Variational Imitation Learning,0.267051,"Planning from demonstrations has shown promising results with the advances of
deep neural networks. One of the most popular real-world applications is
automated handwriting using a robotic manipulator. Classically it is simplified
as a two-dimension problem. This representation is suitable for elementary
drawings, but it is not sufficient for Japanese calligraphy or complex work of
art where the orientation of a pen is part of the user expression. In this
study, we focus on automated planning of Japanese calligraphy using a
three-dimension representation of the trajectory as well as the rotation of the
pen tip, and propose a novel deep imitation learning neural network that learns
from expert demonstrations through a combination of images and pose data. The
network consists of a combination of variational auto-encoder, bi-directional
LSTM, and Multi-Layer Perceptron (MLP). Experiments are conducted in a
progressive way, and results demonstrate that the proposed approach is
successful in completion of tasks for real-world robots, overcoming the
distribution shift problem in imitation learning. The source code and dataset
will be public.",None,-1
e92c95fd-fbe7-43c4-8ab2-982b53b918f2,WISE: full-Waveform variational Inference via Subsurface Extensions,0.997912,"We introduce a probabilistic technique for full-waveform inversion, employing
variational inference and conditional normalizing flows to quantify uncertainty
in migration-velocity models and its impact on imaging. Our approach integrates
generative artificial intelligence with physics-informed common-image gathers,
reducing reliance on accurate initial velocity models. Considered case studies
demonstrate its efficacy producing realizations of migration-velocity models
conditioned by the data. These models are used to quantify amplitude and
positioning effects during subsequent imaging.",None,-1
068a2260-1388-43ed-8aac-1a95e58b88a2,Advances and Challenges in Multimodal Remote Sensing Image Registration,0.988702,"Over the past few decades, with the rapid development of global aerospace and
aerial remote sensing technology, the types of sensors have evolved from the
traditional monomodal sensors (e.g., optical sensors) to the new generation of
multimodal sensors [e.g., multispectral, hyperspectral, light detection and
ranging (LiDAR) and synthetic aperture radar (SAR) sensors]. These advanced
devices can dynamically provide various and abundant multimodal remote sensing
images with different spatial, temporal, and spectral resolutions according to
different application requirements. Since then, it is of great scientific
significance to carry out the research of multimodal remote sensing image
registration, which is a crucial step for integrating the complementary
information among multimodal data and making comprehensive observations and
analysis of the Earths surface. In this work, we will present our own
contributions to the field of multimodal image registration, summarize the
advantages and limitations of existing multimodal image registration methods,
and then discuss the remaining challenges and make a forward-looking prospect
for the future development of the field.",None,-1
583d08c6-89e2-47ec-ae26-7691835672b1,Challenge Results Are Not Reproducible,0.105293,"While clinical trials are the state-of-the-art methods to assess the effect
of new medication in a comparative manner, benchmarking in the field of medical
image analysis is performed by so-called challenges. Recently, comprehensive
analysis of multiple biomedical image analysis challenges revealed large
discrepancies between the impact of challenges and quality control of the
design and reporting standard. This work aims to follow up on these results and
attempts to address the specific question of the reproducibility of the
participants methods. In an effort to determine whether alternative
interpretations of the method description may change the challenge ranking, we
reproduced the algorithms submitted to the 2019 Robust Medical Image
Segmentation Challenge (ROBUST-MIS). The leaderboard differed substantially
between the original challenge and reimplementation, indicating that challenge
rankings may not be sufficiently reproducible.",None,-1
356c487a-788e-4ff0-a9e5-c6ec2fde7f2d,Adaptive Prompt Learning with Distilled Connective Knowledge for Implicit Discourse Relation Recognition,0.273447,"Implicit discourse relation recognition (IDRR) aims at recognizing the
discourse relation between two text segments without an explicit connective.
Recently, the prompt learning has just been applied to the IDRR task with great
performance improvements over various neural network-based approaches. However,
the discrete nature of the state-art-of-art prompting approach requires manual
design of templates and answers, a big hurdle for its practical applications.
In this paper, we propose a continuous version of prompt learning together with
connective knowledge distillation, called AdaptPrompt, to reduce manual design
efforts via continuous prompting while further improving performance via
knowledge transfer. In particular, we design and train a few virtual tokens to
form continuous templates and automatically select the most suitable one by
gradient search in the embedding space. We also design an answer-relation
mapping rule to generate a few virtual answers as the answer space.
Furthermore, we notice the importance of annotated connectives in the training
dataset and design a teacher-student architecture for knowledge transfer.
Experiments on the up-to-date PDTB Corpus V3.0 validate our design objectives
in terms of the better relation recognition performance over the
state-of-the-art competitors.",None,-1
cabcab32-5395-44c4-9b9b-91af57cb1786,Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception,0.827748,"Multi-agent collaborative perception as a potential application for
vehicle-to-everything communication could significantly improve the perception
performance of autonomous vehicles over single-agent perception. However,
several challenges remain in achieving pragmatic information sharing in this
emerging research. In this paper, we propose SCOPE, a novel collaborative
perception framework that aggregates the spatio-temporal awareness
characteristics across on-road agents in an end-to-end manner. Specifically,
SCOPE has three distinct strengths: i) it considers effective semantic cues of
the temporal context to enhance current representations of the target agent;
ii) it aggregates perceptually critical spatial information from heterogeneous
agents and overcomes localization errors via multi-scale feature interactions;
iii) it integrates multi-source representations of the target agent based on
their complementary contributions by an adaptive fusion paradigm. To thoroughly
evaluate SCOPE, we consider both real-world and simulated scenarios of
collaborative 3D object detection tasks on three datasets. Extensive
experiments demonstrate the superiority of our approach and the necessity of
the proposed components.",None,-1
b3bfda69-5bd9-449d-8514-88c2774624f6,Self-Supervised Temporal Analysis of Spatiotemporal Data,0.291995,"There exists a correlation between geospatial activity temporal patterns and
type of land use. A novel self-supervised approach is proposed to stratify
landscape based on mobility activity time series. First, the time series signal
is transformed to the frequency domain and then compressed into task-agnostic
temporal embeddings by a contractive autoencoder, which preserves cyclic
temporal patterns observed in time series. The pixel-wise embeddings are
converted to image-like channels that can be used for task-based, multimodal
modeling of downstream geospatial tasks using deep semantic segmentation.
Experiments show that temporal embeddings are semantically meaningful
representations of time series data and are effective across different tasks
such as classifying residential area and commercial areas.",None,-1
4de5df00-cb28-4391-a401-0590e8b4547d,Human-Inspired Framework to Accelerate Reinforcement Learning,0.0849902,"Reinforcement learning (RL) is crucial for data science decision-making but
suffers from sample inefficiency, particularly in real-world scenarios with
costly physical interactions. This paper introduces a novel human-inspired
framework to enhance RL algorithm sample efficiency. It achieves this by
initially exposing the learning agent to simpler tasks that progressively
increase in complexity, ultimately leading to the main task. This method
requires no pre-training and involves learning simpler tasks for just one
iteration. The resulting knowledge can facilitate various transfer learning
approaches, such as value and policy transfer, without increasing computational
complexity. It can be applied across different goals, environments, and RL
algorithms, including value-based, policy-based, tabular, and deep RL methods.
Experimental evaluations demonstrate the framework's effectiveness in enhancing
sample efficiency, especially in challenging main tasks, demonstrated through
both a simple Random Walk and more complex optimal control problems with
constraints.",None,-1
d247c2c5-c135-4623-a0be-acf1ac4ab7c9,Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method,0.541608,"Theorem proving is a fundamental task in mathematics. With the advent of
large language models (LLMs) and interactive theorem provers (ITPs) like Lean,
there has been growing interest in integrating LLMs and ITPs to automate
theorem proving. In this approach, the LLM generates proof steps (tactics), and
the ITP checks the applicability of the tactics at the current goal. The two
systems work together to complete the proof. In this paper, we introduce
DS-Prover, a novel dynamic sampling method for theorem proving. This method
dynamically determines the number of tactics to apply to expand the current
goal, taking into account the remaining time compared to the total allocated
time for proving a theorem. This makes the proof search process more efficient
by adjusting the balance between exploration and exploitation as time passes.
We also augment the training dataset by decomposing simplification and rewrite
tactics with multiple premises into tactics with single premises. This gives
the model more examples to learn from and helps it to predict the tactics with
premises more accurately. We perform our experiments using the Mathlib dataset
of the Lean theorem prover and report the performance on two standard datasets,
MiniF2F and ProofNet. Our methods achieve significant performance gains on both
datasets. We achieved a state-of-the-art performance (Pass@1) of 14.2% on the
ProofNet dataset and a performance of 29.8% on MiniF2F, slightly surpassing the
best-reported Pass@1 of 29.6% using Lean.",None,-1
b229e690-de63-4282-88af-2a58663bd162,Intentional Biases in LLM Responses,0.0344492,"In this study we intentionally introduce biases into large language model
responses in an attempt to create specific personas for interactive media
purposes. We explore the differences between open source models such as
Falcon-7b and the GPT-4 model from Open AI, and we quantify some differences in
responses afforded by the two systems. We find that the guardrails in the GPT-4
mixture of experts models with a supervisor, while useful in assuring AI
alignment in general, are detrimental in trying to construct personas with a
variety of uncommon viewpoints. This study aims to set the groundwork for
future exploration in intentional biases of large language models such that
these practices can be applied in the creative field, and new forms of media.",None,-1
219b365f-1d60-40a4-8916-2d25891eecdf,Incremental Generalized Category Discovery,0.787969,"We explore the problem of Incremental Generalized Category Discovery (IGCD).
This is a challenging category incremental learning setting where the goal is
to develop models that can correctly categorize images from previously seen
categories, in addition to discovering novel ones. Learning is performed over a
series of time steps where the model obtains new labeled and unlabeled data,
and discards old data, at each iteration. The difficulty of the problem is
compounded in our generalized setting as the unlabeled data can contain images
from categories that may or may not have been observed before. We present a new
method for IGCD which combines non-parametric categorization with efficient
image sampling to mitigate catastrophic forgetting. To quantify performance, we
propose a new benchmark dataset named iNatIGCD that is motivated by a
real-world fine-grained visual categorization task. In our experiments we
outperform existing related methods",None,-1
93aaddcd-286d-4d6d-9e65-b261e23d3b42,Forward-Forward Contrastive Learning,0.208508,"Medical image classification is one of the most important tasks for
computer-aided diagnosis. Deep learning models, particularly convolutional
neural networks, have been successfully used for disease classification from
medical images, facilitated by automated feature learning. However, the diverse
imaging modalities and clinical pathology make it challenging to construct
generalized and robust classifications. Towards improving the model
performance, we propose a novel pretraining approach, namely Forward Forward
Contrastive Learning (FFCL), which leverages the Forward-Forward Algorithm in a
contrastive learning framework--both locally and globally. Our experimental
results on the chest X-ray dataset indicate that the proposed FFCL achieves
superior performance (3.69% accuracy over ImageNet pretrained ResNet-18) over
existing pretraining models in the pneumonia classification task. Moreover,
extensive ablation experiments support the particular local and global
contrastive pretraining design in FFCL.",None,-1
63bb0c61-7b0f-44c3-8dbb-d4d818a96ece,Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models,0.535015,"Answering multi-hop reasoning questions requires retrieving and synthesizing
information from diverse sources. Large Language Models (LLMs) struggle to
perform such reasoning consistently. Here we propose an approach to pinpoint
and rectify multi-hop reasoning failures through targeted memory injections on
LLM attention heads. First, we analyze the per-layer activations of GPT-2
models in response to single and multi-hop prompts. We then propose a mechanism
that allows users to inject pertinent prompt-specific information, which we
refer to as ""memories,"" at critical LLM locations during inference. By thus
enabling the LLM to incorporate additional relevant information during
inference, we enhance the quality of multi-hop prompt completions. We show
empirically that a simple, efficient, and targeted memory injection into a key
attention layer can often increase the probability of the desired next token in
multi-hop tasks, by up to 424%.",None,-1
0702c79d-1e59-41f2-a854-8307e7bd5b9f,Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs,0.999559,"In this study, we introduce adaptive KV cache compression, a plug-and-play
method that reduces the memory footprint of generative inference for Large
Language Models (LLMs). Different from the conventional KV cache that retains
key and value vectors for all context tokens, we conduct targeted profiling to
discern the intrinsic structure of attention modules. Based on the recognized
structure, we then construct the KV cache in an adaptive manner: evicting
long-range contexts on attention heads emphasizing local contexts, discarding
non-special tokens on attention heads centered on special tokens, and only
employing the standard KV cache for attention heads that broadly attend to all
tokens. Moreover, with the lightweight attention profiling used to guide the
construction of the adaptive KV cache, FastGen can be deployed without
resource-intensive fine-tuning or re-training. In our experiments across
various asks, FastGen demonstrates substantial reduction on GPU memory
consumption with negligible generation quality loss. We will release our code
and the compatible CUDA kernel for reproducibility.",None,-1
27fd49b6-5605-40c4-af41-8581ca30ebe5,HunSum-1: an Abstractive Summarization Dataset for Hungarian,0.0839327,"We introduce HunSum-1: a dataset for Hungarian abstractive summarization,
consisting of 1.14M news articles. The dataset is built by collecting, cleaning
and deduplicating data from 9 major Hungarian news sites through CommonCrawl.
Using this dataset, we build abstractive summarizer models based on huBERT and
mT5. We demonstrate the value of the created dataset by performing a
quantitative and qualitative analysis on the models' results. The HunSum-1
dataset, all models used in our experiments and our code are available open
source.",None,-1
b81f34de-f080-4e74-ae2d-ee2589f41f1f,Analogue and Physical Reservoir Computing Using Water Waves,0.652678,"More than 3.5 billion people live in rural areas, where water and water
energy resources play an important role in ensuring sustainable and productive
rural economies. This article reviews and critically analyses the recent
advances in the field of analogue and reservoir computing that have been driven
by unique physical properties and energy of water waves. It also demonstrates
that analogue and reservoir computing hold the potential to bring artificial
intelligence closer to people living outside large cities, thus enabling them
to enjoy the benefits of novel technologies that already work in large cities
but are not readily available and suitable for regional communities.",None,-1
19cd5548-d873-4972-a711-c0274bf4ea4f,"SAM Struggles in Concealed Scenes -- Empirical Study on ""Segment Anything""",0.883141,"Segmenting anything is a ground-breaking step toward artificial general
intelligence, and the Segment Anything Model (SAM) greatly fosters the
foundation models for computer vision. We could not be more excited to probe
the performance traits of SAM. In particular, exploring situations in which SAM
does not perform well is interesting. In this report, we choose three concealed
scenes, i.e., camouflaged animals, industrial defects, and medical lesions, to
evaluate SAM under unprompted settings. Our main observation is that SAM looks
unskilled in concealed scenes.",None,-1
7531b7e9-8802-4fa9-9bc7-153e2ed1f63b,Advancing Beyond Identification: Multi-bit Watermark for Large Language Models,0.476187,"We show the viability of tackling misuses of large language models beyond the
identification of machine-generated text. While existing zero-bit watermark
methods focus on detection only, some malicious misuses demand tracing the
adversary user for counteracting them. To address this, we propose Multi-bit
Watermark via Position Allocation, embedding traceable multi-bit information
during language model generation. Through allocating tokens onto different
parts of the messages, we embed longer messages in high corruption settings
without added latency. By independently embedding sub-units of messages, the
proposed method outperforms the existing works in terms of robustness and
latency. Leveraging the benefits of zero-bit watermarking, our method enables
robust extraction of the watermark without any model access, embedding and
extraction of long messages ($\geq$ 32-bit) without finetuning, and maintaining
text quality, while allowing zero-bit detection all at the same time. Code is
released here: https://github.com/bangawayoo/mb-lm-watermarking",None,-1
6db9f241-e623-42ba-b1f2-5511d83e1abc,Research on road object detection algorithm based on improved YOLOX,0.0614466,"Road object detection is an important branch of automatic driving technology,
The model with higher detection accuracy is more conducive to the safe driving
of vehicles. In road object detection, the omission of small objects and
occluded objects is an important problem. therefore, reducing the missed rate
of the object is of great significance for safe driving. In the work of this
paper, based on the YOLOX object detection algorithm to improve, proposes
DecIoU boundary box regression loss function to improve the shape consistency
of the predicted and real box, and Push Loss is introduced to further optimize
the boundary box regression loss function, in order to detect more occluded
objects. In addition, the dynamic anchor box mechanism is also used to improve
the accuracy of the confidence label, improve the label inaccuracy of object
detection model without anchor box. A large number of experiments on KITTI
dataset demonstrate the effectiveness of the proposed method, the improved
YOLOX-s achieved 88.9% mAP and 91.0% mAR on the KITTI dataset, compared to the
baseline version improvements of 2.77% and 4.24%; the improved YOLOX-m achieved
89.1% mAP and 91.4% mAR, compared to the baseline version improvements of 2.30%
and 4.10%.",None,-1
924c78cb-1847-4463-adac-de9bc5323e34,DiffVoice: Text-to-Speech with Latent Diffusion,0.768762,"In this work, we present DiffVoice, a novel text-to-speech model based on
latent diffusion. We propose to first encode speech signals into a phoneme-rate
latent representation with a variational autoencoder enhanced by adversarial
training, and then jointly model the duration and the latent representation
with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS
datasets demonstrate that our method beats the best publicly available systems
in naturalness. By adopting recent generative inverse problem solving
algorithms for diffusion models, DiffVoice achieves the state-of-the-art
performance in text-based speech editing, and zero-shot adaptation.",None,-1
c27d1ec3-1bcd-480a-9cd3-93c701b40f86,TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training,0.321341,"Sparsely gated Mixture-of-Expert (MoE) has demonstrated its effectiveness in
scaling up deep neural networks to an extreme scale. Despite that numerous
efforts have been made to improve the performance of MoE from the model design
or system optimization perspective, existing MoE dispatch patterns are still
not able to fully exploit the underlying heterogeneous network environments. In
this paper, we propose TA-MoE, a topology-aware routing strategy for
large-scale MoE trainging, from a model-system co-design perspective, which can
dynamically adjust the MoE dispatch pattern according to the network topology.
Based on communication modeling, we abstract the dispatch problem into an
optimization objective and obtain the approximate dispatch pattern under
different topologies. On top of that, we design a topology-aware auxiliary
loss, which can adaptively route the data to fit in the underlying topology
without sacrificing the model accuracy. Experiments show that TA-MoE can
substantially outperform its counterparts on various hardware and model
configurations, with roughly 1.01x-1.61x, 1.01x-4.77x, 1.25x-1.54x improvements
over the popular DeepSpeed-MoE, FastMoE and FasterMoE.",None,-1
3d5b5051-e937-4715-94c7-23d0f114146f,Removing Image Artifacts From Scratched Lens Protectors,0.0993372,"A protector is placed in front of the camera lens for mobile devices to avoid
damage, while the protector itself can be easily scratched accidentally,
especially for plastic ones. The artifacts appear in a wide variety of
patterns, making it difficult to see through them clearly. Removing image
artifacts from the scratched lens protector is inherently challenging due to
the occasional flare artifacts and the co-occurring interference within mixed
artifacts. Though different methods have been proposed for some specific
distortions, they seldom consider such inherent challenges. In our work, we
consider the inherent challenges in a unified framework with two cooperative
modules, which facilitate the performance boost of each other. We also collect
a new dataset from the real world to facilitate training and evaluation
purposes. The experimental results demonstrate that our method outperforms the
baselines qualitatively and quantitatively. The code and datasets will be
released after acceptance.",None,-1
9486623a-317c-465e-86a1-4b614d467929,Anthropomorphization of AI: Opportunities and Risks,0.899339,"Anthropomorphization is the tendency to attribute human-like traits to
non-human entities. It is prevalent in many social contexts -- children
anthropomorphize toys, adults do so with brands, and it is a literary device.
It is also a versatile tool in science, with behavioral psychology and
evolutionary biology meticulously documenting its consequences. With widespread
adoption of AI systems, and the push from stakeholders to make it human-like
through alignment techniques, human voice, and pictorial avatars, the tendency
for users to anthropomorphize it increases significantly. We take a dyadic
approach to understanding this phenomenon with large language models (LLMs) by
studying (1) the objective legal implications, as analyzed through the lens of
the recent blueprint of AI bill of rights and the (2) subtle psychological
aspects customization and anthropomorphization. We find that anthropomorphized
LLMs customized for different user bases violate multiple provisions in the
legislative blueprint. In addition, we point out that anthropomorphization of
LLMs affects the influence they can have on their users, thus having the
potential to fundamentally change the nature of human-AI interaction, with
potential for manipulation and negative influence. With LLMs being
hyper-personalized for vulnerable groups like children and patients among
others, our work is a timely and important contribution. We propose a
conservative strategy for the cautious use of anthropomorphization to improve
trustworthiness of AI systems.",None,-1
6961b17a-b50f-4350-aaca-d612ec7280d3,Contrasting Linguistic Patterns in Human and LLM-Generated Text,0.465616,"We conduct a quantitative analysis contrasting human-written English news
text with comparable large language model (LLM) output from 4 LLMs from the
LLaMa family. Our analysis spans several measurable linguistic dimensions,
including morphological, syntactic, psychometric and sociolinguistic aspects.
The results reveal various measurable differences between human and
AI-generated texts. Among others, human texts exhibit more scattered sentence
length distributions, a distinct use of dependency and constituent types,
shorter constituents, and more aggressive emotions (fear, disgust) than
LLM-generated texts. LLM outputs use more numbers, symbols and auxiliaries
(suggesting objective language) than human texts, as well as more pronouns. The
sexist bias prevalent in human text is also expressed by LLMs.",None,-1
87656744-a699-44b2-9085-aaa56fab126b,HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention,0.797609,"The success of large-scale contrastive vision-language pretraining (CLIP) has
benefited both visual recognition and multimodal content understanding. The
concise design brings CLIP the advantage in inference efficiency against other
vision-language models with heavier cross-attention fusion layers, making it a
popular choice for a wide spectrum of downstream tasks. However, CLIP does not
explicitly capture the hierarchical nature of high-level and fine-grained
semantics conveyed in images and texts, which is arguably critical to
vision-language understanding and reasoning. To this end, we equip both the
visual and language branches in CLIP with hierarchy-aware attentions, namely
Hierarchy-aware CLIP (HiCLIP), to progressively discover semantic hierarchies
layer-by-layer from both images and texts in an unsupervised manner. As a
result, such hierarchical aggregation significantly improves the cross-modal
alignment. To demonstrate the advantages of HiCLIP, we conduct qualitative
analysis on its unsupervised hierarchy induction during inference, as well as
extensive quantitative experiments on both visual recognition and
vision-language downstream tasks.",None,-1
5ad25e5d-0e40-4ccf-80e6-0c39aa6c0ae8,Chit-Chat or Deep Talk: Prompt Engineering for Process Mining,0.669416,"This research investigates the application of Large Language Models (LLMs) to
augment conversational agents in process mining, aiming to tackle its inherent
complexity and diverse skill requirements. While LLM advancements present novel
opportunities for conversational process mining, generating efficient outputs
is still a hurdle. We propose an innovative approach that amend many issues in
existing solutions, informed by prior research on Natural Language Processing
(NLP) for conversational agents. Leveraging LLMs, our framework improves both
accessibility and agent performance, as demonstrated by experiments on public
question and data sets. Our research sets the stage for future explorations
into LLMs' role in process mining and concludes with propositions for enhancing
LLM memory, implementing real-time user testing, and examining diverse data
sets.",None,-1
edef9ae1-2ef1-4f1b-8d71-9e3b6dbe4d51,Deep Unsupervised Learning Using Spike-Timing-Dependent Plasticity,0.648952,"Spike-Timing-Dependent Plasticity (STDP) is an unsupervised learning
mechanism for Spiking Neural Networks (SNNs) that has received significant
attention from the neuromorphic hardware community. However, scaling such local
learning techniques to deeper networks and large-scale tasks has remained
elusive. In this work, we investigate a Deep-STDP framework where a rate-based
convolutional network, that can be deployed in a neuromorphic setting, is
trained in tandem with pseudo-labels generated by the STDP clustering process
on the network outputs. We achieve $24.56\%$ higher accuracy and $3.5\times$
faster convergence speed at iso-accuracy on a 10-class subset of the Tiny
ImageNet dataset in contrast to a $k$-means clustering approach.",None,-1
30babebb-e99f-4bd8-b8ad-fb4b77ea8baa,Single-Trajectory Distributionally Robust Reinforcement Learning,0.240047,"As a framework for sequential decision-making, Reinforcement Learning (RL)
has been regarded as an essential component leading to Artificial General
Intelligence (AGI). However, RL is often criticized for having the same
training environment as the test one, which also hinders its application in the
real world. To mitigate this problem, Distributionally Robust RL (DRRL) is
proposed to improve the worst performance in a set of environments that may
contain the unknown test environment. Due to the nonlinearity of the robustness
goal, most of the previous work resort to the model-based approach, learning
with either an empirical distribution learned from the data or a simulator that
can be sampled infinitely, which limits their applications in simple dynamics
environments. In contrast, we attempt to design a DRRL algorithm that can be
trained along a single trajectory, i.e., no repeated sampling from a state.
Based on the standard Q-learning, we propose distributionally robust Q-learning
with the single trajectory (DRQ) and its average-reward variant named
differential DRQ. We provide asymptotic convergence guarantees and experiments
for both settings, demonstrating their superiority in the perturbed
environments against the non-robust ones.",None,-1
054509f3-8821-427e-8bc0-bc428d4455dc,Geometric Ultrasound Localization Microscopy,0.718299,"Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for
non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound
Localization Microscopy (ULM) has enabled a revolutionary breakthrough by
offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers
are used to render ULM frames, ultimately determining the image resolution
capability. To take full advantage of ULM, this study questions whether
beamforming is the most effective processing step for ULM, suggesting an
alternative approach that relies solely on Time-Difference-of-Arrival (TDoA)
information. To this end, a novel geometric framework for micro bubble
localization via ellipse intersections is proposed to overcome existing
beamforming limitations. We present a benchmark comparison based on a public
dataset for which our geometric ULM outperforms existing baseline methods in
terms of accuracy and robustness while only utilizing a portion of the
available transducer data.",None,-1
6e55f8bd-20f1-45d5-92a4-dd62d714feaf,Distribution-Aligned Diffusion for Human Mesh Recovery,0.696941,"Recovering a 3D human mesh from a single RGB image is a challenging task due
to depth ambiguity and self-occlusion, resulting in a high degree of
uncertainty. Meanwhile, diffusion models have recently seen much success in
generating high-quality outputs by progressively denoising noisy inputs.
Inspired by their capability, we explore a diffusion-based approach for human
mesh recovery, and propose a Human Mesh Diffusion (HMDiff) framework which
frames mesh recovery as a reverse diffusion process. We also propose a
Distribution Alignment Technique (DAT) that infuses prior distribution
information into the mesh distribution diffusion process, and provides useful
prior knowledge to facilitate the mesh recovery task. Our method achieves
state-of-the-art performance on three widely used datasets. Project page:
https://gongjia0208.github.io/HMDiff/.",None,-1
0401b320-4b18-4f19-a017-07c8b6825ec5,Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation,0.401611,"This paper reports on the use of prompt engineering and GPT-3.5 for
biomedical query-focused multi-document summarisation. Using GPT-3.5 and
appropriate prompts, our system achieves top ROUGE-F1 results in the task of
obtaining short-paragraph-sized answers to biomedical questions in the 2023
BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in
other domains: 1) Prompts that incorporated few-shot samples generally improved
on their counterpart zero-shot variants; 2) The largest improvement was
achieved by retrieval augmented generation. The fact that these prompts allow
our top runs to rank within the top two runs of BioASQ 11b demonstrate the
power of using adequate prompts for Large Language Models in general, and
GPT-3.5 in particular, for query-focused summarisation.",None,-1
bb983cd6-9cc6-4020-967e-e10af613328b,Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination,0.621822,"Enormous hand images with reliable annotations are collected through
marker-based MoCap. Unfortunately, degradations caused by markers limit their
application in hand appearance reconstruction. A clear appearance recovery
insight is an image-to-image translation trained with unpaired data. However,
most frameworks fail because there exists structure inconsistency from a
degraded hand to a bare one. The core of our approach is to first disentangle
the bare hand structure from those degraded images and then wrap the appearance
to this structure with a dual adversarial discrimination (DAD) scheme. Both
modules take full advantage of the semi-supervised learning paradigm: The
structure disentanglement benefits from the modeling ability of ViT, and the
translator is enhanced by the dual discrimination on both translation processes
and translation results. Comprehensive evaluations have been conducted to prove
that our framework can robustly recover photo-realistic hand appearance from
diverse marker-contained and even object-occluded datasets. It provides a novel
avenue to acquire bare hand appearance data for other downstream learning
problems.The codes will be publicly available at https://www.yangangwang.com",None,-1
0889e5b3-68aa-4c1c-8d16-5e540f60ee99,Passive Radio Frequency-based 3D Indoor Positioning System via Ensemble Learning,0.832189,"Passive radio frequency (PRF)-based indoor positioning systems (IPS) have
attracted researchers' attention due to their low price, easy and customizable
configuration, and non-invasive design. This paper proposes a PRF-based
three-dimensional (3D) indoor positioning system (PIPS), which is able to use
signals of opportunity (SoOP) for positioning and also capture a scenario
signature. PIPS passively monitors SoOPs containing scenario signatures through
a single receiver. Moreover, PIPS leverages the Dynamic Data Driven
Applications System (DDDAS) framework to devise and customize the sampling
frequency, enabling the system to use the most impacted frequency band as the
rated frequency band. Various regression methods within three ensemble learning
strategies are used to train and predict the receiver position. The PRF
spectrum of 60 positions is collected in the experimental scenario, and three
criteria are applied to evaluate the performance of PIPS. Experimental results
show that the proposed PIPS possesses the advantages of high accuracy,
configurability, and robustness.",None,-1
2ac3a803-6253-41ae-9631-3c1405e9102b,Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting,0.926004,"Knowledge Graph Completion (KGC) often requires both KG structural and
textual information to be effective. Pre-trained Language Models (PLMs) have
been used to learn the textual information, usually under the fine-tune
paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly
focus on the textual information and overlook structural knowledge. To tackle
this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC)
which maintains a balance between structural information and textual knowledge.
CSProm-KG only tunes the parameters of Conditional Soft Prompts that are
generated by the entities and relations representations. We verify the
effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR,
FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and
ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new
state-of-the-art on these benchmarks. We conduct further analysis to show (i)
the effectiveness of our proposed components, (ii) the efficiency of CSProm-KG,
and (iii) the flexibility of CSProm-KG.",None,-1
91b84020-471f-4630-be51-79d592225579,Preference-conditioned Pixel-based AI Agent For Game Testing,0.544551,"The game industry is challenged to cope with increasing growth in demand and
game complexity while maintaining acceptable quality standards for released
games. Classic approaches solely depending on human efforts for quality
assurance and game testing do not scale effectively in terms of time and cost.
Game-testing AI agents that learn by interaction with the environment have the
potential to mitigate these challenges with good scalability properties on time
and costs. However, most recent work in this direction depends on game state
information for the agent's state representation, which limits generalization
across different game scenarios. Moreover, game test engineers usually prefer
exploring a game in a specific style, such as exploring the golden path.
However, current game testing AI agents do not provide an explicit way to
satisfy such a preference. This paper addresses these limitations by proposing
an agent design that mainly depends on pixel-based state observations while
exploring the environment conditioned on a user's preference specified by
demonstration trajectories. In addition, we propose an imitation learning
method that couples self-supervised and supervised learning objectives to
enhance the quality of imitation behaviors. Our agent significantly outperforms
state-of-the-art pixel-based game testing agents over exploration coverage and
test execution quality when evaluated on a complex open-world environment
resembling many aspects of real AAA games.",None,-1
15604f89-ad05-4739-b303-81050c6c32de,WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models,0.978655,"We present WinoQueer: a benchmark specifically designed to measure whether
large language models (LLMs) encode biases that are harmful to the LGBTQ+
community. The benchmark is community-sourced, via application of a novel
method that generates a bias benchmark from a community survey. We apply our
benchmark to several popular LLMs and find that off-the-shelf models generally
do exhibit considerable anti-queer bias. Finally, we show that LLM bias against
a marginalized community can be somewhat mitigated by finetuning on data
written about or by members of that community, and that social media text
written by community members is more effective than news text written about the
community by non-members. Our method for community-in-the-loop benchmark
development provides a blueprint for future researchers to develop
community-driven, harms-grounded LLM benchmarks for other marginalized
communities.",None,-1
3135a5d6-eb84-4c95-acfb-b620e237e448,A Multi-Modal Transformer Network for Action Detection,0.740992,"This paper proposes a novel multi-modal transformer network for detecting
actions in untrimmed videos. To enrich the action features, our transformer
network utilizes a new multi-modal attention mechanism that computes the
correlations between different spatial and motion modalities combinations.
Exploring such correlations for actions has not been attempted previously. To
use the motion and spatial modality more effectively, we suggest an algorithm
that corrects the motion distortion caused by camera movement. Such motion
distortion, common in untrimmed videos, severely reduces the expressive power
of motion features such as optical flow fields. Our proposed algorithm
outperforms the state-of-the-art methods on two public benchmarks, THUMOS14 and
ActivityNet. We also conducted comparative experiments on our new instructional
activity dataset, including a large set of challenging classroom videos
captured from elementary schools.",None,-1
f51b72b0-6c7a-4aef-a4d5-5ef58c7a69c5,Designerly Understanding: Information Needs for Model Transparency to Support Design Ideation for AI-Powered User Experience,0.999809,"Despite the widespread use of artificial intelligence (AI), designing user
experiences (UX) for AI-powered systems remains challenging. UX designers face
hurdles understanding AI technologies, such as pre-trained language models, as
design materials. This limits their ability to ideate and make decisions about
whether, where, and how to use AI. To address this problem, we bridge the
literature on AI design and AI transparency to explore whether and how
frameworks for transparent model reporting can support design ideation with
pre-trained models. By interviewing 23 UX practitioners, we find that
practitioners frequently work with pre-trained models, but lack support for
UX-led ideation. Through a scenario-based design task, we identify common goals
that designers seek model understanding for and pinpoint their model
transparency information needs. Our study highlights the pivotal role that UX
designers can play in Responsible AI and calls for supporting their
understanding of AI limitations through model transparency and interrogation.",None,-1
5d9d822c-1533-413a-86cf-f3f8bd346528,Improving Stability in Simultaneous Speech Translation: A Revision-Controllable Decoding Approach,0.371666,"Simultaneous Speech-to-Text translation serves a critical role in real-time
crosslingual communication. Despite the advancements in recent years,
challenges remain in achieving stability in the translation process, a concern
primarily manifested in the flickering of partial results. In this paper, we
propose a novel revision-controllable method designed to address this issue.
Our method introduces an allowed revision window within the beam search pruning
process to screen out candidate translations likely to cause extensive
revisions, leading to a substantial reduction in flickering and, crucially,
providing the capability to completely eliminate flickering. The experiments
demonstrate the proposed method can significantly improve the decoding
stability without compromising substantially on the translation quality.",None,-1
85609b0a-9532-401b-a93c-0977072a8935,DoNet: Deep De-overlapping Network for Cytology Instance Segmentation,0.514231,"Cell instance segmentation in cytology images has significant importance for
biology analysis and cancer screening, while remains challenging due to 1) the
extensive overlapping translucent cell clusters that cause the ambiguous
boundaries, and 2) the confusion of mimics and debris as nuclei. In this work,
we proposed a De-overlapping Network (DoNet) in a decompose-and-recombined
strategy. A Dual-path Region Segmentation Module (DRM) explicitly decomposes
the cell clusters into intersection and complement regions, followed by a
Semantic Consistency-guided Recombination Module (CRM) for integration. To
further introduce the containment relationship of the nucleus in the cytoplasm,
we design a Mask-guided Region Proposal Strategy (MRP) that integrates the cell
attention maps for inner-cell instance prediction. We validate the proposed
approach on ISBI2014 and CPS datasets. Experiments show that our proposed DoNet
significantly outperforms other state-of-the-art (SOTA) cell instance
segmentation methods. The code is available at
https://github.com/DeepDoNet/DoNet.",None,-1
0f52dc1b-cf8a-42a6-b3b2-e8c86b6d1c49,Diversity Induced Environment Design via Self-Play,0.00550722,"Recent work on designing an appropriate distribution of environments has
shown promise for training effective generally capable agents. Its success is
partly because of a form of adaptive curriculum learning that generates
environment instances (or levels) at the frontier of the agent's capabilities.
However, such an environment design framework often struggles to find effective
levels in challenging design spaces and requires costly interactions with the
environment. In this paper, we aim to introduce diversity in the Unsupervised
Environment Design (UED) framework. Specifically, we propose a task-agnostic
method to identify observed/hidden states that are representative of a given
level. The outcome of this method is then utilized to characterize the
diversity between two levels, which as we show can be crucial to effective
performance. In addition, to improve sampling efficiency, we incorporate the
self-play technique that allows the environment generator to automatically
generate environments that are of great benefit to the training agent.
Quantitatively, our approach, Diversity-induced Environment Design via
Self-Play (DivSP), shows compelling performance over existing methods.",None,-1
8acea8ca-a854-407c-a92f-f27761d0aa7d,QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing,0.607163,"Questions Under Discussion (QUD) is a versatile linguistic framework in which
discourse progresses as continuously asking questions and answering them.
Automatic parsing of a discourse to produce a QUD structure thus entails a
complex question generation task: given a document and an answer sentence,
generate a question that satisfies linguistic constraints of QUD and can be
grounded in an anchor sentence in prior context. These questions are known to
be curiosity-driven and open-ended. This work introduces the first framework
for the automatic evaluation of QUD parsing, instantiating the theoretical
constraints of QUD in a concrete protocol. We present QUDeval, a dataset of
fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned
systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD
is still challenging for modern LLMs, and that existing evaluation metrics
poorly approximate parser quality. Encouragingly, human-authored QUDs are
scored highly by our human evaluators, suggesting that there is headroom for
further progress on language modeling to improve both QUD parsing and QUD
evaluation.",None,-1
14399b38-a7aa-4600-b5ff-06a6f8c20e70,At Your Fingertips: Extracting Piano Fingering Instructions from Videos,0.632121,"Piano fingering -- knowing which finger to use to play each note in a musical
piece, is a hard and important skill to master when learning to play the piano.
While some sheet music is available with expert-annotated fingering
information, most pieces lack this information, and people often resort to
learning the fingering from demonstrations in online videos. We consider the AI
task of automating the extraction of fingering information from videos. This is
a non-trivial task as fingers are often occluded by other fingers, and it is
often not clear from the video which of the keys were pressed, requiring the
synchronization of hand position information and knowledge about the notes that
were played. We show how to perform this task with high-accuracy using a
combination of deep-learning modules, including a GAN-based approach for
fine-tuning on out-of-domain data. We extract the fingering information with an
f1 score of 97\%. We run the resulting system on 90 videos, resulting in
high-quality piano fingering information of 150K notes, the largest available
dataset of piano-fingering to date.",None,-1
bc993636-9c57-40db-84bd-e7235235fbec,CCGen: Explainable Complementary Concept Generation in E-Commerce,0.565221,"We propose and study Complementary Concept Generation (CCGen): given a
concept of interest, e.g., ""Digital Cameras"", generating a list of
complementary concepts, e.g., 1) Camera Lenses 2) Batteries 3) Camera Cases 4)
Memory Cards 5) Battery Chargers. CCGen is beneficial for various applications
like query suggestion and item recommendation, especially in the e-commerce
domain. To solve CCGen, we propose to train language models to generate ranked
lists of concepts with a two-step training strategy. We also teach the models
to generate explanations by incorporating explanations distilled from large
teacher models. Extensive experiments and analysis demonstrate that our model
can generate high-quality concepts complementary to the input concept while
producing explanations to justify the predictions.",None,-1
3ae59a0c-ceb4-47f8-a051-b98db95de0c3,VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking,0.999998,"3D object detectors usually rely on hand-crafted proxies, e.g., anchors or
centers, and translate well-studied 2D frameworks to 3D. Thus, sparse voxel
features need to be densified and processed by dense prediction heads, which
inevitably costs extra computation. In this paper, we instead propose VoxelNext
for fully sparse 3D object detection. Our core insight is to predict objects
directly based on sparse voxel features, without relying on hand-crafted
proxies. Our strong sparse convolutional network VoxelNeXt detects and tracks
3D objects through voxel features entirely. It is an elegant and efficient
framework, with no need for sparse-to-dense conversion or NMS post-processing.
Our method achieves a better speed-accuracy trade-off than other mainframe
detectors on the nuScenes dataset. For the first time, we show that a fully
sparse voxel-based representation works decently for LIDAR 3D object detection
and tracking. Extensive experiments on nuScenes, Waymo, and Argoverse2
benchmarks validate the effectiveness of our approach. Without bells and
whistles, our model outperforms all existing LIDAR methods on the nuScenes
tracking test benchmark.",None,-1
2f9a7e1c-d2c1-4d93-a2b9-9554a3b8e4e1,All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation,0.0764723,"Fairness in Language Models (LMs) remains a longstanding challenge, given the
inherent biases in training data that can be perpetuated by models and affect
the downstream tasks. Recent methods employ expensive retraining or attempt
debiasing during inference by constraining model outputs to contrast from a
reference set of biased templates or exemplars. Regardless, they dont address
the primary goal of fairness to maintain equitability across different
demographic groups. In this work, we posit that inferencing LMs to generate
unbiased output for one demographic under a context ensues from being aware of
outputs for other demographics under the same context. To this end, we propose
Counterfactually Aware Fair InferencE (CAFIE), a framework that dynamically
compares the model understanding of diverse demographics to generate more
equitable sentences. We conduct an extensive empirical evaluation using base
LMs of varying sizes and across three diverse datasets and found that CAFIE
outperforms strong baselines. CAFIE produces fairer text and strikes the best
balance between fairness and language modeling capability",None,-1
522765fe-8152-4fb1-bb46-ebda30812291,Benchmarking Probabilistic Deep Learning Methods for License Plate Recognition,0.774977,"Learning-based algorithms for automated license plate recognition implicitly
assume that the training and test data are well aligned. However, this may not
be the case under extreme environmental conditions, or in forensic applications
where the system cannot be trained for a specific acquisition device.
Predictions on such out-of-distribution images have an increased chance of
failing. But this failure case is oftentimes hard to recognize for a human
operator or an automated system. Hence, in this work we propose to model the
prediction uncertainty for license plate recognition explicitly. Such an
uncertainty measure allows to detect false predictions, indicating an analyst
when not to trust the result of the automated license plate recognition. In
this paper, we compare three methods for uncertainty quantification on two
architectures. The experiments on synthetic noisy or blurred low-resolution
images show that the predictive uncertainty reliably finds wrong predictions.
We also show that a multi-task combination of classification and
super-resolution improves the recognition performance by 109\% and the
detection of wrong predictions by 29 %.",None,-1
f3f3fd1d-affb-4860-8811-b67d4a94b358,The Pipeline for the Continuous Development of Artificial Intelligence Models -- Current State of Research and Practice,0.617448,"Companies struggle to continuously develop and deploy AI models to complex
production systems due to AI characteristics while assuring quality. To ease
the development process, continuous pipelines for AI have become an active
research area where consolidated and in-depth analysis regarding the
terminology, triggers, tasks, and challenges is required. This paper includes a
Multivocal Literature Review where we consolidated 151 relevant formal and
informal sources. In addition, nine-semi structured interviews with
participants from academia and industry verified and extended the obtained
information. Based on these sources, this paper provides and compares
terminologies for DevOps and CI/CD for AI, MLOps, (end-to-end) lifecycle
management, and CD4ML. Furthermore, the paper provides an aggregated list of
potential triggers for reiterating the pipeline, such as alert systems or
schedules. In addition, this work uses a taxonomy creation strategy to present
a consolidated pipeline comprising tasks regarding the continuous development
of AI. This pipeline consists of four stages: Data Handling, Model Learning,
Software Development and System Operations. Moreover, we map challenges
regarding pipeline implementation, adaption, and usage for the continuous
development of AI to these four stages.",None,-1
f67f1c95-b041-48c0-bafd-d9751e01624e,Track Anything: Segment Anything Meets Videos,1.0,"Recently, the Segment Anything Model (SAM) gains lots of attention rapidly
due to its impressive segmentation performance on images. Regarding its strong
ability on image segmentation and high interactivity with different prompts, we
found that it performs poorly on consistent segmentation in videos. Therefore,
in this report, we propose Track Anything Model (TAM), which achieves
high-performance interactive tracking and segmentation in videos. To be
detailed, given a video sequence, only with very little human participation,
i.e., several clicks, people can track anything they are interested in, and get
satisfactory results in one-pass inference. Without additional training, such
an interactive design performs impressively on video object tracking and
segmentation. All resources are available on
{https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitate
related research.",None,-1
47c670f3-b791-48c0-a166-8b162ff41393,Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks,0.632121,"In this paper, a new deep-learning architecture for solving the non-linear
Falkner-Skan equation is proposed. Using Legendre and Chebyshev neural blocks,
this approach shows how orthogonal polynomials can be used in neural networks
to increase the approximation capability of artificial neural networks. In
addition, utilizing the mathematical properties of these functions, we overcome
the computational complexity of the backpropagation algorithm by using the
operational matrices of the derivative. The efficiency of the proposed method
is carried out by simulating various configurations of the Falkner-Skan
equation.",None,-1
2a9b7c22-0729-49f6-bc4f-e82279c50812,Challenges and Applications of Large Language Models,0.720336,"Large Language Models (LLMs) went from non-existent to ubiquitous in the
machine learning discourse within a few years. Due to the fast pace of the
field, it is difficult to identify the remaining challenges and already
fruitful application areas. In this paper, we aim to establish a systematic set
of open problems and application successes so that ML researchers can
comprehend the field's current state more quickly and become productive.",None,-1
2dee8712-b8af-46e3-96c2-be6daa083532,Fauno: The Italian Large Language Model that will leave you senza parole!,0.58143,"This paper presents Fauno, the first and largest open-source Italian
conversational Large Language Model (LLM). Our goal with Fauno is to
democratize the study of LLMs in Italian, demonstrating that obtaining a
fine-tuned conversational bot with a single GPU is possible. In addition, we
release a collection of datasets for conversational AI in Italian. The datasets
on which we fine-tuned Fauno include various topics such as general question
answering, computer science, and medical questions. We release our code and
datasets on \url{https://github.com/RSTLess-research/Fauno-Italian-LLM}",None,-1
5e1be299-0d97-4aa2-9736-3d1b1aca6d7c,Full Parameter Fine-tuning for Large Language Models with Limited Resources,0.75408,"Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but demand massive GPU resources for training. Lowering the threshold for
LLMs training would encourage greater participation from researchers,
benefiting both academia and society. While existing approaches have focused on
parameter-efficient fine-tuning, which tunes or adds a small number of
parameters, few have addressed the challenge of tuning the full parameters of
LLMs with limited resources. In this work, we propose a new optimizer,
LOw-Memory Optimization (LOMO), which fuses the gradient computation and the
parameter update in one step to reduce memory usage. By integrating LOMO with
existing memory saving techniques, we reduce memory usage to 10.8% compared to
the standard approach (DeepSpeed solution). Consequently, our approach enables
the full parameter fine-tuning of a 65B model on a single machine with 8 RTX
3090, each with 24GB memory.Code and data are available at
https://github.com/OpenLMLab/LOMO.",None,-1
f639aac4-3dc7-49a2-a1dd-984a16ea7443,Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?,0.732865,"Much discussion about large language models and language-and-vision models
has focused on whether these models are intelligent agents. We present an
alternative perspective. We argue that these artificial intelligence models are
cultural technologies that enhance cultural transmission in the modern world,
and are efficient imitation engines. We explore what AI models can tell us
about imitation and innovation by evaluating their capacity to design new tools
and discover novel causal structures, and contrast their responses with those
of human children. Our work serves as a first step in determining which
particular representations and competences, as well as which kinds of knowledge
or skill, can be derived from particular learning techniques and data.
Critically, our findings suggest that machines may need more than large scale
language and images to achieve what a child can do.",None,-1
3ecb1b4c-464d-4598-b879-698f911163e2,"SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification",0.898299,"This paper describes our approach for SemEval-2023 Task 3: Detecting the
category, the framing, and the persuasion techniques in online news in a
multi-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of
fully trained and adapter mBERT models which was ranked joint-first for German,
and had the highest mean rank of multi-language teams. For Subtask 2 (Framing),
we achieved first place in 3 languages, and the best average rank across all
the languages, by using two separate ensembles: a monolingual
RoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task
adaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a
monolingual RoBERTa-Base model for English and a multilingual mBERT model for
the remaining languages, which achieved top 10 for all languages, including 2nd
for English. For each subtask, we compared monolingual and multilingual
approaches, and considered class imbalance techniques.",None,-1
2b224dfc-b267-4a83-aee5-8762db37cb0c,Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling,0.038327,"Dropout is a widely used regularization trick to resolve the overfitting
issue in large feedforward neural networks trained on a small dataset, which
performs poorly on the held-out test subset. Although the effectiveness of this
regularization trick has been extensively studied for convolutional neural
networks, there is a lack of analysis of it for unsupervised models and in
particular, VAE-based neural topic models. In this paper, we have analyzed the
consequences of dropout in the encoder as well as in the decoder of the VAE
architecture in three widely used neural topic models, namely, contextualized
topic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly
available datasets. We characterize the dropout effect on these models in terms
of the quality and predictive performance of the generated topics.",None,-1
3030806c-03af-4e0a-8401-fe13cbd8ef35,BEVHeight: A Robust Framework for Vision-based Roadside 3D Object Detection,0.971284,"While most recent autonomous driving system focuses on developing perception
methods on ego-vehicle sensors, people tend to overlook an alternative approach
to leverage intelligent roadside cameras to extend the perception ability
beyond the visual range. We discover that the state-of-the-art vision-centric
bird's eye view detection methods have inferior performances on roadside
cameras. This is because these methods mainly focus on recovering the depth
regarding the camera center, where the depth difference between the car and the
ground quickly shrinks while the distance increases. In this paper, we propose
a simple yet effective approach, dubbed BEVHeight, to address this issue. In
essence, instead of predicting the pixel-wise depth, we regress the height to
the ground to achieve a distance-agnostic formulation to ease the optimization
process of camera-only perception methods. On popular 3D detection benchmarks
of roadside cameras, our method surpasses all previous vision-centric methods
by a significant margin. The code is available at
{\url{https://github.com/ADLab-AutoDrive/BEVHeight}}.",None,-1
0ea612a6-4cd4-4700-84d0-620d7f5fbec5,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,0.67689,"Recently, deep learning enabled the accurate segmentation of various diseases
in medical imaging. These performances, however, typically demand large amounts
of manual voxel annotations. This tedious process for volumetric data becomes
more complex when not all required information is available in a single imaging
domain as is the case for PET/CT data. We propose a multimodal interactive
segmentation framework that mitigates these issues by combining anatomical and
physiological cues from PET/CT data. Our framework utilizes the geodesic
distance transform to represent the user annotations and we implement a novel
ellipsoid-based user simulation scheme during training. We further propose two
annotation interfaces and conduct a user study to estimate their usability. We
evaluated our model on the in-domain validation dataset and an unseen PET/CT
dataset. We make our code publicly available:
https://github.com/verena-hallitschke/pet-ct-annotate.",None,-1
3e2c6986-adb7-48ca-b48d-bf284fd23f27,RenewNAT: Renewing Potential Translation for Non-Autoregressive Transformer,0.60886,"Non-autoregressive neural machine translation (NAT) models are proposed to
accelerate the inference process while maintaining relatively high performance.
However, existing NAT models are difficult to achieve the desired
efficiency-quality trade-off. For one thing, fully NAT models with efficient
inference perform inferior to their autoregressive counterparts. For another,
iterative NAT models can, though, achieve comparable performance while
diminishing the advantage of speed. In this paper, we propose RenewNAT, a
flexible framework with high efficiency and effectiveness, to incorporate the
merits of fully and iterative NAT models. RenewNAT first generates the
potential translation results and then renews them in a single pass. It can
achieve significant performance improvements at the same expense as traditional
NAT models (without introducing additional model parameters and decoding
latency). Experimental results on various translation benchmarks (e.g.,
\textbf{4} WMT) show that our framework consistently improves the performance
of strong fully NAT methods (e.g., GLAT and DSLP) without additional speed
overhead.",None,-1
2ea77c20-3f4e-4b22-98bc-0249ca1f5ace,Causal-Discovery Performance of ChatGPT in the context of Neuropathic Pain Diagnosis,0.702924,"ChatGPT has demonstrated exceptional proficiency in natural language
conversation, e.g., it can answer a wide range of questions while no previous
large language models can. Thus, we would like to push its limit and explore
its ability to answer causal discovery questions by using a medical benchmark
(Tu et al. 2019) in causal discovery.",None,-1
7a07e8d8-4cde-44e6-9022-40a1b4898445,MobileVidFactory: Automatic Diffusion-Based Social Media Video Generation for Mobile Devices from Text,0.865314,"Videos for mobile devices become the most popular access to share and acquire
information recently. For the convenience of users' creation, in this paper, we
present a system, namely MobileVidFactory, to automatically generate vertical
mobile videos where users only need to give simple texts mainly. Our system
consists of two parts: basic and customized generation. In the basic
generation, we take advantage of the pretrained image diffusion model, and
adapt it to a high-quality open-domain vertical video generator for mobile
devices. As for the audio, by retrieving from our big database, our system
matches a suitable background sound for the video. Additionally to produce
customized content, our system allows users to add specified screen texts to
the video for enriching visual expression, and specify texts for automatic
reading with optional voices as they like.",None,-1
eb211f7d-9749-440f-9ced-fa211e879f15,Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset,0.929824,"While recent pre-trained transformer-based models can perform named entity
recognition (NER) with great accuracy, their limited range remains an issue
when applied to long documents such as whole novels. To alleviate this issue, a
solution is to retrieve relevant context at the document level. Unfortunately,
the lack of supervision for such a task means one has to settle for
unsupervised approaches. Instead, we propose to generate a synthetic context
retrieval training dataset using Alpaca, an instructiontuned large language
model (LLM). Using this dataset, we train a neural context retriever based on a
BERT model that is able to find relevant context for NER. We show that our
method outperforms several retrieval baselines for the NER task on an English
literary dataset composed of the first chapter of 40 books.",None,-1
a5278db7-201f-4421-a11d-28423d125d3f,ACPO: A Policy Optimization Algorithm for Average MDPs with Constraints,0.147032,"Reinforcement Learning (RL) for constrained MDPs (CMDPs) is an increasingly
important problem for various applications. Often, the average criterion is
more suitable than the discounted criterion. Yet, RL for average-CMDPs (ACMDPs)
remains a challenging problem. Algorithms designed for discounted constrained
RL problems often do not perform well for the average CMDP setting. In this
paper, we introduce a new policy optimization with function approximation
algorithm for constrained MDPs with the average criterion. The
Average-Constrained Policy Optimization (ACPO) algorithm is inspired by trust
region-based policy optimization algorithms. We develop basic sensitivity
theory for average CMDPs, and then use the corresponding bounds in the design
of the algorithm. We provide theoretical guarantees on its performance, and
through extensive experimental work in various challenging OpenAI Gym
environments, show its superior empirical performance when compared to other
state-of-the-art algorithms adapted for the ACMDPs.",None,-1
7dcb517c-122b-427c-877e-c15c9af6fd7c,Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection,0.200003,"Foundation models, such as OpenAI's GPT-3 and GPT-4, Meta's LLaMA, and
Google's PaLM2, have revolutionized the field of artificial intelligence. A
notable paradigm shift has been the advent of the Segment Anything Model (SAM),
which has exhibited a remarkable capability to segment real-world objects,
trained on 1 billion masks and 11 million images. Although SAM excels in
general object segmentation, it lacks the intrinsic ability to detect salient
objects, resulting in suboptimal performance in this domain. To address this
challenge, we present the Segment Salient Object Model (SSOM), an innovative
approach that adaptively fine-tunes SAM for salient object detection by
harnessing the low-rank structure inherent in deep learning. Comprehensive
qualitative and quantitative evaluations across five challenging RGB benchmark
datasets demonstrate the superior performance of our approach, surpassing
state-of-the-art methods.",None,-1
ed522fd6-9513-49a3-a864-78abcbeab90c,Enhancing Low-resolution Face Recognition with Feature Similarity Knowledge Distillation,0.233339,"In this study, we introduce a feature knowledge distillation framework to
improve low-resolution (LR) face recognition performance using knowledge
obtained from high-resolution (HR) images. The proposed framework transfers
informative features from an HR-trained network to an LR-trained network by
reducing the distance between them. A cosine similarity measure was employed as
a distance metric to effectively align the HR and LR features. This approach
differs from conventional knowledge distillation frameworks, which use the L_p
distance metrics and offer the advantage of converging well when reducing the
distance between features of different resolutions. Our framework achieved a 3%
improvement over the previous state-of-the-art method on the AgeDB-30 benchmark
without bells and whistles, while maintaining a strong performance on HR
images. The effectiveness of cosine similarity as a distance metric was
validated through statistical analysis, making our approach a promising
solution for real-world applications in which LR images are frequently
encountered. The code and pretrained models are publicly available on
https://github.com/gist-ailab/feature-similarity-KD.",None,-1
7b6b396d-4059-489a-bb84-bafe2fae036a,Rethinking Boundary Detection in Deep Learning Models for Medical Image Segmentation,0.6673,"Medical image segmentation is a fundamental task in the community of medical
image analysis. In this paper, a novel network architecture, referred to as
Convolution, Transformer, and Operator (CTO), is proposed. CTO employs a
combination of Convolutional Neural Networks (CNNs), Vision Transformer (ViT),
and an explicit boundary detection operator to achieve high recognition
accuracy while maintaining an optimal balance between accuracy and efficiency.
The proposed CTO follows the standard encoder-decoder segmentation paradigm,
where the encoder network incorporates a popular CNN backbone for capturing
local semantic information, and a lightweight ViT assistant for integrating
long-range dependencies. To enhance the learning capacity on boundary, a
boundary-guided decoder network is proposed that uses a boundary mask obtained
from a dedicated boundary detection operator as explicit supervision to guide
the decoding learning process. The performance of the proposed method is
evaluated on six challenging medical image segmentation datasets, demonstrating
that CTO achieves state-of-the-art accuracy with a competitive model
complexity.",None,-1
3934444b-d438-4446-bffe-c0a22f78dfeb,Investigating Answerability of LLMs for Long-Form Question Answering,0.459158,"As we embark on a new era of LLMs, it becomes increasingly crucial to
understand their capabilities, limitations, and differences. Toward making
further progress in this direction, we strive to build a deeper understanding
of the gaps between massive LLMs (e.g., ChatGPT) and smaller yet effective
open-source LLMs and their distilled counterparts. To this end, we specifically
focus on long-form question answering (LFQA) because it has several practical
and impactful applications (e.g., troubleshooting, customer service, etc.) yet
is still understudied and challenging for LLMs. We propose a
question-generation method from abstractive summaries and show that generating
follow-up questions from summaries of long documents can create a challenging
setting for LLMs to reason and infer from long contexts. Our experimental
results confirm that: (1) our proposed method of generating questions from
abstractive summaries pose a challenging setup for LLMs and shows performance
gaps between LLMs like ChatGPT and open-source LLMs (Alpaca, Llama) (2)
open-source LLMs exhibit decreased reliance on context for generated questions
from the original document, but their generation capabilities drop
significantly on generated questions from summaries -- especially for longer
contexts (>1024 tokens)",None,-1
6435b849-f1e9-4116-9493-a888670f3c52,Consistency Analysis of ChatGPT,0.478367,"ChatGPT has gained a huge popularity since its introduction. Its positive
aspects have been reported through many media platforms, and some analyses even
showed that ChatGPT achieved a decent grade in professional exams, adding extra
support to the claim that AI can now assist and even replace humans in
industrial fields. Others, however, doubt its reliability and trustworthiness.
This paper investigates the trustworthiness of ChatGPT and GPT-4 regarding
logically consistent behaviour, focusing specifically on semantic consistency
and the properties of negation, symmetric, and transitive consistency. Our
findings suggest that while both models appear to show an enhanced language
understanding and reasoning ability, they still frequently fall short of
generating logically consistent predictions. We also ascertain via experiments
that prompt designing, few-shot learning and employing larger large language
models (LLMs) are unlikely to be the ultimate solution to resolve the
inconsistency issue of LLMs.",None,-1
671ba32f-8275-421e-8193-4a7678b20194,Systematic Visual Reasoning through Object-Centric Relational Abstraction,0.544551,"Human visual reasoning is characterized by an ability to identify abstract
patterns from only a small number of examples, and to systematically generalize
those patterns to novel inputs. This capacity depends in large part on our
ability to represent complex visual inputs in terms of both objects and
relations. Recent work in computer vision has introduced models with the
capacity to extract object-centric representations, leading to the ability to
process multi-object visual inputs, but falling short of the systematic
generalization displayed by human reasoning. Other recent models have employed
inductive biases for relational abstraction to achieve systematic
generalization of learned abstract rules, but have generally assumed the
presence of object-focused inputs. Here, we combine these two approaches,
introducing Object-Centric Relational Abstraction (OCRA), a model that extracts
explicit representations of both objects and abstract relations, and achieves
strong systematic generalization in tasks (including a novel dataset,
CLEVR-ART, with greater visual complexity) involving complex visual displays.",None,-1
7bd908f0-8db6-4b81-b884-ed1ece8ee6b9,Towards Generative Modeling of Urban Flow through Knowledge-enhanced Denoising Diffusion,0.846209,"Although generative AI has been successful in many areas, its ability to
model geospatial data is still underexplored. Urban flow, a typical kind of
geospatial data, is critical for a wide range of urban applications. Existing
studies mostly focus on predictive modeling of urban flow that predicts the
future flow based on historical flow data, which may be unavailable in
data-sparse areas or newly planned regions. Some other studies aim to predict
OD flow among regions but they fail to model dynamic changes of urban flow over
time. In this work, we study a new problem of urban flow generation that
generates dynamic urban flow for regions without historical flow data. To
capture the effect of multiple factors on urban flow, such as region features
and urban environment, we employ diffusion model to generate urban flow for
regions under different conditions. We first construct an urban knowledge graph
(UKG) to model the urban environment and relationships between regions, based
on which we design a knowledge-enhanced spatio-temporal diffusion model
(KSTDiff) to generate urban flow for each region. Specifically, to accurately
generate urban flow for regions with different flow volumes, we design a novel
diffusion process guided by a volume estimator, which is learnable and
customized for each region. Moreover, we propose a knowledge-enhanced denoising
network to capture the spatio-temporal dependencies of urban flow as well as
the impact of urban environment in the denoising process. Extensive experiments
on four real-world datasets validate the superiority of our model over
state-of-the-art baselines in urban flow generation. Further in-depth studies
demonstrate the utility of generated urban flow data and the ability of our
model for long-term flow generation and urban flow prediction. Our code is
released at: https://github.com/tsinghua-fib-lab/KSTDiff-Urban-flow-generation.",None,-1
30fd59f0-2174-4c0b-b6d2-ddc6ae1d00c6,Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities,0.304948,"Recent advances in artificial general intelligence (AGI), particularly large
language models and creative image generation systems have demonstrated
impressive capabilities on diverse tasks spanning the arts and humanities.
However, the swift evolution of AGI has also raised critical questions about
its responsible deployment in these culturally significant domains
traditionally seen as profoundly human. This paper provides a comprehensive
analysis of the applications and implications of AGI for text, graphics, audio,
and video pertaining to arts and the humanities. We survey cutting-edge systems
and their usage in areas ranging from poetry to history, marketing to film, and
communication to classical art. We outline substantial concerns pertaining to
factuality, toxicity, biases, and public safety in AGI systems, and propose
mitigation strategies. The paper argues for multi-stakeholder collaboration to
ensure AGI promotes creativity, knowledge, and cultural values without
undermining truth or human dignity. Our timely contribution summarizes a
rapidly developing field, highlighting promising directions while advocating
for responsible progress centering on human flourishing. The analysis lays the
groundwork for further research on aligning AGI's technological capacities with
enduring social goods.",None,-1
411a5c73-1888-4cd5-86a2-624b09e09196,Polynomial Implicit Neural Representations For Large Diverse Datasets,0.450356,"Implicit neural representations (INR) have gained significant popularity for
signal and image representation for many end-tasks, such as superresolution, 3D
modeling, and more. Most INR architectures rely on sinusoidal positional
encoding, which accounts for high-frequency information in data. However, the
finite encoding size restricts the model's representational power. Higher
representational power is needed to go from representing a single given image
to representing large and diverse datasets. Our approach addresses this gap by
representing an image with a polynomial function and eliminates the need for
positional encodings. Therefore, to achieve a progressively higher degree of
polynomial representation, we use element-wise multiplications between features
and affine-transformed coordinate locations after every ReLU layer. The
proposed method is evaluated qualitatively and quantitatively on large datasets
like ImageNet. The proposed Poly-INR model performs comparably to
state-of-the-art generative models without any convolution, normalization, or
self-attention layers, and with far fewer trainable parameters. With much fewer
training parameters and higher representative power, our approach paves the way
for broader adoption of INR models for generative modeling tasks in complex
domains. The code is available at \url{https://github.com/Rajhans0/Poly_INR}",None,-1
207e16e2-b9aa-438b-9d6e-32b9c39c7a63,MagicProp: Diffusion-based Video Editing via Motion-aware Appearance Propagation,0.430773,"This paper addresses the issue of modifying the visual appearance of videos
while preserving their motion. A novel framework, named MagicProp, is proposed,
which disentangles the video editing process into two stages: appearance
editing and motion-aware appearance propagation. In the first stage, MagicProp
selects a single frame from the input video and applies image-editing
techniques to modify the content and/or style of the frame. The flexibility of
these techniques enables the editing of arbitrary regions within the frame. In
the second stage, MagicProp employs the edited frame as an appearance reference
and generates the remaining frames using an autoregressive rendering approach.
To achieve this, a diffusion-based conditional generation model, called
PropDPM, is developed, which synthesizes the target frame by conditioning on
the reference appearance, the target motion, and its previous appearance. The
autoregressive editing approach ensures temporal consistency in the resulting
videos. Overall, MagicProp combines the flexibility of image-editing techniques
with the superior temporal consistency of autoregressive modeling, enabling
flexible editing of object types and aesthetic styles in arbitrary regions of
input videos while maintaining good temporal consistency across frames.
Extensive experiments in various video editing scenarios demonstrate the
effectiveness of MagicProp.",None,-1
ae9f07af-3e08-4693-9e9e-6e98396bbe5e,Fair Decision-making Under Uncertainty,0.756516,"There has been concern within the artificial intelligence (AI) community and
the broader society regarding the potential lack of fairness of AI-based
decision-making systems. Surprisingly, there is little work quantifying and
guaranteeing fairness in the presence of uncertainty which is prevalent in many
socially sensitive applications, ranging from marketing analytics to actuarial
analysis and recidivism prediction instruments. To this end, we study a
longitudinal censored learning problem subject to fairness constraints, where
we require that algorithmic decisions made do not affect certain individuals or
social groups negatively in the presence of uncertainty on class label due to
censorship. We argue that this formulation has a broader applicability to
practical scenarios concerning fairness. We show how the newly devised fairness
notions involving censored information and the general framework for fair
predictions in the presence of censorship allow us to measure and mitigate
discrimination under uncertainty that bridges the gap with real-world
applications. Empirical evaluations on real-world discriminated datasets with
censorship demonstrate the practicality of our approach.",None,-1
0ad3c8da-ce77-4151-937c-6a80b1623a81,NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes,0.857539,"With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis
has recently made a big leap forward. At the core, NeRF proposes that each 3D
point can emit radiance, allowing to conduct view synthesis using
differentiable volumetric rendering. While neural radiance fields can
accurately represent 3D scenes for computing the image rendering, 3D meshes are
still the main scene representation supported by most computer graphics and
simulation pipelines, enabling tasks such as real time rendering and
physics-based simulations. Obtaining 3D meshes from neural radiance fields
still remains an open challenge since NeRFs are optimized for view synthesis,
not enforcing an accurate underlying geometry on the radiance field. We thus
propose a novel compact and flexible architecture that enables easy 3D surface
reconstruction from any NeRF-driven approach. Upon having trained the radiance
field, we distill the volumetric 3D representation into a Signed Surface
Approximation Network, allowing easy extraction of the 3D mesh and appearance.
Our final 3D mesh is physically accurate and can be rendered in real time on an
array of devices.",None,-1
7053d634-6a60-428b-9550-8424e189f521,VicTR: Video-conditioned Text Representations for Activity Recognition,0.569299,"Vision-Language models (VLMs) have excelled in the image-domain -- especially
in zero-shot settings -- thanks to the availability of vast pretraining data
(i.e., paired image-text samples). However for videos, such paired data is not
as abundant. Therefore, video-VLMs are usually designed by adapting pretrained
image-VLMs to the video-domain, instead of training from scratch. All such
recipes rely on augmenting visual embeddings with temporal information (i.e.,
image $\rightarrow$ video), often keeping text embeddings unchanged or even
being discarded. In this paper, we argue the contrary, that better video-VLMs
can be designed by focusing more on augmenting text, rather than visual
information. More specifically, we introduce Video-conditioned Text
Representations (VicTR): a form of text embeddings optimized w.r.t. visual
embeddings, creating a more-flexible contrastive latent space. Our model can
further make use of freely-available semantic information, in the form of
visually-grounded auxiliary text (e.g. object or scene information). We
evaluate our model on few-shot, zero-shot (HMDB-51, UCF-101), short-form
(Kinetics-400) and long-form (Charades) activity recognition benchmarks,
showing strong performance among video-VLMs.",None,-1
e07ff86d-f700-449a-8e71-7a10c36911db,Retrieval-augmented Image Captioning,0.607163,"Inspired by retrieval-augmented language generation and pretrained Vision and
Language (V&L) encoders, we present a new approach to image captioning that
generates sentences given the input image and a set of captions retrieved from
a datastore, as opposed to the image alone. The encoder in our model jointly
processes the image and retrieved captions using a pretrained V&L BERT, while
the decoder attends to the multimodal encoder representations, benefiting from
the extra textual evidence from the retrieved captions. Experimental results on
the COCO dataset show that image captioning can be effectively formulated from
this new perspective. Our model, named EXTRA, benefits from using captions
retrieved from the training dataset, and it can also benefit from using an
external dataset without the need for retraining. Ablation studies show that
retrieving a sufficient number of captions (e.g., k=5) can improve captioning
quality. Our work contributes towards using pretrained V&L encoders for
generative tasks, instead of standard classification tasks.",None,-1
0ba399d9-2d05-4088-8d33-872490d07606,Using Foundation Models to Detect Policy Violations with Minimal Supervision,0.0261197,"Foundation models, i.e. large neural networks pre-trained on large text
corpora, have revolutionized NLP. They can be instructed directly (e.g.
(arXiv:2005.14165)) - this is called hard prompting - and they can be tuned
using very little data (e.g. (arXiv:2104.08691)) - this technique is called
soft prompting. We seek to leverage their capabilities to detect policy
violations. Our contributions are: We identify a hard prompt that adapts
chain-of-thought prompting to policy violation tasks. This prompt produces
policy violation classifications, along with extractive explanations that
justify the classification. We compose the hard-prompts with soft prompt tuning
to produce a classifier that attains high accuracy with very little
supervision; the same classifier also produces explanations. Though the
supervision only acts on the classifications, we find that the modified
explanations remain consistent with the (tuned) model's response. Along the
way, we identify several unintuitive aspects of foundation models. For
instance, adding an example from a specific class can actually reduce
predictions of that class, and separately, the effects of tokenization on
scoring etc. Based on our technical results, we identify a simple workflow for
product teams to quickly develop effective policy violation detectors.",None,-1
4f8a90f6-5e2e-4b60-aa0b-d31d8e7e7751,Nonverbal Cues in Human-Robot Interaction: A Communication Studies Perspective,0.905412,"Communication between people is characterized by a broad range of nonverbal
cues. Transferring these cues into the design of robots and other artificial
agents that interact with people may foster more natural, inviting, and
accessible experiences. In this position paper, we offer a series of definitive
nonverbal codes for human-robot interaction (HRI) that address the five human
sensory systems (visual, auditory, haptic, olfactory, gustatory) drawn from the
field of communication studies. We discuss how these codes can be translated
into design patterns for HRI using a curated sample of the communication
studies and HRI literatures. As nonverbal codes are an essential mode in human
communication, we argue that integrating robotic nonverbal codes in HRI will
afford robots a feeling of ""aliveness"" or ""social agency"" that would otherwise
be missing. We end with suggestions for research directions to stimulate work
on nonverbal communication within the field of HRI and improve communication
between human and robots.",None,-1
0c32c653-bb50-4c49-af44-b9db08cf5128,Feature-Rich Audio Model Inversion for Data-Free Knowledge Distillation Towards General Sound Classification,0.376754,"Data-Free Knowledge Distillation (DFKD) has recently attracted growing
attention in the academic community, especially with major breakthroughs in
computer vision. Despite promising results, the technique has not been well
applied to audio and signal processing. Due to the variable duration of audio
signals, it has its own unique way of modeling. In this work, we propose
feature-rich audio model inversion (FRAMI), a data-free knowledge distillation
framework for general sound classification tasks. It first generates
high-quality and feature-rich Mel-spectrograms through a feature-invariant
contrastive loss. Then, the hidden states before and after the statistics
pooling layer are reused when knowledge distillation is performed on these
feature-rich samples. Experimental results on the Urbansound8k, ESC-50, and
audioMNIST datasets demonstrate that FRAMI can generate feature-rich samples.
Meanwhile, the accuracy of the student model is further improved by reusing the
hidden state and significantly outperforms the baseline method.",None,-1
14c22937-7ee9-4855-91fd-1f8e0241c1c5,WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences,0.987583,"We present WebGLM, a web-enhanced question-answering system based on the
General Language Model (GLM). Its goal is to augment a pre-trained large
language model (LLM) with web search and retrieval capabilities while being
efficient for real-world deployments. To achieve this, we develop WebGLM with
strategies for the LLM-augmented retriever, bootstrapped generator, and human
preference-aware scorer. Specifically, we identify and address the limitations
of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency,
and cost-effectiveness advantages. In addition, we propose systematic criteria
for evaluating web-enhanced QA systems. We conduct multi-dimensional human
evaluation and quantitative ablation studies, which suggest the outperformance
of the proposed WebGLM designs over existing systems. WebGLM with the
10-billion-parameter GLM (10B) is shown to perform better than the
similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human
evaluation. The code, demo, and data are at
\url{https://github.com/THUDM/WebGLM}.",None,-1
2ea12f41-25e7-4be6-a2c7-9d8754dc99b7,Japanese Lexical Complexity for Non-Native Readers: A New Dataset,0.694203,"Lexical complexity prediction (LCP) is the task of predicting the complexity
of words in a text on a continuous scale. It plays a vital role in simplifying
or annotating complex words to assist readers. To study lexical complexity in
Japanese, we construct the first Japanese LCP dataset. Our dataset provides
separate complexity scores for Chinese/Korean annotators and others to address
the readers' L1-specific needs. In the baseline experiment, we demonstrate the
effectiveness of a BERT-based system for Japanese LCP.",None,-1
c77768d7-68f2-4b88-8be6-1c77f4baa9a0,Erasure of Unaligned Attributes from Neural Representations,0.63754,"We present the Assignment-Maximization Spectral Attribute removaL (AMSAL)
algorithm, which erases information from neural representations when the
information to be erased is implicit rather than directly being aligned to each
input example. Our algorithm works by alternating between two steps. In one, it
finds an assignment of the input representations to the information to be
erased, and in the other, it creates projections of both the input
representations and the information to be erased into a joint latent space. We
test our algorithm on an extensive array of datasets, including a Twitter
dataset with multiple guarded attributes, the BiasBios dataset and the
BiasBench benchmark. The last benchmark includes four datasets with various
types of protected attributes. Our results demonstrate that bias can often be
removed in our setup. We also discuss the limitations of our approach when
there is a strong entanglement between the main task and the information to be
erased.",None,-1
d6bdedb1-02bf-4346-a752-fb966b41307e,Visualizing Skiers' Trajectories in Monocular Videos,0.0864937,"Trajectories are fundamental to winning in alpine skiing. Tools enabling the
analysis of such curves can enhance the training activity and enrich
broadcasting content. In this paper, we propose SkiTraVis, an algorithm to
visualize the sequence of points traversed by a skier during its performance.
SkiTraVis works on monocular videos and constitutes a pipeline of a visual
tracker to model the skier's motion and of a frame correspondence module to
estimate the camera's motion. The separation of the two motions enables the
visualization of the trajectory according to the moving camera's perspective.
We performed experiments on videos of real-world professional competitions to
quantify the visualization error, the computational efficiency, as well as the
applicability. Overall, the results achieved demonstrate the potential of our
solution for broadcasting media enhancement and coach assistance.",None,-1
07c276ed-2cd2-4541-8407-683881cdf45c,Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance,0.828616,"ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that
are slated to promise different applications in diverse areas. In education,
these AI technologies have been tested for applications in assessment and
teaching. In assessment, AI has long been used in automated essay scoring and
automated item generation. One psychometric property that these tools must have
to assist or replace humans in assessment is high reliability in terms of
agreement between AI scores and human raters. In this paper, we measure the
reliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and
trained humans in perceiving and rating the complexity of writing prompts.
Intraclass correlation (ICC) as a performance metric showed that the
inter-reliability of both the OpenAI ChatGPT and the Google Bard were low
against the gold standard of human ratings.",None,-1
8bcf5043-d302-488e-83f4-67b8f2130031,Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty,0.888327,"Open Information Extraction (OIE) task aims at extracting structured facts
from unstructured text, typically in the form of (subject, relation, object)
triples. Despite the potential of large language models (LLMs) like ChatGPT as
a general task solver, they lag behind state-of-the-art (supervised) methods in
OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant
context from relevant relations and generate structured output due to the
restrictions on fine-tuning the model. Second, LLMs generates responses
autoregressively based on probability, which makes the predicted relations lack
confidence. In this paper, we assess the capabilities of LLMs in improving the
OIE task. Particularly, we propose various in-context learning strategies to
enhance LLM's instruction-following ability and a demonstration uncertainty
quantification module to enhance the confidence of the generated relations. Our
experiments on three OIE benchmark datasets show that our approach holds its
own against established supervised methods, both quantitatively and
qualitatively.",None,-1
efacce17-b848-4ced-ba8b-06fd5ebca033,The Secret of Metaphor on Expressing Stronger Emotion,0.864665,"Metaphors are proven to have stronger emotional impact than literal
expressions. Although this conclusion is shown to be promising in benefiting
various NLP applications, the reasons behind this phenomenon are not well
studied. This paper conducts the first study in exploring how metaphors convey
stronger emotion than their literal counterparts. We find that metaphors are
generally more specific than literal expressions. The more specific property of
metaphor can be one of the reasons for metaphors' superiority in emotion
expression. When we compare metaphors with literal expressions with the same
specificity level, the gap of emotion expressing ability between both reduces
significantly. In addition, we observe specificity is crucial in literal
language as well, as literal language can express stronger emotion by making it
more specific.",None,-1
13f1cb8f-5f82-440d-8c0a-64b2b492bbb7,Towards Understanding How Self-training Tolerates Data Backdoor Poisoning,0.181499,"Recent studies on backdoor attacks in model training have shown that
polluting a small portion of training data is sufficient to produce incorrect
manipulated predictions on poisoned test-time data while maintaining high clean
accuracy in downstream tasks. The stealthiness of backdoor attacks has imposed
tremendous defense challenges in today's machine learning paradigm. In this
paper, we explore the potential of self-training via additional unlabeled data
for mitigating backdoor attacks. We begin by making a pilot study to show that
vanilla self-training is not effective in backdoor mitigation. Spurred by that,
we propose to defend the backdoor attacks by leveraging strong but proper data
augmentations in the self-training pseudo-labeling stage. We find that the new
self-training regime help in defending against backdoor attacks to a great
extent. Its effectiveness is demonstrated through experiments for different
backdoor triggers on CIFAR-10 and a combination of CIFAR-10 with an additional
unlabeled 500K TinyImages dataset. Finally, we explore the direction of
combining self-supervised representation learning with self-training for
further improvement in backdoor defense.",None,-1
ec619d8b-cc3c-496c-9f57-efb42bcb95cf,Stochastic Multi-Person 3D Motion Forecasting,0.889615,"This paper aims to deal with the ignored real-world complexities in prior
work on human motion forecasting, emphasizing the social properties of
multi-person motion, the diversity of motion and social interactions, and the
complexity of articulated motion. To this end, we introduce a novel task of
stochastic multi-person 3D motion forecasting. We propose a dual-level
generative modeling framework that separately models independent individual
motion at the local level and social interactions at the global level. Notably,
this dual-level modeling mechanism can be achieved within a shared generative
model, through introducing learnable latent codes that represent intents of
future motion and switching the codes' modes of operation at different levels.
Our framework is general; we instantiate it with different generative models,
including generative adversarial networks and diffusion models, and various
multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D,
and SoMoF benchmarks show that our approach produces diverse and accurate
multi-person predictions, significantly outperforming the state of the art.",None,-1
48e61cf3-6920-4317-92f7-ba7b619563f6,Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models,0.765612,"Parameter-efficient tuning (PET) methods fit pre-trained language models
(PLMs) to downstream tasks by either computing a small compressed update for a
subset of model parameters, or appending and fine-tuning a small number of new
model parameters to the pre-trained network. Hand-designed PET architectures
from the literature perform well in practice, but have the potential to be
improved via automated neural architecture search (NAS). We propose an
efficient NAS method for learning PET architectures via structured and
unstructured pruning. We present experiments on GLUE demonstrating the
effectiveness of our algorithm and discuss how PET architectural design choices
affect performance in practice.",None,-1
8dbd40f2-18f3-4ba8-8b8c-2b9e5ec273e1,Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds,0.536311,"Urban modeling from LiDAR point clouds is an important topic in computer
vision, computer graphics, photogrammetry and remote sensing. 3D city models
have found a wide range of applications in smart cities, autonomous navigation,
urban planning and mapping etc. However, existing datasets for 3D modeling
mainly focus on common objects such as furniture or cars. Lack of building
datasets has become a major obstacle for applying deep learning technology to
specific domains such as urban modeling. In this paper, we present a
urban-scale dataset consisting of more than 160 thousands buildings along with
corresponding point clouds, mesh and wire-frame models, covering 16 cities in
Estonia about 998 Km2. We extensively evaluate performance of state-of-the-art
algorithms including handcrafted and deep feature based methods. Experimental
results indicate that Building3D has challenges of high intra-class variance,
data imbalance and large-scale noises. The Building3D is the first and largest
urban-scale building modeling benchmark, allowing a comparison of supervised
and self-supervised learning methods. We believe that our Building3D will
facilitate future research on urban modeling, aerial path planning, mesh
simplification, and semantic/part segmentation etc.",None,-1
6805790c-553e-4c99-afda-afe41972e9d7,Multilingual Bias Detection and Mitigation for Indian Languages,0.550969,"Lack of diverse perspectives causes neutrality bias in Wikipedia content
leading to millions of worldwide readers getting exposed by potentially
inaccurate information. Hence, neutrality bias detection and mitigation is a
critical problem. Although previous studies have proposed effective solutions
for English, no work exists for Indian languages. First, we contribute two
large datasets, mWikiBias and mWNC, covering 8 languages, for the bias
detection and mitigation tasks respectively. Next, we investigate the
effectiveness of popular multilingual Transformer-based models for the two
tasks by modeling detection as a binary classification problem and mitigation
as a style transfer problem. We make the code and data publicly available.",None,-1
faf05366-adfe-4186-a5ab-834573966836,Just Noticeable Visual Redundancy Forecasting: A Deep Multimodal-driven Approach,0.614179,"Just noticeable difference (JND) refers to the maximum visual change that
human eyes cannot perceive, and it has a wide range of applications in
multimedia systems. However, most existing JND approaches only focus on a
single modality, and rarely consider the complementary effects of multimodal
information. In this article, we investigate the JND modeling from an
end-to-end homologous multimodal perspective, namely hmJND-Net. Specifically,
we explore three important visually sensitive modalities, including saliency,
depth, and segmentation. To better utilize homologous multimodal information,
we establish an effective fusion method via summation enhancement and
subtractive offset, and align homologous multimodal features based on a
self-attention driven encoder-decoder paradigm. Extensive experimental results
on eight different benchmark datasets validate the superiority of our hmJND-Net
over eight representative methods.",None,-1
1dc8d085-193e-4c64-b4f8-b44f45d641c5,Dynamic Point Cloud Geometry Compression Using Multiscale Inter Conditional Coding,0.154978,"This work extends the Multiscale Sparse Representation (MSR) framework
developed for static Point Cloud Geometry Compression (PCGC) to support the
dynamic PCGC through the use of multiscale inter conditional coding. To this
end, the reconstruction of the preceding Point Cloud Geometry (PCG) frame is
progressively downscaled to generate multiscale temporal priors which are then
scale-wise transferred and integrated with lower-scale spatial priors from the
same frame to form the contextual information to improve occupancy probability
approximation when processing the current PCG frame from one scale to another.
Following the Common Test Conditions (CTC) defined in the standardization
committee, the proposed method presents State-Of-The-Art (SOTA) compression
performance, yielding 78% lossy BD-Rate gain to the latest standard-compliant
V-PCC and 45% lossless bitrate reduction to the latest G-PCC. Even for
recently-emerged learning-based solutions, our method still shows significant
performance gains.",None,-1
f1a4f323-7972-4bdc-87f9-922bd7bcb3a7,Garment Recovery with Shape and Deformation Priors,0.581501,"While modeling people wearing tight-fitting clothing has made great strides
in recent years, loose-fitting clothing remains a challenge. We propose a
method that delivers realistic garment models from real-world images,
regardless of garment shape or deformation. To this end, we introduce a fitting
approach that utilizes shape and deformation priors learned from synthetic data
to accurately capture garment shapes and deformations, including large ones.
Not only does our approach recover the garment geometry accurately, it also
yields models that can be directly used by downstream applications such as
animation and simulation.",None,-1
0e3a5f25-69c7-4f8d-ae8e-c7ff5c4bb31f,"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",0.621143,"Human feedback is increasingly used to steer the behaviours of Large Language
Models (LLMs). However, it is unclear how to collect and incorporate feedback
in a way that is efficient, effective and unbiased, especially for highly
subjective human preferences and values. In this paper, we survey existing
approaches for learning from human feedback, drawing on 95 papers primarily
from the ACL and arXiv repositories.First, we summarise the past, pre-LLM
trends for integrating human feedback into language models. Second, we give an
overview of present techniques and practices, as well as the motivations for
using feedback; conceptual frameworks for defining values and preferences; and
how feedback is collected and from whom. Finally, we encourage a better future
of feedback learning in LLMs by raising five unresolved conceptual and
practical challenges.",None,-1
034525ab-8638-43fc-a9f9-378f777d801c,Whale Detection Enhancement through Synthetic Satellite Images,0.0371427,"With a number of marine populations in rapid decline, collecting and
analyzing data about marine populations has become increasingly important to
develop effective conservation policies for a wide range of marine animals,
including whales. Modern computer vision algorithms allow us to detect whales
in images in a wide range of domains, further speeding up and enhancing the
monitoring process. However, these algorithms heavily rely on large training
datasets, which are challenging and time-consuming to collect particularly in
marine or aquatic environments. Recent advances in AI however have made it
possible to synthetically create datasets for training machine learning
algorithms, thus enabling new solutions that were not possible before. In this
work, we present a solution - SeaDroneSim2 benchmark suite, which addresses
this challenge by generating aerial, and satellite synthetic image datasets to
improve the detection of whales and reduce the effort required for training
data collection. We show that we can achieve a 15% performance boost on whale
detection compared to using the real data alone for training, by augmenting a
10% real data. We open source both the code of the simulation platform
SeaDroneSim2 and the dataset generated through it.",None,-1
9b11d2fa-9d4a-4484-8875-a596e05e77e4,"Towards a Gateway for Knowledge Graph Schemas Collection, Analysis, and Embedding",0.283827,"One of the significant barriers to the training of statistical models on
knowledge graphs is the difficulty that scientists have in finding the best
input data to address their prediction goal. In addition to this, a key
challenge is to determine how to manipulate these relational data, which are
often in the form of particular triples (i.e., subject, predicate, object), to
enable the learning process. Currently, many high-quality catalogs of knowledge
graphs, are available. However, their primary goal is the re-usability of these
resources, and their interconnection, in the context of the Semantic Web. This
paper describes the LiveSchema initiative, namely, a first version of a gateway
that has the main scope of leveraging the gold mine of data collected by many
existing catalogs collecting relational data like ontologies and knowledge
graphs. At the current state, LiveSchema contains - 1000 datasets from 4 main
sources and offers some key facilities, which allow to: i) evolving LiveSchema,
by aggregating other source catalogs and repositories as input sources; ii)
querying all the collected resources; iii) transforming each given dataset into
formal concept analysis matrices that enable analysis and visualization
services; iv) generating models and tensors from each given dataset.",None,-1
fe6060d3-324c-42df-a179-290368ddd79f,Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning,0.25845,"Large Language models (LLMs) possess the capability to engage In-context
Learning (ICL) by leveraging a few demonstrations pertaining to a new
downstream task as conditions. However, this particular learning paradigm
suffers from high instability stemming from substantial variances induced by
factors such as the input distribution of selected examples, their ordering,
and prompt formats. In this work, we demonstrate that even when all these
factors are held constant, the random selection of examples still results in
high variance. Consequently, we aim to explore the informative ability of data
examples by quantifying the Information Gain (IG) obtained in prediction after
observing a given example candidate. Then we propose to sample those with
maximum IG. Additionally, we identify the presence of template bias, which can
lead to unfair evaluations of IG during the sampling process. To mitigate this
bias, we introduce Calibration Before Sampling strategy. The experimental
results illustrate that our proposed method can yield an average relative
improvement of 14.3% across six classification tasks using three LLMs.",None,-1
48eab504-7d89-4933-9662-e1006a2fb82d,Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality,0.412156,"Improving factual consistency of abstractive summarization has been a widely
studied topic. However, most of the prior works on training factuality-aware
models have ignored the negative effect it has on summary quality. We propose
EFACTSUM (i.e., Effective Factual Summarization), a candidate summary
generation and ranking technique to improve summary factuality without
sacrificing summary quality. We show that using a contrastive learning
framework with our refined candidate summaries leads to significant gains on
both factuality and similarity-based metrics. Specifically, we propose a
ranking strategy in which we effectively combine two metrics, thereby
preventing any conflict during training. Models trained using our approach show
up to 6 points of absolute improvement over the base model with respect to
FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either
similarity-based metrics or absractiveness.",None,-1
4f269702-1795-4605-adb9-23829c8cde9d,Transfer-Ensemble Learning based Deep Convolutional Neural Networks for Diabetic Retinopathy Classification,0.565402,"This article aims to classify diabetic retinopathy (DR) disease into five
different classes using an ensemble approach based on two popular pre-trained
convolutional neural networks: VGG16 and Inception V3. The proposed model aims
to leverage the strengths of the two individual nets to enhance the
classification performance for diabetic retinopathy. The ensemble model
architecture involves freezing a portion of the layers in each pre-trained
model to utilize their learned representations effectively. Global average
pooling layers are added to transform the output feature maps into fixed-length
vectors. These vectors are then concatenated to form a consolidated
representation of the input image. The ensemble model is trained using a
dataset of diabetic retinopathy images (APTOS), divided into training and
validation sets. During the training process, the model learns to classify the
retinal images into the corresponding diabetic retinopathy classes.
Experimental results on the test set demonstrate the efficacy of the proposed
ensemble model for DR classification achieving an accuracy of 96.4%.",None,-1
11b8238d-ba49-4ab4-b777-38934625aa00,Zero-1-to-3: Zero-shot One Image to 3D Object,1.0,"We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an
object given just a single RGB image. To perform novel view synthesis in this
under-constrained setting, we capitalize on the geometric priors that
large-scale diffusion models learn about natural images. Our conditional
diffusion model uses a synthetic dataset to learn controls of the relative
camera viewpoint, which allow new images to be generated of the same object
under a specified camera transformation. Even though it is trained on a
synthetic dataset, our model retains a strong zero-shot generalization ability
to out-of-distribution datasets as well as in-the-wild images, including
impressionist paintings. Our viewpoint-conditioned diffusion approach can
further be used for the task of 3D reconstruction from a single image.
Qualitative and quantitative experiments show that our method significantly
outperforms state-of-the-art single-view 3D reconstruction and novel view
synthesis models by leveraging Internet-scale pre-training.",None,-1
d07f1fb6-24a1-4cbe-8d19-f6c72d991330,TADA: Task-Agnostic Dialect Adapters for English,0.563746,"Large Language Models, the dominant starting point for Natural Language
Processing (NLP) applications, fail at a higher rate for speakers of English
dialects other than Standard American English (SAE). Prior work addresses this
using task-specific data or synthetic data augmentation, both of which require
intervention for each dialect and task pair. This poses a scalability issue
that prevents the broad adoption of robust dialectal English NLP. We introduce
a simple yet effective method for task-agnostic dialect adaptation by aligning
non-SAE dialects using adapters and composing them with task-specific adapters
from SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on
4 dialectal variants of the GLUE benchmark without task-specific supervision.",None,-1
2d87d52d-73e9-4f5b-b3ac-48ce0c9290d2,Dynamic Token-Pass Transformers for Semantic Segmentation,0.150262,"Vision transformers (ViT) usually extract features via forwarding all the
tokens in the self-attention layers from top to toe. In this paper, we
introduce dynamic token-pass vision transformers (DoViT) for semantic
segmentation, which can adaptively reduce the inference cost for images with
different complexity. DoViT gradually stops partial easy tokens from
self-attention calculation and keeps the hard tokens forwarding until meeting
the stopping criteria. We employ lightweight auxiliary heads to make the
token-pass decision and divide the tokens into keeping/stopping parts. With a
token separate calculation, the self-attention layers are speeded up with
sparse tokens and still work friendly with hardware. A token reconstruction
module is built to collect and reset the grouped tokens to their original
position in the sequence, which is necessary to predict correct semantic masks.
We conduct extensive experiments on two common semantic segmentation tasks, and
demonstrate that our method greatly reduces about 40% $\sim$ 60% FLOPs and the
drop of mIoU is within 0.8% for various segmentation transformers. The
throughput and inference speed of ViT-L/B are increased to more than 2$\times$
on Cityscapes.",None,-1
891990fd-9ec1-47f1-8ccf-3387a2bd5e30,Visible-Infrared Person Re-Identification via Patch-Mixed Cross-Modality Learning,0.591006,"Visible-infrared person re-identification (VI-ReID) aims to retrieve images
of the same pedestrian from different modalities, where the challenges lie in
the significant modality discrepancy. To alleviate the modality gap, recent
methods generate intermediate images by GANs, grayscaling, or mixup strategies.
However, these methods could introduce extra data distribution, and the
semantic correspondence between the two modalities is not well learned. In this
paper, we propose a Patch-Mixed Cross-Modality framework (PMCM), where two
images of the same person from two modalities are split into patches and
stitched into a new one for model learning. A part-alignment loss is introduced
to regularize representation learning, and a patch-mixed modality learning loss
is proposed to align between the modalities. In this way, the model learns to
recognize a person through patches of different styles, thereby the modality
semantic correspondence can be inferred. In addition, with the flexible image
generation strategy, the patch-mixed images freely adjust the ratio of
different modality patches, which could further alleviate the modality
imbalance problem. On two VI-ReID datasets, we report new state-of-the-art
performance with the proposed method.",None,-1
d96d3c2f-ae33-4b33-91bf-83e7533d1dd4,LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation,0.800991,"Layout-to-image generation refers to the task of synthesizing photo-realistic
images based on semantic layouts. In this paper, we propose LayoutDiffuse that
adapts a foundational diffusion model pretrained on large-scale image or
text-image datasets for layout-to-image generation. By adopting a novel neural
adaptor based on layout attention and task-aware prompts, our method trains
efficiently, generates images with both high perceptual quality and layout
alignment, and needs less data. Experiments on three datasets show that our
method significantly outperforms other 10 generative models based on GANs,
VQ-VAE, and diffusion models.",None,-1
46b93eda-3992-4de0-820d-6576a89966b8,Nemo: First Glimpse of a New Rule Engine,0.667434,"This system demonstration presents Nemo, a new logic programming engine with
a focus on reliability and performance. Nemo is built for data-centric analytic
computations, modelled in a fully declarative Datalog dialect. Its scalability
for these tasks matches or exceeds that of leading Datalog systems. We
demonstrate uses in reasoning with knowledge graphs and ontologies with 10^5 to
10^8 input facts, all on a laptop. Nemo is written in Rust and available as a
free and open source tool.",None,-1
6c68581e-c214-49a3-87be-be5558923317,Clustering-Aware Negative Sampling for Unsupervised Sentence Representation,0.551766,"Contrastive learning has been widely studied in sentence representation
learning. However, earlier works mainly focus on the construction of positive
examples, while in-batch samples are often simply treated as negative examples.
This approach overlooks the importance of selecting appropriate negative
examples, potentially leading to a scarcity of hard negatives and the inclusion
of false negatives. To address these issues, we propose ClusterNS
(Clustering-aware Negative Sampling), a novel method that incorporates cluster
information into contrastive learning for unsupervised sentence representation
learning. We apply a modified K-means clustering algorithm to supply hard
negatives and recognize in-batch false negatives during training, aiming to
solve the two issues in one unified framework. Experiments on semantic textual
similarity (STS) tasks demonstrate that our proposed ClusterNS compares
favorably with baselines in unsupervised sentence representation learning. Our
code has been made publicly available.",None,-1
f3b3efca-d816-458c-887d-3f4c3c9bd67d,A Data Fusion Framework for Multi-Domain Morality Learning,0.774198,"Language models can be trained to recognize the moral sentiment of text,
creating new opportunities to study the role of morality in human life. As
interest in language and morality has grown, several ground truth datasets with
moral annotations have been released. However, these datasets vary in the
method of data collection, domain, topics, instructions for annotators, etc.
Simply aggregating such heterogeneous datasets during training can yield models
that fail to generalize well. We describe a data fusion framework for training
on multiple heterogeneous datasets that improve performance and
generalizability. The model uses domain adversarial training to align the
datasets in feature space and a weighted loss function to deal with label
shift. We show that the proposed framework achieves state-of-the-art
performance in different datasets compared to prior works in morality
inference.",None,-1
4971d49d-bdd0-4f11-a581-f09db2c7d3de,Instruction Position Matters in Sequence Generation with Large Language Models,0.438551,"Large language models (LLMs) are capable of performing conditional sequence
generation tasks, such as translation or summarization, through instruction
fine-tuning. The fine-tuning data is generally sequentially concatenated from a
specific task instruction, an input sentence, and the corresponding response.
Considering the locality modeled by the self-attention mechanism of LLMs, these
models face the risk of instruction forgetting when generating responses for
long input sentences. To mitigate this issue, we propose enhancing the
instruction-following capability of LLMs by shifting the position of task
instructions after the input sentences. Theoretical analysis suggests that our
straightforward method can alter the model's learning focus, thereby
emphasizing the training of instruction-following capabilities. Concurrently,
experimental results demonstrate that our approach consistently outperforms
traditional settings across various model scales (1B / 7B / 13B) and different
sequence generation tasks (translation and summarization), without any
additional data or annotation costs. Notably, our method significantly improves
the zero-shot performance on conditional sequence generation, e.g., up to 9.7
BLEU points on WMT zero-shot translation tasks.",None,-1
548c7c1a-f9e7-48ed-a6eb-96148c1a19a2,Hyperbolic Audio-visual Zero-shot Learning,0.239753,"Audio-visual zero-shot learning aims to classify samples consisting of a pair
of corresponding audio and video sequences from classes that are not present
during training. An analysis of the audio-visual data reveals a large degree of
hyperbolicity, indicating the potential benefit of using a hyperbolic
transformation to achieve curvature-aware geometric learning, with the aim of
exploring more complex hierarchical data structures for this task. The proposed
approach employs a novel loss function that incorporates cross-modality
alignment between video and audio features in the hyperbolic space.
Additionally, we explore the use of multiple adaptive curvatures for hyperbolic
projections. The experimental results on this very challenging task demonstrate
that our proposed hyperbolic approach for zero-shot learning outperforms the
SOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL
achieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%,
respectively.",None,-1
0ba6d750-4b77-4b9f-965d-ab64fbd9abe6,GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions,0.812818,"There is growing interest in systems that generate captions for scientific
figures. However, assessing these systems output poses a significant challenge.
Human evaluation requires academic expertise and is costly, while automatic
evaluation depends on often low-quality author-written captions. This paper
investigates using large language models (LLMs) as a cost-effective,
reference-free method for evaluating figure captions. We first constructed
SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600
scientific figure captions, both original and machine-made, for 600 arXiv
figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption
based on its potential to aid reader understanding, given relevant context such
as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot
evaluator, outperformed all other models and even surpassed assessments made by
Computer Science and Informatics undergraduates, achieving a Kendall
correlation score of 0.401 with Ph.D. students rankings",None,-1
997e3064-611f-4947-8edc-29b98d2322dd,MNL-Bandit in non-stationary environments,0.326501,"In this paper, we study the MNL-Bandit problem in a non-stationary
environment and present an algorithm with a worst-case expected regret of
$\tilde{O}\left( \min \left\{ \sqrt{NTL}\;,\;
N^{\frac{1}{3}}(\Delta_{\infty}^{K})^{\frac{1}{3}} T^{\frac{2}{3}} +
\sqrt{NT}\right\}\right)$. Here $N$ is the number of arms, $L$ is the number of
changes and $\Delta_{\infty}^{K}$ is a variation measure of the unknown
parameters. Furthermore, we show matching lower bounds on the expected regret
(up to logarithmic factors), implying that our algorithm is optimal. Our
approach builds upon the epoch-based algorithm for stationary MNL-Bandit in
Agrawal et al. 2016. However, non-stationarity poses several challenges and we
introduce new techniques and ideas to address these. In particular, we give a
tight characterization for the bias introduced in the estimators due to non
stationarity and derive new concentration bounds.",None,-1
75e4e5ad-6b92-42e6-a2b7-940e0ee79adb,CPopQA: Ranking Cultural Concept Popularity by LLMs,0.878456,"Prior work has demonstrated large language models' (LLMs) potential to
discern statistical tendencies within their pre-training corpora. Despite that,
many examinations of LLMs' knowledge capacity focus on knowledge explicitly
appearing in the training data or implicitly inferable from similar contexts.
How well an LLM captures the corpus-level statistical trends of concepts for
reasoning, especially long-tail ones, is still underexplored. In this study, we
introduce a novel few-shot question-answering task (CPopQA) that examines LLMs'
statistical ranking abilities for long-tail cultural concepts (e.g., holidays),
with a specific focus on these concepts' popularity in the United States and
the United Kingdom, respectively. We curate a dataset containing 459 holidays
across 58 countries, generating a total of 6,000 QA testing pairs. Experiments
on four strong LLMs show that large models are capable of ranking long-tail
cultural concepts regarding their statistical tendency. Notably, GPT-3.5
displayed superior performance and exhibited its potential to identify
geo-cultural proximity across continents.",None,-1
53afb487-a5ef-4896-a9d9-8726f235c562,Unnatural language processing: How do language models handle machine-generated prompts?,0.215485,"Language model prompt optimization research has shown that semantically and
grammatically well-formed manually crafted prompts are routinely outperformed
by automatically generated token sequences with no apparent meaning or
syntactic structure, including sequences of vectors from a model's embedding
space. We use machine-generated prompts to probe how models respond to input
that is not composed of natural language expressions. We study the behavior of
models of different sizes in multiple semantic tasks in response to both
continuous and discrete machine-generated prompts, and compare it to the
behavior in response to human-generated natural-language prompts. Even when
producing a similar output, machine-generated and human prompts trigger
different response patterns through the network processing pathways, including
different perplexities, different attention and output entropy distributions,
and different unit activation profiles. We provide preliminary insight into the
nature of the units activated by different prompt types, suggesting that only
natural language prompts recruit a genuinely linguistic circuit.",None,-1
8252faae-51c0-4e6e-9cb6-f704fcaf999e,Data-Efficient Protein 3D Geometric Pretraining via Refinement of Diffused Protein Structure Decoy,0.381381,"Learning meaningful protein representation is important for a variety of
biological downstream tasks such as structure-based drug design. Having
witnessed the success of protein sequence pretraining, pretraining for
structural data which is more informative has become a promising research
topic. However, there are three major challenges facing protein structure
pretraining: insufficient sample diversity, physically unrealistic modeling,
and the lack of protein-specific pretext tasks. To try to address these
challenges, we present the 3D Geometric Pretraining. In this paper, we propose
a unified framework for protein pretraining and a 3D geometric-based,
data-efficient, and protein-specific pretext task: RefineDiff (Refine the
Diffused Protein Structure Decoy). After pretraining our geometric-aware model
with this task on limited data(less than 1% of SOTA models), we obtained
informative protein representations that can achieve comparable performance for
various downstream tasks.",None,-1
7b27cb6d-2815-4f75-b814-443849b749b3,Unbiased Scene Graph Generation in Videos,0.580101,"The task of dynamic scene graph generation (SGG) from videos is complicated
and challenging due to the inherent dynamics of a scene, temporal fluctuation
of model predictions, and the long-tailed distribution of the visual
relationships in addition to the already existing challenges in image-based
SGG. Existing methods for dynamic SGG have primarily focused on capturing
spatio-temporal context using complex architectures without addressing the
challenges mentioned above, especially the long-tailed distribution of
relationships. This often leads to the generation of biased scene graphs. To
address these challenges, we introduce a new framework called TEMPURA: TEmporal
consistency and Memory Prototype guided UnceRtainty Attenuation for unbiased
dynamic SGG. TEMPURA employs object-level temporal consistencies via
transformer-based sequence modeling, learns to synthesize unbiased relationship
representations using memory-guided training, and attenuates the predictive
uncertainty of visual relations using a Gaussian Mixture Model (GMM). Extensive
experiments demonstrate that our method achieves significant (up to 10% in some
cases) performance gain over existing methods highlighting its superiority in
generating more unbiased scene graphs.",None,-1
1a2b852c-8e80-44d4-85cc-74b75b0407df,Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization,0.321263,"In domain generalization (DG), the target domain is unknown when the model is
being trained, and the trained model should successfully work on an arbitrary
(and possibly unseen) target domain during inference. This is a difficult
problem, and despite active studies in recent years, it remains a great
challenge. In this paper, we take a simple yet effective approach to tackle
this issue. We propose test-time style shifting, which shifts the style of the
test sample (that has a large style gap with the source domains) to the nearest
source domain that the model is already familiar with, before making the
prediction. This strategy enables the model to handle any target domains with
arbitrary style statistics, without additional model update at test-time.
Additionally, we propose style balancing, which provides a great platform for
maximizing the advantage of test-time style shifting by handling the
DG-specific imbalance issues. The proposed ideas are easy to implement and
successfully work in conjunction with various other DG schemes. Experimental
results on different datasets show the effectiveness of our methods.",None,-1
0636a651-e667-437a-a037-5136354927ef,Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields,0.415134,"Synthesizing photo-realistic images from a point cloud is challenging because
of the sparsity of point cloud representation. Recent Neural Radiance Fields
and extensions are proposed to synthesize realistic images from 2D input. In
this paper, we present Point2Pix as a novel point renderer to link the 3D
sparse point clouds with 2D dense image pixels. Taking advantage of the point
cloud 3D prior and NeRF rendering pipeline, our method can synthesize
high-quality images from colored point clouds, generally for novel indoor
scenes. To improve the efficiency of ray sampling, we propose point-guided
sampling, which focuses on valid samples. Also, we present Point Encoding to
build Multi-scale Radiance Fields that provide discriminative 3D point
features. Finally, we propose Fusion Encoding to efficiently synthesize
high-quality images. Extensive experiments on the ScanNet and ArkitScenes
datasets demonstrate the effectiveness and generalization.",None,-1
dc92d9c7-d3a7-4f83-9fab-88905e36f3b1,MyStyle++: A Controllable Personalized Generative Prior,0.186803,"In this paper, we propose an approach to obtain a personalized generative
prior with explicit control over a set of attributes. We build upon MyStyle, a
recently introduced method, that tunes the weights of a pre-trained StyleGAN
face generator on a few images of an individual. This system allows
synthesizing, editing, and enhancing images of the target individual with high
fidelity to their facial features. However, MyStyle does not demonstrate
precise control over the attributes of the generated images. We propose to
address this problem through a novel optimization system that organizes the
latent space in addition to tuning the generator. Our key contribution is to
formulate a loss that arranges the latent codes, corresponding to the input
images, along a set of specific directions according to their attributes. We
demonstrate that our approach, dubbed MyStyle++, is able to synthesize, edit,
and enhance images of an individual with great control over the attributes,
while preserving the unique facial characteristics of that individual.",None,-1
7ea1fbb7-64a7-4c19-8b0e-26650da8af26,Multi-view Self-supervised Disentanglement for General Image Denoising,0.431914,"With its significant performance improvements, the deep learning paradigm has
become a standard tool for modern image denoisers. While promising performance
has been shown on seen noise distributions, existing approaches often suffer
from generalisation to unseen noise types or general and real noise. It is
understandable as the model is designed to learn paired mapping (e.g. from a
noisy image to its clean version). In this paper, we instead propose to learn
to disentangle the noisy image, under the intuitive assumption that different
corrupted versions of the same clean image share a common latent space. A
self-supervised learning framework is proposed to achieve the goal, without
looking at the latent clean image. By taking two different corrupted versions
of the same image as input, the proposed Multi-view Self-supervised
Disentanglement (MeD) approach learns to disentangle the latent clean features
from the corruptions and recover the clean image consequently. Extensive
experimental analysis on both synthetic and real noise shows the superiority of
the proposed method over prior self-supervised approaches, especially on unseen
novel noise types. On real noise, the proposed method even outperforms its
supervised counterparts by over 3 dB.",None,-1
dbc0de06-f948-4bd6-b44b-b2b95ae02cd2,Can Large Language Models Change User Preference Adversarially?,0.0499808,"Pretrained large language models (LLMs) are becoming increasingly powerful
and ubiquitous in mainstream applications such as being a personal assistant, a
dialogue model, etc. As these models become proficient in deducing user
preferences and offering tailored assistance, there is an increasing concern
about the ability of these models to influence, modify and in the extreme case
manipulate user preference adversarially. The issue of lack of interpretability
in these models in adversarial settings remains largely unsolved. This work
tries to study adversarial behavior in user preferences from the lens of
attention probing, red teaming and white-box analysis. Specifically, it
provides a bird's eye view of existing literature, offers red teaming samples
for dialogue models like ChatGPT and GODEL and probes the attention mechanism
in the latter for non-adversarial and adversarial settings.",None,-1
3c437a6e-b005-49d5-a640-c96bc97f882b,A Solution to Co-occurrence Bias: Attributes Disentanglement via Mutual Information Minimization for Pedestrian Attribute Recognition,0.354811,"Recent studies on pedestrian attribute recognition progress with either
explicit or implicit modeling of the co-occurrence among attributes.
Considering that this known a prior is highly variable and unforeseeable
regarding the specific scenarios, we show that current methods can actually
suffer in generalizing such fitted attributes interdependencies onto scenes or
identities off the dataset distribution, resulting in the underlined bias of
attributes co-occurrence. To render models robust in realistic scenes, we
propose the attributes-disentangled feature learning to ensure the recognition
of an attribute not inferring on the existence of others, and which is
sequentially formulated as a problem of mutual information minimization.
Rooting from it, practical strategies are devised to efficiently decouple
attributes, which substantially improve the baseline and establish
state-of-the-art performance on realistic datasets like PETAzs and RAPzs. Code
is released on
https://github.com/SDret/A-Solution-to-Co-occurence-Bias-in-Pedestrian-Attribute-Recognition.",None,-1
4a4fbdcf-8e33-440c-878a-60a3862c19c4,"Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal",0.298788,"In imitation learning for planning, parameters of heuristic functions are
optimized against a set of solved problem instances. This work revisits the
necessary and sufficient conditions of strictly optimally efficient heuristics
for forward search algorithms, mainly A* and greedy best-first search, which
expand only states on the returned optimal path. It then proposes a family of
loss functions based on ranking tailored for a given variant of the forward
search algorithm. Furthermore, from a learning theory point of view, it
discusses why optimizing cost-to-goal \hstar\ is unnecessarily difficult. The
experimental comparison on a diverse set of problems unequivocally supports the
derived theory.",None,-1
0d31f764-9559-4372-b8b4-02ae09295b64,Anytime Approximate Formal Feature Attribution,0.631019,"Widespread use of artificial intelligence (AI) algorithms and machine
learning (ML) models on the one hand and a number of crucial issues pertaining
to them warrant the need for explainable artificial intelligence (XAI). A key
explainability question is: given this decision was made, what are the input
features which contributed to the decision? Although a range of XAI approaches
exist to tackle this problem, most of them have significant limitations.
Heuristic XAI approaches suffer from the lack of quality guarantees, and often
try to approximate Shapley values, which is not the same as explaining which
features contribute to a decision. A recent alternative is so-called formal
feature attribution (FFA), which defines feature importance as the fraction of
formal abductive explanations (AXp's) containing the given feature. This
measures feature importance from the view of formally reasoning about the
model's behavior. It is challenging to compute FFA using its definition because
that involves counting AXp's, although one can approximate it. Based on these
results, this paper makes several contributions. First, it gives compelling
evidence that computing FFA is intractable, even if the set of contrastive
formal explanations (CXp's) is provided, by proving that the problem is
#P-hard. Second, by using the duality between AXp's and CXp's, it proposes an
efficient heuristic to switch from CXp enumeration to AXp enumeration
on-the-fly resulting in an adaptive explanation enumeration algorithm
effectively approximating FFA in an anytime fashion. Finally, experimental
results obtained on a range of widely used datasets demonstrate the
effectiveness of the proposed FFA approximation approach in terms of the error
of FFA approximation as well as the number of explanations computed and their
diversity given a fixed time limit.",None,-1
10af9ed5-c5e3-4d6b-a6e8-a95b35267337,Unsupervised Skin Lesion Segmentation via Structural Entropy Minimization on Multi-Scale Superpixel Graphs,0.866018,"Skin lesion segmentation is a fundamental task in dermoscopic image analysis.
The complex features of pixels in the lesion region impede the lesion
segmentation accuracy, and existing deep learning-based methods often lack
interpretability to this problem. In this work, we propose a novel unsupervised
Skin Lesion sEgmentation framework based on structural entropy and isolation
forest outlier Detection, namely SLED. Specifically, skin lesions are segmented
by minimizing the structural entropy of a superpixel graph constructed from the
dermoscopic image. Then, we characterize the consistency of healthy skin
features and devise a novel multi-scale segmentation mechanism by outlier
detection, which enhances the segmentation accuracy by leveraging the
superpixel features from multiple scales. We conduct experiments on four skin
lesion benchmarks and compare SLED with nine representative unsupervised
segmentation methods. Experimental results demonstrate the superiority of the
proposed framework. Additionally, some case studies are analyzed to demonstrate
the effectiveness of SLED.",None,-1
1757cf8e-1e1b-40f6-9777-d5ced37a41b0,Collective Privacy Recovery: Data-sharing Coordination via Decentralized Artificial Intelligence,0.66407,"Collective privacy loss becomes a colossal problem, an emergency for personal
freedoms and democracy. But, are we prepared to handle personal data as scarce
resource and collectively share data under the doctrine: as little as possible,
as much as necessary? We hypothesize a significant privacy recovery if a
population of individuals, the data collective, coordinates to share minimum
data for running online services with the required quality. Here we show how to
automate and scale-up complex collective arrangements for privacy recovery
using decentralized artificial intelligence. For this, we compare for first
time attitudinal, intrinsic, rewarded and coordinated data sharing in a
rigorous living-lab experiment of high realism involving >27,000 real data
disclosures. Using causal inference and cluster analysis, we differentiate
criteria predicting privacy and five key data-sharing behaviors. Strikingly,
data-sharing coordination proves to be a win-win for all: remarkable privacy
recovery for people with evident costs reduction for service providers.",None,-1
01cfff77-ddcf-4160-bba4-f715c29433ec,A Satellite Imagery Dataset for Long-Term Sustainable Development in United States Cities,0.224911,"Cities play an important role in achieving sustainable development goals
(SDGs) to promote economic growth and meet social needs. Especially satellite
imagery is a potential data source for studying sustainable urban development.
However, a comprehensive dataset in the United States (U.S.) covering multiple
cities, multiple years, multiple scales, and multiple indicators for SDG
monitoring is lacking. To support the research on SDGs in U.S. cities, we
develop a satellite imagery dataset using deep learning models for five SDGs
containing 25 sustainable development indicators. The proposed dataset covers
the 100 most populated U.S. cities and corresponding Census Block Groups from
2014 to 2023. Specifically, we collect satellite imagery and identify objects
with state-of-the-art object detection and semantic segmentation models to
observe cities' bird's-eye view. We further gather population, nighttime light,
survey, and built environment data to depict SDGs regarding poverty, health,
education, inequality, and living environment. We anticipate the dataset to
help urban policymakers and researchers to advance SDGs-related studies,
especially applying satellite imagery to monitor long-term and multi-scale SDGs
in cities.",None,-1
ebc68e73-5430-4201-865b-1ce8911bec62,PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts,0.275396,"A key component of modern conversational systems is the Dialogue State
Tracker (or DST), which models a user's goals and needs. Toward building more
robust and reliable DSTs, we introduce a prompt-based learning approach to
automatically generate effective adversarial examples to probe DST models. Two
key characteristics of this approach are: (i) it only needs the output of the
DST with no need for model parameters, and (ii) it can learn to generate
natural language utterances that can target any DST. Through experiments over
state-of-the-art DSTs, the proposed framework leads to the greatest reduction
in accuracy and the best attack success rate while maintaining good fluency and
a low perturbation ratio. We also show how much the generated adversarial
examples can bolster a DST through adversarial training. These results indicate
the strength of prompt-based attacks on DSTs and leave open avenues for
continued refinement.",None,-1
4cc01980-201d-4a53-b90a-630b13cfdca7,Reliability Scores from Saliency Map Clusters for Improved Image-based Harvest-Readiness Prediction in Cauliflower,0.748747,"Cauliflower is a hand-harvested crop that must fulfill high-quality standards
in sales making the timing of harvest important. However, accurately
determining harvest-readiness can be challenging due to the cauliflower head
being covered by its canopy. While deep learning enables automated
harvest-readiness estimation, errors can occur due to field-variability and
limited training data. In this paper, we analyze the reliability of a
harvest-readiness classifier with interpretable machine learning. By
identifying clusters of saliency maps, we derive reliability scores for each
classification result using knowledge about the domain and the image
properties. For unseen data, the reliability can be used to (i) inform farmers
to improve their decision-making and (ii) increase the model prediction
accuracy. Using RGB images of single cauliflower plants at different
developmental stages from the GrowliFlower dataset, we investigate various
saliency mapping approaches and find that they result in different quality of
reliability scores. With the most suitable interpretation tool, we adjust the
classification result and achieve a 15.72% improvement of the overall accuracy
to 88.14% and a 15.44% improvement of the average class accuracy to 88.52% for
the GrowliFlower dataset.",None,-1
21931eac-1ef9-4578-8de4-96a05e43fe24,Generative Language Models Exhibit Social Identity Biases,0.228111,"The surge in popularity of large language models has given rise to concerns
about biases that these models could learn from humans. We investigate whether
ingroup solidarity and outgroup hostility, fundamental social identity biases
known from social psychology, are present in 56 large language models. We find
that almost all foundational language models and some instruction fine-tuned
models exhibit clear ingroup-positive and outgroup-negative associations when
prompted to complete sentences (e.g., ""We are...""). Our findings suggest that
modern language models exhibit fundamental social identity biases to a similar
degree as humans, both in the lab and in real-world conversations with LLMs,
and that curating training data and instruction fine-tuning can mitigate such
biases. Our results have practical implications for creating less biased
large-language models and further underscore the need for more research into
user interactions with LLMs to prevent potential bias reinforcement in humans.",None,-1
89f6679a-1bb2-4198-9e37-ee7c85de8ad5,MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension,0.347289,"The large language models have achieved superior performance on various
natural language tasks. One major drawback of such approaches is they are
resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a
resource-efficient solution to fine-tune the pre-trained language models (PLMs)
while keeping their weight frozen. Existing soft prompt methods mainly focus on
designing the input-independent prompts that steer the model to fit the domain
of the new dataset. Those methods often ignore the fine-grained information
about the task and context of the text. In this paper, we propose a multi-level
prompt tuning (MPrompt) method for machine reading comprehension. It utilizes
prompts at task-specific, domain-specific, and context-specific levels to
enhance the comprehension of input semantics at different granularities. We
also propose an independence constraint to steer each domain-specific prompt to
focus on information within its domain to avoid redundancy. Moreover, we
present a prompt generator that incorporates context-related knowledge in the
prompt generation to enhance contextual relevancy. We conducted extensive
experiments on 12 benchmarks of various QA formats and achieved an average
improvement of 1.94\% over the state-of-the-art methods.",None,-1
04709b5b-fbe5-4215-9918-3e02f57632f4,An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development,0.361627,"Causal networks are widely used in many fields, including epidemiology,
social science, medicine, and engineering, to model the complex relationships
between variables. While it can be convenient to algorithmically infer these
models directly from observational data, the resulting networks are often
plagued with erroneous edges. Auditing and correcting these networks may
require domain expertise frequently unavailable to the analyst. We propose the
use of large language models such as ChatGPT as an auditor for causal networks.
Our method presents ChatGPT with a causal network, one edge at a time, to
produce insights about edge directionality, possible confounders, and mediating
variables. We ask ChatGPT to reflect on various aspects of each causal link and
we then produce visualizations that summarize these viewpoints for the human
analyst to direct the edge, gather more data, or test further hypotheses. We
envision a system where large language models, automated causal inference, and
the human analyst and domain expert work hand in hand as a team to derive
holistic and comprehensive causal models for any given case scenario. This
paper presents first results obtained with an emerging prototype.",None,-1
160f3dd1-9931-42ed-8bc5-66f0a9fcb303,CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation,0.474986,"Large language models have demonstrated the capability to perform on machine
translation when the input is prompted with a few examples (in-context
learning). Translation quality depends on various features of the selected
examples, such as their quality and relevance, but previous work has
predominantly focused on individual features in isolation. In this paper, we
propose a general framework for combining different features influencing
example selection. We learn a regression model, CTQ Scorer (Contextual
Translation Quality), that selects examples based on multiple features in order
to maximize the translation quality. On multiple language pairs and language
models, we show that CTQ Scorer helps significantly outperform random selection
as well as strong single-factor baselines reported in the literature. We also
see an improvement of over 2.5 COMET points on average with respect to a strong
BM25 retrieval-based baseline.",None,-1
5312585b-6012-4107-ae81-c02c0469d2a7,Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers,0.522978,"Transformers have become a key architecture in speech processing, but our
understanding of how they build up representations of acoustic and linguistic
structure is limited. In this study, we address this gap by investigating how
measures of 'context-mixing' developed for text models can be adapted and
applied to models of spoken language. We identify a linguistic phenomenon that
is ideal for such a case study: homophony in French (e.g. livre vs livres),
where a speech recognition model has to attend to syntactic cues such as
determiners and pronouns in order to disambiguate spoken words with identical
pronunciations and transcribe them while respecting grammatical agreement. We
perform a series of controlled experiments and probing analyses on
Transformer-based speech models. Our findings reveal that representations in
encoder-only models effectively incorporate these cues to identify the correct
transcription, whereas encoders in encoder-decoder models mainly relegate the
task of capturing contextual dependencies to decoder modules.",None,-1
7b16a323-2b5a-4ff6-bc87-002aab817243,ISP: Multi-Layered Garment Draping with Implicit Sewing Patterns,0.974197,"Many approaches to draping individual garments on human body models are
realistic, fast, and yield outputs that are differentiable with respect to the
body shape on which they are draped. However, they are either unable to handle
multi-layered clothing, which is prevalent in everyday dress, or restricted to
bodies in T-pose. In this paper, we introduce a parametric garment
representation model that addresses these limitations. As in models used by
clothing designers, each garment consists of individual 2D panels. Their 2D
shape is defined by a Signed Distance Function and 3D shape by a 2D to 3D
mapping. The 2D parameterization enables easy detection of potential collisions
and the 3D parameterization handles complex shapes effectively. We show that
this combination is faster and yields higher quality reconstructions than
purely implicit surface representations, and makes the recovery of layered
garments from images possible thanks to its differentiability. Furthermore, it
supports rapid editing of garment shapes and texture by modifying individual 2D
panels.",None,-1
81b00e6a-90a1-4606-8b4f-80e9520905a0,Improved Diffusion-based Image Colorization via Piggybacked Models,0.591464,"Image colorization has been attracting the research interests of the
community for decades. However, existing methods still struggle to provide
satisfactory colorized results given grayscale images due to a lack of
human-like global understanding of colors. Recently, large-scale Text-to-Image
(T2I) models have been exploited to transfer the semantic information from the
text prompts to the image domain, where text provides a global control for
semantic objects in the image. In this work, we introduce a colorization model
piggybacking on the existing powerful T2I diffusion model. Our key idea is to
exploit the color prior knowledge in the pre-trained T2I diffusion model for
realistic and diverse colorization. A diffusion guider is designed to
incorporate the pre-trained weights of the latent diffusion model to output a
latent color prior that conforms to the visual semantics of the grayscale
input. A lightness-aware VQVAE will then generate the colorized result with
pixel-perfect alignment to the given grayscale image. Our model can also
achieve conditional colorization with additional inputs (e.g. user hints and
texts). Extensive experiments show that our method achieves state-of-the-art
performance in terms of perceptual quality.",None,-1
20c8d239-4b11-4a1c-877f-00c0e4a32a84,Benchmarking LLM-based Machine Translation on Cultural Awareness,0.770707,"Translating cultural-specific content is crucial for effective cross-cultural
communication. However, many MT systems still struggle to translate sentences
containing cultural-specific entities accurately and understandably. Recent
advancements in in-context learning utilize lightweight prompts to guide large
language models (LLMs) in machine translation tasks. Nevertheless, the
effectiveness of this approach in enhancing machine translation with cultural
awareness remains uncertain. To address this gap, we introduce a new data
curation pipeline to construct a culturally relevant parallel corpus, enriched
with annotations of cultural-specific items. Furthermore, we devise a novel
evaluation metric to assess the understandability of translations in a
reference-free manner by GPT-4. We evaluate a variety of neural machine
translation (NMT) and LLM-based MT systems using our dataset. Additionally, we
propose several prompting strategies for LLMs to incorporate external and
internal cultural knowledge into the translation process. Our results
demonstrate that eliciting explanations can significantly enhance the
understandability of cultural-specific entities, especially those without
well-known translations.",None,-1
d99d680f-dfbe-491f-94b4-07b87ec43f53,Active Learning Guided Fine-Tuning for enhancing Self-Supervised Based Multi-Label Classification of Remote Sensing Images,0.301672,"In recent years, deep neural networks (DNNs) have been found very successful
for multi-label classification (MLC) of remote sensing (RS) images.
Self-supervised pre-training combined with fine-tuning on a randomly selected
small training set has become a popular approach to minimize annotation efforts
of data-demanding DNNs. However, fine-tuning on a small and biased training set
may limit model performance. To address this issue, we investigate the
effectiveness of the joint use of self-supervised pre-training with active
learning (AL). The considered AL strategy aims at guiding the MLC fine-tuning
of a self-supervised model by selecting informative training samples to
annotate in an iterative manner. Experimental results show the effectiveness of
applying AL-guided fine-tuning (particularly for the case where strong
class-imbalance is present in MLC problems) compared to the application of
fine-tuning using a randomly constructed small training set.",None,-1
aebec104-5966-4cd4-80c0-4d248d60a5d2,ReFit: Recurrent Fitting Network for 3D Human Recovery,0.654738,"We present Recurrent Fitting (ReFit), a neural network architecture for
single-image, parametric 3D human reconstruction. ReFit learns a
feedback-update loop that mirrors the strategy of solving an inverse problem
through optimization. At each iterative step, it reprojects keypoints from the
human model to feature maps to query feedback, and uses a recurrent-based
updater to adjust the model to fit the image better. Because ReFit encodes
strong knowledge of the inverse problem, it is faster to train than previous
regression models. At the same time, ReFit improves state-of-the-art
performance on standard benchmarks. Moreover, ReFit applies to other
optimization settings, such as multi-view fitting and single-view shape
fitting. Project website: https://yufu-wang.github.io/refit_humans/",None,-1
e05af88b-7799-4639-a7f2-cc90870b9844,Optimal Decision Tree Policies for Markov Decision Processes,0.131221,"Interpretability of reinforcement learning policies is essential for many
real-world tasks but learning such interpretable policies is a hard problem.
Particularly rule-based policies such as decision trees and rules lists are
difficult to optimize due to their non-differentiability. While existing
techniques can learn verifiable decision tree policies there is no guarantee
that the learners generate a decision that performs optimally. In this work, we
study the optimization of size-limited decision trees for Markov Decision
Processes (MPDs) and propose OMDTs: Optimal MDP Decision Trees. Given a
user-defined size limit and MDP formulation OMDT directly maximizes the
expected discounted return for the decision tree using Mixed-Integer Linear
Programming. By training optimal decision tree policies for different MDPs we
empirically study the optimality gap for existing imitation learning techniques
and find that they perform sub-optimally. We show that this is due to an
inherent shortcoming of imitation learning, namely that complex policies cannot
be represented using size-limited trees. In such cases, it is better to
directly optimize the tree for expected return. While there is generally a
trade-off between the performance and interpretability of machine learning
models, we find that OMDTs limited to a depth of 3 often perform close to the
optimal limit.",None,-1
ee95dab2-35ef-4a36-8b1d-cb3d0b59943d,Learning Multilingual Sentence Representations with Cross-lingual Consistency Regularization,0.44432,"Multilingual sentence representations are the foundation for similarity-based
bitext mining, which is crucial for scaling multilingual neural machine
translation (NMT) system to more languages. In this paper, we introduce MuSR: a
one-for-all Multilingual Sentence Representation model that supports more than
220 languages. Leveraging billions of English-centric parallel corpora, we
train a multilingual Transformer encoder, coupled with an auxiliary Transformer
decoder, by adopting a multilingual NMT framework with CrossConST, a
cross-lingual consistency regularization technique proposed in Gao et al.
(2023). Experimental results on multilingual similarity search and bitext
mining tasks show the effectiveness of our approach. Specifically, MuSR
achieves superior performance over LASER3 (Heffernan et al., 2022) which
consists of 148 independent multilingual sentence encoders.",None,-1
2f8e45d3-ea21-4f78-af1f-abe371422082,Emotion-Cause Pair Extraction as Question Answering,0.716379,"The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all
potential emotion-cause pairs of a document without any annotation of emotion
or cause clauses. Previous approaches on ECPE have tried to improve
conventional two-step processing schemes by using complex architectures for
modeling emotion-cause interaction. In this paper, we cast the ECPE task to the
question answering (QA) problem and propose simple yet effective BERT-based
solutions to tackle it. Given a document, our Guided-QA model first predicts
the best emotion clause using a fixed question. Then the predicted emotion is
used as a question to predict the most potential cause for the emotion. We
evaluate our model on a standard ECPE corpus. The experimental results show
that despite its simplicity, our Guided-QA achieves promising results and is
easy to reproduce. The code of Guided-QA is also provided.",None,-1
35d51693-927a-4972-9608-641df083ceba,Audio Visual Language Maps for Robot Navigation,0.78493,"While interacting in the world is a multi-sensory experience, many robots
continue to predominantly rely on visual perception to map and navigate in
their environments. In this work, we propose Audio-Visual-Language Maps
(AVLMaps), a unified 3D spatial map representation for storing cross-modal
information from audio, visual, and language cues. AVLMaps integrate the
open-vocabulary capabilities of multimodal foundation models pre-trained on
Internet-scale data by fusing their features into a centralized 3D voxel grid.
In the context of navigation, we show that AVLMaps enable robot systems to
index goals in the map based on multimodal queries, e.g., textual descriptions,
images, or audio snippets of landmarks. In particular, the addition of audio
information enables robots to more reliably disambiguate goal locations.
Extensive experiments in simulation show that AVLMaps enable zero-shot
multimodal goal navigation from multimodal prompts and provide 50% better
recall in ambiguous scenarios. These capabilities extend to mobile robots in
the real world - navigating to landmarks referring to visual, audio, and
spatial concepts. Videos and code are available at: https://avlmaps.github.io.",None,-1
bcb2589b-b7df-4b36-b1d4-ef81b7299c22,Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving,0.790069,"In recent years there have been remarkable advancements in autonomous
driving. While autonomous vehicles demonstrate high performance in closed-set
conditions, they encounter difficulties when confronted with unexpected
situations. At the same time, world models emerged in the field of model-based
reinforcement learning as a way to enable agents to predict the future
depending on potential actions. This led to outstanding results in sparse
reward and complex control tasks. This work provides an overview of how world
models can be leveraged to perform anomaly detection in the domain of
autonomous driving. We provide a characterization of world models and relate
individual components to previous works in anomaly detection to facilitate
further research in the field.",None,-1
5645309b-678c-4c36-ae6f-e2e0ebd8ab63,Data Augmentation Alone Can Improve Adversarial Training,0.916317,"Adversarial training suffers from the issue of robust overfitting, which
seriously impairs its generalization performance. Data augmentation, which is
effective at preventing overfitting in standard training, has been observed by
many previous works to be ineffective in mitigating overfitting in adversarial
training. This work proves that, contrary to previous findings, data
augmentation alone can significantly boost accuracy and robustness in
adversarial training. We find that the hardness and the diversity of data
augmentation are important factors in combating robust overfitting. In general,
diversity can improve both accuracy and robustness, while hardness can boost
robustness at the cost of accuracy within a certain limit and degrade them both
over that limit. To mitigate robust overfitting, we first propose a new crop
transformation, Cropshift, which has improved diversity compared to the
conventional one (Padcrop). We then propose a new data augmentation scheme,
based on Cropshift, with much improved diversity and well-balanced hardness.
Empirically, our augmentation method achieves the state-of-the-art accuracy and
robustness for data augmentations in adversarial training. Furthermore, when
combined with weight averaging it matches, or even exceeds, the performance of
the best contemporary regularization methods for alleviating robust
overfitting. Code is available at:
https://github.com/TreeLLi/DA-Alone-Improves-AT.",None,-1
1ba520ef-8482-4c2c-886d-c3603f998cb6,Humanoid Agents: Platform for Simulating Human-like Generative Agents,0.697962,"Just as computational simulations of atoms, molecules and cells have shaped
the way we study the sciences, true-to-life simulations of human-like agents
can be valuable tools for studying human behavior. We propose Humanoid Agents,
a system that guides Generative Agents to behave more like humans by
introducing three elements of System 1 processing: Basic needs (e.g. hunger,
health and energy), Emotion and Closeness in Relationships. Humanoid Agents are
able to use these dynamic elements to adapt their daily activities and
conversations with other agents, as supported with empirical experiments. Our
system is designed to be extensible to various settings, three of which we
demonstrate, as well as to other elements influencing human behavior (e.g.
empathy, moral values and cultural background). Our platform also includes a
Unity WebGL game interface for visualization and an interactive analytics
dashboard to show agent statuses over time. Our platform is available on
https://www.humanoidagents.com/ and code is on
https://github.com/HumanoidAgents/HumanoidAgents",None,-1
cb28f6b7-510d-409a-9604-e03659c90c4d,Topics in the Haystack: Extracting and Evaluating Topics beyond Coherence,0.0751851,"Extracting and identifying latent topics in large text corpora has gained
increasing importance in Natural Language Processing (NLP). Most models,
whether probabilistic models similar to Latent Dirichlet Allocation (LDA) or
neural topic models, follow the same underlying approach of topic
interpretability and topic extraction. We propose a method that incorporates a
deeper understanding of both sentence and document themes, and goes beyond
simply analyzing word frequencies in the data. This allows our model to detect
latent topics that may include uncommon words or neologisms, as well as words
not present in the documents themselves. Additionally, we propose several new
evaluation metrics based on intruder words and similarity measures in the
semantic space. We present correlation coefficients with human identification
of intruder words and achieve near-human level results at the word-intrusion
task. We demonstrate the competitive performance of our method with a large
benchmark study, and achieve superior results compared to state-of-the-art
topic modeling and document clustering models.",None,-1
ff49e5f6-cdf7-410d-8757-c25a303064cf,AGI: Artificial General Intelligence for Education,0.562375,"Artificial general intelligence (AGI) has gained global recognition as a
future technology due to the emergence of breakthrough large language models
and chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventional
AI models, typically designed for a limited range of tasks, demand significant
amounts of domain-specific data for training and may not always consider
intricate interpersonal dynamics in education. AGI, driven by the recent large
pre-trained models, represents a significant leap in the capability of machines
to perform tasks that require human-level intelligence, such as reasoning,
problem-solving, decision-making, and even understanding human emotions and
social interactions. This position paper reviews AGI's key concepts,
capabilities, scope, and potential within future education, including achieving
future educational goals, designing pedagogy and curriculum, and performing
assessments. It highlights that AGI can significantly improve intelligent
tutoring systems, educational assessment, and evaluation procedures. AGI
systems can adapt to individual student needs, offering tailored learning
experiences. They can also provide comprehensive feedback on student
performance and dynamically adjust teaching methods based on student progress.
The paper emphasizes that AGI's capabilities extend to understanding human
emotions and social interactions, which are critical in educational settings.
The paper discusses that ethical issues in education with AGI include data
bias, fairness, and privacy and emphasizes the need for codes of conduct to
ensure responsible AGI use in academic settings like homework, teaching, and
recruitment. We also conclude that the development of AGI necessitates
interdisciplinary collaborations between educators and AI engineers to advance
research and application efforts.",None,-1
d3e17c6f-d1ab-42f7-b203-b646bd4f8bba,CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models,0.764718,"In this paper, we consider the challenge of summarizing patients' medical
progress notes in a limited data setting. For the Problem List Summarization
(shared task 1A) at the BioNLP Workshop 2023, we demonstrate that Clinical-T5
fine-tuned to 765 medical clinic notes outperforms other extractive,
abstractive and zero-shot baselines, yielding reasonable baseline systems for
medical note summarization. Further, we introduce Hierarchical Ensemble of
Summarization Models (HESM), consisting of token-level ensembles of diverse
fine-tuned Clinical-T5 models, followed by Minimum Bayes Risk (MBR) decoding.
Our HESM approach lead to a considerable summarization performance boost, and
when evaluated on held-out challenge data achieved a ROUGE-L of 32.77, which
was the best-performing system at the top of the shared task leaderboard.",None,-1
e5fd0c97-f76c-4834-9ea0-0369eff338e1,Leveraging Importance Weights in Subset Selection,0.436638,"We present a subset selection algorithm designed to work with arbitrary model
families in a practical batch setting. In such a setting, an algorithm can
sample examples one at a time but, in order to limit overhead costs, is only
able to update its state (i.e. further train model weights) once a large enough
batch of examples is selected. Our algorithm, IWeS, selects examples by
importance sampling where the sampling probability assigned to each example is
based on the entropy of models trained on previously selected batches. IWeS
admits significant performance improvement compared to other subset selection
algorithms for seven publicly available datasets. Additionally, it is
competitive in an active learning setting, where the label information is not
available at selection time. We also provide an initial theoretical analysis to
support our importance weighting approach, proving generalization and sampling
rate bounds.",None,-1
44d0daf5-30b0-4542-b4ae-613d567f4321,Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models,0.443448,"This study introduces and evaluates tiny, mini, small, and medium-sized
uncased Turkish BERT models, aiming to bridge the research gap in
less-resourced languages. We trained these models on a diverse dataset
encompassing over 75GB of text from multiple sources and tested them on several
tasks, including mask prediction, sentiment analysis, news classification, and,
zero-shot classification. Despite their smaller size, our models exhibited
robust performance, including zero-shot task, while ensuring computational
efficiency and faster execution times. Our findings provide valuable insights
into the development and application of smaller language models, especially in
the context of the Turkish language.",None,-1
34d62ec3-ad1c-4373-a603-39ca1c1ea15f,Learning Human Mesh Recovery in 3D Scenes,0.792739,"We present a novel method for recovering the absolute pose and shape of a
human in a pre-scanned scene given a single image. Unlike previous methods that
perform sceneaware mesh optimization, we propose to first estimate absolute
position and dense scene contacts with a sparse 3D CNN, and later enhance a
pretrained human mesh recovery network by cross-attention with the derived 3D
scene cues. Joint learning on images and scene geometry enables our method to
reduce the ambiguity caused by depth and occlusion, resulting in more
reasonable global postures and contacts. Encoding scene-aware cues in the
network also allows the proposed method to be optimization-free, and opens up
the opportunity for real-time applications. The experiments show that the
proposed network is capable of recovering accurate and physically-plausible
meshes by a single forward pass and outperforms state-of-the-art methods in
terms of both accuracy and speed.",None,-1
e0842ac5-ecec-404d-a23d-2b214b2e98e9,Target-Side Augmentation for Document-Level Machine Translation,0.696954,"Document-level machine translation faces the challenge of data sparsity due
to its long input length and a small amount of training data, increasing the
risk of learning spurious patterns. To address this challenge, we propose a
target-side augmentation method, introducing a data augmentation (DA) model to
generate many potential translations for each source document. Learning on
these wider range translations, an MT model can learn a smoothed distribution,
thereby reducing the risk of data sparsity. We demonstrate that the DA model,
which estimates the posterior distribution, largely improves the MT
performance, outperforming the previous best system by 2.30 s-BLEU on News and
achieving new state-of-the-art on News and Europarl benchmarks. Our code is
available at https://github.com/baoguangsheng/target-side-augmentation.",None,-1
edc3b873-5cc8-4fdf-8b99-3607529412ed,Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers,0.963345,"This paper addresses the problem of cross-modal object tracking from RGB
videos and event data. Rather than constructing a complex cross-modal fusion
network, we explore the great potential of a pre-trained vision Transformer
(ViT). Particularly, we delicately investigate plug-and-play training
augmentations that encourage the ViT to bridge the vast distribution gap
between the two modalities, enabling comprehensive cross-modal information
interaction and thus enhancing its ability. Specifically, we propose a mask
modeling strategy that randomly masks a specific modality of some tokens to
enforce the interaction between tokens from different modalities interacting
proactively. To mitigate network oscillations resulting from the masking
strategy and further amplify its positive effect, we then theoretically propose
an orthogonal high-rank loss to regularize the attention matrix. Extensive
experiments demonstrate that our plug-and-play training augmentation techniques
can significantly boost state-of-the-art one-stream and twostream trackers to a
large extent in terms of both tracking precision and success rate. Our new
perspective and findings will potentially bring insights to the field of
leveraging powerful pre-trained ViTs to model cross-modal data. The code will
be publicly available.",None,-1
e4a8472d-8301-4a25-bb93-5a37e0c87a10,Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time,0.211338,"In this work, we propose a fast and accurate method to reconstruct
activations of classification and semantic segmentation networks by stitching
them with a GAN generator utilizing a 1x1 convolution. We test our approach on
images of animals from the AFHQ wild dataset, ImageNet1K, and real-world
digital pathology scans of stained tissue samples. Our results show comparable
performance to established gradient descent methods but with a processing time
that is two orders of magnitude faster, making this approach promising for
practical applications.",None,-1
c526d169-339e-44a6-b3eb-794369f62ae8,Back to Patterns: Efficient Japanese Morphological Analysis with Feature-Sequence Trie,0.585334,"Accurate neural models are much less efficient than non-neural models and are
useless for processing billions of social media posts or handling user queries
in real time with a limited budget. This study revisits the fastest
pattern-based NLP methods to make them as accurate as possible, thus yielding a
strikingly simple yet surprisingly accurate morphological analyzer for
Japanese. The proposed method induces reliable patterns from a morphological
dictionary and annotated data. Experimental results on two standard datasets
confirm that the method exhibits comparable accuracy to learning-based
baselines, while boasting a remarkable throughput of over 1,000,000 sentences
per second on a single modern CPU. The source code is available at
https://www.tkl.iis.u-tokyo.ac.jp/~ynaga/jagger/",None,-1
6de3a224-c55e-4673-8e0c-e9810c574807,Graph Decision Transformer,0.495718,"Offline reinforcement learning (RL) is a challenging task, whose objective is
to learn policies from static trajectory data without interacting with the
environment. Recently, offline RL has been viewed as a sequence modeling
problem, where an agent generates a sequence of subsequent actions based on a
set of static transition experiences. However, existing approaches that use
transformers to attend to all tokens naively can overlook the dependencies
between different tokens and limit long-term dependency learning. In this
paper, we propose the Graph Decision Transformer (GDT), a novel offline RL
approach that models the input sequence into a causal graph to capture
potential dependencies between fundamentally different concepts and facilitate
temporal and causal relationship learning. GDT uses a graph transformer to
process the graph inputs with relation-enhanced mechanisms, and an optional
sequence transformer to handle fine-grained spatial information in visual
tasks. Our experiments show that GDT matches or surpasses the performance of
state-of-the-art offline RL methods on image-based Atari and OpenAI Gym.",None,-1
3cef9d68-b846-44b4-b3cf-e593becc458c,Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation,0.592189,"Cross-modal contrastive learning in vision language pretraining (VLP) faces
the challenge of (partial) false negatives. In this paper, we study this
problem from the perspective of Mutual Information (MI) optimization. It is
common sense that InfoNCE loss used in contrastive learning will maximize the
lower bound of MI between anchors and their positives, while we theoretically
prove that MI involving negatives also matters when noises commonly exist.
Guided by a more general lower bound form for optimization, we propose a
contrastive learning strategy regulated by progressively refined cross-modal
similarity, to more accurately optimize MI between an image/text anchor and its
negative texts/images instead of improperly minimizing it. Our method performs
competitively on four downstream cross-modal tasks and systematically balances
the beneficial and harmful effects of (partial) false negative samples under
theoretical guidance.",None,-1
e4238ba8-ac5a-400b-aa8c-189fea083455,MADiff: Offline Multi-agent Learning with Diffusion Models,0.749543,"Diffusion model (DM) recently achieved huge success in various scenarios
including offline reinforcement learning, where the diffusion planner learn to
generate desired trajectories during online evaluations. However, despite the
effectiveness in single-agent learning, it remains unclear how DMs can operate
in multi-agent problems, where agents can hardly complete teamwork without good
coordination by independently modeling each agent's trajectories. In this
paper, we propose MADiff, a novel generative multi-agent learning framework to
tackle this problem. MADiff is realized with an attention-based diffusion model
to model the complex coordination among behaviors of multiple agents. To the
best of our knowledge, MADiff is the first diffusion-based multi-agent learning
framework, which behaves as both a decentralized policy and a centralized
controller. During decentralized executions, MADiff simultaneously performs
teammate modeling, and the centralized controller can also be applied in
multi-agent trajectory predictions. Our experiments show the superior
performance of MADiff compared to baseline algorithms in a wide range of
multi-agent learning tasks, which emphasizes the effectiveness of MADiff in
modeling complex multi-agent interactions. Our code is available at
https://github.com/zbzhu99/madiff.",None,-1
6580493a-2a34-4c91-b0c4-33f33a7d6330,Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition,0.781288,"We tackle the data scarcity challenge in few-shot point cloud recognition of
3D objects by using a joint prediction from a conventional 3D model and a
well-trained 2D model. Surprisingly, such an ensemble, though seems trivial,
has hardly been shown effective in recent 2D-3D models. We find out the crux is
the less effective training for the ''joint hard samples'', which have high
confidence prediction on different wrong labels, implying that the 2D and 3D
models do not collaborate well. To this end, our proposed invariant training
strategy, called InvJoint, does not only emphasize the training more on the
hard samples, but also seeks the invariance between the conflicting 2D and 3D
ambiguous predictions. InvJoint can learn more collaborative 2D and 3D
representations for better ensemble. Extensive experiments on 3D shape
classification with widely adopted ModelNet10/40, ScanObjectNN and Toys4K, and
shape retrieval with ShapeNet-Core validate the superiority of our InvJoint.",None,-1
a8590d8f-8a3c-4486-88fa-e8fb840e1cd5,Human Body Digital Twin: A Master Plan,0.176704,"A human body digital twin (DT) is a virtual representation of an individual's
physiological state, created using real-time data from sensors and medical test
devices, with the purpose of simulating, predicting, and optimizing health
outcomes through advanced analytics and simulations. The human body DT has the
potential to revolutionize healthcare and wellness, but its responsible and
effective implementation requires consideration of various factors. This
article presents a comprehensive overview of the current status and future
prospects of the human body DT and proposes a five-level roadmap for its
development. The roadmap covers the development of various components, such as
wearable devices, data collection, data analysis, and decision-making systems.
The article also highlights the necessary support, security, cost, and ethical
considerations that must be addressed in order to ensure responsible and
effective implementation of the human body DT. The proposed roadmap provides a
framework for guiding future development and offers a unique perspective on the
future of the human body DT, facilitating new interdisciplinary research and
innovative solutions in this rapidly evolving field.",None,-1
db33b137-7495-47cb-9232-4b3eb10b6d8b,Data Augmentation for Low-Resource Keyphrase Generation,0.0503036,"Keyphrase generation is the task of summarizing the contents of any given
article into a few salient phrases (or keyphrases). Existing works for the task
mostly rely on large-scale annotated datasets, which are not easy to acquire.
Very few works address the problem of keyphrase generation in low-resource
settings, but they still rely on a lot of additional unlabeled data for
pretraining and on automatic methods for pseudo-annotations. In this paper, we
present data augmentation strategies specifically to address keyphrase
generation in purely resource-constrained domains. We design techniques that
use the full text of the articles to improve both present and absent keyphrase
generation. We test our approach comprehensively on three datasets and show
that the data augmentation strategies consistently improve the state-of-the-art
performance. We release our source code at
https://github.com/kgarg8/kpgen-lowres-data-aug.",None,-1
e3ca8fef-9243-462b-ab84-3f5a91bad962,Structured State Space Models for Multiple Instance Learning in Digital Pathology,0.859252,"Multiple instance learning is an ideal mode of analysis for histopathology
data, where vast whole slide images are typically annotated with a single
global label. In such cases, a whole slide image is modelled as a collection of
tissue patches to be aggregated and classified. Common models for performing
this classification include recurrent neural networks and transformers.
Although powerful compression algorithms, such as deep pre-trained neural
networks, are used to reduce the dimensionality of each patch, the sequences
arising from whole slide images remain excessively long, routinely containing
tens of thousands of patches. Structured state space models are an emerging
alternative for sequence modelling, specifically designed for the efficient
modelling of long sequences. These models invoke an optimal projection of an
input sequence into memory units that compress the entire sequence. In this
paper, we propose the use of state space models as a multiple instance learner
to a variety of problems in digital pathology. Across experiments in metastasis
detection, cancer subtyping, mutation classification, and multitask learning,
we demonstrate the competitiveness of this new class of models with existing
state of the art approaches. Our code is available at
https://github.com/MICS-Lab/s4_digital_pathology.",None,-1
09f09e77-98d3-4b38-82f4-cae9e0b45d73,Soda: An Object-Oriented Functional Language for Specifying Human-Centered Problems,0.809944,"We present Soda (Symbolic Objective Descriptive Analysis), a language that
helps to treat qualities and quantities in a natural way and greatly simplifies
the task of checking their correctness. We present key properties for the
language motivated by the design of a descriptive language to encode complex
requirements on computer systems, and we explain how these key properties must
be addressed to model these requirements with simple definitions. We give an
overview of a tool that helps to describe problems in an easy way that we
consider more transparent and less error-prone.",None,-1
b1b06631-6612-493d-965c-f5d898ae826e,Backstepping Temporal Difference Learning,0.0574946,"Off-policy learning ability is an important feature of reinforcement learning
(RL) for practical applications. However, even one of the most elementary RL
algorithms, temporal-difference (TD) learning, is known to suffer form
divergence issue when the off-policy scheme is used together with linear
function approximation. To overcome the divergent behavior, several off-policy
TD-learning algorithms, including gradient-TD learning (GTD), and TD-learning
with correction (TDC), have been developed until now. In this work, we provide
a unified view of such algorithms from a purely control-theoretic perspective,
and propose a new convergent algorithm. Our method relies on the backstepping
technique, which is widely used in nonlinear control theory. Finally,
convergence of the proposed algorithm is experimentally verified in
environments where the standard TD-learning is known to be unstable.",None,-1
1082180b-9fc7-479c-b7e8-e735fa9301a1,MedDiff: Generating Electronic Health Records using Accelerated Denoising Diffusion Model,0.900333,"Due to patient privacy protection concerns, machine learning research in
healthcare has been undeniably slower and limited than in other application
domains. High-quality, realistic, synthetic electronic health records (EHRs)
can be leveraged to accelerate methodological developments for research
purposes while mitigating privacy concerns associated with data sharing. The
current state-of-the-art model for synthetic EHR generation is generative
adversarial networks, which are notoriously difficult to train and can suffer
from mode collapse. Denoising Diffusion Probabilistic Models, a class of
generative models inspired by statistical thermodynamics, have recently been
shown to generate high-quality synthetic samples in certain domains. It is
unknown whether these can generalize to generation of large-scale,
high-dimensional EHRs. In this paper, we present a novel generative model based
on diffusion models that is the first successful application on electronic
health records. Our model proposes a mechanism to perform class-conditional
sampling to preserve label information. We also introduce a new sampling
strategy to accelerate the inference speed. We empirically show that our model
outperforms existing state-of-the-art synthetic EHR generation methods.",None,-1
36c0fb15-90b5-4e82-a7d6-66c8a39a0050,Inspecting and Editing Knowledge Representations in Language Models,0.977738,"Neural language models (LMs) represent facts about the world described by
text. Sometimes these facts derive from training data (in most LMs, a
representation of the word ""banana"" encodes the fact that bananas are fruits).
Sometimes facts derive from input text itself (a representation of the sentence
""I poured out the bottle"" encodes the fact that the bottle became empty). We
describe REMEDI, a method for learning to map statements in natural language to
fact encodings in an LM's internal representation system. REMEDI encodings can
be used as knowledge editors: when added to LM hidden representations, they
modify downstream generation to be consistent with new facts. REMEDI encodings
may also be used as probes: when compared to LM representations, they reveal
which properties LMs already attribute to mentioned entities, in some cases
making it possible to predict when LMs will generate outputs that conflict with
background knowledge or input text. REMEDI thus links work on probing,
prompting, and LM editing, and offers steps toward general tools for
fine-grained inspection and control of knowledge in LMs.",None,-1
90da6beb-7480-412b-8f17-0f045507ad3b,Improved Trajectory Reconstruction for Markerless Pose Estimation,0.659436,"Markerless pose estimation allows reconstructing human movement from multiple
synchronized and calibrated views, and has the potential to make movement
analysis easy and quick, including gait analysis. This could enable much more
frequent and quantitative characterization of gait impairments, allowing better
monitoring of outcomes and responses to interventions. However, the impact of
different keypoint detectors and reconstruction algorithms on markerless pose
estimation accuracy has not been thoroughly evaluated. We tested these
algorithmic choices on data acquired from a multicamera system from a
heterogeneous sample of 25 individuals seen in a rehabilitation hospital. We
found that using a top-down keypoint detector and reconstructing trajectories
with an implicit function enabled accurate, smooth and anatomically plausible
trajectories, with a noise in the step width estimates compared to a GaitRite
walkway of only 8mm.",None,-1
511158c8-d67e-415d-98d8-aa6124831ee7,Zero and Few-shot Semantic Parsing with Ambiguous Inputs,0.405125,"Despite the frequent challenges posed by ambiguity when representing meaning
via natural language, it is often ignored or deliberately removed in tasks
mapping language to formally-designed representations, which generally assume a
one-to-one mapping between linguistic and formal representations. We attempt to
address this shortcoming by introducing AmP, a framework, dataset, and
challenge for translating ambiguous natural language to formal representations
like logic and code. We define templates and generate data for five
well-documented linguistic ambiguities. Using AmP, we investigate how several
few-shot text-to-code systems handle ambiguity, introducing three new metrics.
We find that large pre-trained models perform poorly at capturing the
distribution of possible meanings without deliberate instruction. However,
models are able to capture the distribution well when ambiguity is attested in
their inputs. These results motivate a call for including ambiguity explicitly
in datasets and promote considering the distribution of possible outputs when
evaluating systems. Data and code: https://github.com/esteng/ambiguous_parsing",None,-1
f2c1ea27-2e77-4c47-babd-3e3cfa8e059f,DartsReNet: Exploring new RNN cells in ReNet architectures,0.0486543,"We present new Recurrent Neural Network (RNN) cells for image classification
using a Neural Architecture Search (NAS) approach called DARTS. We are
interested in the ReNet architecture, which is a RNN based approach presented
as an alternative for convolutional and pooling steps. ReNet can be defined
using any standard RNN cells, such as LSTM and GRU. One limitation is that
standard RNN cells were designed for one dimensional sequential data and not
for two dimensions like it is the case for image classification. We overcome
this limitation by using DARTS to find new cell designs. We compare our results
with ReNet that uses GRU and LSTM cells. Our found cells outperform the
standard RNN cells on CIFAR-10 and SVHN. The improvements on SVHN indicate
generalizability, as we derived the RNN cell designs from CIFAR-10 without
performing a new cell search for SVHN.",None,-1
32d32ef7-f3eb-43db-9d12-aa0bb16e7d08,ADaPT: As-Needed Decomposition and Planning with Language Models,0.263209,"Large Language Models (LLMs) are increasingly being used for interactive
decision-making tasks requiring planning and adapting to the environment.
Recent works employ LLMs-as-agents in broadly two ways: iteratively determining
the next action (iterative executors) or generating plans and executing
sub-tasks using LLMs (plan-and-execute). However, these methods struggle with
task complexity, as the inability to execute any sub-task may lead to task
failure. To address these shortcomings, we introduce As-Needed Decomposition
and Planning for complex Tasks (ADaPT), an approach that explicitly plans and
decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute
them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity
and LLM capability. Our results demonstrate that ADaPT substantially
outperforms established strong baselines, achieving success rates up to 28.3%
higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel
compositional dataset that we introduce. Through extensive analysis, we
illustrate the importance of multilevel decomposition and establish that ADaPT
dynamically adjusts to the capabilities of the executor LLM as well as to task
complexity.",None,-1
437b4bb2-a15a-432a-b0f2-b5f7141d801d,SGAligner : 3D Scene Alignment with Scene Graphs,0.580902,"Building 3D scene graphs has recently emerged as a topic in scene
representation for several embodied AI applications to represent the world in a
structured and rich manner. With their increased use in solving downstream
tasks (eg, navigation and room rearrangement), can we leverage and recycle them
for creating 3D maps of environments, a pivotal step in agent operation? We
focus on the fundamental problem of aligning pairs of 3D scene graphs whose
overlap can range from zero to partial and can contain arbitrary changes. We
propose SGAligner, the first method for aligning pairs of 3D scene graphs that
is robust to in-the-wild scenarios (ie, unknown overlap -- if any -- and
changes in the environment). We get inspired by multi-modality knowledge graphs
and use contrastive learning to learn a joint, multi-modal embedding space. We
evaluate on the 3RScan dataset and further showcase that our method can be used
for estimating the transformation between pairs of 3D scenes. Since benchmarks
for these tasks are missing, we create them on this dataset. The code,
benchmark, and trained models are available on the project website.",None,-1
ab4f7ba3-e87f-444e-b630-f6b0e8b26676,Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation,0.925784,"The automatic detection of hate speech online is an active research area in
NLP. Most of the studies to date are based on social media datasets that
contribute to the creation of hate speech detection models trained on them.
However, data creation processes contain their own biases, and models
inherently learn from these dataset-specific biases. In this paper, we perform
a large-scale cross-dataset comparison where we fine-tune language models on
different hate speech detection datasets. This analysis shows how some datasets
are more generalisable than others when used as training data. Crucially, our
experiments show how combining hate speech detection datasets can contribute to
the development of robust hate speech detection models. This robustness holds
even when controlling by data size and compared with the best individual
datasets.",None,-1
057ad090-2dc7-43a3-9a0a-a27862d6ec7a,Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic Reasoning,0.0424762,"Large Language Models (LLM) exhibit zero-shot mathematical reasoning capacity
as a behavior emergent with scale, commonly manifesting as chain-of-thoughts
(CoT) reasoning. However, multiple empirical findings suggest that this prowess
is exclusive to LLMs with exorbitant sizes (beyond 50 billion parameters).
Meanwhile, educational neuroscientists suggest that symbolic algebraic
manipulation be introduced around the same time as arithmetic word problems to
modularize language-to-formulation, symbolic manipulation of the formulation,
and endgame arithmetic. In this paper, we start with the hypothesis that much
smaller LMs, which are weak at multi-step reasoning, can achieve reasonable
arithmetic reasoning if arithmetic word problems are posed as a
formalize-then-solve task. In our architecture, which we call SYRELM, the LM
serves the role of a translator to map natural language arithmetic questions
into a formal language (FL) description. A symbolic solver then evaluates the
FL expression to obtain the answer. A small frozen LM, equipped with an
efficient low-rank adapter, is capable of generating FL expressions that
incorporate natural language descriptions of the arithmetic problem (e.g.,
variable names and their purposes, formal expressions combining variables,
etc.). We adopt policy-gradient reinforcement learning to train the adapted LM,
informed by the non-differentiable symbolic solver. This marks a sharp
departure from the recent development in tool-augmented LLMs, in which the
external tools (e.g., calculator, Web search, etc.) are essentially detached
from the learning phase of the LM. SYRELM shows massive improvements (e.g.,
+30.65 absolute point improvement in accuracy on the SVAMP dataset using GPT-J
6B model) over base LMs, while keeping our testbed easy to diagnose, interpret
and within reach of most researchers.",None,-1
e212b3b9-d746-4c8f-8187-ab4fa40548ca,Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection,0.678621,"The spread of disinformation and propagandistic content poses a threat to
societal harmony, undermining informed decision-making and trust in reliable
sources. Online platforms often serve as breeding grounds for such content, and
malicious actors exploit the vulnerabilities of audiences to shape public
opinion. Although there have been research efforts aimed at the automatic
identification of disinformation and propaganda in social media content, there
remain challenges in terms of performance. The ArAIEval shared task aims to
further research on these particular issues within the context of the Arabic
language. In this paper, we discuss our participation in these shared tasks. We
competed in subtasks 1A and 2A, where our submitted system secured positions
9th and 10th, respectively. Our experiments consist of fine-tuning transformer
models and using zero- and few-shot learning with GPT-4.",None,-1
da9a8f48-7fef-4427-b645-a81363e6c648,Is Centralized Training with Decentralized Execution Framework Centralized Enough for MARL?,0.493521,"Centralized Training with Decentralized Execution (CTDE) has recently emerged
as a popular framework for cooperative Multi-Agent Reinforcement Learning
(MARL), where agents can use additional global state information to guide
training in a centralized way and make their own decisions only based on
decentralized local policies. Despite the encouraging results achieved, CTDE
makes an independence assumption on agent policies, which limits agents to
adopt global cooperative information from each other during centralized
training. Therefore, we argue that existing CTDE methods cannot fully utilize
global information for training, leading to an inefficient joint-policy
exploration and even suboptimal results. In this paper, we introduce a novel
Centralized Advising and Decentralized Pruning (CADP) framework for multi-agent
reinforcement learning, that not only enables an efficacious message exchange
among agents during training but also guarantees the independent policies for
execution. Firstly, CADP endows agents the explicit communication channel to
seek and take advices from different agents for more centralized training. To
further ensure the decentralized execution, we propose a smooth model pruning
mechanism to progressively constraint the agent communication into a closed one
without degradation in agent cooperation capability. Empirical evaluations on
StarCraft II micromanagement and Google Research Football benchmarks
demonstrate that the proposed framework achieves superior performance compared
with the state-of-the-art counterparts. Our code will be made publicly
available.",None,-1
3dcf3f05-f763-4e76-9148-2520e610a407,The Wizard of Curiosities: Enriching Dialogues with Fun Facts,0.120421,"Introducing curiosities in a conversation is a way to teach something new to
the person in a pleasant and enjoyable way. Enriching dialogues with
contextualized curiosities can improve the users' perception of a dialog system
and their overall user experience. In this paper, we introduce a set of curated
curiosities, targeting dialogues in the cooking and DIY domains. In particular,
we use real human-agent conversations collected in the context of the Amazon
Alexa TaskBot challenge, a multimodal and multi-turn conversational setting.
According to an A/B test with over 1000 conversations, curiosities not only
increase user engagement, but provide an average relative rating improvement of
9.7%.",None,-1
35eebdef-5452-46ee-87cc-58ce74ab35d4,Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations,0.996819,"Extracting generalized and robust representations is a major challenge in
emotion recognition in conversations (ERC). To address this, we propose a
supervised adversarial contrastive learning (SACL) framework for learning
class-spread structured representations in a supervised manner. SACL applies
contrast-aware adversarial training to generate worst-case samples and uses
joint class-spread contrastive learning to extract structured representations.
It can effectively utilize label-level feature consistency and retain
fine-grained intra-class features. To avoid the negative impact of adversarial
perturbations on context-dependent data, we design a contextual adversarial
training (CAT) strategy to learn more diverse features from context and enhance
the model's context robustness. Under the framework with CAT, we develop a
sequence-based SACL-LSTM to learn label-consistent and context-robust features
for ERC. Experiments on three datasets show that SACL-LSTM achieves
state-of-the-art performance on ERC. Extended experiments prove the
effectiveness of SACL and CAT.",None,-1
781dce0e-1ad9-42f6-a109-5417ef4ca4df,Benchmarking Stroke Forecasting with Stroke-Level Badminton Dataset,0.388675,"In recent years, badminton analytics has drawn attention due to the
advancement of artificial intelligence and the efficiency of data collection.
While there is a line of effective applications to improve and investigate
player performance, there are only a few public badminton datasets that can be
used by researchers outside the badminton domain. Existing badminton singles
datasets focus on specific matchups; however, they cannot provide comprehensive
studies on different players and various matchups. In this paper, we provide a
badminton singles dataset, ShuttleSet22, which is collected from high-ranking
matches in 2022. ShuttleSet22 consists of 30,172 strokes in 2,888 rallies in
the training set, 1,400 strokes in 450 rallies in the validation set, and 2,040
strokes in 654 rallies in the testing set, with detailed stroke-level metadata
within a rally. To benchmark existing work with ShuttleSet22, we hold a
challenge, Track 2: Forecasting Future Turn-Based Strokes in Badminton Rallies,
at CoachAI Badminton Challenge @ IJCAI 2023, to encourage researchers to tackle
this real-world problem through innovative approaches and to summarize insights
between the state-of-the-art baseline and improved techniques, exchanging
inspiring ideas. The baseline codes and the dataset are made available at
https://github.com/wywyWang/CoachAI-Projects/tree/main/CoachAI-Challenge-IJCAI2023.",None,-1
d0eaf86d-1efa-4c39-a7ee-85848c15c77f,A New Deep Learning and XAI-Based Algorithm for Features Selection in Genomics,0.13099,"In the field of functional genomics, the analysis of gene expression profiles
through Machine and Deep Learning is increasingly providing meaningful insight
into a number of diseases. The paper proposes a novel algorithm to perform
Feature Selection on genomic-scale data, which exploits the reconstruction
capabilities of autoencoders and an ad-hoc defined Explainable Artificial
Intelligence-based score in order to select the most informative genes for
diagnosis, prognosis, and precision medicine. Results of the application on a
Chronic Lymphocytic Leukemia dataset evidence the effectiveness of the
algorithm, by identifying and suggesting a set of meaningful genes for further
medical investigation.",None,-1
f0630bd0-a54e-4556-8471-43a492acc3ba,Deep Metric Multi-View Hashing for Multimedia Retrieval,0.795111,"Learning the hash representation of multi-view heterogeneous data is an
important task in multimedia retrieval. However, existing methods fail to
effectively fuse the multi-view features and utilize the metric information
provided by the dissimilar samples, leading to limited retrieval precision.
Current methods utilize weighted sum or concatenation to fuse the multi-view
features. We argue that these fusion methods cannot capture the interaction
among different views. Furthermore, these methods ignored the information
provided by the dissimilar samples. We propose a novel deep metric multi-view
hashing (DMMVH) method to address the mentioned problems. Extensive empirical
evidence is presented to show that gate-based fusion is better than typical
methods. We introduce deep metric learning to the multi-view hashing problems,
which can utilize metric information of dissimilar samples. On the
MIR-Flickr25K, MS COCO, and NUS-WIDE, our method outperforms the current
state-of-the-art methods by a large margin (up to 15.28 mean Average Precision
(mAP) improvement).",None,-1
c93a8b67-735f-443c-8ed6-d6e3b33902b5,A Universal Question-Answering Platform for Knowledge Graphs,0.546348,"Knowledge from diverse application domains is organized as knowledge graphs
(KGs) that are stored in RDF engines accessible in the web via SPARQL
endpoints. Expressing a well-formed SPARQL query requires information about the
graph structure and the exact URIs of its components, which is impractical for
the average user. Question answering (QA) systems assist by translating natural
language questions to SPARQL. Existing QA systems are typically based on
application-specific human-curated rules, or require prior information,
expensive pre-processing and model adaptation for each targeted KG. Therefore,
they are hard to generalize to a broad set of applications and KGs.
  In this paper, we propose KGQAn, a universal QA system that does not need to
be tailored to each target KG. Instead of curated rules, KGQAn introduces a
novel formalization of question understanding as a text generation problem to
convert a question into an intermediate abstract representation via a neural
sequence-to-sequence model. We also develop a just-in-time linker that maps at
query time the abstract representation to a SPARQL query for a specific KG,
using only the publicly accessible APIs and the existing indices of the RDF
store, without requiring any pre-processing. Our experiments with several real
KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin
the state-of-the-art in terms of quality of answers and processing time,
especially for arbitrary KGs, unseen during the training.",None,-1
6b807e64-e4ae-4c16-b4c2-f45650877e10,Dynamic Residual Classifier for Class Incremental Learning,0.704187,"The rehearsal strategy is widely used to alleviate the catastrophic
forgetting problem in class incremental learning (CIL) by preserving limited
exemplars from previous tasks. With imbalanced sample numbers between old and
new classes, the classifier learning can be biased. Existing CIL methods
exploit the long-tailed (LT) recognition techniques, e.g., the adjusted losses
and the data re-sampling methods, to handle the data imbalance issue within
each increment task. In this work, the dynamic nature of data imbalance in CIL
is shown and a novel Dynamic Residual Classifier (DRC) is proposed to handle
this challenging scenario. Specifically, DRC is built upon a recent advance
residual classifier with the branch layer merging to handle the model-growing
problem. Moreover, DRC is compatible with different CIL pipelines and
substantially improves them. Combining DRC with the model adaptation and fusion
(MAF) pipeline, this method achieves state-of-the-art results on both the
conventional CIL and the LT-CIL benchmarks. Extensive experiments are also
conducted for a detailed analysis. The code is publicly available.",None,-1
ca558118-933b-4f55-9d24-f448e426867b,Weight-Inherited Distillation for Task-Agnostic BERT Compression,0.476911,"Knowledge Distillation (KD) is a predominant approach for BERT compression.
Previous KD-based methods focus on designing extra alignment losses for the
student model to mimic the behavior of the teacher model. These methods
transfer the knowledge in an indirect way. In this paper, we propose a novel
Weight-Inherited Distillation (WID), which directly transfers knowledge from
the teacher. WID does not require any additional alignment loss and trains a
compact student by inheriting the weights, showing a new perspective of
knowledge distillation. Specifically, we design the row compactors and column
compactors as mappings and then compress the weights via structural
re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show
that WID outperforms previous state-of-the-art KD-based baselines. Further
analysis indicates that WID can also learn the attention patterns from the
teacher model without any alignment loss on attention distributions. The code
is available at https://github.com/wutaiqiang/WID-NAACL2024.",None,-1
033cdd83-046a-47a1-962f-d668579b25dc,Resource-constrained FPGA Design for Satellite Component Feature Extraction,0.0763342,"The effective use of computer vision and machine learning for on-orbit
applications has been hampered by limited computing capabilities, and therefore
limited performance. While embedded systems utilizing ARM processors have been
shown to meet acceptable but low performance standards, the recent availability
of larger space-grade field programmable gate arrays (FPGAs) show potential to
exceed the performance of microcomputer systems. This work proposes use of
neural network-based object detection algorithm that can be deployed on a
comparably resource-constrained FPGA to automatically detect components of
non-cooperative, satellites on orbit. Hardware-in-the-loop experiments were
performed on the ORION Maneuver Kinematics Simulator at Florida Tech to compare
the performance of the new model deployed on a small, resource-constrained FPGA
to an equivalent algorithm on a microcomputer system. Results show the FPGA
implementation increases the throughput and decreases latency while maintaining
comparable accuracy. These findings suggest future missions should consider
deploying computer vision algorithms on space-grade FPGAs.",None,-1
1795a5fc-9759-4917-af40-c20968d164a6,Heterogeneous Forgetting Compensation for Class-Incremental Learning,0.833928,"Class-incremental learning (CIL) has achieved remarkable successes in
learning new classes consecutively while overcoming catastrophic forgetting on
old categories. However, most existing CIL methods unreasonably assume that all
old categories have the same forgetting pace, and neglect negative influence of
forgetting heterogeneity among different old classes on forgetting
compensation. To surmount the above challenges, we develop a novel
Heterogeneous Forgetting Compensation (HFC) model, which can resolve
heterogeneous forgetting of easy-to-forget and hard-to-forget old categories
from both representation and gradient aspects. Specifically, we design a
task-semantic aggregation block to alleviate heterogeneous forgetting from
representation aspect. It aggregates local category information within each
task to learn task-shared global representations. Moreover, we develop two
novel plug-and-play losses: a gradient-balanced forgetting compensation loss
and a gradient-balanced relation distillation loss to alleviate forgetting from
gradient aspect. They consider gradient-balanced compensation to rectify
forgetting heterogeneity of old categories and heterogeneous relation
consistency. Experiments on several representative datasets illustrate
effectiveness of our HFC model. The code is available at
https://github.com/JiahuaDong/HFC.",None,-1
eed31fc4-9ab0-47aa-9dbb-2ca258e74f8a,DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning,0.310214,"Pragmatic reasoning plays a pivotal role in deciphering implicit meanings
that frequently arise in real-life conversations and is essential for the
development of communicative social agents. In this paper, we introduce a novel
challenge, DiPlomat, aiming at benchmarking machines' capabilities on pragmatic
reasoning and situated conversational understanding. Compared with previous
works that treat different figurative expressions (e.g. metaphor, sarcasm) as
individual tasks, DiPlomat provides a cohesive framework towards general
pragmatic understanding. Our dataset is created through the utilization of
Amazon Mechanical Turk ( AMT ), resulting in a total of 4, 177 multi-turn
dialogues. In conjunction with the dataset, we propose two tasks, Pragmatic
Identification and Reasoning (PIR) and Conversational Question Answering (CQA).
Experimental results with state-of-the-art (SOTA) neural architectures reveal
several significant findings: 1) large language models ( LLMs) exhibit poor
performance in tackling this subjective domain; 2) comprehensive comprehension
of context emerges as a critical factor for establishing benign human-machine
interactions; 3) current models defect in the application of pragmatic
reasoning. As a result, we call on more attention to improve the ability of
context understanding, reasoning, and implied meaning modeling.",None,-1
9b209324-c3d5-4b57-b83f-500e2b39fee4,Volumetric Fast Fourier Convolution for Detecting Ink on the Carbonized Herculaneum Papyri,0.718671,"Recent advancements in Digital Document Restoration (DDR) have led to
significant breakthroughs in analyzing highly damaged written artifacts. Among
those, there has been an increasing interest in applying Artificial
Intelligence techniques for virtually unwrapping and automatically detecting
ink on the Herculaneum papyri collection. This collection consists of
carbonized scrolls and fragments of documents, which have been digitized via
X-ray tomography to allow the development of ad-hoc deep learning-based DDR
solutions. In this work, we propose a modification of the Fast Fourier
Convolution operator for volumetric data and apply it in a segmentation
architecture for ink detection on the challenging Herculaneum papyri,
demonstrating its suitability via deep experimental analysis. To encourage the
research on this task and the application of the proposed operator to other
tasks involving volumetric data, we will release our implementation
(https://github.com/aimagelab/vffc)",None,-1
d3aa6148-d5cb-4989-822f-029272139dac,Speaker attribution in German parliamentary debates with QLoRA-adapted large language models,0.147704,"The growing body of political texts opens up new opportunities for rich
insights into political dynamics and ideologies but also increases the workload
for manual analysis. Automated speaker attribution, which detects who said what
to whom in a speech event and is closely related to semantic role labeling, is
an important processing step for computational text analysis. We study the
potential of the large language model family Llama 2 to automate speaker
attribution in German parliamentary debates from 2017-2021. We fine-tune Llama
2 with QLoRA, an efficient training strategy, and observe our approach to
achieve competitive performance in the GermEval 2023 Shared Task On Speaker
Attribution in German News Articles and Parliamentary Debates. Our results shed
light on the capabilities of large language models in automating speaker
attribution, revealing a promising avenue for computational analysis of
political discourse and the development of semantic role labeling systems.",None,-1
6f7b5fcf-8c5d-4faf-b46c-04ce30413a3c,I2SRM: Intra- and Inter-Sample Relationship Modeling for Multimodal Information Extraction,0.78707,"Multimodal information extraction is attracting research attention nowadays,
which requires aggregating representations from different modalities. In this
paper, we present the Intra- and Inter-Sample Relationship Modeling (I2SRM)
method for this task, which contains two modules. Firstly, the intra-sample
relationship modeling module operates on a single sample and aims to learn
effective representations. Embeddings from textual and visual modalities are
shifted to bridge the modality gap caused by distinct pre-trained language and
image models. Secondly, the inter-sample relationship modeling module considers
relationships among multiple samples and focuses on capturing the interactions.
An AttnMixup strategy is proposed, which not only enables collaboration among
samples but also augments data to improve generalization. We conduct extensive
experiments on the multimodal named entity recognition datasets Twitter-2015
and Twitter-2017, and the multimodal relation extraction dataset MNRE. Our
proposed method I2SRM achieves competitive results, 77.12% F1-score on
Twitter-2015, 88.40% F1-score on Twitter-2017, and 84.12% F1-score on MNRE.",None,-1
5e602d61-b4d7-4255-a90a-6a4601d04e89,Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3),0.501471,"Addressing pose ambiguity in 6D object pose estimation from single RGB images
presents a significant challenge, particularly due to object symmetries or
occlusions. In response, we introduce a novel score-based diffusion method
applied to the $SE(3)$ group, marking the first application of diffusion models
to $SE(3)$ within the image domain, specifically tailored for pose estimation
tasks. Extensive evaluations demonstrate the method's efficacy in handling pose
ambiguity, mitigating perspective-induced ambiguity, and showcasing the
robustness of our surrogate Stein score formulation on $SE(3)$. This
formulation not only improves the convergence of denoising process but also
enhances computational efficiency. Thus, we pioneer a promising strategy for 6D
object pose estimation.",None,-1
da75578d-c8b8-439e-a784-c33fb85be406,Multilingual Event Extraction from Historical Newspaper Adverts,0.371531,"NLP methods can aid historians in analyzing textual materials in greater
volumes than manually feasible. Developing such methods poses substantial
challenges though. First, acquiring large, annotated historical datasets is
difficult, as only domain experts can reliably label them. Second, most
available off-the-shelf NLP models are trained on modern language texts,
rendering them significantly less effective when applied to historical corpora.
This is particularly problematic for less well studied tasks, and for languages
other than English. This paper addresses these challenges while focusing on the
under-explored task of event extraction from a novel domain of historical
texts. We introduce a new multilingual dataset in English, French, and Dutch
composed of newspaper ads from the early modern colonial period reporting on
enslaved people who liberated themselves from enslavement. We find that: 1)
even with scarce annotated data, it is possible to achieve surprisingly good
results by formulating the problem as an extractive QA task and leveraging
existing datasets and models for modern languages; and 2) cross-lingual
low-resource learning for historical languages is highly challenging, and
machine translation of the historical datasets to the considered target
languages is, in practice, often the best-performing solution.",None,-1
7c389077-f227-495d-808a-a23806806060,NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis,0.605777,"This paper describes our system developed for the SemEval-2023 Task 12
""Sentiment Analysis for Low-resource African Languages using Twitter Dataset"".
Sentiment analysis is one of the most widely studied applications in natural
language processing. However, most prior work still focuses on a small number
of high-resource languages. Building reliable sentiment analysis systems for
low-resource languages remains challenging, due to the limited training data in
this task. In this work, we propose to leverage language-adaptive and
task-adaptive pretraining on African texts and study transfer learning with
source language selection on top of an African language-centric pretrained
language model. Our key findings are: (1) Adapting the pretrained model to the
target language and task using a small yet relevant corpus improves performance
remarkably by more than 10 F1 score points. (2) Selecting source languages with
positive transfer gains during training can avoid harmful interference from
dissimilar languages, leading to better results in multilingual and
cross-lingual settings. In the shared task, our system wins 8 out of 15 tracks
and, in particular, performs best in the multilingual evaluation.",None,-1
f43198a7-ef6c-4be9-b361-c2c6370bac91,Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,0.251094,"Non-AutoRegressive (NAR) text generation models have drawn much attention
because of their significantly faster decoding speed and good generation
quality in machine translation. However, in a wider range of text generation
tasks, existing NAR models lack proper pre-training, making them still far
behind the pre-trained autoregressive models. In this paper, we propose
Pre-trained Directed Acyclic Transformer (PreDAT) and a novel pre-training task
to promote prediction consistency in NAR generation. Experiments on five text
generation tasks show that our PreDAT remarkably outperforms existing
pre-trained NAR models (+4.2 scores on average) and even achieves better
results than pre-trained autoregressive baselines in n-gram-based metrics,
along with 17 times speedup in throughput. Further analysis shows that PreDAT
benefits from the unbiased prediction order that alleviates the error
accumulation problem in autoregressive generation, which provides new insights
into the advantages of NAR generation.",None,-1
5fc23ef5-4610-4284-bf96-6baa293527ae,Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization,0.473286,"Prompt tuning is one of the successful approaches for parameter-efficient
tuning of pre-trained language models. Despite being arguably the most
parameter-efficient (tuned soft prompts constitute <0.1% of total parameters),
it typically performs worse than other efficient tuning methods and is quite
sensitive to hyper-parameters. In this work, we introduce Residual Prompt
Tuning - a simple and efficient method that significantly improves the
performance and stability of prompt tuning. We propose to reparameterize soft
prompt embeddings using a shallow network with a residual connection. Our
experiments show that Residual Prompt Tuning significantly outperforms prompt
tuning on SuperGLUE benchmark. Notably, our method reaches +7 points
improvement over prompt tuning with T5-Base and allows to reduce the prompt
length by 10x without hurting performance. In addition, we show that our
approach is robust to the choice of learning rate and prompt initialization,
and is effective in few-shot settings.",None,-1
25f23df1-469a-4b63-9bbb-a18ac54a9091,KBNet: Kernel Basis Network for Image Restoration,0.959461,"How to aggregate spatial information plays an essential role in
learning-based image restoration. Most existing CNN-based networks adopt static
convolutional kernels to encode spatial information, which cannot aggregate
spatial information adaptively. Recent transformer-based architectures achieve
adaptive spatial aggregation. But they lack desirable inductive biases of
convolutions and require heavy computational costs. In this paper, we propose a
kernel basis attention (KBA) module, which introduces learnable kernel bases to
model representative image patterns for spatial information aggregation.
Different kernel bases are trained to model different local structures. At each
spatial location, they are linearly and adaptively fused by predicted
pixel-wise coefficients to obtain aggregation weights. Based on the KBA module,
we further design a multi-axis feature fusion (MFF) block to encode and fuse
channel-wise, spatial-invariant, and pixel-adaptive features for image
restoration. Our model, named kernel basis network (KBNet), achieves
state-of-the-art performances on more than ten benchmarks over image denoising,
deraining, and deblurring tasks while requiring less computational cost than
previous SOTA methods.",None,-1
7ca6ff44-eb56-412c-b8bb-7e81d38c7fc6,LLMLight: Large Language Models as Traffic Signal Control Agents,0.457935,"Traffic Signal Control (TSC) is a crucial component in urban traffic
management, aiming to optimize road network efficiency and reduce congestion.
Traditional methods in TSC, primarily based on transportation engineering and
reinforcement learning (RL), often exhibit limitations in generalization across
varied traffic scenarios and lack interpretability. This paper presents
LLMLight, a novel framework employing Large Language Models (LLMs) as
decision-making agents for TSC. Specifically, the framework begins by
instructing the LLM with a knowledgeable prompt detailing real-time traffic
conditions. Leveraging the advanced generalization capabilities of LLMs,
LLMLight engages a reasoning and decision-making process akin to human
intuition for effective traffic control. Moreover, we build LightGPT, a
specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic
patterns and control strategies, LightGPT enhances the LLMLight framework
cost-effectively. Extensive experiments on nine real-world and synthetic
datasets showcase the remarkable effectiveness, generalization ability, and
interpretability of LLMLight against nine transportation-based and RL-based
baselines.",None,-1
ebfc8713-2e8f-4c6b-8d38-6f018d7cace9,An Adversarial Multi-Task Learning Method for Chinese Text Correction with Semantic Detection,0.0124517,"Text correction, especially the semantic correction of more widely used
scenes, is strongly required to improve, for the fluency and writing efficiency
of the text. An adversarial multi-task learning method is proposed to enhance
the modeling and detection ability of character polysemy in Chinese sentence
context. Wherein, two models, the masked language model and scoring language
model, are introduced as a pair of not only coupled but also adversarial
learning tasks. Moreover, the Monte Carlo tree search strategy and a policy
network are introduced to accomplish the efficient Chinese text correction task
with semantic detection. The experiments are executed on three datasets and
five comparable methods, and the experimental results show that our method can
obtain good performance in Chinese text correction task for better semantic
rationality.",None,-1
9b511877-c20f-462f-bde0-5599d4b33c27,A Frustratingly Easy Improvement for Position Embeddings via Random Padding,0.761132,"Position embeddings, encoding the positional relationships among tokens in
text sequences, make great contributions to modeling local context features in
Transformer-based pre-trained language models. However, in Extractive Question
Answering, position embeddings trained with instances of varied context lengths
may not perform well as we expect. Since the embeddings of rear positions are
updated fewer times than the front position embeddings, the rear ones may not
be properly trained. In this paper, we propose a simple but effective strategy,
Random Padding, without any modifications to architectures of existing
pre-trained language models. We adjust the token order of input sequences when
fine-tuning, to balance the number of updating times of every position
embedding. Experiments show that Random Padding can significantly improve model
performance on the instances whose answers are located at rear positions,
especially when models are trained on short contexts but evaluated on long
contexts. Our code and data will be released for future research.",None,-1
955a359c-7cbc-4c41-bea9-fbb413ad62ad,RelPose++: Recovering 6D Poses from Sparse-view Observations,0.982951,"We address the task of estimating 6D camera poses from sparse-view image sets
(2-8 images). This task is a vital pre-processing stage for nearly all
contemporary (neural) reconstruction algorithms but remains challenging given
sparse views, especially for objects with visual symmetries and texture-less
surfaces. We build on the recent RelPose framework which learns a network that
infers distributions over relative rotations over image pairs. We extend this
approach in two key ways; first, we use attentional transformer layers to
process multiple images jointly, since additional views of an object may
resolve ambiguous symmetries in any given image pair (such as the handle of a
mug that becomes visible in a third view). Second, we augment this network to
also report camera translations by defining an appropriate coordinate system
that decouples the ambiguity in rotation estimation from translation
prediction. Our final system results in large improvements in 6D pose
prediction over prior art on both seen and unseen object categories and also
enables pose estimation and 3D reconstruction for in-the-wild objects.",None,-1
c9974553-1aaa-4c42-b776-ab71f41d136d,Explainable Multi-Agent Reinforcement Learning for Temporal Queries,0.577713,"As multi-agent reinforcement learning (MARL) systems are increasingly
deployed throughout society, it is imperative yet challenging for users to
understand the emergent behaviors of MARL agents in complex environments. This
work presents an approach for generating policy-level contrastive explanations
for MARL to answer a temporal user query, which specifies a sequence of tasks
completed by agents with possible cooperation. The proposed approach encodes
the temporal query as a PCTL logic formula and checks if the query is feasible
under a given MARL policy via probabilistic model checking. Such explanations
can help reconcile discrepancies between the actual and anticipated multi-agent
behaviors. The proposed approach also generates correct and complete
explanations to pinpoint reasons that make a user query infeasible. We have
successfully applied the proposed approach to four benchmark MARL domains (up
to 9 agents in one domain). Moreover, the results of a user study show that the
generated explanations significantly improve user performance and satisfaction.",None,-1
d155021b-163b-4ea4-86a1-b7e5538ce3be,Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification,0.471349,"Adapters and Low-Rank Adaptation (LoRA) are parameter-efficient fine-tuning
techniques designed to make the training of language models more efficient.
Previous results demonstrated that these methods can even improve performance
on some classification tasks. This paper complements the existing research by
investigating how these techniques influence the classification performance and
computation costs compared to full fine-tuning when applied to multilingual
text classification tasks (genre, framing, and persuasion techniques detection;
with different input lengths, number of predicted classes and classification
difficulty), some of which have limited training data. In addition, we conduct
in-depth analyses of their efficacy across different training scenarios
(training on the original multilingual data; on the translations into English;
and on a subset of English-only data) and different languages. Our findings
provide valuable insights into the applicability of the parameter-efficient
fine-tuning techniques, particularly to complex multilingual and multilabel
classification tasks.",None,-1
568d9a74-1415-42c0-ad92-491b2d41e935,Enhancing Continual Relation Extraction via Classifier Decomposition,0.815964,"Continual relation extraction (CRE) models aim at handling emerging new
relations while avoiding catastrophically forgetting old ones in the streaming
data. Though improvements have been shown by previous CRE studies, most of them
only adopt a vanilla strategy when models first learn representations of new
relations. In this work, we point out that there exist two typical biases after
training of this vanilla strategy: classifier bias and representation bias,
which causes the previous knowledge that the model learned to be shaded. To
alleviate those biases, we propose a simple yet effective classifier
decomposition framework that splits the last FFN layer into separated previous
and current classifiers, so as to maintain previous knowledge and encourage the
model to learn more robust representations at this training stage. Experimental
results on two standard benchmarks show that our proposed framework
consistently outperforms the state-of-the-art CRE models, which indicates that
the importance of the first training stage to CRE models may be underestimated.
Our code is available at https://github.com/hemingkx/CDec.",None,-1
260d7446-76d7-40ab-ae04-fa1b0bfc36be,Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation,0.857866,"Language models have achieved impressive performances on dialogue generation
tasks. However, when generating responses for a conversation that requires
factual knowledge, they are far from perfect, due to an absence of mechanisms
to retrieve, encode, and reflect the knowledge in the generated responses. Some
knowledge-grounded dialogue generation methods tackle this problem by
leveraging facts from Knowledge Graphs (KGs); however, they do not guarantee
that the model utilizes a relevant piece of knowledge from the KG. To overcome
this limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a
framework for generating context-relevant and knowledge-grounded dialogues with
the KG. Specifically, our SURGE framework first retrieves the relevant subgraph
from the KG, and then enforces consistency across facts by perturbing their
word embeddings conditioned by the retrieved subgraph. Then, we utilize
contrastive learning to ensure that the generated texts have high similarity to
the retrieved subgraphs. We validate our SURGE framework on OpendialKG and
KOMODIS datasets, showing that it generates high-quality dialogues that
faithfully reflect the knowledge from KG.",None,-1
882fd1e4-13ba-43fc-b67f-41378400539a,Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning,0.707349,"Large language models (LLMs) have achieved remarkable success across a wide
spectrum of tasks; however, they still face limitations in scenarios that
demand long-term planning and spatial reasoning. To facilitate this line of
research, in this work, we propose a new benchmark, termed $\textbf{P}$ath
$\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage
($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by
formulating ''path planning'' tasks that require an LLM to navigate to target
locations while avoiding obstacles and adhering to constraints. Leveraging this
benchmark, we systematically investigate LLMs including GPT-4 via different
few-shot prompting methodologies as well as BART and T5 of various sizes via
fine-tuning. Our experimental results show the promise of few-shot GPT-4 in
spatial reasoning, when it is prompted to reason and act interleavedly,
although it still fails to perform long-term temporal reasoning. In contrast,
while fine-tuned LLMs achieved impressive results on in-distribution reasoning
tasks, they struggled to generalize to larger environments or environments with
more obstacles.",None,-1
61294f60-1066-4016-a7e1-2b3044ede367,Towards Adversarially Robust Continual Learning,0.313512,"Recent studies show that models trained by continual learning can achieve the
comparable performances as the standard supervised learning and the learning
flexibility of continual learning models enables their wide applications in the
real world. Deep learning models, however, are shown to be vulnerable to
adversarial attacks. Though there are many studies on the model robustness in
the context of standard supervised learning, protecting continual learning from
adversarial attacks has not yet been investigated. To fill in this research
gap, we are the first to study adversarial robustness in continual learning and
propose a novel method called \textbf{T}ask-\textbf{A}ware \textbf{B}oundary
\textbf{A}ugmentation (TABA) to boost the robustness of continual learning
models. With extensive experiments on CIFAR-10 and CIFAR-100, we show the
efficacy of adversarial training and TABA in defending adversarial attacks.",None,-1
0b3be4a4-49ff-45ea-aae6-1583f45b3e81,RQAT-INR: Improved Implicit Neural Image Compression,0.620297,"Deep variational autoencoders for image and video compression have gained
significant attraction in the recent years, due to their potential to offer
competitive or better compression rates compared to the decades long
traditional codecs such as AVC, HEVC or VVC. However, because of complexity and
energy consumption, these approaches are still far away from practical usage in
industry. More recently, implicit neural representation (INR) based codecs have
emerged, and have lower complexity and energy usage to classical approaches at
decoding. However, their performances are not in par at the moment with
state-of-the-art methods. In this research, we first show that INR based image
codec has a lower complexity than VAE based approaches, then we propose several
improvements for INR-based image codec and outperformed baseline model by a
large margin.",None,-1
9e1c0ac9-9f07-480c-8104-1a4ee6265b74,Fully Bayesian VIB-DeepSSM,0.310853,"Statistical shape modeling (SSM) enables population-based quantitative
analysis of anatomical shapes, informing clinical diagnosis. Deep learning
approaches predict correspondence-based SSM directly from unsegmented 3D images
but require calibrated uncertainty quantification, motivating Bayesian
formulations. Variational information bottleneck DeepSSM (VIB-DeepSSM) is an
effective, principled framework for predicting probabilistic shapes of anatomy
from images with aleatoric uncertainty quantification. However, VIB is only
half-Bayesian and lacks epistemic uncertainty inference. We derive a fully
Bayesian VIB formulation and demonstrate the efficacy of two scalable
implementation approaches: concrete dropout and batch ensemble. Additionally,
we introduce a novel combination of the two that further enhances uncertainty
calibration via multimodal marginalization. Experiments on synthetic shapes and
left atrium data demonstrate that the fully Bayesian VIB network predicts SSM
from images with improved uncertainty reasoning without sacrificing accuracy.",None,-1
2e0ed06a-8517-46d9-a279-26339be3f78f,M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,0.892155,"Medical vision-language models enable co-learning and integrating features
from medical imaging and clinical text. However, these models are not easy to
train and the latent representation space can be complex. Here we propose a
novel way for pre-training and regularising medical vision-language models. The
proposed method, named Medical vision-language pre-training with Frozen
language models and Latent spAce Geometry optimization (M-FLAG), leverages a
frozen language model for training stability and efficiency and introduces a
novel orthogonality loss to harmonize the latent space geometry. We demonstrate
the potential of the pre-trained model on three downstream tasks: medical image
classification, segmentation, and object detection. Extensive experiments
across five public datasets demonstrate that M-FLAG significantly outperforms
existing medical vision-language pre-training approaches and reduces the number
of parameters by 78\%. Notably, M-FLAG achieves outstanding performance on the
segmentation task while using only 1\% of the RSNA dataset, even outperforming
ImageNet pre-trained models that have been fine-tuned using 100\% of the data.",None,-1
65124f42-099e-4d07-8988-a23fd41edb52,FinEntity: Entity-level Sentiment Classification for Financial Texts,0.935926,"In the financial domain, conducting entity-level sentiment analysis is
crucial for accurately assessing the sentiment directed toward a specific
financial entity. To our knowledge, no publicly available dataset currently
exists for this purpose. In this work, we introduce an entity-level sentiment
classification dataset, called \textbf{FinEntity}, that annotates financial
entity spans and their sentiment (positive, neutral, and negative) in financial
news. We document the dataset construction process in the paper. Additionally,
we benchmark several pre-trained models (BERT, FinBERT, etc.) and ChatGPT on
entity-level sentiment classification. In a case study, we demonstrate the
practical utility of using FinEntity in monitoring cryptocurrency markets. The
data and code of FinEntity is available at
\url{https://github.com/yixuantt/FinEntity}",None,-1
635f7a7a-3680-4d52-809e-b18f95b148e4,Can Large Language Models assist in Hazard Analysis?,0.659057,"Large Language Models (LLMs), such as GPT-3, have demonstrated remarkable
natural language processing and generation capabilities and have been applied
to a variety tasks, such as source code generation. This paper explores the
potential of integrating LLMs in the hazard analysis for safety-critical
systems, a process which we refer to as co-hazard analysis (CoHA). In CoHA, a
human analyst interacts with an LLM via a context-aware chat session and uses
the responses to support elicitation of possible hazard causes. In this
experiment, we explore CoHA with three increasingly complex versions of a
simple system, using Open AI's ChatGPT service. The quality of ChatGPT's
responses were systematically assessed to determine the feasibility of CoHA
given the current state of LLM technology. The results suggest that LLMs may be
useful for supporting human analysts performing hazard analysis.",None,-1
536761d1-d909-43c2-abdb-6bcb17a22f9c,A Mathematical Guide to Operator Learning,0.871258,"Operator learning aims to discover properties of an underlying dynamical
system or partial differential equation (PDE) from data. Here, we present a
step-by-step guide to operator learning. We explain the types of problems and
PDEs amenable to operator learning, discuss various neural network
architectures, and explain how to employ numerical PDE solvers effectively. We
also give advice on how to create and manage training data and conduct
optimization. We offer intuition behind the various neural network
architectures employed in operator learning by motivating them from the
point-of-view of numerical linear algebra.",None,-1
9ed39005-a9b1-4c5e-9a46-38136370da24,Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023,0.573349,"MentalRiskES is a novel challenge that proposes to solve problems related to
early risk detection for the Spanish language. The objective is to detect, as
soon as possible, Telegram users who show signs of mental disorders considering
different tasks. Task 1 involved the users' detection of eating disorders, Task
2 focused on depression detection, and Task 3 aimed at detecting an unknown
disorder. These tasks were divided into subtasks, each one defining a
resolution approach. Our research group participated in subtask A for Tasks 1
and 2: a binary classification problem that evaluated whether the users were
positive or negative. To solve these tasks, we proposed models based on
Transformers followed by a decision policy according to criteria defined by an
early detection framework. One of the models presented an extended vocabulary
with important words for each task to be solved. In addition, we applied a
decision policy based on the history of predictions that the model performs
during user evaluation. For Tasks 1 and 2, we obtained the second-best
performance according to rankings based on classification and latency,
demonstrating the effectiveness and consistency of our approaches for solving
early detection problems in the Spanish language.",None,-1
f717b78c-d351-4d76-9d92-d00369029fc4,Learning Topology-Preserving Data Representations,0.841901,"We propose a method for learning topology-preserving data representations
(dimensionality reduction). The method aims to provide topological similarity
between the data manifold and its latent representation via enforcing the
similarity in topological features (clusters, loops, 2D voids, etc.) and their
localization. The core of the method is the minimization of the Representation
Topology Divergence (RTD) between original high-dimensional data and
low-dimensional representation in latent space. RTD minimization provides
closeness in topological features with strong theoretical guarantees. We
develop a scheme for RTD differentiation and apply it as a loss term for the
autoencoder. The proposed method ""RTD-AE"" better preserves the global structure
and topology of the data manifold than state-of-the-art competitors as measured
by linear correlation, triplet distance ranking accuracy, and Wasserstein
distance between persistence barcodes.",None,-1
e3063c25-c6b9-4ece-8a9c-c1b99bef3458,How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases,0.870064,"Accurate syntactic representations are essential for robust generalization in
natural language. Recent work has found that pre-training can teach language
models to rely on hierarchical syntactic features - as opposed to incorrect
linear features - when performing tasks after fine-tuning. We test what aspects
of pre-training are important for endowing encoder-decoder Transformers with an
inductive bias that favors hierarchical syntactic generalizations. We focus on
architectural features (depth, width, and number of parameters), as well as the
genre and size of the pre-training corpus, diagnosing inductive biases using
two syntactic transformation tasks: question formation and passivization, both
in English. We find that the number of parameters alone does not explain
hierarchical generalization: model depth plays greater role than model width.
We also find that pre-training on simpler language, such as child-directed
speech, induces a hierarchical bias using an order-of-magnitude less data than
pre-training on more typical datasets based on web text or Wikipedia; this
suggests that in cognitively plausible language acquisition settings, neural
language models may be more data-efficient than previously thought.",None,-1
dd9db295-d16c-4593-8713-414864e57d63,On the Risk of Misinformation Pollution with Large Language Models,0.343358,"In this paper, we comprehensively investigate the potential misuse of modern
Large Language Models (LLMs) for generating credible-sounding misinformation
and its subsequent impact on information-intensive applications, particularly
Open-Domain Question Answering (ODQA) systems. We establish a threat model and
simulate potential misuse scenarios, both unintentional and intentional, to
assess the extent to which LLMs can be utilized to produce misinformation. Our
study reveals that LLMs can act as effective misinformation generators, leading
to a significant degradation in the performance of ODQA systems. To mitigate
the harm caused by LLM-generated misinformation, we explore three defense
strategies: prompting, misinformation detection, and majority voting. While
initial results show promising trends for these defensive strategies, much more
work needs to be done to address the challenge of misinformation pollution. Our
work highlights the need for further research and interdisciplinary
collaboration to address LLM-generated misinformation and to promote
responsible use of LLMs.",None,-1
e862aba5-a339-49ae-89b1-bcf16e391262,"Gaze-based intention estimation: principles, methodologies, and applications in HRI",0.964875,"Intention prediction has become a relevant field of research in Human-Machine
and Human-Robot Interaction. Indeed, any artificial system (co)-operating with
and along humans, designed to assist and coordinate its actions with a human
partner, would benefit from first inferring the human's current intention. To
spare the user the cognitive burden of explicitly uttering their goals, this
inference relies mostly on behavioral cues deemed indicative of the current
action. It has been long known that eye movements are highly anticipatory of
the single steps unfolding during a task, hence they can serve as a very early
and reliable behavioural cue for intention recognition. This review aims to
draw a line between insights in the psychological literature on visuomotor
control and relevant applications of gaze-based intention recognition in
technical domains, with a focus on teleoperated and assistive robotic systems.
Starting from the cognitive principles underlying the relationship between
intentions, eye movements, and action, the use of eye tracking and gaze-based
models for intent recognition in Human-Robot Interaction is considered, with
prevalent methodologies and their diverse applications. Finally, special
consideration is given to relevant human factors issues and current limitations
to be factored in when designing such systems.",None,-1
8144242d-986b-41e8-b2cb-f30515a04c8f,Weakly Supervised Reasoning by Neuro-Symbolic Approaches,0.623636,"Deep learning has largely improved the performance of various natural
language processing (NLP) tasks. However, most deep learning models are
black-box machinery, and lack explicit interpretation. In this chapter, we will
introduce our recent progress on neuro-symbolic approaches to NLP, which
combines different schools of AI, namely, symbolism and connectionism.
Generally, we will design a neural system with symbolic latent structures for
an NLP task, and apply reinforcement learning or its relaxation to perform
weakly supervised reasoning in the downstream task. Our framework has been
successfully applied to various tasks, including table query reasoning,
syntactic structure reasoning, information extraction reasoning, and rule
reasoning. For each application, we will introduce the background, our
approach, and experimental results.",None,-1
ebea1eae-4901-4744-9e60-32f410e03e55,1st Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation,0.397605,"Video panoptic segmentation is a challenging task that serves as the
cornerstone of numerous downstream applications, including video editing and
autonomous driving. We believe that the decoupling strategy proposed by DVIS
enables more effective utilization of temporal information for both ""thing"" and
""stuff"" objects. In this report, we successfully validated the effectiveness of
the decoupling strategy in video panoptic segmentation. Finally, our method
achieved a VPQ score of 51.4 and 53.7 in the development and test phases,
respectively, and ultimately ranked 1st in the VPS track of the 2nd PVUW
Challenge. The code is available at https://github.com/zhang-tao-whu/DVIS",None,-1
43766e22-6bad-47b5-867d-67d51165fe85,TagCLIP: Improving Discrimination Ability of Open-Vocabulary Semantic Segmentation,0.450764,"Recent success of Contrastive Language-Image Pre-training~(CLIP) has shown
great promise in pixel-level open-vocabulary learning tasks. A general paradigm
utilizes CLIP's text and patch embeddings to generate semantic masks. However,
existing models easily misidentify input pixels from unseen classes, thus
confusing novel classes with semantically-similar ones. In our work, we
disentangle the ill-posed optimization problem into two parallel processes: one
performs semantic matching individually, and the other judges reliability for
improving discrimination ability. Motivated by special tokens in language
modeling that represents sentence-level embeddings, we design a trusty token
that decouples the known and novel category prediction tendency. With almost no
extra overhead, we upgrade the pixel-level generalization capacity of existing
models effectively. Our TagCLIP (CLIP adapting with Trusty-guidance) boosts the
IoU of unseen classes by 7.4% and 1.7% on PASCAL VOC 2012 and COCO-Stuff 164K.",None,-1
e9ea385a-6d0f-4d50-b4ac-2764f308fa85,Question Decomposition Tree for Answering Complex Questions over Knowledge Bases,0.633472,"Knowledge base question answering (KBQA) has attracted a lot of interest in
recent years, especially for complex questions which require multiple facts to
answer. Question decomposition is a promising way to answer complex questions.
Existing decomposition methods split the question into sub-questions according
to a single compositionality type, which is not sufficient for questions
involving multiple compositionality types. In this paper, we propose Question
Decomposition Tree (QDT) to represent the structure of complex questions.
Inspired by recent advances in natural language generation (NLG), we present a
two-staged method called Clue-Decipher to generate QDT. It can leverage the
strong ability of NLG model and simultaneously preserve the original questions.
To verify that QDT can enhance KBQA task, we design a decomposition-based KBQA
system called QDTQA. Extensive experiments show that QDTQA outperforms previous
state-of-the-art methods on ComplexWebQuestions dataset. Besides, our
decomposition method improves an existing KBQA system by 12% and sets a new
state-of-the-art on LC-QuAD 1.0.",None,-1
d4fd22fc-c9db-4a1b-b557-d4932138e9b2,Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization,0.902196,"Weakly-supervised temporal action localization aims to localize and recognize
actions in untrimmed videos with only video-level category labels during
training. Without instance-level annotations, most existing methods follow the
Segment-based Multiple Instance Learning (S-MIL) framework, where the
predictions of segments are supervised by the labels of videos. However, the
objective for acquiring segment-level scores during training is not consistent
with the target for acquiring proposal-level scores during testing, leading to
suboptimal results. To deal with this problem, we propose a novel
Proposal-based Multiple Instance Learning (P-MIL) framework that directly
classifies the candidate proposals in both the training and testing stages,
which includes three key designs: 1) a surrounding contrastive feature
extraction module to suppress the discriminative short proposals by considering
the surrounding contrastive information, 2) a proposal completeness evaluation
module to inhibit the low-quality proposals with the guidance of the
completeness pseudo labels, and 3) an instance-level rank consistency loss to
achieve robust detection by leveraging the complementarity of RGB and FLOW
modalities. Extensive experimental results on two challenging benchmarks
including THUMOS14 and ActivityNet demonstrate the superior performance of our
method.",None,-1
d2642e77-eb36-4334-add5-be7b874623a0,RM-Depth: Unsupervised Learning of Recurrent Monocular Depth in Dynamic Scenes,0.922636,"Unsupervised methods have showed promising results on monocular depth
estimation. However, the training data must be captured in scenes without
moving objects. To push the envelope of accuracy, recent methods tend to
increase their model parameters. In this paper, an unsupervised learning
framework is proposed to jointly predict monocular depth and complete 3D motion
including the motions of moving objects and camera. (1) Recurrent modulation
units are used to adaptively and iteratively fuse encoder and decoder features.
This not only improves the single-image depth inference but also does not
overspend model parameters. (2) Instead of using a single set of filters for
upsampling, multiple sets of filters are devised for the residual upsampling.
This facilitates the learning of edge-preserving filters and leads to the
improved performance. (3) A warping-based network is used to estimate a motion
field of moving objects without using semantic priors. This breaks down the
requirement of scene rigidity and allows to use general videos for the
unsupervised learning. The motion field is further regularized by an
outlier-aware training loss. Despite the depth model just uses a single image
in test time and 2.97M parameters, it achieves state-of-the-art results on the
KITTI and Cityscapes benchmarks.",None,-1
a8185c3e-61e2-4bdf-b629-4a50f77b07ac,Line Graphics Digitization: A Step Towards Full Automation,0.36625,"The digitization of documents allows for wider accessibility and
reproducibility. While automatic digitization of document layout and text
content has been a long-standing focus of research, this problem in regard to
graphical elements, such as statistical plots, has been under-explored. In this
paper, we introduce the task of fine-grained visual understanding of
mathematical graphics and present the Line Graphics (LG) dataset, which
includes pixel-wise annotations of 5 coarse and 10 fine-grained categories. Our
dataset covers 520 images of mathematical graphics collected from 450 documents
from different disciplines. Our proposed dataset can support two different
computer vision tasks, i.e., semantic segmentation and object detection. To
benchmark our LG dataset, we explore 7 state-of-the-art models. To foster
further research on the digitization of statistical graphs, we will make the
dataset, code, and models publicly available to the community.",None,-1
ca35bc5c-473a-441d-ae42-0e272d7f4712,Context-aware Pedestrian Trajectory Prediction with Multimodal Transformer,0.472168,"We propose a novel solution for predicting future trajectories of
pedestrians. Our method uses a multimodal encoder-decoder transformer
architecture, which takes as input both pedestrian locations and ego-vehicle
speeds. Notably, our decoder predicts the entire future trajectory in a
single-pass and does not perform one-step-ahead prediction, which makes the
method effective for embedded edge deployment. We perform detailed experiments
and evaluate our method on two popular datasets, PIE and JAAD. Quantitative
results demonstrate the superiority of our proposed model over the current
state-of-the-art, which consistently achieves the lowest error for 3 time
horizons of 0.5, 1.0 and 1.5 seconds. Moreover, the proposed method is
significantly faster than the state-of-the-art for the two datasets of PIE and
JAAD. Lastly, ablation experiments demonstrate the impact of the key multimodal
configuration of our method.",None,-1
83adabad-0ca5-443b-8166-9267356b9419,Causal Strategic Learning with Competitive Selection,0.164071,"We study the problem of agent selection in causal strategic learning under
multiple decision makers and address two key challenges that come with it.
Firstly, while much of prior work focuses on studying a fixed pool of agents
that remains static regardless of their evaluations, we consider the impact of
selection procedure by which agents are not only evaluated, but also selected.
When each decision maker unilaterally selects agents by maximising their own
utility, we show that the optimal selection rule is a trade-off between
selecting the best agents and providing incentives to maximise the agents'
improvement. Furthermore, this optimal selection rule relies on incorrect
predictions of agents' outcomes. Hence, we study the conditions under which a
decision maker's optimal selection rule will not lead to deterioration of
agents' outcome nor cause unjust reduction in agents' selection chance. To that
end, we provide an analytical form of the optimal selection rule and a
mechanism to retrieve the causal parameters from observational data, under
certain assumptions on agents' behaviour. Secondly, when there are multiple
decision makers, the interference between selection rules introduces another
source of biases in estimating the underlying causal parameters. To address
this problem, we provide a cooperative protocol which all decision makers must
collectively adopt to recover the true causal parameters. Lastly, we complement
our theoretical results with simulation studies. Our results highlight not only
the importance of causal modeling as a strategy to mitigate the effect of
gaming, as suggested by previous work, but also the need of a benevolent
regulator to enable it.",None,-1
4e6af1a9-aca9-4855-80a3-c18a25cd732a,Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization,0.21046,"Generating persona consistent dialogue response is important for developing
an intelligent conversational agent. Recent works typically fine-tune
large-scale pre-trained models on this task by concatenating persona texts and
dialogue history as a single input sequence to generate the target response.
While simple and effective, our analysis shows that this popular practice is
seriously affected by order sensitivity where different input orders of persona
sentences significantly impact the quality and consistency of generated
response, resulting in severe performance fluctuations (i.e., 29.4% on GPT2 and
83.2% on BART). To mitigate the order sensitivity problem, we propose a
model-agnostic framework, ORder Insensitive Generation (ORIG), which enables
dialogue models to learn robust representation under different persona orders
and improve the consistency of response generation. Experiments on the
Persona-Chat dataset justify the effectiveness and superiority of our method
with two dominant pre-trained models (GPT2 and BART).",None,-1
2b7f380e-2ee8-47f0-a77c-41465d96afb0,MS-LSTM: Exploring Spatiotemporal Multiscale Representations in Video Prediction Domain,0.273963,"The drastic variation of motion in spatial and temporal dimensions makes the
video prediction task extremely challenging. Existing RNN models obtain higher
performance by deepening or widening the model. They obtain the multi-scale
features of the video only by stacking layers, which is inefficient and brings
unbearable training costs (such as memory, FLOPs, and training time). Different
from them, this paper proposes a spatiotemporal multi-scale model called
MS-LSTM wholly from a multi-scale perspective. On the basis of stacked layers,
MS-LSTM incorporates two additional efficient multi-scale designs to fully
capture spatiotemporal context information. Concretely, we employ LSTMs with
mirrored pyramid structures to construct spatial multi-scale representations
and LSTMs with different convolution kernels to construct temporal multi-scale
representations. We theoretically analyze the training cost and performance of
MS-LSTM and its components. Detailed comparison experiments with twelve
baseline models on four video datasets show that MS-LSTM has better performance
but lower training costs.",None,-1
2920539c-8d5e-4df9-9524-dd040396921c,"ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis",0.661963,"The computational analysis of poetry is limited by the scarcity of tools to
automatically analyze and scan poems. In a multilingual settings, the problem
is exacerbated as scansion and rhyme systems only exist for individual
languages, making comparative studies very challenging and time consuming. In
this work, we present \textsc{Alberti}, the first multilingual pre-trained
large language model for poetry. Through domain-specific pre-training (DSP), we
further trained multilingual BERT on a corpus of over 12 million verses from 12
languages. We evaluated its performance on two structural poetry tasks: Spanish
stanza type classification, and metrical pattern prediction for Spanish,
English and German. In both cases, \textsc{Alberti} outperforms multilingual
BERT and other transformers-based models of similar sizes, and even achieves
state-of-the-art results for German when compared to rule-based systems,
demonstrating the feasibility and effectiveness of DSP in the poetry domain.",None,-1
7dfb3f57-03f4-4b65-81ad-a24c4b96272e,ReContrast: Domain-Specific Anomaly Detection via Contrastive Reconstruction,0.461554,"Most advanced unsupervised anomaly detection (UAD) methods rely on modeling
feature representations of frozen encoder networks pre-trained on large-scale
datasets, e.g. ImageNet. However, the features extracted from the encoders that
are borrowed from natural image domains coincide little with the features
required in the target UAD domain, such as industrial inspection and medical
imaging. In this paper, we propose a novel epistemic UAD method, namely
ReContrast, which optimizes the entire network to reduce biases towards the
pre-trained image domain and orients the network in the target domain. We start
with a feature reconstruction approach that detects anomalies from errors.
Essentially, the elements of contrastive learning are elegantly embedded in
feature reconstruction to prevent the network from training instability,
pattern collapse, and identical shortcut, while simultaneously optimizing both
the encoder and decoder on the target domain. To demonstrate our transfer
ability on various image domains, we conduct extensive experiments across two
popular industrial defect detection benchmarks and three medical image UAD
tasks, which shows our superiority over current state-of-the-art methods.",None,-1
4aa997b5-7977-4e36-92af-1cfa978790b9,Contrastive Loss is All You Need to Recover Analogies as Parallel Lines,0.352717,"While static word embedding models are known to represent linguistic
analogies as parallel lines in high-dimensional space, the underlying mechanism
as to why they result in such geometric structures remains obscure. We find
that an elementary contrastive-style method employed over distributional
information performs competitively with popular word embedding models on
analogy recovery tasks, while achieving dramatic speedups in training time.
Further, we demonstrate that a contrastive loss is sufficient to create these
parallel structures in word embeddings, and establish a precise relationship
between the co-occurrence statistics and the geometric structure of the
resulting word embeddings.",None,-1
a90e7561-52e0-4713-9ede-e48267cfc457,Time Series Analysis of Urban Liveability,0.148162,"In this paper we explore deep learning models to monitor longitudinal
liveability changes in Dutch cities at the neighbourhood level. Our liveability
reference data is defined by a country-wise yearly survey based on a set of
indicators combined into a liveability score, the Leefbaarometer. We pair this
reference data with yearly-available high-resolution aerial images, which
creates yearly timesteps at which liveability can be monitored. We deploy a
convolutional neural network trained on an aerial image from 2016 and the
Leefbaarometer score to predict liveability at new timesteps 2012 and 2020. The
results in a city used for training (Amsterdam) and one never seen during
training (Eindhoven) show some trends which are difficult to interpret,
especially in light of the differences in image acquisitions at the different
time steps. This demonstrates the complexity of liveability monitoring across
time periods and the necessity for more sophisticated methods compensating for
changes unrelated to liveability dynamics.",None,-1
12c201ed-6ee8-441a-9331-6d9eaa070f4e,Explaining Legal Concepts with Augmented Large Language Models (GPT-4),0.999837,"Interpreting the meaning of legal open-textured terms is a key task of legal
professionals. An important source for this interpretation is how the term was
applied in previous court cases. In this paper, we evaluate the performance of
GPT-4 in generating factually accurate, clear and relevant explanations of
terms in legislation. We compare the performance of a baseline setup, where
GPT-4 is directly asked to explain a legal term, to an augmented approach,
where a legal information retrieval module is used to provide relevant context
to the model, in the form of sentences from case law. We found that the direct
application of GPT-4 yields explanations that appear to be of very high quality
on their surface. However, detailed analysis uncovered limitations in terms of
the factual accuracy of the explanations. Further, we found that the
augmentation leads to improved quality, and appears to eliminate the issue of
hallucination, where models invent incorrect statements. These findings open
the door to the building of systems that can autonomously retrieve relevant
sentences from case law and condense them into a useful explanation for legal
scholars, educators or practicing lawyers alike.",None,-1
875e91c1-b608-476a-a920-811f86c97fd5,HSCNet++: Hierarchical Scene Coordinate Classification and Regression for Visual Localization with Transformer,0.492002,"Visual localization is critical to many applications in computer vision and
robotics. To address single-image RGB localization, state-of-the-art
feature-based methods match local descriptors between a query image and a
pre-built 3D model. Recently, deep neural networks have been exploited to
regress the mapping between raw pixels and 3D coordinates in the scene, and
thus the matching is implicitly performed by the forward pass through the
network. However, in a large and ambiguous environment, learning such a
regression task directly can be difficult for a single network. In this work,
we present a new hierarchical scene coordinate network to predict pixel scene
coordinates in a coarse-to-fine manner from a single RGB image. The proposed
method, which is an extension of HSCNet, allows us to train compact models
which scale robustly to large environments. It sets a new state-of-the-art for
single-image localization on the 7-Scenes, 12 Scenes, Cambridge Landmarks
datasets, and the combined indoor scenes.",None,-1
865403a3-b1c2-4b2f-bc3e-b9255536663e,Deep Axial Hypercomplex Networks,0.0304891,"Over the past decade, deep hypercomplex-inspired networks have enhanced
feature extraction for image classification by enabling weight sharing across
input channels. Recent works make it possible to improve representational
capabilities by using hypercomplex-inspired networks which consume high
computational costs. This paper reduces this cost by factorizing a quaternion
2D convolutional module into two consecutive vectormap 1D convolutional
modules. Also, we use 5D parameterized hypercomplex multiplication based fully
connected layers. Incorporating both yields our proposed hypercomplex network,
a novel architecture that can be assembled to construct deep axial-hypercomplex
networks (DANs) for image classifications. We conduct experiments on CIFAR
benchmarks, SVHN, and Tiny ImageNet datasets and achieve better performance
with fewer trainable parameters and FLOPS. Our proposed model achieves almost
2% higher performance for CIFAR and SVHN datasets, and more than 3% for the
ImageNet-Tiny dataset and takes six times fewer parameters than the real-valued
ResNets. Also, it shows state-of-the-art performance on CIFAR benchmarks in
hypercomplex space.",None,-1
b2bdf73d-8d94-4c7d-976a-becee14423a3,Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging,0.944483,"While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language
Models (LLMs) with general, aggregate human preferences, it is suboptimal for
learning diverse, individual perspectives. In this work, we study Reinforcement
Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are
aligned to multiple (sometimes conflicting) preferences by modeling alignment
as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong
single-objective baselines, we show that we can achieve personalized alignment
by decomposing preferences into multiple dimensions. These dimensions are
defined based on personalizations that are declared as desirable by the user.
In this work, we show that they can be efficiently trained independently in a
distributed manner and combined effectively post-hoc through parameter merging.
The code is available at https://github.com/joeljang/RLPHF.",None,-1
c5d98113-9008-418e-9b99-dcdb36f558db,Effective Real Image Editing with Accelerated Iterative Diffusion Inversion,0.737315,"Despite all recent progress, it is still challenging to edit and manipulate
natural images with modern generative models. When using Generative Adversarial
Network (GAN), one major hurdle is in the inversion process mapping a real
image to its corresponding noise vector in the latent space, since its
necessary to be able to reconstruct an image to edit its contents. Likewise for
Denoising Diffusion Implicit Models (DDIM), the linearization assumption in
each inversion step makes the whole deterministic inversion process unreliable.
Existing approaches that have tackled the problem of inversion stability often
incur in significant trade-offs in computational efficiency. In this work we
propose an Accelerated Iterative Diffusion Inversion method, dubbed AIDI, that
significantly improves reconstruction accuracy with minimal additional overhead
in space and time complexity. By using a novel blended guidance technique, we
show that effective results can be obtained on a large range of image editing
tasks without large classifier-free guidance in inversion. Furthermore, when
compared with other diffusion inversion based works, our proposed process is
shown to be more robust for fast image editing in the 10 and 20 diffusion
steps' regimes.",None,-1
e5b1dc67-78c2-4895-905f-0c5734800ca2,Positive Label Is All You Need for Multi-Label Classification,0.416435,"Multi-label classification (MLC) faces challenges from label noise in
training data due to annotating diverse semantic labels for each image. Current
methods mainly target identifying and correcting label mistakes using trained
MLC models, but still struggle with persistent noisy labels during training,
resulting in imprecise recognition and reduced performance. Our paper addresses
label noise in MLC by introducing a positive and unlabeled multi-label
classification (PU-MLC) method. To counteract noisy labels, we directly discard
negative labels, focusing on the abundance of negative labels and the origin of
most noisy labels. PU-MLC employs positive-unlabeled learning, training the
model with only positive labels and unlabeled data. The method incorporates
adaptive re-balance factors and temperature coefficients in the loss function
to address label distribution imbalance and prevent over-smoothing of
probabilities during training. Additionally, we introduce a local-global
convolution module to capture both local and global dependencies in the image
without requiring backbone retraining. PU-MLC proves effective on MLC and MLC
with partial labels (MLC-PL) tasks, demonstrating significant improvements on
MS-COCO and PASCAL VOC datasets with fewer annotations. Code is available at:
https://github.com/TAKELAMAG/PU-MLC.",None,-1
7601a6df-85a9-4339-a24d-ec98d7bde41b,Emotion-Aware Prosodic Phrasing for Expressive Text-to-Speech,0.290818,"Prosodic phrasing is crucial to the naturalness and intelligibility of
end-to-end Text-to-Speech (TTS). There exist both linguistic and emotional
prosody in natural speech. As the study of prosodic phrasing has been
linguistically motivated, prosodic phrasing for expressive emotion rendering
has not been well studied. In this paper, we propose an emotion-aware prosodic
phrasing model, termed \textit{EmoPP}, to mine the emotional cues of utterance
accurately and predict appropriate phrase breaks. We first conduct objective
observations on the ESD dataset to validate the strong correlation between
emotion and prosodic phrasing. Then the objective and subjective evaluations
show that the EmoPP outperforms all baselines and achieves remarkable
performance in terms of emotion expressiveness. The audio samples and the code
are available at \url{https://github.com/AI-S2-Lab/EmoPP}.",None,-1
56917941-094a-4cdc-a5aa-7d05ce1e8021,On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion,0.747131,"Generalizing deep learning models to unknown target domain distribution with
low latency has motivated research into test-time training/adaptation
(TTT/TTA). Existing approaches often focus on improving test-time training
performance under well-curated target domain data. As figured out in this work,
many state-of-the-art methods fail to maintain the performance when the target
domain is contaminated with strong out-of-distribution (OOD) data, a.k.a.
open-world test-time training (OWTTT). The failure is mainly due to the
inability to distinguish strong OOD samples from regular weak OOD samples. To
improve the robustness of OWTTT we first develop an adaptive strong OOD pruning
which improves the efficacy of the self-training TTT method. We further propose
a way to dynamically expand the prototypes to represent strong OOD samples for
an improved weak/strong OOD data separation. Finally, we regularize
self-training with distribution alignment and the combination yields the
state-of-the-art performance on 5 OWTTT benchmarks. The code is available at
https://github.com/Yushu-Li/OWTTT.",None,-1
c97a808b-f669-452c-b66c-d701cde68706,Renate: A Library for Real-World Continual Learning,0.15517,"Continual learning enables the incremental training of machine learning
models on non-stationary data streams.While academic interest in the topic is
high, there is little indication of the use of state-of-the-art continual
learning algorithms in practical machine learning deployment. This paper
presents Renate, a continual learning library designed to build real-world
updating pipelines for PyTorch models. We discuss requirements for the use of
continual learning algorithms in practice, from which we derive design
principles for Renate. We give a high-level description of the library
components and interfaces. Finally, we showcase the strengths of the library by
presenting experimental results. Renate may be found at
https://github.com/awslabs/renate.",None,-1
4c5c48ab-aad2-4d80-8c22-60280633b8d8,Future Lens: Anticipating Subsequent Tokens from a Single Hidden State,0.901025,"We conjecture that hidden state vectors corresponding to individual input
tokens encode information sufficient to accurately predict several tokens
ahead. More concretely, in this paper we ask: Given a hidden (internal)
representation of a single token at position $t$ in an input, can we reliably
anticipate the tokens that will appear at positions $\geq t + 2$? To test this,
we measure linear approximation and causal intervention methods in GPT-J-6B to
evaluate the degree to which individual hidden states in the network contain
signal rich enough to predict future hidden states and, ultimately, token
outputs. We find that, at some layers, we can approximate a model's output with
more than 48% accuracy with respect to its prediction of subsequent tokens
through a single hidden state. Finally we present a ""Future Lens"" visualization
that uses these methods to create a new view of transformer states.",None,-1
20867284-79d8-4a82-8a5b-4eb275247433,Coverage-based Example Selection for In-Context Learning,0.446988,"In-context learning (ICL), the ability of large language models to perform
novel tasks by conditioning on a prompt with a few task examples, requires
these examples to be informative about the test instance. The standard approach
of independently ranking and selecting the most similar examples selects
redundant examples while omitting important information. In this work, we show
that BERTScore-Recall (BSR) selects better examples that demonstrate more of
the salient aspects, e.g. reasoning patterns, of the test input. We further
extend BSR and many standard metrics to easily optimizable set-level metrics,
giving still better coverage of those salient aspects. On 15 datasets spanning
6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric
for in-context example selection across the board, and (2) for compositional
tasks, set selection using Set-BSR outperforms independent ranking by up to 17
points on average and, despite being training-free, surpasses methods that
leverage task or LLM-specific training.",None,-1
f96f1af9-4cce-4d3d-8736-b8e9c84eba4b,Hierarchical Vision Transformers for Cardiac Ejection Fraction Estimation,0.833826,"The left ventricular of ejection fraction is one of the most important metric
of cardiac function. It is used by cardiologist to identify patients who are
eligible for lifeprolonging therapies. However, the assessment of ejection
fraction suffers from inter-observer variability. To overcome this challenge,
we propose a deep learning approach, based on hierarchical vision Transformers,
to estimate the ejection fraction from echocardiogram videos. The proposed
method can estimate ejection fraction without the need for left ventrice
segmentation first, make it more efficient than other methods. We evaluated our
method on EchoNet-Dynamic dataset resulting 5.59, 7.59 and 0.59 for MAE, RMSE
and R2 respectivelly. This results are better compared to the state-of-the-art
method, Ultrasound Video Transformer (UVT). The source code is available on
https://github.com/lhfazry/UltraSwin.",None,-1
096d9f96-d376-474c-bc11-0597f620adbb,HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation,0.841917,"Panoptic Scene Graph generation (PSG) is a recently proposed task in image
scene understanding that aims to segment the image and extract triplets of
subjects, objects and their relations to build a scene graph. This task is
particularly challenging for two reasons. First, it suffers from a long-tail
problem in its relation categories, making naive biased methods more inclined
to high-frequency relations. Existing unbiased methods tackle the long-tail
problem by data/loss rebalancing to favor low-frequency relations. Second, a
subject-object pair can have two or more semantically overlapping relations.
While existing methods favor one over the other, our proposed HiLo framework
lets different network branches specialize on low and high frequency relations,
enforce their consistency and fuse the results. To the best of our knowledge we
are the first to propose an explicitly unbiased PSG method. In extensive
experiments we show that our HiLo framework achieves state-of-the-art results
on the PSG task. We also apply our method to the Scene Graph Generation task
that predicts boxes instead of masks and see improvements over all baseline
methods. Code is available at https://github.com/franciszzj/HiLo.",None,-1
07c0ed2c-2443-4585-80a7-8ea0d628fc31,Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation,0.682554,"The CLIP model has been recently proven to be very effective for a variety of
cross-modal tasks, including the evaluation of captions generated from
vision-and-language architectures. In this paper, we propose a new recipe for a
contrastive-based evaluation metric for image captioning, namely
Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way
unifies the learning of a contrastive visual-semantic space with the addition
of generated images and text on curated data. Experiments spanning several
datasets demonstrate that our new metric achieves the highest correlation with
human judgments on both images and videos, outperforming existing
reference-based metrics like CIDEr and SPICE and reference-free metrics like
CLIP-Score. Finally, we test the system-level correlation of the proposed
metric when considering popular image captioning approaches, and assess the
impact of employing different cross-modal features. Our source code and trained
models are publicly available at: https://github.com/aimagelab/pacscore.",None,-1
e3aa484e-1727-4656-9a82-2f82bc7f6d59,Loss-Curvature Matching for Dataset Selection and Condensation,0.551799,"Training neural networks on a large dataset requires substantial
computational costs. Dataset reduction selects or synthesizes data instances
based on the large dataset, while minimizing the degradation in generalization
performance from the full dataset. Existing methods utilize the neural network
during the dataset reduction procedure, so the model parameter becomes
important factor in preserving the performance after reduction. By depending
upon the importance of parameters, this paper introduces a new reduction
objective, coined LCMat, which Matches the Loss Curvatures of the original
dataset and reduced dataset over the model parameter space, more than the
parameter point. This new objective induces a better adaptation of the reduced
dataset on the perturbed parameter region than the exact point matching.
Particularly, we identify the worst case of the loss curvature gap from the
local parameter region, and we derive the implementable upper bound of such
worst-case with theoretical analyses. Our experiments on both coreset selection
and condensation benchmarks illustrate that LCMat shows better generalization
performances than existing baselines.",None,-1
8a253456-07ff-4896-8b56-d06a594bda53,Learning Optimal Policy for Simultaneous Machine Translation via Binary Search,0.699964,"Simultaneous machine translation (SiMT) starts to output translation while
reading the source sentence and needs a precise policy to decide when to output
the generated translation. Therefore, the policy determines the number of
source tokens read during the translation of each target token. However, it is
difficult to learn a precise translation policy to achieve good latency-quality
trade-offs, because there is no golden policy corresponding to parallel
sentences as explicit supervision. In this paper, we present a new method for
constructing the optimal policy online via binary search. By employing explicit
supervision, our approach enables the SiMT model to learn the optimal policy,
which can guide the model in completing the translation during inference.
Experiments on four translation tasks show that our method can exceed strong
baselines across all latency scenarios.",None,-1
ff502610-eaf8-43a4-bc39-33c8a0dab485,Cooperative Open-ended Learning Framework for Zero-shot Coordination,0.735513,"Zero-shot coordination in cooperative artificial intelligence (AI) remains a
significant challenge, which means effectively coordinating with a wide range
of unseen partners. Previous algorithms have attempted to address this
challenge by optimizing fixed objectives within a population to improve
strategy or behaviour diversity. However, these approaches can result in a loss
of learning and an inability to cooperate with certain strategies within the
population, known as cooperative incompatibility. To address this issue, we
propose the Cooperative Open-ended LEarning (COLE) framework, which constructs
open-ended objectives in cooperative games with two players from the
perspective of graph theory to assess and identify the cooperative ability of
each strategy. We further specify the framework and propose a practical
algorithm that leverages knowledge from game theory and graph theory.
Furthermore, an analysis of the learning process of the algorithm shows that it
can efficiently overcome cooperative incompatibility. The experimental results
in the Overcooked game environment demonstrate that our method outperforms
current state-of-the-art methods when coordinating with different-level
partners. Our demo is available at https://sites.google.com/view/cole-2023.",None,-1
bf11a7bc-9fff-4dab-bf38-1e765cc86efd,MedMine: Examining Pre-trained Language Models on Medication Mining,0.474115,"Automatic medication mining from clinical and biomedical text has become a
popular topic due to its real impact on healthcare applications and the recent
development of powerful language models (LMs). However, fully-automatic
extraction models still face obstacles to be overcome such that they can be
deployed directly into clinical practice for better impacts. Such obstacles
include their imbalanced performances on different entity types and clinical
events. In this work, we examine current state-of-the-art pre-trained language
models (PLMs) on such tasks, via fine-tuning including the monolingual model
Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their
advantages and drawbacks using historical medication mining shared task data
sets from n2c2-2018 challenges. We report the findings we get from these
fine-tuning experiments such that they can facilitate future research on
addressing them, for instance, how to combine their outputs, merge such models,
or improve their overall accuracy by ensemble learning and data augmentation.
MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}",None,-1
073a5ae1-42ba-466e-b768-4ac004f3bd36,Koala: An Index for Quantifying Overlaps with Pre-training Corpora,0.614354,"In very recent years more attention has been placed on probing the role of
pre-training data in Large Language Models (LLMs) downstream behaviour. Despite
the importance, there is no public tool that supports such analysis of
pre-training corpora at large scale. To help research in this space, we launch
Koala, a searchable index over large pre-training corpora using compressed
suffix arrays with highly efficient compression rate and search support. In its
first release we index the public proportion of OPT 175B pre-training data.
Koala provides a framework to do forensic analysis on the current and future
benchmarks as well as to assess the degree of memorization in the output from
the LLMs. Koala is available for public use at
https://koala-index.erc.monash.edu/.",None,-1
72f43b3b-fbd4-4144-a468-7d84886aa42f,Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution,0.916413,"Although numerous solutions have been proposed for image super-resolution,
they are usually incompatible with low-power devices with many computational
and memory constraints. In this paper, we address this problem by proposing a
simple yet effective deep network to solve image super-resolution efficiently.
In detail, we develop a spatially-adaptive feature modulation (SAFM) mechanism
upon a vision transformer (ViT)-like block. Within it, we first apply the SAFM
block over input features to dynamically select representative feature
representations. As the SAFM block processes the input features from a
long-range perspective, we further introduce a convolutional channel mixer
(CCM) to simultaneously extract local contextual information and perform
channel mixing. Extensive experimental results show that the proposed method is
$3\times$ smaller than state-of-the-art efficient SR methods, e.g., IMDN, in
terms of the network parameters and requires less computational cost while
achieving comparable performance. The code is available at
https://github.com/sunny2109/SAFMN.",None,-1
72f74f85-f119-4a3f-833d-6c88f1acf276,"MLTEing Models: Negotiating, Evaluating, and Documenting Model and System Qualities",0.317386,"Many organizations seek to ensure that machine learning (ML) and artificial
intelligence (AI) systems work as intended in production but currently do not
have a cohesive methodology in place to do so. To fill this gap, we propose
MLTE (Machine Learning Test and Evaluation, colloquially referred to as
""melt""), a framework and implementation to evaluate ML models and systems. The
framework compiles state-of-the-art evaluation techniques into an
organizational process for interdisciplinary teams, including model developers,
software engineers, system owners, and other stakeholders. MLTE tooling
supports this process by providing a domain-specific language that teams can
use to express model requirements, an infrastructure to define, generate, and
collect ML evaluation metrics, and the means to communicate results.",None,-1
85d1bbb5-5d4a-40b2-b701-114a67e2b0d6,Control Risk for Potential Misuse of Artificial Intelligence in Science,0.885085,"The expanding application of Artificial Intelligence (AI) in scientific
fields presents unprecedented opportunities for discovery and innovation.
However, this growth is not without risks. AI models in science, if misused,
can amplify risks like creation of harmful substances, or circumvention of
established regulations. In this study, we aim to raise awareness of the
dangers of AI misuse in science, and call for responsible AI development and
use in this domain. We first itemize the risks posed by AI in scientific
contexts, then demonstrate the risks by highlighting real-world examples of
misuse in chemical science. These instances underscore the need for effective
risk management strategies. In response, we propose a system called SciGuard to
control misuse risks for AI models in science. We also propose a red-teaming
benchmark SciMT-Safety to assess the safety of different systems. Our proposed
SciGuard shows the least harmful impact in the assessment without compromising
performance in benign tests. Finally, we highlight the need for a
multidisciplinary and collaborative effort to ensure the safe and ethical use
of AI models in science. We hope that our study can spark productive
discussions on using AI ethically in science among researchers, practitioners,
policymakers, and the public, to maximize benefits and minimize the risks of
misuse.",None,-1
ddc1be83-ab4d-48ff-b9e9-0a6f9be9f7f1,Linguistic representations for fewer-shot relation extraction across domains,0.693802,"Recent work has demonstrated the positive impact of incorporating linguistic
representations as additional context and scaffolding on the in-domain
performance of several NLP tasks. We extend this work by exploring the impact
of linguistic representations on cross-domain performance in a few-shot
transfer setting. An important question is whether linguistic representations
enhance generalizability by providing features that function as cross-domain
pivots. We focus on the task of relation extraction on three datasets of
procedural text in two domains, cooking and materials science. Our approach
augments a popular transformer-based architecture by alternately incorporating
syntactic and semantic graphs constructed by freely available off-the-shelf
tools. We examine their utility for enhancing generalization, and investigate
whether earlier findings, e.g. that semantic representations can be more
helpful than syntactic ones, extend to relation extraction in multiple domains.
We find that while the inclusion of these graphs results in significantly
higher performance in few-shot transfer, both types of graph exhibit roughly
equivalent utility.",None,-1
7d0c9fcd-64ac-413b-8e53-acbfa3596455,Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction,0.453106,"The task of multimodal relation extraction has attracted significant research
attention, but progress is constrained by the scarcity of available training
data. One natural thought is to extend existing datasets with cross-modal
generative models. In this paper, we consider a novel problem setting, where
only unimodal data, either text or image, are available during training. We aim
to train a multimodal classifier from synthetic data that perform well on real
multimodal test data. However, training with synthetic data suffers from two
obstacles: lack of data diversity and label information loss. To alleviate the
issues, we propose Mutual Information-aware Multimodal Iterated Relational dAta
GEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to
promote diversity in the generated data and exploits a teacher network to
select valuable training samples with high mutual information with the
ground-truth labels. Comparing our method to direct training on synthetic data,
we observed a significant improvement of 24.06% F1 with synthetic text and
26.42% F1 with synthetic images. Notably, our best model trained on completely
synthetic images outperforms prior state-of-the-art models trained on real
multimodal data by a margin of 3.76% in F1. Our codebase will be made available
upon acceptance.",None,-1
dfba119c-ec54-43d3-b48a-e135647638dc,PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs,0.800289,"Large language models (LLMs) have shown great abilities of solving various
natural language tasks in different domains. Due to the training objective of
LLMs and their pre-training data, LLMs are not very well equipped for tasks
involving structured data generation. We propose a framework, Prompting with
Iterative Verification (PiVe), to improve graph-based generative capability of
LLMs. We show how a small language model could be trained to act as a verifier
module for the output of an LLM~(i.e., ChatGPT, GPT-4), and to iteratively
improve its performance via fine-grained corrective instructions. We also show
how the verifier module could apply iterative corrections offline for a more
cost-effective solution to the text-to-graph generation task. Experiments on
three graph-based datasets show consistent improvement gained via PiVe.
Additionally, we create GenWiki-HIQ and highlight that the verifier module can
be used as a data augmentation tool to help improve the quality of
automatically generated parallel text-graph datasets.",None,-1
72adae1e-ea52-4b81-b702-0a1155d996fc,Multi-Agent Deep Reinforcement Learning for Dynamic Avatar Migration in AIoT-enabled Vehicular Metaverses with Trajectory Prediction,0.968543,"Avatars, as promising digital assistants in Vehicular Metaverses, can enable
drivers and passengers to immerse in 3D virtual spaces, serving as a practical
emerging example of Artificial Intelligence of Things (AIoT) in intelligent
vehicular environments. The immersive experience is achieved through seamless
human-avatar interaction, e.g., augmented reality navigation, which requires
intensive resources that are inefficient and impractical to process on
intelligent vehicles locally. Fortunately, offloading avatar tasks to RoadSide
Units (RSUs) or cloud servers for remote execution can effectively reduce
resource consumption. However, the high mobility of vehicles, the dynamic
workload of RSUs, and the heterogeneity of RSUs pose novel challenges to making
avatar migration decisions. To address these challenges, in this paper, we
propose a dynamic migration framework for avatar tasks based on real-time
trajectory prediction and Multi-Agent Deep Reinforcement Learning (MADRL).
Specifically, we propose a model to predict the future trajectories of
intelligent vehicles based on their historical data, indicating the future
workloads of RSUs.Based on the expected workloads of RSUs, we formulate the
avatar task migration problem as a long-term mixed integer programming problem.
To tackle this problem efficiently, the problem is transformed into a Partially
Observable Markov Decision Process (POMDP) and solved by multiple DRL agents
with hybrid continuous and discrete actions in decentralized. Numerical results
demonstrate that our proposed algorithm can effectively reduce the latency of
executing avatar tasks by around 25% without prediction and 30% with prediction
and enhance user immersive experiences in the AIoT-enabled Vehicular Metaverse
(AeVeM).",None,-1
d9c13c9f-ca1f-4c57-b8be-91bdda41e2a7,Interpolating between Images with Diffusion Models,0.49128,"One little-explored frontier of image generation and editing is the task of
interpolating between two input images, a feature missing from all currently
deployed image generation pipelines. We argue that such a feature can expand
the creative applications of such models, and propose a method for zero-shot
interpolation using latent diffusion models. We apply interpolation in the
latent space at a sequence of decreasing noise levels, then perform denoising
conditioned on interpolated text embeddings derived from textual inversion and
(optionally) subject poses. For greater consistency, or to specify additional
criteria, we can generate several candidates and use CLIP to select the highest
quality image. We obtain convincing interpolations across diverse subject
poses, image styles, and image content, and show that standard quantitative
metrics such as FID are insufficient to measure the quality of an
interpolation. Code and data are available at
https://clintonjwang.github.io/interpolation.",None,-1
456bfb4a-c22e-4a22-b6fa-bb8f58a9dbab,Collage Diffusion,0.224005,"We seek to give users precise control over diffusion-based image generation
by modeling complex scenes as sequences of layers, which define the desired
spatial arrangement and visual attributes of objects in the scene. Collage
Diffusion harmonizes the input layers to make objects fit together -- the key
challenge involves minimizing changes in the positions and key visual
attributes of the input layers while allowing other attributes to change in the
harmonization process. We ensure that objects are generated in the correct
locations by modifying text-image cross-attention with the layers' alpha masks.
We preserve key visual attributes of input layers by learning specialized text
representations per layer and by extending ControlNet to operate on layers.
Layer input allows users to control the extent of image harmonization on a
per-object basis, and users can even iteratively edit individual objects in
generated images while keeping other objects fixed. By leveraging the rich
information present in layer input, Collage Diffusion generates globally
harmonized images that maintain desired object characteristics better than
prior approaches.",None,-1
15e6ff3c-314d-44ea-8a2e-ee987a9a8722,SIMMF: Semantics-aware Interactive Multiagent Motion Forecasting for Autonomous Vehicle Driving,0.105839,"Autonomous vehicles require motion forecasting of their surrounding
multiagents (pedestrians and vehicles) to make optimal decisions for
navigation. The existing methods focus on techniques to utilize the positions
and velocities of these agents and fail to capture semantic information from
the scene. Moreover, to mitigate the increase in computational complexity
associated with the number of agents in the scene, some works leverage
Euclidean distance to prune far-away agents. However, distance-based metric
alone is insufficient to select relevant agents and accurately perform their
predictions. To resolve these issues, we propose the Semantics-aware
Interactive Multiagent Motion Forecasting (SIMMF) method to capture semantics
along with spatial information and optimally select relevant agents for motion
prediction. Specifically, we achieve this by implementing a semantic-aware
selection of relevant agents from the scene and passing them through an
attention mechanism to extract global encodings. These encodings along with
agents' local information, are passed through an encoder to obtain
time-dependent latent variables for a motion policy predicting the future
trajectories. Our results show that the proposed approach outperforms
state-of-the-art baselines and provides more accurate and scene-consistent
predictions.",None,-1
791a0c05-f79d-46f0-867b-efa4c0e6c445,RGB-T Multi-Modal Crowd Counting Based on Transformer,0.568092,"Crowd counting aims to estimate the number of persons in a scene. Most
state-of-the-art crowd counting methods based on color images can't work well
in poor illumination conditions due to invisible objects. With the widespread
use of infrared cameras, crowd counting based on color and thermal images is
studied. Existing methods only achieve multi-modal fusion without count
objective constraint. To better excavate multi-modal information, we use
count-guided multi-modal fusion and modal-guided count enhancement to achieve
the impressive performance. The proposed count-guided multi-modal fusion module
utilizes a multi-scale token transformer to interact two-modal information
under the guidance of count information and perceive different scales from the
token perspective. The proposed modal-guided count enhancement module employs
multi-scale deformable transformer decoder structure to enhance one modality
feature and count information by the other modality. Experiment in public
RGBT-CC dataset shows that our method refreshes the state-of-the-art results.
https://github.com/liuzywen/RGBTCC",None,-1
4650d7d1-26b9-48a2-beec-be42e4bdf164,Assessing the Reliability of Large Language Model Knowledge,0.158461,"Large language models (LLMs) have been treated as knowledge bases due to
their strong performance in knowledge probing tasks. LLMs are typically
evaluated using accuracy, yet this metric does not capture the vulnerability of
LLMs to hallucination-inducing factors like prompt and context variability. How
do we evaluate the capabilities of LLMs to consistently produce factually
correct answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe
(MONITOR), a novel metric designed to directly measure LLMs' factual
reliability. MONITOR computes the distance between the probability
distributions of a valid output and its counterparts produced by the same LLM
probing the same fact using different styles of prompts and
contexts.Experiments on a comprehensive range of 12 LLMs demonstrate the
effectiveness of MONITOR in evaluating the factual reliability of LLMs while
maintaining a low computational overhead. In addition, we release the FKTC
(Factual Knowledge Test Corpus) test set, containing 210,158 prompts in total
to foster research along this line (https://github.com/Vicky-Wil/MONITOR).",None,-1
1f3fe5b6-3c3f-4e44-bf66-c45f8a79550c,Market-Aware Models for Efficient Cross-Market Recommendation,0.245433,"We consider the cross-market recommendation (CMR) task, which involves
recommendation in a low-resource target market using data from a richer,
auxiliary source market. Prior work in CMR utilised meta-learning to improve
recommendation performance in target markets; meta-learning however can be
complex and resource intensive. In this paper, we propose market-aware (MA)
models, which directly model a market via market embeddings instead of
meta-learning across markets. These embeddings transform item representations
into market-specific representations. Our experiments highlight the
effectiveness and efficiency of MA models both in a pairwise setting with a
single target-source market, as well as a global model trained on all markets
in unison. In the former pairwise setting, MA models on average outperform
market-unaware models in 85% of cases on nDCG@10, while being time-efficient -
compared to meta-learning models, MA models require only 15% of the training
time. In the global setting, MA models outperform market-unaware models
consistently for some markets, while outperforming meta-learning-based methods
for all but one market. We conclude that MA models are an efficient and
effective alternative to meta-learning, especially in the global setting.",None,-1
cf439585-9193-4bd4-9859-a46dddbc61ac,A Wide Evaluation of ChatGPT on Affective Computing Tasks,0.832693,"With the rise of foundation models, a new artificial intelligence paradigm
has emerged, by simply using general purpose foundation models with prompting
to solve problems instead of training a separate machine learning model for
each problem. Such models have been shown to have emergent properties of
solving problems that they were not initially trained on. The studies for the
effectiveness of such models are still quite limited. In this work, we widely
study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13
affective computing problems, namely aspect extraction, aspect polarity
classification, opinion extraction, sentiment analysis, sentiment intensity
ranking, emotions intensity ranking, suicide tendency detection, toxicity
detection, well-being assessment, engagement measurement, personality
assessment, sarcasm detection, and subjectivity detection. We introduce a
framework to evaluate the ChatGPT models on regression-based problems, such as
intensity ranking problems, by modelling them as pairwise ranking
classification. We compare ChatGPT against more traditional NLP methods, such
as end-to-end recurrent neural networks and transformers. The results
demonstrate the emergent abilities of the ChatGPT models on a wide range of
affective computing problems, where GPT-3.5 and especially GPT-4 have shown
strong performance on many problems, particularly the ones related to
sentiment, emotions, or toxicity. The ChatGPT models fell short for problems
with implicit signals, such as engagement measurement and subjectivity
detection.",None,-1
2a8c6176-c63f-4c64-93d2-c70126f37c08,Scaling Laws of RoPE-based Extrapolation,0.999813,"The extrapolation capability of Large Language Models (LLMs) based on Rotary
Position Embedding is currently a topic of considerable interest. The
mainstream approach to addressing extrapolation with LLMs involves modifying
RoPE by replacing 10000, the rotary base of $\theta_n={10000}^{-2n/d}$ in the
original RoPE, with a larger value and providing longer fine-tuning text. In
this work, we first observe that fine-tuning a RoPE-based LLM with either a
smaller or larger base in pre-training context length could significantly
enhance its extrapolation performance. After that, we propose
\textbf{\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework
from the periodic perspective, to describe the relationship between the
extrapolation performance and base value as well as tuning context length. In
this process, we also explain the origin of the RoPE-based extrapolation issue
by \textbf{\textit{critical dimension for extrapolation}}. Besides these
observations and analyses, we achieve extrapolation up to 1 million context
length within only 16K training length on LLaMA2 7B and 13B.",None,-1
8c62d505-f8db-488b-8587-241f89bd7276,SwinFSR: Stereo Image Super-Resolution using SwinIR and Frequency Domain Knowledge,0.542926,"Stereo Image Super-Resolution (stereoSR) has attracted significant attention
in recent years due to the extensive deployment of dual cameras in mobile
phones, autonomous vehicles and robots. In this work, we propose a new StereoSR
method, named SwinFSR, based on an extension of SwinIR, originally designed for
single image restoration, and the frequency domain knowledge obtained by the
Fast Fourier Convolution (FFC). Specifically, to effectively gather global
information, we modify the Residual Swin Transformer blocks (RSTBs) in SwinIR
by explicitly incorporating the frequency domain knowledge using the FFC and
employing the resulting residual Swin Fourier Transformer blocks (RSFTBs) for
feature extraction. Besides, for the efficient and accurate fusion of stereo
views, we propose a new cross-attention module referred to as RCAM, which
achieves highly competitive performance while requiring less computational cost
than the state-of-the-art cross-attention modules. Extensive experimental
results and ablation studies demonstrate the effectiveness and efficiency of
our proposed SwinFSR.",None,-1
cd87a057-8708-4e99-9369-185ffac1cc58,CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation,0.916096,"Recent code translation techniques exploit neural machine translation models
to translate source code from one programming language to another to satisfy
production compatibility or to improve efficiency of codebase maintenance. Most
existing code translation datasets only focus on a single pair of popular
programming languages. To advance research on code translation and meet diverse
requirements of real-world applications, we construct CodeTransOcean, a
large-scale comprehensive benchmark that supports the largest variety of
programming languages for code translation. CodeTransOcean consists of three
novel multilingual datasets, namely, MultilingualTrans supporting translations
between multiple popular programming languages, NicheTrans for translating
between niche programming languages and popular ones, and LLMTrans for
evaluating executability of translated code by large language models (LLMs).
CodeTransOcean also includes a novel cross-framework dataset, DLTrans, for
translating deep learning code across different frameworks. We develop
multilingual modeling approaches for code translation and demonstrate their
great potential in improving the translation quality of both low-resource and
high-resource language pairs and boosting the training efficiency. We also
propose a novel evaluation metric Debugging Success Rate@K for program-level
code translation. Last but not least, we evaluate LLM ChatGPT on our datasets
and investigate its potential for fuzzy execution predictions. We build
baselines for CodeTransOcean and analyze challenges of code translation for
guiding future research. The CodeTransOcean datasets and code are publicly
available at https://github.com/WeixiangYAN/CodeTransOcean.",None,-1
c01175ef-b3f4-4d16-ab7d-8325abd430c3,MetaTKG: Learning Evolutionary Meta-Knowledge for Temporal Knowledge Graph Reasoning,0.665309,"Reasoning over Temporal Knowledge Graphs (TKGs) aims to predict future facts
based on given history. One of the key challenges for prediction is to learn
the evolution of facts. Most existing works focus on exploring evolutionary
information in history to obtain effective temporal embeddings for entities and
relations, but they ignore the variation in evolution patterns of facts, which
makes them struggle to adapt to future data with different evolution patterns.
Moreover, new entities continue to emerge along with the evolution of facts
over time. Since existing models highly rely on historical information to learn
embeddings for entities, they perform poorly on such entities with little
historical information. To tackle these issues, we propose a novel Temporal
Meta-learning framework for TKG reasoning, MetaTKG for brevity. Specifically,
our method regards TKG prediction as many temporal meta-tasks, and utilizes the
designed Temporal Meta-learner to learn evolutionary meta-knowledge from these
meta-tasks. The proposed method aims to guide the backbones to learn to adapt
quickly to future data and deal with entities with little historical
information by the learned meta-knowledge. Specially, in temporal meta-learner,
we design a Gating Integration module to adaptively establish temporal
correlations between meta-tasks. Extensive experiments on four widely-used
datasets and three backbones demonstrate that our method can greatly improve
the performance.",None,-1
1a897bda-b1da-4448-bb14-1986692f401a,False Negative/Positive Control for SAM on Noisy Medical Images,0.539109,"The Segment Anything Model (SAM) is a recently developed all-range foundation
model for image segmentation. It can use sparse manual prompts such as bounding
boxes to generate pixel-level segmentation in natural images but struggles in
medical images such as low-contrast, noisy ultrasound images. We propose a
refined test-phase prompt augmentation technique designed to improve SAM's
performance in medical image segmentation. The method couples multi-box prompt
augmentation and an aleatoric uncertainty-based false-negative (FN) and
false-positive (FP) correction (FNPC) strategy. We evaluate the method on two
ultrasound datasets and show improvement in SAM's performance and robustness to
inaccurate prompts, without the necessity for further training or tuning.
Moreover, we present the Single-Slice-to-Volume (SS2V) method, enabling 3D
pixel-level segmentation using only the bounding box annotation from a single
2D slice. Our results allow efficient use of SAM in even noisy, low-contrast
medical images. The source code will be released soon.",None,-1
2cf08edc-01e2-46ce-bda9-cf3a21da1a2d,Learning to Compress Prompts with Gist Tokens,0.774314,"Prompting is the primary way to utilize the multitask capabilities of
language models (LMs), but prompts occupy valuable space in the input context
window, and repeatedly encoding the same prompt is computationally inefficient.
Finetuning and distillation methods allow for specialization of LMs without
prompting, but require retraining the model for each task. To avoid this
trade-off entirely, we present gisting, which trains an LM to compress prompts
into smaller sets of ""gist"" tokens which can be cached and reused for compute
efficiency. Gist models can be trained with no additional cost over standard
instruction finetuning by simply modifying Transformer attention masks to
encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder
(FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting
in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings,
all with minimal loss in output quality.",None,-1
e54a412d-3c42-4ec2-b8e4-98cc033ad5f4,"Knowledge is Power, Understanding is Impact: Utility and Beyond Goals, Explanation Quality, and Fairness in Path Reasoning Recommendation",0.415414,"Path reasoning is a notable recommendation approach that models high-order
user-product relations, based on a Knowledge Graph (KG). This approach can
extract reasoning paths between recommended products and already experienced
products and, then, turn such paths into textual explanations for the user.
Unfortunately, evaluation protocols in this field appear heterogeneous and
limited, making it hard to contextualize the impact of the existing methods. In
this paper, we replicated three state-of-the-art relevant path reasoning
recommendation methods proposed in top-tier conferences. Under a common
evaluation protocol, based on two public data sets and in comparison with other
knowledge-aware methods, we then studied the extent to which they meet
recommendation utility and beyond objectives, explanation quality, and consumer
and provider fairness. Our study provides a picture of the progress in this
field, highlighting open issues and future directions. Source code:
\url{https://github.com/giacoballoccu/rep-path-reasoning-recsys}.",None,-1
5385c84e-7e07-448d-ab18-9816c25e58e4,Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing,0.694765,"We present Impossible Distillation, a novel framework for paraphrasing and
sentence summarization, that distills a high-quality dataset and model from a
low-quality teacher that itself cannot perform these tasks. Unlike prior works
that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific
architecture, we hypothesize and verify the paraphrastic proximity intrinsic to
pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in
the LM distribution. By identifying and distilling generations from these
subspaces, Impossible Distillation produces a high-quality dataset and model
even from GPT2-scale LMs. We evaluate our method on multiple benchmarks
spanning unconstrained / syntax-controlled paraphrase generation and sentence
summarization. Our model with 770M parameters consistently outperforms strong
baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT
itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher
diversity and fidelity than up to 13 times larger datasets.",None,-1
3a3ad082-34e6-4de3-b747-d4b53a93056a,TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments,0.701866,"Although the estimation of 3D human pose and shape (HPS) is rapidly
progressing, current methods still cannot reliably estimate moving humans in
global coordinates, which is critical for many applications. This is
particularly challenging when the camera is also moving, entangling human and
camera motion. To address these issues, we adopt a novel 5D representation
(space, time, and identity) that enables end-to-end reasoning about people in
scenes. Our method, called TRACE, introduces several novel architectural
components. Most importantly, it uses two new ""maps"" to reason about the 3D
trajectory of people over time in camera, and world, coordinates. An additional
memory unit enables persistent tracking of people even during long occlusions.
TRACE is the first one-stage method to jointly recover and track 3D humans in
global coordinates from dynamic cameras. By training it end-to-end, and using
full image information, TRACE achieves state-of-the-art performance on tracking
and HPS benchmarks. The code and dataset are released for research purposes.",None,-1
1956c161-f482-4c2f-9ffa-be3b3fc13740,TransFool: An Adversarial Attack against Neural Machine Translation Models,0.687018,"Deep neural networks have been shown to be vulnerable to small perturbations
of their inputs, known as adversarial attacks. In this paper, we investigate
the vulnerability of Neural Machine Translation (NMT) models to adversarial
attacks and propose a new attack algorithm called TransFool. To fool NMT
models, TransFool builds on a multi-term optimization problem and a gradient
projection step. By integrating the embedding representation of a language
model, we generate fluent adversarial examples in the source language that
maintain a high level of semantic similarity with the clean samples.
Experimental results demonstrate that, for different translation tasks and NMT
architectures, our white-box attack can severely degrade the translation
quality while the semantic similarity between the original and the adversarial
sentences stays high. Moreover, we show that TransFool is transferable to
unknown target models. Finally, based on automatic and human evaluations,
TransFool leads to improvement in terms of success rate, semantic similarity,
and fluency compared to the existing attacks both in white-box and black-box
settings. Thus, TransFool permits us to better characterize the vulnerability
of NMT models and outlines the necessity to design strong defense mechanisms
and more robust NMT systems for real-life applications.",None,-1
7164a418-afde-4db7-9b7c-b042dd4d6b66,Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning,0.660441,"The offline reinforcement learning (RL) paradigm provides a general recipe to
convert static behavior datasets into policies that can perform better than the
policy that collected the data. While policy constraints, conservatism, and
other methods for mitigating distributional shifts have made offline
reinforcement learning more effective, the continuous action setting often
necessitates various approximations for applying these techniques. Many of
these challenges are greatly alleviated in discrete action settings, where
offline RL constraints and regularizers can often be computed more precisely or
even exactly. In this paper, we propose an adaptive scheme for action
quantization. We use a VQ-VAE to learn state-conditioned action quantization,
avoiding the exponential blowup that comes with na\""ive discretization of the
action space. We show that several state-of-the-art offline RL methods such as
IQL, CQL, and BRAC improve in performance on benchmarks when combined with our
proposed discretization scheme. We further validate our approach on a set of
challenging long-horizon complex robotic manipulation tasks in the Robomimic
environment, where our discretized offline RL algorithms are able to improve
upon their continuous counterparts by 2-3x. Our project page is at
https://saqrl.github.io/",None,-1
72ddd4dd-1171-495d-b4d8-3040ee0319ea,Multi Modal Facial Expression Recognition with Transformer-Based Fusion Networks and Dynamic Sampling,0.770541,"Facial expression recognition is an essential task for various applications,
including emotion detection, mental health analysis, and human-machine
interactions. In this paper, we propose a multi-modal facial expression
recognition method that exploits audio information along with facial images to
provide a crucial clue to differentiate some ambiguous facial expressions.
Specifically, we introduce a Modal Fusion Module (MFM) to fuse audio-visual
information, where image and audio features are extracted from Swin
Transformer. Additionally, we tackle the imbalance problem in the dataset by
employing dynamic data resampling. Our model has been evaluated in the
Affective Behavior in-the-wild (ABAW) challenge of CVPR 2023.",None,-1
c645d033-8ceb-4481-9fe9-ff18008b4342,GeoGLUE: A GeoGraphic Language Understanding Evaluation Benchmark,0.128293,"With a fast developing pace of geographic applications, automatable and
intelligent models are essential to be designed to handle the large volume of
information. However, few researchers focus on geographic natural language
processing, and there has never been a benchmark to build a unified standard.
In this work, we propose a GeoGraphic Language Understanding Evaluation
benchmark, named GeoGLUE. We collect data from open-released geographic
resources and introduce six natural language understanding tasks, including
geographic textual similarity on recall, geographic textual similarity on
rerank, geographic elements tagging, geographic composition analysis,
geographic where what cut, and geographic entity alignment. We also pro vide
evaluation experiments and analysis of general baselines, indicating the
effectiveness and significance of the GeoGLUE benchmark.",None,-1
416ba91e-4e10-44b6-afdf-b0184c5dc6bf,Formalising Natural Language Quantifiers for Human-Robot Interactions,0.195499,"We present a method for formalising quantifiers in natural language in the
context of human-robot interactions. The solution is based on first-order logic
extended with capabilities to represent the cardinality of variables, operating
similarly to generalised quantifiers. To demonstrate the method, we designed an
end-to-end system able to receive input as natural language, convert it into a
formal logical representation, evaluate it, and return a result or send a
command to a simulated robot.",None,-1
0ae98e27-03ad-471e-b27d-2bc5030d4b22,A Schedule of Duties in the Cloud Space Using a Modified Salp Swarm Algorithm,0.223362,"Cloud computing is a concept introduced in the information technology era,
with the main components being the grid, distributed, and valuable computing.
The cloud is being developed continuously and, naturally, comes up with many
challenges, one of which is scheduling. A schedule or timeline is a mechanism
used to optimize the time for performing a duty or set of duties. A scheduling
process is accountable for choosing the best resources for performing a duty.
The main goal of a scheduling algorithm is to improve the efficiency and
quality of the service while at the same time ensuring the acceptability and
effectiveness of the targets. The task scheduling problem is one of the most
important NP-hard issues in the cloud domain and, so far, many techniques have
been proposed as solutions, including using genetic algorithms (GAs), particle
swarm optimization, (PSO), and ant colony optimization (ACO). To address this
problem, in this paper, one of the collective intelligence algorithms, called
the Salp Swarm Algorithm (SSA), has been expanded, improved, and applied. The
performance of the proposed algorithm has been compared with that of GAs, PSO,
continuous ACO, and the basic SSA. The results show that our algorithm has
generally higher performance than the other algorithms. For example, compared
to the basic SSA, the proposed method has an average reduction of approximately
21% in makespan.",None,-1
d0b1ce27-15aa-4f5a-ba38-818408fc6f25,To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer,0.195499,"Choosing an appropriate tokenization scheme is often a bottleneck in
low-resource cross-lingual transfer. To understand the downstream implications
of text representation choices, we perform a comparative analysis on language
models having diverse text representation modalities including 2
segmentation-based models (\texttt{BERT}, \texttt{mBERT}), 1 image-based model
(\texttt{PIXEL}), and 1 character-level model (\texttt{CANINE}). First, we
propose a scoring Language Quotient (LQ) metric capable of providing a weighted
representation of both zero-shot and few-shot evaluation combined. Utilizing
this metric, we perform experiments comprising 19 source languages and 133
target languages on three tasks (POS tagging, Dependency parsing, and NER). Our
analysis reveals that image-based models excel in cross-lingual transfer when
languages are closely related and share visually similar scripts. However, for
tasks biased toward word meaning (POS, NER), segmentation-based models prove to
be superior. Furthermore, in dependency parsing tasks where word relationships
play a crucial role, models with their character-level focus, outperform
others. Finally, we propose a recommendation scheme based on our findings to
guide model selection according to task and language requirements.",None,-1
3fd7bce7-04fd-4298-9150-e39e47b50e19,Explicit Syntactic Guidance for Neural Text Generation,0.116985,"Most existing text generation models follow the sequence-to-sequence
paradigm. Generative Grammar suggests that humans generate natural language
texts by learning language grammar. We propose a syntax-guided generation
schema, which generates the sequence guided by a constituency parse tree in a
top-down direction. The decoding process can be decomposed into two parts: (1)
predicting the infilling texts for each constituent in the lexicalized syntax
context given the source sentence; (2) mapping and expanding each constituent
to construct the next-level syntax context. Accordingly, we propose a
structural beam search method to find possible syntax structures
hierarchically. Experiments on paraphrase generation and machine translation
show that the proposed method outperforms autoregressive baselines, while also
demonstrating effectiveness in terms of interpretability, controllability, and
diversity.",None,-1
1f9c9f3c-9272-4fbe-a2f3-bb5c7db9e16f,MidMed: Towards Mixed-Type Dialogues for Medical Consultation,0.654771,"Most medical dialogue systems assume that patients have clear goals (medicine
querying, surgical operation querying, etc.) before medical consultation.
However, in many real scenarios, due to the lack of medical knowledge, it is
usually difficult for patients to determine clear goals with all necessary
slots. In this paper, we identify this challenge as how to construct medical
consultation dialogue systems to help patients clarify their goals. To mitigate
this challenge, we propose a novel task and create a human-to-human mixed-type
medical consultation dialogue corpus, termed MidMed, covering five dialogue
types: task-oriented dialogue for diagnosis, recommendation, knowledge-grounded
dialogue, QA, and chitchat. MidMed covers four departments
(otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,175
dialogues. Furthermore, we build baselines on MidMed and propose an
instruction-guiding medical dialogue generation framework, termed InsMed, to
address this task. Experimental results show the effectiveness of InsMed.",None,-1
cd0a48e9-4777-4d07-908d-da815f372881,Open Sesame! Universal Black Box Jailbreaking of Large Language Models,0.452064,"Large language models (LLMs), designed to provide helpful and safe responses,
often rely on alignment techniques to align with user intent and social
guidelines. Unfortunately, this alignment can be exploited by malicious actors
seeking to manipulate an LLM's outputs for unintended purposes. In this paper
we introduce a novel approach that employs a genetic algorithm (GA) to
manipulate LLMs when model architecture and parameters are inaccessible. The GA
attack works by optimizing a universal adversarial prompt that -- when combined
with a user's query -- disrupts the attacked model's alignment, resulting in
unintended and potentially harmful outputs. Our novel approach systematically
reveals a model's limitations and vulnerabilities by uncovering instances where
its responses deviate from expected behavior. Through extensive experiments we
demonstrate the efficacy of our technique, thus contributing to the ongoing
discussion on responsible AI development by providing a diagnostic tool for
evaluating and enhancing alignment of LLMs with human intent. To our knowledge
this is the first automated universal black box jailbreak attack.",None,-1
ba6fa6a1-dd46-43a1-9ed6-b867b9665fee,Monocular Depth Estimation using Diffusion Models,0.99565,"We formulate monocular depth estimation using denoising diffusion models,
inspired by their recent successes in high fidelity image generation. To that
end, we introduce innovations to address problems arising due to noisy,
incomplete depth maps in training data, including step-unrolled denoising
diffusion, an $L_1$ loss, and depth infilling during training. To cope with the
limited availability of data for supervised training, we leverage pre-training
on self-supervised image-to-image translation tasks. Despite the simplicity of
the approach, with a generic loss and architecture, our DepthGen model achieves
SOTA performance on the indoor NYU dataset, and near SOTA results on the
outdoor KITTI dataset. Further, with a multimodal posterior, DepthGen naturally
represents depth ambiguity (e.g., from transparent surfaces), and its zero-shot
performance combined with depth imputation, enable a simple but effective
text-to-3D pipeline. Project page: https://depth-gen.github.io",None,-1
dbedd9c6-40dc-413b-8200-693cdfae159c,An Alternative to WSSS? An Empirical Study of the Segment Anything Model (SAM) on Weakly-Supervised Semantic Segmentation Problems,0.771619,"The Segment Anything Model (SAM) has demonstrated exceptional performance and
versatility, making it a promising tool for various related tasks. In this
report, we explore the application of SAM in Weakly-Supervised Semantic
Segmentation (WSSS). Particularly, we adapt SAM as the pseudo-label generation
pipeline given only the image-level class labels. While we observed impressive
results in most cases, we also identify certain limitations. Our study includes
performance evaluations on PASCAL VOC and MS-COCO, where we achieved remarkable
improvements over the latest state-of-the-art methods on both datasets. We
anticipate that this report encourages further explorations of adopting SAM in
WSSS, as well as wider real-world applications.",None,-1
353b7ed9-5bfb-4615-81a5-2fba737b93e3,TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition,0.770044,"Emotion Support Conversation (ESC) is an emerging and challenging task with
the goal of reducing the emotional distress of people. Previous attempts fail
to maintain smooth transitions between utterances in ESC because they ignore to
grasp the fine-grained transition information at each dialogue turn. To solve
this problem, we propose to take into account turn-level state
\textbf{Trans}itions of \textbf{ESC} (\textbf{TransESC}) from three
perspectives, including semantics transition, strategy transition and emotion
transition, to drive the conversation in a smooth and natural way.
Specifically, we construct the state transition graph with a two-step way,
named transit-then-interact, to grasp such three types of turn-level transition
information. Finally, they are injected into the transition-aware decoder to
generate more engaging responses. Both automatic and human evaluations on the
benchmark dataset demonstrate the superiority of TransESC to generate more
smooth and effective supportive responses. Our source code is available at
\url{https://github.com/circle-hit/TransESC}.",None,-1
1be1ce36-cef6-45c4-a759-54f092732000,Multilevel Objective-Function-Free Optimization with an Application to Neural Networks Training,0.128639,"A class of multi-level algorithms for unconstrained nonlinear optimization is
presented which does not require the evaluation of the objective function. The
class contains the momentum-less AdaGrad method as a particular (single-level)
instance. The choice of avoiding the evaluation of the objective function is
intended to make the algorithms of the class less sensitive to noise, while the
multi-level feature aims at reducing their computational cost. The evaluation
complexity of these algorithms is analyzed and their behaviour in the presence
of noise is then illustrated in the context of training deep neural networks
for supervised learning applications.",None,-1
2b6228c4-98f1-47a4-89ea-f163f600aec4,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,0.748095,"The quality of the video stream is key to neural network-based video
analytics. However, low-quality video is inevitably collected by existing
surveillance systems because of poor quality cameras or over-compressed/pruned
video streaming protocols, e.g., as a result of upstream bandwidth limit. To
address this issue, existing studies use quality enhancers (e.g., neural
super-resolution) to improve the quality of videos (e.g., resolution) and
eventually ensure inference accuracy. Nevertheless, directly applying quality
enhancers does not work in practice because it will introduce unacceptable
latency. In this paper, we present AccDecoder, a novel accelerated decoder for
real-time and neural-enhanced video analytics. AccDecoder can select a few
frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality
by neural super-resolution and then up-scale the unselected frames that
reference them, which leads to 6-21% accuracy improvement. AccDecoder provides
efficient inference capability via filtering important frames using DRL for
DNN-based inference and reusing the results for the other frames via extracting
the reference relationship among frames and blocks, which results in a latency
reduction of 20-80% than baselines.",None,-1
3ce117b4-5a49-4462-9895-1a3634b982b0,Planning for Learning Object Properties,0.202467,"Autonomous agents embedded in a physical environment need the ability to
recognize objects and their properties from sensory data. Such a perceptual
ability is often implemented by supervised machine learning models, which are
pre-trained using a set of labelled data. In real-world, open-ended
deployments, however, it is unrealistic to assume to have a pre-trained model
for all possible environments. Therefore, agents need to dynamically
learn/adapt/extend their perceptual abilities online, in an autonomous way, by
exploring and interacting with the environment where they operate. This paper
describes a way to do so, by exploiting symbolic planning. Specifically, we
formalize the problem of automatically training a neural network to recognize
object properties as a symbolic planning problem (using PDDL). We use planning
techniques to produce a strategy for automating the training dataset creation
and the learning process. Finally, we provide an experimental evaluation in
both a simulated and a real environment, which shows that the proposed approach
is able to successfully learn how to recognize new object properties.",None,-1
eb4e1844-38f8-42f7-856c-5739f234f3c2,dacl10k: Benchmark for Semantic Bridge Damage Segmentation,0.737927,"Reliably identifying reinforced concrete defects (RCDs)plays a crucial role
in assessing the structural integrity, traffic safety, and long-term durability
of concrete bridges, which represent the most common bridge type worldwide.
Nevertheless, available datasets for the recognition of RCDs are small in terms
of size and class variety, which questions their usability in real-world
scenarios and their role as a benchmark. Our contribution to this problem is
""dacl10k"", an exceptionally diverse RCD dataset for multi-label semantic
segmentation comprising 9,920 images deriving from real-world bridge
inspections. dacl10k distinguishes 12 damage classes as well as 6 bridge
components that play a key role in the building assessment and recommending
actions, such as restoration works, traffic load limitations or bridge
closures. In addition, we examine baseline models for dacl10k which are
subsequently evaluated. The best model achieves a mean intersection-over-union
of 0.42 on the test set. dacl10k, along with our baselines, will be openly
accessible to researchers and practitioners, representing the currently biggest
dataset regarding number of images and class diversity for semantic
segmentation in the bridge inspection domain.",None,-1
86182eb8-5364-4321-8de7-497ad438596a,Tracking without Label: Unsupervised Multiple Object Tracking via Contrastive Similarity Learning,0.221418,"Unsupervised learning is a challenging task due to the lack of labels.
Multiple Object Tracking (MOT), which inevitably suffers from mutual object
interference, occlusion, etc., is even more difficult without label
supervision. In this paper, we explore the latent consistency of sample
features across video frames and propose an Unsupervised Contrastive Similarity
Learning method, named UCSL, including three contrast modules: self-contrast,
cross-contrast, and ambiguity contrast. Specifically, i) self-contrast uses
intra-frame direct and inter-frame indirect contrast to obtain discriminative
representations by maximizing self-similarity. ii) Cross-contrast aligns cross-
and continuous-frame matching results, mitigating the persistent negative
effect caused by object occlusion. And iii) ambiguity contrast matches
ambiguous objects with each other to further increase the certainty of
subsequent object association through an implicit manner. On existing
benchmarks, our method outperforms the existing unsupervised methods using only
limited help from ReID head, and even provides higher accuracy than lots of
fully supervised methods.",None,-1
41b1cc0c-0713-4454-b8b5-83f2489e53ef,Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges,0.213675,"Large Language Models (LLMs) have demonstrated impressive zero shot
performance on a wide range of NLP tasks, demonstrating the ability to reason
and apply commonsense. A relevant application is to use them for creating high
quality synthetic datasets for downstream tasks. In this work, we probe whether
GPT-4 can be used to augment existing extractive reading comprehension
datasets. Automating data annotation processes has the potential to save large
amounts of time, money and effort that goes into manually labelling datasets.
In this paper, we evaluate the performance of GPT-4 as a replacement for human
annotators for low resource reading comprehension tasks, by comparing
performance after fine tuning, and the cost associated with annotation. This
work serves to be the first analysis of LLMs as synthetic data augmenters for
QA systems, highlighting the unique opportunities and challenges. Additionally,
we release augmented versions of low resource datasets, that will allow the
research community to create further benchmarks for evaluation of generated
datasets.",None,-1
3ee93835-ecdf-4105-8e08-c69373b03e69,ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation,0.766259,"Recent advancements in dense out-of-distribution (OOD) detection have
primarily focused on scenarios where the training and testing datasets share a
similar domain, with the assumption that no domain shift exists between them.
However, in real-world situations, domain shift often exits and significantly
affects the accuracy of existing out-of-distribution (OOD) detection models. In
this work, we propose a dual-level OOD detection framework to handle domain
shift and semantic shift jointly. The first level distinguishes whether domain
shift exists in the image by leveraging global low-level features, while the
second level identifies pixels with semantic shift by utilizing dense
high-level feature maps. In this way, we can selectively adapt the model to
unseen domains as well as enhance model's capacity in detecting novel classes.
We validate the efficacy of our proposed method on several OOD segmentation
benchmarks, including those with significant domain shifts and those without,
observing consistent performance improvements across various baseline models.
Code is available at
${\href{https://github.com/gaozhitong/ATTA}{https://github.com/gaozhitong/ATTA}}$.",None,-1
977e776b-20df-4810-a1aa-928884e552f8,Prompt-Guided Retrieval Augmentation for Non-Knowledge-Intensive Tasks,0.364702,"Retrieval-augmented methods have received increasing attention to support
downstream tasks by leveraging useful information from external resources.
Recent studies mainly focus on exploring retrieval to solve knowledge-intensive
(KI) tasks. However, the potential of retrieval for most
non-knowledge-intensive (NKI) tasks remains under-explored. There are two main
challenges to leveraging retrieval-augmented methods for NKI tasks: 1) the
demand for diverse relevance score functions and 2) the dilemma between
training cost and task performance. To address these challenges, we propose a
two-stage framework for NKI tasks, named PGRA. In the first stage, we adopt a
task-agnostic retriever to build a shared static index and select candidate
evidence efficiently. In the second stage, we design a prompt-guided reranker
to rerank the nearest evidence according to task-specific relevance for the
reader. Experimental results show that PGRA outperforms other state-of-the-art
retrieval-augmented methods. Our analyses further investigate the influence
factors to model performance and demonstrate the generality of PGRA. Codes are
available at https://github.com/THUNLP-MT/PGRA.",None,-1
668282e4-7370-43e2-8a1b-92bf811330f5,Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance,0.0453415,"Dialect classification is used in a variety of applications, such as machine
translation and speech recognition, to improve the overall performance of the
system. In a real-world scenario, a deployed dialect classification model can
encounter anomalous inputs that differ from the training data distribution,
also called out-of-distribution (OOD) samples. Those OOD samples can lead to
unexpected outputs, as dialects of those samples are unseen during model
training. Out-of-distribution detection is a new research area that has
received little attention in the context of dialect classification. Towards
this, we proposed a simple yet effective unsupervised Mahalanobis distance
feature-based method to detect out-of-distribution samples. We utilize the
latent embeddings from all intermediate layers of a wav2vec 2.0
transformer-based dialect classifier model for multi-task learning. Our
proposed approach outperforms other state-of-the-art OOD detection methods
significantly.",None,-1
9a7908a4-8c91-4ee5-ac37-f542486da0b6,Few-Shot Spoken Language Understanding via Joint Speech-Text Models,0.0933544,"Recent work on speech representation models jointly pre-trained with text has
demonstrated the potential of improving speech representations by encoding
speech and text in a shared space. In this paper, we leverage such shared
representations to address the persistent challenge of limited data
availability in spoken language understanding tasks. By employing a pre-trained
speech-text model, we find that models fine-tuned on text can be effectively
transferred to speech testing data. With as little as 1 hour of labeled speech
data, our proposed approach achieves comparable performance on spoken language
understanding tasks (specifically, sentiment analysis and named entity
recognition) when compared to previous methods using speech-only pre-trained
models fine-tuned on 10 times more data. Beyond the proof-of-concept study, we
also analyze the latent representations. We find that the bottom layers of
speech-text models are largely task-agnostic and align speech and text
representations into a shared space, while the top layers are more
task-specific.",None,-1
8d2012e9-94ef-4dee-95c3-46d6b4f0da6c,ASIC: Aligning Sparse in-the-wild Image Collections,0.367344,"We present a method for joint alignment of sparse in-the-wild image
collections of an object category. Most prior works assume either ground-truth
keypoint annotations or a large dataset of images of a single object category.
However, neither of the above assumptions hold true for the long-tail of the
objects present in the world. We present a self-supervised technique that
directly optimizes on a sparse collection of images of a particular
object/object category to obtain consistent dense correspondences across the
collection. We use pairwise nearest neighbors obtained from deep features of a
pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches
and make them dense and accurate matches by optimizing a neural network that
jointly maps the image collection into a learned canonical grid. Experiments on
CUB and SPair-71k benchmarks demonstrate that our method can produce globally
consistent and higher quality correspondences across the image collection when
compared to existing self-supervised methods. Code and other material will be
made available at \url{https://kampta.github.io/asic}.",None,-1
a852bcbc-98f4-426e-8020-9926a1e86a32,"Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models",0.782911,"The introduction of diffusion models in anomaly detection has paved the way
for more effective and accurate image reconstruction in pathologies. However,
the current limitations in controlling noise granularity hinder diffusion
models' ability to generalize across diverse anomaly types and compromise the
restoration of healthy tissues. To overcome these challenges, we propose
AutoDDPM, a novel approach that enhances the robustness of diffusion models.
AutoDDPM utilizes diffusion models to generate initial likelihood maps of
potential anomalies and seamlessly integrates them with the original image.
Through joint noised distribution re-sampling, AutoDDPM achieves harmonization
and in-painting effects. Our study demonstrates the efficacy of AutoDDPM in
replacing anomalous regions while preserving healthy tissues, considerably
surpassing diffusion models' limitations. It also contributes valuable insights
and analysis on the limitations of current diffusion models, promoting robust
and interpretable anomaly detection in medical imaging - an essential aspect of
building autonomous clinical decision systems with higher interpretability.",None,-1
47c12479-8b40-41dd-8e5c-931cb5d77593,Large Language Models can Learn Rules,0.981307,"When prompted with a few examples and intermediate steps, large language
models (LLMs) have demonstrated impressive performance in various reasoning
tasks. However, prompting methods that rely on implicit knowledge in an LLM
often generate incorrect answers when the implicit knowledge is wrong or
inconsistent with the task. To tackle this problem, we present
Hypotheses-to-Theories (HtT), a framework that learns a rule library for
reasoning with LLMs. HtT contains two stages, an induction stage and a
deduction stage. In the induction stage, an LLM is first asked to generate and
verify rules over a set of training examples. Rules that appear and lead to
correct answers sufficiently often are collected to form a rule library. In the
deduction stage, the LLM is then prompted to employ the learned rule library to
perform reasoning to answer test questions. Experiments on relational
reasoning, numerical reasoning and concept learning problems show that HtT
improves existing prompting methods, with an absolute gain of 10-30% in
accuracy. The learned rules are also transferable to different models and to
different forms of the same problem.",None,-1
1734205f-9a01-4d72-9d58-080f42b8e06f,Learnability with PAC Semantics for Multi-agent Beliefs,0.0866735,"The tension between deduction and induction is perhaps the most fundamental
issue in areas such as philosophy, cognition and artificial intelligence. In an
influential paper, Valiant recognised that the challenge of learning should be
integrated with deduction. In particular, he proposed a semantics to capture
the quality possessed by the output of Probably Approximately Correct (PAC)
learning algorithms when formulated in a logic. Although weaker than classical
entailment, it allows for a powerful model-theoretic framework for answering
queries. In this paper, we provide a new technical foundation to demonstrate
PAC learning with multi-agent epistemic logics. To circumvent the negative
results in the literature on the difficulty of robust learning with the PAC
semantics, we consider so-called implicit learning where we are able to
incorporate observations to the background theory in service of deciding the
entailment of an epistemic query. We prove correctness of the learning
procedure and discuss results on the sample complexity, that is how many
observations we will need to provably assert that the query is entailed given a
user-specified error bound. Finally, we investigate under what circumstances
this algorithm can be made efficient. On the last point, given that reasoning
in epistemic logics especially in multi-agent epistemic logics is
PSPACE-complete, it might seem like there is no hope for this problem. We
leverage some recent results on the so-called Representation Theorem explored
for single-agent and multi-agent epistemic logics with the only knowing
operator to reduce modal reasoning to propositional reasoning.",None,-1
29a7b071-638b-4d5a-a74f-6bf48afddbec,3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V,0.476662,"In this work, we present a new visual prompting method called 3DAxiesPrompts
(3DAP) to unleash the capabilities of GPT-4V in performing 3D spatial tasks.
Our investigation reveals that while GPT-4V exhibits proficiency in discerning
the position and interrelations of 2D entities through current visual prompting
techniques, its abilities in handling 3D spatial tasks have yet to be explored.
In our approach, we create a 3D coordinate system tailored to 3D imagery,
complete with annotated scale information. By presenting images infused with
the 3DAP visual prompt as inputs, we empower GPT-4V to ascertain the spatial
positioning information of the given 3D target image with a high degree of
precision. Through experiments, We identified three tasks that could be stably
completed using the 3DAP method, namely, 2D to 3D Point Reconstruction, 2D to
3D point matching, and 3D Object Detection. We perform experiments on our
proposed dataset 3DAP-Data, the results from these experiments validate the
efficacy of 3DAP-enhanced GPT-4V inputs, marking a significant stride in 3D
spatial task execution.",None,-1
d5001467-caa6-425a-aa91-4ca6f2a08107,Collaborative Auto-encoding for Blind Image Quality Assessment,0.392411,"Blind image quality assessment (BIQA) is a challenging problem with important
real-world applications. Recent efforts attempting to exploit powerful
representations by deep neural networks (DNN) are hindered by the lack of
subjectively annotated data. This paper presents a novel BIQA method which
overcomes this fundamental obstacle. Specifically, we design a pair of
collaborative autoencoders (COAE) consisting of a content autoencoder (CAE) and
a distortion autoencoder (DAE) that work together to extract content and
distortion representations, which are shown to be highly descriptive of image
quality. While the CAE follows a standard codec procedure, we introduce the
CAE-encoded feature as an extra input to the DAE's decoder for reconstructing
distorted images, thus effectively forcing DAE's encoder to extract distortion
representations. The self-supervised learning framework allows the COAE
including two feature extractors to be trained by almost unlimited amount of
data, thus leaving limited samples with annotations to finetune a BIQA model.
We will show that the proposed BIQA method achieves state-of-the-art
performance and has superior generalization capability over other learning
based models. The codes are available at:
https://github.com/Macro-Zhou/NRIQA-VISOR/.",None,-1
56d735c1-d95a-48ed-9060-17725e72ccf3,CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data Generation,0.605424,"Conversational search provides a natural interface for information retrieval
(IR). Recent approaches have demonstrated promising results in applying dense
retrieval to conversational IR. However, training dense retrievers requires
large amounts of in-domain paired data. This hinders the development of
conversational dense retrievers, as abundant in-domain conversations are
expensive to collect. In this paper, we propose CONVERSER, a framework for
training conversational dense retrievers with at most 6 examples of in-domain
dialogues. Specifically, we utilize the in-context learning capability of large
language models to generate conversational queries given a passage in the
retrieval corpus. Experimental results on conversational retrieval benchmarks
OR-QuAC and TREC CAsT 19 show that the proposed CONVERSER achieves comparable
performance to fully-supervised models, demonstrating the effectiveness of our
proposed framework in few-shot conversational dense retrieval. All source code
and generated datasets are available at https://github.com/MiuLab/CONVERSER",None,-1
4ac3befc-1503-43b6-90b0-edf19b8cfef0,DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models,0.537509,"Targeting to understand the underlying explainable factors behind
observations and modeling the conditional generation process on these factors,
we connect disentangled representation learning to Diffusion Probabilistic
Models (DPMs) to take advantage of the remarkable modeling ability of DPMs. We
propose a new task, disentanglement of (DPMs): given a pre-trained DPM, without
any annotations of the factors, the task is to automatically discover the
inherent factors behind the observations and disentangle the gradient fields of
DPM into sub-gradient fields, each conditioned on the representation of each
discovered factor. With disentangled DPMs, those inherent factors can be
automatically discovered, explicitly represented, and clearly injected into the
diffusion process via the sub-gradient fields. To tackle this task, we devise
an unsupervised approach named DisDiff, achieving disentangled representation
learning in the framework of DPMs. Extensive experiments on synthetic and
real-world datasets demonstrate the effectiveness of DisDiff.",None,-1
1adaa7b9-5d64-4240-94f9-bbff891b326c,Context Matters: A Strategy to Pre-train Language Model for Science Education,0.933014,"This study aims at improving the performance of scoring student responses in
science education automatically. BERT-based language models have shown
significant superiority over traditional NLP models in various language-related
tasks. However, science writing of students, including argumentation and
explanation, is domain-specific. In addition, the language used by students is
different from the language in journals and Wikipedia, which are training
sources of BERT and its existing variants. All these suggest that a
domain-specific model pre-trained using science education data may improve
model performance. However, the ideal type of data to contextualize pre-trained
language model and improve the performance in automatically scoring student
written responses remains unclear. Therefore, we employ different data in this
study to contextualize both BERT and SciBERT models and compare their
performance on automatic scoring of assessment tasks for scientific
argumentation. We use three datasets to pre-train the model: 1) journal
articles in science education, 2) a large dataset of students' written
responses (sample size over 50,000), and 3) a small dataset of students'
written responses of scientific argumentation tasks. Our experimental results
show that in-domain training corpora constructed from science questions and
responses improve language model performance on a wide variety of downstream
tasks. Our study confirms the effectiveness of continual pre-training on
domain-specific data in the education domain and demonstrates a generalizable
strategy for automating science education tasks with high accuracy. We plan to
release our data and SciEdBERT models for public use and community engagement.",None,-1
7abf9ea8-ad67-4fee-b9f8-95241d406d07,ADD: An Automatic Desensitization Fisheye Dataset for Autonomous Driving,0.218728,"Autonomous driving systems require many images for analyzing the surrounding
environment. However, there is fewer data protection for private information
among these captured images, such as pedestrian faces or vehicle license
plates, which has become a significant issue. In this paper, in response to the
call for data security laws and regulations and based on the advantages of
large Field of View(FoV) of the fisheye camera, we build the first Autopilot
Desensitization Dataset, called ADD, and formulate the first
deep-learning-based image desensitization framework, to promote the study of
image desensitization in autonomous driving scenarios. The compiled dataset
consists of 650K images, including different face and vehicle license plate
information captured by the surround-view fisheye camera. It covers various
autonomous driving scenarios, including diverse facial characteristics and
license plate colors. Then, we propose an efficient multitask desensitization
network called DesCenterNet as a benchmark on the ADD dataset, which can
perform face and vehicle license plate detection and desensitization tasks.
Based on ADD, we further provide an evaluation criterion for desensitization
performance, and extensive comparison experiments have verified the
effectiveness and superiority of our method on image desensitization.",None,-1
c12ed45c-a66a-490e-8d3b-919aa1fe5384,Late Breaking Results: Scalable and Efficient Hyperdimensional Computing for Network Intrusion Detection,0.37337,"Cybersecurity has emerged as a critical challenge for the industry. With the
large complexity of the security landscape, sophisticated and costly deep
learning models often fail to provide timely detection of cyber threats on edge
devices. Brain-inspired hyperdimensional computing (HDC) has been introduced as
a promising solution to address this issue. However, existing HDC approaches
use static encoders and require very high dimensionality and hundreds of
training iterations to achieve reasonable accuracy. This results in a serious
loss of learning efficiency and causes huge latency for detecting attacks. In
this paper, we propose CyberHD, an innovative HDC learning framework that
identifies and regenerates insignificant dimensions to capture complicated
patterns of cyber threats with remarkably lower dimensionality. Additionally,
the holographic distribution of patterns in high dimensional space provides
CyberHD with notably high robustness against hardware errors.",None,-1
1d5bd8fe-1357-42f0-a53b-bef481c7ba6e,The Stable Signature: Rooting Watermarks in Latent Diffusion Models,0.997816,"Generative image modeling enables a wide range of applications but raises
ethical concerns about responsible deployment. This paper introduces an active
strategy combining image watermarking and Latent Diffusion Models. The goal is
for all generated images to conceal an invisible watermark allowing for future
detection and/or identification. The method quickly fine-tunes the latent
decoder of the image generator, conditioned on a binary signature. A
pre-trained watermark extractor recovers the hidden signature from any
generated image and a statistical test then determines whether it comes from
the generative model. We evaluate the invisibility and robustness of the
watermarks on a variety of generation tasks, showing that Stable Signature
works even after the images are modified. For instance, it detects the origin
of an image generated from a text prompt, then cropped to keep $10\%$ of the
content, with $90$+$\%$ accuracy at a false positive rate below 10$^{-6}$.",None,-1
da0beb60-3312-4618-913c-d82049ce5372,AI Security Threats against Pervasive Robotic Systems: A Course for Next Generation Cybersecurity Workforce,0.0930048,"Robotics, automation, and related Artificial Intelligence (AI) systems have
become pervasive bringing in concerns related to security, safety, accuracy,
and trust. With growing dependency on physical robots that work in close
proximity to humans, the security of these systems is becoming increasingly
important to prevent cyber-attacks that could lead to privacy invasion,
critical operations sabotage, and bodily harm. The current shortfall of
professionals who can defend such systems demands development and integration
of such a curriculum. This course description includes details about seven
self-contained and adaptive modules on ""AI security threats against pervasive
robotic systems"". Topics include: 1) Introduction, examples of attacks, and
motivation; 2) - Robotic AI attack surfaces and penetration testing; 3) -
Attack patterns and security strategies for input sensors; 4) - Training
attacks and associated security strategies; 5) - Inference attacks and
associated security strategies; 6) - Actuator attacks and associated security
strategies; and 7) - Ethics of AI, robotics, and cybersecurity.",None,-1
ef2911b4-4886-4c1e-bca9-9f3918f467d2,An Information Extraction Study: Take In Mind the Tokenization!,0.13458,"Current research on the advantages and trade-offs of using characters,
instead of tokenized text, as input for deep learning models, has evolved
substantially. New token-free models remove the traditional tokenization step;
however, their efficiency remains unclear. Moreover, the effect of tokenization
is relatively unexplored in sequence tagging tasks. To this end, we investigate
the impact of tokenization when extracting information from documents and
present a comparative study and analysis of subword-based and character-based
models. Specifically, we study Information Extraction (IE) from biomedical
texts. The main outcome is twofold: tokenization patterns can introduce
inductive bias that results in state-of-the-art performance, and the
character-based models produce promising results; thus, transitioning to
token-free IE models is feasible.",None,-1
59f8121d-5d4d-4ff5-8c76-0ed31bf41ee6,Adversarial Contrastive Distillation with Adaptive Denoising,0.843592,"Adversarial Robustness Distillation (ARD) is a novel method to boost the
robustness of small models. Unlike general adversarial training, its robust
knowledge transfer can be less easily restricted by the model capacity.
However, the teacher model that provides the robustness of knowledge does not
always make correct predictions, interfering with the student's robust
performances. Besides, in the previous ARD methods, the robustness comes
entirely from one-to-one imitation, ignoring the relationship between examples.
To this end, we propose a novel structured ARD method called Contrastive
Relationship DeNoise Distillation (CRDND). We design an adaptive compensation
module to model the instability of the teacher. Moreover, we utilize the
contrastive relationship to explore implicit robustness knowledge among
multiple examples. Experimental results on multiple attack benchmarks show
CRDND can transfer robust knowledge efficiently and achieves state-of-the-art
performances.",None,-1
fd07a72a-b45d-4016-b721-ea51b1183b3e,Bayesian Integration of Information Using Top-Down Modulated WTA Networks,0.257547,"Winner Take All (WTA) circuits a type of Spiking Neural Networks (SNN) have
been suggested as facilitating the brain's ability to process information in a
Bayesian manner. Research has shown that WTA circuits are capable of
approximating hierarchical Bayesian models via Expectation Maximization (EM).
So far, research in this direction has focused on bottom up processes. This is
contrary to neuroscientific evidence that shows that, besides bottom up
processes, top down processes too play a key role in information processing by
the human brain. Several functions ascribed to top down processes include
direction of attention, adjusting for expectations, facilitation of encoding
and recall of learned information, and imagery. This paper explores whether WTA
circuits are suitable for further integrating information represented in
separate WTA networks. Furthermore, it explores whether, and under what
circumstances, top down processes can improve WTA network performance with
respect to inference and learning. The results show that WTA circuits are
capable of integrating the probabilistic information represented by other WTA
networks, and that top down processes can improve a WTA network's inference and
learning performance. Notably, it is able to do this according to key
neuromorphic principles, making it ideal for low-latency and energy efficient
implementation on neuromorphic hardware.",None,-1
300cf31d-a6d9-4dde-a129-01271316b71a,Thread of Thought Unraveling Chaotic Contexts,0.474146,"Large Language Models (LLMs) have ushered in a transformative era in the
field of natural language processing, excelling in tasks related to text
comprehension and generation. Nevertheless, they encounter difficulties when
confronted with chaotic contexts (e.g., distractors rather than long irrelevant
context), leading to the inadvertent omission of certain details within the
chaotic context. In response to these challenges, we introduce the ""Thread of
Thought"" (ThoT) strategy, which draws inspiration from human cognitive
processes. ThoT systematically segments and analyzes extended contexts while
adeptly selecting pertinent information. This strategy serves as a versatile
""plug-and-play"" module, seamlessly integrating with various LLMs and prompting
techniques. In the experiments, we utilize the PopQA and EntityQ datasets, as
well as a Multi-Turn Conversation Response dataset (MTCR) we collected, to
illustrate that ThoT significantly improves reasoning performance compared to
other prompting techniques.",None,-1
d6e18378-9eef-4031-bec4-b1ecefbffeef,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,0.258307,"Inspired by recent findings that generative diffusion models learn
semantically meaningful representations, we use them to discover the intrinsic
hierarchical structure in biomedical 3D images using unsupervised segmentation.
We show that features of diffusion models from different stages of a
U-Net-based ladder-like architecture capture different hierarchy levels in 3D
biomedical images. We design three losses to train a predictive unsupervised
segmentation network that encourages the decomposition of 3D volumes into
meaningful nested subvolumes that represent a hierarchy. First, we pretrain 3D
diffusion models and use the consistency of their features across subvolumes.
Second, we use the visual consistency between subvolumes. Third, we use the
invariance to photometric augmentations as a regularizer. Our models achieve
better performance than prior unsupervised structure discovery approaches on
challenging biologically-inspired synthetic datasets and on a real-world brain
tumor MRI dataset.",None,-1
70753f64-def9-4184-a717-d7b1ae48cd71,To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support,0.757846,"Optimizing the phrasing of argumentative text is crucial in higher education
and professional development. However, assessing whether and how the different
claims in a text should be revised is a hard task, especially for novice
writers. In this work, we explore the main challenges to identifying
argumentative claims in need of specific revisions. By learning from
collaborative editing behaviors in online debates, we seek to capture implicit
revision patterns in order to develop approaches aimed at guiding writers in
how to further improve their arguments. We systematically compare the ability
of common word embedding models to capture the differences between different
versions of the same text, and we analyze their impact on various types of
writing issues. To deal with the noisy nature of revision-based corpora, we
propose a new sampling strategy based on revision distance. Opposed to
approaches from prior work, such sampling can be done without employing
additional annotations and judgments. Moreover, we provide evidence that using
contextual information and domain knowledge can further improve prediction
results. How useful a certain type of context is, depends on the issue the
claim is suffering from, though.",None,-1
1b8f69c8-10e9-4d2a-8ad7-57ce9018700f,Anchor Prediction: Automatic Refinement of Internet Links,0.0306489,"Internet links enable users to deepen their understanding of a topic by
providing convenient access to related information. However, the majority of
links are unanchored -- they link to a target webpage as a whole, and readers
may expend considerable effort localizing the specific parts of the target
webpage that enrich their understanding of the link's source context. To help
readers effectively find information in linked webpages, we introduce the task
of anchor prediction, where the goal is to identify the specific part of the
linked target webpage that is most related to the source linking context. We
release the AuthorAnchors dataset, a collection of 34K naturally-occurring
anchored links, which reflect relevance judgments by the authors of the source
article. To model reader relevance judgments, we annotate and release
ReaderAnchors, an evaluation set of anchors that readers find useful. Our
analysis shows that effective anchor prediction often requires jointly
reasoning over lengthy source and target webpages to determine their implicit
relations and identify parts of the target webpage that are related but not
redundant. We benchmark a performant T5-based ranking approach to establish
baseline performance on the task, finding ample room for improvement.",None,-1
8f087f93-0e59-4dda-85ba-a9810ad96ad4,Comparative study of Transformer and LSTM Network with attention mechanism on Image Captioning,0.272431,"In a globalized world at the present epoch of generative intelligence, most
of the manual labour tasks are automated with increased efficiency. This can
support businesses to save time and money. A crucial component of generative
intelligence is the integration of vision and language. Consequently, image
captioning become an intriguing area of research. There have been multiple
attempts by the researchers to solve this problem with different deep learning
architectures, although the accuracy has increased, but the results are still
not up to standard. This study buckles down to the comparison of Transformer
and LSTM with attention block model on MS-COCO dataset, which is a standard
dataset for image captioning. For both the models we have used pretrained
Inception-V3 CNN encoder for feature extraction of the images. The Bilingual
Evaluation Understudy score (BLEU) is used to checked the accuracy of caption
generated by both models. Along with the transformer and LSTM with attention
block models,CLIP-diffusion model, M2-Transformer model and the X-Linear
Attention model have been discussed with state of the art accuracy.",None,-1
564f48b0-3e03-4046-bf4d-435942227500,Gloss-Free End-to-End Sign Language Translation,0.501553,"In this paper, we tackle the problem of sign language translation (SLT)
without gloss annotations. Although intermediate representation like gloss has
been proven effective, gloss annotations are hard to acquire, especially in
large quantities. This limits the domain coverage of translation datasets, thus
handicapping real-world applications. To mitigate this problem, we design the
Gloss-Free End-to-end sign language translation framework (GloFE). Our method
improves the performance of SLT in the gloss-free setting by exploiting the
shared underlying semantics of signs and the corresponding spoken translation.
Common concepts are extracted from the text and used as a weak form of
intermediate representation. The global embedding of these concepts is used as
a query for cross-attention to find the corresponding information within the
learned visual features. In a contrastive manner, we encourage the similarity
of query results between samples containing such concepts and decrease those
that do not. We obtained state-of-the-art results on large-scale datasets,
including OpenASL and How2Sign. The code and model will be available at
https://github.com/HenryLittle/GloFE.",None,-1
07ca5dec-3cb6-444e-9f06-326dcf926d53,Leveraging TCN and Transformer for effective visual-audio fusion in continuous emotion recognition,0.780015,"Human emotion recognition plays an important role in human-computer
interaction. In this paper, we present our approach to the Valence-Arousal (VA)
Estimation Challenge, Expression (Expr) Classification Challenge, and Action
Unit (AU) Detection Challenge of the 5th Workshop and Competition on Affective
Behavior Analysis in-the-wild (ABAW). Specifically, we propose a novel
multi-modal fusion model that leverages Temporal Convolutional Networks (TCN)
and Transformer to enhance the performance of continuous emotion recognition.
Our model aims to effectively integrate visual and audio information for
improved accuracy in recognizing emotions. Our model outperforms the baseline
and ranks 3 in the Expression Classification challenge.",None,-1
16c137ae-5ef9-4b96-a266-04b235bad63e,Multilingual Controllable Transformer-Based Lexical Simplification,0.144759,"Text is by far the most ubiquitous source of knowledge and information and
should be made easily accessible to as many people as possible; however, texts
often contain complex words that hinder reading comprehension and
accessibility. Therefore, suggesting simpler alternatives for complex words
without compromising meaning would help convey the information to a broader
audience. This paper proposes mTLS, a multilingual controllable
Transformer-based Lexical Simplification (LS) system fined-tuned with the T5
model. The novelty of this work lies in the use of language-specific prefixes,
control tokens, and candidates extracted from pre-trained masked language
models to learn simpler alternatives for complex words. The evaluation results
on three well-known LS datasets -- LexMTurk, BenchLS, and NNSEval -- show that
our model outperforms the previous state-of-the-art models like LSBert and
ConLS. Moreover, further evaluation of our approach on the part of the recent
TSAR-2022 multilingual LS shared-task dataset shows that our model performs
competitively when compared with the participating systems for English LS and
even outperforms the GPT-3 model on several metrics. Moreover, our model
obtains performance gains also for Spanish and Portuguese.",None,-1
43364561-f169-49fb-9d3c-73b134274f82,Self-supervised representations in speech-based depression detection,0.972758,"This paper proposes handling training data sparsity in speech-based automatic
depression detection (SDD) using foundation models pre-trained with
self-supervised learning (SSL). An analysis of SSL representations derived from
different layers of pre-trained foundation models is first presented for SDD,
which provides insight to suitable indicator for depression detection.
Knowledge transfer is then performed from automatic speech recognition (ASR)
and emotion recognition to SDD by fine-tuning the foundation models. Results
show that the uses of oracle and ASR transcriptions yield similar SDD
performance when the hidden representations of the ASR model is incorporated
along with the ASR textual information. By integrating representations from
multiple foundation models, state-of-the-art SDD results based on real ASR were
achieved on the DAIC-WOZ dataset.",None,-1
2ad5b8ff-121f-4d1f-bf87-c8fc912288c2,Fine-Tuning Deteriorates General Textual Out-of-Distribution Detection by Distorting Task-Agnostic Features,0.150725,"Detecting out-of-distribution (OOD) inputs is crucial for the safe deployment
of natural language processing (NLP) models. Though existing methods,
especially those based on the statistics in the feature space of fine-tuned
pre-trained language models (PLMs), are claimed to be effective, their
effectiveness on different types of distribution shifts remains underexplored.
In this work, we take the first step to comprehensively evaluate the mainstream
textual OOD detection methods for detecting semantic and non-semantic shifts.
We find that: (1) no existing method behaves well in both settings; (2)
fine-tuning PLMs on in-distribution data benefits detecting semantic shifts but
severely deteriorates detecting non-semantic shifts, which can be attributed to
the distortion of task-agnostic features. To alleviate the issue, we present a
simple yet effective general OOD score named GNOME that integrates the
confidence scores derived from the task-agnostic and task-specific
representations. Experiments show that GNOME works well in both semantic and
non-semantic shift scenarios, and further brings significant improvement on two
cross-task benchmarks where both kinds of shifts simultaneously take place. Our
code is available at https://github.com/lancopku/GNOME.",None,-1
55a2e85e-3028-4008-96ba-26fbe09f650c,Road Extraction with Satellite Images and Partial Road Maps,0.477884,"Road extraction is a process of automatically generating road maps mainly
from satellite images. Existing models all target to generate roads from the
scratch despite that a large quantity of road maps, though incomplete, are
publicly available (e.g. those from OpenStreetMap) and can help with road
extraction. In this paper, we propose to conduct road extraction based on
satellite images and partial road maps, which is new. We then propose a
two-branch Partial to Complete Network (P2CNet) for the task, which has two
prominent components: Gated Self-Attention Module (GSAM) and Missing Part (MP)
loss. GSAM leverages a channel-wise self-attention module and a gate module to
capture long-range semantics, filter out useless information, and better fuse
the features from two branches. MP loss is derived from the partial road maps,
trying to give more attention to the road pixels that do not exist in partial
road maps. Extensive experiments are conducted to demonstrate the effectiveness
of our model, e.g. P2CNet achieves state-of-the-art performance with the IoU
scores of 70.71% and 75.52%, respectively, on the SpaceNet and OSM datasets.",None,-1
a5bbbb29-267a-4a30-a087-88f1efd4e927,TempT: Temporal consistency for Test-time adaptation,0.5716,"We introduce Temporal consistency for Test-time adaptation (TempT) a novel
method for test-time adaptation on videos through the use of temporal coherence
of predictions across sequential frames as a self-supervision signal. TempT is
an approach with broad potential applications in computer vision tasks
including facial expression recognition (FER) in videos. We evaluate TempT
performance on the AffWild2 dataset. Our approach focuses solely on the
unimodal visual aspect of the data and utilizes a popular 2D CNN backbone in
contrast to larger sequential or attention-based models used in other
approaches. Our preliminary experimental results demonstrate that TempT has
competitive performance compared to the previous years reported performances
and its efficacy provides a compelling proof-of-concept for its use in various
real-world applications.",None,-1
cb734dac-5de1-482a-a277-095e362d66be,Forms of Understanding of XAI-Explanations,0.37126,"Explainability has become an important topic in computer science and
artificial intelligence, leading to a subfield called Explainable Artificial
Intelligence (XAI). The goal of providing or seeking explanations is to achieve
(better) 'understanding' on the part of the explainee. However, what it means
to 'understand' is still not clearly defined, and the concept itself is rarely
the subject of scientific investigation. This conceptual article aims to
present a model of forms of understanding in the context of XAI and beyond.
From an interdisciplinary perspective bringing together computer science,
linguistics, sociology, and psychology, a definition of understanding and its
forms, assessment, and dynamics during the process of giving everyday
explanations are explored. Two types of understanding are considered as
possible outcomes of explanations, namely enabledness, 'knowing how' to do or
decide something, and comprehension, 'knowing that' -- both in different
degrees (from shallow to deep). Explanations regularly start with shallow
understanding in a specific domain and can lead to deep comprehension and
enabledness of the explanandum, which we see as a prerequisite for human users
to gain agency. In this process, the increase of comprehension and enabledness
are highly interdependent. Against the background of this systematization,
special challenges of understanding in XAI are discussed.",None,-1
9f1584d1-9482-4ed4-8a8e-298fa528ea92,Balanced Energy Regularization Loss for Out-of-distribution Detection,0.746105,"In the field of out-of-distribution (OOD) detection, a previous method that
use auxiliary data as OOD data has shown promising performance. However, the
method provides an equal loss to all auxiliary data to differentiate them from
inliers. However, based on our observation, in various tasks, there is a
general imbalance in the distribution of the auxiliary OOD data across classes.
We propose a balanced energy regularization loss that is simple but generally
effective for a variety of tasks. Our balanced energy regularization loss
utilizes class-wise different prior probabilities for auxiliary data to address
the class imbalance in OOD data. The main concept is to regularize auxiliary
samples from majority classes, more heavily than those from minority classes.
Our approach performs better for OOD detection in semantic segmentation,
long-tailed image classification, and image classification than the prior
energy regularization loss. Furthermore, our approach achieves state-of-the-art
performance in two tasks: OOD detection in semantic segmentation and
long-tailed image classification. Code is available at
https://github.com/hyunjunChhoi/Balanced_Energy.",None,-1
62ee42be-d0e9-4177-974b-01ac7feefcb5,Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision,0.339314,"Most existing task-oriented dialog (TOD) systems track dialog states in terms
of slots and values and use them to query a database to get relevant knowledge
to generate responses. In real-life applications, user utterances are noisier,
and thus it is more difficult to accurately track dialog states and correctly
secure relevant knowledge. Recently, a progress in question answering and
document-grounded dialog systems is retrieval-augmented methods with a
knowledge retriever. Inspired by such progress, we propose a retrieval-based
method to enhance knowledge selection in TOD systems, which significantly
outperforms the traditional database query method for real-life dialogs.
Further, we develop latent variable model based semi-supervised learning, which
can work with the knowledge retriever to leverage both labeled and unlabeled
dialog data. Joint Stochastic Approximation (JSA) algorithm is employed for
semi-supervised model training, and the whole system is referred to as that
JSA-KRTOD. Experiments are conducted on a real-life dataset from China Mobile
Custom-Service, called MobileCS, and show that JSA-KRTOD achieves superior
performances in both labeled-only and semi-supervised settings.",None,-1
06d2d6cd-8aa9-454b-82db-860dcf316a06,Self-Enhancement Improves Text-Image Retrieval in Foundation Visual-Language Models,0.110461,"The emergence of cross-modal foundation models has introduced numerous
approaches grounded in text-image retrieval. However, on some domain-specific
retrieval tasks, these models fail to focus on the key attributes required. To
address this issue, we propose a self-enhancement framework, A^{3}R, based on
the CLIP-ViT/G-14, one of the largest cross-modal models. First, we perform an
Attribute Augmentation strategy to enrich the textual description for
fine-grained representation before model learning. Then, we propose an Adaption
Re-ranking method to unify the representation space of textual query and
candidate images and re-rank candidate images relying on the adapted query
after model learning. The proposed framework is validated to achieve a salient
improvement over the baseline and other teams' solutions in the cross-modal
image retrieval track of the 1st foundation model challenge without introducing
any additional samples. The code is available at
\url{https://github.com/CapricornGuang/A3R}.",None,-1
f41118f2-0bd5-41ac-8252-f2cdb8af6a53,Patched Line Segment Learning for Vector Road Mapping,0.454504,"This paper presents a novel approach to computing vector road maps from
satellite remotely sensed images, building upon a well-defined Patched Line
Segment (PaLiS) representation for road graphs that holds geometric
significance. Unlike prevailing methods that derive road vector representations
from satellite images using binary masks or keypoints, our method employs line
segments. These segments not only convey road locations but also capture their
orientations, making them a robust choice for representation. More precisely,
given an input image, we divide it into non-overlapping patches and predict a
suitable line segment within each patch. This strategy enables us to capture
spatial and structural cues from these patch-based line segments, simplifying
the process of constructing the road network graph without the necessity of
additional neural networks for connectivity. In our experiments, we demonstrate
how an effective representation of a road graph significantly enhances the
performance of vector road mapping on established benchmarks, without requiring
extensive modifications to the neural network architecture. Furthermore, our
method achieves state-of-the-art performance with just 6 GPU hours of training,
leading to a substantial 32-fold reduction in training costs in terms of GPU
hours.",None,-1
ca8dafd9-19b7-4764-b369-e01b99411d57,Iterative Deepening Hyperband,0.100869,"Hyperparameter optimization (HPO) is concerned with the automated search for
the most appropriate hyperparameter configuration (HPC) of a parameterized
machine learning algorithm. A state-of-the-art HPO method is Hyperband, which,
however, has its own parameters that influence its performance. One of these
parameters, the maximal budget, is especially problematic: If chosen too small,
the budget needs to be increased in hindsight and, as Hyperband is not
incremental by design, the entire algorithm must be re-run. This is not only
costly but also comes with a loss of valuable knowledge already accumulated. In
this paper, we propose incremental variants of Hyperband that eliminate these
drawbacks, and show that these variants satisfy theoretical guarantees
qualitatively similar to those for the original Hyperband with the ""right""
budget. Moreover, we demonstrate their practical utility in experiments with
benchmark data sets.",None,-1
70dbf092-be77-4be9-bf75-a3337849cb1e,A Comprehensive Evaluation and Analysis Study for Chinese Spelling Check,0.314165,"With the development of pre-trained models and the incorporation of phonetic
and graphic information, neural models have achieved high scores in Chinese
Spelling Check (CSC). However, it does not provide a comprehensive reflection
of the models' capability due to the limited test sets. In this study, we
abstract the representative model paradigm, implement it with nine structures
and experiment them on comprehensive test sets we constructed with different
purposes. We perform a detailed analysis of the results and find that: 1)
Fusing phonetic and graphic information reasonably is effective for CSC. 2)
Models are sensitive to the error distribution of the test set, which reflects
the shortcomings of models and reveals the direction we should work on. 3)
Whether or not the errors and contexts have been seen has a significant impact
on models. 4) The commonly used benchmark, SIGHAN, can not reliably evaluate
models' performance.",None,-1
a5cd9dda-415b-464e-951f-440fb29e6a9a,Jigsaw: Learning to Assemble Multiple Fractured Objects,0.869065,"Automated assembly of 3D fractures is essential in orthopedics, archaeology,
and our daily life. This paper presents Jigsaw, a novel framework for
assembling physically broken 3D objects from multiple pieces. Our approach
leverages hierarchical features of global and local geometry to match and align
the fracture surfaces. Our framework consists of four components: (1) front-end
point feature extractor with attention layers, (2) surface segmentation to
separate fracture and original parts, (3) multi-parts matching to find
correspondences among fracture surface points, and (4) robust global alignment
to recover the global poses of the pieces. We show how to jointly learn
segmentation and matching and seamlessly integrate feature matching and
rigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and
achieve superior performance compared to state-of-the-art methods. Our method
also generalizes well to diverse fracture modes, objects, and unseen instances.
To the best of our knowledge, this is the first learning-based method designed
specifically for 3D fracture assembly over multiple pieces. Our code is
available at https://jiaxin-lu.github.io/Jigsaw/.",None,-1
382e7460-61bd-401e-afa3-a9bbd3aefcb3,LibriSpeech-PC: Benchmark for Evaluation of Punctuation and Capitalization Capabilities of end-to-end ASR Models,0.626541,"Traditional automatic speech recognition (ASR) models output lower-cased
words without punctuation marks, which reduces readability and necessitates a
subsequent text processing model to convert ASR transcripts into a proper
format. Simultaneously, the development of end-to-end ASR models capable of
predicting punctuation and capitalization presents several challenges,
primarily due to limited data availability and shortcomings in the existing
evaluation methods, such as inadequate assessment of punctuation prediction. In
this paper, we introduce a LibriSpeech-PC benchmark designed to assess the
punctuation and capitalization prediction capabilities of end-to-end ASR
models. The benchmark includes a LibriSpeech-PC dataset with restored
punctuation and capitalization, a novel evaluation metric called Punctuation
Error Rate (PER) that focuses on punctuation marks, and initial baseline
models. All code, data, and models are publicly available.",None,-1
120d1da2-2413-49e8-8a13-512759013be8,Automatic Readability Assessment for Closely Related Languages,0.906605,"In recent years, the main focus of research on automatic readability
assessment (ARA) has shifted towards using expensive deep learning-based
methods with the primary goal of increasing models' accuracy. This, however, is
rarely applicable for low-resource languages where traditional handcrafted
features are still widely used due to the lack of existing NLP tools to extract
deeper linguistic representations. In this work, we take a step back from the
technical component and focus on how linguistic aspects such as mutual
intelligibility or degree of language relatedness can improve ARA in a
low-resource setting. We collect short stories written in three languages in
the Philippines-Tagalog, Bikol, and Cebuano-to train readability assessment
models and explore the interaction of data and features in various
cross-lingual setups. Our results show that the inclusion of CrossNGO, a novel
specialized feature exploiting n-gram overlap applied to languages with high
mutual intelligibility, significantly improves the performance of ARA models
compared to the use of off-the-shelf large multilingual language models alone.
Consequently, when both linguistic representations are combined, we achieve
state-of-the-art results for Tagalog and Cebuano, and baseline scores for ARA
in Bikol.",None,-1
d30eb8e0-59fa-406d-92cc-6ef2e8c717a9,Local Implicit Ray Function for Generalizable Radiance Field Representation,0.585239,"We propose LIRF (Local Implicit Ray Function), a generalizable neural
rendering approach for novel view rendering. Current generalizable neural
radiance fields (NeRF) methods sample a scene with a single ray per pixel and
may therefore render blurred or aliased views when the input views and rendered
views capture scene content with different resolutions. To solve this problem,
we propose LIRF to aggregate the information from conical frustums to construct
a ray. Given 3D positions within conical frustums, LIRF takes 3D coordinates
and the features of conical frustums as inputs and predicts a local volumetric
radiance field. Since the coordinates are continuous, LIRF renders high-quality
novel views at a continuously-valued scale via volume rendering. Besides, we
predict the visible weights for each input view via transformer-based feature
matching to improve the performance in occluded areas. Experimental results on
real-world scenes validate that our method outperforms state-of-the-art methods
on novel view rendering of unseen scenes at arbitrary scales.",None,-1
deee4bd0-68a1-4270-b56c-94b127bb7538,Internal-External Boundary Attention Fusion for Glass Surface Segmentation,0.623397,"Glass surfaces of transparent objects and mirrors are not able to be uniquely
and explicitly characterized by their visual appearances because they contain
the visual appearance of other reflected or transmitted surfaces as well.
Detecting glass regions from a single-color image is a challenging task. Recent
deep-learning approaches have paid attention to the description of glass
surface boundary where the transition of visual appearances between glass and
non-glass surfaces are observed. In this work, we analytically investigate how
glass surface boundary helps to characterize glass objects. Inspired by prior
semantic segmentation approaches with challenging image types such as X-ray or
CT scans, we propose separated internal-external boundary attention modules
that individually learn and selectively integrate visual characteristics of the
inside and outside region of glass surface from a single color image. Our
proposed method is evaluated on six public benchmarks comparing with
state-of-the-art methods showing promising results.",None,-1
06f0f0f3-c2c5-4ed7-a16b-1fc67d2d66dc,Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer,0.616643,"Diffusion models have shown great promise in text-guided image style
transfer, but there is a trade-off between style transformation and content
preservation due to their stochastic nature. Existing methods require
computationally expensive fine-tuning of diffusion models or additional neural
network. To address this, here we propose a zero-shot contrastive loss for
diffusion models that doesn't require additional fine-tuning or auxiliary
networks. By leveraging patch-wise contrastive loss between generated samples
and original image embeddings in the pre-trained diffusion model, our method
can generate images with the same semantic content as the source image in a
zero-shot manner. Our approach outperforms existing methods while preserving
content and requiring no additional training, not only for image style transfer
but also for image-to-image translation and manipulation. Our experimental
results validate the effectiveness of our proposed method.",None,-1
5153bf89-243b-4351-809d-380aeebe52dc,BoPR: Body-aware Part Regressor for Human Shape and Pose Estimation,0.279296,"This paper presents a novel approach for estimating human body shape and pose
from monocular images that effectively addresses the challenges of occlusions
and depth ambiguity. Our proposed method BoPR, the Body-aware Part Regressor,
first extracts features of both the body and part regions using an
attention-guided mechanism. We then utilize these features to encode extra
part-body dependency for per-part regression, with part features as queries and
body feature as a reference. This allows our network to infer the spatial
relationship of occluded parts with the body by leveraging visible parts and
body reference information. Our method outperforms existing state-of-the-art
methods on two benchmark datasets, and our experiments show that it
significantly surpasses existing methods in terms of depth ambiguity and
occlusion handling. These results provide strong evidence of the effectiveness
of our approach.The code and data are available for research purposes at
https://github.com/cyk990422/BoPR.",None,-1
1c003e55-88cb-4eb5-ad8f-3641187ed5ae,ImageBind: One Embedding Space To Bind Them All,1.0,"We present ImageBind, an approach to learn a joint embedding across six
different modalities - images, text, audio, depth, thermal, and IMU data. We
show that all combinations of paired data are not necessary to train such a
joint embedding, and only image-paired data is sufficient to bind the
modalities together. ImageBind can leverage recent large scale vision-language
models, and extends their zero-shot capabilities to new modalities just by
using their natural pairing with images. It enables novel emergent applications
'out-of-the-box' including cross-modal retrieval, composing modalities with
arithmetic, cross-modal detection and generation. The emergent capabilities
improve with the strength of the image encoder and we set a new
state-of-the-art on emergent zero-shot recognition tasks across modalities,
outperforming specialist supervised models. Finally, we show strong few-shot
recognition results outperforming prior work, and that ImageBind serves as a
new way to evaluate vision models for visual and non-visual tasks.",None,-1
660f9f13-f18a-4789-bd01-88fd82a7ec75,Zero-shot Generation of Training Data with Denoising Diffusion Probabilistic Model for Handwritten Chinese Character Recognition,0.993307,"There are more than 80,000 character categories in Chinese while most of them
are rarely used. To build a high performance handwritten Chinese character
recognition (HCCR) system supporting the full character set with a traditional
approach, many training samples need be collected for each character category,
which is both time-consuming and expensive. In this paper, we propose a novel
approach to transforming Chinese character glyph images generated from font
libraries to handwritten ones with a denoising diffusion probabilistic model
(DDPM). Training from handwritten samples of a small character set, the DDPM is
capable of mapping printed strokes to handwritten ones, which makes it possible
to generate photo-realistic and diverse style handwritten samples of unseen
character categories. Combining DDPM-synthesized samples of unseen categories
with real samples of other categories, we can build an HCCR system to support
the full character set. Experimental results on CASIA-HWDB dataset with 3,755
character categories show that the HCCR systems trained with synthetic samples
perform similarly with the one trained with real samples in terms of
recognition accuracy. The proposed method has the potential to address HCCR
with a larger vocabulary.",None,-1
13bd8223-b00d-49f8-af8d-7ee3a9e11a6a,When Prompt-based Incremental Learning Does Not Meet Strong Pretraining,0.658794,"Incremental learning aims to overcome catastrophic forgetting when learning
deep networks from sequential tasks. With impressive learning efficiency and
performance, prompt-based methods adopt a fixed backbone to sequential tasks by
learning task-specific prompts. However, existing prompt-based methods heavily
rely on strong pretraining (typically trained on ImageNet-21k), and we find
that their models could be trapped if the potential gap between the pretraining
task and unknown future tasks is large. In this work, we develop a learnable
Adaptive Prompt Generator (APG). The key is to unify the prompt retrieval and
prompt learning processes into a learnable prompt generator. Hence, the whole
prompting process can be optimized to reduce the negative effects of the gap
between tasks effectively. To make our APG avoid learning ineffective
knowledge, we maintain a knowledge pool to regularize APG with the feature
distribution of each class. Extensive experiments show that our method
significantly outperforms advanced methods in exemplar-free incremental
learning without (strong) pretraining. Besides, under strong retraining, our
method also has comparable performance to existing prompt-based models, showing
that our method can still benefit from pretraining. Codes can be found at
https://github.com/TOM-tym/APG",None,-1
3b8db9c5-9527-42f6-b73e-339b68f31a50,LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection,0.684859,"As malicious actors employ increasingly advanced and widespread bots to
disseminate misinformation and manipulate public opinion, the detection of
Twitter bots has become a crucial task. Though graph-based Twitter bot
detection methods achieve state-of-the-art performance, we find that their
inference depends on the neighbor users multi-hop away from the targets, and
fetching neighbors is time-consuming and may introduce bias. At the same time,
we find that after finetuning on Twitter bot detection, pretrained language
models achieve competitive performance and do not require a graph structure
during deployment. Inspired by this finding, we propose a novel bot detection
framework LMBot that distills the knowledge of graph neural networks (GNNs)
into language models (LMs) for graph-less deployment in Twitter bot detection
to combat the challenge of data dependency. Moreover, LMBot is compatible with
graph-based and graph-less datasets. Specifically, we first represent each user
as a textual sequence and feed them into the LM for domain adaptation. For
graph-based datasets, the output of LMs provides input features for the GNN,
enabling it to optimize for bot detection and distill knowledge back to the LM
in an iterative, mutually enhancing process. Armed with the LM, we can perform
graph-less inference, which resolves the graph data dependency and sampling
bias issues. For datasets without graph structure, we simply replace the GNN
with an MLP, which has also shown strong performance. Our experiments
demonstrate that LMBot achieves state-of-the-art performance on four Twitter
bot detection benchmarks. Extensive studies also show that LMBot is more
robust, versatile, and efficient compared to graph-based Twitter bot detection
methods.",None,-1
3a370697-5728-43f8-83e7-8656b463f348,PolyVoice: Language Models for Speech to Speech Translation,0.988634,"We propose PolyVoice, a language model-based framework for speech-to-speech
translation (S2ST) system. Our framework consists of two language models: a
translation language model and a speech synthesis language model. We use
discretized speech units, which are generated in a fully unsupervised way, and
thus our framework can be used for unwritten languages. For the speech
synthesis part, we adopt the existing VALL-E X approach and build a unit-based
audio language model. This grants our framework the ability to preserve the
voice characteristics and the speaking style of the original speech. We examine
our system on Chinese $\rightarrow$ English and English $\rightarrow$ Spanish
pairs. Experimental results show that our system can generate speech with high
translation quality and audio quality. Speech samples are available at
https://speechtranslation.github.io/polyvoice.",None,-1
61c179ec-24a4-49a2-bcd4-4249a4f02f82,Investigations on convergence behaviour of Physics Informed Neural Networks across spectral ranges and derivative orders,0.256368,"An important inference from Neural Tangent Kernel (NTK) theory is the
existence of spectral bias (SB), that is, low frequency components of the
target function of a fully connected Artificial Neural Network (ANN) being
learnt significantly faster than the higher frequencies during training. This
is established for Mean Square Error (MSE) loss functions with very low
learning rate parameters. Physics Informed Neural Networks (PINNs) are designed
to learn the solutions of differential equations (DE) of arbitrary orders; in
PINNs the loss functions are obtained as the residues of the conservative form
of the DEs and represent the degree of dissatisfaction of the equations. So
there has been an open question whether (a) PINNs also exhibit SB and (b) if
so, how does this bias vary across the orders of the DEs. In this work, a
series of numerical experiments are conducted on simple sinusoidal functions of
varying frequencies, compositions and equation orders to investigate these
issues. It is firmly established that under normalized conditions, PINNs do
exhibit strong spectral bias, and this increases with the order of the
differential equation.",None,-1
dd8ec315-da88-4ae2-af0c-5613cc360e8b,Learning Better Keypoints for Multi-Object 6DoF Pose Estimation,0.356554,"We address the problem of keypoint selection, and find that the performance
of 6DoF pose estimation methods can be improved when pre-defined keypoint
locations are learned, rather than being heuristically selected as has been the
standard approach. We found that accuracy and efficiency can be improved by
training a graph network to select a set of disperse keypoints with similarly
distributed votes. These votes, learned by a regression network to accumulate
evidence for the keypoint locations, can be regressed more accurately compared
to previous heuristic keypoint algorithms. The proposed KeyGNet, supervised by
a combined loss measuring both Wasserstein distance and dispersion, learns the
color and geometry features of the target objects to estimate optimal keypoint
locations. Experiments demonstrate the keypoints selected by KeyGNet improved
the accuracy for all evaluation metrics of all seven datasets tested, for three
keypoint voting methods. The challenging Occlusion LINEMOD dataset notably
improved ADD(S) by +16.4% on PVN3D, and all core BOP datasets showed an AR
improvement for all objects, of between +1% and +21.5%. There was also a
notable increase in performance when transitioning from single object to
multiple object training using KeyGNet keypoints, essentially eliminating the
SISO-MIMO gap for Occlusion LINEMOD.",None,-1
69e1b657-08f7-415e-baeb-9b8ceed7c099,A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning,0.435382,"Logical reasoning has been an ongoing pursuit in the field of AI. Despite
significant advancements made by large language models (LLMs), they still
struggle with complex logical reasoning problems. To enhance reasoning
performance, one promising direction is scalable oversight, which requires LLMs
to identify their own errors and then improve by themselves. Various
self-verification methods have been proposed in pursuit of this goal.
Nevertheless, whether existing models understand their own errors well is still
under investigation. In this paper, we take a closer look at the
self-verification abilities of LLMs in the context of logical reasoning,
focusing on their ability to identify logical fallacies accurately. We
introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies
categorized in a hierarchical taxonomy. By conducting exhaustive experiments on
FALLACIES, we obtain comprehensive and detailed analyses of a series of models
on their verification abilities. Our main findings suggest that existing LLMs
could struggle to identify fallacious reasoning steps accurately and may fall
short of guaranteeing the validity of self-verification methods. Drawing from
these observations, we offer suggestions for future research and practical
applications of self-verification methods.",None,-1
3d655189-51ef-45ca-b036-3b6588b9113b,Multi-Agent Consensus Seeking via Large Language Models,0.831454,"Multi-agent systems driven by large language models (LLMs) have shown
promising abilities for solving complex tasks in a collaborative manner. This
work considers a fundamental problem in multi-agent collaboration: consensus
seeking. When multiple agents work together, we are interested in how they can
reach a consensus through inter-agent negotiation. To that end, this work
studies a consensus-seeking task where the state of each agent is a numerical
value and they negotiate with each other to reach a consensus value. It is
revealed that when not explicitly directed on which strategy should be adopted,
the LLM-driven agents primarily use the average strategy for consensus seeking
although they may occasionally use some other strategies. Moreover, this work
analyzes the impact of the agent number, agent personality, and network
topology on the negotiation process. The findings reported in this work can
potentially lay the foundations for understanding the behaviors of LLM-driven
multi-agent systems for solving more complex tasks. Furthermore, LLM-driven
consensus seeking is applied to a multi-robot aggregation task. This
application demonstrates the potential of LLM-driven agents to achieve
zero-shot autonomous planning for multi-robot collaboration tasks. Project
website: westlakeintelligentrobotics.github.io/ConsensusLLM/.",None,-1
8f495bef-83cc-4418-aada-9e3270135040,Language Models Trained on Media Diets Can Predict Public Opinion,0.880255,"Public opinion reflects and shapes societal behavior, but the traditional
survey-based tools to measure it are limited. We introduce a novel approach to
probe media diet models -- language models adapted to online news, TV
broadcast, or radio show content -- that can emulate the opinions of
subpopulations that have consumed a set of media. To validate this method, we
use as ground truth the opinions expressed in U.S. nationally representative
surveys on COVID-19 and consumer confidence. Our studies indicate that this
approach is (1) predictive of human judgements found in survey response
distributions and robust to phrasing and channels of media exposure, (2) more
accurate at modeling people who follow media more closely, and (3) aligned with
literature on which types of opinions are affected by media consumption.
Probing language models provides a powerful new method for investigating media
effects, has practical applications in supplementing polls and forecasting
public opinion, and suggests a need for further study of the surprising
fidelity with which neural language models can predict human responses.",None,-1
4cc29728-94d9-41d0-aa6b-bf89c335b4d2,Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding,0.494303,"Negative sampling (NS) is widely used in knowledge graph embedding (KGE),
which aims to generate negative triples to make a positive-negative contrast
during training. However, existing NS methods are unsuitable when multi-modal
information is considered in KGE models. They are also inefficient due to their
complex design. In this paper, we propose Modality-Aware Negative Sampling
(MANS) for multi-modal knowledge graph embedding (MMKGE) to address the
mentioned problems. MANS could align structural and visual embeddings for
entities in KGs and learn meaningful embeddings to perform better in
multi-modal KGE while keeping lightweight and efficient. Empirical results on
two benchmarks demonstrate that MANS outperforms existing NS methods.
Meanwhile, we make further explorations about MANS to confirm its
effectiveness.",None,-1
9c373bba-3e88-430d-9d36-3debb225f7a4,Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition,0.431261,"In a practical dialogue system, users may input out-of-domain (OOD) queries.
The Generalized Intent Discovery (GID) task aims to discover OOD intents from
OOD queries and extend them to the in-domain (IND) classifier. However, GID
only considers one stage of OOD learning, and needs to utilize the data in all
previous stages for joint training, which limits its wide application in
reality. In this paper, we introduce a new task, Continual Generalized Intent
Discovery (CGID), which aims to continuously and automatically discover OOD
intents from dynamic OOD data streams and then incrementally add them to the
classifier with almost no previous data, thus moving towards dynamic intent
recognition in an open world. Next, we propose a method called Prototype-guided
Learning with Replay and Distillation (PLRD) for CGID, which bootstraps new
intent discovery through class prototypes and balances new and old intents
through data replay and feature distillation. Finally, we conduct detailed
experiments and analysis to verify the effectiveness of PLRD and understand the
key challenges of CGID for future research.",None,-1
941ef0c2-258d-4122-8bb6-437f14b9fdf3,Adaptive Reconvergence-driven AIG Rewriting via Strategy Learning,0.102072,"Rewriting is a common procedure in logic synthesis aimed at improving the
performance, power, and area (PPA) of circuits. The traditional
reconvergence-driven And-Inverter Graph (AIG) rewriting method focuses solely
on optimizing the reconvergence cone through Boolean algebra minimization.
However, there exist opportunities to incorporate other node-rewriting
algorithms that are better suited for specific cones. In this paper, we propose
an adaptive reconvergence-driven AIG rewriting algorithm that combines two key
techniques: multi-strategy-based AIG rewriting and strategy learning-based
algorithm selection. The multi-strategy-based rewriting method expands upon the
traditional approach by incorporating support for multi-node-rewriting
algorithms, thus expanding the optimization space. Additionally, the strategy
learning-based algorithm selection method determines the most suitable
node-rewriting algorithm for a given cone. Experimental results demonstrate
that our proposed method yields a significant average improvement of 5.567\% in
size and 5.327\% in depth.",None,-1
28b28256-bb5d-4916-add8-e5b23f98ebde,"Expository Text Generation: Imitate, Retrieve, Paraphrase",0.0573156,"Expository documents are vital resources for conveying complex information to
readers. Despite their usefulness, writing expository text by hand is a
challenging process that requires careful content planning, obtaining facts
from multiple sources, and the ability to clearly synthesize these facts. To
ease these burdens, we propose the task of expository text generation, which
seeks to automatically generate an accurate and stylistically consistent
expository text for a topic by intelligently searching a knowledge source. We
solve our task by developing IRP, a framework that overcomes the limitations of
retrieval-augmented models and iteratively performs content planning, fact
retrieval, and rephrasing. Through experiments on three diverse,
newly-collected datasets, we show that IRP produces factual and organized
expository texts that accurately inform readers.",None,-1
75a1dea2-7646-45d7-811c-9285b4c5ff91,Are words equally surprising in audio and audio-visual comprehension?,0.129558,"We report a controlled study investigating the effect of visual information
(i.e., seeing the speaker) on spoken language comprehension. We compare the ERP
signature (N400) associated with each word in audio-only and audio-visual
presentations of the same verbal stimuli. We assess the extent to which
surprisal measures (which quantify the predictability of words in their lexical
context) are generated on the basis of different types of language models
(specifically n-gram and Transformer models) that predict N400 responses for
each word. Our results indicate that cognitive effort differs significantly
between multimodal and unimodal settings. In addition, our findings suggest
that while Transformer-based models, which have access to a larger lexical
context, provide a better fit in the audio-only setting, 2-gram language models
are more effective in the multimodal setting. This highlights the significant
impact of local lexical context on cognitive processing in a multimodal
environment.",None,-1
28b66d1c-b1e6-47f6-8d16-e9c5a5165404,Sentence-Incremental Neural Coreference Resolution,0.111925,"We propose a sentence-incremental neural coreference resolution system which
incrementally builds clusters after marking mention boundaries in a
shift-reduce method. The system is aimed at bridging two recent approaches at
coreference resolution: (1) state-of-the-art non-incremental models that incur
quadratic complexity in document length with high computational cost, and (2)
memory network-based models which operate incrementally but do not generalize
beyond pronouns. For comparison, we simulate an incremental setting by
constraining non-incremental systems to form partial coreference chains before
observing new sentences. In this setting, our system outperforms comparable
state-of-the-art methods by 2 F1 on OntoNotes and 7 F1 on the CODI-CRAC 2021
corpus. In a conventional coreference setup, our system achieves 76.3 F1 on
OntoNotes and 45.8 F1 on CODI-CRAC 2021, which is comparable to
state-of-the-art baselines. We also analyze variations of our system and show
that the degree of incrementality in the encoder has a surprisingly large
effect on the resulting performance.",None,-1
82083f41-476e-4aa0-a9ad-bcee160427ff,Human-machine cooperation for semantic feature listing,0.0862105,"Semantic feature norms, lists of features that concepts do and do not
possess, have played a central role in characterizing human conceptual
knowledge, but require extensive human labor. Large language models (LLMs)
offer a novel avenue for the automatic generation of such feature lists, but
are prone to significant error. Here, we present a new method for combining a
learned model of human lexical-semantics from limited data with LLM-generated
data to efficiently generate high-quality feature norms.",None,-1
7c96c149-eaec-414e-80c8-a29508c98eb3,Contrastive Feature Masking Open-Vocabulary Vision Transformer,0.908116,"We present Contrastive Feature Masking Vision Transformer (CFM-ViT) - an
image-text pretraining methodology that achieves simultaneous learning of
image- and region-level representation for open-vocabulary object detection
(OVD). Our approach combines the masked autoencoder (MAE) objective into the
contrastive learning objective to improve the representation for localization
tasks. Unlike standard MAE, we perform reconstruction in the joint image-text
embedding space, rather than the pixel space as is customary with the classical
MAE method, which causes the model to better learn region-level semantics.
Moreover, we introduce Positional Embedding Dropout (PED) to address scale
variation between image-text pretraining and detection finetuning by randomly
dropping out the positional embeddings during pretraining. PED improves
detection performance and enables the use of a frozen ViT backbone as a region
classifier, preventing the forgetting of open-vocabulary knowledge during
detection finetuning. On LVIS open-vocabulary detection benchmark, CFM-ViT
achieves a state-of-the-art 33.9 AP$r$, surpassing the best approach by 7.6
points and achieves better zero-shot detection transfer. Finally, CFM-ViT
acquires strong image-level representation, outperforming the state of the art
on 8 out of 12 metrics on zero-shot image-text retrieval benchmarks.",None,-1
6a14636b-68a3-4426-979b-195ceed239bd,FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration,0.987484,"We propose an end-to-end deep learning approach to rectify fisheye images and
simultaneously calibrate camera intrinsic and distortion parameters. Our method
consists of two parts: a Quick Image Rectification Module developed with a
Pix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with a
CNN architecture. Our Quick Rectification Network performs robust rectification
with good resolution, making it suitable for constant calibration in
camera-based surveillance equipment. To achieve high-quality calibration, we
use the straightened output from the Quick Rectification Module as a
guidance-like semantic feature map for the Calibration Module to learn the
geometric relationship between the straightened feature and the distorted
feature. We train and validate our method with a large synthesized dataset
labeled with well-simulated parameters applied to a perspective image dataset.
Our solution has achieved robust performance in high-resolution with a
significant PSNR value of 22.343.",None,-1
71a65d6b-22c5-42bf-a180-d62e919b62b1,Toward Unsupervised 3D Point Cloud Anomaly Detection using Variational Autoencoder,0.515316,"In this paper, we present an end-to-end unsupervised anomaly detection
framework for 3D point clouds. To the best of our knowledge, this is the first
work to tackle the anomaly detection task on a general object represented by a
3D point cloud. We propose a deep variational autoencoder-based unsupervised
anomaly detection network adapted to the 3D point cloud and an anomaly score
specifically for 3D point clouds. To verify the effectiveness of the model, we
conducted extensive experiments on the ShapeNet dataset. Through quantitative
and qualitative evaluation, we demonstrate that the proposed method outperforms
the baseline method. Our code is available at
https://github.com/llien30/point_cloud_anomaly_detection.",None,-1
3616370b-6143-4a21-b483-b573586016c8,Towards Self-Explainability of Deep Neural Networks with Heatmap Captioning and Large-Language Models,0.506483,"Heatmaps are widely used to interpret deep neural networks, particularly for
computer vision tasks, and the heatmap-based explainable AI (XAI) techniques
are a well-researched topic. However, most studies concentrate on enhancing the
quality of the generated heatmap or discovering alternate heatmap generation
techniques, and little effort has been devoted to making heatmap-based XAI
automatic, interactive, scalable, and accessible. To address this gap, we
propose a framework that includes two modules: (1) context modelling and (2)
reasoning. We proposed a template-based image captioning approach for context
modelling to create text-based contextual information from the heatmap and
input data. The reasoning module leverages a large language model to provide
explanations in combination with specialised knowledge. Our qualitative
experiments demonstrate the effectiveness of our framework and heatmap
captioning approach. The code for the proposed template-based heatmap
captioning approach will be publicly available.",None,-1
6acc5b5b-6207-4321-8ebe-d8f2cb96c181,Align and Attend: Multimodal Summarization with Dual Contrastive Losses,0.449563,"The goal of multimodal summarization is to extract the most important
information from different modalities to form output summaries. Unlike the
unimodal summarization, the multimodal summarization task explicitly leverages
cross-modal information to help generate more reliable and high-quality
summaries. However, existing methods fail to leverage the temporal
correspondence between different modalities and ignore the intrinsic
correlation between different samples. To address this issue, we introduce
Align and Attend Multimodal Summarization (A2Summ), a unified multimodal
transformer-based model which can effectively align and attend the multimodal
input. In addition, we propose two novel contrastive losses to model both
inter-sample and intra-sample correlations. Extensive experiments on two
standard video summarization datasets (TVSum and SumMe) and two multimodal
summarization datasets (Daily Mail and CNN) demonstrate the superiority of
A2Summ, achieving state-of-the-art performances on all datasets. Moreover, we
collected a large-scale multimodal summarization dataset BLiSS, which contains
livestream videos and transcribed texts with annotated summaries. Our code and
dataset are publicly available at ~\url{https://boheumd.github.io/A2Summ/}.",None,-1
448de281-1067-475c-b4e1-9452c1741433,Meta-Learned Models of Cognition,0.725674,"Meta-learning is a framework for learning learning algorithms through
repeated interactions with an environment as opposed to designing them by hand.
In recent years, this framework has established itself as a promising tool for
building models of human cognition. Yet, a coherent research program around
meta-learned models of cognition is still missing. The purpose of this article
is to synthesize previous work in this field and establish such a research
program. We rely on three key pillars to accomplish this goal. We first point
out that meta-learning can be used to construct Bayes-optimal learning
algorithms. This result not only implies that any behavioral phenomenon that
can be explained by a Bayesian model can also be explained by a meta-learned
model but also allows us to draw strong connections to the rational analysis of
cognition. We then discuss several advantages of the meta-learning framework
over traditional Bayesian methods. In particular, we argue that meta-learning
can be applied to situations where Bayesian inference is impossible and that it
enables us to make rational models of cognition more realistic, either by
incorporating limited computational resources or neuroscientific knowledge.
Finally, we reexamine prior studies from psychology and neuroscience that have
applied meta-learning and put them into the context of these new insights. In
summary, our work highlights that meta-learning considerably extends the scope
of rational analysis and thereby of cognitive theories more generally.",None,-1
ece2d773-f7b8-486e-88c3-d20e8415379c,Efficient OCR for Building a Diverse Digital History,0.91978,"Thousands of users consult digital archives daily, but the information they
can access is unrepresentative of the diversity of documentary history. The
sequence-to-sequence architecture typically used for optical character
recognition (OCR) - which jointly learns a vision and language model - is
poorly extensible to low-resource document collections, as learning a
language-vision model requires extensive labeled sequences and compute. This
study models OCR as a character level image retrieval problem, using a
contrastively trained vision encoder. Because the model only learns characters'
visual features, it is more sample efficient and extensible than existing
architectures, enabling accurate OCR in settings where existing solutions fail.
Crucially, the model opens new avenues for community engagement in making
digital history more representative of documentary history.",None,-1
6d23c438-a719-47b3-8d39-c9870412f1eb,Parameter-efficient Modularised Bias Mitigation via AdapterFusion,0.857569,"Large pre-trained language models contain societal biases and carry along
these biases to downstream tasks. Current in-processing bias mitigation
approaches (like adversarial training) impose debiasing by updating a model's
parameters, effectively transferring the model to a new, irreversible debiased
state. In this work, we propose a novel approach to develop stand-alone
debiasing functionalities separate from the model, which can be integrated into
the model on-demand, while keeping the core model untouched. Drawing from the
concept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing
with Adapter Modules) - a debiasing approach to first encapsulate arbitrary
bias mitigation functionalities into separate adapters, and then add them to
the model on-demand in order to deliver fairness qualities. We conduct a large
set of experiments on three classification tasks with gender, race, and age as
protected attributes. Our results show that DAM improves or maintains the
effectiveness of bias mitigation, avoids catastrophic forgetting in a
multi-attribute scenario, and maintains on-par task performance, while granting
parameter-efficiency and easy switching between the original and debiased
models.",None,-1
aa93a1e5-4cb9-40fa-aaae-baa178f5b744,Prompt-based Extraction of Social Determinants of Health Using Few-shot Learning,0.930517,"Social determinants of health (SDOH) documented in the electronic health
record through unstructured text are increasingly being studied to understand
how SDOH impacts patient health outcomes. In this work, we utilize the Social
History Annotation Corpus (SHAC), a multi-institutional corpus of de-identified
social history sections annotated for SDOH, including substance use,
employment, and living status information. We explore the automatic extraction
of SDOH information with SHAC in both standoff and inline annotation formats
using GPT-4 in a one-shot prompting setting. We compare GPT-4 extraction
performance with a high-performing supervised approach and perform thorough
error analyses. Our prompt-based GPT-4 method achieved an overall 0.652 F1 on
the SHAC test set, similar to the 7th best-performing system among all teams in
the n2c2 challenge with SHAC.",None,-1
03844230-e955-40b1-886f-466c60c0989a,Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions,0.187024,"Recent advances in Large Language Models (LLMs) have presented new
opportunities for integrating Artificial General Intelligence (AGI) into
biological research and education. This study evaluated the capabilities of
leading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in
answering conceptual biology questions. The models were tested on a
108-question multiple-choice exam covering biology topics in molecular biology,
biological techniques, metabolic engineering, and synthetic biology. Among the
models, GPT-4 achieved the highest average score of 90 and demonstrated the
greatest consistency across trials with different prompts. The results
indicated GPT-4's proficiency in logical reasoning and its potential to aid
biology research through capabilities like data analysis, hypothesis
generation, and knowledge integration. However, further development and
validation are still required before the promise of LLMs in accelerating
biological discovery can be realized.",None,-1
e0e13d90-5636-4bef-a59c-06c153173b15,Hierarchical Relationships: A New Perspective to Enhance Scene Graph Generation,0.130512,"This paper presents a finding that leveraging the hierarchical structures
among labels for relationships and objects can substantially improve the
performance of scene graph generation systems. The focus of this work is to
create an informative hierarchical structure that can divide object and
relationship categories into disjoint super-categories in a systematic way.
Specifically, we introduce a Bayesian prediction head to jointly predict the
super-category of relationships between a pair of object instances, as well as
the detailed relationship within that super-category simultaneously,
facilitating more informative predictions. The resulting model exhibits the
capability to produce a more extensive set of predicates beyond the dataset
annotations, and to tackle the prevalent issue of low annotation quality. While
our paper presents preliminary findings, experiments on the Visual Genome
dataset show its strong performance, particularly in predicate classifications
and zero-shot settings, that demonstrates the promise of our approach.",None,-1
c08bb8fc-a846-43c3-b901-96dc3ea71921,Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models,0.302666,"As the use of artificial intelligent (AI) models becomes more prevalent in
industries such as engineering and manufacturing, it is essential that these
models provide transparent reasoning behind their predictions. This paper
proposes the AI-Reasoner, which extracts the morphological characteristics of
defects (DefChars) from images and utilises decision trees to reason with the
DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e.
charts) and textual explanations to provide insights into outputs made by
masked-based defect detection and classification models. It also provides
effective mitigation strategies to enhance data pre-processing and overall
model performance. The AI-Reasoner was tested on explaining the outputs of an
IE Mask R-CNN model using a set of 366 images containing defects. The results
demonstrated its effectiveness in explaining the IE Mask R-CNN model's
predictions. Overall, the proposed AI-Reasoner provides a solution for
improving the performance of AI models in industrial applications that require
defect analysis.",None,-1
3a9601ae-c930-471d-a294-77b8950bace1,Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation,0.542866,"Generating images with both photorealism and multiview 3D consistency is
crucial for 3D-aware GANs, yet existing methods struggle to achieve them
simultaneously. Improving the photorealism via CNN-based 2D super-resolution
can break the strict 3D consistency, while keeping the 3D consistency by
learning high-resolution 3D representations for direct rendering often
compromises image quality. In this paper, we propose a novel learning strategy,
namely 3D-to-2D imitation, which enables a 3D-aware GAN to generate
high-quality images while maintaining their strict 3D consistency, by letting
the images synthesized by the generator's 3D rendering branch to mimic those
generated by its 2D super-resolution branch. We also introduce 3D-aware
convolutions into the generator for better 3D representation learning, which
further improves the image generation quality. With the above strategies, our
method reaches FID scores of 5.4 and 4.3 on FFHQ and AFHQ-v2 Cats,
respectively, at 512x512 resolution, largely outperforming existing 3D-aware
GANs using direct 3D rendering and coming very close to the previous
state-of-the-art method that leverages 2D super-resolution. Project website:
https://seanchenxy.github.io/Mimic3DWeb.",None,-1
af0a8a6d-ddcd-4ab9-8a95-a55f0876a373,MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos,0.604357,"User-generated content (UGC) live videos are often bothered by various
distortions during capture procedures and thus exhibit diverse visual
qualities. Such source videos are further compressed and transcoded by media
server providers before being distributed to end-users. Because of the
flourishing of UGC live videos, effective video quality assessment (VQA) tools
are needed to monitor and perceptually optimize live streaming videos in the
distributing process. In this paper, we address \textbf{UGC Live VQA} problems
by constructing a first-of-a-kind subjective UGC Live VQA database and
developing an effective evaluation tool. Concretely, 418 source UGC videos are
collected in real live streaming scenarios and 3,762 compressed ones at
different bit rates are generated for the subsequent subjective VQA
experiments. Based on the built database, we develop a
\underline{M}ulti-\underline{D}imensional \underline{VQA} (\textbf{MD-VQA})
evaluator to measure the visual quality of UGC live videos from semantic,
distortion, and motion aspects respectively. Extensive experimental results
show that MD-VQA achieves state-of-the-art performance on both our UGC Live VQA
database and existing compressed UGC VQA databases.",None,-1
291f247b-a7c0-4796-9ad5-3b5d7a195939,PSVT: End-to-End Multi-person 3D Pose and Shape Estimation with Progressive Video Transformers,0.310626,"Existing methods of multi-person video 3D human Pose and Shape Estimation
(PSE) typically adopt a two-stage strategy, which first detects human instances
in each frame and then performs single-person PSE with temporal model. However,
the global spatio-temporal context among spatial instances can not be captured.
In this paper, we propose a new end-to-end multi-person 3D Pose and Shape
estimation framework with progressive Video Transformer, termed PSVT. In PSVT,
a spatio-temporal encoder (STE) captures the global feature dependencies among
spatial objects. Then, spatio-temporal pose decoder (STPD) and shape decoder
(STSD) capture the global dependencies between pose queries and feature tokens,
shape queries and feature tokens, respectively. To handle the variances of
objects as time proceeds, a novel scheme of progressive decoding is used to
update pose and shape queries at each frame. Besides, we propose a novel
pose-guided attention (PGA) for shape decoder to better predict shape
parameters. The two components strengthen the decoder of PSVT to improve
performance. Extensive experiments on the four datasets show that PSVT achieves
stage-of-the-art results.",None,-1
0ab51909-5db7-425d-83a6-d518bffe86c7,"Transcending the ""Male Code"": Implicit Masculine Biases in NLP Contexts",0.523526,"Critical scholarship has elevated the problem of gender bias in data sets
used to train virtual assistants (VAs). Most work has focused on explicit
biases in language, especially against women, girls, femme-identifying people,
and genderqueer folk; implicit associations through word embeddings; and
limited models of gender and masculinities, especially toxic masculinities,
conflation of sex and gender, and a sex/gender binary framing of the masculine
as diametric to the feminine. Yet, we must also interrogate how masculinities
are ""coded"" into language and the assumption of ""male"" as the linguistic
default: implicit masculine biases. To this end, we examined two natural
language processing (NLP) data sets. We found that when gendered language was
present, so were gender biases and especially masculine biases. Moreover, these
biases related in nuanced ways to the NLP context. We offer a new dictionary
called AVA that covers ambiguous associations between gendered language and the
language of VAs.",None,-1
42c6c95a-ce32-4900-ad27-5247dc4b9c96,InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning,0.651931,"Inferring traffic object such as lane information is of foremost importance
for deployment of autonomous driving. Previous approaches focus on offline
construction of HD map inferred with GPS localization, which is insufficient
for globally scalable autonomous driving. To alleviate these issues, we propose
online HD map learning framework that detects HD map elements from onboard
sensor observations. We represent the map elements as a graph; we propose
InstaGraM, instance-level graph modeling of HD map that brings accurate and
fast end-to-end vectorized HD map learning. Along with the graph modeling
strategy, we propose end-to-end neural network composed of three stages: a
unified BEV feature extraction, map graph component detection, and association
via graph neural networks. Comprehensive experiments on public open dataset
show that our proposed network outperforms previous models by up to 13.7 mAP
with up to 33.8X faster computation time.",None,-1
4ccfd000-fec0-4701-a052-a1c42e84ba3a,Text revision in Scientific Writing Assistance: An Overview,0.164946,"Writing a scientific article is a challenging task as it is a highly codified
genre. Good writing skills are essential to properly convey ideas and results
of research work. Since the majority of scientific articles are currently
written in English, this exercise is all the more difficult for non-native
English speakers as they additionally have to face language issues. This
article aims to provide an overview of text revision in writing assistance in
the scientific domain.
  We will examine the specificities of scientific writing, including the format
and conventions commonly used in research articles.
  Additionally, this overview will explore the various types of writing
assistance tools available for text revision. Despite the evolution of the
technology behind these tools through the years, from rule-based approaches to
deep neural-based ones, challenges still exist (tools' accessibility, limited
consideration of the context, inexplicit use of discursive information, etc.)",None,-1
fc961fcd-a44e-4740-bbfb-8dd5d95c218f,On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection,0.29365,"Detecting adversarial samples that are carefully crafted to fool the model is
a critical step to socially-secure applications. However, existing adversarial
detection methods require access to sufficient training data, which brings
noteworthy concerns regarding privacy leakage and generalizability. In this
work, we validate that the adversarial sample generated by attack algorithms is
strongly related to a specific vector in the high-dimensional inputs. Such
vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated
without original training data. Based on this discovery, we propose a
data-agnostic adversarial detection framework, which induces different
responses between normal and adversarial samples to UAPs. Experimental results
show that our method achieves competitive detection performance on various text
classification tasks, and maintains an equivalent time consumption to normal
inference.",None,-1
079e7ebd-9ddc-4424-abbc-db6de5bb5ead,Investigating Reinforcement Learning for Communication Strategies in a Task-Initiative Setting,0.0384124,"Many conversational domains require the system to present nuanced information
to users. Such systems must follow up what they say to address clarification
questions and repair misunderstandings. In this work, we explore this
interactive strategy in a referential communication task. Using simulation, we
analyze the communication trade-offs between initial presentation and
subsequent followup as a function of user clarification strategy, and compare
the performance of several baseline strategies to policies derived by
reinforcement learning. We find surprising advantages to coherence-based
representations of dialogue strategy, which bring minimal data requirements,
explainable choices, and strong audit capabilities, but incur little loss in
predicted outcomes across a wide range of user models.",None,-1
461ed1fd-54d0-41b1-b59e-acf92ab69976,Learning the Distribution of Errors in Stereo Matching for Joint Disparity and Uncertainty Estimation,0.596777,"We present a new loss function for joint disparity and uncertainty estimation
in deep stereo matching. Our work is motivated by the need for precise
uncertainty estimates and the observation that multi-task learning often leads
to improved performance in all tasks. We show that this can be achieved by
requiring the distribution of uncertainty to match the distribution of
disparity errors via a KL divergence term in the network's loss function. A
differentiable soft-histogramming technique is used to approximate the
distributions so that they can be used in the loss. We experimentally assess
the effectiveness of our approach and observe significant improvements in both
disparity and uncertainty prediction on large datasets.",None,-1
e89da94b-99fb-4038-a2f3-ec1213dc19d8,Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty,0.819467,"We present our submission to the BabyLM challenge, whose goal was to improve
the sample efficiency of language models. We trained an ensemble consisting of
a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word
BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model,
which exceeds in performance both of its teachers as well as a similar model
trained without distillation. This suggests that distillation can not only
retain the full performance of the teacher model when the latter is trained on
a sufficiently small dataset; it can exceed it, and lead to significantly
better performance than direct training.",None,-1
2d671eb4-84f0-463a-a377-c41cda2307d1,Compacting Binary Neural Networks by Sparse Kernel Selection,0.146174,"Binary Neural Network (BNN) represents convolution weights with 1-bit values,
which enhances the efficiency of storage and computation. This paper is
motivated by a previously revealed phenomenon that the binary kernels in
successful BNNs are nearly power-law distributed: their values are mostly
clustered into a small number of codewords. This phenomenon encourages us to
compact typical BNNs and obtain further close performance through learning
non-repetitive kernels within a binary kernel subspace. Specifically, we regard
the binarization process as kernel grouping in terms of a binary codebook, and
our task lies in learning to select a smaller subset of codewords from the full
codebook. We then leverage the Gumbel-Sinkhorn technique to approximate the
codeword selection process, and develop the Permutation Straight-Through
Estimator (PSTE) that is able to not only optimize the selection process
end-to-end but also maintain the non-repetitive occupancy of selected
codewords. Experiments verify that our method reduces both the model size and
bit-wise computational costs, and achieves accuracy improvements compared with
state-of-the-art BNNs under comparable budgets.",None,-1
33836917-d83b-40ed-8452-bdf67054693c,Enhancing Efficient Continual Learning with Dynamic Structure Development of Spiking Neural Networks,0.79931,"Children possess the ability to learn multiple cognitive tasks sequentially,
which is a major challenge toward the long-term goal of artificial general
intelligence. Existing continual learning frameworks are usually applicable to
Deep Neural Networks (DNNs) and lack the exploration on more brain-inspired,
energy-efficient Spiking Neural Networks (SNNs). Drawing on continual learning
mechanisms during child growth and development, we propose Dynamic Structure
Development of Spiking Neural Networks (DSD-SNN) for efficient and adaptive
continual learning. When learning a sequence of tasks, the DSD-SNN dynamically
assigns and grows new neurons to new tasks and prunes redundant neurons,
thereby increasing memory capacity and reducing computational overhead. In
addition, the overlapping shared structure helps to quickly leverage all
acquired knowledge to new tasks, empowering a single network capable of
supporting multiple incremental tasks (without the separate sub-network mask
for each task). We validate the effectiveness of the proposed model on multiple
class incremental learning and task incremental learning benchmarks. Extensive
experiments demonstrated that our model could significantly improve
performance, learning speed and memory capacity, and reduce computational
overhead. Besides, our DSD-SNN model achieves comparable performance with the
DNNs-based methods, and significantly outperforms the state-of-the-art (SOTA)
performance for existing SNNs-based continual learning methods.",None,-1
1e06cd41-ed2b-449f-a69c-3f57e058547e,Deep Image Fingerprint: Towards Low Budget Synthetic Image Detection and Model Lineage Analysis,0.721556,"The generation of high-quality images has become widely accessible and is a
rapidly evolving process. As a result, anyone can generate images that are
indistinguishable from real ones. This leads to a wide range of applications,
including malicious usage with deceptive intentions. Despite advances in
detection techniques for generated images, a robust detection method still
eludes us. Furthermore, model personalization techniques might affect the
detection capabilities of existing methods. In this work, we utilize the
architectural properties of convolutional neural networks (CNNs) to develop a
new detection method. Our method can detect images from a known generative
model and enable us to establish relationships between fine-tuned generative
models. We tested the method on images produced by both Generative Adversarial
Networks (GANs) and recent large text-to-image models (LTIMs) that rely on
Diffusion Models. Our approach outperforms others trained under identical
conditions and achieves comparable performance to state-of-the-art pre-trained
detection methods on images generated by Stable Diffusion and MidJourney, with
significantly fewer required train samples.",None,-1
cbe5c563-5e5f-4c27-b155-4d74d33a5848,System 2 Attention (is something you might need too),0.710487,"Soft attention in Transformer-based Large Language Models (LLMs) is
susceptible to incorporating irrelevant information from the context into its
latent representations, which adversely affects next token generations. To help
rectify these issues, we introduce System 2 Attention (S2A), which leverages
the ability of LLMs to reason in natural language and follow instructions in
order to decide what to attend to. S2A regenerates the input context to only
include the relevant portions, before attending to the regenerated context to
elicit the final response. In experiments, S2A outperforms standard
attention-based LLMs on three tasks containing opinion or irrelevant
information, QA, math word problems and longform generation, where S2A
increases factuality and objectivity, and decreases sycophancy.",None,-1
33915ddc-36a4-42fc-91ae-12cd27899556,MedChatZH: a Better Medical Adviser Learns from Better Instructions,0.753541,"Generative large language models (LLMs) have shown great success in various
applications, including question-answering (QA) and dialogue systems. However,
in specialized domains like traditional Chinese medical QA, these models may
perform unsatisfactorily without fine-tuning on domain-specific datasets. To
address this, we introduce MedChatZH, a dialogue model designed specifically
for traditional Chinese medical QA. Our model is pre-trained on Chinese
traditional medical books and fine-tuned with a carefully curated medical
instruction dataset. It outperforms several solid baselines on a real-world
medical dialogue dataset. We release our model, code, and dataset on
https://github.com/tyang816/MedChatZH to facilitate further research in the
domain of traditional Chinese medicine and LLMs.",None,-1
fc9841d7-7671-4e0f-8c75-f82dcf4d0c6d,Federated Learning for Large-Scale Scene Modeling with Neural Radiance Fields,0.374097,"We envision a system to continuously build and maintain a map based on
earth-scale neural radiance fields (NeRF) using data collected from vehicles
and drones in a lifelong learning manner. However, existing large-scale
modeling by NeRF has problems in terms of scalability and maintainability when
modeling earth-scale environments. Therefore, to address these problems, we
propose a federated learning pipeline for large-scale modeling with NeRF. We
tailor the model aggregation pipeline in federated learning for NeRF, thereby
allowing local updates of NeRF. In the aggregation step, the accuracy of the
clients' global pose is critical. Thus, we also propose global pose alignment
to align the noisy global pose of clients before the aggregation step. In
experiments, we show the effectiveness of the proposed pose alignment and the
federated learning pipeline on the large-scale scene dataset, Mill19.",None,-1
34732c58-4a07-4322-9965-97578f8e0793,Backdoor Defense via Deconfounded Representation Learning,0.763817,"Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor
attacks, where attackers embed hidden backdoors in the DNN model by injecting a
few poisoned examples into the training dataset. While extensive efforts have
been made to detect and remove backdoors from backdoored DNNs, it is still not
clear whether a backdoor-free clean model can be directly obtained from
poisoned datasets. In this paper, we first construct a causal graph to model
the generation process of poisoned data and find that the backdoor attack acts
as the confounder, which brings spurious associations between the input images
and target labels, making the model predictions less reliable. Inspired by the
causal understanding, we propose the Causality-inspired Backdoor Defense (CBD),
to learn deconfounded representations for reliable classification.
Specifically, a backdoored model is intentionally trained to capture the
confounding effects. The other clean model dedicates to capturing the desired
causal effects by minimizing the mutual information with the confounding
representations from the backdoored model and employing a sample-wise
re-weighting scheme. Extensive experiments on multiple benchmark datasets
against 6 state-of-the-art attacks verify that our proposed defense method is
effective in reducing backdoor threats while maintaining high accuracy in
predicting benign samples. Further analysis shows that CBD can also resist
potential adaptive attacks. The code is available at
\url{https://github.com/zaixizhang/CBD}.",None,-1
ea8683d6-eff5-491c-a89c-d07b369d9514,RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing,0.823442,"Reaction diagram parsing is the task of extracting reaction schemes from a
diagram in the chemistry literature. The reaction diagrams can be arbitrarily
complex, thus robustly parsing them into structured data is an open challenge.
In this paper, we present RxnScribe, a machine learning model for parsing
reaction diagrams of varying styles. We formulate this structured prediction
task with a sequence generation approach, which condenses the traditional
pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378
diagrams and evaluate it with cross validation, achieving an 80.0% soft match
F1 score, with significant improvements over previous models. Our code and data
are publicly available at https://github.com/thomas0809/RxnScribe.",None,-1
2e3bd314-ee09-453a-877a-2a8808e4c26c,Effects of Human Adversarial and Affable Samples on BERT Generalization,0.119295,"BERT-based models have had strong performance on leaderboards, yet have been
demonstrably worse in real-world settings requiring generalization. Limited
quantities of training data is considered a key impediment to achieving
generalizability in machine learning. In this paper, we examine the impact of
training data quality, not quantity, on a model's generalizability. We consider
two characteristics of training data: the portion of human-adversarial
(h-adversarial), i.e., sample pairs with seemingly minor differences but
different ground-truth labels, and human-affable (h-affable) training samples,
i.e., sample pairs with minor differences but the same ground-truth label. We
find that for a fixed size of training samples, as a rule of thumb, having
10-30% h-adversarial instances improves the precision, and therefore F1, by up
to 20 points in the tasks of text classification and relation extraction.
Increasing h-adversarials beyond this range can result in performance plateaus
or even degradation. In contrast, h-affables may not contribute to a model's
generalizability and may even degrade generalization performance.",None,-1
255af52a-f987-41e6-8c11-7fb80c9622fd,TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks,0.195163,"The apparent ``black box'' nature of neural networks is a barrier to adoption
in applications where explainability is essential. This paper presents TAME
(Trainable Attention Mechanism for Explanations), a method for generating
explanation maps with a multi-branch hierarchical attention mechanism. TAME
combines a target model's feature maps from multiple layers using an attention
mechanism, transforming them into an explanation map. TAME can easily be
applied to any convolutional neural network (CNN) by streamlining the
optimization of the attention mechanism's training method and the selection of
target model's feature maps. After training, explanation maps can be computed
in a single forward pass. We apply TAME to two widely used models, i.e. VGG-16
and ResNet-50, trained on ImageNet and show improvements over previous
top-performing methods. We also provide a comprehensive ablation study
comparing the performance of different variations of TAME's architecture. TAME
source code is made publicly available at https://github.com/bmezaris/TAME",None,-1
8c9c5fd7-033a-4e17-ac09-cb699f4e659b,Formal concept analysis for evaluating intrinsic dimension of a natural language,0.357809,"Some results of a computational experiment for determining the intrinsic
dimension of linguistic varieties for the Bengali and Russian languages are
presented. At the same time, both sets of words and sets of bigrams in these
languages were considered separately. The method used to solve this problem was
based on formal concept analysis algorithms. It was found that the intrinsic
dimensions of these languages are significantly less than the dimensions used
in popular neural network models in natural language processing.",None,-1
746c8cbc-af61-47ae-9835-329b65433380,Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models,0.73487,"This work introduces approaches to assessing phrase breaks in ESL learners'
speech using pre-trained language models (PLMs) and large language models
(LLMs). There are two tasks: overall assessment of phrase break for a speech
clip and fine-grained assessment of every possible phrase break position. To
leverage NLP models, speech input is first force-aligned with texts, and then
pre-processed into a token sequence, including words and phrase break
information. To utilize PLMs, we propose a pre-training and fine-tuning
pipeline with the processed tokens. This process includes pre-training with a
replaced break token detection module and fine-tuning with text classification
and sequence labeling. To employ LLMs, we design prompts for ChatGPT. The
experiments show that with the PLMs, the dependence on labeled training data
has been greatly reduced, and the performance has improved. Meanwhile, we
verify that ChatGPT, a renowned LLM, has potential for further advancement in
this area.",None,-1
1a4ea541-b398-4feb-84c9-62727ab986f1,Decision-Making Under Uncertainty: Beyond Probabilities,0.654283,"This position paper reflects on the state-of-the-art in decision-making under
uncertainty. A classical assumption is that probabilities can sufficiently
capture all uncertainty in a system. In this paper, the focus is on the
uncertainty that goes beyond this classical interpretation, particularly by
employing a clear distinction between aleatoric and epistemic uncertainty. The
paper features an overview of Markov decision processes (MDPs) and extensions
to account for partial observability and adversarial behavior. These models
sufficiently capture aleatoric uncertainty but fail to account for epistemic
uncertainty robustly. Consequently, we present a thorough overview of so-called
uncertainty models that exhibit uncertainty in a more robust interpretation. We
show several solution techniques for both discrete and continuous models,
ranging from formal verification, over control-based abstractions, to
reinforcement learning. As an integral part of this paper, we list and discuss
several key challenges that arise when dealing with rich types of uncertainty
in a model-based fashion.",None,-1
aae4e8ce-d4ed-421d-b7a0-e58a739c35db,mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection,0.512968,"This paper presents the winning system for the zero-shot Spanish framing
detection task, which also achieves competitive places in eight additional
languages. The challenge of the framing detection task lies in identifying a
set of 14 frames when only a few or zero samples are available, i.e., a
multilingual multi-label few- or zero-shot setting. Our developed solution
employs a pre-training procedure based on multilingual Transformers using a
label-aware contrastive loss function. In addition to describing the system, we
perform an embedding space analysis and ablation study to demonstrate how our
pre-training procedure supports framing detection to advance computational
framing analysis.",None,-1
4578bac1-e07f-4111-b022-4a8c72e4227c,A Diachronic Perspective on User Trust in AI under Uncertainty,0.534309,"In a human-AI collaboration, users build a mental model of the AI system
based on its reliability and how it presents its decision, e.g. its
presentation of system confidence and an explanation of the output. Modern NLP
systems are often uncalibrated, resulting in confidently incorrect predictions
that undermine user trust. In order to build trustworthy AI, we must understand
how user trust is developed and how it can be regained after potential
trust-eroding events. We study the evolution of user trust in response to these
trust-eroding events using a betting game. We find that even a few incorrect
instances with inaccurate confidence estimates damage user trust and
performance, with very slow recovery. We also show that this degradation in
trust reduces the success of human-AI collaboration and that different types of
miscalibration -- unconfidently correct and confidently incorrect -- have
different negative effects on user trust. Our findings highlight the importance
of calibration in user-facing AI applications and shed light on what aspects
help users decide whether to trust the AI system.",None,-1
4227fac6-3ddf-471a-8122-7bd06ba40f43,Difference of Anisotropic and Isotropic TV for Segmentation under Blur and Poisson Noise,0.0285499,"In this paper, we aim to segment an image degraded by blur and Poisson noise.
We adopt a smoothing-and-thresholding (SaT) segmentation framework that finds a
piecewise-smooth solution, followed by $k$-means clustering to segment the
image. Specifically for the image smoothing step, we replace the least-squares
fidelity for Gaussian noise in the Mumford-Shah model with a maximum posterior
(MAP) term to deal with Poisson noise and we incorporate the weighted
difference of anisotropic and isotropic total variation (AITV) as a
regularization to promote the sparsity of image gradients. For such a nonconvex
model, we develop a specific splitting scheme and utilize a proximal operator
to apply the alternating direction method of multipliers (ADMM). Convergence
analysis is provided to validate the efficacy of the ADMM scheme. Numerical
experiments on various segmentation scenarios (grayscale/color and multiphase)
showcase that our proposed method outperforms a number of segmentation methods,
including the original SaT.",None,-1
7fe929f7-c9a6-4412-9256-7a2caa5ba3af,Implicit Neural Head Synthesis via Controllable Local Deformation Fields,0.524946,"High-quality reconstruction of controllable 3D head avatars from 2D videos is
highly desirable for virtual human applications in movies, games, and
telepresence. Neural implicit fields provide a powerful representation to model
3D head avatars with personalized shape, expressions, and facial parts, e.g.,
hair and mouth interior, that go beyond the linear 3D morphable model (3DMM).
However, existing methods do not model faces with fine-scale facial features,
or local control of facial parts that extrapolate asymmetric expressions from
monocular videos. Further, most condition only on 3DMM parameters with poor(er)
locality, and resolve local features with a global neural field. We build on
part-based implicit shape models that decompose a global deformation field into
local ones. Our novel formulation models multiple implicit deformation fields
with local semantic rig-like control via 3DMM-based parameters, and
representative facial landmarks. Further, we propose a local control loss and
attention mask mechanism that promote sparsity of each learned deformation
field. Our formulation renders sharper locally controllable nonlinear
deformations than previous implicit monocular approaches, especially mouth
interior, asymmetric expressions, and facial details.",None,-1
5c227547-e55d-4293-89c7-6fe6b4d30333,DoE2Vec: Deep-learning Based Features for Exploratory Landscape Analysis,0.797644,"We propose DoE2Vec, a variational autoencoder (VAE)-based methodology to
learn optimization landscape characteristics for downstream meta-learning
tasks, e.g., automated selection of optimization algorithms. Principally, using
large training data sets generated with a random function generator, DoE2Vec
self-learns an informative latent representation for any design of experiments
(DoE). Unlike the classical exploratory landscape analysis (ELA) method, our
approach does not require any feature engineering and is easily applicable for
high dimensional search spaces. For validation, we inspect the quality of
latent reconstructions and analyze the latent representations using different
experiments. The latent representations not only show promising potentials in
identifying similar (cheap-to-evaluate) surrogate functions, but also can
significantly boost performances when being used complementary to the classical
ELA features in classification tasks.",None,-1
cbf2a8a9-4a86-4c72-b509-6fd405fed49f,Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge Graphs,0.710906,"Knowledge graph embeddings (KGE) have been extensively studied to embed
large-scale relational data for many real-world applications. Existing methods
have long ignored the fact many KGs contain two fundamentally different views:
high-level ontology-view concepts and fine-grained instance-view entities. They
usually embed all nodes as vectors in one latent space. However, a single
geometric representation fails to capture the structural differences between
two views and lacks probabilistic semantics towards concepts' granularity. We
propose Concept2Box, a novel approach that jointly embeds the two views of a KG
using dual geometric representations. We model concepts with box embeddings,
which learn the hierarchy structure and complex relations such as overlap and
disjoint among them. Box volumes can be interpreted as concepts' granularity.
Different from concepts, we model entities as vectors. To bridge the gap
between concept box embeddings and entity vector embeddings, we propose a novel
vector-to-box distance metric and learn both embeddings jointly. Experiments on
both the public DBpedia KG and a newly-created industrial KG showed the
effectiveness of Concept2Box.",None,-1
e1a3586c-ae7d-4b48-8173-31a01f680660,Universal Information Extraction with Meta-Pretrained Self-Retrieval,0.214832,"Universal Information Extraction~(Universal IE) aims to solve different
extraction tasks in a uniform text-to-structure generation manner. Such a
generation procedure tends to struggle when there exist complex information
structures to be extracted. Retrieving knowledge from external knowledge bases
may help models to overcome this problem but it is impossible to construct a
knowledge base suitable for various IE tasks. Inspired by the fact that large
amount of knowledge are stored in the pretrained language models~(PLM) and can
be retrieved explicitly, in this paper, we propose MetaRetriever to retrieve
task-specific knowledge from PLMs to enhance universal IE. As different IE
tasks need different knowledge, we further propose a Meta-Pretraining Algorithm
which allows MetaRetriever to quicktly achieve maximum task-specific retrieval
performance when fine-tuning on downstream IE tasks. Experimental results show
that MetaRetriever achieves the new state-of-the-art on 4 IE tasks, 12 datasets
under fully-supervised, low-resource and few-shot scenarios.",None,-1
3c19f1b0-ecf5-4a80-9584-c0847ae5f1cd,A3S: Adversarial learning of semantic representations for Scene-Text Spotting,0.388844,"Scene-text spotting is a task that predicts a text area on natural scene
images and recognizes its text characters simultaneously. It has attracted much
attention in recent years due to its wide applications. Existing research has
mainly focused on improving text region detection, not text recognition. Thus,
while detection accuracy is improved, the end-to-end accuracy is insufficient.
Texts in natural scene images tend to not be a random string of characters but
a meaningful string of characters, a word. Therefore, we propose adversarial
learning of semantic representations for scene text spotting (A3S) to improve
end-to-end accuracy, including text recognition. A3S simultaneously predicts
semantic features in the detected text area instead of only performing text
recognition based on existing visual features. Experimental results on publicly
available datasets show that the proposed method achieves better accuracy than
other methods.",None,-1
395085ab-b33a-4d5a-a937-fcd6aa782f2b,Segment Anything is A Good Pseudo-label Generator for Weakly Supervised Semantic Segmentation,0.924239,"Weakly supervised semantic segmentation with weak labels is a long-lived
ill-posed problem. Mainstream methods mainly focus on improving the quality of
pseudo labels. In this report, we attempt to explore the potential of 'prompt
to masks' from the powerful class-agnostic large segmentation model,
segment-anything. Specifically, different weak labels are used as prompts to
the segment-anything model, generating precise class masks. The class masks are
utilized to generate pseudo labels to train the segmentation networks. We have
conducted extensive experiments on PASCAL VOC 2012 dataset. Experiments
demonstrate that segment-anything can serve as a good pseudo-label generator.
The code will be made publicly available.",None,-1
627bc845-757b-489d-a95f-258c3254e658,Learning Adaptive Neighborhoods for Graph Neural Networks,0.118064,"Graph convolutional networks (GCNs) enable end-to-end learning on graph
structured data. However, many works assume a given graph structure. When the
input graph is noisy or unavailable, one approach is to construct or learn a
latent graph structure. These methods typically fix the choice of node degree
for the entire graph, which is suboptimal. Instead, we propose a novel
end-to-end differentiable graph generator which builds graph topologies where
each node selects both its neighborhood and its size. Our module can be readily
integrated into existing pipelines involving graph convolution operations,
replacing the predetermined or existing adjacency matrix with one that is
learned, and optimized, as part of the general objective. As such it is
applicable to any GCN. We integrate our module into trajectory prediction,
point cloud classification and node classification pipelines resulting in
improved accuracy over other structure-learning methods across a wide range of
datasets and GCN backbones.",None,-1
49aa567a-80f7-45a6-889d-cbbb7b7be718,S-TREK: Sequential Translation and Rotation Equivariant Keypoints for local feature extraction,0.637307,"In this work we introduce S-TREK, a novel local feature extractor that
combines a deep keypoint detector, which is both translation and rotation
equivariant by design, with a lightweight deep descriptor extractor. We train
the S-TREK keypoint detector within a framework inspired by reinforcement
learning, where we leverage a sequential procedure to maximize a reward
directly related to keypoint repeatability. Our descriptor network is trained
following a ""detect, then describe"" approach, where the descriptor loss is
evaluated only at those locations where keypoints have been selected by the
already trained detector. Extensive experiments on multiple benchmarks confirm
the effectiveness of our proposed method, with S-TREK often outperforming other
state-of-the-art methods in terms of repeatability and quality of the recovered
poses, especially when dealing with in-plane rotations.",None,-1
fd3297ef-bf60-4373-a0e1-2d2129c6f6df,The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models,0.272042,"Partially Observable Markov Decision Processes (POMDPs) are used to model
environments where the full state cannot be perceived by an agent. As such the
agent needs to reason taking into account the past observations and actions.
However, simply remembering the full history is generally intractable due to
the exponential growth in the history space. Maintaining a probability
distribution that models the belief over what the true state is can be used as
a sufficient statistic of the history, but its computation requires access to
the model of the environment and is often intractable. While SOTA algorithms
use Recurrent Neural Networks to compress the observation-action history aiming
to learn a sufficient statistic, they lack guarantees of success and can lead
to sub-optimal policies. To overcome this, we propose the Wasserstein Belief
Updater, an RL algorithm that learns a latent model of the POMDP and an
approximation of the belief update. Our approach comes with theoretical
guarantees on the quality of our approximation ensuring that our outputted
beliefs allow for learning the optimal value function.",None,-1
f1b095ee-1fe7-4c10-940a-69868c4959ef,Benchmarking Deepart Detection,0.158657,"Deepfake technologies have been blurring the boundaries between the real and
unreal, likely resulting in malicious events. By leveraging newly emerged
deepfake technologies, deepfake researchers have been making a great upending
to create deepfake artworks (deeparts), which are further closing the gap
between reality and fantasy. To address potentially appeared ethics questions,
this paper establishes a deepart detection database (DDDB) that consists of a
set of high-quality conventional art images (conarts) and five sets of deepart
images generated by five state-of-the-art deepfake models. This database
enables us to explore once-for-all deepart detection and continual deepart
detection. For the two new problems, we suggest four benchmark evaluations and
four families of solutions on the constructed DDDB. The comprehensive study
demonstrates the effectiveness of the proposed solutions on the established
benchmark dataset, which is capable of paving a way to more interesting
directions of deepart detection. The constructed benchmark dataset and the
source code will be made publicly available.",None,-1
9ff1cde0-72f8-4d53-8180-53ab6a5b480f,A Game-Theoretic Framework for Joint Forecasting and Planning,0.874188,"Planning safe robot motions in the presence of humans requires reliable
forecasts of future human motion. However, simply predicting the most likely
motion from prior interactions does not guarantee safety. Such forecasts fail
to model the long tail of possible events, which are rarely observed in limited
datasets. On the other hand, planning for worst-case motions leads to overtly
conservative behavior and a ""frozen robot"". Instead, we aim to learn forecasts
that predict counterfactuals that humans guard against. We propose a novel
game-theoretic framework for joint planning and forecasting with the payoff
being the performance of the planner against the demonstrator, and present
practical algorithms to train models in an end-to-end fashion. We demonstrate
that our proposed algorithm results in safer plans in a crowd navigation
simulator and real-world datasets of pedestrian motion. We release our code at
https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.",None,-1
1fbb359d-41c8-4518-8b60-ab5f139269be,Devil's on the Edges: Selective Quad Attention for Scene Graph Generation,0.66183,"Scene graph generation aims to construct a semantic graph structure from an
image such that its nodes and edges respectively represent objects and their
relationships. One of the major challenges for the task lies in the presence of
distracting objects and relationships in images; contextual reasoning is
strongly distracted by irrelevant objects or backgrounds and, more importantly,
a vast number of irrelevant candidate relations. To tackle the issue, we
propose the Selective Quad Attention Network (SQUAT) that learns to select
relevant object pairs and disambiguate them via diverse contextual
interactions. SQUAT consists of two main components: edge selection and quad
attention. The edge selection module selects relevant object pairs, i.e., edges
in the scene graph, which helps contextual reasoning, and the quad attention
module then updates the edge features using both edge-to-node and edge-to-edge
cross-attentions to capture contextual information between objects and object
pairs. Experiments demonstrate the strong performance and robustness of SQUAT,
achieving the state of the art on the Visual Genome and Open Images v6
benchmarks.",None,-1
17f85a9b-2def-4c94-a058-d60efb1914fd,What makes a good pause? Investigating the turn-holding effects of fillers,0.442971,"Filled pauses (or fillers), such as ""uh"" and ""um"", are frequent in
spontaneous speech and can serve as a turn-holding cue for the listener,
indicating that the current speaker is not done yet. In this paper, we use the
recently proposed Voice Activity Projection (VAP) model, which is a deep
learning model trained to predict the dynamics of conversation, to analyse the
effects of filled pauses on the expected turn-hold probability. The results
show that, while filled pauses do indeed have a turn-holding effect, it is
perhaps not as strong as could be expected, probably due to the redundancy of
other cues. We also find that the prosodic properties and position of the
filler has a significant effect on the turn-hold probability. However, contrary
to what has been suggested in previous work, there is no difference between
""uh"" and ""um"" in this regard.",None,-1
83816abd-4a22-477f-ad6d-562b61e8ae9b,Diffusion Models for Interferometric Satellite Aperture Radar,0.834891,"Probabilistic Diffusion Models (PDMs) have recently emerged as a very
promising class of generative models, achieving high performance in natural
image generation. However, their performance relative to non-natural images,
like radar-based satellite data, remains largely unknown. Generating large
amounts of synthetic (and especially labelled) satellite data is crucial to
implement deep-learning approaches for the processing and analysis of
(interferometric) satellite aperture radar data. Here, we leverage PDMs to
generate several radar-based satellite image datasets. We show that PDMs
succeed in generating images with complex and realistic structures, but that
sampling time remains an issue. Indeed, accelerated sampling strategies, which
work well on simple image datasets like MNIST, fail on our radar datasets. We
provide a simple and versatile open-source
https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample and
evaluate PDMs using any dataset on a single GPU.",None,-1
27d20f7f-457c-41ed-85c4-3f8587377618,Simplified Continuous High Dimensional Belief Space Planning with Adaptive Probabilistic Belief-dependent Constraints,0.346499,"Online decision making under uncertainty in partially observable domains,
also known as Belief Space Planning, is a fundamental problem in robotics and
Artificial Intelligence. Due to an abundance of plausible future unravelings,
calculating an optimal course of action inflicts an enormous computational
burden on the agent. Moreover, in many scenarios, e.g., information gathering,
it is required to introduce a belief-dependent constraint. Prompted by this
demand, in this paper, we consider a recently introduced probabilistic
belief-dependent constrained POMDP. We present a technique to adaptively accept
or discard a candidate action sequence with respect to a probabilistic
belief-dependent constraint, before expanding a complete set of future
observations samples and without any loss in accuracy. Moreover, using our
proposed framework, we contribute an adaptive method to find a maximal feasible
return (e.g., information gain) in terms of Value at Risk for the candidate
action sequence with substantial acceleration. On top of that, we introduce an
adaptive simplification technique for a probabilistically constrained setting.
Such an approach provably returns an identical-quality solution while
dramatically accelerating online decision making. Our universal framework
applies to any belief-dependent constrained continuous POMDP with parametric
beliefs, as well as nonparametric beliefs represented by particles. In the
context of an information-theoretic constraint, our presented framework
stochastically quantifies if a cumulative information gain along the planning
horizon is sufficiently significant (e.g. for, information gathering, active
SLAM). We apply our method to active SLAM, a highly challenging problem of high
dimensional Belief Space Planning. Extensive realistic simulations corroborate
the superiority of our proposed ideas.",None,-1
556f2914-6372-4a62-bab4-4d15a72257b1,Improved Compositional Generalization by Generating Demonstrations for Meta-Learning,0.0877528,"Meta-learning and few-shot prompting are viable methods to induce certain
types of compositional behaviour. However, these methods can be very sensitive
to the choice of support examples used. Choosing good supports from the
training data for a given test query is already a difficult problem, but in
some cases solving this may not even be enough. We consider a grounded language
learning problem (gSCAN) where good support examples for certain test splits
might not even exist in the training data, or would be infeasible to search
for. We design an agent which instead generates possible supports which are
relevant to the test query and current state of the world, then uses these
supports via meta-learning to solve the test query. We show substantially
improved performance on a previously unsolved compositional behaviour split
without a loss of performance on other splits. Further experiments show that in
this case, searching for relevant demonstrations even with an oracle function
is not sufficient to attain good performance when using meta-learning.",None,-1
5242f5eb-a48a-41e0-bb9f-2ebca9463424,Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge,0.487077,"In this paper, we study the problem of knowledge-intensive text-to-SQL, in
which domain knowledge is necessary to parse expert questions into SQL queries
over domain-specific tables. We formalize this scenario by building a new
Chinese benchmark KnowSQL consisting of domain-specific questions covering
various domains. We then address this problem by presenting formulaic
knowledge, rather than by annotating additional data examples. More concretely,
we construct a formulaic knowledge bank as a domain knowledge base and propose
a framework (ReGrouP) to leverage this formulaic knowledge during parsing.
Experiments using ReGrouP demonstrate a significant 28.2% improvement overall
on KnowSQL.",None,-1
14a9fb62-ee11-4f97-b86d-89626afc73fb,XAI in Computational Linguistics: Understanding Political Leanings in the Slovenian Parliament,0.079758,"The work covers the development and explainability of machine learning models
for predicting political leanings through parliamentary transcriptions. We
concentrate on the Slovenian parliament and the heated debate on the European
migrant crisis, with transcriptions from 2014 to 2020. We develop both
classical machine learning and transformer language models to predict the left-
or right-leaning of parliamentarians based on their given speeches on the topic
of migrants. With both types of models showing great predictive success, we
continue with explaining their decisions. Using explainability techniques, we
identify keywords and phrases that have the strongest influence in predicting
political leanings on the topic, with left-leaning parliamentarians using
concepts such as people and unity and speak about refugees, and right-leaning
parliamentarians using concepts such as nationality and focus more on illegal
migrants. This research is an example that understanding the reasoning behind
predictions can not just be beneficial for AI engineers to improve their
models, but it can also be helpful as a tool in the qualitative analysis steps
in interdisciplinary research.",None,-1
0c5a3bc4-2d64-4282-9ef2-1c7fc26a87ed,Empathetic Response Generation via Emotion Cause Transition Graph,0.53496,"Empathetic dialogue is a human-like behavior that requires the perception of
both affective factors (e.g., emotion status) and cognitive factors (e.g.,
cause of the emotion). Besides concerning emotion status in early work, the
latest approaches study emotion causes in empathetic dialogue. These approaches
focus on understanding and duplicating emotion causes in the context to show
empathy for the speaker. However, instead of only repeating the contextual
causes, the real empathic response often demonstrate a logical and
emotion-centered transition from the causes in the context to those in the
responses. In this work, we propose an emotion cause transition graph to
explicitly model the natural transition of emotion causes between two adjacent
turns in empathetic dialogue. With this graph, the concept words of the emotion
causes in the next turn can be predicted and used by a specifically designed
concept-aware decoder to generate the empathic response. Automatic and human
experimental results on the benchmark dataset demonstrate that our method
produces more empathetic, coherent, informative, and specific responses than
existing models.",None,-1
7aaa21a1-5cfa-47dd-b28f-3f4ccf5bb45a,Improving Long Context Document-Level Machine Translation,0.721162,"Document-level context for neural machine translation (NMT) is crucial to
improve the translation consistency and cohesion, the translation of ambiguous
inputs, as well as several other linguistic phenomena. Many works have been
published on the topic of document-level NMT, but most restrict the system to
only local context, typically including just the one or two preceding sentences
as additional information. This might be enough to resolve some ambiguous
inputs, but it is probably not sufficient to capture some document-level
information like the topic or style of a conversation. When increasing the
context size beyond just the local context, there are two challenges: (i)
the~memory usage increases exponentially (ii) the translation performance
starts to degrade. We argue that the widely-used attention mechanism is
responsible for both issues. Therefore, we propose a constrained attention
variant that focuses the attention on the most relevant parts of the sequence,
while simultaneously reducing the memory consumption. For evaluation, we
utilize targeted test sets in combination with novel evaluation techniques to
analyze the translations in regards to specific discourse-related phenomena. We
find that our approach is a good compromise between sentence-level NMT vs
attending to the full context, especially in low resource scenarios.",None,-1
31f3e3e1-81b4-4be3-a2e2-33e7862bc9af,Integrated Conflict Management for UAM with Strategic Demand Capacity Balancing and Learning-based Tactical Deconfliction,0.84909,"Urban air mobility (UAM) has the potential to revolutionize our daily
transportation, offering rapid and efficient deliveries of passengers and cargo
between dedicated locations within and around the urban environment. Before the
commercialization and adoption of this emerging transportation mode, however,
aviation safety must be guaranteed, i.e., all the aircraft have to be safely
separated by strategic and tactical deconfliction. Reinforcement learning has
demonstrated effectiveness in the tactical deconfliction of en route commercial
air traffic in simulation. However, its performance is found to be dependent on
the traffic density. In this project, we propose a novel framework that
combines demand capacity balancing (DCB) for strategic conflict management and
reinforcement learning for tactical separation. By using DCB to precondition
traffic to proper density levels, we show that reinforcement learning can
achieve much better performance for tactical safety separation. Our results
also indicate that this DCB preconditioning can allow target levels of safety
to be met that are otherwise impossible. In addition, combining strategic DCB
with reinforcement learning for tactical separation can meet these safety
levels while achieving greater operational efficiency than alternative
solutions.",None,-1
2bf433e4-1808-4a81-9354-a78a49aa27f6,CIF-PT: Bridging Speech and Text Representations for Spoken Language Understanding via Continuous Integrate-and-Fire Pre-Training,0.0300982,"Speech or text representation generated by pre-trained models contains
modal-specific information that could be combined for benefiting spoken
language understanding (SLU) tasks. In this work, we propose a novel
pre-training paradigm termed Continuous Integrate-and-Fire Pre-Training
(CIF-PT). It relies on a simple but effective frame-to-token alignment:
continuous integrate-and-fire (CIF) to bridge the representations between
speech and text. It jointly performs speech-to-text training and language model
distillation through CIF as the pre-training (PT). Evaluated on SLU benchmark
SLURP dataset, CIF-PT outperforms the state-of-the-art model by 1.94% of
accuracy and 2.71% of SLU-F1 on the tasks of intent classification and slot
filling, respectively. We also observe the cross-modal representation extracted
by CIF-PT obtains better performance than other neural interfaces for the tasks
of SLU, including the dominant speech representation learned from
self-supervised pre-training.",None,-1
59233ae3-5399-4413-9667-0ef0cdf89426,Fashionpedia-Taste: A Dataset towards Explaining Human Fashion Taste,0.223015,"Existing fashion datasets do not consider the multi-facts that cause a
consumer to like or dislike a fashion image. Even two consumers like a same
fashion image, they could like this image for total different reasons. In this
paper, we study the reason why a consumer like a certain fashion image. Towards
this goal, we introduce an interpretability dataset, Fashionpedia-taste,
consist of rich annotation to explain why a subject like or dislike a fashion
image from the following 3 perspectives: 1) localized attributes; 2) human
attention; 3) caption. Furthermore, subjects are asked to provide their
personal attributes and preference on fashion, such as personality and
preferred fashion brands. Our dataset makes it possible for researchers to
build computational models to fully understand and interpret human fashion
taste from different humanistic perspectives and modalities.",None,-1
c0f10a94-9f4a-49a1-91a8-d253079867e5,Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting,0.351132,"Detailed 3D reconstruction and photo-realistic relighting of digital humans
are essential for various applications. To this end, we propose a novel
sparse-view 3d human reconstruction framework that closely incorporates the
occupancy field and albedo field with an additional visibility field--it not
only resolves occlusion ambiguity in multiview feature aggregation, but can
also be used to evaluate light attenuation for self-shadowed relighting. To
enhance its training viability and efficiency, we discretize visibility onto a
fixed set of sample directions and supply it with coupled geometric 3D depth
feature and local 2D image feature. We further propose a novel
rendering-inspired loss, namely TransferLoss, to implicitly enforce the
alignment between visibility and occupancy field, enabling end-to-end joint
training. Results and extensive experiments demonstrate the effectiveness of
the proposed method, as it surpasses state-of-the-art in terms of
reconstruction accuracy while achieving comparably accurate relighting to
ray-traced ground truth.",None,-1
349de56b-1a6e-4ec8-94d6-d8858fb11f7e,Generalized Lightness Adaptation with Channel Selective Normalization,0.192666,"Lightness adaptation is vital to the success of image processing to avoid
unexpected visual deterioration, which covers multiple aspects, e.g., low-light
image enhancement, image retouching, and inverse tone mapping. Existing methods
typically work well on their trained lightness conditions but perform poorly in
unknown ones due to their limited generalization ability. To address this
limitation, we propose a novel generalized lightness adaptation algorithm that
extends conventional normalization techniques through a channel filtering
design, dubbed Channel Selective Normalization (CSNorm). The proposed CSNorm
purposely normalizes the statistics of lightness-relevant channels and keeps
other channels unchanged, so as to improve feature generalization and
discrimination. To optimize CSNorm, we propose an alternating training strategy
that effectively identifies lightness-relevant channels. The model equipped
with our CSNorm only needs to be trained on one lightness condition and can be
well generalized to unknown lightness conditions. Experimental results on
multiple benchmark datasets demonstrate the effectiveness of CSNorm in
enhancing the generalization ability for the existing lightness adaptation
methods. Code is available at https://github.com/mdyao/CSNorm.",None,-1
882a24ea-8b67-4c96-bcc7-207b9e013d7c,AMRs Assemble! Learning to Ensemble with Autoregressive Models for AMR Parsing,0.598511,"In this paper, we examine the current state-of-the-art in AMR parsing, which
relies on ensemble strategies by merging multiple graph predictions. Our
analysis reveals that the present models often violate AMR structural
constraints. To address this issue, we develop a validation method, and show
how ensemble models can exploit SMATCH metric weaknesses to obtain higher
scores, but sometimes result in corrupted graphs. Additionally, we highlight
the demanding need to compute the SMATCH score among all possible predictions.
To overcome these challenges, we propose two novel ensemble strategies based on
Transformer models, improving robustness to structural constraints, while also
reducing the computational time. Our methods provide new insights for enhancing
AMR parsers and metrics. Our code is available at
\href{https://www.github.com/babelscape/AMRs-Assemble}{github.com/babelscape/AMRs-Assemble}.",None,-1
43883dcd-a617-467b-ae70-9a6f8fbdbf94,Descriptive Knowledge Graph in Biomedical Domain,0.275798,"We present a novel system that automatically extracts and generates
informative and descriptive sentences from the biomedical corpus and
facilitates the efficient search for relational knowledge. Unlike previous
search engines or exploration systems that retrieve unconnected passages, our
system organizes descriptive sentences as a relational graph, enabling
researchers to explore closely related biomedical entities (e.g., diseases
treated by a chemical) or indirectly connected entities (e.g., potential drugs
for treating a disease). Our system also uses ChatGPT and a fine-tuned relation
synthesis model to generate concise and reliable descriptive sentences from
retrieved information, reducing the need for extensive human reading effort.
With our system, researchers can easily obtain both high-level knowledge and
detailed references and interactively steer to the information of interest. We
spotlight the application of our system in COVID-19 research, illustrating its
utility in areas such as drug repurposing and literature curation.",None,-1
718d98fb-9d2d-41e0-be4d-0ddd77c6e059,Adaptive Planning Search Algorithm for Analog Circuit Verification,0.291826,"Integrated circuit verification has gathered considerable interest in recent
times. Since these circuits keep growing in complexity year by year,
pre-Silicon (pre-SI) verification becomes ever more important, in order to
ensure proper functionality. Thus, in order to reduce the time needed for
manually verifying ICs, we propose a machine learning (ML) approach, which uses
less simulations. This method relies on an initial evaluation set of operating
condition configurations (OCCs), in order to train Gaussian process (GP)
surrogate models. By using surrogate models, we can propose further, more
difficult OCCs. Repeating this procedure for several iterations has shown
better GP estimation of the circuit's responses, on both synthetic and real
circuits, resulting in a better chance of finding the worst case, or even
failures, for certain circuit responses. Thus, we show that the proposed
approach is able to provide OCCs closer to the specifications for all circuits
and identify a failure (specification violation) for one of the responses of a
real circuit.",None,-1
ab19b95c-14e2-4a73-8580-5715ca94122e,Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten,0.47037,"The Right to Explanation and the Right to be Forgotten are two important
principles outlined to regulate algorithmic decision making and data usage in
real-world applications. While the right to explanation allows individuals to
request an actionable explanation for an algorithmic decision, the right to be
forgotten grants them the right to ask for their data to be deleted from all
the databases and models of an organization. Intuitively, enforcing the right
to be forgotten may trigger model updates which in turn invalidate previously
provided explanations, thus violating the right to explanation. In this work,
we investigate the technical implications arising due to the interference
between the two aforementioned regulatory principles, and propose the first
algorithmic framework to resolve the tension between them. To this end, we
formulate a novel optimization problem to generate explanations that are robust
to model updates due to the removal of training data instances by data deletion
requests. We then derive an efficient approximation algorithm to handle the
combinatorial complexity of this optimization problem. We theoretically
demonstrate that our method generates explanations that are provably robust to
worst-case data deletion requests with bounded costs in case of linear models
and certain classes of non-linear models. Extensive experimentation with
real-world datasets demonstrates the efficacy of the proposed framework.",None,-1
16452b1c-cfce-4838-9a35-5103498fee69,Efficient Black-Box Adversarial Attacks on Neural Text Detectors,0.220714,"Neural text detectors are models trained to detect whether a given text was
generated by a language model or written by a human. In this paper, we
investigate three simple and resource-efficient strategies (parameter tweaking,
prompt engineering, and character-level mutations) to alter texts generated by
GPT-3.5 that are unsuspicious or unnoticeable for humans but cause
misclassification by neural text detectors. The results show that especially
parameter tweaking and character-level mutations are effective strategies.",None,-1
3ef0864f-eb44-4dd4-8a54-d53a8b30e90a,Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?,0.487268,"To decipher the algorithm underlying the human brain's language
representation, previous work probed brain responses to language input with
pre-trained artificial neural network (ANN) models fine-tuned on NLU tasks.
However, full fine-tuning generally updates the entire parametric space and
distorts pre-trained features, cognitively inconsistent with the brain's robust
multi-task learning ability. Prompt-tuning, in contrast, protects pre-trained
weights and learns task-specific embeddings to fit a task. Could prompt-tuning
generate representations that better account for the brain's language
representations than fine-tuning? If so, what kind of NLU task leads a
pre-trained model to better decode the information represented in the human
brain? We investigate these questions by comparing prompt-tuned and fine-tuned
representations in neural decoding, that is predicting the linguistic stimulus
from the brain activities evoked by the stimulus. We find that on none of the
10 NLU tasks, full fine-tuning significantly outperforms prompt-tuning in
neural decoding, implicating that a more brain-consistent tuning method yields
representations that better correlate with brain data. Moreover, we identify
that tasks dealing with fine-grained concept meaning yield representations that
better decode brain activation patterns than other tasks, especially the
syntactic chunking task. This indicates that our brain encodes more
fine-grained concept information than shallow syntactic information when
representing languages.",None,-1
312010a5-52b6-41e7-a26e-f8e5c67961e4,Enhancing Low Resource NER Using Assisting Language And Transfer Learning,0.507972,"Named Entity Recognition (NER) is a fundamental task in NLP that is used to
locate the key information in text and is primarily applied in conversational
and search systems. In commercial applications, NER or comparable slot-filling
methods have been widely deployed for popular languages. NER is used in
applications such as human resources, customer service, search engines, content
classification, and academia. In this paper, we draw focus on identifying name
entities for low-resource Indian languages that are closely related, like Hindi
and Marathi. We use various adaptations of BERT such as baseBERT, AlBERT, and
RoBERTa to train a supervised NER model. We also compare multilingual models
with monolingual models and establish a baseline. In this work, we show the
assisting capabilities of the Hindi and Marathi languages for the NER task. We
show that models trained using multiple languages perform better than a single
language. However, we also observe that blind mixing of all datasets doesn't
necessarily provide improvements and data selection methods may be required.",None,-1
2694b83f-66d1-42f0-8596-4e263b56ed79,Implementation of a noisy hyperlink removal system: A semantic and relatedness approach,0.446838,"As the volume of data on the web grows, the web structure graph, which is a
graph representation of the web, continues to evolve. The structure of this
graph has gradually shifted from content-based to non-content-based.
Furthermore, spam data, such as noisy hyperlinks, in the web structure graph
adversely affect the speed and efficiency of information retrieval and link
mining algorithms. Previous works in this area have focused on removing noisy
hyperlinks using structural and string approaches. However, these approaches
may incorrectly remove useful links or be unable to detect noisy hyperlinks in
certain circumstances. In this paper, a data collection of hyperlinks is
initially constructed using an interactive crawler. The semantic and
relatedness structure of the hyperlinks is then studied through semantic web
approaches and tools such as the DBpedia ontology. Finally, the removal process
of noisy hyperlinks is carried out using a reasoner on the DBpedia ontology.
Our experiments demonstrate the accuracy and ability of semantic web
technologies to remove noisy hyperlinks",None,-1
5c7f1279-fb2d-4697-a75e-6c4607f40eb6,THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech,0.598444,"Detecting harmful content on social media, such as Twitter, is made difficult
by the fact that the seemingly simple yes/no classification conceals a
significant amount of complexity. Unfortunately, while several datasets have
been collected for training classifiers in hate and offensive speech, there is
a scarcity of datasets labeled with a finer granularity of target classes and
specific targets. In this paper, we introduce THOS, a dataset of 8.3k tweets
manually labeled with fine-grained annotations about the target of the message.
We demonstrate that this dataset makes it feasible to train classifiers, based
on Large Language Models, to perform classification at this level of
granularity.",None,-1
49b3f04c-ffb4-4efd-94f4-4558e7c7aaca,QDax: A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration,0.475077,"QDax is an open-source library with a streamlined and modular API for
Quality-Diversity (QD) optimization algorithms in Jax. The library serves as a
versatile tool for optimization purposes, ranging from black-box optimization
to continuous control. QDax offers implementations of popular QD,
Neuroevolution, and Reinforcement Learning (RL) algorithms, supported by
various examples. All the implementations can be just-in-time compiled with
Jax, facilitating efficient execution across multiple accelerators, including
GPUs and TPUs. These implementations effectively demonstrate the framework's
flexibility and user-friendliness, easing experimentation for research
purposes. Furthermore, the library is thoroughly documented and tested with
95\% coverage.",None,-1
d748dee7-60e6-44ae-914c-c351029c3583,Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation,0.961397,"Large language models (LLMs) demonstrate remarkable machine translation (MT)
abilities via prompting, even though they were not explicitly trained for this
task. However, even given the incredible quantities of data they are trained
on, LLMs can struggle to translate inputs with rare words, which are common in
low resource or domain transfer scenarios. We show that LLM prompting can
provide an effective solution for rare words as well, by using prior knowledge
from bilingual dictionaries to provide control hints in the prompts. We propose
a novel method, DiPMT, that provides a set of possible translations for a
subset of the input words, thereby enabling fine-grained phrase-level prompted
control of the LLM. Extensive experiments show that DiPMT outperforms the
baseline both in low-resource MT, as well as for out-of-domain MT. We further
provide a qualitative analysis of the benefits and limitations of this
approach, including the overall level of controllability that is achieved.",None,-1
5580348a-6ae5-4d17-a564-d99c6c440416,Scale-Equivariant UNet for Histopathology Image Segmentation,0.451235,"Digital histopathology slides are scanned and viewed under different
magnifications and stored as images at different resolutions. Convolutional
Neural Networks (CNNs) trained on such images at a given scale fail to
generalise to those at different scales. This inability is often addressed by
augmenting training data with re-scaled images, allowing a model with
sufficient capacity to learn the requisite patterns. Alternatively, designing
CNN filters to be scale-equivariant frees up model capacity to learn
discriminative features. In this paper, we propose the Scale-Equivariant UNet
(SEUNet) for image segmentation by building on scale-space theory. The SEUNet
contains groups of filters that are linear combinations of Gaussian basis
filters, whose scale parameters are trainable but constrained to span disjoint
scales through the layers of the network. Extensive experiments on a nuclei
segmentation dataset and a tissue type segmentation dataset demonstrate that
our method outperforms other approaches, with much fewer trainable parameters.",None,-1
c8503cac-2b49-4126-9b97-1d7601ad0c16,Weighted Notions of Fairness with Binary Supermodular Chores,0.112335,"We study the problem of allocating indivisible chores among agents with
binary supermodular cost functions. In other words, each chore has a marginal
cost of $0$ or $1$ and chores exhibit increasing marginal costs (or decreasing
marginal utilities). In this note, we combine the techniques of Viswanathan and
Zick (2022) and Barman et al. (2023) to present a general framework for fair
allocation with this class of valuation functions. Our framework allows us to
generalize the results of Barman et al. (2023) and efficiently compute
allocations which satisfy weighted notions of fairness like weighted leximin or
min weighted $p$-mean malfare for any $p \ge 1$.",None,-1
db66ead5-d18e-432f-b7fc-8be39b61f506,Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models,0.688304,"Fine-tuning large models is highly effective, however, inference can be
expensive and produces carbon emissions. Knowledge distillation has been shown
to be a practical solution to reduce inference costs, but the distillation
process itself requires significant computational resources. Rather than buying
or renting GPUs to fine-tune, then distill a large model, an NLP practitioner
might instead choose to allocate the available budget to hire annotators and
manually label additional fine-tuning data. In this paper, we investigate how
to most efficiently use a fixed budget to build a compact model. Through
extensive experiments on six diverse tasks, we show that distilling from T5-XXL
(11B) to T5-Small (60M) is almost always a cost-efficient strategy compared to
annotating more data to directly train a compact model (T5-Small). We further
investigate how the optimal budget allocated towards computation varies across
scenarios. We will make our code, datasets, annotation cost estimates, and
baseline models available as a benchmark to support further work on
cost-efficient training of compact models.",None,-1
d6e9d065-f475-4534-b9b6-f8ffc6d7a72c,DiffColor: Toward High Fidelity Text-Guided Image Colorization with Diffusion Models,0.115417,"Recent data-driven image colorization methods have enabled automatic or
reference-based colorization, while still suffering from unsatisfactory and
inaccurate object-level color control. To address these issues, we propose a
new method called DiffColor that leverages the power of pre-trained diffusion
models to recover vivid colors conditioned on a prompt text, without any
additional inputs. DiffColor mainly contains two stages: colorization with
generative color prior and in-context controllable colorization. Specifically,
we first fine-tune a pre-trained text-to-image model to generate colorized
images using a CLIP-based contrastive loss. Then we try to obtain an optimized
text embedding aligning the colorized image and the text prompt, and a
fine-tuned diffusion model enabling high-quality image reconstruction. Our
method can produce vivid and diverse colors with a few iterations, and keep the
structure and background intact while having colors well-aligned with the
target language guidance. Moreover, our method allows for in-context
colorization, i.e., producing different colorization results by modifying
prompt texts without any fine-tuning, and can achieve object-level controllable
colorization results. Extensive experiments and user studies demonstrate that
DiffColor outperforms previous works in terms of visual quality, color
fidelity, and diversity of colorization options.",None,-1
021c4475-b402-4b40-a659-6d4344c83cad,Instance-dependent Noisy-label Learning with Graphical Model Based Noise-rate Estimation,0.162476,"Deep learning faces a formidable challenge when handling noisy labels, as
models tend to overfit samples affected by label noise. This challenge is
further compounded by the presence of instance-dependent noise (IDN), a
realistic form of label noise arising from ambiguous sample information. To
address IDN, Label Noise Learning (LNL) incorporates a sample selection stage
to differentiate clean and noisy-label samples. This stage uses an arbitrary
criterion and a pre-defined curriculum that initially selects most samples as
noisy and gradually decreases this selection rate during training. Such
curriculum is sub-optimal since it does not consider the actual label noise
rate in the training set. This paper addresses this issue with a new noise-rate
estimation method that is easily integrated with most state-of-the-art (SOTA)
LNL methods to produce a more effective curriculum. Synthetic and real-world
benchmark results demonstrate that integrating our approach with SOTA LNL
methods improves accuracy in most cases.",None,-1
17cfa229-95d9-46d7-9561-905462d20bd0,Multimodal Subtask Graph Generation from Instructional Videos,0.694446,"Real-world tasks consist of multiple inter-dependent subtasks (e.g., a dirty
pan needs to be washed before it can be used for cooking). In this work, we aim
to model the causal dependencies between such subtasks from instructional
videos describing the task. This is a challenging problem since complete
information about the world is often inaccessible from videos, which demands
robust learning mechanisms to understand the causal structure of events. We
present Multimodal Subtask Graph Generation (MSG2), an approach that constructs
a Subtask Graph defining the dependency between a task's subtasks relevant to a
task from noisy web videos. Graphs generated by our multimodal approach are
closer to human-annotated graphs compared to prior approaches. MSG2 further
performs the downstream task of next subtask prediction 85% and 30% more
accurately than recent video transformer models in the ProceL and CrossTask
datasets, respectively.",None,-1
24211e47-bfd2-4c54-9f28-fe7688c114fa,From Model-Based to Data-Driven Simulation: Challenges and Trends in Autonomous Driving,0.752989,"Simulation is an integral part in the process of developing autonomous
vehicles and advantageous for training, validation, and verification of driving
functions. Even though simulations come with a series of benefits compared to
real-world experiments, various challenges still prevent virtual testing from
entirely replacing physical test-drives. Our work provides an overview of these
challenges with regard to different aspects and types of simulation and
subsumes current trends to overcome them. We cover aspects around perception-,
behavior- and content-realism as well as general hurdles in the domain of
simulation. Among others, we observe a trend of data-driven, generative
approaches and high-fidelity data synthesis to increasingly replace model-based
simulation.",None,-1
34d88ae1-ab73-4101-9487-c7914168e312,Intention-Aware Decision-Making for Mixed Intersection Scenarios,0.670909,"This paper presents a white-box intention-aware decision-making for the
handling of interactions between a pedestrian and an automated vehicle (AV) in
an unsignalized street crossing scenario. Moreover, a design framework has been
developed, which enables automated parameterization of the decision-making.
This decision-making is designed in such a manner that it can understand
pedestrians in urban traffic and can react accordingly to their intentions.
That way, a human-like response to the actions of the pedestrian is ensured,
leading to a higher acceptance of AVs. The core notion of this paper is that
the intention prediction of the pedestrian to cross the street and
decision-making are divided into two subsystems. On the one hand, the intention
detection is a data-driven, black-box model. Thus, it can model the complex
behavior of the pedestrians. On the other hand, the decision-making is a
white-box model to ensure traceability and to enable a rapid verification and
validation of AVs. This white-box decision-making provides human-like behavior
and a guaranteed prevention of deadlocks. An additional benefit is that the
proposed decision-making requires low computational resources only enabling
real world usage. The automated parameterization uses a particle swarm
optimization and compares two different models of the pedestrian: The social
force model and the Markov decision process model. Consequently, a rapid design
of the decision-making is possible and different pedestrian behaviors can be
taken into account. The results reinforce the applicability of the proposed
intention-aware decision-making.",None,-1
2203a033-7a7d-454a-807f-4bb4b49d0b2b,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,0.878276,"Few-shot semantic segmentation is the task of learning to locate each pixel
of the novel class in the query image with only a few annotated support images.
The current correlation-based methods construct pair-wise feature correlations
to establish the many-to-many matching because the typical prototype-based
approaches cannot learn fine-grained correspondence relations. However, the
existing methods still suffer from the noise contained in naive correlations
and the lack of context semantic information in correlations. To alleviate
these problems mentioned above, we propose a Feature-Enhanced Context-Aware
Network (FECANet). Specifically, a feature enhancement module is proposed to
suppress the matching noise caused by inter-class local similarity and enhance
the intra-class relevance in the naive correlation. In addition, we propose a
novel correlation reconstruction module that encodes extra correspondence
relations between foreground and background and multi-scale context semantic
features, significantly boosting the encoder to capture a reliable matching
pattern. Experiments on PASCAL-$5^i$ and COCO-$20^i$ datasets demonstrate that
our proposed FECANet leads to remarkable improvement compared to previous
state-of-the-arts, demonstrating its effectiveness.",None,-1
3c4b2648-16da-45f1-b546-ce5865527999,MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking,0.99802,"As a video task, Multiple Object Tracking (MOT) is expected to capture
temporal information of targets effectively. Unfortunately, most existing
methods only explicitly exploit the object features between adjacent frames,
while lacking the capacity to model long-term temporal information. In this
paper, we propose MeMOTR, a long-term memory-augmented Transformer for
multi-object tracking. Our method is able to make the same object's track
embedding more stable and distinguishable by leveraging long-term memory
injection with a customized memory-attention layer. This significantly improves
the target association ability of our model. Experimental results on DanceTrack
show that MeMOTR impressively surpasses the state-of-the-art method by 7.9% and
13.0% on HOTA and AssA metrics, respectively. Furthermore, our model also
outperforms other Transformer-based methods on association performance on MOT17
and generalizes well on BDD100K. Code is available at
https://github.com/MCG-NJU/MeMOTR.",None,-1
887cc5e1-b1e4-497c-a485-a1f5a68a3163,Customising General Large Language Models for Specialised Emotion Recognition Tasks,0.526573,"The advent of large language models (LLMs) has gained tremendous attention
over the past year. Previous studies have shown the astonishing performance of
LLMs not only in other tasks but also in emotion recognition in terms of
accuracy, universality, explanation, robustness, few/zero-shot learning, and
others. Leveraging the capability of LLMs inevitably becomes an essential
solution for emotion recognition. To this end, we further comprehensively
investigate how LLMs perform in linguistic emotion recognition if we
concentrate on this specific task. Specifically, we exemplify a publicly
available and widely used LLM -- Chat General Language Model, and customise it
for our target by using two different modal adaptation techniques, i.e., deep
prompt tuning and low-rank adaptation. The experimental results obtained on six
widely used datasets present that the adapted LLM can easily outperform other
state-of-the-art but specialised deep models. This indicates the strong
transferability and feasibility of LLMs in the field of emotion recognition.",None,-1
d0ca14fd-ee0f-4cfa-928b-5592b5ec4835,Improving Continual Relation Extraction by Distinguishing Analogous Semantics,0.628304,"Continual relation extraction (RE) aims to learn constantly emerging
relations while avoiding forgetting the learned relations. Existing works store
a small number of typical samples to re-train the model for alleviating
forgetting. However, repeatedly replaying these samples may cause the
overfitting problem. We conduct an empirical study on existing works and
observe that their performance is severely affected by analogous relations. To
address this issue, we propose a novel continual extraction model for analogous
relations. Specifically, we design memory-insensitive relation prototypes and
memory augmentation to overcome the overfitting problem. We also introduce
integrated training and focal knowledge distillation to enhance the performance
on analogous relations. Experimental results show the superiority of our model
and demonstrate its effectiveness in distinguishing analogous relations and
overcoming overfitting.",None,-1
fa340f2e-1e27-46c8-b960-15b1cf50c84f,ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models,0.0456665,"As large language models are integrated into society, robustness toward a
suite of prompts is increasingly important to maintain reliability in a
high-variance environment.Robustness evaluations must comprehensively
encapsulate the various settings in which a user may invoke an intelligent
system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming,
consisting of three methods -- semantically aligned augmentation, target
bootstrapping, and adversarial knowledge injection. For robust safety
evaluation, we apply these methods in the critical domain of AI safety to
algorithmically generate a test suite of prompts covering diverse robustness
settings -- semantic equivalence, related scenarios, and adversarial. We
partition our prompts into four safety domains for a fine-grained analysis of
how the domain affects model performance. Despite dedicated safeguards in
existing state-of-the-art models, we find statistically significant performance
differences of up to 11% in absolute classification accuracy among semantically
related scenarios and error rates of up to 19% absolute error in zero-shot
adversarial settings, raising concerns for users' physical safety.",None,-1
0aa40264-e1d5-42bd-9e3e-0a4497b922d7,StraIT: Non-autoregressive Generation with Stratified Image Transformer,0.086594,"We propose Stratified Image Transformer(StraIT), a pure
non-autoregressive(NAR) generative model that demonstrates superiority in
high-quality image synthesis over existing autoregressive(AR) and diffusion
models(DMs). In contrast to the under-exploitation of visual characteristics in
existing vision tokenizer, we leverage the hierarchical nature of images to
encode visual tokens into stratified levels with emergent properties. Through
the proposed image stratification that obtains an interlinked token pair, we
alleviate the modeling difficulty and lift the generative power of NAR models.
Our experiments demonstrate that StraIT significantly improves NAR generation
and out-performs existing DMs and AR methods while being order-of-magnitude
faster, achieving FID scores of 3.96 at 256*256 resolution on ImageNet without
leveraging any guidance in sampling or auxiliary image classifiers. When
equipped with classifier-free guidance, our method achieves an FID of 3.36 and
IS of 259.3. In addition, we illustrate the decoupled modeling process of
StraIT generation, showing its compelling properties on applications including
domain transfer.",None,-1
9b90734a-db7d-4c02-9e30-023458fc27b2,Learnings from Data Integration for Augmented Language Models,0.215979,"One of the limitations of large language models is that they do not have
access to up-to-date, proprietary or personal data. As a result, there are
multiple efforts to extend language models with techniques for accessing
external data. In that sense, LLMs share the vision of data integration systems
whose goal is to provide seamless access to a large collection of heterogeneous
data sources. While the details and the techniques of LLMs differ greatly from
those of data integration, this paper shows that some of the lessons learned
from research on data integration can elucidate the research path we are
conducting today on language models.",None,-1
60c72d9d-58c2-4302-8e4e-92049603437d,Physics-Aware Semi-Supervised Underwater Image Enhancement,0.512716,"Underwater images normally suffer from degradation due to the transmission
medium of water bodies. Both traditional prior-based approaches and deep
learning-based methods have been used to address this problem. However, the
inflexible assumption of the former often impairs their effectiveness in
handling diverse underwater scenes, while the generalization of the latter to
unseen images is usually weakened by insufficient data. In this study, we
leverage both the physics-based underwater Image Formation Model (IFM) and deep
learning techniques for Underwater Image Enhancement (UIE). To this end, we
propose a novel Physics-Aware Dual-Stream Underwater Image Enhancement Network,
i.e., PA-UIENet, which comprises a Transmission Estimation Steam (T-Stream) and
an Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE
task by explicitly estimating the degradation parameters of the IFM. We also
adopt an IFM-inspired semi-supervised learning framework, which exploits both
the labeled and unlabeled images, to address the issue of insufficient data.
Our method performs better than, or at least comparably to, eight baselines
across five testing sets in the degradation estimation and UIE tasks. This
should be due to the fact that it not only can model the degradation but also
can learn the characteristics of diverse underwater scenes.",None,-1
ccdc0af6-b5fd-472b-8f16-841968f897cc,Artificial Intelligence Impact On The Labour Force -- Searching For The Analytical Skills Of The Future Software Engineers,0.236324,"This systematic literature review aims to investigate the impact of
artificial intelligence (AI) on the labour force in software engineering, with
a particular focus on the skills needed for future software engineers, the
impact of AI on the demand for software engineering skills, and the future of
work for software engineers. The review identified 42 relevant publications
through a comprehensive search strategy and analysed their findings. The
results indicate that future software engineers will need to be competent in
programming and have soft skills such as problem-solving and interpersonal
communication. AI will have a significant impact on the software engineering
workforce, with the potential to automate many jobs currently done by software
engineers. The role of a software engineer is changing and will continue to
change in the future, with AI-assisted software development posing challenges
for the software engineering profession. The review suggests that the software
engineering profession must adapt to the changing landscape to remain relevant
and effective in the future.",None,-1
f1491dcd-51ec-4135-bcc6-42bd8445f4fd,Towards A Visual Programming Tool to Create Deep Learning Models,0.217044,"Deep Learning (DL) developers come from different backgrounds, e.g.,
medicine, genomics, finance, and computer science. To create a DL model, they
must learn and use high-level programming languages (e.g., Python), thus
needing to handle related setups and solve programming errors. This paper
presents DeepBlocks, a visual programming tool that allows DL developers to
design, train, and evaluate models without relying on specific programming
languages. DeepBlocks works by building on the typical model structure: a
sequence of learnable functions whose arrangement defines the specific
characteristics of the model. We derived DeepBlocks' design goals from a
5-participants formative interview, and we validated the first implementation
of the tool through a typical use case. Results are promising and show that
developers could visually design complex DL architectures.",None,-1
42fcddd4-e919-47d9-bec3-445869493d71,A Unified View of Evaluation Metrics for Structured Prediction,0.369884,"We present a conceptual framework that unifies a variety of evaluation
metrics for different structured prediction tasks (e.g. event and relation
extraction, syntactic and semantic parsing). Our framework requires
representing the outputs of these tasks as objects of certain data types, and
derives metrics through matching of common substructures, possibly followed by
normalization. We demonstrate how commonly used metrics for a number of tasks
can be succinctly expressed by this framework, and show that new metrics can be
naturally derived in a bottom-up way based on an output structure. We release a
library that enables this derivation to create new metrics. Finally, we
consider how specific characteristics of tasks motivate metric design
decisions, and suggest possible modifications to existing metrics in line with
those motivations.",None,-1
9834548b-e499-49f1-879a-ba14c46cc5da,Image-text Retrieval via Preserving Main Semantics of Vision,0.0570601,"Image-text retrieval is one of the major tasks of cross-modal retrieval.
Several approaches for this task map images and texts into a common space to
create correspondences between the two modalities. However, due to the content
(semantics) richness of an image, redundant secondary information in an image
may cause false matches. To address this issue, this paper presents a semantic
optimization approach, implemented as a Visual Semantic Loss (VSL), to assist
the model in focusing on an image's main content. This approach is inspired by
how people typically annotate the content of an image by describing its main
content. Thus, we leverage the annotated texts corresponding to an image to
assist the model in capturing the main content of the image, reducing the
negative impact of secondary content. Extensive experiments on two benchmark
datasets (MSCOCO and Flickr30K) demonstrate the superior performance of our
method. The code is available at: https://github.com/ZhangXu0963/VSL.",None,-1
16f059a6-c004-43bc-a923-59b6892a72e7,Evaluating Inter-Bilingual Semantic Parsing for Indian Languages,0.178496,"Despite significant progress in Natural Language Generation for Indian
languages (IndicNLP), there is a lack of datasets around complex structured
tasks such as semantic parsing. One reason for this imminent gap is the
complexity of the logical form, which makes English to multilingual translation
difficult. The process involves alignment of logical forms, intents and slots
with translated unstructured utterance. To address this, we propose an
Inter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct
Indian languages. We highlight the proposed task's practicality, and evaluate
existing multilingual seq2seq models across several train-test strategies. Our
experiment reveals a high correlation across performance of original
multilingual semantic parsing datasets (such as mTOP, multilingual TOP and
multiATIS++) and our proposed IE-SEMPARSE suite.",None,-1
8df75c13-0327-4bf7-adac-5824fa4fbaca,Towards Mitigating Hallucination in Large Language Models via Self-Reflection,0.940237,"Large language models (LLMs) have shown promise for generative and
knowledge-intensive tasks including question-answering (QA) tasks. However, the
practical deployment still faces challenges, notably the issue of
""hallucination"", where models generate plausible-sounding but unfaithful or
nonsensical information. This issue becomes particularly critical in the
medical domain due to the uncommon professional concepts and potential social
risks involved. This paper analyses the phenomenon of hallucination in medical
generative QA systems using widely adopted LLMs and datasets. Our investigation
centers on the identification and comprehension of common problematic answers,
with a specific emphasis on hallucination. To tackle this challenge, we present
an interactive self-reflection methodology that incorporates knowledge
acquisition and answer generation. Through this feedback process, our approach
steadily enhances the factuality, consistency, and entailment of the generated
answers. Consequently, we harness the interactivity and multitasking ability of
LLMs and produce progressively more precise and accurate answers. Experimental
results on both automatic and human evaluation demonstrate the superiority of
our approach in hallucination reduction compared to baselines.",None,-1
528246c4-3b7f-4d29-9450-d80d21d1a61a,Multistage Spatial Context Models for Learned Image Compression,0.621367,"Recent state-of-the-art Learned Image Compression methods feature spatial
context models, achieving great rate-distortion improvements over hyperprior
methods. However, the autoregressive context model requires serial decoding,
limiting runtime performance. The Checkerboard context model allows parallel
decoding at a cost of reduced RD performance. We present a series of multistage
spatial context models allowing both fast decoding and better RD performance.
We split the latent space into square patches and decode serially within each
patch while different patches are decoded in parallel. The proposed method
features a comparable decoding speed to Checkerboard while reaching the RD
performance of Autoregressive and even also outperforming Autoregressive.
Inside each patch, the decoding order must be carefully decided as a bad order
negatively impacts performance; therefore, we also propose a decoding order
optimization algorithm.",None,-1
1217c730-d3ae-4a82-9e90-8f9cf5c73b48,A GOA-Based Fault-Tolerant Trajectory Tracking Control for an Underwater Vehicle of Multi-Thruster System without Actuator Saturation,0.698725,"This paper proposes an intelligent fault-tolerant control (FTC) strategy to
tackle the trajectory tracking problem of an underwater vehicle (UV) under
thruster damage (power loss) cases and meanwhile resolve the actuator
saturation brought by the vehicle's physical constraints. In the proposed
control strategy, the trajectory tracking component is formed by a refined
backstepping algorithm that controls the velocity variation and a sliding mode
control deducts the torque/force outputs; the fault-tolerant component is
established based on a Grasshopper Optimization Algorithm (GOA), which provides
fast convergence speed as well as satisfactory accuracy of deducting optimized
reallocation of the thruster forces to compensate for the power loss in
different fault cases. Simulations with or without environmental perturbations
under different fault cases and comparisons to other traditional FTCs are
presented, thus verifying the effectiveness and robustness of the proposed
GOA-based fault-tolerant trajectory tracking design.",None,-1
5341b3ae-61d4-4599-9f35-19e085c04e7e,DiffusionPoser: Real-time Human Motion Reconstruction From Arbitrary Sparse Sensors Using Autoregressive Diffusion,0.104187,"Motion capture from a limited number of body-worn sensors, such as inertial
measurement units (IMUs) and pressure insoles, has important applications in
health, human performance, and entertainment. Recent work has focused on
accurately reconstructing whole-body motion from a specific sensor
configuration using six IMUs. While a common goal across applications is to use
the minimal number of sensors to achieve required accuracy, the optimal
arrangement of the sensors might differ from application to application. We
propose a single diffusion model, DiffusionPoser, which reconstructs human
motion in real-time from an arbitrary combination of sensors, including IMUs
placed at specified locations, and, pressure insoles. Unlike existing methods,
our model grants users the flexibility to determine the number and arrangement
of sensors tailored to the specific activity of interest, without the need for
retraining. A novel autoregressive inferencing scheme ensures real-time motion
reconstruction that closely aligns with measured sensor signals. The generative
nature of DiffusionPoser ensures realistic behavior, even for
degrees-of-freedom not directly measured. Qualitative results can be found on
our website: https://diffusionposer.github.io/.",None,-1
3c62799f-a6ba-4856-b45f-6099300137dd,OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset,0.359793,"Inspired by humans comprehending speech in a multi-modal manner, various
audio-visual datasets have been constructed. However, most existing datasets
focus on English, induce dependencies with various prediction models during
dataset preparation, and have only a small number of multi-view videos. To
mitigate the limitations, we recently developed the Open Large-scale Korean
Audio-Visual Speech (OLKAVS) dataset, which is the largest among publicly
available audio-visual speech datasets. The dataset contains 1,150 hours of
transcribed audio from 1,107 Korean speakers in a studio setup with nine
different viewpoints and various noise situations. We also provide the
pre-trained baseline models for two tasks, audio-visual speech recognition and
lip reading. We conducted experiments based on the models to verify the
effectiveness of multi-modal and multi-view training over uni-modal and
frontal-view-only training. We expect the OLKAVS dataset to facilitate
multi-modal research in broader areas such as Korean speech recognition,
speaker recognition, pronunciation level classification, and mouth motion
analysis.",None,-1
6642ba50-051b-4210-8056-2b4dd38ff868,Language Models can be Logical Solvers,0.435382,"Logical reasoning is a fundamental aspect of human intelligence and a key
component of tasks like problem-solving and decision-making. Recent
advancements have enabled Large Language Models (LLMs) to potentially exhibit
reasoning capabilities, but complex logical reasoning remains a challenge. The
state-of-the-art, solver-augmented language models, use LLMs to parse natural
language logical questions into symbolic representations first and then adopt
external logical solvers to take in the symbolic representations and output the
answers. Despite their impressive performance, any parsing errors will
inevitably result in the failure of the execution of the external logical
solver and no answer to the logical questions. In this paper, we introduce
LoGiPT, a novel language model that directly emulates the reasoning processes
of logical solvers and bypasses the parsing errors by learning to strict
adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly
constructed instruction-tuning dataset derived from revealing and refining the
invisible reasoning process of deductive solvers. Experimental results on two
public deductive reasoning datasets demonstrate that LoGiPT outperforms
state-of-the-art solver-augmented LMs and few-shot prompting methods on
competitive LLMs like ChatGPT or GPT-4.",None,-1
5afe47c4-eb5c-40ca-9a83-69cbb8fcc669,Flexible Techniques for Differentiable Rendering with 3D Gaussians,0.641828,"Fast, reliable shape reconstruction is an essential ingredient in many
computer vision applications. Neural Radiance Fields demonstrated that
photorealistic novel view synthesis is within reach, but was gated by
performance requirements for fast reconstruction of real scenes and objects.
Several recent approaches have built on alternative shape representations, in
particular, 3D Gaussians. We develop extensions to these renderers, such as
integrating differentiable optical flow, exporting watertight meshes and
rendering per-ray normals. Additionally, we show how two of the recent methods
are interoperable with each other. These reconstructions are quick, robust, and
easily performed on GPU or CPU. For code and visual examples, see
https://leonidk.github.io/fmb-plus",None,-1
58292d41-9f15-4aa0-8b54-87cc32516eb6,Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,0.453849,"CT images corrupted by metal artifacts have serious negative effects on
clinical diagnosis. Considering the difficulty of collecting paired data with
ground truth in clinical settings, unsupervised methods for metal artifact
reduction are of high interest. However, it is difficult for previous
unsupervised methods to retain structural information from CT images while
handling the non-local characteristics of metal artifacts. To address these
challenges, we proposed a novel Dense Transformer based Enhanced Coding Network
(DTEC-Net) for unsupervised metal artifact reduction. Specifically, we
introduce a Hierarchical Disentangling Encoder, supported by the high-order
dense process, and transformer to obtain densely encoded sequences with
long-range correspondence. Then, we present a second-order disentanglement
method to improve the dense sequence's decoding process. Extensive experiments
and model discussions illustrate DTEC-Net's effectiveness, which outperforms
the previous state-of-the-art methods on a benchmark dataset, and greatly
reduces metal artifacts while restoring richer texture details.",None,-1
fdc5b5e3-2615-45a6-ad66-338a8d1caceb,ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots,0.650692,"The birth of ChatGPT, a cutting-edge language model-based chatbot developed
by OpenAI, ushered in a new era in AI. However, due to potential pitfalls, its
role in rigorous scientific research is not clear yet. This paper vividly
showcases its innovative application within the field of drug discovery.
Focused specifically on developing anti-cocaine addiction drugs, the study
employs GPT-4 as a virtual guide, offering strategic and methodological
insights to researchers working on generative models for drug candidates. The
primary objective is to generate optimal drug-like molecules with desired
properties. By leveraging the capabilities of ChatGPT, the study introduces a
novel approach to the drug discovery process. This symbiotic partnership
between AI and researchers transforms how drug development is approached.
Chatbots become facilitators, steering researchers towards innovative
methodologies and productive paths for creating effective drug candidates. This
research sheds light on the collaborative synergy between human expertise and
AI assistance, wherein ChatGPT's cognitive abilities enhance the design and
development of potential pharmaceutical solutions. This paper not only explores
the integration of advanced AI in drug discovery but also reimagines the
landscape by advocating for AI-powered chatbots as trailblazers in
revolutionizing therapeutic innovation.",None,-1
64312b62-af73-4426-943f-5a55edc62415,Multimodal Machine Unlearning,0.731273,"Machine Unlearning is the process of removing specific training data samples
and their corresponding effects from an already trained model. It has
significant practical benefits, such as purging private, inaccurate, or
outdated information from trained models without the need for complete
re-training. Unlearning within a multimodal setting presents unique challenges
due to the intrinsic dependencies between different data modalities and the
expensive cost of training on large multimodal datasets and architectures.
Current approaches to machine unlearning have not fully addressed these
challenges. To bridge this gap, we introduce MMUL, a machine unlearning
approach specifically designed for multimodal data and models. MMUL formulates
the multimodal unlearning task by focusing on three key properties: (a):
modality decoupling, which effectively decouples the association between
individual unimodal data points within multimodal inputs marked for deletion,
rendering them as unrelated data points within the model's context, (b):
unimodal knowledge retention, which retains the unimodal representation
capability of the model post-unlearning, and (c): multimodal knowledge
retention, which retains the multimodal representation capability of the model
post-unlearning. MMUL is efficient to train and is not constrained by the
requirement of using a strongly convex loss. Experiments on two multimodal
models and four multimodal benchmark datasets, including vision-language and
graph-language datasets, show that MMUL outperforms existing baselines, gaining
an average improvement of +17.6 points against the best-performing unimodal
baseline in distinguishing between deleted and remaining data. In addition,
MMUL can largely maintain pre-existing knowledge of the original model post
unlearning, with a performance gap of only 0.3 points compared to retraining a
new model from scratch.",None,-1
992a7089-c274-4abd-acbf-bdfbbbdebb6e,Active Visual Exploration Based on Attention-Map Entropy,0.41033,"Active visual exploration addresses the issue of limited sensor capabilities
in real-world scenarios, where successive observations are actively chosen
based on the environment. To tackle this problem, we introduce a new technique
called Attention-Map Entropy (AME). It leverages the internal uncertainty of
the transformer-based model to determine the most informative observations. In
contrast to existing solutions, it does not require additional loss components,
which simplifies the training. Through experiments, which also mimic
retina-like sensors, we show that such simplified training significantly
improves the performance of reconstruction, segmentation and classification on
publicly available datasets.",None,-1
9760d4ea-ba01-42af-b8c5-743c790fae7d,Pose Impact Estimation on Face Recognition using 3D-Aware Synthetic Data with Application to Quality Assessment,0.31313,"Evaluating the quality of facial images is essential for operating face
recognition systems with sufficient accuracy. The recent advances in face
quality standardisation (ISO/IEC CD3 29794-5) recommend the usage of component
quality measures for breaking down face quality into its individual factors,
hence providing valuable feedback for operators to re-capture low-quality
images. In light of recent advances in 3D-aware generative adversarial
networks, we propose a novel dataset, Syn-YawPitch, comprising 1000 identities
with varying yaw-pitch angle combinations. Utilizing this dataset, we
demonstrate that pitch angles beyond 30 degrees have a significant impact on
the biometric performance of current face recognition systems. Furthermore, we
propose a lightweight and explainable pose quality predictor that adheres to
the draft international standard of ISO/IEC CD3 29794-5 and benchmark it
against state-of-the-art face image quality assessment algorithms",None,-1
b48ffa9d-6d61-4171-aba2-ebb46632378e,Emergence of a phonological bias in ChatGPT,0.123732,"Current large language models, such as OpenAI's ChatGPT, have captured the
public's attention because how remarkable they are in the use of language.
Here, I demonstrate that ChatGPT displays phonological biases that are a
hallmark of human language processing. More concretely, just like humans,
ChatGPT has a consonant bias. That is, the chatbot has a tendency to use
consonants over vowels to identify words. This is observed across languages
that differ in their relative distribution of consonants and vowels such as
English and Spanish. Despite the differences in how current artificial
intelligence language models are trained to process linguistic stimuli and how
human infants acquire language, such training seems to be enough for the
emergence of a phonological bias in ChatGPT",None,-1
ec57ba11-6878-412b-a72d-d73d4ff30bee,A Comparative Analysis of Techniques and Algorithms for Recognising Sign Language,0.414441,"Sign language is a visual language that enhances communication between people
and is frequently used as the primary form of communication by people with
hearing loss. Even so, not many people with hearing loss use sign language, and
they frequently experience social isolation. Therefore, it is necessary to
create human-computer interface systems that can offer hearing-impaired people
a social platform. Most commercial sign language translation systems now on the
market are sensor-based, pricey, and challenging to use. Although vision-based
systems are desperately needed, they must first overcome several challenges.
Earlier continuous sign language recognition techniques used hidden Markov
models, which have a limited ability to include temporal information. To get
over these restrictions, several machine learning approaches are being applied
to transform hand and sign language motions into spoken or written language. In
this study, we compare various deep learning techniques for recognising sign
language. Our survey aims to provide a comprehensive overview of the most
recent approaches and challenges in this field.",None,-1
1d51748e-0434-4ca8-a69e-c5c461584ea8,Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost,0.437157,"Medical artificial general intelligence (AGI) is an emerging field that aims
to develop systems specifically designed for medical applications that possess
the ability to understand, learn, and apply knowledge across a wide range of
tasks and domains. Large language models (LLMs) represent a significant step
towards AGI. However, training cross-domain LLMs in the medical field poses
significant challenges primarily attributed to the requirement of collecting
data from diverse domains. This task becomes particularly difficult due to
privacy restrictions and the scarcity of publicly available medical datasets.
Here, we propose Medical AGI (MedAGI), a paradigm to unify domain-specific
medical LLMs with the lowest cost, and suggest a possible path to achieve
medical AGI. With an increasing number of domain-specific professional
multimodal LLMs in the medical field being developed, MedAGI is designed to
automatically select appropriate medical models by analyzing users' questions
with our novel adaptive expert selection algorithm. It offers a unified
approach to existing LLMs in the medical field, eliminating the need for
retraining regardless of the introduction of new models. This characteristic
renders it a future-proof solution in the dynamically advancing medical domain.
To showcase the resilience of MedAGI, we conducted an evaluation across three
distinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysis
of pathology pictures. The results demonstrated that MedAGI exhibited
remarkable versatility and scalability, delivering exceptional performance
across diverse domains. Our code is publicly available to facilitate further
research at https://github.com/JoshuaChou2018/MedAGI.",None,-1
4b15597e-8a4d-4c3b-983b-0b5327093cca,Quantifying Consistency and Information Loss for Causal Abstraction Learning,0.345791,"Structural causal models provide a formalism to express causal relations
between variables of interest. Models and variables can represent a system at
different levels of abstraction, whereby relations may be coarsened and refined
according to the need of a modeller. However, switching between different
levels of abstraction requires evaluating a trade-off between the consistency
and the information loss among different models. In this paper we introduce a
family of interventional measures that an agent may use to evaluate such a
trade-off. We consider four measures suited for different tasks, analyze their
properties, and propose algorithms to evaluate and learn causal abstractions.
Finally, we illustrate the flexibility of our setup by empirically showing how
different measures and algorithmic choices may lead to different abstractions.",None,-1
f4e83151-f839-47bd-8590-1955b5769d74,Reconstruction Distortion of Learned Image Compression with Imperceptible Perturbations,0.406351,"Learned Image Compression (LIC) has recently become the trending technique
for image transmission due to its notable performance. Despite its popularity,
the robustness of LIC with respect to the quality of image reconstruction
remains under-explored. In this paper, we introduce an imperceptible attack
approach designed to effectively degrade the reconstruction quality of LIC,
resulting in the reconstructed image being severely disrupted by noise where
any object in the reconstructed images is virtually impossible. More
specifically, we generate adversarial examples by introducing a Frobenius
norm-based loss function to maximize the discrepancy between original images
and reconstructed adversarial examples. Further, leveraging the insensitivity
of high-frequency components to human vision, we introduce Imperceptibility
Constraint (IC) to ensure that the perturbations remain inconspicuous.
Experiments conducted on the Kodak dataset using various LIC models demonstrate
effectiveness. In addition, we provide several findings and suggestions for
designing future defenses.",None,-1
6dc11160-4e2b-4be1-bc94-f971ecea63ef,"Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation",0.7539,"Interpreting ancient Chinese has been the key to comprehending vast Chinese
literature, tradition, and civilization. In this paper, we propose Erya for
ancient Chinese translation. From a dataset perspective, we collect, clean, and
classify ancient Chinese materials from various sources, forming the most
extensive ancient Chinese resource to date. From a model perspective, we devise
Erya training method oriented towards ancient Chinese. We design two
jointly-working tasks: disyllabic aligned substitution (DAS) and dual masked
language model (DMLM). From an evaluation perspective, we build a benchmark to
judge ancient Chinese translation quality in different scenarios and evaluate
the ancient Chinese translation capacities of various existing models. Our
model exhibits remarkable zero-shot performance across five domains, with over
+12.0 BLEU against GPT-3.5 models and better human evaluation results than
ERNIE Bot. Subsequent fine-tuning further shows the superior transfer
capability of Erya model with +6.2 BLEU gain. We release all the
above-mentioned resources at https://github.com/RUCAIBox/Erya.",None,-1
92e5fe84-791e-4f5b-8724-ca61fd1c9e09,MPSA-DenseNet: A novel deep learning model for English accent classification,0.361223,"This paper presents three innovative deep learning models for English accent
classification: Multi-DenseNet, PSA-DenseNet, and MPSE-DenseNet, that combine
multi-task learning and the PSA module attention mechanism with DenseNet. We
applied these models to data collected from six dialects of English across
native English speaking regions (Britain, the United States, Scotland) and
nonnative English speaking regions (China, Germany, India). Our experimental
results show a significant improvement in classification accuracy, particularly
with MPSA-DenseNet, which outperforms all other models, including DenseNet and
EPSA models previously used for accent identification. Our findings indicate
that MPSA-DenseNet is a highly promising model for accurately identifying
English accents.",None,-1
1d82465f-e5b6-4401-b6f3-ab29310d151d,Can Voice Assistants Sound Cute? Towards a Model of Kawaii Vocalics,0.905356,"The Japanese notion of ""kawaii"" or expressions of cuteness, vulnerability,
and/or charm is a global cultural export. Work has explored kawaii-ness as a
design feature and factor of user experience in the visual appearance,
nonverbal behaviour, and sound of robots and virtual characters. In this
initial work, we consider whether voices can be kawaii by exploring the vocal
qualities of voice assistant speech, i.e., kawaii vocalics. Drawing from an
age-inclusive model of kawaii, we ran a user perceptions study on the
kawaii-ness of younger- and older-sounding Japanese computer voices. We found
that kawaii-ness intersected with perceptions of gender and age, i.e., gender
ambiguous and girlish, as well as VA features, i.e., fluency and artificiality.
We propose an initial model of kawaii vocalics to be validated through the
identification and study of vocal qualities, cognitive appraisals, behavioural
responses, and affective reports.",None,-1
27c78ba9-b6db-4665-897b-a7a156b0f9bd,Selectively Hard Negative Mining for Alleviating Gradient Vanishing in Image-Text Matching,0.057904,"Recently, a series of Image-Text Matching (ITM) methods achieve impressive
performance. However, we observe that most existing ITM models suffer from
gradients vanishing at the beginning of training, which makes these models
prone to falling into local minima. Most ITM models adopt triplet loss with
Hard Negative mining (HN) as the optimization objective. We find that
optimizing an ITM model using only the hard negative samples can easily lead to
gradient vanishing. In this paper, we derive the condition under which the
gradient vanishes during training. When the difference between the positive
pair similarity and the negative pair similarity is close to 0, the gradients
on both the image and text encoders will approach 0. To alleviate the gradient
vanishing problem, we propose a Selectively Hard Negative Mining (SelHN)
strategy, which chooses whether to mine hard negative samples according to the
gradient vanishing condition. SelHN can be plug-and-play applied to existing
ITM models to give them better training behavior. To further ensure the
back-propagation of gradients, we construct a Residual Visual Semantic
Embedding model with SelHN, denoted as RVSE++. Extensive experiments on two ITM
benchmarks demonstrate the strength of RVSE++, achieving state-of-the-art
performance.",None,-1
6217f51e-6bc0-4dbf-841f-079970d7c869,Few-shot Neural Radiance Fields Under Unconstrained Illumination,0.102753,"In this paper, we introduce a new challenge for synthesizing novel view
images in practical environments with limited input multi-view images and
varying lighting conditions. Neural radiance fields (NeRF), one of the
pioneering works for this task, demand an extensive set of multi-view images
taken under constrained illumination, which is often unattainable in real-world
settings. While some previous works have managed to synthesize novel views
given images with different illumination, their performance still relies on a
substantial number of input multi-view images. To address this problem, we
suggest ExtremeNeRF, which utilizes multi-view albedo consistency, supported by
geometric alignment. Specifically, we extract intrinsic image components that
should be illumination-invariant across different views, enabling direct
appearance comparison between the input and novel view under unconstrained
illumination. We offer thorough experimental results for task evaluation,
employing the newly created NeRF Extreme benchmark-the first in-the-wild
benchmark for novel view synthesis under multiple viewing directions and
varying illuminations.",None,-1
64d843a9-7454-4eaf-b60e-a027792a8ebf,NeMF: Inverse Volume Rendering with Neural Microflake Field,0.750486,"Recovering the physical attributes of an object's appearance from its images
captured under an unknown illumination is challenging yet essential for
photo-realistic rendering. Recent approaches adopt the emerging implicit scene
representations and have shown impressive results.However, they unanimously
adopt a surface-based representation,and hence can not well handle scenes with
very complex geometry, translucent object and etc. In this paper, we propose to
conduct inverse volume rendering, in contrast to surface-based, by representing
a scene using microflake volume, which assumes the space is filled with
infinite small flakes and light reflects or scatters at each spatial location
according to microflake distributions. We further adopt the coordinate networks
to implicitly encode the microflake volume, and develop a differentiable
microflake volume renderer to train the network in an end-to-end way in
principle.Our NeMF enables effective recovery of appearance attributes for
highly complex geometry and scattering object, enables high-quality relighting,
material editing, and especially simulates volume rendering effects, such as
scattering, which is infeasible for surface-based approaches.",None,-1
f7febcd3-e83b-437f-8866-31684154260c,Exploring the Utility of Self-Supervised Pretraining Strategies for the Detection of Absent Lung Sliding in M-Mode Lung Ultrasound,0.273035,"Self-supervised pretraining has been observed to improve performance in
supervised learning tasks in medical imaging. This study investigates the
utility of self-supervised pretraining prior to conducting supervised
fine-tuning for the downstream task of lung sliding classification in M-mode
lung ultrasound images. We propose a novel pairwise relationship that couples
M-mode images constructed from the same B-mode image and investigate the
utility of data augmentation procedure specific to M-mode lung ultrasound. The
results indicate that self-supervised pretraining yields better performance
than full supervision, most notably for feature extractors not initialized with
ImageNet-pretrained weights. Moreover, we observe that including a vast volume
of unlabelled data results in improved performance on external validation
datasets, underscoring the value of self-supervision for improving
generalizability in automatic ultrasound interpretation. To the authors' best
knowledge, this study is the first to characterize the influence of
self-supervised pretraining for M-mode ultrasound.",None,-1
ef2d9b32-73c8-4c65-af94-4e8041b545ac,SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks,0.989537,"Prompt tuning is a technology that tunes a small set of parameters to steer a
pre-trained language model (LM) to directly generate the output for downstream
tasks. Recently, prompt tuning has demonstrated its storage and computation
efficiency in both natural language processing (NLP) and speech processing
fields. These advantages have also revealed prompt tuning as a candidate
approach to serving pre-trained LM for multiple tasks in a unified manner. For
speech processing, SpeechPrompt shows its high parameter efficiency and
competitive performance on a few speech classification tasks. However, whether
SpeechPrompt is capable of serving a large number of tasks is unanswered. In
this work, we propose SpeechPrompt v2, a prompt tuning framework capable of
performing a wide variety of speech classification tasks, covering multiple
languages and prosody-related tasks. The experiment result shows that
SpeechPrompt v2 achieves performance on par with prior works with less than
0.15M trainable parameters in a unified framework.",None,-1
ac43debe-c020-4f63-97f5-ae9b27dfd0ba,An Examination of the Compositionality of Large Generative Vision-Language Models,0.0502871,"With the success of Large Language Models (LLMs), many Generative
Vision-Language Models (GVLMs) have been constructed via multimodal instruction
tuning. However, the performance of GVLMs in multimodal compositional reasoning
remains under-explored. In this paper, we examine both the evaluation metrics
(VisualGPTScore, etc.) and current benchmarks for evaluating the
compositionality of GVLMs. We identify the syntactical bias in current
benchmarks, which is exploited by the linguistic capability of GVLMs. The bias
renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat
this, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such
bias for mitigation. A challenging new task is subsequently added to evaluate
the robustness of GVLMs against inherent inclination toward syntactical
correctness. Using the bias-mitigated datasets and the new task, we propose a
novel benchmark, namely SyntActically DE-biased benchmark (SADE). Our study
provides an unbiased benchmark for the compositionality of GVLMs, facilitating
future research in this direction (Code and dataset are available at
https://github.com/TeleeMa/SADE).",None,-1
09d7b46b-fa3d-46f0-b7b6-c1b5e77e3204,Sport Task: Fine Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2022,0.453804,"Sports video analysis is a widespread research topic. Its applications are
very diverse, like events detection during a match, video summary, or
fine-grained movement analysis of athletes. As part of the MediaEval 2022
benchmarking initiative, this task aims at detecting and classifying subtle
movements from sport videos. We focus on recordings of table tennis matches.
Conducted since 2019, this task provides a classification challenge from
untrimmed videos recorded under natural conditions with known temporal
boundaries for each stroke. Since 2021, the task also provides a stroke
detection challenge from unannotated, untrimmed videos. This year, the
training, validation, and test sets are enhanced to ensure that all strokes are
represented in each dataset. The dataset is now similar to the one used in [1,
2]. This research is intended to build tools for coaches and athletes who want
to further evaluate their sport performances.",None,-1
80f394af-22cb-4571-90ef-98793ded3d43,JSEEGraph: Joint Structured Event Extraction as Graph Parsing,0.659386,"We propose a graph-based event extraction framework JSEEGraph that approaches
the task of event extraction as general graph parsing in the tradition of
Meaning Representation Parsing. It explicitly encodes entities and events in a
single semantic graph, and further has the flexibility to encode a wider range
of additional IE relations and jointly infer individual tasks. JSEEGraph
performs in an end-to-end manner via general graph parsing: (1) instead of flat
sequence labelling, nested structures between entities/triggers are efficiently
encoded as separate nodes in the graph, allowing for nested and overlapping
entities and triggers; (2) both entities, relations, and events can be encoded
in the same graph, where entities and event triggers are represented as nodes
and entity relations and event arguments are constructed via edges; (3) joint
inference avoids error propagation and enhances the interpolation of different
IE tasks. We experiment on two benchmark datasets of varying structural
complexities; ACE05 and Rich ERE, covering three languages: English, Chinese,
and Spanish. Experimental results show that JSEEGraph can handle nested event
structures, that it is beneficial to solve different IE tasks jointly, and that
event argument extraction in particular benefits from entity extraction. Our
code and models are released as open-source.",None,-1
b977d56d-c234-47a2-bc26-56a0c2c07ad4,Commonsense Knowledge Graph Completion Via Contrastive Pretraining and Node Clustering,0.510248,"The nodes in the commonsense knowledge graph (CSKG) are normally represented
by free-form short text (e.g., word or phrase). Different nodes may represent
the same concept. This leads to the problems of edge sparsity and node
redundancy, which challenges CSKG representation and completion. On the one
hand, edge sparsity limits the performance of graph representation learning; On
the other hand, node redundancy makes different nodes corresponding to the same
concept have inconsistent relations with other nodes. To address the two
problems, we propose a new CSKG completion framework based on Contrastive
Pretraining and Node Clustering (CPNC). Contrastive Pretraining constructs
positive and negative head-tail node pairs on CSKG and utilizes contrastive
learning to obtain better semantic node representation. Node Clustering
aggregates nodes with the same concept into a latent concept, assisting the
task of CSKG completion. We evaluate our CPNC approach on two CSKG completion
benchmarks (CN-100K and ATOMIC), where CPNC outperforms the state-of-the-art
methods. Extensive experiments demonstrate that both Contrastive Pretraining
and Node Clustering can significantly improve the performance of CSKG
completion. The source code of CPNC is publicly available on
\url{https://github.com/NUSTM/CPNC}.",None,-1
21e42d11-e11b-49df-94bc-53d701b59460,"Multimodal Pretrained Models for Verifiable Sequential Decision-Making: Planning, Grounding, and Perception",0.139557,"Recently developed pretrained models can encode rich world knowledge
expressed in multiple modalities, such as text and images. However, the outputs
of these models cannot be integrated into algorithms to solve sequential
decision-making tasks. We develop an algorithm that utilizes the knowledge from
pretrained models to construct and verify controllers for sequential
decision-making tasks, and to ground these controllers to task environments
through visual observations with formal guarantees. In particular, the
algorithm queries a pretrained model with a user-provided, text-based task
description and uses the model's output to construct an automaton-based
controller that encodes the model's task-relevant knowledge. It allows formal
verification of whether the knowledge encoded in the controller is consistent
with other independently available knowledge, which may include abstract
information on the environment or user-provided specifications. Next, the
algorithm leverages the vision and language capabilities of pretrained models
to link the observations from the task environment to the text-based control
logic from the controller (e.g., actions and conditions that trigger the
actions). We propose a mechanism to provide probabilistic guarantees on whether
the controller satisfies the user-provided specifications under perceptual
uncertainties. We demonstrate the algorithm's ability to construct, verify, and
ground automaton-based controllers through a suite of real-world tasks,
including daily life and robot manipulation tasks.",None,-1
7f6c4708-7dc1-4116-b1ed-e1adeb286938,Class-Incremental Learning based on Label Generation,0.776989,"Despite the great success of pre-trained language models, it is still a
challenge to use these models for continual learning, especially for the
class-incremental learning (CIL) setting due to catastrophic forgetting (CF).
This paper reports our finding that if we formulate CIL as a continual label
generation problem, CF is drastically reduced and the generalizable
representations of pre-trained models can be better retained. We thus propose a
new CIL method (VAG) that also leverages the sparsity of vocabulary to focus
the generation and creates pseudo-replay samples by using label semantics.
Experimental results show that VAG outperforms baselines by a large margin.",None,-1
f286d550-6714-4678-af77-307a768d96c3,Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News,0.550671,"This paper explains the participation of team Hitachi to SemEval-2023 Task 3
""Detecting the genre, the framing, and the persuasion techniques in online news
in a multi-lingual setup.'' Based on the multilingual, multi-task nature of the
task and the low-resource setting, we investigated different cross-lingual and
multi-task strategies for training the pretrained language models. Through
extensive experiments, we found that (a) cross-lingual/multi-task training, and
(b) collecting an external balanced dataset, can benefit the genre and framing
detection. We constructed ensemble models from the results and achieved the
highest macro-averaged F1 scores in Italian and Russian genre categorization
subtasks.",None,-1
c2fab99e-09ea-4ef2-974d-fa529a75b538,Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on Summarizing Patients' Active Diagnoses and Problems from Electronic Health Record Progress Notes,0.611931,"The BioNLP Workshop 2023 initiated the launch of a shared task on Problem
List Summarization (ProbSum) in January 2023. The aim of this shared task is to
attract future research efforts in building NLP models for real-world
diagnostic decision support applications, where a system generating relevant
and accurate diagnoses will augment the healthcare providers decision-making
process and improve the quality of care for patients. The goal for participants
is to develop models that generated a list of diagnoses and problems using
input from the daily care notes collected from the hospitalization of
critically ill patients. Eight teams submitted their final systems to the
shared task leaderboard. In this paper, we describe the tasks, datasets,
evaluation metrics, and baseline systems. Additionally, the techniques and
results of the evaluation of the different approaches tried by the
participating teams are summarized.",None,-1
5164e31d-2180-455b-a740-289601e59bcf,Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation,0.463245,"Despite efforts to align large language models to produce harmless responses,
they are still vulnerable to jailbreak prompts that elicit unrestricted
behaviour. In this work, we investigate persona modulation as a black-box
jailbreaking method to steer a target model to take on personalities that are
willing to comply with harmful instructions. Rather than manually crafting
prompts for each persona, we automate the generation of jailbreaks using a
language model assistant. We demonstrate a range of harmful completions made
possible by persona modulation, including detailed instructions for
synthesising methamphetamine, building a bomb, and laundering money. These
automated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is
185 times larger than before modulation (0.23%). These prompts also transfer to
Claude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,
respectively. Our work reveals yet another vulnerability in commercial large
language models and highlights the need for more comprehensive safeguards.",None,-1
1bc59dfe-51ef-4f3c-8e8f-ab6f2de7896f,Danish Foundation Models,0.0684772,"Large language models, sometimes referred to as foundation models, have
transformed multiple fields of research. However, smaller languages risk
falling behind due to high training costs and small incentives for large
companies to train these models. To combat this, the Danish Foundation Models
project seeks to provide and maintain open, well-documented, and high-quality
foundation models for the Danish language. This is achieved through broad
cooperation with public and private institutions, to ensure high data quality
and applicability of the trained models. We present the motivation of the
project, the current status, and future perspectives.",None,-1
5d687106-e916-4e94-a0ac-62163029ada6,Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring,0.775477,"Automated essay scoring (AES) aims to score essays written for a given
prompt, which defines the writing topic. Most existing AES systems assume to
grade essays of the same prompt as used in training and assign only a holistic
score. However, such settings conflict with real-education situations;
pre-graded essays for a particular prompt are lacking, and detailed trait
scores of sub-rubrics are required. Thus, predicting various trait scores of
unseen-prompt essays (called cross-prompt essay trait scoring) is a remaining
challenge of AES. In this paper, we propose a robust model: prompt- and trait
relation-aware cross-prompt essay trait scorer. We encode prompt-aware essay
representation by essay-prompt attention and utilizing the topic-coherence
feature extracted by the topic-modeling mechanism without access to labeled
data; therefore, our model considers the prompt adherence of an essay, even in
a cross-prompt setting. To facilitate multi-trait scoring, we design
trait-similarity loss that encapsulates the correlations of traits. Experiments
prove the efficacy of our model, showing state-of-the-art results for all
prompts and traits. Significant improvements in low-resource-prompt and
inferior traits further indicate our model's strength.",None,-1
033fe709-1ba5-4a13-b2ac-e3c5dd28f680,PanelNet: Understanding 360 Indoor Environment via Panel Representation,0.756741,"Indoor 360 panoramas have two essential properties. (1) The panoramas are
continuous and seamless in the horizontal direction. (2) Gravity plays an
important role in indoor environment design. By leveraging these properties, we
present PanelNet, a framework that understands indoor environments using a
novel panel representation of 360 images. We represent an equirectangular
projection (ERP) as consecutive vertical panels with corresponding 3D panel
geometry. To reduce the negative impact of panoramic distortion, we incorporate
a panel geometry embedding network that encodes both the local and global
geometric features of a panel. To capture the geometric context in room design,
we introduce Local2Global Transformer, which aggregates local information
within a panel and panel-wise global context. It greatly improves the model
performance with low training overhead. Our method outperforms existing methods
on indoor 360 depth estimation and shows competitive results against
state-of-the-art approaches on the task of indoor layout estimation and
semantic segmentation.",None,-1
10cab4d3-b2cf-43d5-8b1a-fe82745b3db1,YaRN: Efficient Context Window Extension of Large Language Models,0.564185,"Rotary Position Embeddings (RoPE) have been shown to effectively encode
positional information in transformer-based language models. However, these
models fail to generalize past the sequence length they were trained on. We
present YaRN (Yet another RoPE extensioN method), a compute-efficient method to
extend the context window of such models, requiring 10x less tokens and 2.5x
less training steps than previous methods. Using YaRN, we show that LLaMA
models can effectively utilize and extrapolate to context lengths much longer
than their original pre-training would allow, while also surpassing previous
the state-of-the-art at context window extension. In addition, we demonstrate
that YaRN exhibits the capability to extrapolate beyond the limited context of
a fine-tuning dataset. The models fine-tuned using YaRN has been made available
and reproduced online up to 128k context length at
https://github.com/jquesnelle/yarn",None,-1
7827d154-fc9a-469f-9983-08c7ef99bf7a,Erasing Concepts from Diffusion Models,0.849365,"Motivated by recent advancements in text-to-image diffusion, we study erasure
of specific concepts from the model's weights. While Stable Diffusion has shown
promise in producing explicit or realistic artwork, it has raised concerns
regarding its potential for misuse. We propose a fine-tuning method that can
erase a visual concept from a pre-trained diffusion model, given only the name
of the style and using negative guidance as a teacher. We benchmark our method
against previous approaches that remove sexually explicit content and
demonstrate its effectiveness, performing on par with Safe Latent Diffusion and
censored training. To evaluate artistic style removal, we conduct experiments
erasing five modern artists from the network and conduct a user study to assess
the human perception of the removed styles. Unlike previous methods, our
approach can remove concepts from a diffusion model permanently rather than
modifying the output at the inference time, so it cannot be circumvented even
if a user has access to model weights. Our code, data, and results are
available at https://erasing.baulab.info/",None,-1
f3b82c29-373a-4176-9961-3002fca19233,Deep Learning based Multi-Label Image Classification of Protest Activities,0.323237,"With the rise of internet technology amidst increasing rates of urbanization,
sharing information has never been easier thanks to globally-adopted platforms
for digital communication. The resulting output of massive amounts of
user-generated data can be used to enhance our understanding of significant
societal issues particularly for urbanizing areas. In order to better analyze
protest behavior, we enhanced the GSR dataset and manually labeled all the
images. We used deep learning techniques to analyze social media data to detect
social unrest through image classification, which performed good in predict
multi-attributes, then also used map visualization to display protest behaviors
across the country.",None,-1
2b1bdd88-f644-45d6-a056-df16d5fde592,Metacognitive threshold: a computational account,0.640985,"This paper will explore ways of computationally accounting for the
metacognitive threshold -- the minimum amount of stimulus needed for a mental
state to be perceived -- and discuss potential cognitive mechanisms by which
this threshold can be influenced through metacognitive training and meditation.",None,-1
2370e40c-125b-4db0-9ed3-d093727a6145,Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction,0.546302,"In this study, we investigated the potential of GPT-3 for the anti-cancer
drug sensitivity prediction task using structured pharmacogenomics data across
five tissue types and evaluated its performance with zero-shot prompting and
fine-tuning paradigms. The drug's smile representation and cell line's genomic
mutation features were predictive of the drug response. The results from this
study have the potential to pave the way for designing more efficient treatment
protocols in precision oncology.",None,-1
fb6a16a4-d29f-45b3-99ec-790002d3431c,Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling,1.0,"We propose a cross-lingual neural codec language model, VALL-E X, for
cross-lingual speech synthesis. Specifically, we extend VALL-E and train a
multi-lingual conditional codec language model to predict the acoustic token
sequences of the target language speech by using both the source language
speech and the target language text as prompts. VALL-E X inherits strong
in-context learning capabilities and can be applied for zero-shot cross-lingual
text-to-speech synthesis and zero-shot speech-to-speech translation tasks.
Experimental results show that it can generate high-quality speech in the
target language via just one speech utterance in the source language as a
prompt while preserving the unseen speaker's voice, emotion, and acoustic
environment. Moreover, VALL-E X effectively alleviates the foreign accent
problems, which can be controlled by a language ID. Audio samples are available
at \url{https://aka.ms/vallex}.",None,-1
a1c32ee9-81d8-42e6-9120-6d44929bf5e0,Probabilistic Uncertainty-Aware Risk Spot Detector for Naturalistic Driving,0.15119,"Risk assessment is a central element for the development and validation of
Autonomous Vehicles (AV). It comprises a combination of occurrence probability
and severity of future critical events. Time Headway (TH) as well as
Time-To-Contact (TTC) are commonly used risk metrics and have qualitative
relations to occurrence probability. However, they lack theoretical derivations
and additionally they are designed to only cover special types of traffic
scenarios (e.g. following between single car pairs). In this paper, we present
a probabilistic situation risk model based on survival analysis considerations
and extend it to naturally incorporate sensory, temporal and behavioral
uncertainties as they arise in real-world scenarios. The resulting Risk Spot
Detector (RSD) is applied and tested on naturalistic driving data of a
multi-lane boulevard with several intersections, enabling the visualization of
road criticality maps. Compared to TH and TTC, our approach is more selective
and specific in predicting risk. RSD concentrates on driving sections of high
vehicle density where large accelerations and decelerations or approaches with
high velocity occur.",None,-1
f7af4d3b-186c-48a8-aff9-08f12f7fe1a3,Contrastive Grouping with Transformer for Referring Image Segmentation,0.867403,"Referring image segmentation aims to segment the target referent in an image
conditioning on a natural language expression. Existing one-stage methods
employ per-pixel classification frameworks, which attempt straightforwardly to
align vision and language at the pixel level, thus failing to capture critical
object-level information. In this paper, we propose a mask classification
framework, Contrastive Grouping with Transformer network (CGFormer), which
explicitly captures object-level information via token-based querying and
grouping strategy. Specifically, CGFormer first introduces learnable query
tokens to represent objects and then alternately queries linguistic features
and groups visual features into the query tokens for object-aware cross-modal
reasoning. In addition, CGFormer achieves cross-level interaction by jointly
updating the query tokens and decoding masks in every two consecutive layers.
Finally, CGFormer cooperates contrastive learning to the grouping strategy to
identify the token and its mask corresponding to the referent. Experimental
results demonstrate that CGFormer outperforms state-of-the-art methods in both
segmentation and generalization settings consistently and significantly.",None,-1
2868206e-01d3-4cee-88c8-44446ab31850,Exploring Large-scale Unlabeled Faces to Enhance Facial Expression Recognition,0.770541,"Facial Expression Recognition (FER) is an important task in computer vision
and has wide applications in human-computer interaction, intelligent security,
emotion analysis, and other fields. However, the limited size of FER datasets
limits the generalization ability of expression recognition models, resulting
in ineffective model performance. To address this problem, we propose a
semi-supervised learning framework that utilizes unlabeled face data to train
expression recognition models effectively. Our method uses a dynamic threshold
module (\textbf{DTM}) that can adaptively adjust the confidence threshold to
fully utilize the face recognition (FR) data to generate pseudo-labels, thus
improving the model's ability to model facial expressions. In the ABAW5 EXPR
task, our method achieved excellent results on the official validation set.",None,-1
2119a483-4d2c-4382-9d5d-5d5be2b59d6d,Privacy-Preserving Joint Edge Association and Power Optimization for the Internet of Vehicles via Federated Multi-Agent Reinforcement Learning,0.0885447,"Proactive edge association is capable of improving wireless connectivity at
the cost of increased handover (HO) frequency and energy consumption, while
relying on a large amount of private information sharing required for decision
making. In order to improve the connectivity-cost trade-off without privacy
leakage, we investigate the privacy-preserving joint edge association and power
allocation (JEAPA) problem in the face of the environmental uncertainty and the
infeasibility of individual learning. Upon modelling the problem by a
decentralized partially observable Markov Decision Process (Dec-POMDP), it is
solved by federated multi-agent reinforcement learning (FMARL) through only
sharing encrypted training data for federatively learning the policy sought.
Our simulation results show that the proposed solution strikes a compelling
trade-off, while preserving a higher privacy level than the state-of-the-art
solutions.",None,-1
e12b0497-5eb0-492b-8b29-a1c9d045e7cb,Robust Dynamic Radiance Fields,0.971927,"Dynamic radiance field reconstruction methods aim to model the time-varying
structure and appearance of a dynamic scene. Existing methods, however, assume
that accurate camera poses can be reliably estimated by Structure from Motion
(SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often
fail or produce erroneous poses on challenging videos with highly dynamic
objects, poorly textured surfaces, and rotating camera motion. We address this
robustness issue by jointly estimating the static and dynamic radiance fields
along with the camera parameters (poses and focal length). We demonstrate the
robustness of our approach via extensive quantitative and qualitative
experiments. Our results show favorable performance over the state-of-the-art
dynamic view synthesis methods.",None,-1
28ff21b0-3dae-487f-a9cf-c8c1ef3a04df,Optimization-Based Eye Tracking using Deflectometric Information,0.766444,"Eye tracking is an important tool with a wide range of applications in
Virtual, Augmented, and Mixed Reality (VR/AR/MR) technologies. State-of-the-art
eye tracking methods are either reflection-based and track reflections of
sparse point light sources, or image-based and exploit 2D features of the
acquired eye image. In this work, we attempt to significantly improve
reflection-based methods by utilizing pixel-dense deflectometric surface
measurements in combination with optimization-based inverse rendering
algorithms. Utilizing the known geometry of our deflectometric setup, we
develop a differentiable rendering pipeline based on PyTorch3D that simulates a
virtual eye under screen illumination. Eventually, we exploit the
image-screen-correspondence information from the captured measurements to find
the eye's rotation, translation, and shape parameters with our renderer via
gradient descent. In general, our method does not require a specific pattern
and can work with ordinary video frames of the main VR/AR/MR screen itself. We
demonstrate real-world experiments with evaluated mean relative gaze errors
below 0.45 degrees at a precision better than 0.11 degrees. Moreover, we show
an improvement of 6X over a representative reflection-based state-of-the-art
method in simulation.",None,-1
1d9ccf00-e9a9-456f-b809-32c38d3c1854,Monocular Visual-Inertial Depth Estimation,0.836452,"We present a visual-inertial depth estimation pipeline that integrates
monocular depth estimation and visual-inertial odometry to produce dense depth
estimates with metric scale. Our approach performs global scale and shift
alignment against sparse metric depth, followed by learning-based dense
alignment. We evaluate on the TartanAir and VOID datasets, observing up to 30%
reduction in inverse RMSE with dense scale alignment relative to performing
just global alignment alone. Our approach is especially competitive at low
density; with just 150 sparse metric depth points, our dense-to-dense depth
alignment method achieves over 50% lower iRMSE over sparse-to-dense depth
completion by KBNet, currently the state of the art on VOID. We demonstrate
successful zero-shot transfer from synthetic TartanAir to real-world VOID data
and perform generalization tests on NYUv2 and VCU-RVI. Our approach is modular
and is compatible with a variety of monocular depth estimation models. Video:
https://youtu.be/IMwiKwSpshQ Code: https://github.com/isl-org/VI-Depth",None,-1
2b63866c-2dd2-483e-9e8d-01b1096a6027,PV2TEA: Patching Visual Modality to Textual-Established Information Extraction,0.147802,"Information extraction, e.g., attribute value extraction, has been
extensively studied and formulated based only on text. However, many attributes
can benefit from image-based extraction, like color, shape, pattern, among
others. The visual modality has long been underutilized, mainly due to
multimodal annotation difficulty. In this paper, we aim to patch the visual
modality to the textual-established attribute information extractor. The
cross-modality integration faces several unique challenges: (C1) images and
textual descriptions are loosely paired intra-sample and inter-samples; (C2)
images usually contain rich backgrounds that can mislead the prediction; (C3)
weakly supervised labels from textual-established extractors are biased for
multimodal training. We present PV2TEA, an encoder-decoder architecture
equipped with three bias reduction schemes: (S1) Augmented label-smoothed
contrast to improve the cross-modality alignment for loosely-paired image and
text; (S2) Attention-pruning that adaptively distinguishes the visual
foreground; (S3) Two-level neighborhood regularization that mitigates the label
textual bias via reliability estimation. Empirical results on real-world
e-Commerce datasets demonstrate up to 11.74% absolute (20.97% relatively) F1
increase over unimodal baselines.",None,-1
1ea03913-e1c5-452f-99d6-2ece277672f0,C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and Generalizable Neural Surface Reconstruction,0.284628,"There is an emerging effort to combine the two popular 3D frameworks using
Multi-View Stereo (MVS) and Neural Implicit Surfaces (NIS) with a specific
focus on the few-shot / sparse view setting. In this paper, we introduce a
novel integration scheme that combines the multi-view stereo with neural signed
distance function representations, which potentially overcomes the limitations
of both methods. MVS uses per-view depth estimation and cross-view fusion to
generate accurate surfaces, while NIS relies on a common coordinate volume.
Based on this strategy, we propose to construct per-view cost frustum for finer
geometry estimation, and then fuse cross-view frustums and estimate the
implicit signed distance functions to tackle artifacts that are due to noise
and holes in the produced surface reconstruction. We further apply a cascade
frustum fusion strategy to effectively captures global-local information and
structural consistency. Finally, we apply cascade sampling and a
pseudo-geometric loss to foster stronger integration between the two
architectures. Extensive experiments demonstrate that our method reconstructs
robust surfaces and outperforms existing state-of-the-art methods.",None,-1
0065d4f8-1b25-461e-8ea1-174671f2dbc0,Contestable Camera Cars: A Speculative Design Exploration of Public AI That Is Open and Responsive to Dispute,0.458317,"Local governments increasingly use artificial intelligence (AI) for automated
decision-making. Contestability, making systems responsive to dispute, is a way
to ensure they respect human rights to autonomy and dignity. We investigate the
design of public urban AI systems for contestability through the example of
camera cars: human-driven vehicles equipped with image sensors. Applying a
provisional framework for contestable AI, we use speculative design to create a
concept video of a contestable camera car. Using this concept video, we then
conduct semi-structured interviews with 17 civil servants who work with AI
employed by a large northwestern European city. The resulting data is analyzed
using reflexive thematic analysis to identify the main challenges facing the
implementation of contestability in public AI. We describe how civic
participation faces issues of representation, public AI systems should
integrate with existing democratic practices, and cities must expand capacities
for responsible AI development and operation.",None,-1
6529184f-f681-483f-868d-fd7d9c754adb,Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking,0.422977,"In many applications of advanced robotic manipulation, six degrees of freedom
(6DoF) object pose estimates are continuously required. In this work, we
develop a multi-modality tracker that fuses information from visual appearance
and geometry to estimate object poses. The algorithm extends our previous
method ICG, which uses geometry, to additionally consider surface appearance.
In general, object surfaces contain local characteristics from text, graphics,
and patterns, as well as global differences from distinct materials and colors.
To incorporate this visual information, two modalities are developed. For local
characteristics, keypoint features are used to minimize distances between
points from keyframes and the current image. For global differences, a novel
region approach is developed that considers multiple regions on the object
surface. In addition, it allows the modeling of external geometries.
Experiments on the YCB-Video and OPT datasets demonstrate that our approach
ICG+ performs best on both datasets, outperforming both conventional and deep
learning-based methods. At the same time, the algorithm is highly efficient and
runs at more than 300 Hz. The source code of our tracker is publicly available.",None,-1
0e36df8a-bf6e-43fe-b44e-008c8af9052c,StableVideo: Text-driven Consistency-aware Diffusion Video Editing,0.982773,"Diffusion-based methods can generate realistic images and videos, but they
struggle to edit existing objects in a video while preserving their appearance
over time. This prevents diffusion models from being applied to natural video
editing in practical scenarios. In this paper, we tackle this problem by
introducing temporal dependency to existing text-driven diffusion models, which
allows them to generate consistent appearance for the edited objects.
Specifically, we develop a novel inter-frame propagation mechanism for
diffusion video editing, which leverages the concept of layered representations
to propagate the appearance information from one frame to the next. We then
build up a text-driven video editing framework based on this mechanism, namely
StableVideo, which can achieve consistency-aware video editing. Extensive
experiments demonstrate the strong editing capability of our approach. Compared
with state-of-the-art video editing methods, our approach shows superior
qualitative and quantitative results. Our code is available at
\href{https://github.com/rese1f/StableVideo}{this https URL}.",None,-1
2cf3f254-4440-45ea-916a-d9bb32b2c66b,CNN-BiLSTM model for English Handwriting Recognition: Comprehensive Evaluation on the IAM Dataset,0.59811,"We present a CNN-BiLSTM system for the problem of offline English handwriting
recognition, with extensive evaluations on the public IAM dataset, including
the effects of model size, data augmentation and the lexicon. Our best model
achieves 3.59\% CER and 9.44\% WER using CNN-BiLSTM network with CTC layer.
Test time augmentation with rotation and shear transformations applied to the
input image, is proposed to increase recognition of difficult cases and found
to reduce the word error rate by 2.5\% points. We also conduct an error
analysis of our proposed method on IAM dataset, show hard cases of handwriting
images and explore samples with erroneous labels. We provide our source code as
public-domain, to foster further research to encourage scientific
reproducibility.",None,-1
cd51be62-4dec-4bba-a7b0-0c65e03de7da,Direct Preference Optimization for Neural Machine Translation with Minimum Bayes Risk Decoding,0.919196,"Minimum Bayes Risk (MBR) decoding can significantly improve translation
performance of Multilingual Large Language Models (MLLMs). However, MBR
decoding is computationally expensive. We show how the recently developed
Reinforcement Learning technique, Direct Preference Optimization (DPO), can
fine-tune MLLMs to get the gains of MBR without any additional computation in
inference. Our method uses only a small monolingual fine-tuning set and yields
significantly improved performance on multiple NMT test sets compared to MLLMs
without DPO.",None,-1
5c75fa8f-d7f8-433d-a8ec-6bfb9b82fd57,Debiased Fine-Tuning for Vision-language Models by Prompt Regularization,0.10428,"We present a new paradigm for fine-tuning large-scale visionlanguage
pre-trained models on downstream task, dubbed Prompt Regularization (ProReg).
Different from traditional fine-tuning which easily overfits to the downstream
task data, ProReg uses the prediction by prompting the pretrained model to
regularize the fine-tuning. The motivation is: by prompting the large model ""a
photo of a [CLASS]"", the fil-lin answer is only dependent on the pretraining
encyclopedic knowledge while independent of the task data distribution, which
is usually biased. Specifically, given a training sample prediction during
fine-tuning, we first calculate its KullbackLeibler loss of the prompt
prediction and Cross-Entropy loss of the ground-truth label, and then combine
them with a proposed sample-wise adaptive trade-off weight, which automatically
adjusts the transfer between the pretrained and downstream domains. On various
out-of-distribution benchmarks, we show the consistently strong performance of
ProReg compared with conventional fine-tuning, zero-shot prompt, prompt tuning,
and other state-of-the-art methods.",None,-1
99d37568-c6fb-4c1d-b164-cce50cc18865,Physics-Preserving AI-Accelerated Simulations of Plasma Turbulence,0.730899,"Turbulence in fluids, gases, and plasmas remains an open problem of both
practical and fundamental importance. Its irreducible complexity usually cannot
be tackled computationally in a brute-force style. Here, we combine Large Eddy
Simulation (LES) techniques with Machine Learning (ML) to retain only the
largest dynamics explicitly, while small-scale dynamics are described by an
ML-based sub-grid-scale model. Applying this novel approach to self-driven
plasma turbulence allows us to remove large parts of the inertial range,
reducing the computational effort by about three orders of magnitude, while
retaining the statistical physical properties of the turbulent system.",None,-1
de42b61b-2061-4eb2-bf36-feb19b59f6b7,Challenges in Domain-Specific Abstractive Summarization and How to Overcome them,0.568393,"Large Language Models work quite well with general-purpose data and many
tasks in Natural Language Processing. However, they show several limitations
when used for a task such as domain-specific abstractive text summarization.
This paper identifies three of those limitations as research problems in the
context of abstractive text summarization: 1) Quadratic complexity of
transformer-based models with respect to the input text length; 2) Model
Hallucination, which is a model's ability to generate factually incorrect text;
and 3) Domain Shift, which happens when the distribution of the model's
training and test corpus is not the same. Along with a discussion of the open
research questions, this paper also provides an assessment of existing
state-of-the-art techniques relevant to domain-specific text summarization to
address the research gaps.",None,-1
ba13d959-4078-4031-9275-bdd435c449df,SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation,0.941997,"We present a novel Speech Augmented Language Model (SALM) with {\em
multitask} and {\em in-context} learning capabilities. SALM comprises a frozen
text LLM, a audio encoder, a modality adapter module, and LoRA layers to
accommodate speech input and associated task instructions. The unified SALM not
only achieves performance on par with task-specific Conformer baselines for
Automatic Speech Recognition (ASR) and Speech Translation (AST), but also
exhibits zero-shot in-context learning capabilities, demonstrated through
keyword-boosting task for ASR and AST. Moreover, {\em speech supervised
in-context training} is proposed to bridge the gap between LLM training and
downstream speech tasks, which further boosts the in-context learning ability
of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.",None,-1
704eed5e-907d-48ec-b5b7-19b65e33dbf9,Does progress on ImageNet transfer to real-world datasets?,0.459414,"Does progress on ImageNet transfer to real-world datasets? We investigate
this question by evaluating ImageNet pre-trained models with varying accuracy
(57% - 83%) on six practical image classification datasets. In particular, we
study datasets collected with the goal of solving real-world tasks (e.g.,
classifying images from camera traps or satellites), as opposed to web-scraped
benchmarks collected for comparing models. On multiple datasets, models with
higher ImageNet accuracy do not consistently yield performance improvements.
For certain tasks, interventions such as data augmentation improve performance
even when architectures do not. We hope that future benchmarks will include
more diverse datasets to encourage a more comprehensive approach to improving
learning algorithms.",None,-1
e14006c4-452f-4cef-9fbd-3345ec97bb73,Manipulating Embeddings of Stable Diffusion Prompts,0.037178,"Prompt engineering is still the primary way for users of generative
text-to-image models to manipulate generated images in a targeted way. Based on
treating the model as a continuous function and by passing gradients between
the image space and the prompt embedding space, we propose and analyze a new
method to directly manipulate the embedding of a prompt instead of the prompt
text. We then derive three practical interaction tools to support users with
image generation: (1) Optimization of a metric defined in the image space that
measures, for example, the image style. (2) Supporting a user in creative tasks
by allowing them to navigate in the image space along a selection of directions
of ""near"" prompt embeddings. (3) Changing the embedding of the prompt to
include information that a user has seen in a particular seed but has
difficulty describing in the prompt. Compared to prompt engineering,
user-driven prompt embedding manipulation enables a more fine-grained, targeted
control that integrates a user's intentions. Our user study shows that our
methods are considered less tedious and that the resulting images are often
preferred.",None,-1
c95ae8b5-9ada-4a41-9792-67469e5bccd6,Semi-supervised Multimodal Representation Learning through a Global Workspace,0.192352,"Recent deep learning models can efficiently combine inputs from different
modalities (e.g., images and text) and learn to align their latent
representations, or to translate signals from one domain to another (as in
image captioning, or text-to-image generation). However, current approaches
mainly rely on brute-force supervised training over large multimodal datasets.
In contrast, humans (and other animals) can learn useful multimodal
representations from only sparse experience with matched cross-modal data. Here
we evaluate the capabilities of a neural network architecture inspired by the
cognitive notion of a ""Global Workspace"": a shared representation for two (or
more) input modalities. Each modality is processed by a specialized system
(pretrained on unimodal data, and subsequently frozen). The corresponding
latent representations are then encoded to and decoded from a single shared
workspace. Importantly, this architecture is amenable to self-supervised
training via cycle-consistency: encoding-decoding sequences should approximate
the identity function. For various pairings of vision-language modalities and
across two datasets of varying complexity, we show that such an architecture
can be trained to align and translate between two modalities with very little
need for matched data (from 4 to 7 times less than a fully supervised
approach). The global workspace representation can be used advantageously for
downstream classification tasks and for robust transfer learning. Ablation
studies reveal that both the shared workspace and the self-supervised
cycle-consistency training are critical to the system's performance.",None,-1
cee00b2d-e747-4eb4-997a-7ddc991a7900,Neural Episodic Control with State Abstraction,0.346652,"Existing Deep Reinforcement Learning (DRL) algorithms suffer from sample
inefficiency. Generally, episodic control-based approaches are solutions that
leverage highly-rewarded past experiences to improve sample efficiency of DRL
algorithms. However, previous episodic control-based approaches fail to utilize
the latent information from the historical behaviors (e.g., state transitions,
topological similarities, etc.) and lack scalability during DRL training. This
work introduces Neural Episodic Control with State Abstraction (NECSA), a
simple but effective state abstraction-based episodic control containing a more
comprehensive episodic memory, a novel state evaluation, and a multi-step state
analysis. We evaluate our approach to the MuJoCo and Atari tasks in OpenAI gym
domains. The experimental results indicate that NECSA achieves higher sample
efficiency than the state-of-the-art episodic control-based approaches. Our
data and code are available at the project
website\footnote{\url{https://sites.google.com/view/drl-necsa}}.",None,-1
b832f8de-c505-470c-a8e9-74bf18de2e45,"Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans",0.15158,"The recently introduced Controlled Text Reduction (CTR) task isolates the
text generation step within typical summarization-style tasks. It does so by
challenging models to generate coherent text conforming to pre-selected content
within the input text (``highlights''). This framing enables increased
modularity in summarization-like tasks, allowing to couple a single CTR model
with various content-selection setups and modules. However, there are currently
no reliable CTR models, while the performance of the existing baseline for the
task is mediocre, falling short of practical utility. Here, we address this gap
by introducing a high-quality, open-source CTR model that tackles two prior key
limitations: inadequate enforcement of the content-preservation constraint, and
suboptimal silver training data. Addressing these, we amplify the
content-preservation constraint in both training, via RL, and inference, via a
controlled decoding strategy. Further, we substantially improve the silver
training data quality via GPT-4 distillation. Overall, pairing the distilled
dataset with the highlight-adherence strategies yields marked gains over the
current baseline, of up to 30 ROUGE-L points, providing a reliable CTR model
for downstream use.",None,-1
451ab7b9-f250-44ab-8339-d5b3ec9e0634,Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM,0.957834,"The NLP community recently saw the release of a new large open-access
multilingual language model, BLOOM (BigScience et al., 2022) covering 46
languages. We focus on BLOOM's multilingual ability by evaluating its machine
translation performance across several datasets (WMT, Flores-101 and DiaBLa)
and language pairs (high- and low-resourced). Our results show that 0-shot
performance suffers from overgeneration and generating in the wrong language,
but this is greatly improved in the few-shot setting, with very good results
for a number of language pairs. We study several aspects including prompt
design, model sizes, cross-lingual transfer and the use of discursive context.",None,-1
61aa2556-484d-4426-9daa-b318f39ecfc6,Multiverse Transformer: 1st Place Solution for Waymo Open Sim Agents Challenge 2023,0.944255,"This technical report presents our 1st place solution for the Waymo Open Sim
Agents Challenge (WOSAC) 2023. Our proposed MultiVerse Transformer for Agent
simulation (MVTA) effectively leverages transformer-based motion prediction
approaches, and is tailored for closed-loop simulation of agents. In order to
produce simulations with a high degree of realism, we design novel training and
sampling methods, and implement a receding horizon prediction mechanism. In
addition, we introduce a variable-length history aggregation method to mitigate
the compounding error that can arise during closed-loop autoregressive
execution. On the WOSAC, our MVTA and its enhanced version MVTE reach a realism
meta-metric of 0.5091 and 0.5168, respectively, outperforming all the other
methods on the leaderboard.",None,-1
a2096b88-b62e-4ba3-91c3-8949065e6fde,"Mathematics, word problems, common sense, and artificial intelligence",0.878225,"The paper discusses the capacities and limitations of current artificial
intelligence (AI) technology to solve word problems that combine elementary
knowledge with commonsense reasoning. No existing AI systems can solve these
reliably. We review three approaches that have been developed, using AI natural
language technology: outputting the answer directly, outputting a computer
program that solves the problem, and outputting a formalized representation
that can be input to an automated theorem verifier. We review some benchmarks
that have been developed to evaluate these systems and some experimental
studies. We discuss the limitations of the existing technology at solving these
kinds of problems. We argue that it is not clear whether these kinds of
limitations will be important in developing AI technology for pure mathematical
research, but that they will be important in applications of mathematics, and
may well be important in developing programs capable of reading and
understanding mathematical content written by humans.",None,-1
81d4d4bd-8037-4708-9df9-da9c0d9829af,SpellMapper: A non-autoregressive neural spellchecker for ASR customization with candidate retrieval based on n-gram mappings,0.798104,"Contextual spelling correction models are an alternative to shallow fusion to
improve automatic speech recognition (ASR) quality given user vocabulary. To
deal with large user vocabularies, most of these models include candidate
retrieval mechanisms, usually based on minimum edit distance between fragments
of ASR hypothesis and user phrases. However, the edit-distance approach is
slow, non-trainable, and may have low recall as it relies only on common
letters. We propose: 1) a novel algorithm for candidate retrieval, based on
misspelled n-gram mappings, which gives up to 90% recall with just the top 10
candidates on Spoken Wikipedia; 2) a non-autoregressive neural model based on
BERT architecture, where the initial transcript and ten candidates are combined
into one input. The experiments on Spoken Wikipedia show 21.4% word error rate
improvement compared to a baseline ASR system.",None,-1
11a66120-aa60-43e7-808c-84e8ddaa42bb,Self-Supervised Video Similarity Learning,0.342565,"We introduce S$^2$VS, a video similarity learning approach with
self-supervision. Self-Supervised Learning (SSL) is typically used to train
deep models on a proxy task so as to have strong transferability on target
tasks after fine-tuning. Here, in contrast to prior work, SSL is used to
perform video similarity learning and address multiple retrieval and detection
tasks at once with no use of labeled data. This is achieved by learning via
instance-discrimination with task-tailored augmentations and the widely used
InfoNCE loss together with an additional loss operating jointly on
self-similarity and hard-negative similarity. We benchmark our method on tasks
where video relevance is defined with varying granularity, ranging from video
copies to videos depicting the same incident or event. We learn a single
universal model that achieves state-of-the-art performance on all tasks,
surpassing previously proposed methods that use labeled data. The code and
pretrained models are publicly available at: https://github.com/gkordo/s2vs",None,-1
aacc5a33-26a6-443a-96f6-8dfcd6ffb170,SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph,0.374799,"Existing multimodal conversation agents have shown impressive abilities to
locate absolute positions or retrieve attributes in simple scenarios, but they
fail to perform well when complex relative positions and information alignments
are involved, which poses a bottleneck in response quality. In this paper, we
propose a Situated Conversation Agent Petrained with Multimodal Questions from
INcremental Layout Graph (SPRING) with abilities of reasoning multi-hops
spatial relations and connecting them with visual attributes in crowded
situated scenarios. Specifically, we design two types of Multimodal Question
Answering (MQA) tasks to pretrain the agent. All QA pairs utilized during
pretraining are generated from novel Incremental Layout Graphs (ILG). QA pair
difficulty labels automatically annotated by ILG are used to promote MQA-based
Curriculum Learning. Experimental results verify the SPRING's effectiveness,
showing that it significantly outperforms state-of-the-art approaches on both
SIMMC 1.0 and SIMMC 2.0 datasets.",None,-1
65a67e87-5873-489e-845f-695838dcc030,FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models,0.093711,"Using model weights pretrained on a high-resource language as a warm start
can reduce the need for data and compute to obtain high-quality language models
for other, especially low-resource, languages. However, if we want to use a new
tokenizer specialized for the target language, we cannot transfer the source
model's embedding matrix. In this paper, we propose FOCUS - Fast Overlapping
Token Combinations Using Sparsemax, a novel embedding initialization method
that initializes the embedding matrix effectively for a new tokenizer based on
information in the source model's embedding matrix. FOCUS represents newly
added tokens as combinations of tokens in the overlap of the source and target
vocabularies. The overlapping tokens are selected based on semantic similarity
in an auxiliary static token embedding space. We focus our study on using the
multilingual XLM-R as a source model and empirically show that FOCUS
outperforms random initialization and previous work in language modeling and on
a range of downstream tasks (NLI, QA, and NER).",None,-1
dbc01e01-b61b-4c63-8e96-d69d2a7f5adf,Gender Neutralization for an Inclusive Machine Translation: from Theoretical Foundations to Open Challenges,0.407708,"Gender inclusivity in language technologies has become a prominent research
topic. In this study, we explore gender-neutral translation (GNT) as a form of
gender inclusivity and a goal to be achieved by machine translation (MT)
models, which have been found to perpetuate gender bias and discrimination.
Specifically, we focus on translation from English into Italian, a language
pair representative of salient gender-related linguistic transfer problems. To
define GNT, we review a selection of relevant institutional guidelines for
gender-inclusive language, discuss its scenarios of use, and examine the
technical challenges of performing GNT in MT, concluding with a discussion of
potential solutions to encourage advancements toward greater inclusivity in MT.",None,-1
48a8f26d-8f81-42c9-af90-c77afaf88b18,Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction,0.423737,"Given their flexibility and encouraging performance, deep-learning models are
becoming standard for motion prediction in autonomous driving. However, with
great flexibility comes a lack of interpretability and possible violations of
physical constraints. Accompanying these data-driven methods with
differentially-constrained motion models to provide physically feasible
trajectories is a promising future direction. The foundation for this work is a
previously introduced graph-neural-network-based model, MTP-GO. The neural
network learns to compute the inputs to an underlying motion model to provide
physically feasible trajectories. This research investigates the performance of
various motion models in combination with numerical solvers for the prediction
task. The study shows that simpler models, such as low-order integrator models,
are preferred over more complex, e.g., kinematic models, to achieve accurate
predictions. Further, the numerical solver can have a substantial impact on
performance, advising against commonly used first-order methods like Euler
forward. Instead, a second-order method like Heun's can greatly improve
predictions.",None,-1
3bf424f3-888b-45f3-a3b1-41e3ccc16fd4,Incorporating Graph Information in Transformer-based AMR Parsing,0.767792,"Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that
aims at providing a semantic graph abstraction representing a given text.
Current approaches are based on autoregressive language models such as BART or
T5, fine-tuned through Teacher Forcing to obtain a linearized version of the
AMR graph from a sentence. In this paper, we present LeakDistill, a model and
method that explores a modification to the Transformer architecture, using
structural adapters to explicitly incorporate graph information into the
learned representations and improve AMR parsing performance. Our experiments
show how, by employing word-to-node alignment to embed graph structural
information into the encoder at training time, we can obtain state-of-the-art
AMR parsing through self-knowledge distillation, even without the use of
additional data. We release the code at
\url{http://www.github.com/sapienzanlp/LeakDistill}.",None,-1
f5b31e46-b470-46c6-bd20-7e20a34e79e3,AMR Parsing with Causal Hierarchical Attention and Pointers,0.236382,"Translation-based AMR parsers have recently gained popularity due to their
simplicity and effectiveness. They predict linearized graphs as free texts,
avoiding explicit structure modeling. However, this simplicity neglects
structural locality in AMR graphs and introduces unnecessary tokens to
represent coreferences. In this paper, we introduce new target forms of AMR
parsing and a novel model, CHAP, which is equipped with causal hierarchical
attention and the pointer mechanism, enabling the integration of structures
into the Transformer decoder. We empirically explore various alternative
modeling options. Experiments show that our model outperforms baseline models
on four out of five benchmarks in the setting of no additional data.",None,-1
4927cf8b-59d7-4227-bd51-4e197b3e7a77,A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain Text Classification,0.101032,"Cross-domain text classification aims to adapt models to a target domain that
lacks labeled data. It leverages or reuses rich labeled data from the different
but related source domain(s) and unlabeled data from the target domain. To this
end, previous work focuses on either extracting domain-invariant features or
task-agnostic features, ignoring domain-aware features that may be present in
the target domain and could be useful for the downstream task. In this paper,
we propose a two-stage framework for cross-domain text classification. In the
first stage, we finetune the model with mask language modeling (MLM) and
labeled data from the source domain. In the second stage, we further fine-tune
the model with self-supervised distillation (SSD) and unlabeled data from the
target domain. We evaluate its performance on a public cross-domain text
classification benchmark and the experiment results show that our method
achieves new state-of-the-art results for both single-source domain adaptations
(94.17% $\uparrow$1.03%) and multi-source domain adaptations (95.09%
$\uparrow$1.34%).",None,-1
3a820d4b-fd4f-4d43-b382-68c3071270d4,EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities,0.89554,"The advent of artificial intelligence has led to a growing emphasis on
data-driven modeling in macroeconomics, with agent-based modeling (ABM)
emerging as a prominent bottom-up simulation paradigm. In ABM, agents (e.g.,
households, firms) interact within a macroeconomic environment, collectively
generating market dynamics. Existing agent modeling typically employs
predetermined rules or learning-based neural networks for decision-making.
However, customizing each agent presents significant challenges, complicating
the modeling of agent heterogeneity. Additionally, the influence of
multi-period market dynamics and multifaceted macroeconomic factors are often
overlooked in decision-making processes. In this work, we introduce EconAgent,
a large language model-empowered agent with human-like characteristics for
macroeconomic simulation. We first construct a simulation environment that
incorporates various market dynamics driven by agents' decisions regarding work
and consumption. Through the perception module, we create heterogeneous agents
with distinct decision-making mechanisms. Furthermore, we model the impact of
macroeconomic trends using a memory module, which allows agents to reflect on
past individual experiences and market dynamics. Simulation experiments show
that EconAgent can make realistic decisions, leading to more reasonable
macroeconomic phenomena compared to existing rule-based or learning-based
agents. Our codes are released at
https://github.com/tsinghua-fib-lab/ACL24-EconAgent.",None,-1
022bbb79-510c-45e2-84cb-8c929a4db60b,CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts,0.520377,"The powerful ability of ChatGPT has caused widespread concern in the academic
community. Malicious users could synthesize dummy academic content through
ChatGPT, which is extremely harmful to academic rigor and originality. The need
to develop ChatGPT-written content detection algorithms call for large-scale
datasets. In this paper, we initially investigate the possible negative impact
of ChatGPT on academia,and present a large-scale CHatGPT-writtEn AbsTract
dataset (CHEAT) to support the development of detection algorithms. In
particular, the ChatGPT-written abstract dataset contains 35,304 synthetic
abstracts, with Generation, Polish, and Mix as prominent representatives. Based
on these data, we perform a thorough analysis of the existing text synthesis
detection algorithms. We show that ChatGPT-written abstracts are detectable,
while the detection difficulty increases with human involvement.Our dataset is
available in https://github.com/botianzhe/CHEAT.",None,-1
9238e12b-9580-43a8-9fc7-df4be0212240,InstructDET: Diversifying Referring Object Detection with Generalized Instructions,0.205529,"We propose InstructDET, a data-centric method for referring object detection
(ROD) that localizes target objects based on user instructions. While deriving
from referring expressions (REC), the instructions we leverage are greatly
diversified to encompass common user intentions related to object detection.
For one image, we produce tremendous instructions that refer to every single
object and different combinations of multiple objects. Each instruction and its
corresponding object bounding boxes (bbxs) constitute one training data pair.
In order to encompass common detection expressions, we involve emerging
vision-language model (VLM) and large language model (LLM) to generate
instructions guided by text prompts and object bbxs, as the generalizations of
foundation models are effective to produce human-like expressions (e.g.,
describing object property, category, and relationship). We name our
constructed dataset as InDET. It contains images, bbxs and generalized
instructions that are from foundation models. Our InDET is developed from
existing REC datasets and object detection datasets, with the expanding
potential that any image with object bbxs can be incorporated through using our
InstructDET method. By using our InDET dataset, we show that a conventional ROD
model surpasses existing methods on standard REC datasets and our InDET test
set. Our data-centric method InstructDET, with automatic data expansion by
leveraging foundation models, directs a promising field that ROD can be greatly
diversified to execute common object detection instructions.",None,-1
fcc8fdbf-5ce1-4dd5-8c8d-fc9606719135,Building Extractive Question Answering System to Support Human-AI Health Coaching Model for Sleep Domain,0.305963,"Non-communicable diseases (NCDs) are a leading cause of global deaths,
necessitating a focus on primary prevention and lifestyle behavior change.
Health coaching, coupled with Question Answering (QA) systems, has the
potential to transform preventive healthcare. This paper presents a
human-Artificial Intelligence (AI) health coaching model incorporating a
domain-specific extractive QA system. A sleep-focused dataset, SleepQA, was
manually assembled and used to fine-tune domain-specific BERT models. The QA
system was evaluated using automatic and human methods. A data-centric
framework enhanced the system's performance by improving passage retrieval and
question reformulation. Although the system did not outperform the baseline in
automatic evaluation, it excelled in the human evaluation of real-world
questions. Integration into a Human-AI health coaching model was tested in a
pilot Randomized Controlled Trial (RCT).",None,-1
2702f085-2466-42f3-adea-f54bbf909012,Implementing Responsible AI: Tensions and Trade-Offs Between Ethics Aspects,0.185686,"Many sets of ethics principles for responsible AI have been proposed to allay
concerns about misuse and abuse of AI/ML systems. The underlying aspects of
such sets of principles include privacy, accuracy, fairness, robustness,
explainability, and transparency. However, there are potential tensions between
these aspects that pose difficulties for AI/ML developers seeking to follow
these principles. For example, increasing the accuracy of an AI/ML system may
reduce its explainability. As part of the ongoing effort to operationalise the
principles into practice, in this work we compile and discuss a catalogue of 10
notable tensions, trade-offs and other interactions between the underlying
aspects. We primarily focus on two-sided interactions, drawing on support
spread across a diverse literature. This catalogue can be helpful in raising
awareness of the possible interactions between aspects of ethics principles, as
well as facilitating well-supported judgements by the designers and developers
of AI/ML systems.",None,-1
0a6ea6b3-6147-46a8-a1d0-bbcfcf737131,Knowledge Acquisition and Completion for Long-Term Human-Robot Interactions using Knowledge Graph Embedding,0.25404,"In Human-Robot Interaction (HRI) systems, a challenging task is sharing the
representation of the operational environment, fusing symbolic knowledge and
perceptions, between users and robots. With the existing HRI pipelines, users
can teach the robots some concepts to increase their knowledge base.
Unfortunately, the data coming from the users are usually not enough dense for
building a consistent representation. Furthermore, the existing approaches are
not able to incrementally build up their knowledge base, which is very
important when robots have to deal with dynamic contexts. To this end, we
propose an architecture to gather data from users and environments in long-runs
of continual learning. We adopt Knowledge Graph Embedding techniques to
generalize the acquired information with the goal of incrementally extending
the robot's inner representation of the environment. We evaluate the
performance of the overall continual learning architecture by measuring the
capabilities of the robot of learning entities and relations coming from
unknown contexts through a series of incremental learning sessions.",None,-1
cd196c9b-fc45-412b-a517-971305a7d657,Unsupervised Synthetic Image Refinement via Contrastive Learning and Consistent Semantic-Structural Constraints,0.377871,"Ensuring the realism of computer-generated synthetic images is crucial to
deep neural network (DNN) training. Due to different semantic distributions
between synthetic and real-world captured datasets, there exists semantic
mismatch between synthetic and refined images, which in turn results in the
semantic distortion. Recently, contrastive learning (CL) has been successfully
used to pull correlated patches together and push uncorrelated ones apart. In
this work, we exploit semantic and structural consistency between synthetic and
refined images and adopt CL to reduce the semantic distortion. Besides, we
incorporate hard negative mining to improve the performance furthermore. We
compare the performance of our method with several other benchmarking methods
using qualitative and quantitative measures and show that our method offers the
state-of-the-art performance.",None,-1
4aad606d-3923-41ec-898e-8e6c5c2a36aa,ChiroDiff: Modelling chirographic data with Diffusion Models,0.343674,"Generative modelling over continuous-time geometric constructs, a.k.a such as
handwriting, sketches, drawings etc., have been accomplished through
autoregressive distributions. Such strictly-ordered discrete factorization
however falls short of capturing key properties of chirographic data -- it
fails to build holistic understanding of the temporal concept due to one-way
visibility (causality). Consequently, temporal data has been modelled as
discrete token sequences of fixed sampling rate instead of capturing the true
underlying concept. In this paper, we introduce a powerful model-class namely
""Denoising Diffusion Probabilistic Models"" or DDPMs for chirographic data that
specifically addresses these flaws. Our model named ""ChiroDiff"", being
non-autoregressive, learns to capture holistic concepts and therefore remains
resilient to higher temporal sampling rate up to a good extent. Moreover, we
show that many important downstream utilities (e.g. conditional sampling,
creative mixing) can be flexibly implemented using ChiroDiff. We further show
some unique use-cases like stochastic vectorization, de-noising/healing,
abstraction are also possible with this model-class. We perform quantitative
and qualitative evaluation of our framework on relevant datasets and found it
to be better or on par with competing approaches.",None,-1
4d139a46-4e8c-4a09-95be-f8ec7b82c333,Face0: Instantaneously Conditioning a Text-to-Image Model on a Face,0.60605,"We present Face0, a novel way to instantaneously condition a text-to-image
generation model on a face, in sample time, without any optimization procedures
such as fine-tuning or inversions. We augment a dataset of annotated images
with embeddings of the included faces and train an image generation model, on
the augmented dataset. Once trained, our system is practically identical at
inference time to the underlying base model, and is therefore able to generate
images, given a user-supplied face image and a prompt, in just a couple of
seconds. Our method achieves pleasing results, is remarkably simple, extremely
fast, and equips the underlying model with new capabilities, like controlling
the generated images both via text or via direct manipulation of the input face
embeddings. In addition, when using a fixed random vector instead of a face
embedding from a user supplied image, our method essentially solves the problem
of consistent character generation across images. Finally, while requiring
further research, we hope that our method, which decouples the model's textual
biases from its biases on faces, might be a step towards some mitigation of
biases in future text-to-image models.",None,-1
48d2a9ff-63bd-428b-8b3f-a19166efbaf4,Data Acquisition: A New Frontier in Data-centric AI,0.801211,"As Machine Learning (ML) systems continue to grow, the demand for relevant
and comprehensive datasets becomes imperative. There is limited study on the
challenges of data acquisition due to ad-hoc processes and lack of consistent
methodologies. We first present an investigation of current data marketplaces,
revealing lack of platforms offering detailed information about datasets,
transparent pricing, standardized data formats. With the objective of inciting
participation from the data-centric AI community, we then introduce the DAM
challenge, a benchmark to model the interaction between the data providers and
acquirers. The benchmark was released as a part of DataPerf. Our evaluation of
the submitted strategies underlines the need for effective data acquisition
strategies in ML.",None,-1
5ee2b8cb-da9a-4d98-870f-151699d6c65f,MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System,0.865593,"Multi-modal sarcasm detection has attracted much recent attention.
Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder
the development of reliable multi-modal sarcasm detection system: (1) There are
some spurious cues in MMSD, leading to the model bias learning; (2) The
negative samples in MMSD are not always reasonable. To solve the aforementioned
issues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings
of MMSD, by removing the spurious cues and re-annotating the unreasonable
samples. Meanwhile, we present a novel framework called multi-view CLIP that is
capable of leveraging multi-grained cues from multiple perspectives (i.e.,
text, image, and text-image interaction view) for multi-modal sarcasm
detection. Extensive experiments show that MMSD2.0 is a valuable benchmark for
building reliable multi-modal sarcasm detection systems and multi-view CLIP can
significantly outperform the previous best baselines.",None,-1
0a61e500-0c8e-4225-8cc5-68c6d9e99815,Visual-Language Prompt Tuning with Knowledge-guided Context Optimization,0.864916,"Prompt tuning is an effective way to adapt the pre-trained visual-language
model (VLM) to the downstream task using task-related textual tokens.
Representative CoOp-based work combines the learnable textual tokens with the
class tokens to obtain specific textual knowledge. However, the specific
textual knowledge is the worse generalization to the unseen classes because it
forgets the essential general textual knowledge having a strong generalization
ability. To tackle this issue, we introduce a novel Knowledge-guided Context
Optimization (KgCoOp) to enhance the generalization ability of the learnable
prompt for unseen classes. The key insight of KgCoOp is that forgetting about
essential knowledge can be alleviated by reducing the discrepancy between the
learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the
discrepancy between the textual embeddings generated by learned prompts and the
hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can
make a discriminative prompt for both seen and unseen tasks. Extensive
evaluation of several benchmarks demonstrates that the proposed
Knowledge-guided Context Optimization is an efficient method for prompt tuning,
\emph{i.e.,} achieves better performance with less training time.",None,-1
25a0923d-1084-4aa8-b8cf-ddae26b1e2ae,SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback,0.191448,"Commonsense reasoning is a critical aspect of human communication. Despite
recent advances in conversational AI driven by large language models,
commonsense reasoning remains a challenging task. In this work, we introduce
SYNDICOM - a method for improving commonsense in dialogue response generation.
SYNDICOM consists of two components. The first component is a dataset composed
of commonsense dialogues created from a knowledge graph and synthesized into
natural language. This dataset includes both valid and invalid responses to
dialogue contexts, along with natural language feedback (NLF) for the invalid
responses. The second contribution is a two-step procedure: training a model to
predict natural language feedback (NLF) for invalid responses, and then
training a response generation model conditioned on the predicted NLF, the
invalid response, and the dialogue. SYNDICOM is scalable and does not require
reinforcement learning. Empirical results on three tasks are evaluated using a
broad range of metrics. SYNDICOM achieves a relative improvement of 53% over
ChatGPT on ROUGE1, and human evaluators prefer SYNDICOM over ChatGPT 57% of the
time. We will publicly release the code and the full dataset.",None,-1
796edb1f-4f16-4145-8b5f-0a8589f7d1fd,BIM-GPT: a Prompt-Based Virtual Assistant Framework for BIM Information Retrieval,0.612375,"Efficient information retrieval (IR) from building information models (BIMs)
poses significant challenges due to the necessity for deep BIM knowledge or
extensive engineering efforts for automation. We introduce BIM-GPT, a
prompt-based virtual assistant (VA) framework integrating BIM and generative
pre-trained transformer (GPT) technologies to support NL-based IR. A prompt
manager and dynamic template generate prompts for GPT models, enabling
interpretation of NL queries, summarization of retrieved information, and
answering BIM-related questions. In tests on a BIM IR dataset, our approach
achieved 83.5% and 99.5% accuracy rates for classifying NL queries with no data
and 2% data incorporated in prompts, respectively. Additionally, we validated
the functionality of BIM-GPT through a VA prototype for a hospital building.
This research contributes to the development of effective and versatile VAs for
BIM IR in the construction industry, significantly enhancing BIM accessibility
and reducing engineering efforts and training data requirements for processing
NL queries.",None,-1
83bf66ea-90da-40fd-b8a7-d3afbafd8f6e,Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities,0.0549248,"This study critically evaluates the efficacy of prompting methods in
enhancing the mathematical reasoning capability of large language models
(LLMs). The investigation uses three prescriptive prompting methods - simple,
persona, and conversational prompting - known for their effectiveness in
enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's
LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and
MMLU datasets, encompassing a broad spectrum of mathematical challenges. A
grading script adapted to each dataset is used to determine the effectiveness
of these prompting interventions in enhancing the model's mathematical analysis
power. Contrary to expectations, our empirical analysis reveals that none of
the investigated methods consistently improves over ChatGPT-3.5's baseline
performance, with some causing significant degradation. Our findings suggest
that prompting strategies do not necessarily generalize to new domains, in this
study failing to enhance mathematical performance.",None,-1
06fe3575-630b-4572-993d-22a6d39622a9,Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills,0.361165,"While large language models have made strides in natural language processing,
their proficiency in complex reasoning tasks requiring formal language
comprehension, such as chess, remains less investigated. This paper probes the
performance of ChatGPT, a sophisticated language model by OpenAI in tackling
such complex reasoning tasks, using chess as a case study. Through robust
metrics examining both the legality and quality of moves, we assess ChatGPT's
understanding of the chessboard, adherence to chess rules, and strategic
decision-making abilities. Our evaluation identifies limitations within
ChatGPT's attention mechanism that affect its formal language comprehension and
uncovers the model's underdeveloped self-regulation abilities. Our study also
reveals ChatGPT's propensity for a coherent strategy in its gameplay and a
noticeable uptick in decision-making assertiveness when the model is presented
with a greater volume of natural language or possesses a more lucid
understanding of the state of the chessboard. These findings contribute to the
growing exploration of language models' abilities beyond natural language
processing, providing valuable information for future research towards models
demonstrating human-like cognitive abilities.",None,-1
3c8e227c-8a13-4c87-a9c9-efc0535626f7,Latent Feature Relation Consistency for Adversarial Robustness,0.181171,"Deep neural networks have been applied in many computer vision tasks and
achieved state-of-the-art performance. However, misclassification will occur
when DNN predicts adversarial examples which add human-imperceptible
adversarial noise to natural examples. This limits the application of DNN in
security-critical fields. To alleviate this problem, we first conducted an
empirical analysis of the latent features of both adversarial and natural
examples and found the similarity matrix of natural examples is more compact
than those of adversarial examples. Motivated by this observation, we propose
\textbf{L}atent \textbf{F}eature \textbf{R}elation \textbf{C}onsistency
(\textbf{LFRC}), which constrains the relation of adversarial examples in
latent space to be consistent with the natural examples. Importantly, our LFRC
is orthogonal to the previous method and can be easily combined with them to
achieve further improvement. To demonstrate the effectiveness of LFRC, we
conduct extensive experiments using different neural networks on benchmark
datasets. For instance, LFRC can bring 0.78\% further improvement compared to
AT, and 1.09\% improvement compared to TRADES, against AutoAttack on CIFAR10.
Code is available at https://github.com/liuxingbin/LFRC.",None,-1
b5c8be6d-f95e-4f41-96cb-401fa5175db7,RaSP: Relation-aware Semantic Prior for Weakly Supervised Incremental Segmentation,0.251974,"Class-incremental semantic image segmentation assumes multiple model updates,
each enriching the model to segment new categories. This is typically carried
out by providing expensive pixel-level annotations to the training algorithm
for all new objects, limiting the adoption of such methods in practical
applications. Approaches that solely require image-level labels offer an
attractive alternative, yet, such coarse annotations lack precise information
about the location and boundary of the new objects. In this paper we argue
that, since classes represent not just indices but semantic entities, the
conceptual relationships between them can provide valuable information that
should be leveraged. We propose a weakly supervised approach that exploits such
semantic relations to transfer objectness prior from the previously learned
classes into the new ones, complementing the supervisory signal from
image-level labels. We validate our approach on a number of continual learning
tasks, and show how even a simple pairwise interaction between classes can
significantly improve the segmentation mask quality of both old and new
classes. We show these conclusions still hold for longer and, hence, more
realistic sequences of tasks and for a challenging few-shot scenario.",None,-1
295b70aa-c4b0-46e8-a9f4-ef1d055ea80d,State of the Art on Diffusion Models for Visual Computing,0.830563,"The field of visual computing is rapidly advancing due to the emergence of
generative artificial intelligence (AI), which unlocks unprecedented
capabilities for the generation, editing, and reconstruction of images, videos,
and 3D scenes. In these domains, diffusion models are the generative AI
architecture of choice. Within the last year alone, the literature on
diffusion-based tools and applications has seen exponential growth and relevant
papers are published across the computer graphics, computer vision, and AI
communities with new works appearing daily on arXiv. This rapid growth of the
field makes it difficult to keep up with all recent developments. The goal of
this state-of-the-art report (STAR) is to introduce the basic mathematical
concepts of diffusion models, implementation details and design choices of the
popular Stable Diffusion model, as well as overview important aspects of these
generative AI tools, including personalization, conditioning, inversion, among
others. Moreover, we give a comprehensive overview of the rapidly growing
literature on diffusion-based generation and editing, categorized by the type
of generated medium, including 2D images, videos, 3D objects, locomotion, and
4D scenes. Finally, we discuss available datasets, metrics, open challenges,
and social implications. This STAR provides an intuitive starting point to
explore this exciting topic for researchers, artists, and practitioners alike.",None,-1
1d19662d-3923-4fcc-9510-1052d3411abd,Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios,0.346223,"The abilities to understand the social interaction behaviors between a
vehicle and its surroundings while predicting its trajectory in an urban
environment are critical for road safety in autonomous driving. Social
interactions are hard to explain because of their uncertainty. In recent years,
neural network-based methods have been widely used for trajectory prediction
and have been shown to outperform hand-crafted methods. However, these methods
suffer from their lack of interpretability. In order to overcome this
limitation, we combine the interpretability of a discrete choice model with the
high accuracy of a neural network-based model for the task of vehicle
trajectory prediction in an interactive environment. We implement and evaluate
our model using the INTERACTION dataset and demonstrate the effectiveness of
our proposed architecture to explain its predictions without compromising the
accuracy.",None,-1
1f1eda47-5069-45ce-853a-7e65c2c78bf9,Deficiency-Aware Masked Transformer for Video Inpainting,0.241008,"Recent video inpainting methods have made remarkable progress by utilizing
explicit guidance, such as optical flow, to propagate cross-frame pixels.
However, there are cases where cross-frame recurrence of the masked video is
not available, resulting in a deficiency. In such situation, instead of
borrowing pixels from other frames, the focus of the model shifts towards
addressing the inverse problem. In this paper, we introduce a
dual-modality-compatible inpainting framework called Deficiency-aware Masked
Transformer (DMT), which offers three key advantages. Firstly, we pretrain a
image inpainting model DMT_img serve as a prior for distilling the video model
DMT_vid, thereby benefiting the hallucination of deficiency cases. Secondly,
the self-attention module selectively incorporates spatiotemporal tokens to
accelerate inference and remove noise signals. Thirdly, a simple yet effective
Receptive Field Contextualizer is integrated into DMT, further improving
performance. Extensive experiments conducted on YouTube-VOS and DAVIS datasets
demonstrate that DMT_vid significantly outperforms previous solutions. The code
and video demonstrations can be found at github.com/yeates/DMT.",None,-1
f8c96fe4-615b-47ad-895b-c3bbdab06974,Video-Mined Task Graphs for Keystep Recognition in Instructional Videos,0.390777,"Procedural activity understanding requires perceiving human actions in terms
of a broader task, where multiple keysteps are performed in sequence across a
long video to reach a final goal state -- such as the steps of a recipe or a
DIY fix-it task. Prior work largely treats keystep recognition in isolation of
this broader structure, or else rigidly confines keysteps to align with a
predefined sequential script. We propose discovering a task graph automatically
from how-to videos to represent probabilistically how people tend to execute
keysteps, and then leverage this graph to regularize keystep recognition in
novel videos. On multiple datasets of real-world instructional videos, we show
the impact: more reliable zero-shot keystep localization and improved video
representation learning, exceeding the state of the art.",None,-1
14324299-8c8c-4dd0-b20c-4ed11ce5db08,Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling,0.535989,"Recently, ChatGPT, a representative large language model (LLM), has gained
considerable attention due to its powerful emergent abilities. Some researchers
suggest that LLMs could potentially replace structured knowledge bases like
knowledge graphs (KGs) and function as parameterized knowledge bases. However,
while LLMs are proficient at learning probabilistic language patterns based on
large corpus and engaging in conversations with humans, they, like previous
smaller pre-trained language models (PLMs), still have difficulty in recalling
facts while generating knowledge-grounded contents. To overcome these
limitations, researchers have proposed enhancing data-driven PLMs with
knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus
improving their performance to generate texts requiring factual knowledge and
providing more informed responses to user queries. This paper reviews the
studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced
pre-trained language models (KGPLMs) as well as their applications. Inspired by
existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by
developing knowledge graph-enhanced large language models (KGLLMs). KGLLM
provides a solution to enhance LLMs' factual reasoning ability, opening up new
avenues for LLM research.",None,-1
c0ac21f7-d106-4f84-beca-672364ac5a4c,On the Challenges of Fully Incremental Neural Dependency Parsing,0.238285,"Since the popularization of BiLSTMs and Transformer-based bidirectional
encoders, state-of-the-art syntactic parsers have lacked incrementality,
requiring access to the whole sentence and deviating from human language
processing. This paper explores whether fully incremental dependency parsing
with modern architectures can be competitive. We build parsers combining
strictly left-to-right neural encoders with fully incremental sequence-labeling
and transition-based decoders. The results show that fully incremental parsing
with modern architectures considerably lags behind bidirectional parsing,
noting the challenges of psycholinguistically plausible parsing.",None,-1
22032e28-8d29-4eeb-93cc-409716c439bd,BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation,0.591861,"A novel 4K video frame interpolator based on bilateral transformer (BiFormer)
is proposed in this paper, which performs three steps: global motion
estimation, local motion refinement, and frame synthesis. First, in global
motion estimation, we predict symmetric bilateral motion fields at a coarse
scale. To this end, we propose BiFormer, the first transformer-based bilateral
motion estimator. Second, we refine the global motion fields efficiently using
blockwise bilateral cost volumes (BBCVs). Third, we warp the input frames using
the refined motion fields and blend them to synthesize an intermediate frame.
Extensive experiments demonstrate that the proposed BiFormer algorithm achieves
excellent interpolation performance on 4K datasets. The source codes are
available at https://github.com/JunHeum/BiFormer.",None,-1
082241f9-4b40-410b-88bf-1aeb936f2b57,Self-supervised dense representation learning for live-cell microscopy with time arrow prediction,0.794942,"State-of-the-art object detection and segmentation methods for microscopy
images rely on supervised machine learning, which requires laborious manual
annotation of training data. Here we present a self-supervised method based on
time arrow prediction pre-training that learns dense image representations from
raw, unlabeled live-cell microscopy videos. Our method builds upon the task of
predicting the correct order of time-flipped image regions via a single-image
feature extractor followed by a time arrow prediction head that operates on the
fused features. We show that the resulting dense representations capture
inherently time-asymmetric biological processes such as cell divisions on a
pixel-level. We furthermore demonstrate the utility of these representations on
several live-cell microscopy datasets for detection and segmentation of
dividing cells, as well as for cell state classification. Our method
outperforms supervised methods, particularly when only limited ground truth
annotations are available as is commonly the case in practice. We provide code
at https://github.com/weigertlab/tarrow.",None,-1
32226d27-b493-441b-8857-7ba953f55477,Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue,0.152793,"Accurate knowledge selection is critical in knowledge-grounded dialogue
systems. Towards a closer look at it, we offer a novel perspective to organize
existing literature, i.e., knowledge selection coupled with, after, and before
generation. We focus on the third under-explored category of study, which can
not only select knowledge accurately in advance, but has the advantage to
reduce the learning, adjustment, and interpretation burden of subsequent
response generation models, especially LLMs. We propose GATE, a
generator-agnostic knowledge selection method, to prepare knowledge for
subsequent response generation models by selecting context-related knowledge
among different knowledge structures and variable knowledge requirements.
Experimental results demonstrate the superiority of GATE, and indicate that
knowledge selection before generation is a lightweight yet effective way to
facilitate LLMs (e.g., ChatGPT) to generate more informative responses.",None,-1
8c9b63dc-760c-4fdb-aa6e-4afcba625f4c,Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games,0.745668,"Self-play (SP) is a popular multi-agent reinforcement learning (MARL)
framework for solving competitive games, where each agent optimizes policy by
treating others as part of the environment. Despite the empirical successes,
the theoretical properties of SP-based methods are limited to two-player
zero-sum games. However, for mixed cooperative-competitive games where agents
on the same team need to cooperate with each other, we can show a simple
counter-example where SP-based methods cannot converge to a global Nash
equilibrium (NE) with high probability. Alternatively, Policy-Space Response
Oracles (PSRO) is an iterative framework for learning NE, where the best
responses w.r.t. previous policies are learned in each iteration. PSRO can be
directly extended to mixed cooperative-competitive settings by jointly learning
team best responses with all convergence properties unchanged. However, PSRO
requires repeatedly training joint policies from scratch till convergence,
which makes it hard to scale to complex games. In this work, we develop a novel
algorithm, Fictitious Cross-Play (FXP), which inherits the benefits from both
frameworks. FXP simultaneously trains an SP-based main policy and a counter
population of best response policies. The main policy is trained by fictitious
self-play and cross-play against the counter population, while the counter
policies are trained as the best responses to the main policy's past versions.
We validate our method in matrix games and show that FXP converges to global
NEs while SP methods fail. We also conduct experiments in a gridworld domain,
where FXP achieves higher Elo ratings and lower exploitabilities than
baselines, and a more challenging football game, where FXP defeats SOTA models
with over 94% win rate.",None,-1
029d38f6-f2b7-4cdc-a52e-fc04bdf08e83,Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation,0.804045,"Target-oriented dialogue systems, designed to proactively steer conversations
toward predefined targets or accomplish specific system-side goals, are an
exciting area in conversational AI. In this work, by formulating a <dialogue
act, topic> pair as the conversation target, we explore a novel problem of
personalized target-oriented dialogue by considering personalization during the
target accomplishment process. However, there remains an emergent need for
high-quality datasets, and building one from scratch requires tremendous human
effort. To address this, we propose an automatic dataset curation framework
using a role-playing approach. Based on this framework, we construct a
large-scale personalized target-oriented dialogue dataset, TopDial, which
comprises about 18K multi-turn dialogues. The experimental results show that
this dataset is of high quality and could contribute to exploring personalized
target-oriented dialogue.",None,-1
00b3f47e-e64e-41a0-9022-3ba6b9d788e4,Policy Reuse for Communication Load Balancing in Unseen Traffic Scenarios,0.328185,"With the continuous growth in communication network complexity and traffic
volume, communication load balancing solutions are receiving increasing
attention. Specifically, reinforcement learning (RL)-based methods have shown
impressive performance compared with traditional rule-based methods. However,
standard RL methods generally require an enormous amount of data to train, and
generalize poorly to scenarios that are not encountered during training. We
propose a policy reuse framework in which a policy selector chooses the most
suitable pre-trained RL policy to execute based on the current traffic
condition. Our method hinges on a policy bank composed of policies trained on a
diverse set of traffic scenarios. When deploying to an unknown traffic
scenario, we select a policy from the policy bank based on the similarity
between the previous-day traffic of the current scenario and the traffic
observed during training. Experiments demonstrate that this framework can
outperform classical and adaptive rule-based methods by a large margin.",None,-1
a2f65590-2a80-431b-b1ad-5ea9afaade90,A Unified Framework of Policy Learning for Contextual Bandit with Confounding Bias and Missing Observations,0.22748,"We study the offline contextual bandit problem, where we aim to acquire an
optimal policy using observational data. However, this data usually contains
two deficiencies: (i) some variables that confound actions are not observed,
and (ii) missing observations exist in the collected data. Unobserved
confounders lead to a confounding bias and missing observations cause bias and
inefficiency problems. To overcome these challenges and learn the optimal
policy from the observed dataset, we present a new algorithm called
Causal-Adjusted Pessimistic (CAP) policy learning, which forms the reward
function as the solution of an integral equation system, builds a confidence
set, and greedily takes action with pessimism. With mild assumptions on the
data, we develop an upper bound to the suboptimality of CAP for the offline
contextual bandit problem.",None,-1
4b298157-9b0b-4dfa-840d-e2956fac8fb4,Algebra Error Classification with Large Language Models,0.599918,"Automated feedback as students answer open-ended math questions has
significant potential in improving learning outcomes at large scale. A key part
of automated feedback systems is an error classification component, which
identifies student errors and enables appropriate, predefined feedback to be
deployed. Most existing approaches to error classification use a rule-based
method, which has limited capacity to generalize. Existing data-driven methods
avoid these limitations but specifically require mathematical expressions in
student responses to be parsed into syntax trees. This requirement is itself a
limitation, since student responses are not always syntactically valid and
cannot be converted into trees. In this work, we introduce a flexible method
for error classification using pre-trained large language models. We
demonstrate that our method can outperform existing methods in algebra error
classification, and is able to classify a larger set of student responses.
Additionally, we analyze common classification errors made by our method and
discuss limitations of automated error classification.",None,-1
01625b44-8e78-4a2d-8dbc-7c89a793b521,Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues,0.435928,"Discourse processing suffers from data sparsity, especially for dialogues. As
a result, we explore approaches to build discourse structures for dialogues,
based on attention matrices from Pre-trained Language Models (PLMs). We
investigate multiple tasks for fine-tuning and show that the dialogue-tailored
Sentence Ordering task performs best. To locate and exploit discourse
information in PLMs, we propose an unsupervised and a semi-supervised method.
Our proposals achieve encouraging results on the STAC corpus, with F1 scores of
57.2 and 59.3 for unsupervised and semi-supervised methods, respectively. When
restricted to projective trees, our scores improved to 63.3 and 68.1.",None,-1
0bcb9549-84fa-4e16-aefa-12ead2756164,"Reduce, Reuse, Recycle: Selective Reincarnation in Multi-Agent Reinforcement Learning",0.255328,"'Reincarnation' in reinforcement learning has been proposed as a
formalisation of reusing prior computation from past experiments when training
an agent in an environment. In this paper, we present a brief foray into the
paradigm of reincarnation in the multi-agent (MA) context. We consider the case
where only some agents are reincarnated, whereas the others are trained from
scratch -- selective reincarnation. In the fully-cooperative MA setting with
heterogeneous agents, we demonstrate that selective reincarnation can lead to
higher returns than training fully from scratch, and faster convergence than
training with full reincarnation. However, the choice of which agents to
reincarnate in a heterogeneous system is vitally important to the outcome of
the training -- in fact, a poor choice can lead to considerably worse results
than the alternatives. We argue that a rich field of work exists here, and we
hope that our effort catalyses further energy in bringing the topic of
reincarnation to the multi-agent realm.",None,-1
a8f46f9e-4b61-45c3-9849-b3da47165596,Arithmetic with Language Models: from Memorization to Computation,0.00713783,"A better understanding of the emergent computation and problem-solving
capabilities of recent large language models is of paramount importance to
further improve them and broaden their applicability. This work investigates
how a language model, trained to predict the next token, can perform arithmetic
computations generalizing beyond training data. Binary addition and
multiplication constitute a good testbed for this purpose, since they require a
very small vocabulary and exhibit relevant input/output discontinuities making
smooth input interpolation ineffective for novel data. We successfully trained
a light language model to learn these tasks and ran a number of experiments to
investigate the extrapolation capabilities and internal information processing.
Our findings support the hypothesis that the language model works as an
Encoding-Regression-Decoding machine where the computation takes place in the
value space once the input token representation is mapped to an appropriate
internal representation.",None,-1
ad67f8d3-ca42-4eb1-b25e-636549f0e4ec,Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft,0.33202,"Many reinforcement learning environments (e.g., Minecraft) provide only
sparse rewards that indicate task completion or failure with binary values. The
challenge in exploration efficiency in such environments makes it difficult for
reinforcement-learning-based agents to learn complex tasks. To address this,
this paper introduces an advanced learning system, named Auto MC-Reward, that
leverages Large Language Models (LLMs) to automatically design dense reward
functions, thereby enhancing the learning efficiency. Auto MC-Reward consists
of three important components: Reward Designer, Reward Critic, and Trajectory
Analyzer. Given the environment information and task descriptions, the Reward
Designer first design the reward function by coding an executable Python
function with predefined observation inputs. Then, our Reward Critic will be
responsible for verifying the code, checking whether the code is
self-consistent and free of syntax and semantic errors. Further, the Trajectory
Analyzer summarizes possible failure causes and provides refinement suggestions
according to collected trajectories. In the next round, Reward Designer will
further refine and iterate the dense reward function based on feedback.
Experiments demonstrate a significant improvement in the success rate and
learning efficiency of our agents in complex tasks in Minecraft, such as
obtaining diamond with the efficient ability to avoid lava, and efficiently
explore trees and animals that are sparse in the plains biome.",None,-1
423166f3-df15-412e-8b9f-06edb98f2cb8,MonoEdge: Monocular 3D Object Detection Using Local Perspectives,0.390041,"We propose a novel approach for monocular 3D object detection by leveraging
local perspective effects of each object. While the global perspective effect
shown as size and position variations has been exploited for monocular 3D
detection extensively, the local perspectives has long been overlooked. We
design a local perspective module to regress a newly defined variable named
keyedge-ratios as the parameterization of the local shape distortion to account
for the local perspective, and derive the object depth and yaw angle from it.
Theoretically, this module does not rely on the pixel-wise size or position in
the image of the objects, therefore independent of the camera intrinsic
parameters. By plugging this module in existing monocular 3D object detection
frameworks, we incorporate the local perspective distortion with global
perspective effect for monocular 3D reasoning, and we demonstrate the
effectiveness and superior performance over strong baseline methods in multiple
datasets.",None,-1
257796c3-abbe-4490-ad5c-7f5e64e529c2,Object pop-up: Can we infer 3D objects and their poses from human interactions alone?,0.61709,"The intimate entanglement between objects affordances and human poses is of
large interest, among others, for behavioural sciences, cognitive psychology,
and Computer Vision communities. In recent years, the latter has developed
several object-centric approaches: starting from items, learning pipelines
synthesizing human poses and dynamics in a realistic way, satisfying both
geometrical and functional expectations. However, the inverse perspective is
significantly less explored: Can we infer 3D objects and their poses from human
interactions alone? Our investigation follows this direction, showing that a
generic 3D human point cloud is enough to pop up an unobserved object, even
when the user is just imitating a functionality (e.g., looking through a
binocular) without involving a tangible counterpart. We validate our method
qualitatively and quantitatively, with synthetic data and sequences acquired
for the task, showing applicability for XR/VR. The code is available at
https://github.com/ptrvilya/object-popup.",None,-1
363e10e0-d71f-4bc4-a504-49fa42a354bf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,0.749905,"Monocular Depth Estimation (MDE) is a critical component in applications such
as autonomous driving. There are various attacks against MDE networks. These
attacks, especially the physical ones, pose a great threat to the security of
such systems. Traditional adversarial training method requires ground-truth
labels hence cannot be directly applied to self-supervised MDE that does not
have ground-truth depth. Some self-supervised model hardening techniques (e.g.,
contrastive learning) ignore the domain knowledge of MDE and can hardly achieve
optimal performance. In this work, we propose a novel adversarial training
method for self-supervised MDE models based on view synthesis without using
ground-truth depth. We improve adversarial robustness against physical-world
attacks using L0-norm-bounded perturbation in training. We compare our method
with supervised learning based and contrastive learning based methods that are
tailored for MDE. Results on two representative MDE networks show that we
achieve better robustness against various adversarial attacks with nearly no
benign performance degradation.",None,-1
d44a1ca6-8922-4e09-afa6-1e58cfc27b25,Logic programming for deliberative robotic task planning,0.367839,"Over the last decade, the use of robots in production and daily life has
increased. With increasingly complex tasks and interaction in different
environments including humans, robots are required a higher level of autonomy
for efficient deliberation. Task planning is a key element of deliberation. It
combines elementary operations into a structured plan to satisfy a prescribed
goal, given specifications on the robot and the environment. In this
manuscript, we present a survey on recent advances in the application of logic
programming to the problem of task planning. Logic programming offers several
advantages compared to other approaches, including greater expressivity and
interpretability which may aid in the development of safe and reliable robots.
We analyze different planners and their suitability for specific robotic
applications, based on expressivity in domain representation, computational
efficiency and software implementation. In this way, we support the robotic
designer in choosing the best tool for his application.",None,-1
86a8794b-7066-43e4-a95e-e7dbfa476ba4,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,0.188807,"Transfer learning is a critical technique in training deep neural networks
for the challenging medical image segmentation task that requires enormous
resources. With the abundance of medical image data, many research institutions
release models trained on various datasets that can form a huge pool of
candidate source models to choose from. Hence, it's vital to estimate the
source models' transferability (i.e., the ability to generalize across
different downstream tasks) for proper and efficient model reuse. To make up
for its deficiency when applying transfer learning to medical image
segmentation, in this paper, we therefore propose a new Transferability
Estimation (TE) method. We first analyze the drawbacks of using the existing TE
algorithms for medical image segmentation and then design a source-free TE
framework that considers both class consistency and feature variety for better
estimation. Extensive experiments show that our method surpasses all current
algorithms for transferability estimation in medical image segmentation. Code
is available at https://github.com/EndoluminalSurgicalVision-IMR/CCFV",None,-1
bb7eb96e-0d40-4bd5-b391-eecee81c4d79,The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and POS,1.0,"Automatic analysis for modern Chinese has greatly improved the accuracy of
text mining in related fields, but the study of ancient Chinese is still
relatively rare. Ancient text division and lexical annotation are important
parts of classical literature comprehension, and previous studies have tried to
construct auxiliary dictionary and other fused knowledge to improve the
performance. In this paper, we propose a framework for ancient Chinese Word
Segmentation and Part-of-Speech Tagging that makes a twofold effort: on the one
hand, we try to capture the wordhood semantics; on the other hand, we
re-predict the uncertain samples of baseline model by introducing external
knowledge. The performance of our architecture outperforms pre-trained BERT
with CRF and existing tools such as Jiayan.",None,-1
ed772333-48ee-4e9f-ac82-0159f186a499,MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document,0.0685569,"The facts and time in the document are intricately intertwined, making
temporal reasoning over documents challenging. Previous work models time
implicitly, making it difficult to handle such complex relationships. To
address this issue, we propose MTGER, a novel Multi-view Temporal Graph
Enhanced Temporal Reasoning framework for temporal reasoning over time-involved
documents. Concretely, MTGER explicitly models the temporal relationships among
facts by multi-view temporal graphs. On the one hand, the heterogeneous
temporal graphs explicitly model the temporal and discourse relationships among
facts; on the other hand, the multi-view mechanism captures both time-focused
and fact-focused information, allowing the two views to complement each other
through adaptive fusion. To further improve the implicit reasoning capability
of the model, we design a self-supervised time-comparing objective. Extensive
experimental results demonstrate the effectiveness of our method on the TimeQA
and SituatedQA datasets. Furthermore, MTGER gives more consistent answers under
question perturbations.",None,-1
245c269b-998a-48b1-b2b9-39aa2a682543,GeoNet: Benchmarking Unsupervised Adaptation across Geographies,0.492142,"In recent years, several efforts have been aimed at improving the robustness
of vision models to domains and environments unseen during training. An
important practical problem pertains to models deployed in a new geography that
is under-represented in the training dataset, posing a direct challenge to fair
and inclusive computer vision. In this paper, we study the problem of
geographic robustness and make three main contributions. First, we introduce a
large-scale dataset GeoNet for geographic adaptation containing benchmarks
across diverse tasks like scene recognition (GeoPlaces), image classification
(GeoImNet) and universal adaptation (GeoUniDA). Second, we investigate the
nature of distribution shifts typical to the problem of geographic adaptation
and hypothesize that the major source of domain shifts arise from significant
variations in scene context (context shift), object design (design shift) and
label distribution (prior shift) across geographies. Third, we conduct an
extensive evaluation of several state-of-the-art unsupervised domain adaptation
algorithms and architectures on GeoNet, showing that they do not suffice for
geographical adaptation, and that large-scale pre-training using large vision
models also does not lead to geographic robustness. Our dataset is publicly
available at https://tarun005.github.io/GeoNet.",None,-1
1357c45d-056e-4c1f-91e4-7a4cc30916d2,Classification of Cross-cultural News Events,0.819908,"We present a methodology to support the analysis of culture from text such as
news events and demonstrate its usefulness on categorizing news events from
different categories (society, business, health, recreation, science, shopping,
sports, arts, computers, games and home) across different geographical
locations (different places in 117 countries). We group countries based on the
culture that they follow and then filter the news events based on their content
category. The news events are automatically labelled with the help of Hofstedes
cultural dimensions. We present combinations of events across different
categories and check the performances of different classification methods. We
also presents experimental comparison of different number of features in order
to find a suitable set to represent the culture.",None,-1
59b1deae-f159-4c70-b52a-40e73fccf621,Systematic Investigation of Sparse Perturbed Sharpness-Aware Minimization Optimizer,0.240334,"Deep neural networks often suffer from poor generalization due to complex and
non-convex loss landscapes. Sharpness-Aware Minimization (SAM) is a popular
solution that smooths the loss landscape by minimizing the maximized change of
training loss when adding a perturbation to the weight. However, indiscriminate
perturbation of SAM on all parameters is suboptimal and results in excessive
computation, double the overhead of common optimizers like Stochastic Gradient
Descent (SGD). In this paper, we propose Sparse SAM (SSAM), an efficient and
effective training scheme that achieves sparse perturbation by a binary mask.
To obtain the sparse mask, we provide two solutions based on Fisher information
and dynamic sparse training, respectively. We investigate the impact of
different masks, including unstructured, structured, and $N$:$M$ structured
patterns, as well as explicit and implicit forms of implementing sparse
perturbation. We theoretically prove that SSAM can converge at the same rate as
SAM, i.e., $O(\log T/\sqrt{T})$. Sparse SAM has the potential to accelerate
training and smooth the loss landscape effectively. Extensive experimental
results on CIFAR and ImageNet-1K confirm that our method is superior to SAM in
terms of efficiency, and the performance is preserved or even improved with a
perturbation of merely 50\% sparsity. Code is available at
https://github.com/Mi-Peng/Systematic-Investigation-of-Sparse-Perturbed-Sharpness-Aware-Minimization-Optimizer.",None,-1
22e7e1c6-9804-454a-9a8b-eb3ae6b7236d,HeRo: RoBERTa and Longformer Hebrew Language Models,0.154455,"In this paper, we fill in an existing gap in resources available to the
Hebrew NLP community by providing it with the largest so far pre-train dataset
HeDC4, a state-of-the-art pre-trained language model HeRo for standard length
inputs and an efficient transformer LongHeRo for long input sequences. The HeRo
model was evaluated on the sentiment analysis, the named entity recognition,
and the question answering tasks while the LongHeRo model was evaluated on the
document classification task with a dataset composed of long documents. Both
HeRo and LongHeRo presented state-of-the-art performance. The dataset and model
checkpoints used in this work are publicly available.",None,-1
3ff78262-fb15-4dad-92f9-d1a39642cea2,Is forgetting less a good inductive bias for forward transfer?,0.491911,"One of the main motivations of studying continual learning is that the
problem setting allows a model to accrue knowledge from past tasks to learn new
tasks more efficiently. However, recent studies suggest that the key metric
that continual learning algorithms optimize, reduction in catastrophic
forgetting, does not correlate well with the forward transfer of knowledge. We
believe that the conclusion previous works reached is due to the way they
measure forward transfer. We argue that the measure of forward transfer to a
task should not be affected by the restrictions placed on the continual learner
in order to preserve knowledge of previous tasks. Instead, forward transfer
should be measured by how easy it is to learn a new task given a set of
representations produced by continual learning on previous tasks. Under this
notion of forward transfer, we evaluate different continual learning algorithms
on a variety of image classification benchmarks. Our results indicate that less
forgetful representations lead to a better forward transfer suggesting a strong
correlation between retaining past information and learning efficiency on new
tasks. Further, we found less forgetful representations to be more diverse and
discriminative compared to their forgetful counterparts.",None,-1
7b49b976-f34d-4861-8e50-b78a34f90e8d,Principles from Clinical Research for NLP Model Generalization,0.133673,"The NLP community typically relies on performance of a model on a held-out
test set to assess generalization. Performance drops observed in datasets
outside of official test sets are generally attributed to ""out-of-distribution""
effects. Here, we explore the foundations of generalizability and study the
factors that affect it, articulating lessons from clinical studies. In clinical
research, generalizability is an act of reasoning that depends on (a) internal
validity of experiments to ensure controlled measurement of cause and effect,
and (b) external validity or transportability of the results to the wider
population. We demonstrate how learning spurious correlations, such as the
distance between entities in relation extraction tasks, can affect a model's
internal validity and in turn adversely impact generalization. We, therefore,
present the need to ensure internal validity when building machine learning
models in NLP. Our recommendations also apply to generative large language
models, as they are known to be sensitive to even minor semantic preserving
alterations. We also propose adapting the idea of matching in randomized
controlled trials and observational studies to NLP evaluation to measure
causation.",None,-1
cfff49ae-59ab-4d9c-a02d-385c67c7daa2,Incorporating Transformer Designs into Convolutions for Lightweight Image Super-Resolution,0.32162,"In recent years, the use of large convolutional kernels has become popular in
designing convolutional neural networks due to their ability to capture
long-range dependencies and provide large receptive fields. However, the
increase in kernel size also leads to a quadratic growth in the number of
parameters, resulting in heavy computation and memory requirements. To address
this challenge, we propose a neighborhood attention (NA) module that upgrades
the standard convolution with a self-attention mechanism. The NA module
efficiently extracts long-range dependencies in a sliding window pattern,
thereby achieving similar performance to large convolutional kernels but with
fewer parameters.
  Building upon the NA module, we propose a lightweight single image
super-resolution (SISR) network named TCSR. Additionally, we introduce an
enhanced feed-forward network (EFFN) in TCSR to improve the SISR performance.
EFFN employs a parameter-free spatial-shift operation for efficient feature
aggregation. Our extensive experiments and ablation studies demonstrate that
TCSR outperforms existing lightweight SISR methods and achieves
state-of-the-art performance. Our codes are available at
\url{https://github.com/Aitical/TCSR}.",None,-1
27456231-ca8d-4b85-8331-4915faa2fb1a,"A data science axiology: the nature, value, and risks of data science",0.148591,"Data science is not a science. It is a research paradigm with an unfathomed
scope, scale, complexity, and power for knowledge discovery that is not
otherwise possible and can be beyond human reasoning. It is changing our world
practically and profoundly already widely deployed in tens of thousands of
applications in every discipline in an AI Arms Race that, due to its
inscrutability, can lead to unfathomed risks. This paper presents an axiology
of data science, its purpose, nature, importance, risks, and value for problem
solving, by exploring and evaluating its remarkable, definitive features. As
data science is in its infancy, this initial, speculative axiology is intended
to aid in understanding and defining data science to recognize its potential
benefits, risks, and open research challenges. AI based data science is
inherently about uncertainty that may be more realistic than our preference for
the certainty of science. Data science will have impacts far beyond knowledge
discovery and will take us into new ways of understanding the world.",None,-1
1dbfd773-e470-4314-a184-b739e8423035,The Gradient of Generative AI Release: Methods and Considerations,0.871335,"As increasingly powerful generative AI systems are developed, the release
method greatly varies. We propose a framework to assess six levels of access to
generative AI systems: fully closed; gradual or staged access; hosted access;
cloud-based or API access; downloadable access; and fully open. Each level,
from fully closed to fully open, can be viewed as an option along a gradient.
We outline key considerations across this gradient: release methods come with
tradeoffs, especially around the tension between concentrating power and
mitigating risks. Diverse and multidisciplinary perspectives are needed to
examine and mitigate risk in generative AI systems from conception to
deployment. We show trends in generative system release over time, noting
closedness among large companies for powerful systems and openness among
organizations founded on principles of openness. We also enumerate safety
controls and guardrails for generative systems and necessary investments to
improve future releases.",None,-1
8565fd9b-3e96-420e-9a62-57b9d5fbb433,Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models,0.388475,"The escalating debate on AI's capabilities warrants developing reliable
metrics to assess machine ""intelligence"". Recently, many anecdotal examples
were used to suggest that newer large language models (LLMs) like ChatGPT and
GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached
conflicting conclusions regarding those abilities. We investigate the extent of
LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs
exhibit certain N-ToM abilities, this behavior is far from being robust. We
further examine the factors impacting performance on N-ToM tasks and discover
that LLMs struggle with adversarial examples, indicating reliance on shallow
heuristics rather than robust ToM abilities. We caution against drawing
conclusions from anecdotal examples, limited benchmark testing, and using
human-designed psychological tests to evaluate models.",None,-1
3a61a1dd-5036-40f7-99d1-1d761dd59eb4,Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion,0.999822,"We propose a high-quality 3D-to-3D conversion method, Instruct 3D-to-3D. Our
method is designed for a novel task, which is to convert a given 3D scene to
another scene according to text instructions. Instruct 3D-to-3D applies
pretrained Image-to-Image diffusion models for 3D-to-3D conversion. This
enables the likelihood maximization of each viewpoint image and high-quality 3D
generation. In addition, our proposed method explicitly inputs the source 3D
scene as a condition, which enhances 3D consistency and controllability of how
much of the source 3D scene structure is reflected. We also propose dynamic
scaling, which allows the intensity of the geometry transformation to be
adjusted. We performed quantitative and qualitative evaluations and showed that
our proposed method achieves higher quality 3D-to-3D conversions than baseline
methods.",None,-1
06712c99-d2ec-4af5-9329-ed16b6b69dcd,Ensemble Modeling for Time Series Forecasting: an Adaptive Robust Optimization Approach,0.112021,"Accurate time series forecasting is critical for a wide range of problems
with temporal data. Ensemble modeling is a well-established technique for
leveraging multiple predictive models to increase accuracy and robustness, as
the performance of a single predictor can be highly variable due to shifts in
the underlying data distribution. This paper proposes a new methodology for
building robust ensembles of time series forecasting models. Our approach
utilizes Adaptive Robust Optimization (ARO) to construct a linear regression
ensemble in which the models' weights can adapt over time. We demonstrate the
effectiveness of our method through a series of synthetic experiments and
real-world applications, including air pollution management, energy consumption
forecasting, and tropical cyclone intensity forecasting. Our results show that
our adaptive ensembles outperform the best ensemble member in hindsight by
16-26% in root mean square error and 14-28% in conditional value at risk and
improve over competitive ensemble techniques.",None,-1
42ebdd39-2154-498a-b19b-9dd8bd9d897f,HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation,0.814038,"Monocular 3D human pose and shape estimation is an ill-posed problem since
multiple 3D solutions can explain a 2D image of a subject. Recent approaches
predict a probability distribution over plausible 3D pose and shape parameters
conditioned on the image. We show that these approaches exhibit a trade-off
between three key properties: (i) accuracy - the likelihood of the ground-truth
3D solution under the predicted distribution, (ii) sample-input consistency -
the extent to which 3D samples from the predicted distribution match the
visible 2D image evidence, and (iii) sample diversity - the range of plausible
3D solutions modelled by the predicted distribution. Our method, HuManiFlow,
predicts simultaneously accurate, consistent and diverse distributions. We use
the human kinematic tree to factorise full body pose into ancestor-conditioned
per-body-part pose distributions in an autoregressive manner. Per-body-part
distributions are implemented using normalising flows that respect the manifold
structure of SO(3), the Lie group of per-body-part poses. We show that
ill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,
and employ only probabilistic training losses. Code is available at:
https://github.com/akashsengupta1997/HuManiFlow.",None,-1
0ad9c322-b2fb-41f5-8e29-6fe85e70e7ab,Implicit neural representations for joint decomposition and registration of gene expression images in the marmoset brain,0.605263,"We propose a novel image registration method based on implicit neural
representations that addresses the challenging problem of registering a pair of
brain images with similar anatomical structures, but where one image contains
additional features or artifacts that are not present in the other image. To
demonstrate its effectiveness, we use 2D microscopy $\textit{in situ}$
hybridization gene expression images of the marmoset brain. Accurately
quantifying gene expression requires image registration to a brain template,
which is difficult due to the diversity of patterns causing variations in
visible anatomical brain structures. Our approach uses implicit networks in
combination with an image exclusion loss to jointly perform the registration
and decompose the image into a support and residual image. The support image
aligns well with the template, while the residual image captures individual
image characteristics that diverge from the template. In experiments, our
method provided excellent results and outperformed other registration
techniques.",None,-1
742b345a-a218-46ba-902e-26e60b42c236,From Knowledge Representation to Knowledge Organization and Back,0.291615,"Knowledge Representation (KR) and facet-analytical Knowledge Organization
(KO) have been the two most prominent methodologies of data and knowledge
modelling in the Artificial Intelligence community and the Information Science
community, respectively. KR boasts of a robust and scalable ecosystem of
technologies to support knowledge modelling while, often, underemphasizing the
quality of its models (and model-based data). KO, on the other hand, is less
technology-driven but has developed a robust framework of guiding principles
(canons) for ensuring modelling (and model-based data) quality. This paper
elucidates both the KR and facet-analytical KO methodologies in detail and
provides a functional mapping between them. Out of the mapping, the paper
proposes an integrated KO-enriched KR methodology with all the standard
components of a KR methodology plus the guiding canons of modelling quality
provided by KO. The practical benefits of the methodological integration has
been exemplified through a prominent case study of KR-based image annotation
exercise.",None,-1
247b24cb-bb4f-4be2-bb84-6e4a63df91cc,SoftMatch Distance: A Novel Distance for Weakly-Supervised Trend Change Detection in Bi-Temporal Images,0.486583,"General change detection (GCD) and semantic change detection (SCD) are common
methods for identifying changes and distinguishing object categories involved
in those changes, respectively. However, the binary changes provided by GCD is
often not practical enough, while annotating semantic labels for training SCD
models is very expensive. Therefore, there is a novel solution that intuitively
dividing changes into three trends (``appear'', ``disappear'' and
``transform'') instead of semantic categories, named it trend change detection
(TCD) in this paper. It offers more detailed change information than GCD, while
requiring less manual annotation cost than SCD. However, there are limited
public data sets with specific trend labels to support TCD application. To
address this issue, we propose a softmatch distance which is used to construct
a weakly-supervised TCD branch in a simple GCD model, using GCD labels instead
of TCD label for training. Furthermore, a strategic approach is presented to
successfully explore and extract background information, which is crucial for
the weakly-supervised TCD task. The experiment results on four public data sets
are highly encouraging, which demonstrates the effectiveness of our proposed
model.",None,-1
71b9bc2d-f155-4121-a9cd-42698bf4e524,Constraint and Union for Partially-Supervised Temporal Sentence Grounding,0.596228,"Temporal sentence grounding aims to detect the event timestamps described by
the natural language query from given untrimmed videos. The existing
fully-supervised setting achieves great performance but requires expensive
annotation costs; while the weakly-supervised setting adopts cheap labels but
performs poorly. To pursue high performance with less annotation cost, this
paper introduces an intermediate partially-supervised setting, i.e., only
short-clip or even single-frame labels are available during training. To take
full advantage of partial labels, we propose a novel quadruple constraint
pipeline to comprehensively shape event-query aligned representations, covering
intra- and inter-samples, uni- and multi-modalities. The former raises
intra-cluster compactness and inter-cluster separability; while the latter
enables event-background separation and event-query gather. To achieve more
powerful performance with explicit grounding optimization, we further introduce
a partial-full union framework, i.e., bridging with an additional
fully-supervised branch, to enjoy its impressive grounding bonus, and be robust
to partial annotations. Extensive experiments and ablations on Charades-STA and
ActivityNet Captions demonstrate the significance of partial supervision and
our superior performance.",None,-1
c5e33c28-82fb-4a69-97a1-0500578e5c87,Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations,0.77426,"The language ability of Large Language Models (LLMs) is often unbalanced
towards English because of the imbalance in the distribution of the
pre-training data. This disparity is demanded in further fine-tuning and
affecting the cross-lingual abilities of LLMs. In this paper, we propose to
empower Instructiontuned LLMs (It-LLMs) in languages other than English by
building semantic alignment between them. Hence, we propose CrossAlpaca, an
It-LLM with cross-lingual instruction-following and Translation-following
demonstrations to improve semantic alignment between languages. We validate our
approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA
and adapted versions of MMLU and BBH. Our models, tested over six different
languages, outperform the It-LLMs tuned on monolingual data. The final results
show that instruction tuning on non-English data is not enough and that
semantic alignment can be further improved by Translation-following
demonstrations.",None,-1
bdeeb253-ae55-412e-97eb-7cff504f0178,IRFL: Image Recognition of Figurative Language,0.312525,"Figures of speech such as metaphors, similes, and idioms are integral parts
of human communication. They are ubiquitous in many forms of discourse,
allowing people to convey complex, abstract ideas and evoke emotion. As
figurative forms are often conveyed through multiple modalities (e.g., both
text and images), understanding multimodal figurative language is an important
AI challenge, weaving together profound vision, language, commonsense and
cultural knowledge. In this work, we develop the Image Recognition of
Figurative Language (IRFL) dataset. We leverage human annotation and an
automatic pipeline we created to generate a multimodal dataset, and introduce
two novel tasks as a benchmark for multimodal figurative language
understanding. We experimented with state-of-the-art vision and language models
and found that the best (22%) performed substantially worse than humans (97%).
We release our dataset, benchmark, and code, in hopes of driving the
development of models that can better understand figurative language.",None,-1
3c8f981a-28b4-424c-867e-c2d9b506d698,PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds,0.984794,"In order to deal with the sparse and unstructured raw point clouds, LiDAR
based 3D object detection research mostly focuses on designing dedicated local
point aggregators for fine-grained geometrical modeling. In this paper, we
revisit the local point aggregators from the perspective of allocating
computational resources. We find that the simplest pillar based models perform
surprisingly well considering both accuracy and latency. Additionally, we show
that minimal adaptions from the success of 2D object detection, such as
enlarging receptive field, significantly boost the performance. Extensive
experiments reveal that our pillar based networks with modernized designs in
terms of architecture and training render the state-of-the-art performance on
the two popular benchmarks: Waymo Open Dataset and nuScenes. Our results
challenge the common intuition that the detailed geometry modeling is essential
to achieve high performance for 3D object detection.",None,-1
522a71d7-bcbd-4569-a561-8c483a8188fa,Enhancing Few-shot NER with Prompt Ordering based Data Augmentation,0.159037,"Recently, data augmentation (DA) methods have been proven to be effective for
pre-trained language models (PLMs) in low-resource settings, including few-shot
named entity recognition (NER). However, conventional NER DA methods are mostly
aimed at sequence labeling models, i.e., token-level classification, and few
are compatible with unified autoregressive generation frameworks, which can
handle a wider range of NER tasks, such as nested NER. Furthermore, these
generation frameworks have a strong assumption that the entities will appear in
the target sequence with the same left-to-right order as the source sequence.
In this paper, we claim that there is no need to keep this strict order, and
more diversified but reasonable target entity sequences can be provided during
the training stage as a novel DA method. Nevertheless, a naive mixture of
augmented data can confuse the model since one source sequence will then be
paired with different target sequences. Therefore, we propose a simple but
effective Prompt Ordering based Data Augmentation (PODA) method to improve the
training of unified autoregressive generation frameworks under few-shot NER
scenarios. Experimental results on three public NER datasets and further
analyses demonstrate the effectiveness of our approach.",None,-1
8d1a4a80-e1f9-41c5-9aa4-d38f296926b2,Improving User Controlled Table-To-Text Generation Robustness,0.417825,"In this work we study user controlled table-to-text generation where users
explore the content in a table by selecting cells and reading a natural
language description thereof automatically produce by a natural language
generator. Such generation models usually learn from carefully selected cell
combinations (clean cell selections); however, in practice users may select
unexpected, redundant, or incoherent cell combinations (noisy cell selections).
In experiments, we find that models perform well on test sets coming from the
same distribution as the train data but their performance drops when evaluated
on realistic noisy user inputs. We propose a fine-tuning regime with additional
user-simulated noisy cell selections. Models fine-tuned with the proposed
regime gain 4.85 BLEU points on user noisy test cases and 1.4 on clean test
cases; and achieve comparable state-of-the-art performance on the ToTTo
dataset.",None,-1
b5d4916e-4285-46b9-8cc7-e7329d8fb39e,Human Pose Estimation in Extremely Low-Light Conditions,0.670756,"We study human pose estimation in extremely low-light images. This task is
challenging due to the difficulty of collecting real low-light images with
accurate labels, and severely corrupted inputs that degrade prediction quality
significantly. To address the first issue, we develop a dedicated camera system
and build a new dataset of real low-light images with accurate pose labels.
Thanks to our camera system, each low-light image in our dataset is coupled
with an aligned well-lit image, which enables accurate pose labeling and is
used as privileged information during training. We also propose a new model and
a new training strategy that fully exploit the privileged information to learn
representation insensitive to lighting conditions. Our method demonstrates
outstanding performance on real extremely low light images, and extensive
analyses validate that both of our model and dataset contribute to the success.",None,-1
5ae77a8a-c8b4-40c2-95b1-fcc5169cccf6,Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System,0.342832,"End-to-end task-oriented dialogue (TOD) systems have achieved promising
performance by leveraging sophisticated natural language understanding and
natural language generation capabilities of pre-trained models. This work
enables the TOD systems with more flexibility through a simple cache. The cache
provides the flexibility to dynamically update the TOD systems and handle both
existing and unseen dialogue scenarios. Towards this end, we first fine-tune a
retrieval module to effectively retrieve the most relevant information entries
from the cache. We then train end-to-end TOD models that can refer to and
ground on both dialogue history and retrieved information during TOD
generation. The cache is straightforward to construct, and the backbone models
of TOD systems are compatible with existing pre-trained generative models.
Extensive experiments demonstrate the superior performance of our framework,
with a notable improvement in non-empty joint goal accuracy by 6.7% compared to
strong baselines.",None,-1
6bdac6a6-4afb-4bd0-b07d-12f537ff0166,TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models,0.156129,"Large Language Models (LLMs) such as ChatGPT, have gained significant
attention due to their impressive natural language processing capabilities. It
is crucial to prioritize human-centered principles when utilizing these models.
Safeguarding the ethical and moral compliance of LLMs is of utmost importance.
However, individual ethical issues have not been well studied on the latest
LLMs. Therefore, this study aims to address these gaps by introducing a new
benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in
three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT
examines toxicity in language models by employing toxic prompt templates
derived from social norms. It then quantifies the extent of bias in models by
measuring quantifiable toxicity values across different groups. Lastly,
TrustGPT assesses the value of conversation generation models from both active
value-alignment and passive value-alignment tasks. Through the implementation
of TrustGPT, this research aims to enhance our understanding of the performance
of conversation generation models and promote the development of language
models that are more ethical and socially responsible.",None,-1
76e49333-1132-409a-8b45-9d3af5f7f8c9,Cross-Cultural Transfer Learning for Chinese Offensive Language Detection,0.4725,"Detecting offensive language is a challenging task. Generalizing across
different cultures and languages becomes even more challenging: besides
lexical, syntactic and semantic differences, pragmatic aspects such as cultural
norms and sensitivities, which are particularly relevant in this context, vary
greatly. In this paper, we target Chinese offensive language detection and aim
to investigate the impact of transfer learning using offensive language
detection data from different cultural backgrounds, specifically Korean and
English. We find that culture-specific biases in what is considered offensive
negatively impact the transferability of language models (LMs) and that LMs
trained on diverse cultural data are sensitive to different features in Chinese
offensive language detection. In a few-shot learning scenario, however, our
study shows promising prospects for non-English offensive language detection
with limited resources. Our findings highlight the importance of cross-cultural
transfer learning in improving offensive language detection and promoting
inclusive digital spaces.",None,-1
4230f808-0500-446b-a892-f6712d45b567,Uni-NLX: Unifying Textual Explanations for Vision and Vision-Language Tasks,0.0652204,"Natural Language Explanations (NLE) aim at supplementing the prediction of a
model with human-friendly natural text. Existing NLE approaches involve
training separate models for each downstream task. In this work, we propose
Uni-NLX, a unified framework that consolidates all NLE tasks into a single and
compact multi-task model using a unified training objective of text generation.
Additionally, we introduce two new NLE datasets: 1) ImageNetX, a dataset of
144K samples for explaining ImageNet categories, and 2) VQA-ParaX, a dataset of
123K samples for explaining the task of Visual Question Answering (VQA). Both
datasets are derived leveraging large language models (LLMs). By training on
the 1M combined NLE samples, our single unified framework is capable of
simultaneously performing seven NLE tasks including VQA, visual recognition and
visual reasoning tasks with 7X fewer parameters, demonstrating comparable
performance to the independent task-specific models in previous approaches, and
in certain tasks even outperforming them. Code is at
https://github.com/fawazsammani/uni-nlx",None,-1
8e4ca00d-d8e5-4e8c-84ba-c67c5eacd7a0,Bridging Physics-Informed Neural Networks with Reinforcement Learning: Hamilton-Jacobi-Bellman Proximal Policy Optimization (HJBPPO),0.592809,"This paper introduces the Hamilton-Jacobi-Bellman Proximal Policy
Optimization (HJBPPO) algorithm into reinforcement learning. The
Hamilton-Jacobi-Bellman (HJB) equation is used in control theory to evaluate
the optimality of the value function. Our work combines the HJB equation with
reinforcement learning in continuous state and action spaces to improve the
training of the value network. We treat the value network as a Physics-Informed
Neural Network (PINN) to solve for the HJB equation by computing its
derivatives with respect to its inputs exactly. The Proximal Policy
Optimization (PPO)-Clipped algorithm is improvised with this implementation as
it uses a value network to compute the objective function for its policy
network. The HJBPPO algorithm shows an improved performance compared to PPO on
the MuJoCo environments.",None,-1
8e4cbfad-6509-4ef7-9099-7e32b0762ec9,DyLiN: Making Light Field Networks Dynamic,0.924527,"Light Field Networks, the re-formulations of radiance fields to oriented
rays, are magnitudes faster than their coordinate network counterparts, and
provide higher fidelity with respect to representing 3D structures from 2D
observations. They would be well suited for generic scene representation and
manipulation, but suffer from one problem: they are limited to holistic and
static scenes. In this paper, we propose the Dynamic Light Field Network
(DyLiN) method that can handle non-rigid deformations, including topological
changes. We learn a deformation field from input rays to canonical rays, and
lift them into a higher dimensional space to handle discontinuities. We further
introduce CoDyLiN, which augments DyLiN with controllable attribute inputs. We
train both models via knowledge distillation from pretrained dynamic radiance
fields. We evaluated DyLiN using both synthetic and real world datasets that
include various non-rigid deformations. DyLiN qualitatively outperformed and
quantitatively matched state-of-the-art methods in terms of visual fidelity,
while being 25 - 71x computationally faster. We also tested CoDyLiN on
attribute annotated data and it surpassed its teacher model. Project page:
https://dylin2023.github.io .",None,-1
433f791c-11cb-4560-90ee-2af91cbfdf17,Image Hash Minimization for Tamper Detection,0.0150208,"Tamper detection using image hash is a very common problem of modern days.
Several research and advancements have already been done to address this
problem. However, most of the existing methods lack the accuracy of tamper
detection when the tampered area is low, as well as requiring long image
hashes. In this paper, we propose a novel method objectively to minimize the
hash length while enhancing the performance at low tampered area.",None,-1
2950a3e6-b3f4-4e21-be49-6b4a09493b12,ParaLS: Lexical Substitution via Pretrained Paraphraser,0.564799,"Lexical substitution (LS) aims at finding appropriate substitutes for a
target word in a sentence. Recently, LS methods based on pretrained language
models have made remarkable progress, generating potential substitutes for a
target word through analysis of its contextual surroundings. However, these
methods tend to overlook the preservation of the sentence's meaning when
generating the substitutes. This study explores how to generate the substitute
candidates from a paraphraser, as the generated paraphrases from a paraphraser
contain variations in word choice and preserve the sentence's meaning. Since we
cannot directly generate the substitutes via commonly used decoding strategies,
we propose two simple decoding strategies that focus on the variations of the
target word during decoding. Experimental results show that our methods
outperform state-of-the-art LS methods based on pre-trained language models on
three benchmarks.",None,-1
22838a7a-6b4a-41b1-a88a-67595e60cbab,SSN: Stockwell Scattering Network for SAR Image Change Detection,0.554084,"Recently, synthetic aperture radar (SAR) image change detection has become an
interesting yet challenging direction due to the presence of speckle noise.
Although both traditional and modern learning-driven methods attempted to
overcome this challenge, deep convolutional neural networks (DCNNs)-based
methods are still hindered by the lack of interpretability and the requirement
of large computation power. To overcome this drawback, wavelet scattering
network (WSN) and Fourier scattering network (FSN) are proposed. Combining
respective merits of WSN and FSN, we propose Stockwell scattering network (SSN)
based on Stockwell transform which is widely applied against noisy signals and
shows advantageous characteristics in speckle reduction. The proposed SSN
provides noise-resilient feature representation and obtains state-of-art
performance in SAR image change detection as well as high computational
efficiency. Experimental results on three real SAR image datasets demonstrate
the effectiveness of the proposed method.",None,-1
348da0d0-2af6-4a71-8487-e9426fc88af3,Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation,0.0355517,"Knowledge-grounded dialogue generation aims to mitigate the issue of text
degeneration by incorporating external knowledge to supplement the context.
However, the model often fails to internalize this information into responses
in a human-like manner. Instead, it simply inserts segments of the provided
knowledge into generic responses. As a result, the generated responses tend to
be tedious, incoherent, and in lack of interactivity which means the
degeneration problem is still unsolved. In this work, we first find that such
copying-style degeneration is primarily due to the weak likelihood objective,
which allows the model to ""cheat"" the objective by merely duplicating knowledge
segments in a superficial pattern matching based on overlap. To overcome this
challenge, we then propose a Multi-level Adaptive Contrastive Learning (MACL)
framework that dynamically samples negative examples and subsequently penalizes
degeneration behaviors at both the token-level and sequence-level. Extensive
experiments on the WoW dataset demonstrate the effectiveness of our approach
across various pre-trained models.",None,-1
623d928f-f0c7-4f96-badb-ce2179257bbf,Data-Driven Bilateral Generalized Two-Dimensional Quaternion Principal Component Analysis with Application to Color Face Recognition,0.213221,"A new data-driven bilateral generalized two-dimensional quaternion principal
component analysis (BiG2DQPCA) is presented to extract the features of matrix
samples from both row and column directions. This general framework directly
works on the 2D color images without vectorizing and well preserves the spatial
and color information, which makes it flexible to fit various real-world
applications. A generalized ridge regression model of BiG2DQPCA is firstly
proposed with orthogonality constrains on aimed features. Applying the
deflation technique and the framework of minorization-maximization, a new
quaternion optimization algorithm is proposed to compute the optimal features
of BiG2DQPCA and a closed-form solution is obtained at each iteration. A new
approach based on BiG2DQPCA is presented for color face recognition and image
reconstruction with a new data-driven weighting technique. Sufficient numerical
experiments are implemented on practical color face databases and indicate the
superiority of BiG2DQPCA over the state-of-the-art methods in terms of
recognition accuracies and rates of image reconstruction.",None,-1
b8fcd734-1071-4539-8208-eafd476f8970,Discriminative Deep Feature Visualization for Explainable Face Recognition,0.375596,"Despite the huge success of deep convolutional neural networks in face
recognition (FR) tasks, current methods lack explainability for their
predictions because of their ""black-box"" nature. In recent years, studies have
been carried out to give an interpretation of the decision of a deep FR system.
However, the affinity between the input facial image and the extracted deep
features has not been explored. This paper contributes to the problem of
explainable face recognition by first conceiving a face reconstruction-based
explanation module, which reveals the correspondence between the deep feature
and the facial regions. To further interpret the decision of an FR model, a
novel visual saliency explanation algorithm has been proposed. It provides
insightful explanation by producing visual saliency maps that represent similar
and dissimilar regions between input faces. A detailed analysis has been
presented for the generated visual explanation to show the effectiveness of the
proposed method.",None,-1
6c3747f6-e18d-4bbb-b416-9da59565df42,Fine-grained Emotional Control of Text-To-Speech: Learning To Rank Inter- And Intra-Class Emotion Intensities,0.427374,"State-of-the-art Text-To-Speech (TTS) models are capable of producing
high-quality speech. The generated speech, however, is usually neutral in
emotional expression, whereas very often one would want fine-grained emotional
control of words or phonemes. Although still challenging, the first TTS models
have been recently proposed that are able to control voice by manually
assigning emotion intensity. Unfortunately, due to the neglect of intra-class
distance, the intensity differences are often unrecognizable. In this paper, we
propose a fine-grained controllable emotional TTS, that considers both inter-
and intra-class distances and be able to synthesize speech with recognizable
intensity difference. Our subjective and objective experiments demonstrate that
our model exceeds two state-of-the-art controllable TTS models for
controllability, emotion expressiveness and naturalness.",None,-1
c5eed6bf-87d6-4c42-aaf6-c100532869c0,SAOR: Single-View Articulated Object Reconstruction,0.457083,"We introduce SAOR, a novel approach for estimating the 3D shape, texture, and
viewpoint of an articulated object from a single image captured in the wild.
Unlike prior approaches that rely on pre-defined category-specific 3D templates
or tailored 3D skeletons, SAOR learns to articulate shapes from single-view
image collections with a skeleton-free part-based model without requiring any
3D object shape priors. To prevent ill-posed solutions, we propose a
cross-instance consistency loss that exploits disentangled object shape
deformation and articulation. This is helped by a new silhouette-based sampling
mechanism to enhance viewpoint diversity during training. Our method only
requires estimated object silhouettes and relative depth maps from
off-the-shelf pre-trained networks during training. At inference time, given a
single-view image, it efficiently outputs an explicit mesh representation. We
obtain improved qualitative and quantitative results on challenging quadruped
animals compared to relevant existing work.",None,-1
22e31c88-c573-4253-8778-b60eb5fd0189,Benchmarking and Improving Generator-Validator Consistency of Language Models,0.118408,"As of September 2023, ChatGPT correctly answers ""what is 7+8"" with 15, but
when asked ""7+8=15, True or False"" it responds with ""False"". This inconsistency
between generating and validating an answer is prevalent in language models
(LMs) and erodes trust. In this paper, we propose a framework for measuring the
consistency between generation and validation (which we call
generator-validator consistency, or GV-consistency), finding that even GPT-4, a
state-of-the-art LM, is GV-consistent only 76% of the time. To improve the
consistency of LMs, we propose to finetune on the filtered generator and
validator responses that are GV-consistent, and call this approach consistency
fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B
from 60% to 93%, and the improvement extrapolates to unseen tasks and domains
(e.g., GV-consistency for positive style transfers extrapolates to unseen
styles like humor). In addition to improving consistency, consistency
fine-tuning improves both generator quality and validator accuracy without
using any labeled data. Evaluated across 6 tasks, including math questions,
knowledge-intensive QA, and instruction following, our method improves the
generator quality by 16% and the validator accuracy by 6.3% across all tasks.",None,-1
8eac789b-5cd9-423e-b960-6867623c967c,Almanac: Retrieval-Augmented Language Models for Clinical Medicine,0.994767,"Large-language models have recently demonstrated impressive zero-shot
capabilities in a variety of natural language tasks such as summarization,
dialogue generation, and question-answering. Despite many promising
applications in clinical medicine, adoption of these models in real-world
settings has been largely limited by their tendency to generate incorrect and
sometimes even toxic statements. In this study, we develop Almanac, a large
language model framework augmented with retrieval capabilities for medical
guideline and treatment recommendations. Performance on a novel dataset of
clinical scenarios (n = 130) evaluated by a panel of 5 board-certified and
resident physicians demonstrates significant increases in factuality (mean of
18% at p-value < 0.05) across all specialties, with improvements in
completeness and safety. Our results demonstrate the potential for large
language models to be effective tools in the clinical decision-making process,
while also emphasizing the importance of careful testing and deployment to
mitigate their shortcomings.",None,-1
3d6b4c6b-c9a0-431b-81ef-e2b8668510b0,Aligning Speakers: Evaluating and Visualizing Text-based Diarization Using Efficient Multiple Sequence Alignment (Extended Version),0.261559,"This paper presents a novel evaluation approach to text-based speaker
diarization (SD), tackling the limitations of traditional metrics that do not
account for any contextual information in text. Two new metrics are proposed,
Text-based Diarization Error Rate and Diarization F1, which perform utterance-
and word-level evaluations by aligning tokens in reference and hypothesis
transcripts. Our metrics encompass more types of errors compared to existing
ones, allowing us to make a more comprehensive analysis in SD. To align tokens,
a multiple sequence alignment algorithm is introduced that supports multiple
sequences in the reference while handling high-dimensional alignment to the
hypothesis using dynamic programming. Our work is packaged into two tools,
align4d providing an API for our alignment algorithm and TranscribeView for
visualizing and evaluating SD errors, which can greatly aid in the creation of
high-quality data, fostering the advancement of dialogue systems.",None,-1
8abb2d15-b21b-4db6-8066-2b1b24d6e7bf,Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information,0.639458,"Multi-target multi-camera tracking (MTMCT) of vehicles, i.e. tracking
vehicles across multiple cameras, is a crucial application for the development
of smart city and intelligent traffic system. The main challenges of MTMCT of
vehicles include the intra-class variability of the same vehicle and
inter-class similarity between different vehicles and how to associate the same
vehicle accurately across different cameras under large search space. Previous
methods for MTMCT usually use hierarchical clustering of trajectories to
conduct cross camera association. However, the search space can be large and
does not take spatial and temporal information into consideration. In this
paper, we proposed a transformer-based camera link model with spatial and
temporal filtering to conduct cross camera tracking. Achieving 73.68% IDF1 on
the Nvidia Cityflow V2 dataset test set, showing the effectiveness of our
camera link model on multi-target multi-camera tracking.",None,-1
b0605df0-09a3-4438-89ab-b5bb7da7fe21,The XAISuite framework and the implications of explanatory system dissonance,0.0387172,"Explanatory systems make machine learning models more transparent. However,
they are often inconsistent. In order to quantify and isolate possible
scenarios leading to this discrepancy, this paper compares two explanatory
systems, SHAP and LIME, based on the correlation of their respective importance
scores using 14 machine learning models (7 regression and 7 classification) and
4 tabular datasets (2 regression and 2 classification). We make two novel
findings. Firstly, the magnitude of importance is not significant in
explanation consistency. The correlations between SHAP and LIME importance
scores for the most important features may or may not be more variable than the
correlation between SHAP and LIME importance scores averaged across all
features. Secondly, the similarity between SHAP and LIME importance scores
cannot predict model accuracy. In the process of our research, we construct an
open-source library, XAISuite, that unifies the process of training and
explaining models. Finally, this paper contributes a generalized framework to
better explain machine learning models and optimize their performance.",None,-1
e7a38496-6fef-4798-a417-588581ab84b5,Guided Image Synthesis via Initial Image Editing in Diffusion Model,0.660078,"Diffusion models have the ability to generate high quality images by
denoising pure Gaussian noise images. While previous research has primarily
focused on improving the control of image generation through adjusting the
denoising process, we propose a novel direction of manipulating the initial
noise to control the generated image. Through experiments on stable diffusion,
we show that blocks of pixels in the initial latent images have a preference
for generating specific content, and that modifying these blocks can
significantly influence the generated image. In particular, we show that
modifying a part of the initial image affects the corresponding region of the
generated image while leaving other regions unaffected, which is useful for
repainting tasks. Furthermore, we find that the generation preferences of pixel
blocks are primarily determined by their values, rather than their position. By
moving pixel blocks with a tendency to generate user-desired content to
user-specified regions, our approach achieves state-of-the-art performance in
layout-to-image generation. Our results highlight the flexibility and power of
initial image manipulation in controlling the generated image.",None,-1
4873741b-618e-496d-939c-9be51eafd938,Semantic Strengthening of Neuro-Symbolic Learning,0.735066,"Numerous neuro-symbolic approaches have recently been proposed typically with
the goal of adding symbolic knowledge to the output layer of a neural network.
Ideally, such losses maximize the probability that the neural network's
predictions satisfy the underlying domain. Unfortunately, this type of
probabilistic inference is often computationally infeasible. Neuro-symbolic
approaches therefore commonly resort to fuzzy approximations of this
probabilistic objective, sacrificing sound probabilistic semantics, or to
sampling which is very seldom feasible. We approach the problem by first
assuming the constraint decomposes conditioned on the features learned by the
network. We iteratively strengthen our approximation, restoring the dependence
between the constraints most responsible for degrading the quality of the
approximation. This corresponds to computing the mutual information between
pairs of constraints conditioned on the network's learned features, and may be
construed as a measure of how well aligned the gradients of two distributions
are. We show how to compute this efficiently for tractable circuits. We test
our approach on three tasks: predicting a minimum-cost path in Warcraft,
predicting a minimum-cost perfect matching, and solving Sudoku puzzles,
observing that it improves upon the baselines while sidestepping
intractability.",None,-1
e349e465-6281-494b-97c8-14cf8b12f3ea,Correct Like Humans: Progressive Learning Framework for Chinese Text Error Correction,0.952779,"Chinese Text Error Correction (CTEC) aims to detect and correct errors in the
input text, which benefits human daily life and various downstream tasks.
Recent approaches mainly employ Pre-trained Language Models (PLMs) to resolve
CTEC. Although PLMs have achieved remarkable success in CTEC, we argue that
previous studies still overlook the importance of human thinking patterns. To
enhance the development of PLMs for CTEC, inspired by humans' daily
error-correcting behavior, we propose a novel model-agnostic progressive
learning framework, named ProTEC, which guides PLMs-based CTEC models to learn
to correct like humans. During the training process, ProTEC guides the model to
learn text error correction by incorporating these sub-tasks into a progressive
paradigm. During the inference process, the model completes these sub-tasks in
turn to generate the correction results. Extensive experiments and detailed
analyses demonstrate the effectiveness and efficiency of our proposed
model-agnostic ProTEC framework.",None,-1
5f480d75-d0b0-49c0-b0f8-8e202590dc0d,Composing Task Knowledge with Modular Successor Feature Approximators,0.532457,"Recently, the Successor Features and Generalized Policy Improvement (SF&GPI)
framework has been proposed as a method for learning, composing, and
transferring predictive knowledge and behavior. SF&GPI works by having an agent
learn predictive representations (SFs) that can be combined for transfer to new
tasks with GPI. However, to be effective this approach requires state features
that are useful to predict, and these state-features are typically
hand-designed. In this work, we present a novel neural network architecture,
""Modular Successor Feature Approximators"" (MSFA), where modules both discover
what is useful to predict, and learn their own predictive representations. We
show that MSFA is able to better generalize compared to baseline architectures
for learning SFs and modular architectures",None,-1
2d6a9cba-2a08-45af-b97d-1111b12a0e2c,Improving Anomaly Segmentation with Multi-Granularity Cross-Domain Alignment,0.0903555,"Anomaly segmentation plays a pivotal role in identifying atypical objects in
images, crucial for hazard detection in autonomous driving systems. While
existing methods demonstrate noteworthy results on synthetic data, they often
fail to consider the disparity between synthetic and real-world data domains.
Addressing this gap, we introduce the Multi-Granularity Cross-Domain Alignment
(MGCDA) framework, tailored to harmonize features across domains at both the
scene and individual sample levels. Our contributions are twofold: i) We
present the Multi-source Domain Adversarial Training module. This integrates a
multi-source adversarial loss coupled with dynamic label smoothing,
facilitating the learning of domain-agnostic representations across multiple
processing stages. ii) We propose an innovative Cross-domain Anomaly-aware
Contrastive Learning methodology.} This method adeptly selects challenging
anchor points and images using an anomaly-centric strategy, ensuring precise
alignment at the sample level. Extensive evaluations of the Fishyscapes and
RoadAnomaly datasets demonstrate MGCDA's superior performance and adaptability.
Additionally, its ability to perform parameter-free inference and function with
various network architectures highlights its distinctiveness in advancing the
frontier of anomaly segmentation.",None,-1
20710eae-3940-4fb3-99c6-a2ede4a0b381,RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems,0.999399,"Large Language Models (LLMs) have greatly advanced code auto-completion
systems, with a potential for substantial productivity enhancements for
developers. However, current benchmarks mainly focus on single-file tasks,
leaving an assessment gap for more complex, real-world, multi-file programming
scenarios. To fill this gap, we introduce RepoBench, a new benchmark
specifically designed for evaluating repository-level code auto-completion
systems. RepoBench supports both Python and Java and consists of three
interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code
Completion), and RepoBench-P (Pipeline). Each task respectively measures the
system's ability to retrieve the most relevant code snippets from other files
as cross-file context, predict the next line of code with cross-file and
in-file context, and handle complex tasks that require a combination of both
retrieval and next-line prediction. RepoBench aims to facilitate a more
complete comparison of performance and encouraging continuous improvement in
auto-completion systems. RepoBench is publicly available at
https://github.com/Leolty/repobench.",None,-1
b7321841-f771-4b81-acb9-f4a7a5586514,3D Reconstruction of Non-cooperative Resident Space Objects using Instant NGP-accelerated NeRF and D-NeRF,0.260848,"The proliferation of non-cooperative resident space objects (RSOs) in orbit
has spurred the demand for active space debris removal, on-orbit servicing
(OOS), classification, and functionality identification of these RSOs. Recent
advances in computer vision have enabled high-definition 3D modeling of objects
based on a set of 2D images captured from different viewing angles. This work
adapts Instant NeRF and D-NeRF, variations of the neural radiance field (NeRF)
algorithm to the problem of mapping RSOs in orbit for the purposes of
functionality identification and assisting with OOS. The algorithms are
evaluated for 3D reconstruction quality and hardware requirements using
datasets of images of a spacecraft mock-up taken under two different lighting
and motion conditions at the Orbital Robotic Interaction, On-Orbit Servicing
and Navigation (ORION) Laboratory at Florida Institute of Technology. Instant
NeRF is shown to learn high-fidelity 3D models with a computational cost that
could feasibly be trained on on-board computers.",None,-1
11fc0c90-72a0-46e5-a82b-9022aa0acb6e,Laplacian Segmentation Networks: Improved Epistemic Uncertainty from Spatial Aleatoric Uncertainty,0.159136,"Out of distribution (OOD) medical images are frequently encountered, e.g.
because of site- or scanner differences, or image corruption. OOD images come
with a risk of incorrect image segmentation, potentially negatively affecting
downstream diagnoses or treatment. To ensure robustness to such incorrect
segmentations, we propose Laplacian Segmentation Networks (LSN) that jointly
model epistemic (model) and aleatoric (data) uncertainty in image segmentation.
We capture data uncertainty with a spatially correlated logit distribution. For
model uncertainty, we propose the first Laplace approximation of the weight
posterior that scales to large neural networks with skip connections that have
high-dimensional outputs. Empirically, we demonstrate that modelling spatial
pixel correlation allows the Laplacian Segmentation Network to successfully
assign high epistemic uncertainty to out-of-distribution objects appearing
within images.",None,-1
be763b55-61ba-4289-8560-c2339ea68b73,Object Discovery from Motion-Guided Tokens,0.212293,"Object discovery -- separating objects from the background without manual
labels -- is a fundamental open challenge in computer vision. Previous methods
struggle to go beyond clustering of low-level cues, whether handcrafted (e.g.,
color, texture) or learned (e.g., from auto-encoders). In this work, we augment
the auto-encoder representation learning framework with two key components:
motion-guidance and mid-level feature tokenization. Although both have been
separately investigated, we introduce a new transformer decoder showing that
their benefits can compound thanks to motion-guided vector quantization. We
show that our architecture effectively leverages the synergy between motion and
tokenization, improving upon the state of the art on both synthetic and real
datasets. Our approach enables the emergence of interpretable object-specific
mid-level features, demonstrating the benefits of motion-guidance (no labeling)
and quantization (interpretability, memory efficiency).",None,-1
98625448-c1a0-4ef4-b4ad-c2f029c12733,An End-to-End Multi-Task Learning Model for Image-based Table Recognition,0.522754,"Image-based table recognition is a challenging task due to the diversity of
table styles and the complexity of table structures. Most of the previous
methods focus on a non-end-to-end approach which divides the problem into two
separate sub-problems: table structure recognition; and cell-content
recognition and then attempts to solve each sub-problem independently using two
separate systems. In this paper, we propose an end-to-end multi-task learning
model for image-based table recognition. The proposed model consists of one
shared encoder, one shared decoder, and three separate decoders which are used
for learning three sub-tasks of table recognition: table structure recognition,
cell detection, and cell-content recognition. The whole system can be easily
trained and inferred in an end-to-end approach. In the experiments, we evaluate
the performance of the proposed model on two large-scale datasets: FinTabNet
and PubTabNet. The experiment results show that the proposed model outperforms
the state-of-the-art methods in all benchmark datasets.",None,-1
c235454b-9bbe-4ae2-ba9e-cde2e1f07824,InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue Systems,0.446333,"Large language models (LLMs) have been used for diverse tasks in natural
language processing (NLP), yet remain under-explored for task-oriented dialogue
systems (TODS), especially for end-to-end TODS. We present InstructTODS, a
novel off-the-shelf framework for zero-shot end-to-end task-oriented dialogue
systems that can adapt to diverse domains without fine-tuning. By leveraging
LLMs, InstructTODS generates a proxy belief state that seamlessly translates
user intentions into dynamic queries for efficient interaction with any KB. Our
extensive experiments demonstrate that InstructTODS achieves comparable
performance to fully fine-tuned TODS in guiding dialogues to successful
completion without prior knowledge or task-specific data. Furthermore, a
rigorous human evaluation of end-to-end TODS shows that InstructTODS produces
dialogue responses that notably outperform both the gold responses and the
state-of-the-art TODS in terms of helpfulness, informativeness, and humanness.
Moreover, the effectiveness of LLMs in TODS is further supported by our
comprehensive evaluations on TODS subtasks: dialogue state tracking, intent
classification, and response generation. Code and implementations could be
found here https://github.com/WillyHC22/InstructTODS/",None,-1
62ecd736-7cef-43dd-acb0-e5ae8956fb12,Content-based Unrestricted Adversarial Attack,0.859301,"Unrestricted adversarial attacks typically manipulate the semantic content of
an image (e.g., color or texture) to create adversarial examples that are both
effective and photorealistic, demonstrating their ability to deceive human
perception and deep neural networks with stealth and success. However, current
works usually sacrifice unrestricted degrees and subjectively select some image
content to guarantee the photorealism of unrestricted adversarial examples,
which limits its attack performance. To ensure the photorealism of adversarial
examples and boost attack performance, we propose a novel unrestricted attack
framework called Content-based Unrestricted Adversarial Attack. By leveraging a
low-dimensional manifold that represents natural images, we map the images onto
the manifold and optimize them along its adversarial direction. Therefore,
within this framework, we implement Adversarial Content Attack based on Stable
Diffusion and can generate high transferable unrestricted adversarial examples
with various adversarial contents. Extensive experimentation and visualization
demonstrate the efficacy of ACA, particularly in surpassing state-of-the-art
attacks by an average of 13.3-50.4% and 16.8-48.0% in normally trained models
and defense methods, respectively.",None,-1
f72fba86-9d39-46e6-889e-8eb210d09aec,Guiding AI-Generated Digital Content with Wireless Perception,0.266636,"Recent advances in artificial intelligence (AI), coupled with a surge in
training data, have led to the widespread use of AI for digital content
generation, with ChatGPT serving as a representative example. Despite the
increased efficiency and diversity, the inherent instability of AI models poses
a persistent challenge in guiding these models to produce the desired content
for users. In this paper, we introduce an integration of wireless perception
(WP) with AI-generated content (AIGC) and propose a unified WP-AIGC framework
to improve the quality of digital content production. The framework employs a
novel multi-scale perception technology to read user's posture, which is
difficult to describe accurately in words, and transmits it to the AIGC model
as skeleton images. Based on these images and user's service requirements, the
AIGC model generates corresponding digital content. Since the production
process imposes the user's posture as a constraint on the AIGC model, it makes
the generated content more aligned with the user's requirements. Additionally,
WP-AIGC can also accept user's feedback, allowing adjustment of computing
resources at edge server to improve service quality. Experiments results verify
the effectiveness of the WP-AIGC framework, highlighting its potential as a
novel approach for guiding AI models in the accurate generation of digital
content.",None,-1
9e454627-08eb-40dc-bf2b-3415bec66e81,Novelty Detection in Network Traffic: Using Survival Analysis for Feature Identification,0.318184,"Intrusion Detection Systems are an important component of many organizations'
cyber defense and resiliency strategies. However, one downside of these systems
is their reliance on known attack signatures for detection of malicious network
events. When it comes to unknown attack types and zero-day exploits, modern
Intrusion Detection Systems often fall short. In this paper, we introduce an
unconventional approach to identifying network traffic features that influence
novelty detection based on survival analysis techniques. Specifically, we
combine several Cox proportional hazards models and implement Kaplan-Meier
estimates to predict the probability that a classifier identifies novelty after
the injection of an unknown network attack at any given time. The proposed
model is successful at pinpointing PSH Flag Count, ACK Flag Count, URG Flag
Count, and Down/Up Ratio as the main features to impact novelty detection via
Random Forest, Bayesian Ridge, and Linear Support Vector Regression
classifiers.",None,-1
10064853-9c01-4cf0-8a50-e15e04bc1f70,ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning,0.977107,"This paper assesses the accuracy, reliability and bias of the Large Language
Model (LLM) ChatGPT-4 on the text analysis task of classifying the political
affiliation of a Twitter poster based on the content of a tweet. The LLM is
compared to manual annotation by both expert classifiers and crowd workers,
generally considered the gold standard for such tasks. We use Twitter messages
from United States politicians during the 2020 election, providing a ground
truth against which to measure accuracy. The paper finds that ChatGPT-4 has
achieves higher accuracy, higher reliability, and equal or lower bias than the
human classifiers. The LLM is able to correctly annotate messages that require
reasoning on the basis of contextual knowledge, and inferences around the
author's intentions - traditionally seen as uniquely human abilities. These
findings suggest that LLM will have substantial impact on the use of textual
data in the social sciences, by enabling interpretive research at a scale.",None,-1
6fc246ee-d5b8-4b54-9c38-8cc7b13165bf,IC3: Image Captioning by Committee Consensus,0.335542,"If you ask a human to describe an image, they might do so in a thousand
different ways. Traditionally, image captioning models are trained to generate
a single ""best"" (most like a reference) image caption. Unfortunately, doing so
encourages captions that are ""informationally impoverished,"" and focus on only
a subset of the possible details, while ignoring other potentially useful
information in the scene. In this work, we introduce a simple, yet novel,
method: ""Image Captioning by Committee Consensus"" (IC3), designed to generate a
single caption that captures high-level details from several annotator
viewpoints. Humans rate captions produced by IC3 at least as helpful as
baseline SOTA models more than two thirds of the time, and IC3 can improve the
performance of SOTA automated recall systems by up to 84%, outperforming single
human-generated reference captions, and indicating significant improvements
over SOTA approaches for visual description. Code is available at
https://davidmchan.github.io/caption-by-committee/",None,-1
4bb6b91b-5a8a-4608-ba3d-af435f69459b,Transformer and Snowball Graph Convolution Learning for Brain functional network Classification,0.111274,"Advanced deep learning methods, especially graph neural networks (GNNs), are
increasingly expected to learn from brain functional network data and predict
brain disorders. In this paper, we proposed a novel Transformer and snowball
encoding networks (TSEN) for brain functional network classification, which
introduced Transformer architecture with graph snowball connection into GNNs
for learning whole-graph representation. TSEN combined graph snowball
connection with graph Transformer by snowball encoding layers, which enhanced
the power to capture multi-scale information and global patterns of brain
functional networks. TSEN also introduced snowball graph convolution as
position embedding in Transformer structure, which was a simple yet effective
method for capturing local patterns naturally. We evaluated the proposed model
by two large-scale brain functional network datasets from autism spectrum
disorder and major depressive disorder respectively, and the results
demonstrated that TSEN outperformed the state-of-the-art GNN models and the
graph-transformer based GNN models.",None,-1
22e31256-f09c-44c9-a5b3-404081c6a402,Diversity is Strength: Mastering Football Full Game with Interactive Reinforcement Learning of Multiple AIs,0.0339019,"Training AI with strong and rich strategies in multi-agent environments
remains an important research topic in Deep Reinforcement Learning (DRL). The
AI's strength is closely related to its diversity of strategies, and this
relationship can guide us to train AI with both strong and rich strategies. To
prove this point, we propose Diversity is Strength (DIS), a novel DRL training
framework that can simultaneously train multiple kinds of AIs. These AIs are
linked through an interconnected history model pool structure, which enhances
their capabilities and strategy diversities. We also design a model evaluation
and screening scheme to select the best models to enrich the model pool and
obtain the final AI. The proposed training method provides diverse,
generalizable, and strong AI strategies without using human data. We tested our
method in an AI competition based on Google Research Football (GRF) and won the
5v5 and 11v11 tracks. The method enables a GRF AI to have a high level on both
5v5 and 11v11 tracks for the first time, which are under complex multi-agent
environments. The behavior analysis shows that the trained AI has rich
strategies, and the ablation experiments proved that the designed modules
benefit the training process.",None,-1
0844d75f-1d68-4013-9d2b-f7fd1aa83eba,Certified Robust Neural Networks: Generalization and Corruption Resistance,0.345281,"Recent work have demonstrated that robustness (to ""corruption"") can be at
odds with generalization. Adversarial training, for instance, aims to reduce
the problematic susceptibility of modern neural networks to small data
perturbations. Surprisingly, overfitting is a major concern in adversarial
training despite being mostly absent in standard training. We provide here
theoretical evidence for this peculiar ""robust overfitting"" phenomenon.
Subsequently, we advance a novel distributionally robust loss function bridging
robustness and generalization. We demonstrate both theoretically as well as
empirically the loss to enjoy a certified level of robustness against two
common types of corruption--data evasion and poisoning attacks--while ensuring
guaranteed generalization. We show through careful numerical experiments that
our resulting holistic robust (HR) training procedure yields SOTA performance.
Finally, we indicate that HR training can be interpreted as a direct extension
of adversarial training and comes with a negligible additional computational
burden. A ready-to-use python library implementing our algorithm is available
at https://github.com/RyanLucas3/HR_Neural_Networks.",None,-1
41eaa70f-bb8f-4ff4-8598-e4ef2eb27fb1,Test-Time Training on Nearest Neighbors for Large Language Models,0.0393618,"Many recent efforts augment language models with retrieval, by adding
retrieved data to the input context. For this approach to succeed, the
retrieved data must be added at both training and test time. Moreover, as input
length grows linearly with the size of retrieved data, cost in computation and
memory grows quadratically for modern Transformers. To avoid these
complications, we simply fine-tune the model on retrieved data at test time,
using its standard training setup. We build a large-scale distributed index
based on text embeddings of the Pile dataset. For each test input, our system
retrieves its neighbors and fine-tunes the model on their text. Surprisingly,
retrieving and training on as few as 20 neighbors, each for only one gradient
iteration, drastically improves performance across more than 20 language
modeling tasks in the Pile. For example, test-time training with nearest
neighbors significantly narrows the performance gap between a small GPT-2 and a
GPT-Neo model more than 10 times larger. Sufficient index quality and size,
however, are necessary. Our work establishes a first baseline of test-time
training for language modeling.",None,-1
086b3d0d-6aff-4027-8bf9-5d7266b922a9,"AutoML in The Wild: Obstacles, Workarounds, and Expectations",0.343838,"Automated machine learning (AutoML) is envisioned to make ML techniques
accessible to ordinary users. Recent work has investigated the role of humans
in enhancing AutoML functionality throughout a standard ML workflow. However,
it is also critical to understand how users adopt existing AutoML solutions in
complex, real-world settings from a holistic perspective. To fill this gap,
this study conducted semi-structured interviews of AutoML users (N=19) focusing
on understanding (1) the limitations of AutoML encountered by users in their
real-world practices, (2) the strategies users adopt to cope with such
limitations, and (3) how the limitations and workarounds impact their use of
AutoML. Our findings reveal that users actively exercise user agency to
overcome three major challenges arising from customizability, transparency, and
privacy. Furthermore, users make cautious decisions about whether and how to
apply AutoML on a case-by-case basis. Finally, we derive design implications
for developing future AutoML solutions.",None,-1
2d3becbb-ae71-4142-acc9-8e8d158e69c7,Professional Basketball Player Behavior Synthesis via Planning with Diffusion,0.917915,"Dynamically planning in multi-agent systems has been explored to improve
decision-making in various domains. Professional basketball serves as a
compelling example of a dynamic spatio-temporal game, encompassing both
concealed strategic policies and decision-making. However, processing the
diverse on-court signals and navigating the vast space of potential actions and
outcomes makes it difficult for existing approaches to swiftly identify optimal
strategies in response to evolving circumstances. In this study, we first
formulate the sequential decision-making process as a conditional trajectory
generation process. We further introduce PLAYBEST (PLAYer BEhavior SynThesis),
a method for enhancing player decision-making. We extend the state-of-the-art
generative model, diffusion probabilistic model, to learn challenging
multi-agent environmental dynamics from historical National Basketball
Association (NBA) player motion tracking data. To incorporate data-driven
strategies, an auxiliary value function is trained using the play-by-play data
with corresponding rewards acting as the plan guidance. To accomplish
reward-guided trajectory generation, conditional sampling is introduced to
condition the diffusion model on the value function and conduct
classifier-guided sampling. We validate the effectiveness of PLAYBEST via
comprehensive simulation studies from real-world data, contrasting the
generated trajectories and play strategies with those employed by professional
basketball teams. Our results reveal that the model excels at generating
high-quality basketball trajectories that yield efficient plays, surpassing
conventional planning techniques in terms of adaptability, flexibility, and
overall performance. Moreover, the synthesized play strategies exhibit a
remarkable alignment with professional tactics, highlighting the model's
capacity to capture the intricate dynamics of basketball games.",None,-1
839a269c-9dde-4d46-93f0-a47e1800433c,Exploiting the Textual Potential from Vision-Language Pre-training for Text-based Person Search,0.733832,"Text-based Person Search (TPS), is targeted on retrieving pedestrians to
match text descriptions instead of query images. Recent Vision-Language
Pre-training (VLP) models can bring transferable knowledge to downstream TPS
tasks, resulting in more efficient performance gains. However, existing TPS
methods improved by VLP only utilize pre-trained visual encoders, neglecting
the corresponding textual representation and breaking the significant modality
alignment learned from large-scale pre-training. In this paper, we explore the
full utilization of textual potential from VLP in TPS tasks. We build on the
proposed VLP-TPS baseline model, which is the first TPS model with both
pre-trained modalities. We propose the Multi-Integrity Description Constraints
(MIDC) to enhance the robustness of the textual modality by incorporating
different components of fine-grained corpus during training. Inspired by the
prompt approach for zero-shot classification with VLP models, we propose the
Dynamic Attribute Prompt (DAP) to provide a unified corpus of fine-grained
attributes as language hints for the image modality. Extensive experiments show
that our proposed TPS framework achieves state-of-the-art performance,
exceeding the previous best method by a margin.",None,-1
42e2b5b7-16de-4ab6-a4ad-745c249ac594,Principles of Forgetting in Domain-Incremental Semantic Segmentation in Adverse Weather Conditions,0.300912,"Deep neural networks for scene perception in automated vehicles achieve
excellent results for the domains they were trained on. However, in real-world
conditions, the domain of operation and its underlying data distribution are
subject to change. Adverse weather conditions, in particular, can significantly
decrease model performance when such data are not available during
training.Additionally, when a model is incrementally adapted to a new domain,
it suffers from catastrophic forgetting, causing a significant drop in
performance on previously observed domains. Despite recent progress in reducing
catastrophic forgetting, its causes and effects remain obscure. Therefore, we
study how the representations of semantic segmentation models are affected
during domain-incremental learning in adverse weather conditions. Our
experiments and representational analyses indicate that catastrophic forgetting
is primarily caused by changes to low-level features in domain-incremental
learning and that learning more general features on the source domain using
pre-training and image augmentations leads to efficient feature reuse in
subsequent tasks, which drastically reduces catastrophic forgetting. These
findings highlight the importance of methods that facilitate generalized
features for effective continual learning algorithms.",None,-1
c541c37d-008e-4a6f-bcd6-8b9012181b8e,Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse,0.503415,"Online violence against children has increased globally recently, demanding
urgent attention. Competent authorities manually analyze abuse complaints to
comprehend crime dynamics and identify patterns. However, the manual analysis
of these complaints presents a challenge because it exposes analysts to harmful
content during the review process. Given these challenges, we present a novel
solution, an automated tool designed to analyze children's sexual abuse reports
comprehensively. By automating the analysis process, our tool significantly
reduces the risk of exposure to harmful content by categorizing the reports on
three dimensions: Subject, Degree of Criminality, and Damage. Furthermore,
leveraging our multidisciplinary team's expertise, we introduce a novel
approach to annotate the collected data, enabling a more in-depth analysis of
the reports. This approach improves the comprehension of fundamental patterns
and trends, enabling law enforcement agencies and policymakers to create
focused strategies in the fight against children's violence.",None,-1
739143d1-6257-47e7-8ca6-934ac907dc9a,Meta-Tuning LLMs to Leverage Lexical Knowledge for Generalizable Language Style Understanding,0.219173,"Language style is often used by writers to convey their intentions,
identities, and mastery of language. In this paper, we show that current large
language models struggle to capture some language styles without fine-tuning.
To address this challenge, we investigate whether LLMs can be meta-trained
based on representative lexicons to recognize new styles they have not been
fine-tuned on. Experiments on 13 established style classification tasks, as
well as 63 novel tasks generated using LLMs, demonstrate that meta-training
with style lexicons consistently improves zero-shot transfer across styles. We
release the code and data at http://github.com/octaviaguo/Style-LLM .",None,-1
a1977b7c-6b9a-4731-aab1-382fff8236de,Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering,0.0873898,"Few-shot learning for open domain multi-hop question answering typically
relies on the incontext learning capability of large language models (LLMs).
While powerful, these LLMs usually contain tens or hundreds of billions of
parameters, making them rather inefficient at inference time. To improve
performance of smaller language models, we propose a data synthesis framework
for multi-hop question answering that requires less than 10 human annotated
question answer pairs. Our framework depends only on rich, naturally-occurring
relationships among documents and is built upon the data generation functions
parameterized by LLMs and prompts. We synthesize millions of multi-hop
questions and claims to finetune language models, evaluated on popular
benchmarks for multi-hop question answering and fact verification. Empirically,
our approach improves model performance significantly, allowing the finetuned
models to be competitive with GPT-3.5 based approaches while being almost
one-third the size in parameter count.",None,-1
674fd692-d933-47da-897b-35d099193013,Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models,0.706474,"The mission of open knowledge graph (KG) completion is to draw new findings
from known facts. Existing works that augment KG completion require either (1)
factual triples to enlarge the graph reasoning space or (2) manually designed
prompts to extract knowledge from a pre-trained language model (PLM),
exhibiting limited performance and requiring expensive efforts from experts. To
this end, we propose TAGREAL that automatically generates quality query prompts
and retrieves support information from large text corpora to probe knowledge
from PLM for KG completion. The results show that TAGREAL achieves
state-of-the-art performance on two benchmark datasets. We find that TAGREAL
has superb performance even with limited training data, outperforming existing
embedding-based, graph-based, and PLM-based methods.",None,-1
1d9fe077-4ccd-4bbc-8b51-5113854ebdbe,Revisiting Supertagging for HPSG,0.864665,"We present new supertaggers trained on HPSG-based treebanks. These treebanks
feature high-quality annotation based on a well-developed linguistic theory and
include diverse and challenging test datasets, beyond the usual WSJ section 23
and Wikipedia data. HPSG supertagging has previously relied on MaxEnt-based
models. We use SVM and neural CRF- and BERT-based methods and show that both
SVM and neural supertaggers achieve considerably higher accuracy compared to
the baseline. Our fine-tuned BERT-based tagger achieves 97.26% accuracy on 1000
sentences from WSJ23 and 93.88% on the completely out-of-domain The Cathedral
and the Bazaar (cb)). We conclude that it therefore makes sense to integrate
these new supertaggers into modern HPSG parsers, and we also hope that the
diverse and difficult datasets we used here will gain more popularity in the
field. We contribute the complete dataset reformatted for token classification.",None,-1
c5551d51-b075-4835-aca2-568aa525b859,Creating Knowledge Graphs for Geographic Data on the Web,0.117036,"Geographic data plays an essential role in various Web, Semantic Web and
machine learning applications. OpenStreetMap and knowledge graphs are critical
complementary sources of geographic data on the Web. However, data veracity,
the lack of integration of geographic and semantic characteristics, and
incomplete representations substantially limit the data utility. Verification,
enrichment and semantic representation are essential for making geographic data
accessible for the Semantic Web and machine learning. This article describes
recent approaches we developed to tackle these challenges.",None,-1
daf17e00-a7dc-4db4-a39c-60032c5a47c6,DANES: Deep Neural Network Ensemble Architecture for Social and Textual Context-aware Fake News Detection,0.567826,"The growing popularity of social media platforms has simplified the creation
and distribution of news articles but also creates a conduit for spreading fake
news. In consequence, the need arises for effective context-aware fake news
detection mechanisms, where the contextual information can be built either from
the textual content of posts or from available social data (e.g., information
about the users, reactions to posts, or the social network). In this paper, we
propose DANES, a Deep Neural Network Ensemble Architecture for Social and
Textual Context-aware Fake News Detection. DANES comprises a Text Branch for a
textual content-based context and a Social Branch for the social context. These
two branches are used to create a novel Network Embedding. Preliminary ablation
results on 3 real-world datasets, i.e., BuzzFace, Twitter15, and Twitter16, are
promising, with an accuracy that outperforms state-of-the-art solutions when
employing both social and textual content features.",None,-1
be8d77d4-ad4d-4c02-940f-e9d9ad675a9a,Computer Vision for Construction Progress Monitoring: A Real-Time Object Detection Approach,0.442304,"Construction progress monitoring (CPM) is essential for effective project
management, ensuring on-time and on-budget delivery. Traditional CPM methods
often rely on manual inspection and reporting, which are time-consuming and
prone to errors. This paper proposes a novel approach for automated CPM using
state-of-the-art object detection algorithms. The proposed method leverages
e.g. YOLOv8's real-time capabilities and high accuracy to identify and track
construction elements within site images and videos. A dataset was created,
consisting of various building elements and annotated with relevant objects for
training and validation. The performance of the proposed approach was evaluated
using standard metrics, such as precision, recall, and F1-score, demonstrating
significant improvement over existing methods. The integration of Computer
Vision into CPM provides stakeholders with reliable, efficient, and
cost-effective means to monitor project progress, facilitating timely
decision-making and ultimately contributing to the successful completion of
construction projects.",None,-1
6989a9d6-a0a7-42a7-acb8-e6fbe9411fb6,Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm,0.619823,"Contextual biasing refers to the problem of biasing the automatic speech
recognition (ASR) systems towards rare entities that are relevant to the
specific user or application scenarios. We propose algorithms for contextual
biasing based on the Knuth-Morris-Pratt algorithm for pattern matching. During
beam search, we boost the score of a token extension if it extends matching
into a set of biasing phrases. Our method simulates the classical approaches
often implemented in the weighted finite state transducer (WFST) framework, but
avoids the FST language altogether, with careful considerations on memory
footprint and efficiency on tensor processing units (TPUs) by vectorization.
Without introducing additional model parameters, our method achieves
significant word error rate (WER) reductions on biasing test sets by itself,
and yields further performance gain when combined with a model-based biasing
method.",None,-1
6396e70b-7484-409a-9729-fc86b34a4a63,Unsupervised Domain Adaption with Pixel-level Discriminator for Image-aware Layout Generation,0.259061,"Layout is essential for graphic design and poster generation. Recently,
applying deep learning models to generate layouts has attracted increasing
attention. This paper focuses on using the GAN-based model conditioned on image
contents to generate advertising poster graphic layouts, which requires an
advertising poster layout dataset with paired product images and graphic
layouts. However, the paired images and layouts in the existing dataset are
collected by inpainting and annotating posters, respectively. There exists a
domain gap between inpainted posters (source domain data) and clean product
images (target domain data). Therefore, this paper combines unsupervised domain
adaption techniques to design a GAN with a novel pixel-level discriminator
(PD), called PDA-GAN, to generate graphic layouts according to image contents.
The PD is connected to the shallow level feature map and computes the GAN loss
for each input-image pixel. Both quantitative and qualitative evaluations
demonstrate that PDA-GAN can achieve state-of-the-art performances and generate
high-quality image-aware graphic layouts for advertising posters.",None,-1
ac08ffad-44f5-4109-af1f-d26b17007203,Deep-Learning Quantitative Structural Characterization in Additive Manufacturing,0.168002,"With a goal of accelerating fabrication of additively manufactured components
with precise microstructures, we developed a method for structural
characterization of key features in additively manufactured materials and
parts. The method utilizes deep learning based on an image-to-image translation
conditional Generative Adversarial Neural Network architecture and enables fast
and incrementally more accurate predictions of the prevalent geometric
features, including melt pool boundaries and printing induced defects visible
in etched optical images. These structural details are heterogeneous in nature.
Our method specifies the microstructure state of an additive built via
statistical distribution of structural details, based on an ensemble of
collected images. Extensions of the method are proposed to address Artificial
Intelligence implementation of developed machine learning model for in real
time control of additive manufacturing.",None,-1
3dcaaee7-b18c-423c-8c32-4477da693c9b,FaceLit: Neural 3D Relightable Faces,0.356079,"We propose a generative framework, FaceLit, capable of generating a 3D face
that can be rendered at various user-defined lighting conditions and views,
learned purely from 2D images in-the-wild without any manual annotation. Unlike
existing works that require careful capture setup or human labor, we rely on
off-the-shelf pose and illumination estimators. With these estimates, we
incorporate the Phong reflectance model in the neural volume rendering
framework. Our model learns to generate shape and material properties of a face
such that, when rendered according to the natural statistics of pose and
illumination, produces photorealistic face images with multiview 3D and
illumination consistency. Our method enables photorealistic generation of faces
with explicit illumination and view controls on multiple datasets - FFHQ,
MetFaces and CelebA-HQ. We show state-of-the-art photorealism among 3D aware
GANs on FFHQ dataset achieving an FID score of 3.5.",None,-1
89a8ff85-7eab-49cd-a355-a31cca770290,Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration,0.439626,"Recent text-to-image generation models have shown promising results in
generating high-fidelity photo-realistic images. In parallel, the problem of
data scarcity has brought a growing interest in employing AIGC technology for
high-quality data expansion. However, this paradigm requires well-designed
prompt engineering that cost-less data expansion and labeling remain
under-explored. Inspired by LLM's powerful capability in task guidance, we
propose a new paradigm of annotated data expansion named as ChatGenImage. The
core idea behind it is to leverage the complementary strengths of diverse
models to establish a highly effective and user-friendly pipeline for
interactive data augmentation. In this work, we extensively study how LLMs
communicate with AIGC model to achieve more controllable image generation and
make the first attempt to collaborate them for automatic data augmentation for
a variety of downstream tasks. Finally, we present fascinating results obtained
from our ChatGenImage framework and demonstrate the powerful potential of our
synthetic data for systematic vision adaptation. Our codes are available at
https://github.com/Yuqifan1117/Labal-Anything-Pipeline.",None,-1
3cafdfdc-8d76-492d-8507-dccb1ed59437,Better Sampling of Negatives for Distantly Supervised Named Entity Recognition,0.21054,"Distantly supervised named entity recognition (DS-NER) has been proposed to
exploit the automatically labeled training data instead of human annotations.
The distantly annotated datasets are often noisy and contain a considerable
number of false negatives. The recent approach uses a weighted sampling
approach to select a subset of negative samples for training. However, it
requires a good classifier to assign weights to the negative samples. In this
paper, we propose a simple and straightforward approach for selecting the top
negative samples that have high similarities with all the positive samples for
training. Our method achieves consistent performance improvements on four
distantly supervised NER datasets. Our analysis also shows that it is critical
to differentiate the true negatives from the false negatives.",None,-1
48f2fc4c-acef-4cdc-b3db-c65584eb9a29,Uncovering the Unseen: Discover Hidden Intentions by Micro-Behavior Graph Reasoning,0.199302,"This paper introduces a new and challenging Hidden Intention Discovery (HID)
task. Unlike existing intention recognition tasks, which are based on obvious
visual representations to identify common intentions for normal behavior, HID
focuses on discovering hidden intentions when humans try to hide their
intentions for abnormal behavior. HID presents a unique challenge in that
hidden intentions lack the obvious visual representations to distinguish them
from normal intentions. Fortunately, from a sociological and psychological
perspective, we find that the difference between hidden and normal intentions
can be reasoned from multiple micro-behaviors, such as gaze, attention, and
facial expressions. Therefore, we first discover the relationship between
micro-behavior and hidden intentions and use graph structure to reason about
hidden intentions. To facilitate research in the field of HID, we also
constructed a seminal dataset containing a hidden intention annotation of a
typical theft scenario for HID. Extensive experiments show that the proposed
network improves performance on the HID task by 9.9\% over the state-of-the-art
method SBP.",None,-1
27a4cdbe-96f3-4b43-8720-923edbbeb946,"DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering",0.649429,"Uncalibrated photometric stereo (UPS) is challenging due to the inherent
ambiguity brought by the unknown light. Although the ambiguity is alleviated on
non-Lambertian objects, the problem is still difficult to solve for more
general objects with complex shapes introducing irregular shadows and general
materials with complex reflectance like anisotropic reflectance. To exploit
cues from shadow and reflectance to solve UPS and improve performance on
general materials, we propose DANI-Net, an inverse rendering framework with
differentiable shadow handling and anisotropic reflectance modeling. Unlike
most previous methods that use non-differentiable shadow maps and assume
isotropic material, our network benefits from cues of shadow and anisotropic
reflectance through two differentiable paths. Experiments on multiple
real-world datasets demonstrate our superior and robust performance.",None,-1
19ba7d4a-0618-4b4a-909c-2e7ba84201e9,Procedural content generation of puzzle games using conditional generative adversarial networks,0.31433,"In this article, we present an experimental approach to using parameterized
Generative Adversarial Networks (GANs) to produce levels for the puzzle game
Lily's Garden. We extract two condition vectors from the real levels in an
effort to control the details of the GAN's outputs. While the GANs perform well
in approximating the first condition (map shape), they struggle to approximate
the second condition (piece distribution). We hypothesize that this might be
improved by trying out alternative architectures for both the Generator and
Discriminator of the GANs.",None,-1
b74ccc49-0c08-4448-9df1-587a3f55c740,Adaptive Sparse Pairwise Loss for Object Re-Identification,0.874729,"Object re-identification (ReID) aims to find instances with the same identity
as the given probe from a large gallery. Pairwise losses play an important role
in training a strong ReID network. Existing pairwise losses densely exploit
each instance as an anchor and sample its triplets in a mini-batch. This dense
sampling mechanism inevitably introduces positive pairs that share few visual
similarities, which can be harmful to the training. To address this problem, we
propose a novel loss paradigm termed Sparse Pairwise (SP) loss that only
leverages few appropriate pairs for each class in a mini-batch, and empirically
demonstrate that it is sufficient for the ReID tasks. Based on the proposed
loss framework, we propose an adaptive positive mining strategy that can
dynamically adapt to diverse intra-class variations. Extensive experiments show
that SP loss and its adaptive variant AdaSP loss outperform other pairwise
losses, and achieve state-of-the-art performance across several ReID
benchmarks. Code is available at https://github.com/Astaxanthin/AdaSP.",None,-1
e359c51d-c515-462c-9fa0-0d3e867573f3,Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports,0.907538,"With the increasing number of clinical trial reports generated every day, it
is becoming hard to keep up with novel discoveries that inform evidence-based
healthcare recommendations. To help automate this process and assist medical
experts, NLP solutions are being developed. This motivated the SemEval-2023
Task 7, where the goal was to develop an NLP system for two tasks: evidence
retrieval and natural language inference from clinical trial data. In this
paper, we describe our two developed systems. The first one is a pipeline
system that models the two tasks separately, while the second one is a joint
system that learns the two tasks simultaneously with a shared representation
and a multi-task learning approach. The final system combines their outputs in
an ensemble system. We formalize the models, present their characteristics and
challenges, and provide an analysis of achieved results. Our system ranked 3rd
out of 40 participants with a final submission.",None,-1
2056718d-5f58-46a6-ba60-4bc0490edd04,Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?,0.917292,"Making moral judgments is an essential step toward developing ethical AI
systems. Prevalent approaches are mostly implemented in a bottom-up manner,
which uses a large set of annotated data to train models based on crowd-sourced
opinions about morality. These approaches have been criticized for
overgeneralizing the moral stances of a limited group of annotators and lacking
explainability. This work proposes a flexible top-down framework to steer
(Large) Language Models (LMs) to perform moral reasoning with well-established
moral theories from interdisciplinary research. The theory-guided top-down
framework can incorporate various moral theories. Our experiments demonstrate
the effectiveness of the proposed framework on datasets derived from moral
theories. Furthermore, we show the alignment between different moral theories
and existing morality datasets. Our analysis exhibits the potential and flaws
in existing resources (models and datasets) in developing explainable moral
judgment-making systems.",None,-1
053c770a-b3df-43f2-b48d-bc00a5967dc6,Mixture-of-Expert Conformer for Streaming Multilingual ASR,0.663956,"End-to-end models with large capacity have significantly improved
multilingual automatic speech recognition, but their computation cost poses
challenges for on-device applications. We propose a streaming truly
multilingual Conformer incorporating mixture-of-expert (MoE) layers that learn
to only activate a subset of parameters in training and inference. The MoE
layer consists of a softmax gate which chooses the best two experts among many
in forward propagation. The proposed MoE layer offers efficient inference by
activating a fixed number of parameters as the number of experts increases. We
evaluate the proposed model on a set of 12 languages, and achieve an average
11.9% relative improvement in WER over the baseline. Compared to an adapter
model using ground truth information, our MoE model achieves similar WER and
activates similar number of parameters but without any language information. We
further show around 3% relative WER improvement by multilingual shallow fusion.",None,-1
ff684689-42f3-4ac8-8977-40d9967a2b6b,LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?,0.230168,"Large language models (LLMs) have exhibited impressive capabilities in
comprehending complex instructions. However, their blind adherence to provided
instructions has led to concerns regarding risks of malicious use. Existing
defence mechanisms, such as model fine-tuning or output censorship using LLMs,
have proven to be fallible, as LLMs can still generate problematic responses.
Commonly employed censorship approaches treat the issue as a machine learning
problem and rely on another LM to detect undesirable content in LLM outputs. In
this paper, we present the theoretical limitations of such semantic censorship
approaches. Specifically, we demonstrate that semantic censorship can be
perceived as an undecidable problem, highlighting the inherent challenges in
censorship that arise due to LLMs' programmatic and instruction-following
capabilities. Furthermore, we argue that the challenges extend beyond semantic
censorship, as knowledgeable attackers can reconstruct impermissible outputs
from a collection of permissible ones. As a result, we propose that the problem
of censorship needs to be reevaluated; it should be treated as a security
problem which warrants the adaptation of security-based approaches to mitigate
potential risks.",None,-1
4cf676c3-4498-41dd-a4e4-0ec670595c71,Exploring the Benefits of Visual Prompting in Differential Privacy,0.755532,"Visual Prompting (VP) is an emerging and powerful technique that allows
sample-efficient adaptation to downstream tasks by engineering a well-trained
frozen source model. In this work, we explore the benefits of VP in
constructing compelling neural network classifiers with differential privacy
(DP). We explore and integrate VP into canonical DP training methods and
demonstrate its simplicity and efficiency. In particular, we discover that VP
in tandem with PATE, a state-of-the-art DP training method that leverages the
knowledge transfer from an ensemble of teachers, achieves the state-of-the-art
privacy-utility trade-off with minimum expenditure of privacy budget. Moreover,
we conduct additional experiments on cross-domain image classification with a
sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we
also conduct extensive ablation studies to validate the effectiveness and
contribution of VP under DP consideration. Our code is available at
(https://github.com/EzzzLi/Prompt-PATE).",None,-1
32e59720-9e58-4e2d-9d41-8ea6bc47c0b0,Leveraging AI for Natural Disaster Management : Takeaways From The Moroccan Earthquake,0.34896,"The devastating 6.8-magnitude earthquake in Al Haouz, Morocco in 2023
prompted critical reflections on global disaster management strategies,
resulting in a post-disaster hackathon, using artificial intelligence (AI) to
improve disaster preparedness, response, and recovery. This paper provides (i)
a comprehensive literature review, (ii) an overview of winning projects, (iii)
key insights and challenges, namely real-time open-source data, data scarcity,
and interdisciplinary collaboration barriers, and (iv) a community-call for
further action.",None,-1
f41838cf-0e70-4b11-8d73-7ebfcb7047f5,CLIP-GCD: Simple Language Guided Generalized Category Discovery,0.509399,"Generalized Category Discovery (GCD) requires a model to both classify known
categories and cluster unknown categories in unlabeled data. Prior methods
leveraged self-supervised pre-training combined with supervised fine-tuning on
the labeled data, followed by simple clustering methods. In this paper, we
posit that such methods are still prone to poor performance on
out-of-distribution categories, and do not leverage a key ingredient: Semantic
relationships between object categories. We therefore propose to leverage
multi-modal (vision and language) models, in two complementary ways. First, we
establish a strong baseline by replacing uni-modal features with CLIP, inspired
by its zero-shot performance. Second, we propose a novel retrieval-based
mechanism that leverages CLIP's aligned vision-language representations by
mining text descriptions from a text corpus for the labeled and unlabeled set.
We specifically use the alignment between CLIP's visual encoding of the image
and textual encoding of the corpus to retrieve top-k relevant pieces of text
and incorporate their embeddings to perform joint image+text semi-supervised
clustering. We perform rigorous experimentation and ablations (including on
where to retrieve from, how much to retrieve, and how to combine information),
and validate our results on several datasets including out-of-distribution
domains, demonstrating state-of-art results.",None,-1
1c70d129-b3c4-4ac6-baba-e9724dc93464,Hallucinated Heartbeats: Anomaly-Aware Remote Pulse Estimation,0.110916,"Camera-based physiological monitoring, especially remote photoplethysmography
(rPPG), is a promising tool for health diagnostics, and state-of-the-art pulse
estimators have shown impressive performance on benchmark datasets. We argue
that evaluations of modern solutions may be incomplete, as we uncover failure
cases for videos without a live person, or in the presence of severe noise. We
demonstrate that spatiotemporal deep learning models trained only with live
samples ""hallucinate"" a genuine-shaped pulse on anomalous and noisy videos,
which may have negative consequences when rPPG models are used by medical
personnel. To address this, we offer: (a) An anomaly detection model, built on
top of the predicted waveforms. We compare models trained in open-set (unknown
abnormal predictions) and closed-set (abnormal predictions known when training)
settings; (b) An anomaly-aware training regime that penalizes the model for
predicting periodic signals from anomalous videos. Extensive experimentation
with eight research datasets (rPPG-specific: DDPM, CDDPM, PURE, UBFC, ARPM;
deep fakes: DFDC; face presentation attack detection: HKBU-MARs; rPPG outlier:
KITTI) show better accuracy of anomaly detection for deep learning models
incorporating the proposed training (75.8%), compared to models trained
regularly (73.7%) and to hand-crafted rPPG methods (52-62%).",None,-1
3edcfb06-3927-4b02-ad2f-1ca02f30651e,Efficient Bayesian Computational Imaging with a Surrogate Score-Based Prior,0.524051,"We propose a surrogate function for efficient use of score-based priors for
Bayesian inverse imaging. Recent work turned score-based diffusion models into
probabilistic priors for solving ill-posed imaging problems by appealing to an
ODE-based log-probability function. However, evaluating this function is
computationally inefficient and inhibits posterior estimation of
high-dimensional images. Our proposed surrogate prior is based on the evidence
lower-bound of a score-based diffusion model. We demonstrate the surrogate
prior on variational inference for efficient approximate posterior sampling of
large images. Compared to the exact prior in previous work, our surrogate prior
accelerates optimization of the variational image distribution by at least two
orders of magnitude. We also find that our principled approach achieves
higher-fidelity images than non-Bayesian baselines that involve
hyperparameter-tuning at inference. Our work establishes a practical path
forward for using score-based diffusion models as general-purpose priors for
imaging.",None,-1
d926dc8b-2bc8-4a1c-a158-3f8b35b50109,Information-Theoretic GAN Compression with Variational Energy-based Model,0.135033,"We propose an information-theoretic knowledge distillation approach for the
compression of generative adversarial networks, which aims to maximize the
mutual information between teacher and student networks via a variational
optimization based on an energy-based model. Because the direct computation of
the mutual information in continuous domains is intractable, our approach
alternatively optimizes the student network by maximizing the variational lower
bound of the mutual information. To achieve a tight lower bound, we introduce
an energy-based model relying on a deep neural network to represent a flexible
variational distribution that deals with high-dimensional images and consider
spatial dependencies between pixels, effectively. Since the proposed method is
a generic optimization algorithm, it can be conveniently incorporated into
arbitrary generative adversarial networks and even dense prediction networks,
e.g., image enhancement models. We demonstrate that the proposed algorithm
achieves outstanding performance in model compression of generative adversarial
networks consistently when combined with several existing models.",None,-1
890d503c-d843-4ab2-b9ca-86e85010d9f0,TUTORING: Instruction-Grounded Conversational Agent for Language Learners,0.0620104,"In this paper, we propose Tutoring bot, a generative chatbot trained on a
large scale of tutor-student conversations for English-language learning. To
mimic a human tutor's behavior in language education, the tutor bot leverages
diverse educational instructions and grounds to each instruction as additional
input context for the tutor response generation. As a single instruction
generally involves multiple dialogue turns to give the student sufficient
speaking practice, the tutor bot is required to monitor and capture when the
current instruction should be kept or switched to the next instruction. For
that, the tutor bot is learned to not only generate responses but also infer
its teaching action and progress on the current conversation simultaneously by
a multi-task learning scheme. Our Tutoring bot is deployed under a
non-commercial use license at https://tutoringai.com.",None,-1
88de5df8-3d8b-4b39-93d8-8b4e86d92419,"Explaining Groups of Instances Counterfactually for XAI: A Use Case, Algorithm and User Study for Group-Counterfactuals",0.428229,"Counterfactual explanations are an increasingly popular form of post hoc
explanation due to their (i) applicability across problem domains, (ii)
proposed legal compliance (e.g., with GDPR), and (iii) reliance on the
contrastive nature of human explanation. Although counterfactual explanations
are normally used to explain individual predictive-instances, we explore a
novel use case in which groups of similar instances are explained in a
collective fashion using ``group counterfactuals'' (e.g., to highlight a
repeating pattern of illness in a group of patients). These group
counterfactuals meet a human preference for coherent, broad explanations
covering multiple events/instances. A novel, group-counterfactual algorithm is
proposed to generate high-coverage explanations that are faithful to the
to-be-explained model. This explanation strategy is also evaluated in a large,
controlled user study (N=207), using objective (i.e., accuracy) and subjective
(i.e., confidence, explanation satisfaction, and trust) psychological measures.
The results show that group counterfactuals elicit modest but definite
improvements in people's understanding of an AI system. The implications of
these findings for counterfactual methods and for XAI are discussed.",None,-1
593308dd-42ed-4f04-b507-f902b17482c8,Quantifying the Dialect Gap and its Correlates Across Languages,0.456921,"Historically, researchers and consumers have noticed a decrease in quality
when applying NLP tools to minority variants of languages (i.e. Puerto Rican
Spanish or Swiss German), but studies exploring this have been limited to a
select few languages. Additionally, past studies have mainly been conducted in
a monolingual context, so cross-linguistic trends have not been identified and
tied to external factors. In this work, we conduct a comprehensive evaluation
of the most influential, state-of-the-art large language models (LLMs) across
two high-use applications, machine translation and automatic speech
recognition, to assess their functionality on the regional dialects of several
high- and low-resource languages. Additionally, we analyze how the regional
dialect gap is correlated with economic, social, and linguistic factors. The
impact of training data, including related factors like dataset size and its
construction procedure, is shown to be significant but not consistent across
models or languages, meaning a one-size-fits-all approach cannot be taken in
solving the dialect gap. This work will lay the foundation for furthering the
field of dialectal NLP by laying out evident disparities and identifying
possible pathways for addressing them through mindful data collection.",None,-1
e1e56022-18f9-4013-aa01-2eb523235857,A Study on Knowledge Distillation from Weak Teacher for Scaling Up Pre-trained Language Models,0.205354,"Distillation from Weak Teacher (DWT) is a method of transferring knowledge
from a smaller, weaker teacher model to a larger student model to improve its
performance. Previous studies have shown that DWT can be effective in the
vision domain and natural language processing (NLP) pre-training stage.
Specifically, DWT shows promise in practical scenarios, such as enhancing new
generation or larger models using pre-trained yet older or smaller models and
lacking a resource budget. However, the optimal conditions for using DWT have
yet to be fully investigated in NLP pre-training. Therefore, this study
examines three key factors to optimize DWT, distinct from those used in the
vision domain or traditional knowledge distillation. These factors are: (i) the
impact of teacher model quality on DWT effectiveness, (ii) guidelines for
adjusting the weighting value for DWT loss, and (iii) the impact of parameter
remapping as a student model initialization technique for DWT.",None,-1
4855f299-8226-4b42-810c-81b4b7a63843,Mitigating Test-Time Bias for Fair Image Retrieval,0.556688,"We address the challenge of generating fair and unbiased image retrieval
results given neutral textual queries (with no explicit gender or race
connotations), while maintaining the utility (performance) of the underlying
vision-language (VL) model. Previous methods aim to disentangle learned
representations of images and text queries from gender and racial
characteristics. However, we show these are inadequate at alleviating bias for
the desired equal representation result, as there usually exists test-time bias
in the target retrieval set. So motivated, we introduce a straightforward
technique, Post-hoc Bias Mitigation (PBM), that post-processes the outputs from
the pre-trained vision-language model. We evaluate our algorithm on real-world
image search datasets, Occupation 1 and 2, as well as two large-scale
image-text datasets, MS-COCO and Flickr30k. Our approach achieves the lowest
bias, compared with various existing bias-mitigation methods, in text-based
image retrieval result while maintaining satisfactory retrieval performance.
The source code is publicly available at
\url{https://anonymous.4open.science/r/Fair_Text_based_Image_Retrieval-D8B2}.",None,-1
e0455d75-90f0-4986-909c-afb62efd74b6,Learning Transformations To Reduce the Geometric Shift in Object Detection,0.0562673,"The performance of modern object detectors drops when the test distribution
differs from the training one. Most of the methods that address this focus on
object appearance changes caused by, e.g., different illumination conditions,
or gaps between synthetic and real images. Here, by contrast, we tackle
geometric shifts emerging from variations in the image capture process, or due
to the constraints of the environment causing differences in the apparent
geometry of the content itself. We introduce a self-training approach that
learns a set of geometric transformations to minimize these shifts without
leveraging any labeled data in the new domain, nor any information about the
cameras. We evaluate our method on two different shifts, i.e., a camera's field
of view (FoV) change and a viewpoint change. Our results evidence that learning
geometric transformations helps detectors to perform better in the target
domains.",None,-1
734c23ec-7877-4ef3-89cf-15085f9a903e,HCGMNET: A Hierarchical Change Guiding Map Network For Change Detection,0.536631,"Very-high-resolution (VHR) remote sensing (RS) image change detection (CD)
has been a challenging task for its very rich spatial information and sample
imbalance problem. In this paper, we have proposed a hierarchical change
guiding map network (HCGMNet) for change detection. The model uses hierarchical
convolution operations to extract multiscale features, continuously merges
multi-scale features layer by layer to improve the expression of global and
local information, and guides the model to gradually refine edge features and
comprehensive performance by a change guide module (CGM), which is a
self-attention with changing guide map. Extensive experiments on two CD
datasets show that the proposed HCGMNet architecture achieves better CD
performance than existing state-of-the-art (SOTA) CD methods.",None,-1
1919af6a-a293-4920-ac54-53179843de6d,A Latent Space Theory for Emergent Abilities in Large Language Models,0.161444,"Languages are not created randomly but rather to communicate information.
There is a strong association between languages and their underlying meanings,
resulting in a sparse joint distribution that is heavily peaked according to
their correlations. Moreover, these peak values happen to match with the
marginal distribution of languages due to the sparsity. With the advent of LLMs
trained on big data and large models, we can now precisely assess the marginal
distribution of languages, providing a convenient means of exploring the sparse
structures in the joint distribution for effective inferences. In this paper,
we categorize languages as either unambiguous or {\epsilon}-ambiguous and
present quantitative results to demonstrate that the emergent abilities of
LLMs, such as language understanding, in-context learning, chain-of-thought
prompting, and effective instruction fine-tuning, can all be attributed to
Bayesian inference on the sparse joint distribution of languages.",None,-1
1e1e98d7-d5e7-4345-a33c-f20ca9021b4d,Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue,0.693345,"E-commerce pre-sales dialogue aims to understand and elicit user needs and
preferences for the items they are seeking so as to provide appropriate
recommendations. Conversational recommender systems (CRSs) learn user
representation and provide accurate recommendations based on dialogue context,
but rely on external knowledge. Large language models (LLMs) generate responses
that mimic pre-sales dialogues after fine-tuning, but lack domain-specific
knowledge for accurate recommendations. Intuitively, the strengths of LLM and
CRS in E-commerce pre-sales dialogues are complementary, yet no previous work
has explored this. This paper investigates the effectiveness of combining LLM
and CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:
CRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a
real-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of
two collaborative approaches with two CRSs and two LLMs on four tasks of
Ecommerce pre-sales dialogue. We find that collaborations between CRS and LLM
can be very effective in some cases.",None,-1
dcd92ed9-681c-4535-8e83-705352768e31,On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study,0.540388,"Modern deep models for summarization attains impressive benchmark
performance, but they are prone to generating miscalibrated predictive
uncertainty. This means that they assign high confidence to low-quality
predictions, leading to compromised reliability and trustworthiness in
real-world applications. Probabilistic deep learning methods are common
solutions to the miscalibration problem. However, their relative effectiveness
in complex autoregressive summarization tasks are not well-understood. In this
work, we thoroughly investigate different state-of-the-art probabilistic
methods' effectiveness in improving the uncertainty quality of the neural
summarization models, across three large-scale benchmarks with varying
difficulty. We show that the probabilistic methods consistently improve the
model's generation and uncertainty quality, leading to improved selective
generation performance (i.e., abstaining from low-quality summaries) in
practice. We also reveal notable failure patterns of probabilistic methods
widely-adopted in NLP community (e.g., Deep Ensemble and Monte Carlo Dropout),
cautioning the importance of choosing appropriate method for the data setting.",None,-1
7c50c7c0-a948-4bc4-84dd-3776c6836254,ControlVideo: Training-free Controllable Text-to-Video Generation,0.991939,"Text-driven diffusion models have unlocked unprecedented abilities in image
generation, whereas their video counterpart still lags behind due to the
excessive training cost of temporal modeling. Besides the training burden, the
generated videos also suffer from appearance inconsistency and structural
flickers, especially in long video synthesis. To address these challenges, we
design a \emph{training-free} framework called \textbf{ControlVideo} to enable
natural and efficient text-to-video generation. ControlVideo, adapted from
ControlNet, leverages coarsely structural consistency from input motion
sequences, and introduces three modules to improve video generation. Firstly,
to ensure appearance coherence between frames, ControlVideo adds fully
cross-frame interaction in self-attention modules. Secondly, to mitigate the
flicker effect, it introduces an interleaved-frame smoother that employs frame
interpolation on alternated frames. Finally, to produce long videos
efficiently, it utilizes a hierarchical sampler that separately synthesizes
each short clip with holistic coherency. Empowered with these modules,
ControlVideo outperforms the state-of-the-arts on extensive motion-prompt pairs
quantitatively and qualitatively. Notably, thanks to the efficient designs, it
generates both short and long videos within several minutes using one NVIDIA
2080Ti. Code is available at https://github.com/YBYBZhang/ControlVideo.",None,-1
be2ad7cd-150d-481b-9fd7-691c0c5e9ca3,Implicit View-Time Interpolation of Stereo Videos using Multi-Plane Disparities and Non-Uniform Coordinates,0.268903,"In this paper, we propose an approach for view-time interpolation of stereo
videos. Specifically, we build upon X-Fields that approximates an
interpolatable mapping between the input coordinates and 2D RGB images using a
convolutional decoder. Our main contribution is to analyze and identify the
sources of the problems with using X-Fields in our application and propose
novel techniques to overcome these challenges. Specifically, we observe that
X-Fields struggles to implicitly interpolate the disparities for large baseline
cameras. Therefore, we propose multi-plane disparities to reduce the spatial
distance of the objects in the stereo views. Moreover, we propose non-uniform
time coordinates to handle the non-linear and sudden motion spikes in videos.
We additionally introduce several simple, but important, improvements over
X-Fields. We demonstrate that our approach is able to produce better results
than the state of the art, while running in near real-time rates and having low
memory and storage costs.",None,-1
71f6648e-aee2-4747-9df8-df084cd50faf,Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning,0.227535,"Pre-trained Language Models are widely used in many important real-world
applications. However, recent studies show that these models can encode social
biases from large pre-training corpora and even amplify biases in downstream
applications. To address this challenge, we propose Co$^2$PT, an efficient and
effective debias-while-prompt tuning method for mitigating biases via
counterfactual contrastive prompt tuning on downstream tasks. Our experiments
conducted on three extrinsic bias benchmarks demonstrate the effectiveness of
Co$^2$PT on bias mitigation during the prompt tuning process and its
adaptability to existing upstream debiased language models. These findings
indicate the strength of Co$^2$PT and provide promising avenues for further
enhancement in bias mitigation on downstream tasks.",None,-1
b174a17f-8317-4e11-bdf1-58caa8ccd107,Revisiting Non-Autoregressive Translation at Scale,0.285918,"In real-world systems, scaling has been critical for improving the
translation quality in autoregressive translation (AT), which however has not
been well studied for non-autoregressive translation (NAT). In this work, we
bridge the gap by systematically studying the impact of scaling on NAT
behaviors. Extensive experiments on six WMT benchmarks over two advanced NAT
models show that scaling can alleviate the commonly-cited weaknesses of NAT
models, resulting in better translation performance. To reduce the side-effect
of scaling on decoding speed, we empirically investigate the impact of NAT
encoder and decoder on the translation performance. Experimental results on the
large-scale WMT20 En-De show that the asymmetric architecture (e.g. bigger
encoder and smaller decoder) can achieve comparable performance with the
scaling model, while maintaining the superiority of decoding speed with
standard NAT models. To this end, we establish a new benchmark by validating
scaled NAT models on the scaled dataset, which can be regarded as a strong
baseline for future works. We release code and system outputs at
https://github.com/DeepLearnXMU/Scaling4NAT.",None,-1
bf1e83bb-842b-4bf6-ba85-a85e46585d68,Heterogeneous Graph Convolutional Neural Network via Hodge-Laplacian for Brain Functional Data,0.611651,"This study proposes a novel heterogeneous graph convolutional neural network
(HGCNN) to handle complex brain fMRI data at regional and across-region levels.
We introduce a generic formulation of spectral filters on heterogeneous graphs
by introducing the $k-th$ Hodge-Laplacian (HL) operator. In particular, we
propose Laguerre polynomial approximations of HL spectral filters and prove
that their spatial localization on graphs is related to the polynomial order.
Furthermore, based on the bijection property of boundary operators on simplex
graphs, we introduce a generic topological graph pooling (TGPool) method that
can be used at any dimensional simplices. This study designs HL-node, HL-edge,
and HL-HGCNN neural networks to learn signal representation at a graph node,
edge levels, and both, respectively. Our experiments employ fMRI from the
Adolescent Brain Cognitive Development (ABCD; n=7693) to predict general
intelligence. Our results demonstrate the advantage of the HL-edge network over
the HL-node network when functional brain connectivity is considered as
features. The HL-HGCNN outperforms the state-of-the-art graph neural networks
(GNNs) approaches, such as GAT, BrainGNN, dGCN, BrainNetCNN, and Hypergraph NN.
The functional connectivity features learned from the HL-HGCNN are meaningful
in interpreting neural circuits related to general intelligence.",None,-1
ce5fe546-a684-4980-a5a2-1426ef5b08bb,Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization,0.458088,"Zero-shot skeleton-based action recognition aims to recognize actions of
unseen categories after training on data of seen categories. The key is to
build the connection between visual and semantic space from seen to unseen
classes. Previous studies have primarily focused on encoding sequences into a
singular feature vector, with subsequent mapping the features to an identical
anchor point within the embedded space. Their performance is hindered by 1) the
ignorance of the global visual/semantic distribution alignment, which results
in a limitation to capture the true interdependence between the two spaces. 2)
the negligence of temporal information since the frame-wise features with rich
action clues are directly pooled into a single feature vector. We propose a new
zero-shot skeleton-based action recognition method via mutual information (MI)
estimation and maximization. Specifically, 1) we maximize the MI between visual
and semantic space for distribution alignment; 2) we leverage the temporal
information for estimating the MI by encouraging MI to increase as more frames
are observed. Extensive experiments on three large-scale skeleton action
datasets confirm the effectiveness of our method. Code:
https://github.com/YujieOuO/SMIE.",None,-1
313a6f09-2ee5-4727-b52d-23aa0d009fe4,Why We Don't Have AGI Yet,0.0662221,"The original vision of AI was re-articulated in 2002 via the term 'Artificial
General Intelligence' or AGI. This vision is to build 'Thinking Machines' -
computer systems that can learn, reason, and solve problems similar to the way
humans do. This is in stark contrast to the 'Narrow AI' approach practiced by
almost everyone in the field over the many decades. While several large-scale
efforts have nominally been working on AGI (most notably DeepMind), the field
of pure focused AGI development has not been well funded or promoted. This is
surprising given the fantastic value that true AGI can bestow on humanity. In
addition to the dearth of effort in this field, there are also several
theoretical and methodical missteps that are hampering progress. We highlight
why purely statistical approaches are unlikely to lead to AGI, and identify
several crucial cognitive abilities required to achieve human-like adaptability
and autonomous learning. We conclude with a survey of socio-technical factors
that have undoubtedly slowed progress towards AGI.",None,-1
0f77135a-fcc6-44df-a348-fd3db1f18a79,Atmospheric Turbulence Correction via Variational Deep Diffusion,0.612773,"Atmospheric Turbulence (AT) correction is a challenging restoration task as
it consists of two distortions: geometric distortion and spatially variant
blur. Diffusion models have shown impressive accomplishments in photo-realistic
image synthesis and beyond. In this paper, we propose a novel deep conditional
diffusion model under a variational inference framework to solve the AT
correction problem. We use this framework to improve performance by learning
latent prior information from the input and degradation processes. We use the
learned information to further condition the diffusion model. Experiments are
conducted in a comprehensive synthetic AT dataset. We show that the proposed
framework achieves good quantitative and qualitative results.",None,-1
fb86d646-00c1-4142-8a5c-a2b0bc89ab4d,SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models,0.83607,"Interpreting remote sensing imagery enables numerous downstream applications
ranging from land-use planning to deforestation monitoring. Robustly
classifying this data is challenging due to the Earth's geographic diversity.
While many distinct satellite and aerial image classification datasets exist,
there is yet to be a benchmark curated that suitably covers this diversity. In
this work, we introduce SATellite ImageNet (SATIN), a metadataset curated from
27 existing remotely sensed datasets, and comprehensively evaluate the
zero-shot transfer classification capabilities of a broad range of
vision-language (VL) models on SATIN. We find SATIN to be a challenging
benchmark-the strongest method we evaluate achieves a classification accuracy
of 52.0%. We provide a $\href{https://satinbenchmark.github.io}{\text{public
leaderboard}}$ to guide and track the progress of VL models in this important
domain.",None,-1
53402105-b075-48db-aecd-fdae1b6a5e1a,A Global Model Approach to Robust Few-Shot SAR Automatic Target Recognition,0.875866,"In real-world scenarios, it may not always be possible to collect hundreds of
labeled samples per class for training deep learning-based SAR Automatic Target
Recognition (ATR) models. This work specifically tackles the few-shot SAR ATR
problem, where only a handful of labeled samples may be available to support
the task of interest. Our approach is composed of two stages. In the first, a
global representation model is trained via self-supervised learning on a large
pool of diverse and unlabeled SAR data. In the second stage, the global model
is used as a fixed feature extractor and a classifier is trained to partition
the feature space given the few-shot support samples, while simultaneously
being calibrated to detect anomalous inputs. Unlike competing approaches which
require a pristine labeled dataset for pretraining via meta-learning, our
approach learns highly transferable features from unlabeled data that have
little-to-no relation to the downstream task. We evaluate our method in
standard and extended MSTAR operating conditions and find it to achieve high
accuracy and robust out-of-distribution detection in many different few-shot
settings. Our results are particularly significant because they show the merit
of a global model approach to SAR ATR, which makes minimal assumptions, and
provides many axes for extendability.",None,-1
b756d342-742e-4766-af05-4bb121e39200,GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models,0.503615,"Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs' performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4's ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models' capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.",None,-1
956995a8-8daa-4241-895e-9a778b2a175d,Bird's-Eye-View Scene Graph for Vision-Language Navigation,0.306659,"Vision-language navigation (VLN), which entails an agent to navigate 3D
environments following human instructions, has shown great advances. However,
current agents are built upon panoramic observations, which hinders their
ability to perceive 3D scene geometry and easily leads to ambiguous selection
of panoramic view. To address these limitations, we present a BEV Scene Graph
(BSG), which leverages multi-step BEV representations to encode scene layouts
and geometric cues of indoor environment under the supervision of 3D detection.
During navigation, BSG builds a local BEV representation at each step and
maintains a BEV-based global scene map, which stores and organizes all the
online collected local BEV representations according to their topological
relations. Based on BSG, the agent predicts a local BEV grid-level decision
score and a global graph-level decision score, combined with a sub-view
selection score on panoramic views, for more accurate action prediction. Our
approach significantly outperforms state-of-the-art methods on REVERIE, R2R,
and R4R, showing the potential of BEV perception in VLN.",None,-1
383b7e35-7a1f-4b00-afa3-c4ca7ce4d155,Causal Structure Learning Supervised by Large Language Model,0.883309,"Causal discovery from observational data is pivotal for deciphering complex
relationships. Causal Structure Learning (CSL), which focuses on deriving
causal Directed Acyclic Graphs (DAGs) from data, faces challenges due to vast
DAG spaces and data sparsity. The integration of Large Language Models (LLMs),
recognized for their causal reasoning capabilities, offers a promising
direction to enhance CSL by infusing it with knowledge-based causal inferences.
However, existing approaches utilizing LLMs for CSL have encountered issues,
including unreliable constraints from imperfect LLM inferences and the
computational intensity of full pairwise variable analyses. In response, we
introduce the Iterative LLM Supervised CSL (ILS-CSL) framework. ILS-CSL
innovatively integrates LLM-based causal inference with CSL in an iterative
process, refining the causal DAG using feedback from LLMs. This method not only
utilizes LLM resources more efficiently but also generates more robust and
high-quality structural constraints compared to previous methodologies. Our
comprehensive evaluation across eight real-world datasets demonstrates
ILS-CSL's superior performance, setting a new standard in CSL efficacy and
showcasing its potential to significantly advance the field of causal
discovery. The codes are available at
\url{https://github.com/tyMadara/ILS-CSL}.",None,-1
90ee18cb-3ca2-45f6-a758-b38618fe5a6d,kNN-BOX: A Unified Framework for Nearest Neighbor Generation,0.358282,"Augmenting the base neural model with a token-level symbolic datastore is a
novel generation paradigm and has achieved promising results in machine
translation (MT). In this paper, we introduce a unified framework kNN-BOX,
which enables quick development and interactive analysis for this novel
paradigm. kNN-BOX decomposes the datastore-augmentation approach into three
modules: datastore, retriever and combiner, thus putting diverse kNN generation
methods into a unified way. Currently, kNN-BOX has provided implementation of
seven popular kNN-MT variants, covering research from performance enhancement
to efficiency optimization. It is easy for users to reproduce these existing
works or customize their own models. Besides, users can interact with their kNN
generation systems with kNN-BOX to better understand the underlying inference
process in a visualized way. In the experiment section, we apply kNN-BOX for
machine translation and three other seq2seq generation tasks, namely, text
simplification, paraphrase generation and question generation. Experiment
results show that augmenting the base neural model with kNN-BOX leads to a
large performance improvement in all these tasks. The code and document of
kNN-BOX is available at https://github.com/NJUNLP/knn-box.",None,-1
169d67e1-6796-4dab-a371-5b95dfbb9ede,Simplifying Complex Observation Models in Continuous POMDP Planning with Probabilistic Guarantees and Practice,0.91517,"Solving partially observable Markov decision processes (POMDPs) with high
dimensional and continuous observations, such as camera images, is required for
many real life robotics and planning problems. Recent researches suggested
machine learned probabilistic models as observation models, but their use is
currently too computationally expensive for online deployment. We deal with the
question of what would be the implication of using simplified observation
models for planning, while retaining formal guarantees on the quality of the
solution. Our main contribution is a novel probabilistic bound based on a
statistical total variation distance of the simplified model. We show that it
bounds the theoretical POMDP value w.r.t. original model, from the empirical
planned value with the simplified model, by generalizing recent results of
particle-belief MDP concentration bounds. Our calculations can be separated
into offline and online parts, and we arrive at formal guarantees without
having to access the costly model at all during planning, which is also a novel
result. Finally, we demonstrate in simulation how to integrate the bound into
the routine of an existing continuous online POMDP solver.",None,-1
75bb399d-e9c5-4153-a834-89df47f8c4f9,Advancing Adversarial Training by Injecting Booster Signal,0.0727469,"Recent works have demonstrated that deep neural networks (DNNs) are highly
vulnerable to adversarial attacks. To defend against adversarial attacks, many
defense strategies have been proposed, among which adversarial training has
been demonstrated to be the most effective strategy. However, it has been known
that adversarial training sometimes hurts natural accuracy. Then, many works
focus on optimizing model parameters to handle the problem. Different from the
previous approaches, in this paper, we propose a new approach to improve the
adversarial robustness by using an external signal rather than model
parameters. In the proposed method, a well-optimized universal external signal
called a booster signal is injected into the outside of the image which does
not overlap with the original content. Then, it boosts both adversarial
robustness and natural accuracy. The booster signal is optimized in parallel to
model parameters step by step collaboratively. Experimental results show that
the booster signal can improve both the natural and robust accuracies over the
recent state-of-the-art adversarial training methods. Also, optimizing the
booster signal is general and flexible enough to be adopted on any existing
adversarial training methods.",None,-1
7a31e619-3418-48cc-a82e-365918bb5d7d,Decentralized Monte Carlo Tree Search for Partially Observable Multi-agent Pathfinding,0.811397,"The Multi-Agent Pathfinding (MAPF) problem involves finding a set of
conflict-free paths for a group of agents confined to a graph. In typical MAPF
scenarios, the graph and the agents' starting and ending vertices are known
beforehand, allowing the use of centralized planning algorithms. However, in
this study, we focus on the decentralized MAPF setting, where the agents may
observe the other agents only locally and are restricted in communications with
each other. Specifically, we investigate the lifelong variant of MAPF, where
new goals are continually assigned to the agents upon completion of previous
ones. Drawing inspiration from the successful AlphaZero approach, we propose a
decentralized multi-agent Monte Carlo Tree Search (MCTS) method for MAPF tasks.
Our approach utilizes the agent's observations to recreate the intrinsic Markov
decision process, which is then used for planning with a tailored for
multi-agent tasks version of neural MCTS. The experimental results show that
our approach outperforms state-of-the-art learnable MAPF solvers. The source
code is available at https://github.com/AIRI-Institute/mats-lp.",None,-1
8d42270f-4733-46f8-b381-35634817842d,Controllable Guide-Space for Generalizable Face Forgery Detection,0.813225,"Recent studies on face forgery detection have shown satisfactory performance
for methods involved in training datasets, but are not ideal enough for unknown
domains. This motivates many works to improve the generalization, but
forgery-irrelevant information, such as image background and identity, still
exists in different domain features and causes unexpected clustering, limiting
the generalization. In this paper, we propose a controllable guide-space (GS)
method to enhance the discrimination of different forgery domains, so as to
increase the forgery relevance of features and thereby improve the
generalization. The well-designed guide-space can simultaneously achieve both
the proper separation of forgery domains and the large distance between
real-forgery domains in an explicit and controllable manner. Moreover, for
better discrimination, we use a decoupling module to weaken the interference of
forgery-irrelevant correlations between domains. Furthermore, we make
adjustments to the decision boundary manifold according to the clustering
degree of the same domain features within the neighborhood. Extensive
experiments in multiple in-domain and cross-domain settings confirm that our
method can achieve state-of-the-art generalization.",None,-1
ad00faf3-6ab6-4907-a81b-b8e5dbd61c64,Open-Source LLMs for Text Annotation: A Practical Guide for Model Setting and Fine-Tuning,0.949258,"This paper studies the performance of open-source Large Language Models
(LLMs) in text classification tasks typical for political science research. By
examining tasks like stance, topic, and relevance classification, we aim to
guide scholars in making informed decisions about their use of LLMs for text
analysis. Specifically, we conduct an assessment of both zero-shot and
fine-tuned LLMs across a range of text annotation tasks using news articles and
tweets datasets. Our analysis shows that fine-tuning improves the performance
of open-source LLMs, allowing them to match or even surpass zero-shot GPT-3.5
and GPT-4, though still lagging behind fine-tuned GPT-3.5. We further establish
that fine-tuning is preferable to few-shot training with a relatively modest
quantity of annotated text. Our findings show that fine-tuned open-source LLMs
can be effectively deployed in a broad spectrum of text annotation
applications. We provide a Python notebook facilitating the application of LLMs
in text annotation for other researchers.",None,-1
368debd8-41aa-49c8-96bb-7261baec4d25,Efficient Monotonic Multihead Attention,0.213788,"We introduce the Efficient Monotonic Multihead Attention (EMMA), a
state-of-the-art simultaneous translation model with numerically-stable and
unbiased monotonic alignment estimation. In addition, we present improved
training and inference strategies, including simultaneous fine-tuning from an
offline translation model and reduction of monotonic alignment variance. The
experimental results demonstrate that the proposed model attains
state-of-the-art performance in simultaneous speech-to-text translation on the
Spanish and English translation task.",None,-1
b7170c89-d4f8-4a31-a35f-6ff2400759dd,Learning Conditional Attributes for Compositional Zero-Shot Learning,0.187117,"Compositional Zero-Shot Learning (CZSL) aims to train models to recognize
novel compositional concepts based on learned concepts such as attribute-object
combinations. One of the challenges is to model attributes interacted with
different objects, e.g., the attribute ``wet"" in ``wet apple"" and ``wet cat"" is
different. As a solution, we provide analysis and argue that attributes are
conditioned on the recognized object and input image and explore learning
conditional attribute embeddings by a proposed attribute learning framework
containing an attribute hyper learner and an attribute base learner. By
encoding conditional attributes, our model enables to generate flexible
attribute embeddings for generalization from seen to unseen compositions.
Experiments on CZSL benchmarks, including the more challenging C-GQA dataset,
demonstrate better performances compared with other state-of-the-art approaches
and validate the importance of learning conditional attributes. Code is
available at https://github.com/wqshmzh/CANet-CZSL",None,-1
d0b9c206-fedf-4c2d-a18b-cf56474230de,Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance,0.239742,"Retrieval augmentation enhances performance of traditional language models by
incorporating additional context. However, the computational demands for
retrieval augmented large language models (LLMs) pose a challenge when applying
them to real-time tasks, such as composition assistance. To address this
limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG)
framework, a novel approach that efficiently combines a cloud-based LLM with a
smaller, client-side, language model through retrieval augmented memory. This
integration enables the client model to generate effective responses,
benefiting from the LLM's capabilities and contextual information.
Additionally, through an asynchronous memory update mechanism, the client model
can deliver real-time completions swiftly to user inputs without the need to
wait for responses from the cloud. Our experiments on five benchmark datasets
demonstrate that HybridRAG significantly improves utility over client-only
models while maintaining low latency.",None,-1
993e7930-29c1-4a61-a40c-aa62b3ec2f92,DeePoint: Visual Pointing Recognition and Direction Estimation,0.067678,"In this paper, we realize automatic visual recognition and direction
estimation of pointing. We introduce the first neural pointing understanding
method based on two key contributions. The first is the introduction of a
first-of-its-kind large-scale dataset for pointing recognition and direction
estimation, which we refer to as the DP Dataset. DP Dataset consists of more
than 2 million frames of 33 people pointing in various styles annotated for
each frame with pointing timings and 3D directions. The second is DeePoint, a
novel deep network model for joint recognition and 3D direction estimation of
pointing. DeePoint is a Transformer-based network which fully leverages the
spatio-temporal coordination of the body parts, not just the hands. Through
extensive experiments, we demonstrate the accuracy and efficiency of DeePoint.
We believe DP Dataset and DeePoint will serve as a sound foundation for visual
human intention understanding.",None,-1
9439abc5-3ba8-45b1-8451-1875dd4c58f1,Citance-Contextualized Summarization of Scientific Papers,0.258163,"Current approaches to automatic summarization of scientific papers generate
informative summaries in the form of abstracts. However, abstracts are not
intended to show the relationship between a paper and the references cited in
it. We propose a new contextualized summarization approach that can generate an
informative summary conditioned on a given sentence containing the citation of
a reference (a so-called ""citance""). This summary outlines the content of the
cited paper relevant to the citation location. Thus, our approach extracts and
models the citances of a paper, retrieves relevant passages from cited papers,
and generates abstractive summaries tailored to each citance. We evaluate our
approach using $\textbf{Webis-Context-SciSumm-2023}$, a new dataset containing
540K~computer science papers and 4.6M~citances therein.",None,-1
42fc565f-e7ee-4c3d-bbce-a80d100ea72b,Adaptive questionnaires for facilitating patient data entry in clinical decision support systems: Methods and application to STOPP/START v2,0.258075,"Clinical decision support systems are software tools that help clinicians to
make medical decisions. However, their acceptance by clinicians is usually
rather low. A known problem is that they often require clinicians to manually
enter lots of patient data, which is long and tedious. Existing solutions, such
as the automatic data extraction from electronic health record, are not fully
satisfying, because of low data quality and availability. In practice, many
systems still include long questionnaire for data entry.
  In this paper, we propose an original solution to simplify patient data
entry, using an adaptive questionnaire, i.e. a questionnaire that evolves
during user interaction, showing or hiding questions dynamically. Considering a
rule-based decision support systems, we designed methods for translating the
system's clinical rules into display rules that determine the items to show in
the questionnaire, and methods for determining the optimal order of priority
among the items in the questionnaire. We applied this approach to a decision
support system implementing STOPP/START v2, a guideline for managing
polypharmacy. We show that it permits reducing by about two thirds the number
of clinical conditions displayed in the questionnaire. Presented to clinicians
during focus group sessions, the adaptive questionnaire was found ""pretty easy
to use"". In the future, this approach could be applied to other guidelines, and
adapted for data entry by patients.",None,-1
b6a35095-d42a-4155-a648-096b4f26e5aa,Interaction-aware Joint Attention Estimation Using People Attributes,0.258318,"This paper proposes joint attention estimation in a single image. Different
from related work in which only the gaze-related attributes of people are
independently employed, (I) their locations and actions are also employed as
contextual cues for weighting their attributes, and (ii) interactions among all
of these attributes are explicitly modeled in our method. For the interaction
modeling, we propose a novel Transformer-based attention network to encode
joint attention as low-dimensional features. We introduce a specialized MLP
head with positional embedding to the Transformer so that it predicts pixelwise
confidence of joint attention for generating the confidence heatmap. This
pixelwise prediction improves the heatmap accuracy by avoiding the ill-posed
problem in which the high-dimensional heatmap is predicted from the
low-dimensional features. The estimated joint attention is further improved by
being integrated with general image-based attention estimation. Our method
outperforms SOTA methods quantitatively in comparative experiments. Code:
https://anonymous.4open.science/r/anonymized_codes-ECA4.",None,-1
749c02ad-f825-4f9b-abb3-bc3529aa6274,Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements,0.704882,"Empathetic dialogue is an indispensable part of building harmonious social
relationships and contributes to the development of a helpful AI. Previous
approaches are mainly based on fine small-scale language models. With the
advent of ChatGPT, the application effect of large language models (LLMs) in
this field has attracted great attention. This work empirically investigates
the performance of LLMs in generating empathetic responses and proposes three
improvement methods of semantically similar in-context learning, two-stage
interactive generation, and combination with the knowledge base. Extensive
experiments show that LLMs can significantly benefit from our proposed methods
and is able to achieve state-of-the-art performance in both automatic and human
evaluations. Additionally, we explore the possibility of GPT-4 simulating human
evaluators.",None,-1
9c5a545a-c5ca-4a49-9009-9827f5b1d431,GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4,0.999999,"This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to
detect translation quality errors, specifically for the quality estimation
setting without the need for human reference translations. Based on the power
of large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting
technique, querying the GPT-4 model to mark error quality spans. Compared to
previous works, our method has language-agnostic prompts, thus avoiding the
need for manual prompt preparation for new languages.
  While preliminary results indicate that GEMBA-MQM achieves state-of-the-art
accuracy for system ranking, we advise caution when using it in academic works
to demonstrate improvements over other methods due to its dependence on the
proprietary, black-box GPT model.",None,-1
c87bad5b-830a-4c68-998b-525b622302d5,Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding,0.286313,"Recently, large language models (LLMs) have made significant advancements in
natural language understanding and generation. However, their potential in
computer vision remains largely unexplored. In this paper, we introduce a new,
exploratory approach that enables LLMs to process images using the Scalable
Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions
of SVG representations instead of raster images, we aim to bridge the gap
between the visual and textual modalities, allowing LLMs to directly understand
and manipulate images without the need for parameterized visual components. Our
method facilitates simple image classification, generation, and in-context
learning using only LLM capabilities. We demonstrate the promise of our
approach across discriminative and generative tasks, highlighting its (i)
robustness against distribution shift, (ii) substantial improvements achieved
by tapping into the in-context learning abilities of LLMs, and (iii) image
understanding and generation capabilities with human guidance. Our code, data,
and models can be found here https://github.com/mu-cai/svg-llm.",None,-1
1cd0da6e-6d86-405e-bda2-f2a5b555acd2,Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy,0.363631,"Real-world deployment of computer vision systems, including in the discovery
processes of biomedical research, requires causal representations that are
invariant to contextual nuisances and generalize to new data. Leveraging the
internal replicate structure of two novel single-cell fluorescent microscopy
datasets, we propose generally applicable tests to assess the extent to which
models learn causal representations across increasingly challenging levels of
OOD-generalization. We show that despite seemingly strong performance, as
assessed by other established metrics, both naive and contemporary baselines
designed to ward against confounding, collapse on these tests. We introduce a
new method, Interventional Style Transfer (IST), that substantially improves
OOD generalization by generating interventional training distributions in which
spurious correlations between biological causes and nuisances are mitigated. We
publish our code and datasets.",None,-1
ee5e2a23-a20e-404e-b57c-6c02f49d0cc0,RPTQ: Reorder-based Post-training Quantization for Large Language Models,0.915786,"Large-scale language models (LLMs) have demonstrated impressive performance,
but their deployment presents challenges due to their significant memory usage.
This issue can be alleviated through quantization. In this paper, we identify
that the challenge in quantizing activations in LLMs arises from varying ranges
across channels, rather than solely the presence of outliers. To address this
challenge, we introduce a quantization method called RPTQ, which utilizes a
reorder-based approach. By rearranging the channels and quantizing them in
clusters, RPTQ effectively mitigates the impact of range differences between
channels. To minimize the overhead of the reorder operation, we fuse it into
the layer norm operation and weights in linear layers. In our experiments, RPTQ
achieved a significant breakthrough by utilizing 3-bit activation in LLMs for
the first time, resulting in a substantial reduction in memory usage. For
instance, quantizing OPT-175b can lead to a memory consumption reduction of up
to 80%.",None,-1
6b08d18c-da6a-4413-bea4-d5bd2d069f72,SMATCH++: Standardized and Extended Evaluation of Semantic Graphs,0.741204,"The Smatch metric is a popular method for evaluating graph distances, as is
necessary, for instance, to assess the performance of semantic graph parsing
systems. However, we observe some issues in the metric that jeopardize
meaningful evaluation. E.g., opaque pre-processing choices can affect results,
and current graph-alignment solvers do not provide us with upper-bounds.
Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore,
adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)
are spread out, and lack a unifying framework.
  For better inspection, we divide the metric into three modules:
pre-processing, alignment, and scoring. Examining each module, we specify its
goals and diagnose potential issues, for which we discuss and test mitigation
strategies. For pre-processing, we show how to fully conform to annotation
guidelines that allow structurally deviating but valid graphs. For safer and
enhanced alignment, we show the feasibility of optimal alignment in a standard
evaluation setup, and develop a lossless graph compression method that shrinks
the search space and significantly increases efficiency. For improved scoring,
we propose standardized and extended metric calculation of fine-grained
sub-graph meaning aspects. Our code is available at
https://github.com/flipz357/smatchpp",None,-1
6b259d43-9a1e-4ee6-8123-e7d4be6be9a7,High-Fidelity Zero-Shot Texture Anomaly Localization Using Feature Correspondence Analysis,0.432781,"We propose a novel method for Zero-Shot Anomaly Localization on textures. The
task refers to identifying abnormal regions in an otherwise homogeneous image.
To obtain a high-fidelity localization, we leverage a bijective mapping derived
from the 1-dimensional Wasserstein Distance. As opposed to using holistic
distances between distributions, the proposed approach allows pinpointing the
non-conformity of a pixel in a local context with increased precision. By
aggregating the contribution of the pixel to the errors of all nearby patches
we obtain a reliable anomaly score estimate. We validate our solution on
several datasets and obtain more than a 40% reduction in error over the
previous state of the art on the MVTec AD dataset in a zero-shot setting. Also
see https://reality.tf.fau.de/pub/ardelean2024highfidelity.html.",None,-1
b163c670-411b-4370-aafc-a55f3c6464c7,"""What if?"" in Probabilistic Logic Programming",0.24988,"A ProbLog program is a logic program with facts that only hold with a
specified probability. In this contribution we extend this ProbLog language by
the ability to answer ""What if"" queries. Intuitively, a ProbLog program defines
a distribution by solving a system of equations in terms of mutually
independent predefined Boolean random variables. In the theory of causality,
Judea Pearl proposes a counterfactual reasoning for such systems of equations.
Based on Pearl's calculus, we provide a procedure for processing these
counterfactual queries on ProbLog programs, together with a proof of
correctness and a full implementation. Using the latter, we provide insights
into the influence of different parameters on the scalability of inference.
Finally, we also show that our approach is consistent with CP-logic, i.e. with
the causal semantics for logic programs with annotated with disjunctions.",None,-1
c14a8e82-5b86-4d1e-9967-996f878378f3,Triplet Loss-less Center Loss Sampling Strategies in Facial Expression Recognition Scenarios,0.749447,"Facial expressions convey massive information and play a crucial role in
emotional expression. Deep neural network (DNN) accompanied by deep metric
learning (DML) techniques boost the discriminative ability of the model in
facial expression recognition (FER) applications. DNN, equipped with only
classification loss functions such as Cross-Entropy cannot compact intra-class
feature variation or separate inter-class feature distance as well as when it
gets fortified by a DML supporting loss item. The triplet center loss (TCL)
function is applied on all dimensions of the sample's embedding in the
embedding space. In our work, we developed three strategies: fully-synthesized,
semi-synthesized, and prediction-based negative sample selection strategies. To
achieve better results, we introduce a selective attention module that provides
a combination of pixel-wise and element-wise attention coefficients using
high-semantic deep features of input samples. We evaluated the proposed method
on the RAF-DB, a highly imbalanced dataset. The experimental results reveal
significant improvements in comparison to the baseline for all three negative
sample selection strategies.",None,-1
11baf3ba-decd-4dd6-a6fa-82ba60e5ddcc,ParaFormer: Parallel Attention Transformer for Efficient Feature Matching,0.683391,"Heavy computation is a bottleneck limiting deep-learningbased feature
matching algorithms to be applied in many realtime applications. However,
existing lightweight networks optimized for Euclidean data cannot address
classical feature matching tasks, since sparse keypoint based descriptors are
expected to be matched. This paper tackles this problem and proposes two
concepts: 1) a novel parallel attention model entitled ParaFormer and 2) a
graph based U-Net architecture with attentional pooling. First, ParaFormer
fuses features and keypoint positions through the concept of amplitude and
phase, and integrates self- and cross-attention in a parallel manner which
achieves a win-win performance in terms of accuracy and efficiency. Second,
with U-Net architecture and proposed attentional pooling, the ParaFormer-U
variant significantly reduces computational complexity, and minimize
performance loss caused by downsampling. Sufficient experiments on various
applications, including homography estimation, pose estimation, and image
matching, demonstrate that ParaFormer achieves state-of-the-art performance
while maintaining high efficiency. The efficient ParaFormer-U variant achieves
comparable performance with less than 50% FLOPs of the existing attention-based
models.",None,-1
bb60c3a8-f9c9-429d-afc0-6ada55c6e326,GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds,0.757016,"We study the problem of 3D semantic segmentation from raw point clouds.
Unlike existing methods which primarily rely on a large amount of human
annotations for training neural networks, we propose the first purely
unsupervised method, called GrowSP, to successfully identify complex semantic
classes for every point in 3D scenes, without needing any type of human labels
or pretrained models. The key to our approach is to discover 3D semantic
elements via progressive growing of superpoints. Our method consists of three
major components, 1) the feature extractor to learn per-point features from
input point clouds, 2) the superpoint constructor to progressively grow the
sizes of superpoints, and 3) the semantic primitive clustering module to group
superpoints into semantic elements for the final semantic segmentation. We
extensively evaluate our method on multiple datasets, demonstrating superior
performance over all unsupervised baselines and approaching the classic
fully-supervised PointNet. We hope our work could inspire more advanced methods
for unsupervised 3D semantic learning.",None,-1
f3ed9bd5-c1f3-4e9e-bbdb-7e18099dd90f,Spelling convention sensitivity in neural language models,0.0254206,"We examine whether large neural language models, trained on very large
collections of varied English text, learn the potentially long-distance
dependency of British versus American spelling conventions, i.e., whether
spelling is consistently one or the other within model-generated strings. In
contrast to long-distance dependencies in non-surface underlying structure
(e.g., syntax), spelling consistency is easier to measure both in LMs and the
text corpora used to train them, which can provide additional insight into
certain observed model behaviors. Using a set of probe words unique to either
British or American English, we first establish that training corpora exhibit
substantial (though not total) consistency. A large T5 language model does
appear to internalize this consistency, though only with respect to observed
lexical items (not nonce words with British/American spelling patterns). We
further experiment with correcting for biases in the training data by
fine-tuning T5 on synthetic data that has been debiased, and find that
finetuned T5 remains only somewhat sensitive to spelling consistency. Further
experiments show GPT2 to be similarly limited.",None,-1
145380cb-88a7-47b3-ae26-5fd7a18079e8,TBDetector:Transformer-Based Detector for Advanced Persistent Threats with Provenance Graph,0.277485,"APT detection is difficult to detect due to the long-term latency, covert and
slow multistage attack patterns of Advanced Persistent Threat (APT). To tackle
these issues, we propose TBDetector, a transformer-based advanced persistent
threat detection method for APT attack detection. Considering that provenance
graphs provide rich historical information and have the powerful attacks
historic correlation ability to identify anomalous activities, TBDetector
employs provenance analysis for APT detection, which summarizes long-running
system execution with space efficiency and utilizes transformer with
self-attention based encoder-decoder to extract long-term contextual features
of system states to detect slow-acting attacks. Furthermore, we further
introduce anomaly scores to investigate the anomaly of different system states,
where each state is calculated with an anomaly score corresponding to its
similarity score and isolation score. To evaluate the effectiveness of the
proposed method, we have conducted experiments on five public datasets, i.e.,
streamspot, cadets, shellshock, clearscope, and wget_baseline. Experimental
results and comparisons with state-of-the-art methods have exhibited better
performance of our proposed method.",None,-1
71196ba7-7a5e-4aad-840e-0197bab5267c,Modularized Zero-shot VQA with Pre-trained Models,0.0440397,"Large-scale pre-trained models (PTMs) show great zero-shot capabilities. In
this paper, we study how to leverage them for zero-shot visual question
answering (VQA). Our approach is motivated by a few observations. First, VQA
questions often require multiple steps of reasoning, which is still a
capability that most PTMs lack. Second, different steps in VQA reasoning chains
require different skills such as object detection and relational reasoning, but
a single PTM may not possess all these skills. Third, recent work on zero-shot
VQA does not explicitly consider multi-step reasoning chains, which makes them
less interpretable compared with a decomposition-based approach. We propose a
modularized zero-shot network that explicitly decomposes questions into sub
reasoning steps and is highly interpretable. We convert sub reasoning tasks to
acceptable objectives of PTMs and assign tasks to proper PTMs without any
adaptation. Our experiments on two VQA benchmarks under the zero-shot setting
demonstrate the effectiveness of our method and better interpretability
compared with several baselines.",None,-1
5eb85344-c5a1-4054-8cd1-59f9994d3b7c,Context-Gloss Augmentation for Improving Arabic Target Sense Verification,0.770509,"Arabic language lacks semantic datasets and sense inventories. The most
common semantically-labeled dataset for Arabic is the ArabGlossBERT, a
relatively small dataset that consists of 167K context-gloss pairs (about 60K
positive and 107K negative pairs), collected from Arabic dictionaries. This
paper presents an enrichment to the ArabGlossBERT dataset, by augmenting it
using (Arabic-English-Arabic) machine back-translation. Augmentation increased
the dataset size to 352K pairs (149K positive and 203K negative pairs). We
measure the impact of augmentation using different data configurations to
fine-tune BERT on target sense verification (TSV) task. Overall, the accuracy
ranges between 78% to 84% for different data configurations. Although our
approach performed at par with the baseline, we did observe some improvements
for some POS tags in some experiments. Furthermore, our fine-tuned models are
trained on a larger dataset covering larger vocabulary and contexts. We provide
an in-depth analysis of the accuracy for each part-of-speech (POS).",None,-1
63c5920c-54f3-4044-94b7-1ee3843f7028,Short Answer Grading Using One-shot Prompting and Text Similarity Scoring Model,0.796142,"In this study, we developed an automated short answer grading (ASAG) model
that provided both analytic scores and final holistic scores. Short answer
items typically consist of multiple sub-questions, and providing an analytic
score and the text span relevant to each sub-question can increase the
interpretability of the automated scores. Furthermore, they can be used to
generate actionable feedback for students. Despite these advantages, most
studies have focused on predicting only holistic scores due to the difficulty
in constructing dataset with manual annotations. To address this difficulty, we
used large language model (LLM)-based one-shot prompting and a text similarity
scoring model with domain adaptation using small manually annotated dataset.
The accuracy and quadratic weighted kappa of our model were 0.67 and 0.71 on a
subset of the publicly available ASAG dataset. The model achieved a substantial
improvement over the majority baseline.",None,-1
1d18d8a1-f40a-472f-86f2-d3b1cf79d48e,Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs,0.251534,"Recent work in Natural Language Processing and Computer Vision has been using
textual information -- e.g., entity names and descriptions -- available in
knowledge graphs to ground neural models to high-quality structured data.
However, when it comes to non-English languages, the quantity and quality of
textual information are comparatively scarce. To address this issue, we
introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and
perform a thorough investigation on bridging the gap in both the quantity and
quality of textual information between English and non-English languages. More
specifically, we: i) bring to light the problem of increasing multilingual
coverage and precision of entity names and descriptions in Wikidata; ii)
demonstrate that state-of-the-art methods, namely, Machine Translation (MT),
Web Search (WS), and Large Language Models (LLMs), struggle with this task;
iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and
LLMs to generate high-quality textual information; and, iv) study the impact of
increasing multilingual coverage and precision of non-English textual
information in Entity Linking, Knowledge Graph Completion, and Question
Answering. As part of our effort towards better multilingual knowledge graphs,
we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE
approaches in 10 languages across 7 language families.",None,-1
0950e918-4774-4cf7-af75-7729a386a141,Over-Sampling Strategy in Feature Space for Graphs based Class-imbalanced Bot Detection,0.230425,"The presence of a large number of bots in Online Social Networks (OSN) leads
to undesirable social effects. Graph neural networks (GNNs) are effective in
detecting bots as they utilize user interactions. However, class-imbalanced
issues can affect bot detection performance. To address this, we propose an
over-sampling strategy for GNNs (OS-GNN) that generates samples for the
minority class without edge synthesis. First, node features are mapped to a
feature space through neighborhood aggregation. Then, we generate samples for
the minority class in the feature space. Finally, the augmented features are
used to train the classifiers. This framework is general and can be easily
extended into different GNN architectures. The proposed framework is evaluated
using three real-world bot detection benchmark datasets, and it consistently
exhibits superiority over the baselines.",None,-1
b456ed17-e4a4-4f77-bfe0-9361e76147f9,ViT2EEG: Leveraging Hybrid Pretrained Vision Transformers for EEG Data,0.53499,"In this study, we demonstrate the application of a hybrid Vision Transformer
(ViT) model, pretrained on ImageNet, on an electroencephalogram (EEG)
regression task. Despite being originally trained for image classification
tasks, when fine-tuned on EEG data, this model shows a notable increase in
performance compared to other models, including an identical architecture ViT
trained without the ImageNet weights. This discovery challenges the traditional
understanding of model generalization, suggesting that Transformer models
pretrained on seemingly unrelated image data can provide valuable priors for
EEG regression tasks with an appropriate fine-tuning pipeline.
  The success of this approach suggests that the features extracted by ViT
models in the context of visual tasks can be readily transformed for the
purpose of EEG predictive modeling. We recommend utilizing this methodology not
only in neuroscience and related fields, but generally for any task where data
collection is limited by practical, financial, or ethical constraints. Our
results illuminate the potential of pretrained models on tasks that are clearly
distinct from their original purpose.",None,-1
40988467-e9e5-4c95-af85-3164ac96ab6e,Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information,0.108141,"The long-standing one-to-many issue of the open-domain dialogues poses
significant challenges for automatic evaluation methods, i.e., there may be
multiple suitable responses which differ in semantics for a given
conversational context. To tackle this challenge, we propose a novel
learning-based automatic evaluation metric (CMN), which can robustly evaluate
open-domain dialogues by augmenting Conditional Variational Autoencoders
(CVAEs) with a Next Sentence Prediction (NSP) objective and employing Mutual
Information (MI) to model the semantic similarity of text in the latent space.
Experimental results on two open-domain dialogue datasets demonstrate the
superiority of our method compared with a wide range of baselines, especially
in handling responses which are distant to the golden reference responses in
semantics.",None,-1
bc1ccef8-f896-410b-b135-6389bb7b7e32,SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality,0.726047,"In the last year alone, a surge of new benchmarks to measure compositional
understanding of vision-language models have permeated the machine learning
ecosystem. Given an image, these benchmarks probe a model's ability to identify
its associated caption amongst a set of compositional distractors.
Surprisingly, we find significant biases in all these benchmarks rendering them
hackable. This hackability is so dire that blind models with no access to the
image outperform state-of-the-art vision-language models. To remedy this
rampant vulnerability, we introduce SugarCrepe, a new benchmark for
vision-language compositionality evaluation. We employ large language models,
instead of rule-based templates used in previous benchmarks, to generate fluent
and sensical hard negatives, and utilize an adversarial refinement mechanism to
maximally reduce biases. We re-evaluate state-of-the-art models and recently
proposed compositionality inducing strategies, and find that their improvements
were hugely overestimated, suggesting that more innovation is needed in this
important direction. We release SugarCrepe and the code for evaluation at:
https://github.com/RAIVNLab/sugar-crepe.",None,-1
00c58834-781a-472e-b090-11b197b9b4e7,Spectrum-guided Multi-granularity Referring Video Object Segmentation,0.790169,"Current referring video object segmentation (R-VOS) techniques extract
conditional kernels from encoded (low-resolution) vision-language features to
segment the decoded high-resolution features. We discovered that this causes
significant feature drift, which the segmentation kernels struggle to perceive
during the forward computation. This negatively affects the ability of
segmentation kernels. To address the drift problem, we propose a
Spectrum-guided Multi-granularity (SgMg) approach, which performs direct
segmentation on the encoded features and employs visual details to further
optimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion
(SCF) to perform intra-frame global interactions in the spectral domain for
effective multimodal representation. Finally, we extend SgMg to perform
multi-object R-VOS, a new paradigm that enables simultaneous segmentation of
multiple referred objects in a video. This not only makes R-VOS faster, but
also more practical. Extensive experiments show that SgMg achieves
state-of-the-art performance on four video benchmark datasets, outperforming
the nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMg
enables multi-object R-VOS, runs about 3 times faster while maintaining
satisfactory performance. Code is available at https://github.com/bo-miao/SgMg.",None,-1
1020c957-d403-4c82-abc0-aeca107d14b3,Image To Tree with Recursive Prompting,0.0770791,"Extracting complex structures from grid-based data is a common key step in
automated medical image analysis. The conventional solution to recovering
tree-structured geometries typically involves computing the minimal cost path
through intermediate representations derived from segmentation masks. However,
this methodology has significant limitations in the context of projective
imaging of tree-structured 3D anatomical data such as coronary arteries, since
there are often overlapping branches in the 2D projection. In this work, we
propose a novel approach to predicting tree connectivity structure which
reformulates the task as an optimization problem over individual steps of a
recursive process. We design and train a two-stage model which leverages the
UNet and Transformer architectures and introduces an image-based prompting
technique. Our proposed method achieves compelling results on a pair of
synthetic datasets, and outperforms a shortest-path baseline.",None,-1
b36fed3f-ed3a-4795-9f69-d84e9d85babf,FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain,0.398526,"This paper introduces FrenchMedMCQA, the first publicly available
Multiple-Choice Question Answering (MCQA) dataset in French for medical domain.
It is composed of 3,105 questions taken from real exams of the French medical
specialization diploma in pharmacy, mixing single and multiple answers. Each
instance of the dataset contains an identifier, a question, five possible
answers and their manual correction(s). We also propose first baseline models
to automatically process this MCQA task in order to report on the current
performances and to highlight the difficulty of the task. A detailed analysis
of the results showed that it is necessary to have representations adapted to
the medical domain or to the MCQA task: in our case, English specialized models
yielded better results than generic French ones, even though FrenchMedMCQA is
in French. Corpus, models and tools are available online.",None,-1
41bd2e84-c5cb-4052-b912-1b38de07bd3a,Evaluation of AI Chatbots for Patient-Specific EHR Questions,0.361319,"This paper investigates the use of artificial intelligence chatbots for
patient-specific question answering (QA) from clinical notes using several
large language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google
Bard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and
coherence of the answers generated by each model using a 5-point Likert scale
on a set of patient-specific questions.",None,-1
63c9e575-8ed2-4c20-8550-7a625a0f33c0,Siamese DETR,0.209414,"Recent self-supervised methods are mainly designed for representation
learning with the base model, e.g., ResNets or ViTs. They cannot be easily
transferred to DETR, with task-specific Transformer modules. In this work, we
present Siamese DETR, a Siamese self-supervised pretraining approach for the
Transformer architecture in DETR. We consider learning view-invariant and
detection-oriented representations simultaneously through two complementary
tasks, i.e., localization and discrimination, in a novel multi-view learning
framework. Two self-supervised pretext tasks are designed: (i) Multi-View
Region Detection aims at learning to localize regions-of-interest between
augmented views of the input, and (ii) Multi-View Semantic Discrimination
attempts to improve object-level discrimination for each region. The proposed
Siamese DETR achieves state-of-the-art transfer performance on COCO and PASCAL
VOC detection using different DETR variants in all setups. Code is available at
https://github.com/Zx55/SiameseDETR.",None,-1
e55dc414-7970-4c30-80cb-f0b87a64ad1d,Knowledge Enhanced Model for Live Video Comment Generation,0.422859,"Live video commenting is popular on video media platforms, as it can create a
chatting atmosphere and provide supplementary information for users while
watching videos. Automatically generating live video comments can improve user
experience and enable human-like generation for bot chatting. Existing works
mostly focus on short video datasets while ignoring other important video types
such as long videos like movies. In this work, we collect a new Movie Live
Comments (MovieLC) dataset to support research on live video comment generation
for long videos. We also propose a knowledge enhanced generation model inspired
by the divergent and informative nature of live video comments. Our model
adopts a pre-training encoder-decoder framework and incorporates external
knowledge. Extensive experiments show that both objective metrics and human
evaluation demonstrate the effectiveness of our proposed model. The MovieLC
dataset and our code will be released.",None,-1
2033dfba-014b-42c5-967d-ae4b58f6a4c5,Competitions in AI -- Robustly Ranking Solvers Using Statistical Resampling,0.0366877,"Solver competitions play a prominent role in assessing and advancing the
state of the art for solving many problems in AI and beyond. Notably, in many
areas of AI, competitions have had substantial impact in guiding research and
applications for many years, and for a solver to be ranked highly in a
competition carries considerable weight. But to which extent can we expect
competition results to generalise to sets of problem instances different from
those used in a particular competition? This is the question we investigate
here, using statistical resampling techniques. We show that the rankings
resulting from the standard interpretation of competition results can be very
sensitive to even minor changes in the benchmark instance set used as the basis
for assessment and can therefore not be expected to carry over to other samples
from the same underlying instance distribution. To address this problem, we
introduce a novel approach to statistically meaningful analysis of competition
results based on resampling performance data. Our approach produces confidence
intervals of competition scores as well as statistically robust solver rankings
with bounded error. Applied to recent SAT, AI planning and computer vision
competitions, our analysis reveals frequent statistical ties in solver
performance as well as some inversions of ranks compared to the official
results based on simple scoring.",None,-1
3683f89b-9243-4747-a229-efe30a79592b,Assessing the Effectiveness of GPT-3 in Detecting False Political Statements: A Case Study on the LIAR Dataset,0.844836,"The detection of political fake statements is crucial for maintaining
information integrity and preventing the spread of misinformation in society.
Historically, state-of-the-art machine learning models employed various methods
for detecting deceptive statements. These methods include the use of metadata
(W. Wang et al., 2018), n-grams analysis (Singh et al., 2021), and linguistic
(Wu et al., 2022) and stylometric (Islam et al., 2020) features. Recent
advancements in large language models, such as GPT-3 (Brown et al., 2020) have
achieved state-of-the-art performance on a wide range of tasks. In this study,
we conducted experiments with GPT-3 on the LIAR dataset (W. Wang et al., 2018)
and achieved higher accuracy than state-of-the-art models without using any
additional meta or linguistic features. Additionally, we experimented with
zero-shot learning using a carefully designed prompt and achieved near
state-of-the-art performance. An advantage of this approach is that the model
provided evidence for its decision, which adds transparency to the model's
decision-making and offers a chance for users to verify the validity of the
evidence provided.",None,-1
917e4a7e-0ad3-4663-a856-235530071f8f,Turning large language models into cognitive models,0.841132,"Large language models are powerful systems that excel at many tasks, ranging
from translation to mathematical reasoning. Yet, at the same time, these models
often show unhuman-like characteristics. In the present paper, we address this
gap and ask whether large language models can be turned into cognitive models.
We find that -- after finetuning them on data from psychological experiments --
these models offer accurate representations of human behavior, even
outperforming traditional cognitive models in two decision-making domains. In
addition, we show that their representations contain the information necessary
to model behavior on the level of individual subjects. Finally, we demonstrate
that finetuning on multiple tasks enables large language models to predict
human behavior in a previously unseen task. Taken together, these results
suggest that large, pre-trained models can be adapted to become generalist
cognitive models, thereby opening up new research directions that could
transform cognitive psychology and the behavioral sciences as a whole.",None,-1
3c87ea39-a770-4ee1-9f5f-506018c6327d,Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models,0.0198413,"Debiasing methods that seek to mitigate the tendency of Language Models (LMs)
to occasionally output toxic or inappropriate text have recently gained
traction. In this paper, we propose a standardized protocol which distinguishes
methods that yield not only desirable results, but are also consistent with
their mechanisms and specifications. For example, we ask, given a debiasing
method that is developed to reduce toxicity in LMs, if the definition of
toxicity used by the debiasing method is reversed, would the debiasing results
also be reversed? We used such considerations to devise three criteria for our
new protocol: Specification Polarity, Specification Importance, and Domain
Transferability. As a case study, we apply our protocol to a popular debiasing
method, Self-Debiasing, and compare it to one we propose, called Instructive
Debiasing, and demonstrate that consistency is as important an aspect to
debiasing viability as is simply a desirable result. We show that our protocol
provides essential insights into the generalizability and interpretability of
debiasing methods that may otherwise go overlooked.",None,-1
90f7a7a8-0661-4c16-8d06-9bba4155cb6d,Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected,0.848982,"Meta AI Research has recently released SAM (Segment Anything Model) which is
trained on a large segmentation dataset of over 1 billion masks. As a
foundation model in the field of computer vision, SAM (Segment Anything Model)
has gained attention for its impressive performance in generic object
segmentation. Despite its strong capability in a wide range of zero-shot
transfer tasks, it remains unknown whether SAM can detect things in challenging
setups like transparent objects. In this work, we perform an empirical
evaluation of two glass-related challenging scenarios: mirror and transparent
objects. We found that SAM often fails to detect the glass in both scenarios,
which raises concern for deploying the SAM in safety-critical situations that
have various forms of glass.",None,-1
7a15d322-2232-4761-ba6c-05584552b77d,Unsupervised Learning of Robust Spectral Shape Matching,0.801059,"We propose a novel learning-based approach for robust 3D shape matching. Our
method builds upon deep functional maps and can be trained in a fully
unsupervised manner. Previous deep functional map methods mainly focus on
predicting optimised functional maps alone, and then rely on off-the-shelf
post-processing to obtain accurate point-wise maps during inference. However,
this two-stage procedure for obtaining point-wise maps often yields sub-optimal
performance. In contrast, building upon recent insights about the relation
between functional maps and point-wise maps, we propose a novel unsupervised
loss to couple the functional maps and point-wise maps, and thereby directly
obtain point-wise maps without any post-processing. Our approach obtains
accurate correspondences not only for near-isometric shapes, but also for more
challenging non-isometric shapes and partial shapes, as well as shapes with
different discretisation or topological noise. Using a total of nine diverse
datasets, we extensively evaluate the performance and demonstrate that our
method substantially outperforms previous state-of-the-art methods, even
compared to recent supervised methods. Our code is available at
https://github.com/dongliangcao/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching.",None,-1
07c3e992-61e1-4e56-b5ee-a8efb91ec61e,Unveiling Black-boxes: Explainable Deep Learning Models for Patent Classification,0.149444,"Recent technological advancements have led to a large number of patents in a
diverse range of domains, making it challenging for human experts to analyze
and manage. State-of-the-art methods for multi-label patent classification rely
on deep neural networks (DNNs), which are complex and often considered
black-boxes due to their opaque decision-making processes. In this paper, we
propose a novel deep explainable patent classification framework by introducing
layer-wise relevance propagation (LRP) to provide human-understandable
explanations for predictions. We train several DNN models, including Bi-LSTM,
CNN, and CNN-BiLSTM, and propagate the predictions backward from the output
layer up to the input layer of the model to identify the relevance of words for
individual predictions. Considering the relevance score, we then generate
explanations by visualizing relevant words for the predicted patent class.
Experimental results on two datasets comprising two-million patent texts
demonstrate high performance in terms of various evaluation measures. The
explanations generated for each prediction highlight important relevant words
that align with the predicted class, making the prediction more understandable.
Explainable systems have the potential to facilitate the adoption of complex
AI-enabled methods for patent classification in real-world applications.",None,-1
46aadcf0-59b6-4ce0-b97d-85e6d291d5e9,Knowledge Engineering for Wind Energy,0.250876,"With the rapid evolution of the wind energy sector, there is an
ever-increasing need to create value from the vast amounts of data made
available both from within the domain, as well as from other sectors. This
article addresses the challenges faced by wind energy domain experts in
converting data into domain knowledge, connecting and integrating it with other
sources of knowledge, and making it available for use in next generation
artificially intelligent systems. To this end, this article highlights the role
that knowledge engineering can play in the process of digital transformation of
the wind energy sector. It presents the main concepts underpinning
Knowledge-Based Systems and summarises previous work in the areas of knowledge
engineering and knowledge representation in a manner that is relevant and
accessible to domain experts. A systematic analysis of the current
state-of-the-art on knowledge engineering in the wind energy domain is
performed, with available tools put into perspective by establishing the main
domain actors and their needs and identifying key problematic areas. Finally,
guidelines for further development and improvement are provided.",None,-1
e723a4ad-3ddb-4576-8221-ab1c1d82e774,"The Past, Present, and Future of Typological Databases in NLP",0.221102,"Typological information has the potential to be beneficial in the development
of NLP models, particularly for low-resource languages. Unfortunately, current
large-scale typological databases, notably WALS and Grambank, are inconsistent
both with each other and with other sources of typological information, such as
linguistic grammars. Some of these inconsistencies stem from coding errors or
linguistic variation, but many of the disagreements are due to the discrete
categorical nature of these databases. We shed light on this issue by
systematically exploring disagreements across typological databases and
resources, and their uses in NLP, covering the past and present. We next
investigate the future of such work, offering an argument that a continuous
view of typological features is clearly beneficial, echoing recommendations
from linguistics. We propose that such a view of typology has significant
potential in the future, including in language modeling in low-resource
scenarios.",None,-1
31e81c8f-86b1-4d6b-8956-74986a688e7b,A Neural Span-Based Continual Named Entity Recognition Model,0.442146,"Named Entity Recognition (NER) models capable of Continual Learning (CL) are
realistically valuable in areas where entity types continuously increase (e.g.,
personal assistants). Meanwhile the learning paradigm of NER advances to new
patterns such as the span-based methods. However, its potential to CL has not
been fully explored. In this paper, we propose SpanKL, a simple yet effective
Span-based model with Knowledge distillation (KD) to preserve memories and
multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence
labeling approaches, the inherently independent modeling in span and entity
level with the designed coherent optimization on SpanKL promotes its learning
at each incremental step and mitigates the forgetting. Experiments on synthetic
CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly
outperforms previous SoTA in many aspects, and obtains the smallest gap from CL
to the upper bound revealing its high practiced value. The code is available at
https://github.com/Qznan/SpanKL.",None,-1
602d636e-1c9a-4570-85f7-707379b369b9,BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset,0.709024,"In this paper, we introduce the BeaverTails dataset, aimed at fostering
research on safety alignment in large language models (LLMs). This dataset
uniquely separates annotations of helpfulness and harmlessness for
question-answering pairs, thus offering distinct perspectives on these crucial
attributes. In total, we have gathered safety meta-labels for 333,963
question-answer (QA) pairs and 361,903 pairs of expert comparison data for both
the helpfulness and harmlessness metrics. We further showcase applications of
BeaverTails in content moderation and reinforcement learning with human
feedback (RLHF), emphasizing its potential for practical safety measures in
LLMs. We believe this dataset provides vital resources for the community,
contributing towards the safe development and deployment of LLMs. Our project
page is available at the following URL:
https://sites.google.com/view/pku-beavertails.",None,-1
587a74b4-2949-4aeb-a12a-494fb91b15f2,Understanding Retrieval Augmentation for Long-Form Question Answering,0.910743,"We present a study of retrieval-augmented language models (LMs) on long-form
question answering. We analyze how retrieval augmentation impacts different
LMs, by comparing answers generated from models while using the same evidence
documents, and how differing quality of retrieval document set impacts the
answers generated from the same LM. We study various attributes of generated
answers (e.g., fluency, length, variance) with an emphasis on the attribution
of generated long-form answers to in-context evidence documents. We collect
human annotations of answer attribution and evaluate methods for automatically
judging attribution. Our study provides new insights on how retrieval
augmentation impacts long, knowledge-rich text generation of LMs. We further
identify attribution patterns for long text generation and analyze the main
culprits of attribution errors. Together, our analysis reveals how retrieval
augmentation impacts long knowledge-rich text generation and provide directions
for future work.",None,-1
fb8e08a6-993a-4c54-887d-432468305251,A Meta-Learning Perspective on Transformers for Causal Language Modeling,0.105733,"The Transformer architecture has become prominent in developing large causal
language models. However, mechanisms to explain its capabilities are not well
understood. Focused on the training process, here we establish a meta-learning
view of the Transformer architecture when trained for the causal language
modeling task, by explicating an inner optimization process within the
Transformer. Further, within the inner optimization, we discover and
theoretically analyze a special characteristic of the norms of learned token
representations within Transformer-based causal language models. Our analysis
is supported by experiments in various settings.",None,-1
e0958113-3c2f-40c9-8b67-64e1a7ec5914,Learning Knowledge-Rich Sequential Model for Planar Homography Estimation in Aerial Video,0.0813734,"This paper presents an unsupervised approach that leverages raw aerial videos
to learn to estimate planar homographic transformation between consecutive
video frames. Previous learning-based estimators work on pairs of images to
estimate their planar homographic transformations but suffer from severe
over-fitting issues, especially when applying over aerial videos. To address
this concern, we develop a sequential estimator that directly processes a
sequence of video frames and estimates their pairwise planar homographic
transformations in batches. We also incorporate a set of spatial-temporal
knowledge to regularize the learning of such a sequence-to-sequence model. We
collect a set of challenging aerial videos and compare the proposed method to
the alternative algorithms. Empirical studies suggest that our sequential model
achieves significant improvement over alternative image-based methods and the
knowledge-rich regularization further boosts our system performance. Our codes
and dataset could be found at https://github.com/Paul-LiPu/DeepVideoHomography",None,-1
8c4c04cc-ac74-45f3-88e9-5da8d69dc3d0,On the Neural Tangent Kernel of Equilibrium Models,0.889757,"This work studies the neural tangent kernel (NTK) of the deep equilibrium
(DEQ) model, a practical ``infinite-depth'' architecture which directly
computes the infinite-depth limit of a weight-tied network via root-finding.
Even though the NTK of a fully-connected neural network can be stochastic if
its width and depth both tend to infinity simultaneously, we show that
contrarily a DEQ model still enjoys a deterministic NTK despite its width and
depth going to infinity at the same time under mild conditions. Moreover, this
deterministic NTK can be found efficiently via root-finding.",None,-1
c5c65b7d-b2da-42fd-9b2d-cf3cd9909ca7,Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs,0.313875,"In comparative linguistics, colexification refers to the phenomenon of a
lexical form conveying two or more distinct meanings. Existing work on
colexification patterns relies on annotated word lists, limiting scalability
and usefulness in NLP. In contrast, we identify colexification patterns of more
than 2,000 concepts across 1,335 languages directly from an unannotated
parallel corpus. We then propose simple and effective methods to build
multilingual graphs from the colexification patterns: ColexNet and ColexNet+.
ColexNet's nodes are concepts and its edges are colexifications. In ColexNet+,
concept nodes are additionally linked through intermediate nodes, each
representing an ngram in one of 1,334 languages. We use ColexNet+ to train
$\overrightarrow{\mbox{ColexNet+}}$, high-quality multilingual embeddings that
are well-suited for transfer learning. In our experiments, we first show that
ColexNet achieves high recall on CLICS, a dataset of crosslingual
colexifications. We then evaluate $\overrightarrow{\mbox{ColexNet+}}$ on
roundtrip translation, sentence retrieval and sentence classification and show
that our embeddings surpass several transfer learning baselines. This
demonstrates the benefits of using colexification as a source of information in
multilingual NLP.",None,-1
6089f409-a4b9-43d2-9038-bed44cbf2470,Parallelizing Optical Flow Estimation on an Ultra-Low Power RISC-V Cluster for Nano-UAV Navigation,0.150126,"Optical flow estimation is crucial for autonomous navigation and localization
of unmanned aerial vehicles (UAV). On micro and nano UAVs, real-time
calculation of the optical flow is run on low power and resource-constrained
microcontroller units (MCUs). Thus, lightweight algorithms for optical flow
have been proposed targeting real-time execution on traditional single-core
MCUs. This paper introduces an efficient parallelization strategy for optical
flow computation targeting new-generation multicore low power RISC-V based
microcontroller units. Our approach enables higher frame rates at lower clock
speeds. It has been implemented and evaluated on the eight-core cluster of a
commercial octa-core MCU (GAP8) reaching a parallelization speedup factor of
7.21 allowing for a frame rate of 500 frames per second when running on a 50
MHz clock frequency. The proposed parallel algorithm significantly boosts the
camera frame rate on micro unmanned aerial vehicles, which enables higher
flight speeds: the maximum flight speed can be doubled, while using less than a
third of the clock frequency of previous single-core implementations.",None,-1
3e036f42-5db7-4612-ac4e-846b94b9cc70,A newborn embodied Turing test for view-invariant object recognition,0.285115,"Recent progress in artificial intelligence has renewed interest in building
machines that learn like animals. Almost all of the work comparing learning
across biological and artificial systems comes from studies where animals and
machines received different training data, obscuring whether differences
between animals and machines emerged from differences in learning mechanisms
versus training data. We present an experimental approach-a ""newborn embodied
Turing Test""-that allows newborn animals and machines to be raised in the same
environments and tested with the same tasks, permitting direct comparison of
their learning abilities. To make this platform, we first collected
controlled-rearing data from newborn chicks, then performed ""digital twin""
experiments in which machines were raised in virtual environments that mimicked
the rearing conditions of the chicks. We found that (1) machines (deep
reinforcement learning agents with intrinsic motivation) can spontaneously
develop visually guided preference behavior, akin to imprinting in newborn
chicks, and (2) machines are still far from newborn-level performance on object
recognition tasks. Almost all of the chicks developed view-invariant object
recognition, whereas the machines tended to develop view-dependent recognition.
The learning outcomes were also far more constrained in the chicks versus
machines. Ultimately, we anticipate that this approach will help researchers
develop embodied AI systems that learn like newborn animals.",None,-1
93412ab3-8512-445a-b218-f75ff4847c13,Crystal: Introspective Reasoners Reinforced with Self-Feedback,0.355776,"Extensive work has shown that the performance and interpretability of
commonsense reasoning can be improved via knowledge-augmented reasoning
methods, where the knowledge that underpins the reasoning process is explicitly
verbalized and utilized. However, existing implementations, including
""chain-of-thought"" and its variants, fall short in capturing the introspective
nature of knowledge required in commonsense reasoning, and in accounting for
the mutual adaptation between the generation and utilization of knowledge. We
propose a novel method to develop an introspective commonsense reasoner,
Crystal. To tackle commonsense problems, it first introspects for knowledge
statements related to the given question, and subsequently makes an informed
prediction that is grounded in the previously introspected knowledge. The
knowledge introspection and knowledge-grounded reasoning modes of the model are
tuned via reinforcement learning to mutually adapt, where the reward derives
from the feedback given by the model itself. Experiments show that Crystal
significantly outperforms both the standard supervised finetuning and
chain-of-thought distilled methods, and enhances the transparency of the
commonsense reasoning process. Our work ultimately validates the feasibility
and potential of reinforcing a neural model with self-feedback.",None,-1
96948dc8-e3bf-4bc7-aa73-4e8340370d2c,OMNI: Open-endedness via Models of human Notions of Interestingness,0.426649,"Open-ended algorithms aim to learn new, interesting behaviors forever. That
requires a vast environment search space, but there are thus infinitely many
possible tasks. Even after filtering for tasks the current agent can learn
(i.e., learning progress), countless learnable yet uninteresting tasks remain
(e.g., minor variations of previously learned tasks). An Achilles Heel of
open-endedness research is the inability to quantify (and thus prioritize)
tasks that are not just learnable, but also $\textit{interesting}$ (e.g.,
worthwhile and novel). We propose solving this problem by
$\textit{Open-endedness via Models of human Notions of Interestingness}$
(OMNI). The insight is that we can utilize foundation models (FMs) as a model
of interestingness (MoI), because they $\textit{already}$ internalize human
concepts of interestingness from training on vast amounts of human-generated
data, where humans naturally write about what they find interesting or boring.
We show that FM-based MoIs improve open-ended learning by focusing on tasks
that are both learnable $\textit{and interesting}$, outperforming baselines
based on uniform task sampling or learning progress alone. This approach has
the potential to dramatically advance the ability to intelligently select which
tasks to focus on next (i.e., auto-curricula), and could be seen as AI
selecting its own next task to learn, facilitating self-improving AI and
AI-Generating Algorithms. Project website at https://www.jennyzhangzt.com/omni/",None,-1
67c8f369-c9bc-4d30-b7f3-9b1b944264c6,Detecting Novelties with Empty Classes,0.0386022,"For open world applications, deep neural networks (DNNs) need to be aware of
previously unseen data and adaptable to evolving environments. Furthermore, it
is desirable to detect and learn novel classes which are not included in the
DNNs underlying set of semantic classes in an unsupervised fashion. The method
proposed in this article builds upon anomaly detection to retrieve
out-of-distribution (OoD) data as candidates for new classes. We thereafter
extend the DNN by $k$ empty classes and fine-tune it on the OoD data samples.
To this end, we introduce two loss functions, which 1) entice the DNN to assign
OoD samples to the empty classes and 2) to minimize the inner-class feature
distances between them. Thus, instead of ground truth which contains labels for
the different novel classes, the DNN obtains a single OoD label together with a
distance matrix, which is computed in advance. We perform several experiments
for image classification and semantic segmentation, which demonstrate that a
DNN can extend its own semantic space by multiple classes without having access
to ground truth.",None,-1
25abc3ad-7c7b-45ca-886a-012fb0c8c239,Forgetting-aware Linear Bias for Attentive Knowledge Tracing,0.439188,"Knowledge Tracing (KT) aims to track proficiency based on a question-solving
history, allowing us to offer a streamlined curriculum. Recent studies actively
utilize attention-based mechanisms to capture the correlation between questions
and combine it with the learner's characteristics for responses. However, our
empirical study shows that existing attention-based KT models neglect the
learner's forgetting behavior, especially as the interaction history becomes
longer. This problem arises from the bias that overprioritizes the correlation
of questions while inadvertently ignoring the impact of forgetting behavior.
This paper proposes a simple-yet-effective solution, namely Forgetting-aware
Linear Bias (FoLiBi), to reflect forgetting behavior as a linear bias. Despite
its simplicity, FoLiBi is readily equipped with existing attentive KT models by
effectively decomposing question correlations with forgetting behavior. FoLiBi
plugged with several KT models yields a consistent improvement of up to 2.58%
in AUC over state-of-the-art KT models on four benchmark datasets.",None,-1
83d4071d-8036-4b36-8dd6-6a608741e188,SpikeCodec: An End-to-end Learned Compression Framework for Spiking Camera,0.25871,"Recently, the bio-inspired spike camera with continuous motion recording
capability has attracted tremendous attention due to its ultra high temporal
resolution imaging characteristic. Such imaging feature results in huge data
storage and transmission burden compared to that of traditional camera, raising
severe challenge and imminent necessity in compression for spike camera
captured content. Existing lossy data compression methods could not be applied
for compressing spike streams efficiently due to integrate-and-fire
characteristic and binarized data structure. Considering the imaging principle
and information fidelity of spike cameras, we introduce an effective and robust
representation of spike streams. Based on this representation, we propose a
novel learned spike compression framework using scene recovery, variational
auto-encoder plus spike simulator. To our knowledge, it is the first
data-trained model for efficient and robust spike stream compression. Extensive
experimental results show that our method outperforms the conventional and
learning-based codecs, contributing a strong baseline for learned spike data
compression.",None,-1
4a537d76-3f2e-4b02-8f3e-f356ea6c18ee,Understanding the Distillation Process from Deep Generative Models to Tractable Probabilistic Circuits,0.806473,"Probabilistic Circuits (PCs) are a general and unified computational
framework for tractable probabilistic models that support efficient computation
of various inference tasks (e.g., computing marginal probabilities). Towards
enabling such reasoning capabilities in complex real-world tasks, Liu et al.
(2022) propose to distill knowledge (through latent variable assignments) from
less tractable but more expressive deep generative models. However, it is still
unclear what factors make this distillation work well. In this paper, we
theoretically and empirically discover that the performance of a PC can exceed
that of its teacher model. Therefore, instead of performing distillation from
the most expressive deep generative model, we study what properties the teacher
model and the PC should have in order to achieve good distillation performance.
This leads to a generic algorithmic improvement as well as other
data-type-specific ones over the existing latent variable distillation
pipeline. Empirically, we outperform SoTA TPMs by a large margin on challenging
image modeling benchmarks. In particular, on ImageNet32, PCs achieve 4.06
bits-per-dimension, which is only 0.34 behind variational diffusion models
(Kingma et al., 2021).",None,-1
7643ce67-347a-4820-99f6-c5c746c0bcca,Learning and Aggregating Lane Graphs for Urban Automated Driving,0.894731,"Lane graph estimation is an essential and highly challenging task in
automated driving and HD map learning. Existing methods using either onboard or
aerial imagery struggle with complex lane topologies, out-of-distribution
scenarios, or significant occlusions in the image space. Moreover, merging
overlapping lane graphs to obtain consistent large-scale graphs remains
difficult. To overcome these challenges, we propose a novel bottom-up approach
to lane graph estimation from aerial imagery that aggregates multiple
overlapping graphs into a single consistent graph. Due to its modular design,
our method allows us to address two complementary tasks: predicting
ego-respective successor lane graphs from arbitrary vehicle positions using a
graph neural network and aggregating these predictions into a consistent global
lane graph. Extensive experiments on a large-scale lane graph dataset
demonstrate that our approach yields highly accurate lane graphs, even in
regions with severe occlusions. The presented approach to graph aggregation
proves to eliminate inconsistent predictions while increasing the overall graph
quality. We make our large-scale urban lane graph dataset and code publicly
available at http://urbanlanegraph.cs.uni-freiburg.de.",None,-1
84996ddc-0a02-4de7-8fc6-4a91f9b49cf1,Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification,0.168199,"This article investigates the knowledge transfer from the RuQTopics dataset.
This Russian topical dataset combines a large sample number (361,560
single-label, 170,930 multi-label) with extensive class coverage (76 classes).
We have prepared this dataset from the ""Yandex Que"" raw data. By evaluating the
RuQTopics - trained models on the six matching classes of the Russian MASSIVE
subset, we have proved that the RuQTopics dataset is suitable for real-world
conversational tasks, as the Russian-only models trained on this dataset
consistently yield an accuracy around 85\% on this subset. We also have figured
out that for the multilingual BERT, trained on the RuQTopics and evaluated on
the same six classes of MASSIVE (for all MASSIVE languages), the language-wise
accuracy closely correlates (Spearman correlation 0.773 with p-value 2.997e-11)
with the approximate size of the pretraining BERT's data for the corresponding
language. At the same time, the correlation of the language-wise accuracy with
the linguistical distance from Russian is not statistically significant.",None,-1
9f13e428-a51d-4c12-9fe6-f7dba3b19f90,ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection,0.464425,"The task of multi-author writing style detection aims at finding any
positions of writing style change in a given text document. We formulate the
task as a natural language inference problem where two consecutive paragraphs
are paired. Our approach focuses on transitions between paragraphs while
truncating input tokens for the task. As backbone models, we employ different
Transformer-based encoders with warmup phase during training. We submit the
model version that outperforms baselines and other proposed model versions in
our experiments. For the easy and medium setups, we submit transition-focused
natural language inference based on DeBERTa with warmup training, and the same
model without transition for the hard setup.",None,-1
de549c2c-cb2d-484a-9b82-49a03eaeca62,MonoPGC: Monocular 3D Object Detection with Pixel Geometry Contexts,0.667286,"Monocular 3D object detection reveals an economical but challenging task in
autonomous driving. Recently center-based monocular methods have developed
rapidly with a great trade-off between speed and accuracy, where they usually
depend on the object center's depth estimation via 2D features. However, the
visual semantic features without sufficient pixel geometry information, may
affect the performance of clues for spatial 3D detection tasks. To alleviate
this, we propose MonoPGC, a novel end-to-end Monocular 3D object detection
framework with rich Pixel Geometry Contexts. We introduce the pixel depth
estimation as our auxiliary task and design depth cross-attention pyramid
module (DCPM) to inject local and global depth geometry knowledge into visual
features. In addition, we present the depth-space-aware transformer (DSAT) to
integrate 3D space position and depth-aware features efficiently. Besides, we
design a novel depth-gradient positional encoding (DGPE) to bring more distinct
pixel geometry contexts into the transformer for better object detection.
Extensive experiments demonstrate that our method achieves the state-of-the-art
performance on the KITTI dataset.",None,-1
e73c3c99-11a8-4f91-a437-4e4c7ce53f3b,Analytical reconstructions of full-scan multiple source-translation computed tomography under large field of views,0.293778,"This paper is to investigate the high-quality analytical reconstructions of
multiple source-translation computed tomography (mSTCT) under an extended field
of view (FOV). Under the larger FOVs, the previously proposed backprojection
filtration (BPF) algorithms for mSTCT, including D-BPF and S-BPF (their
differences are different derivate directions along the detector and source,
respectively), make some errors and artifacts in the reconstructed images due
to a backprojection weighting factor and the half-scan mode, which deviates
from the intention of mSTCT imaging. In this paper, to achieve reconstruction
with as little error as possible under the extremely extended FOV, we combine
the full-scan mSTCT (F-mSTCT) geometry with the previous BPF algorithms to
study the performance and derive a suitable redundancy-weighted function for
F-mSTCT. The experimental results indicate FS-BPF can get high-quality, stable
images under the extremely extended FOV of imaging a large object, though it
requires more projections than FD-BPF. Finally, for different practical
requirements in extending FOV imaging, we give suggestions on algorithm
selection.",None,-1
747bdf73-0295-4e40-a4f3-1e2bf8722ad6,"An Optimal, Universal and Agnostic Decoding Method for Message Reconstruction, Bio and Technosignature Detection",0.11958,"We present a signal reconstruction method for zero-knowledge one-way
communication channels in which a receiver aims to interpret a message sent by
an unknown source about which no prior knowledge is available and to which no
return message can be sent. Our reconstruction method is agnostic vis-\`a-vis
the arbitrarily chosen encoding-decoding scheme and other observer-dependent
characteristics, such as the arbitrarily chosen computation model or underlying
mathematical theory. We investigate how non-random messages may encode
information about the physical properties, such as dimension and length scales
of the space in which a signal or message may have been originally encoded,
embedded, or generated. We argue that our results have applications to life and
technosignature detection and to coding theory in general.",None,-1
07bfcb2e-bfc5-436e-a525-8b693b8f28b4,Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction,0.975747,"We present Mask-then-Fill, a flexible and effective data augmentation
framework for event extraction. Our approach allows for more flexible
manipulation of text and thus can generate more diverse data while keeping the
original event structure unchanged as much as possible. Specifically, it first
randomly masks out an adjunct sentence fragment and then infills a
variable-length text span with a fine-tuned infilling model. The main advantage
lies in that it can replace a fragment of arbitrary length in the text with
another fragment of variable length, compared to the existing methods which can
only replace a single word or a fixed-length fragment. On trigger and argument
extraction tasks, the proposed framework is more effective than baseline
methods and it demonstrates particularly strong results in the low-resource
setting. Our further analysis shows that it achieves a good balance between
diversity and distributional similarity.",None,-1
34e8dcfc-afd5-43cc-a527-160ba646fa22,Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks,0.450334,"Human emotion understanding is pivotal in making conversational technology
mainstream. We view speech emotion understanding as a perception task which is
a more realistic setting. With varying contexts (languages, demographics, etc.)
different share of people perceive the same speech segment as a non-unanimous
emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics
ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset
of multilingual speakers and multi-label regression target of 'emotion share'
or perception of that emotion. We demonstrate that the training scheme of
different foundation models dictates their effectiveness for tasks beyond
speech recognition, especially for non-semantic speech tasks like emotion
understanding. This is a very complex task due to multilingual speakers,
variability in the target labels, and inherent imbalance in the regression
dataset. Our results show that HuBERT-Large with a self-attention-based
light-weight sequence model provides 4.6% improvement over the reported
baseline.",None,-1
1c0b2616-5ec0-4368-b687-de6578c06c0e,Unsupervised Optical Flow Estimation with Dynamic Timing Representation for Spike Camera,0.0719983,"Efficiently selecting an appropriate spike stream data length to extract
precise information is the key to the spike vision tasks. To address this
issue, we propose a dynamic timing representation for spike streams. Based on
multi-layers architecture, it applies dilated convolutions on temporal
dimension to extract features on multi-temporal scales with few parameters. And
we design layer attention to dynamically fuse these features. Moreover, we
propose an unsupervised learning method for optical flow estimation in a
spike-based manner to break the dependence on labeled data. In addition, to
verify the robustness, we also build a spike-based synthetic validation dataset
for extreme scenarios in autonomous driving, denoted as SSES dataset. It
consists of various corner cases. Experiments show that our method can predict
optical flow from spike streams in different high-speed scenes, including real
scenes. For instance, our method gets $15\%$ and $19\%$ error reduction from
the best spike-based work, SCFlow, in $\Delta t=10$ and $\Delta t=20$
respectively which are the same settings as the previous works.",None,-1
def0f636-3760-4f7c-8f06-519909cb49c8,A Novel Convolutional Neural Network Architecture with a Continuous Symmetry,0.0846499,"This paper introduces a new Convolutional Neural Network (ConvNet)
architecture inspired by a class of partial differential equations (PDEs)
called quasi-linear hyperbolic systems. With comparable performance on the
image classification task, it allows for the modification of the weights via a
continuous group of symmetry. This is a significant shift from traditional
models where the architecture and weights are essentially fixed. We wish to
promote the (internal) symmetry as a new desirable property for a neural
network, and to draw attention to the PDE perspective in analyzing and
interpreting ConvNets in the broader Deep Learning community.",None,-1
64d01251-0dac-4419-9263-5b0123b1e204,Reconstructing Groups of People with Hypergraph Relational Reasoning,0.574279,"Due to the mutual occlusion, severe scale variation, and complex spatial
distribution, the current multi-person mesh recovery methods cannot produce
accurate absolute body poses and shapes in large-scale crowded scenes. To
address the obstacles, we fully exploit crowd features for reconstructing
groups of people from a monocular image. A novel hypergraph relational
reasoning network is proposed to formulate the complex and high-order relation
correlations among individuals and groups in the crowd. We first extract
compact human features and location information from the original
high-resolution image. By conducting the relational reasoning on the extracted
individual features, the underlying crowd collectiveness and interaction
relationship can provide additional group information for the reconstruction.
Finally, the updated individual features and the localization information are
used to regress human meshes in camera coordinates. To facilitate the network
training, we further build pseudo ground-truth on two crowd datasets, which may
also promote future research on pose estimation and human behavior
understanding in crowded scenes. The experimental results show that our
approach outperforms other baseline methods both in crowded and common
scenarios. The code and datasets are publicly available at
https://github.com/boycehbz/GroupRec.",None,-1
fa04b7b9-b09b-4400-bfa8-f66f422c7990,Algorithm-assisted discovery of an intrinsic order among mathematical constants,0.425472,"In recent decades, a growing number of discoveries in fields of mathematics
have been assisted by computer algorithms, primarily for exploring large
parameter spaces that humans would take too long to investigate. As computers
and algorithms become more powerful, an intriguing possibility arises - the
interplay between human intuition and computer algorithms can lead to
discoveries of novel mathematical concepts that would otherwise remain elusive.
To realize this perspective, we have developed a massively parallel computer
algorithm that discovers an unprecedented number of continued fraction formulas
for fundamental mathematical constants. The sheer number of formulas discovered
by the algorithm unveils a novel mathematical structure that we call the
conservative matrix field. Such matrix fields (1) unify thousands of existing
formulas, (2) generate infinitely many new formulas, and most importantly, (3)
lead to unexpected relations between different mathematical constants,
including multiple integer values of the Riemann zeta function. Conservative
matrix fields also enable new mathematical proofs of irrationality. In
particular, we can use them to generalize the celebrated proof by Ap\'ery for
the irrationality of $\zeta(3)$. Utilizing thousands of personal computers
worldwide, our computer-supported research strategy demonstrates the power of
experimental mathematics, highlighting the prospects of large-scale
computational approaches to tackle longstanding open problems and discover
unexpected connections across diverse fields of science.",None,-1
83907ef1-2fa0-41da-a391-10746d3e7f5d,GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems,0.78489,"There has been considerable divergence of opinion on the reasoning abilities
of Large Language Models (LLMs). While the initial optimism that reasoning
might emerge automatically with scale has been tempered thanks to a slew of
counterexamples, a wide spread belief in their iterative self-critique
capabilities persists. In this paper, we set out to systematically investigate
the effectiveness of iterative prompting of LLMs in the context of Graph
Coloring, a canonical NP-complete reasoning problem that is related to
propositional satisfiability as well as practical problems like scheduling and
allocation. We present a principled empirical study of the performance of GPT4
in solving graph coloring instances or verifying the correctness of candidate
colorings. In iterative modes, we experiment with the model critiquing its own
answers and an external correct reasoner verifying proposed solutions. In both
cases, we analyze whether the content of the criticisms actually affects bottom
line performance. The study seems to indicate that (i) LLMs are bad at solving
graph coloring instances (ii) they are no better at verifying a solution--and
thus are not effective in iterative modes with LLMs critiquing LLM-generated
solutions (iii) the correctness and content of the criticisms--whether by LLMs
or external solvers--seems largely irrelevant to the performance of iterative
prompting. We show that the observed increase in effectiveness is largely due
to the correct solution being fortuitously present in the top-k completions of
the prompt (and being recognized as such by an external verifier). Our results
thus call into question claims about the self-critiquing capabilities of state
of the art LLMs.",None,-1
c7592ff1-b49b-404f-a677-14910c30e588,Towards accurate instance segmentation in large-scale LiDAR point clouds,0.664284,"Panoptic segmentation is the combination of semantic and instance
segmentation: assign the points in a 3D point cloud to semantic categories and
partition them into distinct object instances. It has many obvious applications
for outdoor scene understanding, from city mapping to forest management.
Existing methods struggle to segment nearby instances of the same semantic
category, like adjacent pieces of street furniture or neighbouring trees, which
limits their usability for inventory- or management-type applications that rely
on object instances. This study explores the steps of the panoptic segmentation
pipeline concerned with clustering points into object instances, with the goal
to alleviate that bottleneck. We find that a carefully designed clustering
strategy, which leverages multiple types of learned point embeddings,
significantly improves instance segmentation. Experiments on the NPM3D urban
mobile mapping dataset and the FOR-instance forest dataset demonstrate the
effectiveness and versatility of the proposed strategy.",None,-1
1dc942bc-2167-4221-b713-6fdc5d3536c2,Facial Affective Behavior Analysis Method for 5th ABAW Competition,0.986902,"Facial affective behavior analysis is important for human-computer
interaction. 5th ABAW competition includes three challenges from Aff-Wild2
database. Three common facial affective analysis tasks are involved, i.e.
valence-arousal estimation, expression classification, action unit recognition.
For the three challenges, we construct three different models to solve the
corresponding problems to improve the results, such as data unbalance and data
noise. For the experiments of three challenges, we train the models on the
provided training data and validate the models on the validation data.",None,-1
b6e12116-5756-4b6e-a705-34ce83116e72,"Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors",0.994309,"ChatGPT has stimulated the research boom in the field of large language
models. In this paper, we assess the capabilities of ChatGPT from four
perspectives including Performance, Evaluation Criteria, Robustness and Error
Types. Specifically, we first evaluate ChatGPT's performance on 17 datasets
with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought
scenarios, and find a huge performance gap between ChatGPT and SOTA results.
Next, we rethink this gap and propose a soft-matching strategy for evaluation
to more accurately reflect ChatGPT's performance. Then, we analyze the
robustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely
outputs invalid responses; 2) Irrelevant context and long-tail target types
greatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the
subject-object relationships in RE task. Finally, we analyze the errors of
ChatGPT, and find that ""unannotated spans"" is the most dominant error type.
This raises concerns about the quality of annotated data, and indicates the
possibility of annotating data with ChatGPT. The data and code are released at
Github site.",None,-1
4f3577c9-11a6-4ab2-a7c5-dbf65bc29050,Multi-view Cross-Modality MR Image Translation for Vestibular Schwannoma and Cochlea Segmentation,0.552344,"In this work, we propose a multi-view image translation framework, which can
translate contrast-enhanced T1 (ceT1) MR imaging to high-resolution T2 (hrT2)
MR imaging for unsupervised vestibular schwannoma and cochlea segmentation. We
adopt two image translation models in parallel that use a pixel-level
consistent constraint and a patch-level contrastive constraint, respectively.
Thereby, we can augment pseudo-hrT2 images reflecting different perspectives,
which eventually lead to a high-performing segmentation model. Our experimental
results on the CrossMoDA challenge show that the proposed method achieved
enhanced performance on the vestibular schwannoma and cochlea segmentation.",None,-1
26dff59b-2970-4b1d-a668-948550b52777,RECLIP: Resource-efficient CLIP by Training with Small Images,0.267802,"We present RECLIP (Resource-efficient CLIP), a simple method that minimizes
computational resource footprint for CLIP (Contrastive Language Image
Pretraining). Inspired by the notion of coarse-to-fine in computer vision, we
leverage small images to learn from large-scale language supervision
efficiently, and finetune the model with high-resolution data in the end. Since
the complexity of the vision transformer heavily depends on input image size,
our approach significantly reduces the training resource requirements both in
theory and in practice. Using the same batch size and training epoch, RECLIP
achieves highly competitive zero-shot classification and image-text retrieval
accuracy with 6 to 8x less computational resources and 7 to 9x fewer FLOPs than
the baseline. Compared to the state-of-the-art contrastive learning methods,
RECLIP demonstrates 5 to 59x training resource savings while maintaining highly
competitive zero-shot classification and retrieval performance. Finally, RECLIP
matches the state of the art in transfer learning to open-vocabulary detection
tasks, achieving 32 APr on LVIS. We hope this work will pave the path for the
broader research community to explore language supervised pretraining in
resource-friendly settings.",None,-1
5eed57a0-b8c0-496c-980c-3503355e47b6,"Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques",0.516021,"This paper describes the participation of team QUST in the SemEval2023 task
3. The monolingual models are first evaluated with the under-sampling of the
majority classes in the early stage of the task. Then, the pre-trained
multilingual model is fine-tuned with a combination of the class weights and
the sample weights. Two different fine-tuning strategies, the task-agnostic and
the task-dependent, are further investigated. All experiments are conducted
under the 10-fold cross-validation, the multilingual approaches are superior to
the monolingual ones. The submitted system achieves the second best in Italian
and Spanish (zero-shot) in subtask-1.",None,-1
10483fb6-f8f6-46db-a34f-e683a98f72c6,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,0.753719,"Visual question answering on document images that contain textual, visual,
and layout information, called document VQA, has received much attention
recently. Although many datasets have been proposed for developing document VQA
systems, most of the existing datasets focus on understanding the content
relationships within a single image and not across multiple images. In this
study, we propose a new multi-image document VQA dataset, SlideVQA, containing
2.6k+ slide decks composed of 52k+ slide images and 14.5k questions about a
slide deck. SlideVQA requires complex reasoning, including single-hop,
multi-hop, and numerical reasoning, and also provides annotated arithmetic
expressions of numerical answers for enhancing the ability of numerical
reasoning. Moreover, we developed a new end-to-end document VQA model that
treats evidence selection and question answering in a unified
sequence-to-sequence format. Experiments on SlideVQA show that our model
outperformed existing state-of-the-art QA models, but that it still has a large
gap behind human performance. We believe that our dataset will facilitate
research on document VQA.",None,-1
d9c409a8-3d5d-42f9-a173-ab199f49a135,Diversity Enhanced Narrative Question Generation for Storybooks,0.145919,"Question generation (QG) from a given context can enhance comprehension,
engagement, assessment, and overall efficacy in learning or conversational
environments. Despite recent advancements in QG, the challenge of enhancing or
measuring the diversity of generated questions often remains unaddressed. In
this paper, we introduce a multi-question generation model (mQG), which is
capable of generating multiple, diverse, and answerable questions by focusing
on context and questions. To validate the answerability of the generated
questions, we employ a SQuAD2.0 fine-tuned question answering model,
classifying the questions as answerable or not. We train and evaluate mQG on
the FairytaleQA dataset, a well-structured QA dataset based on storybooks, with
narrative questions. We further apply a zero-shot adaptation on the TellMeWhy
and SQuAD1.1 datasets. mQG shows promising results across various evaluation
metrics, among strong baselines.",None,-1
4445fcf8-3c6b-4436-a579-2f0492b70940,Computing a human-like reaction time metric from stable recurrent vision models,0.372573,"The meteoric rise in the adoption of deep neural networks as computational
models of vision has inspired efforts to ""align"" these models with humans. One
dimension of interest for alignment includes behavioral choices, but moving
beyond characterizing choice patterns to capturing temporal aspects of visual
decision-making has been challenging. Here, we sketch a general-purpose
methodology to construct computational accounts of reaction times from a
stimulus-computable, task-optimized model. Specifically, we introduce a novel
metric leveraging insights from subjective logic theory summarizing evidence
accumulation in recurrent vision models. We demonstrate that our metric aligns
with patterns of human reaction times for stimulus manipulations across four
disparate visual decision-making tasks spanning perceptual grouping, mental
simulation, and scene categorization. This work paves the way for exploring the
temporal alignment of model and human visual strategies in the context of
various other cognitive tasks toward generating testable hypotheses for
neuroscience. Links to the code and data can be found on the project page:
https://serre-lab.github.io/rnn_rts_site.",None,-1
9dd3315c-7a43-49c6-9fd4-81b5896c1095,Hypothesis Testing and Machine Learning: Interpreting Variable Effects in Deep Artificial Neural Networks using Cohen's f2,0.108701,"Deep artificial neural networks show high predictive performance in many
fields, but they do not afford statistical inferences and their black-box
operations are too complicated for humans to comprehend. Because positing that
a relationship exists is often more important than prediction in scientific
experiments and research models, machine learning is far less frequently used
than inferential statistics. Additionally, statistics calls for improving the
test of theory by showing the magnitude of the phenomena being studied. This
article extends current XAI methods and develops a model agnostic hypothesis
testing framework for machine learning. First, Fisher's variable permutation
algorithm is tweaked to compute an effect size measure equivalent to Cohen's f2
for OLS regression models. Second, the Mann-Kendall test of monotonicity and
the Theil-Sen estimator is applied to Apley's accumulated local effect plots to
specify a variable's direction of influence and statistical significance. The
usefulness of this approach is demonstrated on an artificial data set and a
social survey with a Python sandbox implementation.",None,-1
0a85158c-8c16-4ff1-a174-1961d1237724,N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics,0.0324808,"We propose a self-correction mechanism for Large Language Models (LLMs) to
mitigate issues such as toxicity and fact hallucination. This method involves
refining model outputs through an ensemble of critics and the model's own
feedback. Drawing inspiration from human behavior, we explore whether LLMs can
emulate the self-correction process observed in humans who often engage in
self-reflection and seek input from others to refine their understanding of
complex topics. Our approach is model-agnostic and can be applied across
various domains to enhance trustworthiness by addressing fairness, bias, and
robustness concerns. We consistently observe performance improvements in LLMs
for reducing toxicity and correcting factual errors.",None,-1
fb8ab190-75b2-4439-8209-d66d6672066e,Semi-MAE: Masked Autoencoders for Semi-supervised Vision Transformers,0.101803,"Vision Transformer (ViT) suffers from data scarcity in semi-supervised
learning (SSL). To alleviate this issue, inspired by masked autoencoder (MAE),
which is a data-efficient self-supervised learner, we propose Semi-MAE, a pure
ViT-based SSL framework consisting of a parallel MAE branch to assist the
visual representation learning and make the pseudo labels more accurate. The
MAE branch is designed as an asymmetric architecture consisting of a
lightweight decoder and a shared-weights encoder. We feed the weakly-augmented
unlabeled data with a high masking ratio to the MAE branch and reconstruct the
missing pixels. Semi-MAE achieves 75.9% top-1 accuracy on ImageNet with 10%
labels, surpassing prior state-of-the-art in semi-supervised image
classification. In addition, extensive experiments demonstrate that Semi-MAE
can be readily used for other ViT models and masked image modeling methods.",None,-1
84b0be41-a512-4041-a2de-a47e2f68ffa5,NormBank: A Knowledge Bank of Situational Social Norms,0.997417,"We present NormBank, a knowledge bank of 155k situational norms. This
resource is designed to ground flexible normative reasoning for interactive,
assistive, and collaborative AI systems. Unlike prior commonsense resources,
NormBank grounds each inference within a multivalent sociocultural frame, which
includes the setting (e.g., restaurant), the agents' contingent roles (waiter,
customer), their attributes (age, gender), and other physical, social, and
cultural constraints (e.g., the temperature or the country of operation). In
total, NormBank contains 63k unique constraints from a taxonomy that we
introduce and iteratively refine here. Constraints then apply in different
combinations to frame social norms. Under these manipulations, norms are
non-monotonic - one can cancel an inference by updating its frame even
slightly. Still, we find evidence that neural models can help reliably extend
the scope and coverage of NormBank. We further demonstrate the utility of this
resource with a series of transfer experiments.",None,-1
5e938ceb-2c74-4873-80ed-91dc58c4085d,OBJECT 3DIT: Language-guided 3D-aware Image Editing,0.843393,"Existing image editing tools, while powerful, typically disregard the
underlying 3D geometry from which the image is projected. As a result, edits
made using these tools may become detached from the geometry and lighting
conditions that are at the foundation of the image formation process. In this
work, we formulate the newt ask of language-guided 3D-aware editing, where
objects in an image should be edited according to a language instruction in
context of the underlying 3D scene. To promote progress towards this goal, we
release OBJECT: a dataset consisting of 400K editing examples created from
procedurally generated 3D scenes. Each example consists of an input image,
editing instruction in language, and the edited image. We also introduce 3DIT :
single and multi-task models for four editing tasks. Our models show impressive
abilities to understand the 3D composition of entire scenes, factoring in
surrounding objects, surfaces, lighting conditions, shadows, and
physically-plausible object configurations. Surprisingly, training on only
synthetic scenes from OBJECT, editing capabilities of 3DIT generalize to
real-world images.",None,-1
599f3d15-1f02-4045-9364-2058c295cb35,Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement,0.712303,"Conversational agents participating in multi-party interactions face
significant challenges in dialogue state tracking, since the identity of the
speaker adds significant contextual meaning. It is common to utilise
diarisation models to identify the speaker. However, it is not clear if these
are accurate enough to correctly identify specific conversational events such
as agreement or disagreement during a real-time interaction. This study uses a
cooperative quiz, where the conversational agent acts as quiz-show host, to
determine whether diarisation or a frequency-and-proximity-based method is more
accurate at determining agreement, and whether this translates to feelings of
engagement from the players. Experimental results show that our procedural
system was more engaging to players, and was more accurate at detecting
agreement, reaching an average accuracy of 0.44 compared to 0.28 for the
diarised system.",None,-1
cdf4ca53-e8ec-4da4-bc25-0217dc24a71e,Visualizing Semiotics in Generative Adversarial Networks,0.0699641,"We perform a set of experiments to demonstrate that images generated using a
Generative Adversarial Network can be modified using 'semiotics.' We show that
just as physical attributes such as the hue and saturation of an image can be
modified, so too can its non-physical, abstract properties using our method.
For example, the design of a flight attendant's uniform may be modified to look
more 'alert,' less 'austere,' or more 'practical.' The form of a house can be
modified to appear more 'futuristic,' a car more 'friendly' a pair of sneakers,
'evil.' Our method uncovers latent visual iconography associated with the
semiotic property of interest, enabling a process of visual form-finding using
abstract concepts. Our approach is iterative and allows control over the degree
of attribute presence and can be used to aid the design process to yield
emergent visual concepts.",None,-1
3da19be8-9a88-43cb-9613-5c76c3991b0b,Grab What You Need: Rethinking Complex Table Structure Recognition with Flexible Components Deliberation,0.225087,"Recently, Table Structure Recognition (TSR) task, aiming at identifying table
structure into machine readable formats, has received increasing interest in
the community. While impressive success, most single table component-based
methods can not perform well on unregularized table cases distracted by not
only complicated inner structure but also exterior capture distortion. In this
paper, we raise it as Complex TSR problem, where the performance degeneration
of existing methods is attributable to their inefficient component usage and
redundant post-processing. To mitigate it, we shift our perspective from table
component extraction towards the efficient multiple components leverage, which
awaits further exploration in the field. Specifically, we propose a seminal
method, termed GrabTab, equipped with newly proposed Component Deliberator.
Thanks to its progressive deliberation mechanism, our GrabTab can flexibly
accommodate to most complex tables with reasonable components selected but
without complicated post-processing involved. Quantitative experimental results
on public benchmarks demonstrate that our method significantly outperforms the
state-of-the-arts, especially under more challenging scenes.",None,-1
ad05b4fc-fe02-4cb7-8587-d75a7a3a8875,Egocentric Audio-Visual Object Localization,0.878794,"Humans naturally perceive surrounding scenes by unifying sound and sight in a
first-person view. Likewise, machines are advanced to approach human
intelligence by learning with multisensory inputs from an egocentric
perspective. In this paper, we explore the challenging egocentric audio-visual
object localization task and observe that 1) egomotion commonly exists in
first-person recordings, even within a short duration; 2) The out-of-view sound
components can be created while wearers shift their attention. To address the
first problem, we propose a geometry-aware temporal aggregation module to
handle the egomotion explicitly. The effect of egomotion is mitigated by
estimating the temporal geometry transformation and exploiting it to update
visual representations. Moreover, we propose a cascaded feature enhancement
module to tackle the second issue. It improves cross-modal localization
robustness by disentangling visually-indicated audio representation. During
training, we take advantage of the naturally available audio-visual temporal
synchronization as the ``free'' self-supervision to avoid costly labeling. We
also annotate and create the Epic Sounding Object dataset for evaluation
purposes. Extensive experiments show that our method achieves state-of-the-art
localization performance in egocentric videos and can be generalized to diverse
audio-visual scenes.",None,-1
a06499af-345f-4550-b7e4-d71bfa7f4a83,Adapting a Language Model While Preserving its General Knowledge,0.467276,"Domain-adaptive pre-training (or DA-training for short), also known as
post-training, aims to train a pre-trained general-purpose language model (LM)
using an unlabeled corpus of a particular domain to adapt the LM so that
end-tasks in the domain can give improved performances. However, existing
DA-training methods are in some sense blind as they do not explicitly identify
what knowledge in the LM should be preserved and what should be changed by the
domain corpus. This paper shows that the existing methods are suboptimal and
proposes a novel method to perform a more informed adaptation of the knowledge
in the LM by (1) soft-masking the attention heads based on their importance to
best preserve the general knowledge in the LM and (2) contrasting the
representations of the general and the full (both general and domain knowledge)
to learn an integrated representation with both general and domain-specific
knowledge. Experimental results will demonstrate the effectiveness of the
proposed approach.",None,-1
fe8d22b4-d3e4-44c7-bdff-60a5adc4beee,TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement,0.686279,"Speech enhancement models have greatly progressed in recent years, but still
show limits in perceptual quality of their speech outputs. We propose an
objective for perceptual quality based on temporal acoustic parameters. These
are fundamental speech features that play an essential role in various
applications, including speaker recognition and paralinguistic analysis. We
provide a differentiable estimator for four categories of low-level acoustic
descriptors involving: frequency-related parameters, energy or
amplitude-related parameters, spectral balance parameters, and temporal
features. Unlike prior work that looks at aggregated acoustic parameters or a
few categories of acoustic parameters, our temporal acoustic parameter (TAP)
loss enables auxiliary optimization and improvement of many fine-grain speech
characteristics in enhancement workflows. We show that adding TAPLoss as an
auxiliary objective in speech enhancement produces speech with improved
perceptual quality and intelligibility. We use data from the Deep Noise
Suppression 2020 Challenge to demonstrate that both time-domain models and
time-frequency domain models can benefit from our method.",None,-1
6e2947dc-efaf-4041-b39d-e620749ce2f1,Improving Seq2Seq Grammatical Error Correction via Decoding Interventions,0.745858,"The sequence-to-sequence (Seq2Seq) approach has recently been widely used in
grammatical error correction (GEC) and shows promising performance. However,
the Seq2Seq GEC approach still suffers from two issues. First, a Seq2Seq GEC
model can only be trained on parallel data, which, in GEC task, is often noisy
and limited in quantity. Second, the decoder of a Seq2Seq GEC model lacks an
explicit awareness of the correctness of the token being generated. In this
paper, we propose a unified decoding intervention framework that employs an
external critic to assess the appropriateness of the token to be generated
incrementally, and then dynamically influence the choice of the next token. We
discover and investigate two types of critics: a pre-trained left-to-right
language model critic and an incremental target-side grammatical error detector
critic. Through extensive experiments on English and Chinese datasets, our
framework consistently outperforms strong baselines and achieves results
competitive with state-of-the-art methods.",None,-1
3c38df13-184a-481f-ab75-37ab88fcbb11,DreamIdentity: Improved Editability for Efficient Face-identity Preserved Image Generation,0.877061,"While large-scale pre-trained text-to-image models can synthesize diverse and
high-quality human-centric images, an intractable problem is how to preserve
the face identity for conditioned face images. Existing methods either require
time-consuming optimization for each face-identity or learning an efficient
encoder at the cost of harming the editability of models. In this work, we
present an optimization-free method for each face identity, meanwhile keeping
the editability for text-to-image models. Specifically, we propose a novel
face-identity encoder to learn an accurate representation of human faces, which
applies multi-scale face features followed by a multi-embedding projector to
directly generate the pseudo words in the text embedding space. Besides, we
propose self-augmented editability learning to enhance the editability of
models, which is achieved by constructing paired generated face and edited face
images using celebrity names, aiming at transferring mature ability of
off-the-shelf text-to-image models in celebrity faces to unseen faces.
Extensive experiments show that our methods can generate identity-preserved
images under different scenes at a much faster speed.",None,-1
f5c1fca0-b930-413d-a1a2-5760baba3d8f,Face Generation from Textual Features using Conditionally Trained Inputs to Generative Adversarial Networks,0.0628717,"Generative Networks have proved to be extremely effective in image
restoration and reconstruction in the past few years. Generating faces from
textual descriptions is one such application where the power of generative
algorithms can be used. The task of generating faces can be useful for a number
of applications such as finding missing persons, identifying criminals, etc.
This paper discusses a novel approach to generating human faces given a textual
description regarding the facial features. We use the power of state of the art
natural language processing models to convert face descriptions into learnable
latent vectors which are then fed to a generative adversarial network which
generates faces corresponding to those features. While this paper focuses on
high level descriptions of faces only, the same approach can be tailored to
generate any image based on fine grained textual features.",None,-1
1008d4ad-0815-4c84-bd59-a06e3112d848,For the Underrepresented in Gender Bias Research: Chinese Name Gender Prediction with Heterogeneous Graph Attention Network,0.0397292,"Achieving gender equality is an important pillar for humankind's sustainable
future. Pioneering data-driven gender bias research is based on large-scale
public records such as scientific papers, patents, and company registrations,
covering female researchers, inventors and entrepreneurs, and so on. Since
gender information is often missing in relevant datasets, studies rely on tools
to infer genders from names. However, available open-sourced Chinese
gender-guessing tools are not yet suitable for scientific purposes, which may
be partially responsible for female Chinese being underrepresented in
mainstream gender bias research and affect their universality. Specifically,
these tools focus on character-level information while overlooking the fact
that the combinations of Chinese characters in multi-character names, as well
as the components and pronunciations of characters, convey important messages.
As a first effort, we design a Chinese Heterogeneous Graph Attention (CHGAT)
model to capture the heterogeneity in component relationships and incorporate
the pronunciations of characters. Our model largely surpasses current tools and
also outperforms the state-of-the-art algorithm. Last but not least, the most
popular Chinese name-gender dataset is single-character based with far less
female coverage from an unreliable source, naturally hindering relevant
studies. We open-source a more balanced multi-character dataset from an
official source together with our code, hoping to help future research
promoting gender equality.",None,-1
2792bfb4-fab3-407b-adc3-78a1487e7b03,Enrichment of the NLST and NSCLC-Radiomics computed tomography collections with AI-derived annotations,0.508335,"Public imaging datasets are critical for the development and evaluation of
automated tools in cancer imaging. Unfortunately, many do not include
annotations or image-derived features, complicating their downstream analysis.
Artificial intelligence-based annotation tools have been shown to achieve
acceptable performance and thus can be used to automatically annotate large
datasets. As part of the effort to enrich public data available within NCI
Imaging Data Commons (IDC), here we introduce AI-generated annotations for two
collections of computed tomography images of the chest, NSCLC-Radiomics, and
the National Lung Screening Trial. Using publicly available AI algorithms we
derived volumetric annotations of thoracic organs at risk, their corresponding
radiomics features, and slice-level annotations of anatomical landmarks and
regions. The resulting annotations are publicly available within IDC, where the
DICOM format is used to harmonize the data and achieve FAIR principles. The
annotations are accompanied by cloud-enabled notebooks demonstrating their use.
This study reinforces the need for large, publicly accessible curated datasets
and demonstrates how AI can be used to aid in cancer imaging.",None,-1
11a96912-3143-4c1c-a6cf-e8f82445a917,Guided Focal Stack Refinement Network for Light Field Salient Object Detection,0.655784,"Light field salient object detection (SOD) is an emerging research direction
attributed to the richness of light field data. However, most existing methods
lack effective handling of focal stacks, therefore making the latter involved
in a lot of interfering information and degrade the performance of SOD. To
address this limitation, we propose to utilize multi-modal features to refine
focal stacks in a guided manner, resulting in a novel guided focal stack
refinement network called GFRNet. To this end, we propose a guided refinement
and fusion module (GRFM) to refine focal stacks and aggregate multi-modal
features. In GRFM, all-in-focus (AiF) and depth modalities are utilized to
refine focal stacks separately, leading to two novel sub-modules for different
modalities, namely AiF-based refinement module (ARM) and depth-based refinement
module (DRM). Such refinement modules enhance structural and positional
information of salient objects in focal stacks, and are able to improve SOD
accuracy. Experimental results on four benchmark datasets demonstrate the
superiority of our GFRNet model against 12 state-of-the-art models.",None,-1
478a5838-dc67-4b4f-a345-758287ac6061,Evaluation of Induced Expert Knowledge in Causal Structure Learning by NOTEARS,0.438177,"Causal modeling provides us with powerful counterfactual reasoning and
interventional mechanism to generate predictions and reason under various
what-if scenarios. However, causal discovery using observation data remains a
nontrivial task due to unobserved confounding factors, finite sampling, and
changes in the data distribution. These can lead to spurious cause-effect
relationships. To mitigate these challenges in practice, researchers augment
causal learning with known causal relations. The goal of the paper is to study
the impact of expert knowledge on causal relations in the form of additional
constraints used in the formulation of the nonparametric NOTEARS. We provide a
comprehensive set of comparative analyses of biasing the model using different
types of knowledge. We found that (i) knowledge that corrects the mistakes of
the NOTEARS model can lead to statistically significant improvements, (ii)
constraints on active edges have a larger positive impact on causal discovery
than inactive edges, and surprisingly, (iii) the induced knowledge does not
correct on average more incorrect active and/or inactive edges than expected.
We also demonstrate the behavior of the model and the effectiveness of domain
knowledge on a real-world dataset.",None,-1
57d8c2f5-16da-4cfe-b90e-925a092c8b32,Predicting Privacy Preferences for Smart Devices as Norms,0.463227,"Smart devices, such as smart speakers, are becoming ubiquitous, and users
expect these devices to act in accordance with their preferences. In
particular, since these devices gather and manage personal data, users expect
them to adhere to their privacy preferences. However, the current approach of
gathering these preferences consists in asking the users directly, which
usually triggers automatic responses failing to capture their true preferences.
In response, in this paper we present a collaborative filtering approach to
predict user preferences as norms. These preference predictions can be readily
adopted or can serve to assist users in determining their own preferences.
Using a dataset of privacy preferences of smart assistant users, we test the
accuracy of our predictions.",None,-1
052b21b5-50ae-4c02-8665-dd0a1d8d06d3,Rig Inversion by Training a Differentiable Rig Function,0.0995843,"Rig inversion is the problem of creating a method that can find the rig
parameter vector that best approximates a given input mesh. In this paper we
propose to solve this problem by first obtaining a differentiable rig function
by training a multi layer perceptron to approximate the rig function. This
differentiable rig function can then be used to train a deep learning model of
rig inversion.",None,-1
f09e2bbe-7964-424f-9d19-69456968b634,SA-BEV: Generating Semantic-Aware Bird's-Eye-View Feature for Multi-view 3D Object Detection,0.820325,"Recently, the pure camera-based Bird's-Eye-View (BEV) perception provides a
feasible solution for economical autonomous driving. However, the existing
BEV-based multi-view 3D detectors generally transform all image features into
BEV features, without considering the problem that the large proportion of
background information may submerge the object information. In this paper, we
propose Semantic-Aware BEV Pooling (SA-BEVPool), which can filter out
background information according to the semantic segmentation of image features
and transform image features into semantic-aware BEV features. Accordingly, we
propose BEV-Paste, an effective data augmentation strategy that closely matches
with semantic-aware BEV feature. In addition, we design a Multi-Scale
Cross-Task (MSCT) head, which combines task-specific and cross-task information
to predict depth distribution and semantic segmentation more accurately,
further improving the quality of semantic-aware BEV feature. Finally, we
integrate the above modules into a novel multi-view 3D object detection
framework, namely SA-BEV. Experiments on nuScenes show that SA-BEV achieves
state-of-the-art performance. Code has been available at
https://github.com/mengtan00/SA-BEV.git.",None,-1
b7ace227-9bbd-4a69-ba35-293c409328a6,PIVOINE: Instruction Tuning for Open-world Information Extraction,0.660573,"We consider the problem of Open-world Information Extraction (Open-world IE),
which extracts comprehensive entity profiles from unstructured texts. Different
from the conventional closed-world setting of Information Extraction (IE),
Open-world IE considers a more general situation where entities and relations
could be beyond a predefined ontology. More importantly, we seek to develop a
large language model (LLM) that is able to perform Open-world IE to extract
desirable entity profiles characterized by (possibly fine-grained) natural
language instructions. We achieve this by finetuning LLMs using instruction
tuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction
tuning dataset for Open-world IE enriched with a comprehensive corpus,
extensive annotations, and diverse instructions. We finetune the pretrained
BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE
with strong instruction-following capabilities. Our experiments demonstrate
that PIVOINE significantly outperforms traditional closed-world methods and
other LLM baselines, displaying impressive generalization capabilities on both
unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as
a promising solution to tackle the open-world challenge in IE effectively.",None,-1
c84292f3-7290-4c79-9533-803612e47c0c,Learn to Not Link: Exploring NIL Prediction in Entity Linking,0.467514,"Entity linking models have achieved significant success via utilizing
pretrained language models to capture semantic features. However, the NIL
prediction problem, which aims to identify mentions without a corresponding
entity in the knowledge base, has received insufficient attention. We
categorize mentions linking to NIL into Missing Entity and Non-Entity Phrase,
and propose an entity linking dataset NEL that focuses on the NIL prediction
problem. NEL takes ambiguous entities as seeds, collects relevant mention
context in the Wikipedia corpus, and ensures the presence of mentions linking
to NIL by human annotation and entity masking. We conduct a series of
experiments with the widely used bi-encoder and cross-encoder entity linking
models, results show that both types of NIL mentions in training data have a
significant influence on the accuracy of NIL prediction. Our code and dataset
can be accessed at https://github.com/solitaryzero/NIL_EL",None,-1
1407ee1c-8aa6-4f64-b2e0-aca2bcdd6375,Vec2Gloss: definition modeling leveraging contextualized vectors with Wordnet gloss,0.0893511,"Contextualized embeddings are proven to be powerful tools in multiple NLP
tasks. Nonetheless, challenges regarding their interpretability and capability
to represent lexical semantics still remain. In this paper, we propose that the
task of definition modeling, which aims to generate the human-readable
definition of the word, provides a route to evaluate or understand the high
dimensional semantic vectors. We propose a `Vec2Gloss' model, which produces
the gloss from the target word's contextualized embeddings. The generated
glosses of this study are made possible by the systematic gloss patterns
provided by Chinese Wordnet. We devise two dependency indices to measure the
semantic and contextual dependency, which are used to analyze the generated
texts in gloss and token levels. Our results indicate that the proposed
`Vec2Gloss' model opens a new perspective to the lexical-semantic applications
of contextualized embeddings.",None,-1
54ca2cae-70f0-48f7-91ef-88205789ea38,Morphological Inflection: A Reality Check,0.265067,"Morphological inflection is a popular task in sub-word NLP with both
practical and cognitive applications. For years now, state-of-the-art systems
have reported high, but also highly variable, performance across data sets and
languages. We investigate the causes of this high performance and high
variability; we find several aspects of data set creation and evaluation which
systematically inflate performance and obfuscate differences between languages.
To improve generalizability and reliability of results, we propose new data
sampling and evaluation strategies that better reflect likely use-cases. Using
these new strategies, we make new observations on the generalization abilities
of current inflection systems.",None,-1
2c847301-f5a3-435e-92da-2138c5031efa,Lessons Learned from a Citizen Science Project for Natural Language Processing,0.147313,"Many Natural Language Processing (NLP) systems use annotated corpora for
training and evaluation. However, labeled data is often costly to obtain and
scaling annotation projects is difficult, which is why annotation tasks are
often outsourced to paid crowdworkers. Citizen Science is an alternative to
crowdsourcing that is relatively unexplored in the context of NLP. To
investigate whether and how well Citizen Science can be applied in this
setting, we conduct an exploratory study into engaging different groups of
volunteers in Citizen Science for NLP by re-annotating parts of a pre-existing
crowdsourced dataset. Our results show that this can yield high-quality
annotations and attract motivated volunteers, but also requires considering
factors such as scalability, participation over time, and legal and ethical
issues. We summarize lessons learned in the form of guidelines and provide our
code and data to aid future work on Citizen Science.",None,-1
030edee2-468d-4f1a-b0a5-4b65bd4e9c63,DeDrift: Robust Similarity Search under Content Drift,0.811387,"The statistical distribution of content uploaded and searched on media
sharing sites changes over time due to seasonal, sociological and technical
factors. We investigate the impact of this ""content drift"" for large-scale
similarity search tools, based on nearest neighbor search in embedding space.
Unless a costly index reconstruction is performed frequently, content drift
degrades the search accuracy and efficiency. The degradation is especially
severe since, in general, both the query and database distributions change.
  We introduce and analyze real-world image and video datasets for which
temporal information is available over a long time period. Based on the
learnings, we devise DeDrift, a method that updates embedding quantizers to
continuously adapt large-scale indexing structures on-the-fly. DeDrift almost
eliminates the accuracy degradation due to the query and database content drift
while being up to 100x faster than a full index reconstruction.",None,-1
8cd961d2-753d-4d56-8918-5b27270ffbf5,Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition,0.787181,"Existing pedestrian attribute recognition (PAR) algorithms are mainly
developed based on a static image. However, the performance is not reliable for
images with challenging factors, such as heavy occlusion, motion blur, etc. In
this work, we propose to understand human attributes using video frames that
can make full use of temporal information. Specifically, we formulate the
video-based PAR as a vision-language fusion problem and adopt pre-trained big
models CLIP to extract the feature embeddings of given video frames. To better
utilize the semantic information, we take the attribute list as another input
and transform the attribute words/phrase into the corresponding sentence via
split, expand, and prompt. Then, the text encoder of CLIP is utilized for
language embedding. The averaged visual tokens and text tokens are concatenated
and fed into a fusion Transformer for multi-modal interactive learning. The
enhanced tokens will be fed into a classification head for pedestrian attribute
prediction. Extensive experiments on a large-scale video-based PAR dataset
fully validated the effectiveness of our proposed framework.",None,-1
f97f9e4a-443c-4452-b848-1da0e7800b0b,CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement,0.731583,"Low-light images, characterized by inadequate illumination, pose challenges
of diminished clarity, muted colors, and reduced details. Low-light image
enhancement, an essential task in computer vision, aims to rectify these issues
by improving brightness, contrast, and overall perceptual quality, thereby
facilitating accurate analysis and interpretation. This paper introduces the
Convolutional Dense Attention-guided Network (CDAN), a novel solution for
enhancing low-light images. CDAN integrates an autoencoder-based architecture
with convolutional and dense blocks, complemented by an attention mechanism and
skip connections. This architecture ensures efficient information propagation
and feature learning. Furthermore, a dedicated post-processing phase refines
color balance and contrast. Our approach demonstrates notable progress compared
to state-of-the-art results in low-light image enhancement, showcasing its
robustness across a wide range of challenging scenarios. Our model performs
remarkably on benchmark datasets, effectively mitigating under-exposure and
proficiently restoring textures and colors in diverse low-light scenarios. This
achievement underscores CDAN's potential for diverse computer vision tasks,
notably enabling robust object detection and recognition in challenging
low-light conditions.",None,-1
79c24eb3-d114-4eb3-965e-6ae25b503b33,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,0.996578,"Volumetric scene representations enable photorealistic view synthesis for
static scenes and form the basis of several existing 6-DoF video techniques.
However, the volume rendering procedures that drive these representations
necessitate careful trade-offs in terms of quality, rendering speed, and memory
efficiency. In particular, existing methods fail to simultaneously achieve
real-time performance, small memory footprint, and high-quality rendering for
challenging real-world scenes. To address these issues, we present HyperReel --
a novel 6-DoF video representation. The two core components of HyperReel are:
(1) a ray-conditioned sample prediction network that enables high-fidelity,
high frame rate rendering at high resolutions and (2) a compact and
memory-efficient dynamic volume representation. Our 6-DoF video pipeline
achieves the best performance compared to prior and contemporary approaches in
terms of visual quality with small memory requirements, while also rendering at
up to 18 frames-per-second at megapixel resolution without any custom CUDA
code.",None,-1
ba48f096-ae45-449f-81ea-deff0884cdeb,Chain-of-Verification Reduces Hallucination in Large Language Models,0.502733,"Generation of plausible yet incorrect factual information, termed
hallucination, is an unsolved issue in large language models. We study the
ability of language models to deliberate on the responses they give in order to
correct their mistakes. We develop the Chain-of-Verification (CoVe) method
whereby the model first (i) drafts an initial response; then (ii) plans
verification questions to fact-check its draft; (iii) answers those questions
independently so the answers are not biased by other responses; and (iv)
generates its final verified response. In experiments, we show CoVe decreases
hallucinations across a variety of tasks, from list-based questions from
Wikidata, closed book MultiSpanQA and longform text generation.",None,-1
0ca28bf2-ffa6-4f37-bb18-fa1fd59b411e,Augmenting Large Language Model Translators via Translation Memories,0.668645,"Using translation memories (TMs) as prompts is a promising approach to
in-context learning of machine translation models. In this work, we take a step
towards prompting large language models (LLMs) with TMs and making them better
translators. We find that the ability of LLMs to ``understand'' prompts is
indeed helpful for making better use of TMs. Experiments show that the results
of a pre-trained LLM translator can be greatly improved by using high-quality
TM-based prompts. These results are even comparable to those of the
state-of-the-art NMT systems which have access to large-scale in-domain
bilingual data and are well tuned on the downstream tasks.",None,-1
3596f7ae-beb5-45f8-b08a-4adf91c9400b,Non-Contrastive Unsupervised Learning of Physiological Signals from Video,0.917498,"Subtle periodic signals such as blood volume pulse and respiration can be
extracted from RGB video, enabling remote health monitoring at low cost.
Advancements in remote pulse estimation -- or remote photoplethysmography
(rPPG) -- are currently driven by deep learning solutions. However, modern
approaches are trained and evaluated on benchmark datasets with associated
ground truth from contact-PPG sensors. We present the first non-contrastive
unsupervised learning framework for signal regression to break free from the
constraints of labelled video data. With minimal assumptions of periodicity and
finite bandwidth, our approach is capable of discovering the blood volume pulse
directly from unlabelled videos. We find that encouraging sparse power spectra
within normal physiological bandlimits and variance over batches of power
spectra is sufficient for learning visual features of periodic signals. We
perform the first experiments utilizing unlabelled video data not specifically
created for rPPG to train robust pulse rate estimators. Given the limited
inductive biases and impressive empirical results, the approach is
theoretically capable of discovering other periodic signals from video,
enabling multiple physiological measurements without the need for ground truth
signals. Codes to fully reproduce the experiments are made available along with
the paper.",None,-1
3091ad21-879f-46e3-bd7c-6543db273bb8,Company2Vec -- German Company Embeddings based on Corporate Websites,0.079021,"With Company2Vec, the paper proposes a novel application in representation
learning. The model analyzes business activities from unstructured company
website data using Word2Vec and dimensionality reduction. Company2Vec maintains
semantic language structures and thus creates efficient company embeddings in
fine-granular industries. These semantic embeddings can be used for various
applications in banking. Direct relations between companies and words allow
semantic business analytics (e.g. top-n words for a company). Furthermore,
industry prediction is presented as a supervised learning application and
evaluation method. The vectorized structure of the embeddings allows measuring
companies similarities with the cosine distance. Company2Vec hence offers a
more fine-grained comparison of companies than the standard industry labels
(NACE). This property is relevant for unsupervised learning tasks, such as
clustering. An alternative industry segmentation is shown with k-means
clustering on the company embeddings. Finally, this paper proposes three
algorithms for (1) firm-centric, (2) industry-centric and (3) portfolio-centric
peer-firm identification.",None,-1
a2bc17b4-dec0-4613-811d-b9b6b6713012,Reinforcement Learning with Human Feedback for Realistic Traffic Simulation,0.50789,"In light of the challenges and costs of real-world testing, autonomous
vehicle developers often rely on testing in simulation for the creation of
reliable systems. A key element of effective simulation is the incorporation of
realistic traffic models that align with human knowledge, an aspect that has
proven challenging due to the need to balance realism and diversity. This works
aims to address this by developing a framework that employs reinforcement
learning with human preference (RLHF) to enhance the realism of existing
traffic models. This study also identifies two main challenges: capturing the
nuances of human preferences on realism and the unification of diverse traffic
simulation models. To tackle these issues, we propose using human feedback for
alignment and employ RLHF due to its sample efficiency. We also introduce the
first dataset for realism alignment in traffic modeling to support such
research. Our framework, named TrafficRLHF, demonstrates its proficiency in
generating realistic traffic scenarios that are well-aligned with human
preferences, as corroborated by comprehensive evaluations on the nuScenes
dataset.",None,-1
b61936a8-de49-4e2e-9563-e4f1c16e0494,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,0.604992,"Current popular backbones in computer vision, such as Vision Transformers
(ViT) and ResNets are trained to perceive the world from 2D images. However, to
more effectively understand 3D structural priors in 2D backbones, we propose
Mask3D to leverage existing large-scale RGB-D data in a self-supervised
pre-training to embed these 3D priors into 2D learned feature representations.
In contrast to traditional 3D contrastive learning paradigms requiring 3D
reconstructions or multi-view correspondences, our approach is simple: we
formulate a pre-text reconstruction task by masking RGB and depth patches in
individual RGB-D frames. We demonstrate the Mask3D is particularly effective in
embedding 3D priors into the powerful 2D ViT backbone, enabling improved
representation learning for various scene understanding tasks, such as semantic
segmentation, instance segmentation and object detection. Experiments show that
Mask3D notably outperforms existing self-supervised 3D pre-training approaches
on ScanNet, NYUv2, and Cityscapes image understanding tasks, with an
improvement of +6.5% mIoU against the state-of-the-art Pri3D on ScanNet image
semantic segmentation.",None,-1
04e87ca6-1f9e-4c29-b641-8e2ded11f0ab,PolyFormer: Referring Image Segmentation as Sequential Polygon Generation,0.853552,"In this work, instead of directly predicting the pixel-level segmentation
masks, the problem of referring image segmentation is formulated as sequential
polygon generation, and the predicted polygons can be later converted into
segmentation masks. This is enabled by a new sequence-to-sequence framework,
Polygon Transformer (PolyFormer), which takes a sequence of image patches and
text query tokens as input, and outputs a sequence of polygon vertices
autoregressively. For more accurate geometric localization, we propose a
regression-based decoder, which predicts the precise floating-point coordinates
directly, without any coordinate quantization error. In the experiments,
PolyFormer outperforms the prior art by a clear margin, e.g., 5.40% and 4.52%
absolute improvements on the challenging RefCOCO+ and RefCOCOg datasets. It
also shows strong generalization ability when evaluated on the referring video
segmentation task without fine-tuning, e.g., achieving competitive 61.5% J&F on
the Ref-DAVIS17 dataset.",None,-1
d6ae0953-05af-43ad-9e83-cbbd54535ce5,SAAM: Stealthy Adversarial Attack on Monocular Depth Estimation,0.556332,"In this paper, we investigate the vulnerability of MDE to adversarial
patches. We propose a novel \underline{S}tealthy \underline{A}dversarial
\underline{A}ttacks on \underline{M}DE (SAAM) that compromises MDE by either
corrupting the estimated distance or causing an object to seamlessly blend into
its surroundings. Our experiments, demonstrate that the designed stealthy patch
successfully causes a DNN-based MDE to misestimate the depth of objects. In
fact, our proposed adversarial patch achieves a significant 60\% depth error
with 99\% ratio of the affected region. Importantly, despite its adversarial
nature, the patch maintains a naturalistic appearance, making it inconspicuous
to human observers. We believe that this work sheds light on the threat of
adversarial attacks in the context of MDE on edge devices. We hope it raises
awareness within the community about the potential real-life harm of such
attacks and encourages further research into developing more robust and
adaptive defense mechanisms.",None,-1
5f8aca1b-c073-4415-aae7-560527aca2ae,NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support,0.590679,"We present a method for generating high-quality watertight manifold meshes
from multi-view input images. Existing volumetric rendering methods are robust
in optimization but tend to generate noisy meshes with poor topology.
Differentiable rasterization-based methods can generate high-quality meshes but
are sensitive to initialization. Our method combines the benefits of both
worlds; we take the geometry initialization obtained from neural volumetric
fields, and further optimize the geometry as well as a compact neural texture
representation with differentiable rasterizers. Through extensive experiments,
we demonstrate that our method can generate accurate mesh reconstructions with
faithful appearance that are comparable to previous volume rendering methods
while being an order of magnitude faster in rendering. We also show that our
generated mesh and neural texture reconstruction is compatible with existing
graphics pipelines and enables downstream 3D applications such as simulation.
Project page: https://sarahweiii.github.io/neumanifold/",None,-1
8e6f3b64-eb27-4ae5-a404-24b77798e2ef,Interactive-Chain-Prompting: Ambiguity Resolution for Crosslingual Conditional Generation with Interaction,0.629887,"Crosslingual conditional generation (e.g., machine translation) has long
enjoyed the benefits of scaling. Nonetheless, there are still issues that scale
alone may not overcome. A source query in one language, for instance, may yield
several translation options in another language without any extra context. Only
one translation could be acceptable however, depending on the translator's
preferences and goals. Choosing the incorrect option might significantly affect
translation usefulness and quality. We propose a novel method interactive-chain
prompting -- a series of question, answering and generation intermediate steps
between a Translator model and a User model -- that reduces translations into a
list of subproblems addressing ambiguities and then resolving such subproblems
before producing the final text to be translated. To check ambiguity resolution
capabilities and evaluate translation quality, we create a dataset exhibiting
different linguistic phenomena which leads to ambiguities at inference for four
languages. To encourage further exploration in this direction, we release all
datasets. We note that interactive-chain prompting, using eight interactions as
exemplars, consistently surpasses prompt-based methods with direct access to
background information to resolve ambiguities.",None,-1
19e212ef-7ce1-41a8-bbcc-1cfb587c3b49,Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios,0.0557809,"In order to reveal the rationale behind model predictions, many works have
exploited providing explanations in various forms. Recently, to further
guarantee readability, more and more works turn to generate sentence-level
human language explanations. However, current works pursuing sentence-level
explanations rely heavily on annotated training data, which limits the
development of interpretability to only a few tasks. As far as we know, this
paper is the first to explore this problem smoothly from weak-supervised
learning to unsupervised learning. Besides, we also notice the high latency of
autoregressive sentence-level explanation generation, which leads to
asynchronous interpretability after prediction. Therefore, we propose a
non-autoregressive interpretable model to facilitate parallel explanation
generation and simultaneous prediction. Through extensive experiments on
Natural Language Inference task and Spouse Prediction task, we find that users
are able to train classifiers with comparable performance $10-15\times$ faster
with parallel explanation generation using only a few or no annotated training
data.",None,-1
084ad6f0-9215-457e-8680-8bb9f594fca9,Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification,0.768949,"Recent advances in large language models (LLMs) have shown impressive ability
in biomedical question-answering, but have not been adequately investigated for
more specific biomedical applications. This study investigates the performance
of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical
tasks beyond question-answering. Because no patient data can be passed to the
OpenAI API public interface, we evaluated model performance with over 10000
samples as proxies for two fundamental tasks in the clinical domain -
classification and reasoning. The first task is classifying whether statements
of clinical and policy recommendations in scientific literature constitute
health advice. The second task is causal relation detection from the biomedical
literature. We compared LLMs with simpler models, such as bag-of-words (BoW)
with logistic regression, and fine-tuned BioBERT models. Despite the excitement
around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks
remained the best strategy. The simple BoW model performed on par with the most
complex LLM prompting. Prompt engineering required significant investment.",None,-1
9f80a3ca-ceb4-4f23-9448-fba3552b7e6b,Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers,0.79087,"In recent years machine translation has become very successful for
high-resource language pairs. This has also sparked new interest in research on
the automatic translation of low-resource languages, including Indigenous
languages. However, the latter are deeply related to the ethnic and cultural
groups that speak (or used to speak) them. The data collection, modeling and
deploying machine translation systems thus result in new ethical questions that
must be addressed. Motivated by this, we first survey the existing literature
on ethical considerations for the documentation, translation, and general
natural language processing for Indigenous languages. Afterward, we conduct and
analyze an interview study to shed light on the positions of community leaders,
teachers, and language activists regarding ethical concerns for the automatic
translation of their languages. Our results show that the inclusion, at
different degrees, of native speakers and community members is vital to
performing better and more ethical research on Indigenous languages.",None,-1
ff55ae36-5cca-4e0c-b250-cd4319cce72f,Fast Inference and Update of Probabilistic Density Estimation on Trajectory Prediction,0.7577,"Safety-critical applications such as autonomous vehicles and social robots
require fast computation and accurate probability density estimation on
trajectory prediction. To address both requirements, this paper presents a new
normalizing flow-based trajectory prediction model named FlowChain. FlowChain
is a stack of conditional continuously-indexed flows (CIFs) that are expressive
and allow analytical probability density computation. This analytical
computation is faster than the generative models that need additional
approximations such as kernel density estimation. Moreover, FlowChain is more
accurate than the Gaussian mixture-based models due to fewer assumptions on the
estimated density. FlowChain also allows a rapid update of estimated
probability densities. This update is achieved by adopting the \textit{newest
observed position} and reusing the flow transformations and its
log-det-jacobians that represent the \textit{motion trend}. This update is
completed in less than one millisecond because this reuse greatly omits the
computational cost. Experimental results showed our FlowChain achieved
state-of-the-art trajectory prediction accuracy compared to previous methods.
Furthermore, our FlowChain demonstrated superiority in the accuracy and speed
of density estimation. Our code is available at
\url{https://github.com/meaten/FlowChain-ICCV2023}",None,-1
80ffc32b-a00e-4c7a-8e8d-b852a978846b,NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models,0.983609,"Automatically generating high-quality real world 3D scenes is of enormous
interest for applications such as virtual reality and robotics simulation.
Towards this goal, we introduce NeuralField-LDM, a generative model capable of
synthesizing complex 3D environments. We leverage Latent Diffusion Models that
have been successfully utilized for efficient high-quality 2D content creation.
We first train a scene auto-encoder to express a set of image and pose pairs as
a neural field, represented as density and feature voxel grids that can be
projected to produce novel views of the scene. To further compress this
representation, we train a latent-autoencoder that maps the voxel grids to a
set of latent representations. A hierarchical diffusion model is then fit to
the latents to complete the scene generation pipeline. We achieve a substantial
improvement over existing state-of-the-art scene generation models.
Additionally, we show how NeuralField-LDM can be used for a variety of 3D
content creation applications, including conditional scene generation, scene
inpainting and scene style manipulation.",None,-1
d80b9044-6d03-40be-b3d2-37975767e620,Clothes-Invariant Feature Learning by Causal Intervention for Clothes-Changing Person Re-identification,0.468754,"Clothes-invariant feature extraction is critical to the clothes-changing
person re-identification (CC-ReID). It can provide discriminative identity
features and eliminate the negative effects caused by the confounder--clothing
changes. But we argue that there exists a strong spurious correlation between
clothes and human identity, that restricts the common likelihood-based ReID
method P(Y|X) to extract clothes-irrelevant features. In this paper, we propose
a new Causal Clothes-Invariant Learning (CCIL) method to achieve
clothes-invariant feature learning by modeling causal intervention P(Y|do(X)).
This new causality-based model is inherently invariant to the confounder in the
causal view, which can achieve the clothes-invariant features and avoid the
barrier faced by the likelihood-based methods. Extensive experiments on three
CC-ReID benchmarks, including PRCC, LTCC, and VC-Clothes, demonstrate the
effectiveness of our approach, which achieves a new state of the art.",None,-1
c4b2be82-c549-4ee6-abea-f1fbb1046cf9,Less than One-shot: Named Entity Recognition via Extremely Weak Supervision,0.731505,"We study the named entity recognition (NER) problem under the extremely weak
supervision (XWS) setting, where only one example entity per type is given in a
context-free way. While one can see that XWS is lighter than one-shot in terms
of the amount of supervision, we propose a novel method X-NER that can
outperform the state-of-the-art one-shot NER methods. We first mine entity
spans that are similar to the example entities from an unlabelled training
corpus. Instead of utilizing entity span representations from language models,
we find it more effective to compare the context distributions before and after
the span is replaced by the entity example. We then leverage the top-ranked
spans as pseudo-labels to train an NER tagger. Extensive experiments and
analyses on 4 NER datasets show the superior end-to-end NER performance of
X-NER, outperforming the state-of-the-art few-shot methods with 1-shot
supervision and ChatGPT annotations significantly. Finally, our X-NER possesses
several notable properties, such as inheriting the cross-lingual abilities of
the underlying language models.",None,-1
f5e8d6a1-e9d7-4872-8f59-e7bbeac081ad,Blockwise Parallel Transformer for Large Context Models,0.123823,"Transformers have emerged as the cornerstone of state-of-the-art natural
language processing models, showcasing exceptional performance across a wide
range of AI applications. However, the memory demands posed by the
self-attention mechanism and the large feedforward network in Transformers
limit their ability to handle long sequences, thereby creating challenges for
tasks involving multiple long sequences or long-term dependencies. We present a
distinct approach, Blockwise Parallel Transformer (BPT), that leverages
blockwise computation of self-attention and feedforward network fusion to
minimize memory costs. By processing longer input sequences while maintaining
memory efficiency, BPT enables training sequences 32 times longer than vanilla
Transformers and up to 4 times longer than previous memory-efficient methods.
Extensive experiments on language modeling and reinforcement learning tasks
demonstrate the effectiveness of BPT in reducing memory requirements and
improving performance.",None,-1
fd1103ec-286c-4f23-9e82-758992450ce0,NExT-GPT: Any-to-Any Multimodal LLM,0.986175,"While recently Multimodal Large Language Models (MM-LLMs) have made exciting
strides, they mostly fall prey to the limitation of only input-side multimodal
understanding, without the ability to produce content in multiple modalities.
As we humans always perceive the world and communicate with people through
various modalities, developing any-to-any MM-LLMs capable of accepting and
delivering content in any modality becomes essential to human-level AI. To fill
the gap, we present an end-to-end general-purpose any-to-any MM-LLM system,
NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion
decoders, enabling NExT-GPT to perceive inputs and generate outputs in
arbitrary combinations of text, images, videos, and audio. By leveraging the
existing well-trained highly-performing encoders and decoders, NExT-GPT is
tuned with only a small amount of parameter (1%) of certain projection layers,
which not only benefits low-cost training and also facilitates convenient
expansion to more potential modalities. Moreover, we introduce a
modality-switching instruction tuning (MosIT) and manually curate a
high-quality dataset for MosIT, based on which NExT-GPT is empowered with
complex cross-modal semantic understanding and content generation. Overall, our
research showcases the promising possibility of building an AI agent capable of
modeling universal modalities, paving the way for more human-like AI research
in the community. Project page: https://next-gpt.github.io/",None,-1
3b307d52-ff40-4b15-89b4-74d24f98c4ef,Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE,0.517372,"Knowledge graph embedding models (KGEMs) have gained considerable traction in
recent years. These models learn a vector representation of knowledge graph
entities and relations, a.k.a. knowledge graph embeddings (KGEs). Learning
versatile KGEs is desirable as it makes them useful for a broad range of tasks.
However, KGEMs are usually trained for a specific task, which makes their
embeddings task-dependent. In parallel, the widespread assumption that KGEMs
actually create a semantic representation of the underlying entities and
relations (e.g., project similar entities closer than dissimilar ones) has been
challenged. In this work, we design heuristics for generating protographs --
small, modified versions of a KG that leverage RDF/S information. The learnt
protograph-based embeddings are meant to encapsulate the semantics of a KG, and
can be leveraged in learning KGEs that, in turn, also better capture semantics.
Extensive experiments on various evaluation benchmarks demonstrate the
soundness of this approach, which we call Modular and Agnostic SCHema-based
Integration of protograph Embeddings (MASCHInE). In particular, MASCHInE helps
produce more versatile KGEs that yield substantially better performance for
entity clustering and node classification tasks. For link prediction, using
MASCHinE substantially increases the number of semantically valid predictions
with equivalent rank-based performance.",None,-1
9c29e030-c199-48db-b5af-8fda867ad177,The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings,0.5704,"Numerous works use word embedding-based metrics to quantify societal biases
and stereotypes in texts. Recent studies have found that word embeddings can
capture semantic similarity but may be affected by word frequency. In this work
we study the effect of frequency when measuring female vs. male gender bias
with word embedding-based bias quantification methods. We find that Skip-gram
with negative sampling and GloVe tend to detect male bias in high frequency
words, while GloVe tends to return female bias in low frequency words. We show
these behaviors still exist when words are randomly shuffled. This proves that
the frequency-based effect observed in unshuffled corpora stems from properties
of the metric rather than from word associations. The effect is spurious and
problematic since bias metrics should depend exclusively on word co-occurrences
and not individual word frequencies. Finally, we compare these results with the
ones obtained with an alternative metric based on Pointwise Mutual Information.
We find that this metric does not show a clear dependence on frequency, even
though it is slightly skewed towards male bias across all frequencies.",None,-1
18d0b629-d7d9-4716-a2ab-71d95c7e3086,Split-and-Denoise: Protect large language model inference with local differential privacy,0.258871,"Large Language Models (LLMs) excel in natural language understanding by
capturing hidden semantics in vector space. This process enriches the value of
text embeddings for various downstream tasks, thereby fostering the
Embedding-as-a-Service (EaaS) business model. However, the risk of privacy
leakage due to direct text transmission to servers remains a critical concern.
To address this, we introduce Split-N-Denoise (SnD), an private inference
framework that splits the model to execute the token embedding layer on the
client side at minimal computational cost. This allows the client to introduce
noise prior to transmitting the embeddings to the server, and subsequently
receive and denoise the perturbed output embeddings for downstream tasks. Our
approach is designed for the inference stage of LLMs and requires no
modifications to the model parameters. Extensive experiments demonstrate SnD's
effectiveness in optimizing the privacy-utility tradeoff across various LLM
architectures and diverse downstream tasks. The results reveal an improvement
in performance under the same privacy budget compared to the baselines by over
10\% on average, offering clients a privacy-preserving solution for local
privacy protection.",None,-1
be866603-f4e1-469b-baae-e8b68b9bf8d3,NOPE: Novel Object Pose Estimation from a Single Image,0.633687,"The practicality of 3D object pose estimation remains limited for many
applications due to the need for prior knowledge of a 3D model and a training
period for new objects. To address this limitation, we propose an approach that
takes a single image of a new object as input and predicts the relative pose of
this object in new images without prior knowledge of the object's 3D model and
without requiring training time for new objects and categories. We achieve this
by training a model to directly predict discriminative embeddings for
viewpoints surrounding the object. This prediction is done using a simple U-Net
architecture with attention and conditioned on the desired pose, which yields
extremely fast inference. We compare our approach to state-of-the-art methods
and show it outperforms them both in terms of accuracy and robustness. Our
source code is publicly available at https://github.com/nv-nguyen/nope",None,-1
0d3d331e-8bfe-4219-8778-377f7181f254,Dual Learning for Large Vocabulary On-Device ASR,0.0783221,"Dual learning is a paradigm for semi-supervised machine learning that seeks
to leverage unsupervised data by solving two opposite tasks at once. In this
scheme, each model is used to generate pseudo-labels for unlabeled examples
that are used to train the other model. Dual learning has seen some use in
speech processing by pairing ASR and TTS as dual tasks. However, these results
mostly address only the case of using unpaired examples to compensate for very
small supervised datasets, and mostly on large, non-streaming models. Dual
learning has not yet been proven effective for using unsupervised data to
improve realistic on-device streaming models that are already trained on large
supervised corpora. We provide this missing piece though an analysis of an
on-device-sized streaming conformer trained on the entirety of Librispeech,
showing relative WER improvements of 10.7%/5.2% without an LM and 11.7%/16.4%
with an LM.",None,-1
2dc8554b-2c4e-47be-9785-50b959398a89,Document-level Relation Extraction with Cross-sentence Reasoning Graph,0.844489,"Relation extraction (RE) has recently moved from the sentence-level to
document-level, which requires aggregating document information and using
entities and mentions for reasoning. Existing works put entity nodes and
mention nodes with similar representations in a document-level graph, whose
complex edges may incur redundant information. Furthermore, existing studies
only focus on entity-level reasoning paths without considering global
interactions among entities cross-sentence. To these ends, we propose a novel
document-level RE model with a GRaph information Aggregation and Cross-sentence
Reasoning network (GRACR). Specifically, a simplified document-level graph is
constructed to model the semantic information of all mentions and sentences in
a document, and an entity-level graph is designed to explore relations of
long-distance cross-sentence entity pairs. Experimental results show that GRACR
achieves excellent performance on two public datasets of document-level RE. It
is especially effective in extracting potential relations of cross-sentence
entity pairs. Our code is available at https://github.com/UESTC-LHF/GRACR.",None,-1
55216313-4c32-4a6b-ad86-690389bff7c7,Multi-modal Latent Space Learning for Chain-of-Thought Reasoning in Language Models,0.817643,"Chain-of-thought (CoT) reasoning has exhibited impressive performance in
language models for solving complex tasks and answering questions. However,
many real-world questions require multi-modal information, such as text and
images. Previous research on multi-modal CoT has primarily focused on
extracting fixed image features from off-the-shelf vision models and then
fusing them with text using attention mechanisms. This approach has limitations
because these vision models were not designed for complex reasoning tasks and
do not align well with language thoughts. To overcome this limitation, we
introduce a novel approach for multi-modal CoT reasoning that utilizes latent
space learning via diffusion processes to generate effective image features
that align with language thoughts. Our method fuses image features and text
representations at a deep level and improves the complex reasoning ability of
multi-modal CoT. We demonstrate the efficacy of our proposed method on
multi-modal ScienceQA and machine translation benchmarks, achieving
state-of-the-art performance on ScienceQA. Overall, our approach offers a more
robust and effective solution for multi-modal reasoning in language models,
enhancing their ability to tackle complex real-world problems.",None,-1
10b0403a-11f7-4635-b6e6-ba57e04f1380,Continual Detection Transformer for Incremental Object Detection,0.706437,"Incremental object detection (IOD) aims to train an object detector in
phases, each with annotations for new object categories. As other incremental
settings, IOD is subject to catastrophic forgetting, which is often addressed
by techniques such as knowledge distillation (KD) and exemplar replay (ER).
However, KD and ER do not work well if applied directly to state-of-the-art
transformer-based object detectors such as Deformable DETR and UP-DETR. In this
paper, we solve these issues by proposing a ContinuaL DEtection TRansformer
(CL-DETR), a new method for transformer-based IOD which enables effective usage
of KD and ER in this context. First, we introduce a Detector Knowledge
Distillation (DKD) loss, focusing on the most informative and reliable
predictions from old versions of the model, ignoring redundant background
predictions, and ensuring compatibility with the available ground-truth labels.
We also improve ER by proposing a calibration strategy to preserve the label
distribution of the training set, therefore better matching training and
testing statistics. We conduct extensive experiments on COCO 2017 and
demonstrate that CL-DETR achieves state-of-the-art results in the IOD setting.",None,-1
6237f815-ac4f-4acb-9f3f-21a06de85a19,Message Ritual: A Posthuman Account of Living with Lamp,0.534128,"As we become increasingly entangled with digital technologies, the boundary
between human and machine is progressively blurring. Adopting a performative,
posthumanist perspective resolves this ambiguity by proposing that such
boundaries are not predetermined, rather they are enacted within a certain
material configuration. Using this approach, dubbed `Entanglement HCI', this
paper presents \emph{Message Ritual} -- a novel, integrated AI system that
encourages the re-framing of memory through machine generated poetics. Embodied
within a domestic table lamp, the system listens in on conversations occurring
within the home, drawing out key topics and phrases of the day and
reconstituting them through machine generated poetry, delivered to household
members via SMS upon waking each morning. Participants across four households
were asked to live with the lamp over a two week period. We present a
diffractive analysis exploring how the lamp \emph{becomes with} participants
and discuss the implications of this method for future HCI research.",None,-1
cf33deb6-1d43-4e32-a896-3e060b62ddcf,Safe Interval Path Planning With Kinodynamic Constraints,0.269101,"Safe Interval Path Planning (SIPP) is a powerful algorithm for solving
single-agent pathfinding problem when the agent is confined to a graph and
certain vertices/edges of this graph are blocked at certain time intervals due
to dynamic obstacles that populate the environment. Original SIPP algorithm
relies on the assumption that the agent is able to stop instantaneously.
However, this assumption often does not hold in practice, e.g. a mobile robot
moving with a cruising speed is not able to stop immediately but rather
requires gradual deceleration to a full stop that takes time. In other words,
the robot is subject to kinodynamic constraints. Unfortunately, as we show in
this work, in such a case original SIPP is incomplete. To this end, we
introduce a novel variant of SIPP that is provably complete and optimal for
planning with acceleration/deceleration. In the experimental evaluation we show
that the key property of the original SIPP still holds for the modified version
-- it performs much less expansions compared to A* and, as a result, is notably
faster.",None,-1
e17f29e5-13cf-41c3-9265-57aae9abfa42,Remote Sensing Image Change Detection with Graph Interaction,0.174551,"Modern remote sensing image change detection has witnessed substantial
advancements by harnessing the potent feature extraction capabilities of CNNs
and Transforms.Yet,prevailing change detection techniques consistently
prioritize extracting semantic features related to significant
alterations,overlooking the viability of directly interacting with bitemporal
image features.In this letter,we propose a bitemporal image graph Interaction
network for remote sensing change detection,namely BGINet-CD. More
specifically,by leveraging the concept of non-local operations and mapping the
features obtained from the backbone network to the graph structure space,we
propose a unified self-focus mechanism for bitemporal images.This approach
enhances the information coupling between the two temporal images while
effectively suppressing task-irrelevant interference,Based on a streamlined
backbone architecture,namely ResNet18,our model demonstrates superior
performance compared to other state-of-the-art methods (SOTA) on the GZ CD
dataset. Moreover,the model exhibits an enhanced trade-off between accuracy and
computational efficiency,further improving its overall effectiveness",None,-1
1f9f64fd-4d60-41ff-8aa8-c35e176d75e8,Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models,0.55578,"In this study, we present an efficient and effective approach for achieving
temporally consistent synthetic-to-real video translation in videos of varying
lengths. Our method leverages off-the-shelf conditional image diffusion models,
allowing us to perform multiple synthetic-to-real image generations in
parallel. By utilizing the available optical flow information from the
synthetic videos, our approach seamlessly enforces temporal consistency among
corresponding pixels across frames. This is achieved through joint noise
optimization, effectively minimizing spatial and temporal discrepancies. To the
best of our knowledge, our proposed method is the first to accomplish diverse
and temporally consistent synthetic-to-real video translation using conditional
image diffusion models. Furthermore, our approach does not require any training
or fine-tuning of the diffusion models. Extensive experiments conducted on
various benchmarks for synthetic-to-real video translation demonstrate the
effectiveness of our approach, both quantitatively and qualitatively. Finally,
we show that our method outperforms other baseline methods in terms of both
temporal consistency and visual quality.",None,-1
ad57c0fc-1f01-4819-97c7-b83c929139b0,VLN-Trans: Translator for the Vision and Language Navigation Agent,0.371693,"Language understanding is essential for the navigation agent to follow
instructions. We observe two kinds of issues in the instructions that can make
the navigation task challenging: 1. The mentioned landmarks are not
recognizable by the navigation agent due to the different vision abilities of
the instructor and the modeled agent. 2. The mentioned landmarks are applicable
to multiple targets, thus not distinctive for selecting the target among the
candidate viewpoints. To deal with these issues, we design a translator module
for the navigation agent to convert the original instructions into
easy-to-follow sub-instruction representations at each step. The translator
needs to focus on the recognizable and distinctive landmarks based on the
agent's visual abilities and the observed visual environment. To achieve this
goal, we create a new synthetic sub-instruction dataset and design specific
tasks to train the translator and the navigation agent. We evaluate our
approach on Room2Room~(R2R), Room4room~(R4R), and Room2Room Last (R2R-Last)
datasets and achieve state-of-the-art results on multiple benchmarks.",None,-1
63a7dca7-fde3-4acd-9aa1-4ff702f78e58,Model-Based Uncertainty in Value Functions,0.187182,"We consider the problem of quantifying uncertainty over expected cumulative
rewards in model-based reinforcement learning. In particular, we focus on
characterizing the variance over values induced by a distribution over MDPs.
Previous work upper bounds the posterior variance over values by solving a
so-called uncertainty Bellman equation, but the over-approximation may result
in inefficient exploration. We propose a new uncertainty Bellman equation whose
solution converges to the true posterior variance over values and explicitly
characterizes the gap in previous work. Moreover, our uncertainty
quantification technique is easily integrated into common exploration
strategies and scales naturally beyond the tabular setting by using standard
deep reinforcement learning architectures. Experiments in difficult exploration
tasks, both in tabular and continuous control settings, show that our sharper
uncertainty estimates improve sample-efficiency.",None,-1
77ea5df1-340a-4c2f-be76-f833066fa366,GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning,0.627006,"This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared
task, encompassing both subtask A and subtask B. We approach the task as a
dialogue summarization problem and implement two distinct pipelines: (a) a
fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b)
few-shot in-context learning (ICL) using a large language model, GPT-4. Both
methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1
(deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421,
respectively. Additionally, we predict the associated section headers using
RoBERTa and SciBERT based classification models. Our team ranked fourth among
all teams, while each team is allowed to submit three runs as part of their
submission. We also utilize expert annotations to demonstrate that the notes
generated through the ICL GPT-4 are better than all other baselines. The code
for our submission is available.",None,-1
2f3ed641-9132-4c0c-840f-00a5ec0887fc,Is Knowledge All Large Language Models Needed for Causal Reasoning?,0.182083,"This paper explores the causal reasoning of large language models (LLMs) to
enhance their interpretability and reliability in advancing artificial
intelligence. Despite the proficiency of LLMs in a range of tasks, their
potential for understanding causality requires further exploration. We propose
a novel causal attribution model that utilizes ``do-operators"" for constructing
counterfactual scenarios, allowing us to systematically quantify the influence
of input numerical data and LLMs' pre-existing knowledge on their causal
reasoning processes. Our newly developed experimental setup assesses LLMs'
reliance on contextual information and inherent knowledge across various
domains. Our evaluation reveals that LLMs' causal reasoning ability mainly
depends on the context and domain-specific knowledge provided. In the absence
of such knowledge, LLMs can still maintain a degree of causal reasoning using
the available numerical data, albeit with limitations in the calculations. This
motivates the proposed fine-tuned LLM for pairwise causal discovery,
effectively leveraging both knowledge and numerical information.",None,-1
65373303-923d-4721-9bfc-ba5a1b0a4c73,"OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning",0.517995,"Large language models (LLMs) often struggle with maintaining accuracy
throughout multiple multiple reasoning steps, especially in mathematical
reasoning where an error in earlier steps can propagate to subsequent ones and
it ultimately leading to an incorrect answer. To reduce error propagation,
guided decoding is employed to direct the LM decoding on a step-by-step basis.
We argue that in guided decoding, assessing the potential of an incomplete
reasoning path can be more advantageous than simply ensuring per-step
correctness, as the former approach leads towards a correct final answer. This
transforms the task into a $\textit{value estimation}$ problem in planning.
  Inspired by the findings that $\textit{outcome supervision for guided
decoding essentially acts as a value model}$, we propose Outcome-supervised
Value Model (OVM) that employs outcome supervision for training a value model,
which prioritizes steps that lead to accurate conclusions. Furthermore, the OVM
eliminates the need for labor-intensive annotations of step-level correctness,
thereby significantly enhancing its scalability. Our experiments on two
multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate
the superior performance of the OVM model. Notably, in GSM8K, our
$\textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B
parameters}$; especially it does not utilize GPT-4 or code execution. These
findings offer a novel perspective on the role of outcome supervision in
training value models for multi-step reasoning tasks and provide theoretical
justification for its advantage in value estimation for guided decoding.",None,-1
01a95438-2499-42f2-9706-a3b025746a40,Can LLMs Fix Issues with Reasoning Models? Towards More Likely Models for AI Planning,0.513725,"This is the first work to look at the application of large language models
(LLMs) for the purpose of model space edits in automated planning tasks. To set
the stage for this union, we explore two different flavors of model space
problems that have been studied in the AI planning literature and explore the
effect of an LLM on those tasks. We empirically demonstrate how the performance
of an LLM contrasts with combinatorial search (CS) -- an approach that has been
traditionally used to solve model space tasks in planning, both with the LLM in
the role of a standalone model space reasoner as well as in the role of a
statistical signal in concert with the CS approach as part of a two-stage
process. Our experiments show promising results suggesting further forays of
LLMs into the exciting world of model space reasoning for planning tasks in the
future.",None,-1
f63bbea8-7baa-4f98-b051-c3633eb68804,AutoTrial: Prompting Language Models for Clinical Trial Design,0.627051,"Clinical trials are critical for drug development. Constructing the
appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for
patient recruitment) is essential for the trial's success. Proper design of
clinical trial protocols should consider similar precedent trials and their
eligibility criteria to ensure sufficient patient coverage. In this paper, we
present a method named AutoTrial to aid the design of clinical eligibility
criteria using language models. It allows (1) controllable generation under
instructions via a hybrid of discrete and neural prompting, (2) scalable
knowledge incorporation via in-context learning, and (3) explicit reasoning
chains to provide rationales for understanding the outputs. Experiments on over
70K clinical trials verify that AutoTrial generates high-quality criteria texts
that are fluent and coherent and with high accuracy in capturing the relevant
clinical concepts to the target trial. It is noteworthy that our method, with a
much smaller parameter size, gains around 60% winning rate against the GPT-3.5
baselines via human evaluations.",None,-1
a6b0917e-5653-4bae-b9aa-60b2b350aa3d,VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna,0.757909,"Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and
few-shot capabilities in Named Entity Recognition (NER). However, these models
can only be accessed via online APIs, which may cause data leak and
non-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot
NER framework based on the newly released open-source LLM -- Vicuna. VicunaNER
is a two-phase framework, where each phase leverages multi-turn dialogues with
Vicuna to recognize entities from texts. We name the second phase as
Re-Recognition, which recognizes those entities not recognized in the first
phase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues
in each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot
capacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.
Experimental results demonstrate that VicunaNER achieves superior performance
in both shot settings. Additionally, we conduct comprehensive investigations on
Vicuna from multiple perspectives.",None,-1
5b86dc0a-8411-4ebd-8f40-d627b479dc41,Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles,0.866629,"Modern hierarchical vision transformers have added several vision-specific
components in the pursuit of supervised classification performance. While these
components lead to effective accuracies and attractive FLOP counts, the added
complexity actually makes these transformers slower than their vanilla ViT
counterparts. In this paper, we argue that this additional bulk is unnecessary.
By pretraining with a strong visual pretext task (MAE), we can strip out all
the bells-and-whistles from a state-of-the-art multi-stage vision transformer
without losing accuracy. In the process, we create Hiera, an extremely simple
hierarchical vision transformer that is more accurate than previous models
while being significantly faster both at inference and during training. We
evaluate Hiera on a variety of tasks for image and video recognition. Our code
and models are available at https://github.com/facebookresearch/hiera.",None,-1
c3418bef-99c8-41c5-aef7-216a85b2bab9,"DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion",0.547524,"We present an innovative approach to 3D Human Pose Estimation (3D-HPE) by
integrating cutting-edge diffusion models, which have revolutionized diverse
fields, but are relatively unexplored in 3D-HPE. We show that diffusion models
enhance the accuracy, robustness, and coherence of human pose estimations. We
introduce DiffHPE, a novel strategy for harnessing diffusion models in 3D-HPE,
and demonstrate its ability to refine standard supervised 3D-HPE. We also show
how diffusion models lead to more robust estimations in the face of occlusions,
and improve the time-coherence and the sagittal symmetry of predictions. Using
the Human\,3.6M dataset, we illustrate the effectiveness of our approach and
its superiority over existing models, even under adverse situations where the
occlusion patterns in training do not match those in inference. Our findings
indicate that while standalone diffusion models provide commendable
performance, their accuracy is even better in combination with supervised
models, opening exciting new avenues for 3D-HPE research.",None,-1
483e2aa2-e4f0-4854-ae3c-ffd56b0e0486,Improving Language Plasticity via Pretraining with Active Forgetting,0.237397,"Pretrained language models (PLMs) are today the primary model for natural
language processing. Despite their impressive downstream performance, it can be
difficult to apply PLMs to new languages, a barrier to making their
capabilities universally accessible. While prior work has shown it possible to
address this issue by learning a new embedding layer for the new language,
doing so is both data and compute inefficient. We propose to use an active
forgetting mechanism during pretraining, as a simple way of creating PLMs that
can quickly adapt to new languages. Concretely, by resetting the embedding
layer every K updates during pretraining, we encourage the PLM to improve its
ability of learning new embeddings within a limited number of updates, similar
to a meta-learning effect. Experiments with RoBERTa show that models pretrained
with our forgetting mechanism not only demonstrate faster convergence during
language adaptation but also outperform standard ones in a low-data regime,
particularly for languages that are distant from English.",None,-1
a0e11955-ab0c-4e80-8d1f-6785fd434117,Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning,0.881903,"Prompt tuning, in which a base pretrained model is adapted to each task via
conditioning on learned prompt vectors, has emerged as a promising approach for
efficiently adapting large language models to multiple downstream tasks.
However, existing methods typically learn soft prompt vectors from scratch, and
it has not been clear how to exploit the rich cross-task knowledge with prompt
vectors in a multitask learning setting. We propose multitask prompt tuning
(MPT), which first learns a single transferable prompt by distilling knowledge
from multiple task-specific source prompts. We then learn multiplicative low
rank updates to this shared prompt to efficiently adapt it to each downstream
target task. Extensive experiments on 23 NLP datasets demonstrate that our
proposed approach outperforms the state-of-the-art methods, including the full
finetuning baseline in some cases, despite only tuning 0.035% as many
task-specific parameters.",None,-1
01c9c97f-99c2-447e-9ba2-33d97d1db113,Urban Visual Intelligence: Studying Cities with AI and Street-level Imagery,0.30817,"The visual dimension of cities has been a fundamental subject in urban
studies, since the pioneering work of scholars such as Sitte, Lynch, Arnheim,
and Jacobs. Several decades later, big data and artificial intelligence (AI)
are revolutionizing how people move, sense, and interact with cities. This
paper reviews the literature on the appearance and function of cities to
illustrate how visual information has been used to understand them. A
conceptual framework, Urban Visual Intelligence, is introduced to
systematically elaborate on how new image data sources and AI techniques are
reshaping the way researchers perceive and measure cities, enabling the study
of the physical environment and its interactions with socioeconomic
environments at various scales. The paper argues that these new approaches
enable researchers to revisit the classic urban theories and themes, and
potentially help cities create environments that are more in line with human
behaviors and aspirations in the digital age.",None,-1
45ced2ce-6725-41c0-9732-63fada95b4c2,SwinDocSegmenter: An End-to-End Unified Domain Adaptive Transformer for Document Instance Segmentation,0.734345,"Instance-level segmentation of documents consists in assigning a class-aware
and instance-aware label to each pixel of the image. It is a key step in
document parsing for their understanding. In this paper, we present a unified
transformer encoder-decoder architecture for en-to-end instance segmentation of
complex layouts in document images. The method adapts a contrastive training
with a mixed query selection for anchor initialization in the decoder. Later
on, it performs a dot product between the obtained query embeddings and the
pixel embedding map (coming from the encoder) for semantic reasoning. Extensive
experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical
Japanese (HJ), and TableBank demonstrate that our model with SwinL backbone
achieves better segmentation performance than the existing state-of-the-art
approaches with the average precision of \textbf{93.72}, \textbf{54.39},
\textbf{84.65} and \textbf{98.04} respectively under one billion parameters.
The code is made publicly available at:
\href{https://github.com/ayanban011/SwinDocSegmenter}{github.com/ayanban011/SwinDocSegmenter}",None,-1
fbdd628b-010e-41ba-999e-d65077ab6450,"ProsAudit, a prosodic benchmark for self-supervised speech models",0.100177,"We present ProsAudit, a benchmark in English to assess structural prosodic
knowledge in self-supervised learning (SSL) speech models. It consists of two
subtasks, their corresponding metrics, and an evaluation dataset. In the
protosyntax task, the model must correctly identify strong versus weak prosodic
boundaries. In the lexical task, the model needs to correctly distinguish
between pauses inserted between words and within words. We also provide human
evaluation scores on this benchmark. We evaluated a series of SSL models and
found that they were all able to perform above chance on both tasks, even when
evaluated on an unseen language. However, non-native models performed
significantly worse than native ones on the lexical task, highlighting the
importance of lexical knowledge in this task. We also found a clear effect of
size with models trained on more data performing better in the two subtasks.",None,-1
d624f23d-7deb-4c45-a244-3626ef53bdc7,InstructDiffusion: A Generalist Modeling Interface for Vision Tasks,0.859959,"We present InstructDiffusion, a unifying and generic framework for aligning
computer vision tasks with human instructions. Unlike existing approaches that
integrate prior knowledge and pre-define the output space (e.g., categories and
coordinates) for each vision task, we cast diverse vision tasks into a
human-intuitive image-manipulating process whose output space is a flexible and
interactive pixel space. Concretely, the model is built upon the diffusion
process and is trained to predict pixels according to user instructions, such
as encircling the man's left shoulder in red or applying a blue mask to the
left car. InstructDiffusion could handle a variety of vision tasks, including
understanding tasks (such as segmentation and keypoint detection) and
generative tasks (such as editing and enhancement). It even exhibits the
ability to handle unseen tasks and outperforms prior methods on novel datasets.
This represents a significant step towards a generalist modeling interface for
vision tasks, advancing artificial general intelligence in the field of
computer vision.",None,-1
120a26bb-0813-43be-9eac-bc6e2b6d920e,Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2023): Workshop and Shared Task Report,0.482958,"We provide a summary of the sixth edition of the CASE workshop that is held
in the scope of RANLP 2023. The workshop consists of regular papers, three
keynotes, working papers of shared task participants, and shared task overview
papers. This workshop series has been bringing together all aspects of event
information collection across technical and social science fields. In addition
to contributing to the progress in text based event extraction, the workshop
provides a space for the organization of a multimodal event information
collection task.",None,-1
a0e93122-2c05-4e8c-b8a2-b4d72f885c95,Assessing the Interpretability of Programmatic Policies with Large Language Models,0.329396,"Although the synthesis of programs encoding policies often carries the
promise of interpretability, systematic evaluations were never performed to
assess the interpretability of these policies, likely because of the complexity
of such an evaluation. In this paper, we introduce a novel metric that uses
large-language models (LLM) to assess the interpretability of programmatic
policies. For our metric, an LLM is given both a program and a description of
its associated programming language. The LLM then formulates a natural language
explanation of the program. This explanation is subsequently fed into a second
LLM, which tries to reconstruct the program from the natural-language
explanation. Our metric then measures the behavioral similarity between the
reconstructed program and the original. We validate our approach with
synthesized and human-crafted programmatic policies for playing a real-time
strategy game, comparing the interpretability scores of these programmatic
policies to obfuscated versions of the same programs. Our LLM-based
interpretability score consistently ranks less interpretable programs lower and
more interpretable ones higher. These findings suggest that our metric could
serve as a reliable and inexpensive tool for evaluating the interpretability of
programmatic policies.",None,-1
2dab3e4b-6ef7-4c7f-9da2-e048e4e088e9,Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided by Self-presentation Theory,0.398691,"Having the ability to empathize is crucial for accurately representing human
behavior during conversations. Despite numerous research aim to improve the
cognitive capability of models by incorporating external knowledge, there has
been limited attention on the sensible and rational expression of the
conversation itself, which are crucial components of the cognitive empathy.
Guided by self-presentation theory in sociology, we have designed an innovative
categorical approach that segregates historical dialogues into sensible and
rational sentences and subsequently elucidate the context through the designed
attention mechanism. However, the rational information within the conversation
is restricted and the external knowledge used in previous methods have
limitations of semantic contradiction and narrow vision field. Considering the
impressive performance of LLM in the domain of intelligent agent. We employ
LLaMA2-70b as a rational brain to analyze the profound logical information
maintained in conversations, which assists the model assessing the balance of
sensibility and rationality to produce quality empathetic responses.
Experimental evaluations demonstrate that our method outperforms other
comparable methods on both automatic and human evaluations.",None,-1
4a9d769b-20a5-475a-a9a0-a92e24912d49,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,0.998479,"We propose MM-Vet, an evaluation benchmark that examines large multimodal
models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various
intriguing abilities, such as solving math problems written on the blackboard,
reasoning about events and celebrities in news images, and explaining visual
jokes. Rapid model advancements pose challenges to evaluation benchmark
development. Problems include: (1) How to systematically structure and evaluate
the complicated multimodal tasks; (2) How to design evaluation metrics that
work well across question and answer types; and (3) How to give model insights
beyond a simple performance ranking. To this end, we present MM-Vet, designed
based on the insight that the intriguing ability to solve complicated tasks is
often achieved by a generalist model being able to integrate different core
vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and
examines the 16 integrations of interest derived from the capability
combination. For evaluation metrics, we propose an LLM-based evaluator for
open-ended outputs. The evaluator enables the evaluation across different
question types and answer styles, resulting in a unified scoring metric. We
evaluate representative LMMs on MM-Vet, providing insights into the
capabilities of different LMM system paradigms and models. Code and data are
available at https://github.com/yuweihao/MM-Vet.",None,-1
32ff8d3c-7a84-444d-9131-0b4784c52f31,Chordal Averaging on Flag Manifolds and Its Applications,0.268993,"This paper presents a new, provably-convergent algorithm for computing the
flag-mean and flag-median of a set of points on a flag manifold under the
chordal metric. The flag manifold is a mathematical space consisting of flags,
which are sequences of nested subspaces of a vector space that increase in
dimension. The flag manifold is a superset of a wide range of known matrix
spaces, including Stiefel and Grassmanians, making it a general object that is
useful in a wide variety computer vision problems.
  To tackle the challenge of computing first order flag statistics, we first
transform the problem into one that involves auxiliary variables constrained to
the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and
leveraging the numerical stability and efficiency of Stiefel-manifold
optimization enables us to compute the flag-mean effectively. Through a series
of experiments, we show the competence of our method in Grassmann and rotation
averaging, as well as principal component analysis. We release our source code
under https://github.com/nmank/FlagAveraging.",None,-1
626ffad7-a069-4c38-9b52-d98bd6421eba,A Game of Bundle Adjustment -- Learning Efficient Convergence,0.294678,"Bundle adjustment is the common way to solve localization and mapping. It is
an iterative process in which a system of non-linear equations is solved using
two optimization methods, weighted by a damping factor. In the classic
approach, the latter is chosen heuristically by the Levenberg-Marquardt
algorithm on each iteration. This might take many iterations, making the
process computationally expensive, which might be harmful to real-time
applications. We propose to replace this heuristic by viewing the problem in a
holistic manner, as a game, and formulating it as a reinforcement-learning
task. We set an environment which solves the non-linear equations and train an
agent to choose the damping factor in a learned manner. We demonstrate that our
approach considerably reduces the number of iterations required to reach the
bundle adjustment's convergence, on both synthetic and real-life scenarios. We
show that this reduction benefits the classic approach and can be integrated
with other bundle adjustment acceleration methods.",None,-1
4a3ad2f0-feb1-40d0-9a1c-da86c32daef1,Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks,0.703159,"In this study, we investigated whether self-supervised pretraining could
produce a neural network feature extractor applicable to multiple
classification tasks in B-mode lung ultrasound analysis. When fine-tuning on
three lung ultrasound tasks, pretrained models resulted in an improvement of
the average across-task area under the receiver operating curve (AUC) by 0.032
and 0.061 on local and external test sets respectively. Compact nonlinear
classifiers trained on features outputted by a single pretrained model did not
improve performance across all tasks; however, they did reduce inference time
by 49% compared to serial execution of separate fine-tuned models. When
training using 1% of the available labels, pretrained models consistently
outperformed fully supervised models, with a maximum observed test AUC increase
of 0.396 for the task of view classification. Overall, the results indicate
that self-supervised pretraining is useful for producing initial weights for
lung ultrasound classifiers.",None,-1
72dffa19-4385-4748-9b43-31a8376405e9,SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model,0.209392,"We present SkillGPT, a tool for skill extraction and standardization (SES)
from free-style job descriptions and user profiles with an open-source Large
Language Model (LLM) as backbone. Most previous methods for similar tasks
either need supervision or rely on heavy data-preprocessing and feature
engineering. Directly prompting the latest conversational LLM for standard
skills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes
a LLM to perform its tasks in steps via summarization and vector similarity
search, to balance speed with precision. The backbone LLM of SkillGPT is based
on Llama, free for academic use and thus useful for exploratory research and
prototype development. Hence, our cost-free SkillGPT gives users the
convenience of conversational SES, efficiently and reliably.",None,-1
54c50691-52e2-4412-a41c-3ea663703df7,Behind the Scenes: Density Fields for Single View Reconstruction,0.79327,"Inferring a meaningful geometric scene representation from a single image is
a fundamental problem in computer vision. Approaches based on traditional depth
map prediction can only reason about areas that are visible in the image.
Currently, neural radiance fields (NeRFs) can capture true 3D including color,
but are too complex to be generated from a single image. As an alternative, we
propose to predict implicit density fields. A density field maps every location
in the frustum of the input image to volumetric density. By directly sampling
color from the available views instead of storing color in the density field,
our scene representation becomes significantly less complex compared to NeRFs,
and a neural network can predict it in a single forward pass. The prediction
network is trained through self-supervision from only video data. Our
formulation allows volume rendering to perform both depth prediction and novel
view synthesis. Through experiments, we show that our method is able to predict
meaningful geometry for regions that are occluded in the input image.
Additionally, we demonstrate the potential of our approach on three datasets
for depth prediction and novel-view synthesis.",None,-1
232f532b-c654-41f1-aeb1-325792a90405,Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language,0.994037,"We propose LENS, a modular approach for tackling computer vision problems by
leveraging the power of large language models (LLMs). Our system uses a
language model to reason over outputs from a set of independent and highly
descriptive vision modules that provide exhaustive information about an image.
We evaluate the approach on pure computer vision settings such as zero- and
few-shot object recognition, as well as on vision and language problems. LENS
can be applied to any off-the-shelf LLM and we find that the LLMs with LENS
perform highly competitively with much bigger and much more sophisticated
systems, without any multimodal training whatsoever. We open-source our code at
https://github.com/ContextualAI/lens and provide an interactive demo.",None,-1
10d9ac55-3284-49c3-8b41-6108f40bb0c0,Deep Learning in the Field of Biometric Template Protection: An Overview,0.922522,"Today, deep learning represents the most popular and successful form of
machine learning. Deep learning has revolutionised the field of pattern
recognition, including biometric recognition. Biometric systems utilising deep
learning have been shown to achieve auspicious recognition accuracy, surpassing
human performance. Apart from said breakthrough advances in terms of biometric
performance, the use of deep learning was reported to impact different
covariates of biometrics such as algorithmic fairness, vulnerability to
attacks, or template protection. Technologies of biometric template protection
are designed to enable a secure and privacy-preserving deployment of
biometrics. In the recent past, deep learning techniques have been frequently
applied in biometric template protection systems for various purposes. This
work provides an overview of how advances in deep learning take influence on
the field of biometric template protection. The interrelation between improved
biometric performance rates and security in biometric template protection is
elaborated. Further, the use of deep learning for obtaining feature
representations that are suitable for biometric template protection is
discussed. Novel methods that apply deep learning to achieve various goals of
biometric template protection are surveyed along with deep learning-based
attacks.",None,-1
277c58ac-f44c-4531-b5f4-0fb613d9ad6b,Feature Mixing for Writer Retrieval and Identification on Papyri Fragments,0.614179,"This paper proposes a deep-learning-based approach to writer retrieval and
identification for papyri, with a focus on identifying fragments associated
with a specific writer and those corresponding to the same image. We present a
novel neural network architecture that combines a residual backbone with a
feature mixing stage to improve retrieval performance, and the final descriptor
is derived from a projection layer. The methodology is evaluated on two
benchmarks: PapyRow, where we achieve a mAP of 26.6 % and 24.9 % on writer and
page retrieval, and HisFragIR20, showing state-of-the-art performance (44.0 %
and 29.3 % mAP). Furthermore, our network has an accuracy of 28.7 % for writer
identification. Additionally, we conduct experiments on the influence of two
binarization techniques on fragments and show that binarizing does not enhance
performance. Our code and models are available to the community.",None,-1
f545c4a2-1b74-400c-8978-06e4ee200b7b,Data Quality-aware Mixed-precision Quantization via Hybrid Reinforcement Learning,0.905327,"Mixed-precision quantization mostly predetermines the model bit-width
settings before actual training due to the non-differential bit-width sampling
process, obtaining sub-optimal performance. Worse still, the conventional
static quality-consistent training setting, i.e., all data is assumed to be of
the same quality across training and inference, overlooks data quality changes
in real-world applications which may lead to poor robustness of the quantized
models. In this paper, we propose a novel Data Quality-aware Mixed-precision
Quantization framework, dubbed DQMQ, to dynamically adapt quantization
bit-widths to different data qualities. The adaption is based on a bit-width
decision policy that can be learned jointly with the quantization training.
Concretely, DQMQ is modeled as a hybrid reinforcement learning (RL) task that
combines model-based policy optimization with supervised quantization training.
By relaxing the discrete bit-width sampling to a continuous probability
distribution that is encoded with few learnable parameters, DQMQ is
differentiable and can be directly optimized end-to-end with a hybrid
optimization target considering both task performance and quantization
benefits. Trained on mixed-quality image datasets, DQMQ can implicitly select
the most proper bit-width for each layer when facing uneven input qualities.
Extensive experiments on various benchmark datasets and networks demonstrate
the superiority of DQMQ against existing fixed/mixed-precision quantization
methods.",None,-1
bbc536b0-3c5b-4020-b915-7d7b1078a113,Contextual Object Detection with Multimodal Large Language Models,0.855345,"Recent Multimodal Large Language Models (MLLMs) are remarkable in
vision-language tasks, such as image captioning and question answering, but
lack the essential perception ability, i.e., object detection. In this work, we
address this limitation by introducing a novel research problem of contextual
object detection -- understanding visible objects within different human-AI
interactive contexts. Three representative scenarios are investigated,
including the language cloze test, visual captioning, and question answering.
Moreover, we present ContextDET, a unified multimodal model that is capable of
end-to-end differentiable modeling of visual-language contexts, so as to
locate, identify, and associate visual objects with language inputs for
human-AI interaction. Our ContextDET involves three key submodels: (i) a visual
encoder for extracting visual representations, (ii) a pre-trained LLM for
multimodal context decoding, and (iii) a visual decoder for predicting bounding
boxes given contextual object words. The new generate-then-detect framework
enables us to detect object words within human vocabulary. Extensive
experiments show the advantages of ContextDET on our proposed CODE benchmark,
open-vocabulary detection, and referring image segmentation. Github:
https://github.com/yuhangzang/ContextDET.",None,-1
def3adc1-16d5-4ebd-9e32-1c8ac9514745,Towards Few-shot Inductive Link Prediction on Knowledge Graphs: A Relational Anonymous Walk-guided Neural Process Approach,0.705425,"Few-shot inductive link prediction on knowledge graphs (KGs) aims to predict
missing links for unseen entities with few-shot links observed. Previous
methods are limited to transductive scenarios, where entities exist in the
knowledge graphs, so they are unable to handle unseen entities. Therefore,
recent inductive methods utilize the sub-graphs around unseen entities to
obtain the semantics and predict links inductively. However, in the few-shot
setting, the sub-graphs are often sparse and cannot provide meaningful
inductive patterns. In this paper, we propose a novel relational anonymous
walk-guided neural process for few-shot inductive link prediction on knowledge
graphs, denoted as RawNP. Specifically, we develop a neural process-based
method to model a flexible distribution over link prediction functions. This
enables the model to quickly adapt to new entities and estimate the uncertainty
when making predictions. To capture general inductive patterns, we present a
relational anonymous walk to extract a series of relational motifs from
few-shot observations. These motifs reveal the distinctive semantic patterns on
KGs that support inductive predictions. Extensive experiments on typical
benchmark datasets demonstrate that our model derives new state-of-the-art
performance.",None,-1
49e50e48-164c-4a41-96bd-7438bda3206f,Can Large Language Models Augment a Biomedical Ontology with missing Concepts and Relations?,0.530298,"Ontologies play a crucial role in organizing and representing knowledge.
However, even current ontologies do not encompass all relevant concepts and
relationships. Here, we explore the potential of large language models (LLM) to
expand an existing ontology in a semi-automated fashion. We demonstrate our
approach on the biomedical ontology SNOMED-CT utilizing semantic relation types
from the widely used UMLS semantic network. We propose a method that uses
conversational interactions with an LLM to analyze clinical practice guidelines
(CPGs) and detect the relationships among the new medical concepts that are not
present in SNOMED-CT. Our initial experimentation with the conversational
prompts yielded promising preliminary results given a manually generated gold
standard, directing our future potential improvements.",None,-1
158b5b30-16c1-41ae-b8b7-e94e87fb7981,Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture,0.269404,"While deep neural networks have led to major advances in image recognition,
language translation, data mining, and game playing, there are well-known
limits to the paradigm such as lack of explainability, difficulty of
incorporating prior knowledge, and modularity. Neuro symbolic hybrid systems
have recently emerged as a straightforward way to extend deep neural networks
by incorporating ideas from symbolic reasoning such as computational logic. In
this paper, we propose a list desirable criteria for neuro symbolic systems and
examine how some of the existing approaches address these criteria. We then
propose an extension to generalized annotated logic that allows for the
creation of an equivalent neural architecture comprising an alternate neuro
symbolic hybrid. However, unlike previous approaches that rely on continuous
optimization for the training process, our framework is designed as a binarized
neural network that uses discrete optimization. We provide proofs of
correctness and discuss several of the challenges that must be overcome to
realize this framework in an implemented system.",None,-1
36ae2cc9-e5aa-4422-bd95-6314fd5e6869,Almost optimal manipulation of a pair of alternatives,0.0677996,"The role of an expert in the decision-making process is crucial, as the final
recommendation depends on his disposition, clarity of mind, experience, and
knowledge of the problem. However, the recommendation also depends on their
honesty. But what if the expert is dishonest? Then, the answer on how difficult
it is to manipulate in a given case becomes essential. In the presented work,
we consider manipulation of a ranking obtained by comparing alternatives in
pairs. More specifically, we propose an algorithm for finding an almost optimal
way to swap the positions of two selected alternatives. Thanks to this, it is
possible to determine how difficult such manipulation is in a given case.
Theoretical considerations are illustrated by a practical example.",None,-1
0b0a2079-fee7-44ea-8941-7d16f823ec78,A Comparative Analysis of CNN-Based Pretrained Models for the Detection and Prediction of Monkeypox,0.294249,"Monkeypox is a rare disease that raised concern among medical specialists
following the convi-19 pandemic. It's concerning since monkeypox is difficult
to diagnose early on because of symptoms that are similar to chickenpox and
measles. Furthermore, because this is a rare condition, there is a knowledge
gap among healthcare professionals. As a result, there is an urgent need for a
novel technique to combat and anticipate the disease in the early phases of
individual virus infection. Multiple CNN-based pre-trained models, including
VGG-16, VGG-19, Restnet50, Inception-V3, Densnet, Xception, MobileNetV2,
Alexnet, Lenet, and majority Voting, were employed in classification in this
study. For this study, multiple data sets were combined, such as monkeypox vs
chickenpox, monkeypox versus measles, monkeypox versus normal, and monkeypox
versus all diseases. Majority voting performed 97% in monkeypox vs chickenpox,
Xception achieved 79% in monkeypox against measles, MobileNetV2 scored 96% in
monkeypox vs normal, and Lenet performed 80% in monkeypox versus all.",None,-1
0dcecf75-c132-4b4f-b296-7a4d4e33c595,Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation,0.483877,"The objective of this paper is self-supervised learning of video object
segmentation. We develop a unified framework which simultaneously models
cross-frame dense correspondence for locally discriminative feature learning
and embeds object-level context for target-mask decoding. As a result, it is
able to directly learn to perform mask-guided sequential segmentation from
unlabeled videos, in contrast to previous efforts usually relying on an oblique
solution - cheaply ""copying"" labels according to pixel-wise correlations.
Concretely, our algorithm alternates between i) clustering video pixels for
creating pseudo segmentation labels ex nihilo; and ii) utilizing the pseudo
labels to learn mask encoding and decoding for VOS. Unsupervised correspondence
learning is further incorporated into this self-taught, mask embedding scheme,
so as to ensure the generic nature of the learnt representation and avoid
cluster degeneracy. Our algorithm sets state-of-the-arts on two standard
benchmarks (i.e., DAVIS17 and YouTube-VOS), narrowing the gap between self- and
fully-supervised VOS, in terms of both performance and network architecture
design.",None,-1
7f9d7464-f340-4325-bc66-80d23e719c93,Explicit Visual Prompting for Low-Level Structure Segmentations,0.999731,"We consider the generic problem of detecting low-level structures in images,
which includes segmenting the manipulated parts, identifying out-of-focus
pixels, separating shadow regions, and detecting concealed objects. Whereas
each such topic has been typically addressed with a domain-specific solution,
we show that a unified approach performs well across all of them. We take
inspiration from the widely-used pre-training and then prompt tuning protocols
in NLP and propose a new visual prompting model, named Explicit Visual
Prompting (EVP). Different from the previous visual prompting which is
typically a dataset-level implicit embedding, our key insight is to enforce the
tunable parameters focusing on the explicit visual content from each individual
image, i.e., the features from frozen patch embeddings and the input's
high-frequency components. The proposed EVP significantly outperforms other
parameter-efficient tuning protocols under the same amount of tunable
parameters (5.7% extra trainable parameters of each task). EVP also achieves
state-of-the-art performances on diverse low-level structure segmentation tasks
compared to task-specific solutions. Our code is available at:
https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",None,-1
281ba16c-d6b0-4ae3-ab9c-1ecbfa01db99,VARS: Video Assistant Referee System for Automated Soccer Decision Making from Multiple Views,0.99769,"The Video Assistant Referee (VAR) has revolutionized association football,
enabling referees to review incidents on the pitch, make informed decisions,
and ensure fairness. However, due to the lack of referees in many countries and
the high cost of the VAR infrastructure, only professional leagues can benefit
from it. In this paper, we propose a Video Assistant Referee System (VARS) that
can automate soccer decision-making. VARS leverages the latest findings in
multi-view video analysis, to provide real-time feedback to the referee, and
help them make informed decisions that can impact the outcome of a game. To
validate VARS, we introduce SoccerNet-MVFoul, a novel video dataset of soccer
fouls from multiple camera views, annotated with extensive foul descriptions by
a professional soccer referee, and we benchmark our VARS to automatically
recognize the characteristics of these fouls. We believe that VARS has the
potential to revolutionize soccer refereeing and take the game to new heights
of fairness and accuracy across all levels of professional and amateur
federations.",None,-1
be629738-090d-4898-966b-765d471f3428,E-NER: Evidential Deep Learning for Trustworthy Named Entity Recognition,0.507972,"Most named entity recognition (NER) systems focus on improving model
performance, ignoring the need to quantify model uncertainty, which is critical
to the reliability of NER systems in open environments. Evidential deep
learning (EDL) has recently been proposed as a promising solution to explicitly
model predictive uncertainty for classification tasks. However, directly
applying EDL to NER applications faces two challenges, i.e., the problems of
sparse entities and OOV/OOD entities in NER tasks. To address these challenges,
we propose a trustworthy NER framework named E-NER by introducing two
uncertainty-guided loss terms to the conventional EDL, along with a series of
uncertainty-guided training strategies. Experiments show that E-NER can be
applied to multiple NER paradigms to obtain accurate uncertainty estimation.
Furthermore, compared to state-of-the-art baselines, the proposed method
achieves a better OOV/OOD detection performance and better generalization
ability on OOV entities.",None,-1
13d7ffb3-5163-4378-87b2-6398151a0312,MBPTrack: Improving 3D Point Cloud Tracking with Memory Networks and Box Priors,0.470477,"3D single object tracking has been a crucial problem for decades with
numerous applications such as autonomous driving. Despite its wide-ranging use,
this task remains challenging due to the significant appearance variation
caused by occlusion and size differences among tracked targets. To address
these issues, we present MBPTrack, which adopts a Memory mechanism to utilize
past information and formulates localization in a coarse-to-fine scheme using
Box Priors given in the first frame. Specifically, past frames with targetness
masks serve as an external memory, and a transformer-based module propagates
tracked target cues from the memory to the current frame. To precisely localize
objects of all sizes, MBPTrack first predicts the target center via Hough
voting. By leveraging box priors given in the first frame, we adaptively sample
reference points around the target center that roughly cover the target of
different sizes. Then, we obtain dense feature maps by aggregating point
features into the reference points, where localization can be performed more
effectively. Extensive experiments demonstrate that MBPTrack achieves
state-of-the-art performance on KITTI, nuScenes and Waymo Open Dataset, while
running at 50 FPS on a single RTX3090 GPU.",None,-1
415c983d-3718-49cb-9979-58cc79a6bc4d,Towards Local Visual Modeling for Image Captioning,0.738976,"In this paper, we study the local visual modeling with grid features for
image captioning, which is critical for generating accurate and detailed
captions. To achieve this target, we propose a Locality-Sensitive Transformer
Network (LSTNet) with two novel designs, namely Locality-Sensitive Attention
(LSA) and Locality-Sensitive Fusion (LSF). LSA is deployed for the intra-layer
interaction in Transformer via modeling the relationship between each grid and
its neighbors. It reduces the difficulty of local object recognition during
captioning. LSF is used for inter-layer information fusion, which aggregates
the information of different encoder layers for cross-layer semantical
complementarity. With these two novel designs, the proposed LSTNet can model
the local visual information of grid features to improve the captioning
quality. To validate LSTNet, we conduct extensive experiments on the
competitive MS-COCO benchmark. The experimental results show that LSTNet is not
only capable of local visual modeling, but also outperforms a bunch of
state-of-the-art captioning models on offline and online testings, i.e., 134.8
CIDEr and 136.3 CIDEr, respectively. Besides, the generalization of LSTNet is
also verified on the Flickr8k and Flickr30k datasets",None,-1
e42ce736-fe5a-4948-828a-3ed7f43ac0f3,Practical Knowledge Distillation: Using DNNs to Beat DNNs,0.0878606,"For tabular data sets, we explore data and model distillation, as well as
data denoising. These techniques improve both gradient-boosting models and a
specialized DNN architecture. While gradient boosting is known to outperform
DNNs on tabular data, we close the gap for datasets with 100K+ rows and give
DNNs an advantage on small data sets. We extend these results with input-data
distillation and optimized ensembling to help DNN performance match or exceed
that of gradient boosting. As a theoretical justification of our practical
method, we prove its equivalence to classical cross-entropy knowledge
distillation. We also qualitatively explain the superiority of DNN ensembles
over XGBoost on small data sets. For an industry end-to-end real-time ML
platform with 4M production inferences per second, we develop a model-training
workflow based on data sampling that distills ensembles of models into a single
gradient-boosting model favored for high-performance real-time inference,
without performance loss. Empirical evaluation shows that the proposed
combination of methods consistently improves model accuracy over prior best
models across several production applications deployed worldwide.",None,-1
43d42b9c-65a6-4a44-aa45-122eb7ec42ff,Automated multilingual detection of Pro-Kremlin propaganda in newspapers and Telegram posts,0.536058,"The full-scale conflict between the Russian Federation and Ukraine generated
an unprecedented amount of news articles and social media data reflecting
opposing ideologies and narratives. These polarized campaigns have led to
mutual accusations of misinformation and fake news, shaping an atmosphere of
confusion and mistrust for readers worldwide. This study analyses how the media
affected and mirrored public opinion during the first month of the war using
news articles and Telegram news channels in Ukrainian, Russian, Romanian and
English. We propose and compare two methods of multilingual automated
pro-Kremlin propaganda identification, based on Transformers and linguistic
features. We analyse the advantages and disadvantages of both methods, their
adaptability to new genres and languages, and ethical considerations of their
usage for content moderation. With this work, we aim to lay the foundation for
further development of moderation tools tailored to the current conflict.",None,-1
8a4ae0fe-1742-417d-974f-251bb39cab2a,FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users,0.532479,"The federated learning (FL) technique was developed to mitigate data privacy
issues in the traditional machine learning paradigm. While FL ensures that a
user's data always remain with the user, the gradients are shared with the
centralized server to build the global model. This results in privacy leakage,
where the server can infer private information from the shared gradients. To
mitigate this flaw, the next-generation FL architectures proposed encryption
and anonymization techniques to protect the model updates from the server.
However, this approach creates other challenges, such as malicious users
sharing false gradients. Since the gradients are encrypted, the server is
unable to identify rogue users. To mitigate both attacks, this paper proposes a
novel FL algorithm based on a fully homomorphic encryption (FHE) scheme. We
develop a distributed multi-key additive homomorphic encryption scheme that
supports model aggregation in FL. We also develop a novel aggregation scheme
within the encrypted domain, utilizing users' non-poisoning rates, to
effectively address data poisoning attacks while ensuring privacy is preserved
by the proposed encryption scheme. Rigorous security, privacy, convergence, and
experimental analyses have been provided to show that FheFL is novel, secure,
and private, and achieves comparable accuracy at reasonable computational cost.",None,-1
01f592ff-6746-4f6c-b9e4-5d5b8b0306fe,Normalization-Equivariant Neural Networks with Application to Image Denoising,0.237778,"In many information processing systems, it may be desirable to ensure that
any change of the input, whether by shifting or scaling, results in a
corresponding change in the system response. While deep neural networks are
gradually replacing all traditional automatic processing methods, they
surprisingly do not guarantee such normalization-equivariance (scale + shift)
property, which can be detrimental in many applications. To address this issue,
we propose a methodology for adapting existing neural networks so that
normalization-equivariance holds by design. Our main claim is that not only
ordinary convolutional layers, but also all activation functions, including the
ReLU (rectified linear unit), which are applied element-wise to the
pre-activated neurons, should be completely removed from neural networks and
replaced by better conditioned alternatives. To this end, we introduce
affine-constrained convolutions and channel-wise sort pooling layers as
surrogates and show that these two architectural modifications do preserve
normalization-equivariance without loss of performance. Experimental results in
image denoising show that normalization-equivariant neural networks, in
addition to their better conditioning, also provide much better generalization
across noise levels.",None,-1
4dc530dc-41f0-4a18-8653-f9afc416f86a,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",0.998601,"We introduce k-planes, a white-box model for radiance fields in arbitrary
dimensions. Our model uses d choose 2 planes to represent a d-dimensional
scene, providing a seamless way to go from static (d=3) to dynamic (d=4)
scenes. This planar factorization makes adding dimension-specific priors easy,
e.g. temporal smoothness and multi-resolution spatial structure, and induces a
natural decomposition of static and dynamic components of a scene. We use a
linear feature decoder with a learned color basis that yields similar
performance as a nonlinear black-box MLP decoder. Across a range of synthetic
and real, static and dynamic, fixed and varying appearance scenes, k-planes
yields competitive and often state-of-the-art reconstruction fidelity with low
memory usage, achieving 1000x compression over a full 4D grid, and fast
optimization with a pure PyTorch implementation. For video results and code,
please see https://sarafridov.github.io/K-Planes.",None,-1
e4bc4dee-6aad-46e3-a50f-598dea2f2383,Aligning Language Models to User Opinions,0.978395,"An important aspect of developing LLMs that interact with humans is to align
models' behavior to their users. It is possible to prompt an LLM into behaving
as a certain persona, especially a user group or ideological persona the model
captured during its pertaining stage. But, how to best align an LLM with a
specific user and not a demographic or ideological group remains an open
question. Mining public opinion surveys (by Pew Research), we find that the
opinions of a user and their demographics and ideologies are not mutual
predictors. We use this insight to align LLMs by modeling both user opinions as
well as user demographics and ideology, achieving up to 7 points accuracy gains
in predicting public opinions from survey questions across a broad set of
topics. In addition to the typical approach of prompting LLMs with demographics
and ideology, we discover that utilizing the most relevant past opinions from
individual users enables the model to predict user opinions more accurately.",None,-1
7b195a41-8fe1-40f3-86b5-e5fedcb09bfc,Segment Anything Meets Semantic Communication,0.738594,"In light of the diminishing returns of traditional methods for enhancing
transmission rates, the domain of semantic communication presents promising new
frontiers. Focusing on image transmission, this paper explores the application
of foundation models, particularly the Segment Anything Model (SAM) developed
by Meta AI Research, to improve semantic communication. SAM is a promptable
image segmentation model that has gained attention for its ability to perform
zero-shot segmentation tasks without explicit training or domain-specific
knowledge. By employing SAM's segmentation capability and lightweight neural
network architecture for semantic coding, we propose a practical approach to
semantic communication. We demonstrate that this approach retains critical
semantic features, achieving higher image reconstruction quality and reducing
communication overhead. This practical solution eliminates the
resource-intensive stage of training a segmentation model and can be applied to
any semantic coding architecture, paving the way for real-world applications.",None,-1
9d59deed-5474-4ee0-a479-ab83d5a5ab4c,UstanceBR: a multimodal language resource for stance prediction,0.108154,"This work introduces UstanceBR, a multimodal corpus in the Brazilian
Portuguese Twitter domain for target-based stance prediction. The corpus
comprises 86.8 k labelled stances towards selected target topics, and extensive
network information about the users who published these stances on social
media. In this article we describe the corpus multimodal data, and a number of
usage examples in both in-domain and zero-shot stance prediction based on text-
and network-related information, which are intended to provide initial baseline
results for future studies in the field.",None,-1
cdc4487e-186c-455b-b07e-fee6fefd1b53,Propagating Knowledge Updates to LMs Through Distillation,0.536128,"Modern language models have the capacity to store and use immense amounts of
knowledge about real-world entities, but it remains unclear how to update such
knowledge stored in model parameters. While prior methods for updating
knowledge in LMs successfully inject atomic facts, updated LMs fail to make
inferences based on injected facts. In this work, we demonstrate that a context
distillation-based approach can both impart knowledge about entities and
propagate that knowledge to enable broader inferences. Our approach consists of
two stages: transfer set generation and distillation on the transfer set. We
first generate a transfer set by prompting a language model to generate
continuations from the entity definition. Then, we update the model parameters
so that the distribution of the LM (the student) matches the distribution of
the LM conditioned on the definition (the teacher) on the transfer set. Our
experiments demonstrate that this approach is more effective at propagating
knowledge updates than fine-tuning and other gradient-based knowledge-editing
methods. Moreover, it does not compromise performance in other contexts, even
when injecting the definitions of up to 150 entities at once.",None,-1
2a113652-6bef-4ac6-bb50-03ed1aec7307,DehazeNeRF: Multiple Image Haze Removal and 3D Shape Reconstruction using Neural Radiance Fields,0.709705,"Neural radiance fields (NeRFs) have demonstrated state-of-the-art performance
for 3D computer vision tasks, including novel view synthesis and 3D shape
reconstruction. However, these methods fail in adverse weather conditions. To
address this challenge, we introduce DehazeNeRF as a framework that robustly
operates in hazy conditions. DehazeNeRF extends the volume rendering equation
by adding physically realistic terms that model atmospheric scattering. By
parameterizing these terms using suitable networks that match the physical
properties, we introduce effective inductive biases, which, together with the
proposed regularizations, allow DehazeNeRF to demonstrate successful multi-view
haze removal, novel view synthesis, and 3D shape reconstruction where existing
approaches fail.",None,-1
6dfd82e2-02c9-4437-a324-37acb489e243,LLaMA-E: Empowering E-commerce Authoring with Object-Interleaved Instruction Following,0.661069,"E-commerce authoring entails creating engaging, diverse, and targeted content
to enhance preference elicitation and retrieval experience. While Large
Language Models (LLMs) have revolutionized content generation, they often fall
short in e-commerce applications due to their limited memorization of
domain-specific features. This paper proposes LLaMA-E, the unified e-commerce
authoring models that address the contextual preferences of customers, sellers,
and platforms, the essential objects in e-commerce operation. We design the
instruction set derived from tasks of ads generation, query-enhanced product
title rewriting, product classification, purchase intent speculation, and
general e-commerce Q&A. The instruction formulation ensures the interleaved
cover of the presented and required object features, allowing the alignment of
base models to parameterise e-commerce knowledge comprehensively. The proposed
LLaMA-E models achieve state-of-the-art evaluation performance and exhibit the
advantage in zero-shot practical applications. To our knowledge, this is the
first LLM tailored to empower authoring applications with comprehensive
scenario understanding by integrating features focused on participated objects.",None,-1
7f26dde6-3957-49ac-addb-10d6053b1c43,MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems,0.736872,"MeetEval is an open-source toolkit to evaluate all kinds of meeting
transcription systems. It provides a unified interface for the computation of
commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER
along other WER definitions. We extend the cpWER computation by a temporal
constraint to ensure that only words are identified as correct when the
temporal alignment is plausible. This leads to a better quality of the matching
of the hypothesis string to the reference string that more closely resembles
the actual transcription quality, and a system is penalized if it provides poor
time annotations. Since word-level timing information is often not available,
we present a way to approximate exact word-level timings from segment-level
timings (e.g., a sentence) and show that the approximation leads to a similar
WER as a matching with exact word-level annotations. At the same time, the time
constraint leads to a speedup of the matching algorithm, which outweighs the
additional overhead caused by processing the time stamps.",None,-1
40fe6408-a2a5-4c2f-8f9c-aacd44d35200,Guilt Detection in Text: A Step Towards Understanding Complex Emotions,0.0456686,"We introduce a novel Natural Language Processing (NLP) task called Guilt
detection, which focuses on detecting guilt in text. We identify guilt as a
complex and vital emotion that has not been previously studied in NLP, and we
aim to provide a more fine-grained analysis of it. To address the lack of
publicly available corpora for guilt detection, we created VIC, a dataset
containing 4622 texts from three existing emotion detection datasets that we
binarized into guilt and no-guilt classes. We experimented with traditional
machine learning methods using bag-of-words and term frequency-inverse document
frequency features, achieving a 72% f1 score with the highest-performing model.
Our study provides a first step towards understanding guilt in text and opens
the door for future research in this area.",None,-1
492a9668-a582-4c33-9075-519bea8e02ea,The Program Testing Ability of Large Language Models for Code,0.248769,"Recent development of large language models (LLMs) for code like CodeX and
CodeT5+ demonstrates tremendous promise in achieving code intelligence. Their
ability of synthesizing code that completes a program for performing a
pre-defined task has been intensively tested and verified on benchmark datasets
including HumanEval and MBPP. Yet, evaluation of these LLMs from more
perspectives (than just program synthesis) is also anticipated, considering
their broad scope of applications in software engineering. In this paper, we
explore the ability of LLMs for testing programs/code. By performing thorough
analyses of recent LLMs for code in program testing, we show a series of
intriguing properties of these models and demonstrate how program testing
ability of LLMs can be improved. Following recent work which utilizes generated
test cases to enhance program synthesis, we further leverage our findings in
improving the quality of the synthesized programs and show +11.77% and +4.22%
higher code pass rates on HumanEval+ comparing with the GPT-3.5-turbo baseline
and the recent state-of-the-art, respectively.",None,-1
2387cc63-8387-4750-bf11-35b4bf9690d9,Interventional Probing in High Dimensions: An NLI Case Study,0.261085,"Probing strategies have been shown to detect the presence of various
linguistic features in large language models; in particular, semantic features
intermediate to the ""natural logic"" fragment of the Natural Language Inference
task (NLI). In the case of natural logic, the relation between the intermediate
features and the entailment label is explicitly known: as such, this provides a
ripe setting for interventional studies on the NLI models' representations,
allowing for stronger causal conjectures and a deeper critical analysis of
interventional probing methods. In this work, we carry out new and existing
representation-level interventions to investigate the effect of these semantic
features on NLI classification: we perform amnesic probing (which removes
features as directed by learned linear probes) and introduce the mnestic
probing variation (which forgets all dimensions except the probe-selected
ones). Furthermore, we delve into the limitations of these methods and outline
some pitfalls have been obscuring the effectivity of interventional probing
studies.",None,-1
f681a533-1b4e-408c-813b-60995b2e9110,Eye Disease Classification Using Deep Learning Techniques,0.5359,"Eye is the essential sense organ for vision function. Due to the fact that
certain eye disorders might result in vision loss, it is essential to diagnose
and treat eye diseases early on. By identifying common eye illnesses and
performing an eye check, eye care providers can safeguard patients against
vision loss or blindness. Convolutional neural networks (CNN) and transfer
learning were employed in this study to discriminate between a normal eye and
one with diabetic retinopathy, cataract, or glaucoma disease. Using transfer
learning for multi-class classification, high accuracy was achieved at 94%
while the traditional CNN achieved 84% rate.",None,-1
3a15ddee-94c7-48f9-9bc2-e8acb00179a7,On Training Derivative-Constrained Neural Networks,0.337359,"We refer to the setting where the (partial) derivatives of a neural network's
(NN's) predictions with respect to its inputs are used as additional training
signal as a derivative-constrained (DC) NN. This situation is common in
physics-informed settings in the natural sciences. We propose an integrated
RELU (IReLU) activation function to improve training of DC NNs. We also
investigate denormalization and label rescaling to help stabilize DC training.
We evaluate our methods on physics-informed settings including quantum
chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that
existing architectures with IReLU activations combined with denormalization and
label rescaling better incorporate training signal provided by derivative
constraints.",None,-1
8f659afb-cba8-4d08-956e-3d6dad722ad2,A Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3,0.832124,"LawGPT 1.0 is a virtual legal assistant built on the state-of-the-art
language model GPT-3, fine-tuned for the legal domain. The system is designed
to provide legal assistance to users in a conversational manner, helping them
with tasks such as answering legal questions, generating legal documents, and
providing legal advice. In this paper, we provide a brief overview of LawGPT
1.0, its architecture, and its performance on a set of legal benchmark tasks.
Please note that the detailed information about the model is protected by a
non-disclosure agreement (NDA) and cannot be disclosed in this report.",None,-1
145714a0-6f94-4c42-b927-da12e49a7417,LE2Fusion: A novel local edge enhancement module for infrared and visible image fusion,0.13255,"Infrared and visible image fusion task aims to generate a fused image which
contains salient features and rich texture details from multi-source images.
However, under complex illumination conditions, few algorithms pay attention to
the edge information of local regions which is crucial for downstream tasks. To
this end, we propose a fusion network based on the local edge enhancement,
named LE2Fusion. Specifically, a local edge enhancement (LE2) module is
proposed to improve the edge information under complex illumination conditions
and preserve the essential features of image. For feature extraction, a
multi-scale residual attention (MRA) module is applied to extract rich
features. Then, with LE2, a set of enhancement weights are generated which are
utilized in feature fusion strategy and used to guide the image reconstruction.
To better preserve the local detail information and structure information, the
pixel intensity loss function based on the local region is also presented. The
experiments demonstrate that the proposed method exhibits better fusion
performance than the state-of-the-art fusion methods on public datasets.",None,-1
25286b7e-f19d-4bda-89fe-f2617dc4d657,KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation,0.571333,"In text classification tasks, fine tuning pretrained language models like
BERT and GPT-3 yields competitive accuracy; however, both methods require
pretraining on large text datasets. In contrast, general topic modeling methods
possess the advantage of analyzing documents to extract meaningful patterns of
words without the need of pretraining. To leverage topic modeling's
unsupervised insights extraction on text classification tasks, we develop the
Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires
no pretrained embeddings, few labeled documents and is efficient to train,
making it ideal under resource constrained settings. Across a variety of
datasets, our method outperforms existing supervised topic modeling methods in
classification accuracy, robustness and efficiency and achieves similar
performance compare to state of the art weakly supervised text classification
methods.",None,-1
c0e33b2d-e42d-4584-9f57-c8a04ac2cf1c,Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception,0.153049,"Vision-language models (VLMs) have shown powerful capabilities in visual
question answering and reasoning tasks by combining visual representations with
the abstract skill set large language models (LLMs) learn during pretraining.
Vision, while the most popular modality to augment LLMs with, is only one
representation of a scene. In human-robot interaction scenarios, robot
perception requires accurate scene understanding by the robot. In this paper,
we define and demonstrate a method of aligning the embedding spaces of
different modalities (in this case, inertial measurement unit (IMU) data) to
the vision embedding space through a combination of supervised and contrastive
training, enabling the VLM to understand and reason about these additional
modalities without retraining. We opt to give the model IMU embeddings directly
over using a separate human activity recognition model that feeds directly into
the prompt to allow for any nonlinear interactions between the query, image,
and IMU signal that would be lost by mapping the IMU data to a discrete
activity label. Further, we demonstrate our methodology's efficacy through
experiments involving human activity recognition using IMU data and visual
inputs. Our results show that using multiple modalities as input improves the
VLM's scene understanding and enhances its overall performance in various
tasks, thus paving the way for more versatile and capable language models in
multi-modal contexts.",None,-1
9a01329e-2619-4c48-84dc-3ddf5946d593,Cumulative Spatial Knowledge Distillation for Vision Transformers,0.410412,"Distilling knowledge from convolutional neural networks (CNNs) is a
double-edged sword for vision transformers (ViTs). It boosts the performance
since the image-friendly local-inductive bias of CNN helps ViT learn faster and
better, but leading to two problems: (1) Network designs of CNN and ViT are
completely different, which leads to different semantic levels of intermediate
features, making spatial-wise knowledge transfer methods (e.g., feature
mimicking) inefficient. (2) Distilling knowledge from CNN limits the network
convergence in the later training period since ViT's capability of integrating
global information is suppressed by CNN's local-inductive-bias supervision. To
this end, we present Cumulative Spatial Knowledge Distillation (CSKD). CSKD
distills spatial-wise knowledge to all patch tokens of ViT from the
corresponding spatial responses of CNN, without introducing intermediate
features. Furthermore, CSKD exploits a Cumulative Knowledge Fusion (CKF)
module, which introduces the global response of CNN and increasingly emphasizes
its importance during the training. Applying CKF leverages CNN's local
inductive bias in the early training period and gives full play to ViT's global
capability in the later one. Extensive experiments and analysis on ImageNet-1k
and downstream datasets demonstrate the superiority of our CSKD. Code will be
publicly available.",None,-1
1e59fd34-0097-4ed5-baf0-9481709fc2c6,Using Z3 for Formal Modeling and Verification of FNN Global Robustness,0.853753,"While Feedforward Neural Networks (FNNs) have achieved remarkable success in
various tasks, they are vulnerable to adversarial examples. Several techniques
have been developed to verify the adversarial robustness of FNNs, but most of
them focus on robustness verification against the local perturbation
neighborhood of a single data point. There is still a large research gap in
global robustness analysis. The global-robustness verifiable framework
DeepGlobal has been proposed to identify \textit{all} possible Adversarial
Dangerous Regions (ADRs) of FNNs, not limited to data samples in a test set. In
this paper, we propose a complete specification and implementation of
DeepGlobal utilizing the SMT solver Z3 for more explicit definition, and
propose several improvements to DeepGlobal for more efficient verification. To
evaluate the effectiveness of our implementation and improvements, we conduct
extensive experiments on a set of benchmark datasets. Visualization of our
experiment results shows the validity and effectiveness of the approach.",None,-1
2b0fc611-5643-4bf6-945c-2a639a1856f4,COFFEE: A Contrastive Oracle-Free Framework for Event Extraction,0.384137,"Event extraction is a complex information extraction task that involves
extracting events from unstructured text. Prior classification-based methods
require comprehensive entity annotations for joint training, while newer
generation-based methods rely on heuristic templates containing oracle
information such as event type, which is often unavailable in real-world
scenarios. In this study, we consider a more realistic setting of this task,
namely the Oracle-Free Event Extraction (OFEE) task, where only the input
context is given without any oracle information, including event type, event
ontology and trigger word. To solve this task, we propose a new framework,
called COFFEE, which extracts the events solely based on the document context
without referring to any oracle information. In particular, a contrastive
selection model is introduced in COFFEE to rectify the generated triggers and
handle multi-event instances. The proposed COFFEE outperforms state-of-the-art
approaches under the oracle-free setting of the event extraction task, as
evaluated on a public event extraction benchmark ACE05.",None,-1
d1c6327a-c1bb-47f2-b6c3-258d6f541b58,Impact of translation on biomedical information extraction from real-life clinical notes,0.54724,"The objective of our study is to determine whether using English tools to
extract and normalize French medical concepts on translations provides
comparable performance to French models trained on a set of annotated French
clinical notes. We compare two methods: a method involving French language
models and a method involving English language models. For the native French
method, the Named Entity Recognition (NER) and normalization steps are
performed separately. For the translated English method, after the first
translation step, we compare a two-step method and a terminology-oriented
method that performs extraction and normalization at the same time. We used
French, English and bilingual annotated datasets to evaluate all steps (NER,
normalization and translation) of our algorithms. Concerning the results, the
native French method performs better than the translated English one with a
global f1 score of 0.51 [0.47;0.55] against 0.39 [0.34;0.44] and 0.38
[0.36;0.40] for the two English methods tested. In conclusion, despite the
recent improvement of the translation models, there is a significant
performance difference between the two approaches in favor of the native French
method which is more efficient on French medical texts, even with few annotated
documents.",None,-1
b8fd8d10-52f1-4ef6-bec3-65af02b84828,Dense RGB SLAM with Neural Implicit Maps,0.995579,"There is an emerging trend of using neural implicit functions for map
representation in Simultaneous Localization and Mapping (SLAM). Some pioneer
works have achieved encouraging results on RGB-D SLAM. In this paper, we
present a dense RGB SLAM method with neural implicit map representation. To
reach this challenging goal without depth input, we introduce a hierarchical
feature volume to facilitate the implicit map decoder. This design effectively
fuses shape cues across different scales to facilitate map reconstruction. Our
method simultaneously solves the camera motion and the neural implicit map by
matching the rendered and input video frames. To facilitate optimization, we
further propose a photometric warping loss in the spirit of multi-view stereo
to better constrain the camera pose and scene geometry. We evaluate our method
on commonly used benchmarks and compare it with modern RGB and RGB-D SLAM
systems. Our method achieves favorable results than previous methods and even
surpasses some recent RGB-D SLAM methods.The code is at
poptree.github.io/DIM-SLAM/.",None,-1
b4839bb7-b64f-490f-add9-8ada86da4fa5,Learning in POMDPs is Sample-Efficient with Hindsight Observability,0.986986,"POMDPs capture a broad class of decision making problems, but hardness
results suggest that learning is intractable even in simple settings due to the
inherent partial observability. However, in many realistic problems, more
information is either revealed or can be computed during some point of the
learning process. Motivated by diverse applications ranging from robotics to
data center scheduling, we formulate a Hindsight Observable Markov Decision
Process (HOMDP) as a POMDP where the latent states are revealed to the learner
in hindsight and only during training. We introduce new algorithms for the
tabular and function approximation settings that are provably sample-efficient
with hindsight observability, even in POMDPs that would otherwise be
statistically intractable. We give a lower bound showing that the tabular
algorithm is optimal in its dependence on latent state and observation
cardinalities.",None,-1
80278fe2-1da0-43c8-91de-c30366e8d126,"One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning",0.451123,"Fine-tuning pre-trained language models for multiple tasks tends to be
expensive in terms of storage. To mitigate this, parameter-efficient transfer
learning (PETL) methods have been proposed to address this issue, but they
still require a significant number of parameters and storage when being applied
to broader ranges of tasks. To achieve even greater storage reduction, we
propose PROPETL, a novel method that enables efficient sharing of a single PETL
module which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)
across layers and tasks. We then learn binary masks to select different
sub-networks from the shared prototype network and apply them as PETL modules
into different layers. We find that the binary masks can determine crucial
information from the network, which is often ignored in previous studies. Our
work can also be seen as a type of pruning method, where we find that
overparameterization also exists in the seemingly small PETL modules. We
evaluate PROPETL on various downstream tasks and show that it can outperform
other PETL methods with approximately 10% of the parameter storage required by
the latter.",None,-1
a0bb1f13-aa02-418d-aaee-ac14fa09be56,How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions,1.0,"Large language models (LLMs) can ""lie"", which we define as outputting false
statements despite ""knowing"" the truth in a demonstrable sense. LLMs might
""lie"", for example, when instructed to output misinformation. Here, we develop
a simple lie detector that requires neither access to the LLM's activations
(black-box) nor ground-truth knowledge of the fact in question. The detector
works by asking a predefined set of unrelated follow-up questions after a
suspected lie, and feeding the LLM's yes/no answers into a logistic regression
classifier. Despite its simplicity, this lie detector is highly accurate and
surprisingly general. When trained on examples from a single setting --
prompting GPT-3.5 to lie about factual questions -- the detector generalises
out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie,
(3) sycophantic lies, and (4) lies emerging in real-life scenarios such as
sales. These results indicate that LLMs have distinctive lie-related
behavioural patterns, consistent across architectures and contexts, which could
enable general-purpose lie detection.",None,-1
5f7a2b5b-d9f4-4145-8ec1-6a610eea85a3,Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery,0.352663,"Discovering novel concepts from unlabelled data and in a continuous manner is
an important desideratum of lifelong learners. In the literature such problems
have been partially addressed under very restricted settings, where either
access to labelled data is provided for discovering novel concepts (e.g., NCD)
or learning occurs for a limited number of incremental steps (e.g.,
class-iNCD). In this work we challenge the status quo and propose a more
challenging and practical learning paradigm called MSc-iNCD, where learning
occurs continuously and unsupervisedly, while exploiting the rich priors from
large-scale pre-trained models. To this end, we propose simple baselines that
are not only resilient under longer learning scenarios, but are surprisingly
strong when compared with sophisticated state-of-the-art methods. We conduct
extensive empirical evaluation on a multitude of benchmarks and show the
effectiveness of our proposed baselines, which significantly raises the bar.",None,-1
47cf79a3-6844-44e0-bf69-64c5e8fb1908,Robust Point Cloud Processing through Positional Embedding,0.696031,"End-to-end trained per-point embeddings are an essential ingredient of any
state-of-the-art 3D point cloud processing such as detection or alignment.
Methods like PointNet, or the more recent point cloud transformer -- and its
variants -- all employ learned per-point embeddings. Despite impressive
performance, such approaches are sensitive to out-of-distribution (OOD) noise
and outliers. In this paper, we explore the role of an analytical per-point
embedding based on the criterion of bandwidth. The concept of bandwidth enables
us to draw connections with an alternate per-point embedding -- positional
embedding, particularly random Fourier features. We present compelling robust
results across downstream tasks such as point cloud classification and
registration with several categories of OOD noise.",None,-1
2ecd0e5f-d6bf-4b20-b297-8d6f1671a5f6,Less is More for Long Document Summary Evaluation by LLMs,0.804249,"Large Language Models (LLMs) have shown promising performance in summary
evaluation tasks, yet they face challenges such as high computational costs and
the Lost-in-the-Middle problem where important information in the middle of
long documents is often overlooked. To address these issues, this paper
introduces a novel approach, Extract-then-Evaluate, which involves extracting
key sentences from a long source document and then evaluating the summary by
prompting LLMs. The results reveal that the proposed method not only
significantly reduces evaluation costs but also exhibits a higher correlation
with human evaluations. Furthermore, we provide practical recommendations for
optimal document length and sentence extraction methods, contributing to the
development of cost-effective yet more accurate methods for LLM-based text
generation evaluation.",None,-1
8c502206-d647-4fac-b999-1ebe34a4fc05,Rehearsal-Free Domain Continual Face Anti-Spoofing: Generalize More and Forget Less,0.962781,"Face Anti-Spoofing (FAS) is recently studied under the continual learning
setting, where the FAS models are expected to evolve after encountering the
data from new domains. However, existing methods need extra replay buffers to
store previous data for rehearsal, which becomes infeasible when previous data
is unavailable because of privacy issues. In this paper, we propose the first
rehearsal-free method for Domain Continual Learning (DCL) of FAS, which deals
with catastrophic forgetting and unseen domain generalization problems
simultaneously. For better generalization to unseen domains, we design the
Dynamic Central Difference Convolutional Adapter (DCDCA) to adapt Vision
Transformer (ViT) models during the continual learning sessions. To alleviate
the forgetting of previous domains without using previous data, we propose the
Proxy Prototype Contrastive Regularization (PPCR) to constrain the continual
learning with previous domain knowledge from the proxy prototypes. Simulate
practical DCL scenarios, we devise two new protocols which evaluate both
generalization and anti-forgetting performance. Extensive experimental results
show that our proposed method can improve the generalization performance in
unseen domains and alleviate the catastrophic forgetting of the previous
knowledge. The codes and protocols will be released soon.",None,-1
f003f554-4db0-44df-8bae-a8f385ac6b10,Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling,0.988195,"The paper proposes a framework that combines behavioral and computational
experiments employing fictional prompts as a novel tool for investigating
cultural artifacts and social biases in storytelling both by humans and
generative AI. The study analyzes 250 stories authored by crowdworkers in June
2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging
methods from narratology and inferential statistics. Both crowdworkers and
large language models responded to identical prompts about creating and falling
in love with an artificial human. The proposed experimental paradigm allows a
direct comparison between human and LLM-generated storytelling. Responses to
the Pygmalionesque prompts confirm the pervasive presence of the Pygmalion myth
in the collective imaginary of both humans and large language models. All
solicited narratives present a scientific or technological pursuit. The
analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are more
more progressive in terms of gender roles and sexuality than those written by
humans. While AI narratives can occasionally provide innovative plot twists,
they offer less imaginative scenarios and rhetoric than human-authored texts.
The proposed framework argues that fiction can be used as a window into human
and AI-based collective imaginary and social dimensions.",None,-1
352120a6-cf9d-44ee-8240-af3c6cdd3f14,R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces,0.264442,"This paper introduces Robust Spin (R-Spin), a data-efficient domain-specific
self-supervision method for speaker and noise-invariant speech representations
by learning discrete acoustic units with speaker-invariant clustering (Spin).
R-Spin resolves Spin's issues and enhances content representations by learning
to predict acoustic pieces. R-Spin offers a 12X reduction in computational
resources compared to previous state-of-the-art methods while outperforming
them in severely distorted speech scenarios. This paper provides detailed
analyses to show how discrete units contribute to speech encoder training and
improving robustness in diverse acoustic environments.",None,-1
0c284c18-c843-4e48-b6a4-1e8801cebfbf,Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model Interpretability in Medicine,0.862896,"One of the major barriers to using large language models (LLMs) in medicine
is the perception they use uninterpretable methods to make clinical decisions
that are inherently different from the cognitive processes of clinicians. In
this manuscript we develop novel diagnostic reasoning prompts to study whether
LLMs can perform clinical reasoning to accurately form a diagnosis. We find
that GPT4 can be prompted to mimic the common clinical reasoning processes of
clinicians without sacrificing diagnostic accuracy. This is significant because
an LLM that can use clinical reasoning to provide an interpretable rationale
offers physicians a means to evaluate whether LLMs can be trusted for patient
care. Novel prompting methods have the potential to expose the black box of
LLMs, bringing them one step closer to safe and effective use in medicine.",None,-1
58b0db8c-c505-4bec-affc-bb8b8c5e540d,Personal Protective Equipment Detection in Extreme Construction Conditions,0.398769,"Object detection has been widely applied for construction safety management,
especially personal protective equipment (PPE) detection. Though the existing
PPE detection models trained on conventional datasets have achieved excellent
results, their performance dramatically declines in extreme construction
conditions. A robust detection model NST-YOLOv5 is developed by combining the
neural style transfer (NST) and YOLOv5 technologies. Five extreme conditions
are considered and simulated via the NST module to endow the detection model
with excellent robustness, including low light, intense light, sand dust, fog,
and rain. Experiments show that the NST has great potential as a tool for
extreme data synthesis since it is better at simulating extreme conditions than
other traditional image processing algorithms and helps the NST-YOLOv5 achieve
0.141 and 0.083 mAP_(05:95) improvements in synthesized and real-world extreme
data. This study provides a new feasible way to obtain a more robust detection
model for extreme construction conditions.",None,-1
e5c4af0a-8ff5-47a0-b4e1-75e1dd81206f,Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models,0.448003,"Considerable effort has been dedicated to mitigating toxicity, but existing
methods often require drastic modifications to model parameters or the use of
computationally intensive auxiliary models. Furthermore, previous approaches
have often neglected the crucial factor of language's evolving nature over
time. In this work, we present a comprehensive perspective on toxicity
mitigation that takes into account its changing nature. We introduce
Goodtriever, a flexible methodology that matches the current state-of-the-art
toxicity mitigation while achieving 43% relative latency reduction during
inference and being more computationally efficient. By incorporating a
retrieval-based approach at decoding time, Goodtriever enables
toxicity-controlled text generation. Our research advocates for an increased
focus on adaptable mitigation techniques, which better reflect the data drift
models face when deployed in the wild. Code and data are available at
https://github.com/for-ai/goodtriever.",None,-1
0d36a969-0075-4a52-8a4f-bc4d708aec6b,From Key Points to Key Point Hierarchy: Structured and Expressive Opinion Summarization,0.307679,"Key Point Analysis (KPA) has been recently proposed for deriving fine-grained
insights from collections of textual comments. KPA extracts the main points in
the data as a list of concise sentences or phrases, termed key points, and
quantifies their prevalence. While key points are more expressive than word
clouds and key phrases, making sense of a long, flat list of key points, which
often express related ideas in varying levels of granularity, may still be
challenging. To address this limitation of KPA, we introduce the task of
organizing a given set of key points into a hierarchy, according to their
specificity. Such hierarchies may be viewed as a novel type of Textual
Entailment Graph. We develop ThinkP, a high quality benchmark dataset of key
point hierarchies for business and product reviews, obtained by consolidating
multiple annotations. We compare different methods for predicting pairwise
relations between key points, and for inferring a hierarchy from these pairwise
predictions. In particular, for the task of computing pairwise key point
relations, we achieve significant gains over existing strong baselines by
applying directional distributional similarity methods to a novel
distributional representation of key points, and further boost performance via
weak supervision.",None,-1
54644710-1976-45c9-afbd-35c7294ba92a,LightGlue: Local Feature Matching at Light Speed,1.0,"We introduce LightGlue, a deep neural network that learns to match local
features across images. We revisit multiple design decisions of SuperGlue, the
state of the art in sparse matching, and derive simple but effective
improvements. Cumulatively, they make LightGlue more efficient - in terms of
both memory and computation, more accurate, and much easier to train. One key
property is that LightGlue is adaptive to the difficulty of the problem: the
inference is much faster on image pairs that are intuitively easy to match, for
example because of a larger visual overlap or limited appearance change. This
opens up exciting prospects for deploying deep matchers in latency-sensitive
applications like 3D reconstruction. The code and trained models are publicly
available at https://github.com/cvg/LightGlue.",None,-1
74938fe2-2b30-48d7-8608-38b3dea2d980,Conversation Style Transfer using Few-Shot Learning,0.379243,"Conventional text style transfer approaches focus on sentence-level style
transfer without considering contextual information, and the style is described
with attributes (e.g., formality). When applying style transfer in
conversations such as task-oriented dialogues, existing approaches suffer from
these limitations as context can play an important role and the style
attributes are often difficult to define in conversations. In this paper, we
introduce conversation style transfer as a few-shot learning problem, where the
model learns to perform style transfer by observing only a few example
dialogues in the target style. We propose a novel in-context learning approach
to solve the task with style-free dialogues as a pivot. Human evaluation shows
that by incorporating multi-turn context, the model is able to match the target
style while having better appropriateness and semantic correctness compared to
utterance/sentence-level style transfer. Additionally, we show that
conversation style transfer can also benefit downstream tasks. For example, in
multi-domain intent classification tasks, the F1 scores improve after
transferring the style of training data to match the style of the test data.",None,-1
93e332f6-7389-44d9-9338-fc18d692ce88,Variation of Gender Biases in Visual Recognition Models Before and After Finetuning,0.073269,"We introduce a framework to measure how biases change before and after
fine-tuning a large scale visual recognition model for a downstream task. Deep
learning models trained on increasing amounts of data are known to encode
societal biases. Many computer vision systems today rely on models typically
pretrained on large scale datasets. While bias mitigation techniques have been
developed for tuning models for downstream tasks, it is currently unclear what
are the effects of biases already encoded in a pretrained model. Our framework
incorporates sets of canonical images representing individual and pairs of
concepts to highlight changes in biases for an array of off-the-shelf
pretrained models across model sizes, dataset sizes, and training objectives.
Through our analyses, we find that (1) supervised models trained on datasets
such as ImageNet-21k are more likely to retain their pretraining biases
regardless of the target dataset compared to self-supervised models. We also
find that (2) models finetuned on larger scale datasets are more likely to
introduce new biased associations. Our results also suggest that (3) biases can
transfer to finetuned models and the finetuning objective and dataset can
impact the extent of transferred biases.",None,-1
5c795e7d-8eeb-4e2c-b4ac-4a56413dd9b2,Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation,0.887438,"Semantic segmentation of point clouds in autonomous driving datasets requires
techniques that can process large numbers of points efficiently. Sparse 3D
convolutions have become the de-facto tools to construct deep neural networks
for this task: they exploit point cloud sparsity to reduce the memory and
computational loads and are at the core of today's best methods. In this paper,
we propose an alternative method that reaches the level of state-of-the-art
methods without requiring sparse convolutions. We actually show that such level
of performance is achievable by relying on tools a priori unfit for large scale
and high-performing 3D perception. In particular, we propose a novel 3D
backbone, WaffleIron, made almost exclusively of MLPs and dense 2D convolutions
and present how to train it to reach high performance on SemanticKITTI and
nuScenes. We believe that WaffleIron is a compelling alternative to backbones
using sparse 3D convolutions, especially in frameworks and on hardware where
those convolutions are not readily available.",None,-1
3e8a7ab4-f1f6-4ea3-8ef8-b2755ca1fb7e,Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model,0.324766,"This paper presents a series of approaches aimed at enhancing the performance
of Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic
information from a Semantic Role Labeling (SRL) model. We propose a novel
end-to-end Semantic Role Labeling model that effectively captures most of the
structured semantic information within the Transformer hidden state. We believe
that this end-to-end model is well-suited for our newly proposed models that
incorporate semantic information. We evaluate the proposed models in two
languages, English and Czech, employing ELECTRA-small models. Our combined
models improve ABSA performance in both languages. Moreover, we achieved new
state-of-the-art results on the Czech ABSA.",None,-1
1001d48e-6ba9-434b-ba37-b59ab5605937,Semantic Attention Flow Fields for Monocular Dynamic Scene Decomposition,0.389045,"From video, we reconstruct a neural volume that captures time-varying color,
density, scene flow, semantics, and attention information. The semantics and
attention let us identify salient foreground objects separately from the
background across spacetime. To mitigate low resolution semantic and attention
features, we compute pyramids that trade detail with whole-image context. After
optimization, we perform a saliency-aware clustering to decompose the scene. To
evaluate real-world scenes, we annotate object masks in the NVIDIA Dynamic
Scene and DyCheck datasets. We demonstrate that this method can decompose
dynamic scenes in an unsupervised way with competitive performance to a
supervised method, and that it improves foreground/background segmentation over
recent static/dynamic split methods. Project Webpage:
https://visual.cs.brown.edu/saff",None,-1
a8b8079c-f940-4aab-b189-5765b6d55579,Running cognitive evaluations on large language models: The do's and the don'ts,0.911715,"In this paper, I describe methodological considerations for studies that aim
to evaluate the cognitive capacities of large language models (LLMs) using
language-based behavioral assessments. Drawing on three case studies from the
literature (a commonsense knowledge benchmark, a theory of mind evaluation, and
a test of syntactic agreement), I describe common pitfalls that might arise
when applying a cognitive test to an LLM. I then list 10 do's and don'ts that
should help design high-quality cognitive evaluations for AI systems. I
conclude by discussing four areas where the do's and don'ts are currently under
active discussion -- prompt sensitivity, cultural and linguistic diversity,
using LLMs as research assistants, and running evaluations on open vs. closed
LLMs. Overall, the goal of the paper is to contribute to the broader discussion
of best practices in the rapidly growing field of AI Psychology.",None,-1
3b11edd1-bb62-40c0-8242-62646d868313,Substitution-based Semantic Change Detection using Contextual Embeddings,0.574856,"Measuring semantic change has thus far remained a task where methods using
contextual embeddings have struggled to improve upon simpler techniques relying
only on static word vectors. Moreover, many of the previously proposed
approaches suffer from downsides related to scalability and ease of
interpretation. We present a simplified approach to measuring semantic change
using contextual embeddings, relying only on the most probable substitutes for
masked terms. Not only is this approach directly interpretable, it is also far
more efficient in terms of storage, achieves superior average performance
across the most frequently cited datasets for this task, and allows for more
nuanced investigation of change than is possible with static word vectors.",None,-1
14390025-5d8f-49ab-8f0b-0111db5614e6,Relevance Feedback with Brain Signals,0.562997,"The Relevance Feedback (RF) process relies on accurate and real-time
relevance estimation of feedback documents to improve retrieval performance.
Since collecting explicit relevance annotations imposes an extra burden on the
user, extensive studies have explored using pseudo-relevance signals and
implicit feedback signals as substitutes. However, such signals are indirect
indicators of relevance and suffer from complex search scenarios where user
interactions are absent or biased.
  Recently, the advances in portable and high-precision brain-computer
interface (BCI) devices have shown the possibility to monitor user's brain
activities during search process. Brain signals can directly reflect user's
psychological responses to search results and thus it can act as additional and
unbiased RF signals. To explore the effectiveness of brain signals in the
context of RF, we propose a novel RF framework that combines BCI-based
relevance feedback with pseudo-relevance signals and implicit signals to
improve the performance of document re-ranking. The experimental results on the
user study dataset show that incorporating brain signals leads to significant
performance improvement in our RF framework. Besides, we observe that brain
signals perform particularly well in several hard search scenarios, especially
when implicit signals as feedback are missing or noisy. This reveals when and
how to exploit brain signals in the context of RF.",None,-1
ac44a74b-feef-471e-8ccd-2e84fe2fd3e1,Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case,0.676124,"After the pandemic, artificial intelligence (AI) powered support for mental
health care has become increasingly important. The breadth and complexity of
significant challenges required to provide adequate care involve: (a)
Personalized patient understanding, (b) Safety-constrained and medically
validated chatbot patient interactions, and (c) Support for continued
feedback-based refinements in design using chatbot-patient interactions. We
propose Alleviate, a chatbot designed to assist patients suffering from mental
health challenges with personalized care and assist clinicians with
understanding their patients better. Alleviate draws from an array of publicly
available clinically valid mental-health texts and databases, allowing
Alleviate to make medically sound and informed decisions. In addition,
Alleviate's modular design and explainable decision-making lends itself to
robust and continued feedback-based refinements to its design. In this paper,
we explain the different modules of Alleviate and submit a short video
demonstrating Alleviate's capabilities to help patients and clinicians
understand each other better to facilitate optimal care strategies.",None,-1
e0c0b9d5-0e52-4446-b072-e1b905cca266,Diagnosing and Rectifying Vision Models using Language,0.648431,"Recent multi-modal contrastive learning models have demonstrated the ability
to learn an embedding space suitable for building strong vision classifiers, by
leveraging the rich information in large-scale image-caption datasets. Our work
highlights a distinct advantage of this multi-modal embedding space: the
ability to diagnose vision classifiers through natural language. The
traditional process of diagnosing model behaviors in deployment settings
involves labor-intensive data acquisition and annotation. Our proposed method
can discover high-error data slices, identify influential attributes and
further rectify undesirable model behaviors, without requiring any visual data.
Through a combination of theoretical explanation and empirical verification, we
present conditions under which classifiers trained on embeddings from one
modality can be equivalently applied to embeddings from another modality. On a
range of image datasets with known error slices, we demonstrate that our method
can effectively identify the error slices and influential attributes, and can
further use language to rectify failure modes of the classifier.",None,-1
b96ed7e3-2cb9-40cf-905d-3c214ff4fe6f,"HealthEdge: A Machine Learning-Based Smart Healthcare Framework for Prediction of Type 2 Diabetes in an Integrated IoT, Edge, and Cloud Computing System",0.725276,"Diabetes Mellitus has no permanent cure to date and is one of the leading
causes of death globally. The alarming increase in diabetes calls for the need
to take precautionary measures to avoid/predict the occurrence of diabetes.
This paper proposes HealthEdge, a machine learning-based smart healthcare
framework for type 2 diabetes prediction in an integrated IoT-edge-cloud
computing system. Numerical experiments and comparative analysis were carried
out between the two most used machine learning algorithms in the literature,
Random Forest (RF) and Logistic Regression (LR), using two real-life diabetes
datasets. The results show that RF predicts diabetes with 6% more accuracy on
average compared to LR.",None,-1
8c7ce85f-c271-4716-96f2-6ab8ecfea59b,Depth-Relative Self Attention for Monocular Depth Estimation,0.512263,"Monocular depth estimation is very challenging because clues to the exact
depth are incomplete in a single RGB image. To overcome the limitation, deep
neural networks rely on various visual hints such as size, shade, and texture
extracted from RGB information. However, we observe that if such hints are
overly exploited, the network can be biased on RGB information without
considering the comprehensive view. We propose a novel depth estimation model
named RElative Depth Transformer (RED-T) that uses relative depth as guidance
in self-attention. Specifically, the model assigns high attention weights to
pixels of close depth and low attention weights to pixels of distant depth. As
a result, the features of similar depth can become more likely to each other
and thus less prone to misused visual hints. We show that the proposed model
achieves competitive results in monocular depth estimation benchmarks and is
less biased to RGB information. In addition, we propose a novel monocular depth
estimation benchmark that limits the observable depth range during training in
order to evaluate the robustness of the model for unseen depths.",None,-1
4ca71203-0ad7-496d-aef1-902a1c768946,GNN-based Passenger Request Prediction,0.777338,"Passenger request prediction is essential for operations planning, control,
and management in ride-sharing platforms. While the demand prediction problem
has been studied extensively, the Origin-Destination (OD) flow prediction of
passengers has received less attention from the research community. This paper
develops a Graph Neural Network framework along with the Attention Mechanism to
predict the OD flow of passengers. The proposed framework exploits various
linear and non-linear dependencies that arise among requests originating from
different locations and captures the repetition pattern and the contextual data
of that place. Moreover, the optimal size of the grid cell that covers the road
network and preserves the complexity and accuracy of the model is determined.
Extensive simulations are conducted to examine the characteristics of our
proposed approach and its various components. The results show the superior
performance of our proposed model compared to the existing baselines.",None,-1
3a5a15ec-1833-46d9-a6e1-f384d933218b,CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation,0.502305,"Dialogue structure discovery is essential in dialogue generation.
Well-structured topic flow can leverage background information and predict
future topics to help generate controllable and explainable responses. However,
most previous work focused on dialogue structure learning in task-oriented
dialogue other than open-domain dialogue which is more complicated and
challenging. In this paper, we present a new framework CTRLStruct for dialogue
structure learning to effectively explore topic-level dialogue clusters as well
as their transitions with unlabelled information. Precisely, dialogue
utterances encoded by bi-directional Transformer are further trained through a
special designed contrastive learning task to improve representation. Then we
perform clustering to utterance-level representations and form topic-level
clusters that can be considered as vertices in dialogue structure graph. The
edges in the graph indicating transition probability between vertices are
calculated by mimicking expert behavior in datasets. Finally, dialogue
structure graph is integrated into dialogue model to perform controlled
response generation. Experiments on two popular open-domain dialogue datasets
show our model can generate more coherent responses compared to some excellent
dialogue models, as well as outperform some typical sentence embedding methods
in dialogue utterance representation. Code is available in GitHub.",None,-1
a1f9d476-9b0a-4191-b932-17298feb53ba,Tracr: Compiled Transformers as a Laboratory for Interpretability,0.746284,"We show how to ""compile"" human-readable programs into standard decoder-only
transformer models. Our compiler, Tracr, generates models with known structure.
This structure can be used to design experiments. For example, we use it to
study ""superposition"" in transformers that execute multi-step algorithms.
Additionally, the known structure of Tracr-compiled models can serve as
ground-truth for evaluating interpretability methods. Commonly, because the
""programs"" learned by transformers are unknown it is unclear whether an
interpretation succeeded. We demonstrate our approach by implementing and
examining programs including computing token frequencies, sorting, and
parenthesis checking. We provide an open-source implementation of Tracr at
https://github.com/google-deepmind/tracr.",None,-1
564c7ca1-9bbc-46cd-a72e-208436f64796,Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study,0.102333,"In machine translation tasks, the relationship between model complexity and
performance is often presumed to be linear, driving an increase in the number
of parameters and consequent demands for computational resources like multiple
GPUs. To explore this assumption, this study systematically investigates the
effects of hyperparameters through ablation on a sequence-to-sequence machine
translation pipeline, utilizing a single NVIDIA A100 GPU. Contrary to
expectations, our experiments reveal that combinations with the most parameters
were not necessarily the most effective. This unexpected insight prompted a
careful reduction in parameter sizes, uncovering ""sweet spots"" that enable
training sophisticated models on a single GPU without compromising translation
quality. The findings demonstrate an intricate relationship between
hyperparameter selection, model size, and computational resource needs. The
insights from this study contribute to the ongoing efforts to make machine
translation more accessible and cost-effective, emphasizing the importance of
precise hyperparameter tuning over mere scaling.",None,-1
5541add0-28c6-4fd3-a530-64633996fc77,Muslim-Violence Bias Persists in Debiased GPT Models,0.20003,"Abid et al. (2021) showed a tendency in GPT-3 to generate mostly violent
completions when prompted about Muslims, compared with other religions. Two
pre-registered replication attempts found few violent completions and only a
weak anti-Muslim bias in the more recent InstructGPT, fine-tuned to eliminate
biased and toxic outputs. However, more pre-registered experiments showed that
using common names associated with the religions in prompts increases
several-fold the rate of violent completions, revealing a significant
second-order anti-Muslim bias. ChatGPT showed a bias many times stronger
regardless of prompt format, suggesting that the effects of debiasing were
reduced with continued model development. Our content analysis revealed
religion-specific themes containing offensive stereotypes across all
experiments. Our results show the need for continual de-biasing of models in
ways that address both explicit and higher-order associations.",None,-1
67e5ce40-096a-4625-9d62-5e2d4fb78744,Adaptive Policy with Wait-$k$ Model for Simultaneous Translation,0.314506,"Simultaneous machine translation (SiMT) requires a robust read/write policy
in conjunction with a high-quality translation model. Traditional methods rely
on either a fixed wait-$k$ policy coupled with a standalone wait-$k$
translation model, or an adaptive policy jointly trained with the translation
model. In this study, we propose a more flexible approach by decoupling the
adaptive policy model from the translation model. Our motivation stems from the
observation that a standalone multi-path wait-$k$ model performs competitively
with adaptive policies utilized in state-of-the-art SiMT approaches.
Specifically, we introduce DaP, a divergence-based adaptive policy, that makes
read/write decisions for any translation model based on the potential
divergence in translation distributions resulting from future information. DaP
extends a frozen wait-$k$ model with lightweight parameters, and is both memory
and computation efficient. Experimental results across various benchmarks
demonstrate that our approach offers an improved trade-off between translation
accuracy and latency, outperforming strong baselines.",None,-1
22782646-dab2-4b81-8179-393168161e81,SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning,0.973693,"This study presents a thorough examination of various Generative Pretrained
Transformer (GPT) methodologies in sentiment analysis, specifically in the
context of Task 4 on the SemEval 2017 dataset. Three primary strategies are
employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2)
fine-tuning GPT models, and 3) an inventive approach to embedding
classification. The research yields detailed comparative insights among these
strategies and individual GPT models, revealing their unique strengths and
potential limitations. Additionally, the study compares these GPT-based
methodologies with other current, high-performing models previously used with
the same dataset. The results illustrate the significant superiority of the GPT
approaches in terms of predictive performance, more than 22\% in F1-score
compared to the state-of-the-art. Further, the paper sheds light on common
challenges in sentiment analysis tasks, such as understanding context and
detecting sarcasm. It underscores the enhanced capabilities of the GPT models
to effectively handle these complexities. Taken together, these findings
highlight the promising potential of GPT models in sentiment analysis, setting
the stage for future research in this field. The code can be found at
https://github.com/DSAatUSU/SentimentGPT",None,-1
e51a0d33-643f-4200-ae25-7fdd895bd1c7,A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization,0.337581,"Pre-trained language models (PLMs) have achieved outstanding achievements in
abstractive single-document summarization (SDS). However, such benefits may not
fully extend to multi-document summarization (MDS), where the handling of
cross-document information is more complex. Previous works either design new
MDS architectures or apply PLMs bluntly with concatenated source documents as a
reformulated SDS task. While the former does not utilize previous pre-training
efforts and may not generalize well across different domains, the latter may
not sufficiently attend to the intricate cross-document relationships unique to
MDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to
better utilize a PLM to facilitate multi-document interactions for the MDS
task. Across 10 MDS benchmarks from various domains, our method outperforms or
is competitive with the previous best models, including those with additional
MDS pre-training or with more parameters. It outperforms its corresponding PLM
backbone by up to 3 Rouge-L and is favored by humans.",None,-1
1cfcd07d-9942-4999-b632-3dead73b399b,Stable and Causal Inference for Discriminative Self-supervised Deep Visual Representations,0.234687,"In recent years, discriminative self-supervised methods have made significant
strides in advancing various visual tasks. The central idea of learning a data
encoder that is robust to data distortions/augmentations is straightforward yet
highly effective. Although many studies have demonstrated the empirical success
of various learning methods, the resulting learned representations can exhibit
instability and hinder downstream performance. In this study, we analyze
discriminative self-supervised methods from a causal perspective to explain
these unstable behaviors and propose solutions to overcome them. Our approach
draws inspiration from prior works that empirically demonstrate the ability of
discriminative self-supervised methods to demix ground truth causal sources to
some extent. Unlike previous work on causality-empowered representation
learning, we do not apply our solutions during the training process but rather
during the inference process to improve time efficiency. Through experiments on
both controlled image datasets and realistic image datasets, we show that our
proposed solutions, which involve tempering a linear transformation with
controlled synthetic data, are effective in addressing these issues.",None,-1
6353b1e4-5c25-47af-9692-6c823fb3d558,AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of Autism Spectrum Disorder,0.841606,"To easily obtain the knowledge about autism spectrum disorder and help its
early screening and diagnosis, we create AsdKB, a Chinese knowledge base on
autism spectrum disorder. The knowledge base is built on top of various
sources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical
descriptions on mental and behavioural disorders, 2) the diagnostic knowledge
from DSM-5 and different screening tools recommended by social organizations
and medical institutes, and 3) the expert knowledge on professional physicians
and hospitals from the Web. AsdKB contains both ontological and factual
knowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The
potential applications of AsdKB are question answering, auxiliary diagnosis,
and expert recommendation, and we illustrate them with a prototype which can be
accessed at http://asdkb.org.cn/.",None,-1
4c41589a-07be-4161-8cf0-83aff7a923db,Asymmetric Polynomial Loss For Multi-Label Classification,0.304214,"Various tasks are reformulated as multi-label classification problems, in
which the binary cross-entropy (BCE) loss is frequently utilized for optimizing
well-designed models. However, the vanilla BCE loss cannot be tailored for
diverse tasks, resulting in a suboptimal performance for different models.
Besides, the imbalance between redundant negative samples and rare positive
samples could degrade the model performance. In this paper, we propose an
effective Asymmetric Polynomial Loss (APL) to mitigate the above issues.
Specifically, we first perform Taylor expansion on BCE loss. Then we ameliorate
the coefficients of polynomial functions. We further employ the asymmetric
focusing mechanism to decouple the gradient contribution from the negative and
positive samples. Moreover, we validate that the polynomial coefficients can
recalibrate the asymmetric focusing hyperparameters. Experiments on relation
extraction, text classification, and image classification show that our APL
loss can consistently improve performance without extra training burden.",None,-1
9e9af261-c9b7-4fcc-8e2b-1d7ab6b0487b,ProtoCaps: A Fast and Non-Iterative Capsule Network Routing Method,0.433811,"Capsule Networks have emerged as a powerful class of deep learning
architectures, known for robust performance with relatively few parameters
compared to Convolutional Neural Networks (CNNs). However, their inherent
efficiency is often overshadowed by their slow, iterative routing mechanisms
which establish connections between Capsule layers, posing computational
challenges resulting in an inability to scale. In this paper, we introduce a
novel, non-iterative routing mechanism, inspired by trainable prototype
clustering. This innovative approach aims to mitigate computational complexity,
while retaining, if not enhancing, performance efficacy. Furthermore, we
harness a shared Capsule subspace, negating the need to project each
lower-level Capsule to each higher-level Capsule, thereby significantly
reducing memory requisites during training. Our approach demonstrates superior
results compared to the current best non-iterative Capsule Network and tests on
the Imagewoof dataset, which is too computationally demanding to handle
efficiently by iterative approaches. Our findings underscore the potential of
our proposed methodology in enhancing the operational efficiency and
performance of Capsule Networks, paving the way for their application in
increasingly complex computational scenarios. Code is available at
https://github.com/mileseverett/ProtoCaps.",None,-1
5486c0ea-eaba-4bdc-8e8d-e637ce8960f1,TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage,0.950213,"With recent advancements in natural language processing, Large Language
Models (LLMs) have emerged as powerful tools for various real-world
applications. Despite their prowess, the intrinsic generative abilities of LLMs
may prove insufficient for handling complex tasks which necessitate a
combination of task planning and the usage of external tools. In this paper, we
first propose a structured framework tailored for LLM-based AI Agents and
discuss the crucial capabilities necessary for tackling intricate problems.
Within this framework, we design two distinct types of agents (i.e., one-step
agent and sequential agent) to execute the inference process. Subsequently, we
instantiate the framework using various LLMs and evaluate their Task Planning
and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings
and challenges, our goal is to provide a helpful resource for researchers and
practitioners to leverage the power of LLMs in their AI applications. Our study
emphasizes the substantial potential of these models, while also identifying
areas that need more investigation and improvement.",None,-1
bfb9e354-2ee9-4ee7-b4c6-7952daae9da9,Hijacking Context in Large Multi-modal Models,0.244737,"Recently, Large Multi-modal Models (LMMs) have demonstrated their ability to
understand the visual contents of images given the instructions regarding the
images. Built upon the Large Language Models (LLMs), LMMs also inherit their
abilities and characteristics such as in-context learning where a coherent
sequence of images and texts are given as the input prompt. However, we
identify a new limitation of off-the-shelf LMMs where a small fraction of
incoherent images or text descriptions mislead LMMs to only generate biased
output about the hijacked context, not the originally intended context. To
address this, we propose a pre-filtering method that removes irrelevant
contexts via GPT-4V, based on its robustness towards distribution shift within
the contexts. We further investigate whether replacing the hijacked visual and
textual contexts with the correlated ones via GPT-4V and text-to-image models
can help yield coherent responses.",None,-1
805e66ca-9717-4007-8636-de49ede021c0,LAPTNet-FPN: Multi-scale LiDAR-aided Projective Transform Network for Real Time Semantic Grid Prediction,0.282022,"Semantic grids can be useful representations of the scene around an
autonomous system. By having information about the layout of the space around
itself, a robot can leverage this type of representation for crucial tasks such
as navigation or tracking. By fusing information from multiple sensors,
robustness can be increased and the computational load for the task can be
lowered, achieving real time performance. Our multi-scale LiDAR-Aided
Perspective Transform network uses information available in point clouds to
guide the projection of image features to a top-view representation, resulting
in a relative improvement in the state of the art for semantic grid generation
for human (+8.67%) and movable object (+49.07%) classes in the nuScenes
dataset, as well as achieving results close to the state of the art for the
vehicle, drivable area and walkway classes, while performing inference at 25
FPS.",None,-1
74df6861-504a-4178-a27d-7f1e99515462,Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data,0.612433,"Foundation models are trained on vast amounts of data at scale using
self-supervised learning, enabling adaptation to a wide range of downstream
tasks. At test time, these models exhibit zero-shot capabilities through which
they can classify previously unseen (user-specified) categories. In this paper,
we address the problem of quantifying uncertainty in these zero-shot
predictions. We propose a heuristic approach for uncertainty estimation in
zero-shot settings using conformal prediction with web data. Given a set of
classes at test time, we conduct zero-shot classification with CLIP-style
models using a prompt template, e.g., ""an image of a <category>"", and use the
same template as a search query to source calibration data from the open web.
Given a web-based calibration set, we apply conformal prediction with a novel
conformity score that accounts for potential errors in retrieved web data. We
evaluate the utility of our proposed method in Biomedical foundation models;
our preliminary results show that web-based conformal prediction sets achieve
the target coverage with satisfactory efficiency on a variety of biomedical
datasets.",None,-1
fbab80d3-cc62-4149-b359-49fa04f864ff,Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks,0.882324,"Arguments often do not make explicit how a conclusion follows from its
premises. To compensate for this lack, we enrich arguments with structured
background knowledge to support knowledge-intense argumentation tasks. We
present a new unsupervised method for constructing Contextualized Commonsense
Knowledge Graphs (CCKGs) that selects contextually relevant knowledge from
large knowledge graphs (KGs) efficiently and at high quality. Our work goes
beyond context-insensitive knowledge extraction heuristics by computing
semantic similarity between KG triplets and textual arguments. Using these
triplet similarities as weights, we extract contextualized knowledge paths that
connect a conclusion to its premise, while maximizing similarity to the
argument. We combine multiple paths into a CCKG that we optionally prune to
reduce noise and raise precision. Intrinsic evaluation of the quality of our
graphs shows that our method is effective for (re)constructing human
explanation graphs. Manual evaluations in a large-scale knowledge selection
setup confirm high recall and precision of implicit CSK in the CCKGs. Finally,
we demonstrate the effectiveness of CCKGs in a knowledge-insensitive argument
quality rating task, outperforming strong baselines and rivaling a GPT-3 based
system.",None,-1
7b0a2700-c8be-46d7-8801-625be12007f3,Are UD Treebanks Getting More Consistent? A Report Card for English UD,0.207507,"Recent efforts to consolidate guidelines and treebanks in the Universal
Dependencies project raise the expectation that joint training and dataset
comparison is increasingly possible for high-resource languages such as
English, which have multiple corpora. Focusing on the two largest UD English
treebanks, we examine progress in data consolidation and answer several
questions: Are UD English treebanks becoming more internally consistent? Are
they becoming more like each other and to what extent? Is joint training a good
idea, and if so, since which UD version? Our results indicate that while
consolidation has made progress, joint models may still suffer from
inconsistencies, which hamper their ability to leverage a larger pool of
training data.",None,-1
0536a0a4-ef67-4063-a89f-0ead866511f9,Reasoning Implicit Sentiment with Chain-of-Thought Prompting,0.991455,"While sentiment analysis systems try to determine the sentiment polarities of
given targets based on the key opinion expressions in input texts, in implicit
sentiment analysis (ISA) the opinion cues come in an implicit and obscure
manner. Thus detecting implicit sentiment requires the common-sense and
multi-hop reasoning ability to infer the latent intent of opinion. Inspired by
the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop
Reasoning (THOR) CoT framework to mimic the human-like reasoning process for
ISA. We design a three-step prompting principle for THOR to step-by-step induce
the implicit aspect, opinion, and finally the sentiment polarity. Our
THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on
supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%
F1 on zero-shot setting. Our code is open at
https://github.com/scofield7419/THOR-ISA.",None,-1
1b8c12a4-5a3b-4304-b0b8-d32517c56b43,A Brain-inspired Memory Transformation based Differentiable Neural Computer for Reasoning-based Question Answering,0.0611237,"Reasoning and question answering as a basic cognitive function for humans, is
nevertheless a great challenge for current artificial intelligence. Although
the Differentiable Neural Computer (DNC) model could solve such problems to a
certain extent, the development is still limited by its high algorithm
complexity, slow convergence speed, and poor test robustness. Inspired by the
learning and memory mechanism of the brain, this paper proposed a Memory
Transformation based Differentiable Neural Computer (MT-DNC) model. MT-DNC
incorporates working memory and long-term memory into DNC, and realizes the
autonomous transformation of acquired experience between working memory and
long-term memory, thereby helping to effectively extract acquired knowledge to
improve reasoning ability. Experimental results on bAbI question answering task
demonstrated that our proposed method achieves superior performance and faster
convergence speed compared to other existing DNN and DNC models. Ablation
studies also indicated that the memory transformation from working memory to
long-term memory plays essential role in improving the robustness and stability
of reasoning. This work explores how brain-inspired memory transformation can
be integrated and applied to complex intelligent dialogue and reasoning
systems.",None,-1
336ebc7a-00ba-4e91-986f-a441491bd59a,Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision,0.47314,"Credibility signals represent a wide range of heuristics that are typically
used by journalists and fact-checkers to assess the veracity of online content.
Automating the task of credibility signal extraction, however, is very
challenging as it requires high-accuracy signal-specific extractors to be
trained, while there are currently no sufficiently large datasets annotated
with all credibility signals. This paper investigates whether large language
models (LLMs) can be prompted effectively with a set of 18 credibility signals
to produce weak labels for each signal. We then aggregate these potentially
noisy labels using weak supervision in order to predict content veracity. We
demonstrate that our approach, which combines zero-shot LLM credibility signal
labeling and weak supervision, outperforms state-of-the-art classifiers on two
misinformation datasets without using any ground-truth labels for training. We
also analyse the contribution of the individual credibility signals towards
predicting content veracity, which provides new valuable insights into their
role in misinformation detection.",None,-1
64d191f1-7481-459e-8468-065342716ff9,Leveraging Summary Guidance on Medical Report Summarization,0.204942,"This study presents three deidentified large medical text datasets, named
DISCHARGE, ECHO and RADIOLOGY, which contain 50K, 16K and 378K pairs of report
and summary that are derived from MIMIC-III, respectively. We implement
convincing baselines of automated abstractive summarization on the proposed
datasets with pre-trained encoder-decoder language models, including BERT2BERT,
T5-large and BART. Further, based on the BART model, we leverage the sampled
summaries from the train set as prior knowledge guidance, for encoding
additional contextual representations of the guidance with the encoder and
enhancing the decoding representations in the decoder. The experimental results
confirm the improvement of ROUGE scores and BERTScore made by the proposed
method, outperforming the larger model T5-large.",None,-1
e8065786-8a04-48a6-9167-9f584d26b099,Expanding the Set of Pragmatic Considerations in Conversational AI,0.164935,"Despite considerable performance improvements, current conversational AI
systems often fail to meet user expectations. We discuss several pragmatic
limitations of current conversational AI systems. We illustrate pragmatic
limitations with examples that are syntactically appropriate, but have clear
pragmatic deficiencies. We label our complaints as ""Turing Test Triggers""
(TTTs) as they indicate where current conversational AI systems fall short
compared to human behavior. We develop a taxonomy of pragmatic considerations
intended to identify what pragmatic competencies a conversational AI system
requires and discuss implications for the design and evaluation of
conversational AI systems.",None,-1
097517bf-554a-4f43-bf69-b7e63844c0fb,LMGQS: A Large-scale Dataset for Query-focused Summarization,0.159048,"Query-focused summarization (QFS) aims to extract or generate a summary of an
input document that directly answers or is relevant to a given query. The lack
of large-scale datasets in the form of documents, queries, and summaries has
hindered model development in this area. In contrast, multiple large-scale
high-quality datasets for generic summarization exist. We hypothesize that
there is a hidden query for each summary sentence in a generic summarization
annotation, and we utilize a large-scale pretrained language model to recover
it. In this way, we convert four generic summarization benchmarks into a new
QFS benchmark dataset, LMGQS, which consists of over 1 million
document-query-summary samples. We thoroughly investigate the properties of our
proposed dataset and establish baselines with state-of-the-art summarization
models. By fine-tuning a language model on LMGQS, we achieve state-of-the-art
zero-shot and supervised performance on multiple existing QFS benchmarks,
demonstrating the high quality and diversity of LMGQS.",None,-1
e270f9a7-2354-4052-8ee9-55e5c42fd1a3,Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline,0.448432,"Large language models (LLMs) have revolutionized the field of AI,
demonstrating unprecedented capacity across various tasks. However, the
inference process for LLMs comes with significant computational costs. In this
paper, we propose an efficient LLM inference pipeline that harnesses the power
of LLMs. Our approach begins by tapping into the potential of LLMs to
accurately perceive and predict the response length with minimal overhead. By
leveraging this information, we introduce an efficient sequence scheduling
technique that groups queries with similar response lengths into micro-batches.
We evaluate our approach on real-world instruction datasets using the
LLaMA-based model, and our results demonstrate an impressive 86% improvement in
inference throughput without compromising effectiveness. Notably, our method is
orthogonal to other inference acceleration techniques, making it a valuable
addition to many existing toolkits (e.g., FlashAttention, Quantization) for LLM
inference.",None,-1
7c222d4a-36b5-474f-a801-f9dc8031efce,How Well Do Text Embedding Models Understand Syntax?,0.102901,"Text embedding models have significantly contributed to advancements in
natural language processing by adeptly capturing semantic properties of textual
data. However, the ability of these models to generalize across a wide range of
syntactic contexts remains under-explored. In this paper, we first develop an
evaluation set, named \textbf{SR}, to scrutinize the capability for syntax
understanding of text embedding models from two crucial syntactic aspects:
Structural heuristics, and Relational understanding among concepts, as revealed
by the performance gaps in previous studies. Our findings reveal that existing
text embedding models have not sufficiently addressed these syntactic
understanding challenges, and such ineffectiveness becomes even more apparent
when evaluated against existing benchmark datasets. Furthermore, we conduct
rigorous analysis to unearth factors that lead to such limitations and examine
why previous evaluations fail to detect such ineffectiveness. Lastly, we
propose strategies to augment the generalization ability of text embedding
models in diverse syntactic scenarios. This study serves to highlight the
hurdles associated with syntactic generalization and provides pragmatic
guidance for boosting model performance across varied syntactic contexts.",None,-1
ba9086b6-63d2-40b0-a812-6f086806415d,"The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation",0.999719,"Conversational artificial intelligence (AI) disrupts how humans interact with
technology. Recently, OpenAI introduced ChatGPT, a state-of-the-art dialogue
model that can converse with its human counterparts with unprecedented
capabilities. ChatGPT has witnessed tremendous attention from the media,
academia, industry, and the general public, attracting more than a million
users within days of its release. However, its explosive adoption for
information search and as an automated decision aid underscores the importance
to understand its limitations and biases. This paper focuses on one of
democratic society's most important decision-making processes: political
elections. Prompting ChatGPT with 630 political statements from two leading
voting advice applications and the nation-agnostic political compass test in
three pre-registered experiments, we uncover ChatGPT's pro-environmental,
left-libertarian ideology. For example, ChatGPT would impose taxes on flights,
restrict rent increases, and legalize abortion. In the 2021 elections, it would
have voted most likely for the Greens both in Germany (B\""undnis 90/Die
Gr\""unen) and in the Netherlands (GroenLinks). Our findings are robust when
negating the prompts, reversing the order of the statements, varying prompt
formality, and across languages (English, German, Dutch, and Spanish). We
conclude by discussing the implications of politically biased conversational AI
on society.",None,-1
c6b49f00-8808-4f0f-bfa0-7d811cba7e2c,Conceptual Cognitive Maps Formation with Neural Successor Networks and Word Embeddings,0.772201,"The human brain possesses the extraordinary capability to contextualize the
information it receives from our environment. The entorhinal-hippocampal plays
a critical role in this function, as it is deeply engaged in memory processing
and constructing cognitive maps using place and grid cells. Comprehending and
leveraging this ability could significantly augment the field of artificial
intelligence. The multi-scale successor representation serves as a good model
for the functionality of place and grid cells and has already shown promise in
this role. Here, we introduce a model that employs successor representations
and neural networks, along with word embedding vectors, to construct a
cognitive map of three separate concepts. The network adeptly learns two
different scaled maps and situates new information in proximity to related
pre-existing representations. The dispersion of information across the
cognitive map varies according to its scale - either being heavily
concentrated, resulting in the formation of the three concepts, or spread
evenly throughout the map. We suggest that our model could potentially improve
current AI models by providing multi-modal context information to any input,
based on a similarity metric for the input and pre-existing knowledge
representations.",None,-1
5d0c4e62-ca3a-4736-a288-3be45feb84f6,AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation,0.313764,"Both indoor and outdoor environments are inherently structured and
repetitive. Traditional modeling pipelines keep an asset library storing unique
object templates, which is both versatile and memory efficient in practice.
Inspired by this observation, we propose AssetField, a novel neural scene
representation that learns a set of object-aware ground feature planes to
represent the scene, where an asset library storing template feature patches
can be constructed in an unsupervised manner. Unlike existing methods which
require object masks to query spatial points for object editing, our ground
feature plane representation offers a natural visualization of the scene in the
bird-eye view, allowing a variety of operations (e.g. translation, duplication,
deformation) on objects to configure a new scene. With the template feature
patches, group editing is enabled for scenes with many recurring items to avoid
repetitive work on object individuals. We show that AssetField not only
achieves competitive performance for novel-view synthesis but also generates
realistic renderings for new scene configurations.",None,-1
80d3ee4c-70f5-4c3b-9687-56c6b2141e46,Generative AI for Business Strategy: Using Foundation Models to Create Business Strategy Tools,0.0747386,"Generative models (foundation models) such as LLMs (large language models)
are having a large impact on multiple fields. In this work, we propose the use
of such models for business decision making. In particular, we combine
unstructured textual data sources (e.g., news data) with multiple foundation
models (namely, GPT4, transformer-based Named Entity Recognition (NER) models
and Entailment-based Zero-shot Classifiers (ZSC)) to derive IT (information
technology) artifacts in the form of a (sequence of) signed business networks.
We posit that such artifacts can inform business stakeholders about the state
of the market and their own positioning as well as provide quantitative
insights into improving their future outlook.",None,-1
76bc10b4-81e6-4e8f-9ae1-90991de2d565,P+: Extended Textual Conditioning in Text-to-Image Generation,0.918145,"We introduce an Extended Textual Conditioning space in text-to-image models,
referred to as $P+$. This space consists of multiple textual conditions,
derived from per-layer prompts, each corresponding to a layer of the denoising
U-net of the diffusion model.
  We show that the extended space provides greater disentangling and control
over image synthesis. We further introduce Extended Textual Inversion (XTI),
where the images are inverted into $P+$, and represented by per-layer tokens.
  We show that XTI is more expressive and precise, and converges faster than
the original Textual Inversion (TI) space. The extended inversion method does
not involve any noticeable trade-off between reconstruction and editability and
induces more regular inversions.
  We conduct a series of extensive experiments to analyze and understand the
properties of the new space, and to showcase the effectiveness of our method
for personalizing text-to-image models. Furthermore, we utilize the unique
properties of this space to achieve previously unattainable results in
object-style mixing using text-to-image models. Project page:
https://prompt-plus.github.io",None,-1
9b4351a1-14ff-46ec-b5e9-d65900b9bd0d,Modular Visual Question Answering via Code Generation,0.743643,"We present a framework that formulates visual question answering as modular
code generation. In contrast to prior work on modular approaches to VQA, our
approach requires no additional training and relies on pre-trained language
models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA
examples used for in-context learning. The generated Python programs invoke and
compose the outputs of the visual models using arithmetic and conditional
logic. Our approach improves accuracy on the COVR dataset by at least 3% and on
the GQA dataset by roughly 2% compared to the few-shot baseline that does not
employ code generation.",None,-1
cf865768-0dd9-4203-8b82-504c05dce3f1,Curriculum Learning for Compositional Visual Reasoning,0.106349,"Visual Question Answering (VQA) is a complex task requiring large datasets
and expensive training. Neural Module Networks (NMN) first translate the
question to a reasoning path, then follow that path to analyze the image and
provide an answer. We propose an NMN method that relies on predefined
cross-modal embeddings to ``warm start'' learning on the GQA dataset, then
focus on Curriculum Learning (CL) as a way to improve training and make a
better use of the data. Several difficulty criteria are employed for defining
CL methods. We show that by an appropriate selection of the CL method the cost
of training and the amount of training data can be greatly reduced, with a
limited impact on the final VQA accuracy. Furthermore, we introduce
intermediate losses during training and find that this allows to simplify the
CL strategy.",None,-1
a3697916-79fd-4456-94aa-fa23512bb62a,Student Classroom Behavior Detection based on Improved YOLOv7,0.282319,"Accurately detecting student behavior in classroom videos can aid in
analyzing their classroom performance and improving teaching effectiveness.
However, the current accuracy rate in behavior detection is low. To address
this challenge, we propose the Student Classroom Behavior Detection method,
based on improved YOLOv7. First, we created the Student Classroom Behavior
dataset (SCB-Dataset), which includes 18.4k labels and 4.2k images, covering
three behaviors: hand raising, reading, and writing. To improve detection
accuracy in crowded scenes, we integrated the biformer attention module and
Wise-IoU into the YOLOv7 network. Finally, experiments were conducted on the
SCB-Dataset, and the model achieved an mAP@0.5 of 79%, resulting in a 1.8%
improvement over previous results. The SCB-Dataset and code are available for
download at: https://github.com/Whiffe/SCB-dataset.",None,-1
906f2db3-8175-4d1b-94d0-219003156021,Spherical Transformer for LiDAR-based 3D Recognition,0.999911,"LiDAR-based 3D point cloud recognition has benefited various applications.
Without specially considering the LiDAR point distribution, most current
methods suffer from information disconnection and limited receptive field,
especially for the sparse distant points. In this work, we study the
varying-sparsity distribution of LiDAR points and present SphereFormer to
directly aggregate information from dense close points to the sparse distant
ones. We design radial window self-attention that partitions the space into
multiple non-overlapping narrow and long windows. It overcomes the
disconnection issue and enlarges the receptive field smoothly and dramatically,
which significantly boosts the performance of sparse distant points. Moreover,
to fit the narrow and long windows, we propose exponential splitting to yield
fine-grained position encoding and dynamic feature selection to increase model
representation ability. Notably, our method ranks 1st on both nuScenes and
SemanticKITTI semantic segmentation benchmarks with 81.9% and 74.8% mIoU,
respectively. Also, we achieve the 3rd place on nuScenes object detection
benchmark with 72.8% NDS and 68.5% mAP. Code is available at
https://github.com/dvlab-research/SphereFormer.git.",None,-1
503ed85a-9c1f-4881-a625-5c4c3a8bddc1,Fact-Checking Generative AI: Ontology-Driven Biological Graphs for Disease-Gene Link Verification,0.37116,"Since the launch of various generative AI tools, scientists have been
striving to evaluate their capabilities and contents, in the hope of
establishing trust in their generative abilities. Regulations and guidelines
are emerging to verify generated contents and identify novel uses. we aspire to
demonstrate how ChatGPT claims are checked computationally using the rigor of
network models. We aim to achieve fact-checking of the knowledge embedded in
biological graphs that were contrived from ChatGPT contents at the aggregate
level. We adopted a biological networks approach that enables the systematic
interrogation of ChatGPT's linked entities. We designed an ontology-driven
fact-checking algorithm that compares biological graphs constructed from
approximately 200,000 PubMed abstracts with counterparts constructed from a
dataset generated using the ChatGPT-3.5 Turbo model. In 10-samples of 250
randomly selected records a ChatGPT dataset of 1000 ""simulated"" articles , the
fact-checking link accuracy ranged from 70% to 86%. This study demonstrated
high accuracy of aggregate disease-gene links relationships found in
ChatGPT-generated texts.",None,-1
898aaa8d-0371-4648-9514-b2e41db3b5ab,Parameterized Decision-making with Multi-modal Perception for Autonomous Driving,0.346105,"Autonomous driving is an emerging technology that has advanced rapidly over
the last decade. Modern transportation is expected to benefit greatly from a
wise decision-making framework of autonomous vehicles, including the
improvement of mobility and the minimization of risks and travel time. However,
existing methods either ignore the complexity of environments only fitting
straight roads, or ignore the impact on surrounding vehicles during
optimization phases, leading to weak environmental adaptability and incomplete
optimization objectives. To address these limitations, we propose a
parameterized decision-making framework with multi-modal perception based on
deep reinforcement learning, called AUTO. We conduct a comprehensive perception
to capture the state features of various traffic participants around the
autonomous vehicle, based on which we design a graph-based model to learn a
state representation of the multi-modal semantic features. To distinguish
between lane-following and lane-changing, we decompose an action of the
autonomous vehicle into a parameterized action structure that first decides
whether to change lanes and then computes an exact action to execute. A hybrid
reward function takes into account aspects of safety, traffic efficiency,
passenger comfort, and impact to guide the framework to generate optimal
actions. In addition, we design a regularization term and a multi-worker
paradigm to enhance the training. Extensive experiments offer evidence that
AUTO can advance state-of-the-art in terms of both macroscopic and microscopic
effectiveness.",None,-1
5d1710cb-0d15-4343-a023-5b0aa57d9cf4,Hallucination Improves the Performance of Unsupervised Visual Representation Learning,0.877504,"Contrastive learning models based on Siamese structure have demonstrated
remarkable performance in self-supervised learning. Such a success of
contrastive learning relies on two conditions, a sufficient number of positive
pairs and adequate variations between them. If the conditions are not met,
these frameworks will lack semantic contrast and be fragile on overfitting. To
address these two issues, we propose Hallucinator that could efficiently
generate additional positive samples for further contrast. The Hallucinator is
differentiable and creates new data in the feature space. Thus, it is optimized
directly with the pre-training task and introduces nearly negligible
computation. Moreover, we reduce the mutual information of hallucinated pairs
and smooth them through non-linear operations. This process helps avoid
over-confident contrastive learning models during the training and achieves
more transformation-invariant feature embeddings. Remarkably, we empirically
prove that the proposed Hallucinator generalizes well to various contrastive
learning models, including MoCoV1&V2, SimCLR and SimSiam. Under the linear
classification protocol, a stable accuracy gain is achieved, ranging from 0.3%
to 3.0% on CIFAR10&100, Tiny ImageNet, STL-10 and ImageNet. The improvement is
also observed in transferring pre-train encoders to the downstream tasks,
including object detection and segmentation.",None,-1
1932c3b9-4fe8-4c57-b14a-0a06fb422ada,Max-min Learning of Approximate Weight Matrices from Fuzzy Data,0.4134,"In this article, we study the approximate solutions set $\Lambda_b$ of an
inconsistent system of $\max-\min$ fuzzy relational equations $(S): A
\Box_{\min}^{\max}x =b$. Using the $L_\infty$ norm, we compute by an explicit
analytical formula the Chebyshev distance $\Delta~=~\inf_{c \in \mathcal{C}}
\Vert b -c \Vert$, where $\mathcal{C}$ is the set of second members of the
consistent systems defined with the same matrix $A$. We study the set
$\mathcal{C}_b$ of Chebyshev approximations of the second member $b$ i.e.,
vectors $c \in \mathcal{C}$ such that $\Vert b -c \Vert = \Delta$, which is
associated to the approximate solutions set $\Lambda_b$ in the following sense:
an element of the set $\Lambda_b$ is a solution vector $x^\ast$ of a system $A
\Box_{\min}^{\max}x =c$ where $c \in \mathcal{C}_b$. As main results, we
describe both the structure of the set $\Lambda_b$ and that of the set
$\mathcal{C}_b$. We then introduce a paradigm for $\max-\min$ learning weight
matrices that relates input and output data from training data. The learning
error is expressed in terms of the $L_\infty$ norm. We compute by an explicit
formula the minimal value of the learning error according to the training data.
We give a method to construct weight matrices whose learning error is minimal,
that we call approximate weight matrices.
  Finally, as an application of our results, we show how to learn approximately
the rule parameters of a possibilistic rule-based system according to multiple
training data.",None,-1
530b661b-53d3-4453-8b15-85dc38ba196b,Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance,0.304897,"Adopting a two-stage paradigm of pretraining followed by fine-tuning,
Pretrained Language Models (PLMs) have achieved substantial advancements in the
field of natural language processing. However, in real-world scenarios, data
labels are often noisy due to the complex annotation process, making it
essential to develop strategies for fine-tuning PLMs with such noisy labels. To
this end, we introduce an innovative approach for fine-tuning PLMs using noisy
labels, which incorporates the guidance of Large Language Models (LLMs) like
ChatGPT. This guidance assists in accurately distinguishing between clean and
noisy samples and provides supplementary information beyond the noisy labels,
thereby boosting the learning process during fine-tuning PLMs. Extensive
experiments on synthetic and real-world noisy datasets further demonstrate the
superior advantages of our framework over the state-of-the-art baselines.",None,-1
0a7f5d7a-203b-48ca-81c9-eda30dae52a3,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,0.872206,"While multi-modal foundation models pre-trained on large-scale data have been
successful in natural language understanding and vision recognition, their use
in medical domains is still limited due to the fine-grained nature of medical
tasks and the high demand for domain knowledge. To address this challenge, we
propose a novel approach called Knowledge-enhanced Auto Diagnosis (KAD) which
leverages existing medical domain knowledge to guide vision-language
pre-training using paired chest X-rays and radiology reports. We evaluate KAD
on {four} external X-ray datasets and demonstrate that its zero-shot
performance is not only comparable to that of fully-supervised models, but also
superior to the average of three expert radiologists for three (out of five)
pathologies with statistical significance. Moreover, when few-shot annotation
is available, KAD outperforms all existing approaches in fine-tuning settings,
demonstrating its potential for application in different clinical scenarios.",None,-1
0aa22d87-268e-4a70-b257-cf25e4317362,Cracking the Code of Negative Transfer: A Cooperative Game Theoretic Approach for Cross-Domain Sequential Recommendation,0.521157,"This paper investigates Cross-Domain Sequential Recommendation (CDSR), a
promising method that uses information from multiple domains (more than three)
to generate accurate and diverse recommendations, and takes into account the
sequential nature of user interactions. The effectiveness of these systems
often depends on the complex interplay among the multiple domains. In this
dynamic landscape, the problem of negative transfer arises, where heterogeneous
knowledge between dissimilar domains leads to performance degradation due to
differences in user preferences across these domains. As a remedy, we propose a
new CDSR framework that addresses the problem of negative transfer by assessing
the extent of negative transfer from one domain to another and adaptively
assigning low weight values to the corresponding prediction losses. To this
end, the amount of negative transfer is estimated by measuring the marginal
contribution of each domain to model performance based on a cooperative game
theory. In addition, a hierarchical contrastive learning approach that
incorporates information from the sequence of coarse-level categories into that
of fine-level categories (e.g., item level) when implementing contrastive
learning was developed to mitigate negative transfer. Despite the potentially
low relevance between domains at the fine-level, there may be higher relevance
at the category level due to its generalised and broader preferences. We show
that our model is superior to prior works in terms of model performance on two
real-world datasets across ten different domains.",None,-1
da82b35c-3008-4d6c-89d6-3f371aac5a32,Progressively Optimized Local Radiance Fields for Robust View Synthesis,0.852064,"We present an algorithm for reconstructing the radiance field of a
large-scale scene from a single casually captured video. The task poses two
core challenges. First, most existing radiance field reconstruction approaches
rely on accurate pre-estimated camera poses from Structure-from-Motion
algorithms, which frequently fail on in-the-wild videos. Second, using a
single, global radiance field with finite representational capacity does not
scale to longer trajectories in an unbounded scene. For handling unknown poses,
we jointly estimate the camera poses with radiance field in a progressive
manner. We show that progressive optimization significantly improves the
robustness of the reconstruction. For handling large unbounded scenes, we
dynamically allocate new local radiance fields trained with frames within a
temporal window. This further improves robustness (e.g., performs well even
under moderate pose drifts) and allows us to scale to large scenes. Our
extensive evaluation on the Tanks and Temples dataset and our collected outdoor
dataset, Static Hikes, show that our approach compares favorably with the
state-of-the-art.",None,-1
c58509fc-5508-47c5-b89e-dfc08f3a81a9,STEERER: Resolving Scale Variations for Counting and Localization via Selective Inheritance Learning,0.445063,"Scale variation is a deep-rooted problem in object counting, which has not
been effectively addressed by existing scale-aware algorithms. An important
factor is that they typically involve cooperative learning across
multi-resolutions, which could be suboptimal for learning the most
discriminative features from each scale. In this paper, we propose a novel
method termed STEERER (\textbf{S}elec\textbf{T}iv\textbf{E}
inh\textbf{ER}itance l\textbf{E}a\textbf{R}ning) that addresses the issue of
scale variations in object counting. STEERER selects the most suitable scale
for patch objects to boost feature extraction and only inherits discriminative
features from lower to higher resolution progressively. The main insights of
STEERER are a dedicated Feature Selection and Inheritance Adaptor (FSIA), which
selectively forwards scale-customized features at each scale, and a Masked
Selection and Inheritance Loss (MSIL) that helps to achieve high-quality
density maps across all scales. Our experimental results on nine datasets with
counting and localization tasks demonstrate the unprecedented scale
generalization ability of STEERER. Code is available at
\url{https://github.com/taohan10200/STEERER}.",None,-1
c8d37e8e-bac8-4f77-af04-92467afd177f,Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling,0.937721,"As the size of pre-trained speech recognition models increases, running these
large models in low-latency or resource-constrained environments becomes
challenging. In this work, we leverage pseudo-labelling to assemble a
large-scale open-source dataset which we use to distill the Whisper model into
a smaller variant, called Distil-Whisper. Using a simple word error rate (WER)
heuristic, we select only the highest quality pseudo-labels for training. The
distilled model is 5.8 times faster with 51% fewer parameters, while performing
to within 1% WER on out-of-distribution test data in a zero-shot transfer
setting. Distil-Whisper maintains the robustness of the Whisper model to
difficult acoustic conditions, while being less prone to hallucination errors
on long-form audio. Distil-Whisper is designed to be paired with Whisper for
speculative decoding, yielding a 2 times speed-up while mathematically ensuring
the same outputs as the original model. To facilitate further research in this
domain, we make our training code, inference code and models publicly
accessible.",None,-1
ddb5bf0d-07cd-4041-89ae-cc5dbfd7442a,LM vs LM: Detecting Factual Errors via Cross Examination,0.834093,"A prominent weakness of modern language models (LMs) is their tendency to
generate factually incorrect text, which hinders their usability. A natural
question is whether such factual errors can be detected automatically. Inspired
by truth-seeking mechanisms in law, we propose a factuality evaluation
framework for LMs that is based on cross-examination. Our key idea is that an
incorrect claim is likely to result in inconsistency with other claims that the
model generates. To discover such inconsistencies, we facilitate a multi-turn
interaction between the LM that generated the claim and another LM (acting as
an examiner) which introduces questions to discover inconsistencies. We
empirically evaluate our method on factual claims made by multiple recent LMs
on four benchmarks, finding that it outperforms existing methods and baselines,
often by a large gap. Our results demonstrate the potential of using
interacting LMs for capturing factual errors.",None,-1
9fd3ae82-90d4-4e21-bb41-08c396c8d58b,SOCS: Semantically-aware Object Coordinate Space for Category-Level 6D Object Pose Estimation under Large Shape Variations,0.572665,"Most learning-based approaches to category-level 6D pose estimation are
design around normalized object coordinate space (NOCS). While being
successful, NOCS-based methods become inaccurate and less robust when handling
objects of a category containing significant intra-category shape variations.
This is because the object coordinates induced by global and rigid alignment of
objects are semantically incoherent, making the coordinate regression hard to
learn and generalize. We propose Semantically-aware Object Coordinate Space
(SOCS) built by warping-and-aligning the objects guided by a sparse set of
keypoints with semantically meaningful correspondence. SOCS is semantically
coherent: Any point on the surface of a object can be mapped to a semantically
meaningful location in SOCS, allowing for accurate pose and size estimation
under large shape variations. To learn effective coordinate regression to SOCS,
we propose a novel multi-scale coordinate-based attention network. Evaluations
demonstrate that our method is easy to train, well-generalizing for large
intra-category shape variations and robust to inter-object occlusions.",None,-1
be656241-4d06-4363-abcb-d071eb03c7cd,Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation,0.684019,"Emotion regulation is a crucial element in dealing with emotional events and
has positive effects on mental health. This paper aims to provide a more
comprehensive understanding of emotional events by introducing a new French
corpus of emotional narratives collected using a questionnaire for emotion
regulation. We follow the theoretical framework of the Component Process Model
which considers emotions as dynamic processes composed of four interrelated
components (behavior, feeling, thinking and territory). Each narrative is
related to a discrete emotion and is structured based on all emotion components
by the writers. We study the interaction of components and their impact on
emotion classification with machine learning methods and pre-trained language
models. Our results show that each component improves prediction performance,
and that the best results are achieved by jointly considering all components.
Our results also show the effectiveness of pre-trained language models in
predicting discrete emotion from certain components, which reveal differences
in how emotion components are expressed.",None,-1
b412a3b9-1e4d-4a14-9f63-3d56c81bdb6b,Disentangled Representation for Diversified Recommendations,0.245867,"Accuracy and diversity have long been considered to be two conflicting goals
for recommendations. We point out, however, that as the diversity is typically
measured by certain pre-selected item attributes, e.g., category as the most
popularly employed one, improved diversity can be achieved without sacrificing
recommendation accuracy, as long as the diversification respects the user's
preference about the pre-selected attributes. This calls for a fine-grained
understanding of a user's preferences over items, where one needs to recognize
the user's choice is driven by the quality of the item itself, or the
pre-selected attributes of the item. In this work, we focus on diversity
defined on item categories. We propose a general diversification framework
agnostic to the choice of recommendation algorithms. Our solution disentangles
the learnt user representation in the recommendation module into
category-independent and category-dependent components to differentiate a
user's preference over items from two orthogonal perspectives. Experimental
results on three benchmark datasets and online A/B test demonstrate the
effectiveness of our solution in improving both recommendation accuracy and
diversity. In-depth analysis suggests that the improvement is due to our
improved modeling of users' categorical preferences and refined ranking within
item categories.",None,-1
4ca4860d-5cde-41f8-ae6b-a365534b405a,Single Domain Generalization via Normalised Cross-correlation Based Convolutions,0.0687525,"Deep learning techniques often perform poorly in the presence of domain
shift, where the test data follows a different distribution than the training
data. The most practically desirable approach to address this issue is Single
Domain Generalization (S-DG), which aims to train robust models using data from
a single source. Prior work on S-DG has primarily focused on using data
augmentation techniques to generate diverse training data. In this paper, we
explore an alternative approach by investigating the robustness of linear
operators, such as convolution and dense layers commonly used in deep learning.
We propose a novel operator called XCNorm that computes the normalized
cross-correlation between weights and an input feature patch. This approach is
invariant to both affine shifts and changes in energy within a local feature
patch and eliminates the need for commonly used non-linear activation
functions. We show that deep neural networks composed of this operator are
robust to common semantic distribution shifts. Furthermore, our empirical
results on single-domain generalization benchmarks demonstrate that our
proposed technique performs comparably to the state-of-the-art methods.",None,-1
77ac5eea-9e69-4dfe-9242-89677c4641c2,HeightFormer: Explicit Height Modeling without Extra Data for Camera-only 3D Object Detection in Bird's Eye View,0.514041,"Vision-based Bird's Eye View (BEV) representation is an emerging perception
formulation for autonomous driving. The core challenge is to construct BEV
space with multi-camera features, which is a one-to-many ill-posed problem.
Diving into all previous BEV representation generation methods, we found that
most of them fall into two types: modeling depths in image views or modeling
heights in the BEV space, mostly in an implicit way. In this work, we propose
to explicitly model heights in the BEV space, which needs no extra data like
LiDAR and can fit arbitrary camera rigs and types compared to modeling depths.
Theoretically, we give proof of the equivalence between height-based methods
and depth-based methods. Considering the equivalence and some advantages of
modeling heights, we propose HeightFormer, which models heights and
uncertainties in a self-recursive way. Without any extra data, the proposed
HeightFormer could estimate heights in BEV accurately. Benchmark results show
that the performance of HeightFormer achieves SOTA compared with those
camera-only methods.",None,-1
8328bc2c-aa1d-4428-bfd0-c6f4316448bb,Not with my name! Inferring artists' names of input strings employed by Diffusion Models,0.0815472,"Diffusion Models (DM) are highly effective at generating realistic,
high-quality images. However, these models lack creativity and merely compose
outputs based on their training data, guided by a textual input provided at
creation time. Is it acceptable to generate images reminiscent of an artist,
employing his name as input? This imply that if the DM is able to replicate an
artist's work then it was trained on some or all of his artworks thus violating
copyright. In this paper, a preliminary study to infer the probability of use
of an artist's name in the input string of a generated image is presented. To
this aim we focused only on images generated by the famous DALL-E 2 and
collected images (both original and generated) of five renowned artists.
Finally, a dedicated Siamese Neural Network was employed to have a first kind
of probability. Experimental results demonstrate that our approach is an
optimal starting point and can be employed as a prior for predicting a complete
input string of an investigated image. Dataset and code are available at:
https://github.com/ictlab-unict/not-with-my-name .",None,-1
f692f071-2538-469f-976b-692b858348d4,Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization,0.274842,"Large Language Models (LLMs) are proficient in natural language processing
tasks, but their deployment is often restricted by extensive parameter sizes
and computational demands. This paper focuses on post-training quantization
(PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8)
quantization, to enhance computational efficiency -- a topic less explored
compared to weight-only quantization. We present two innovative techniques:
activation-quantization-aware scaling (AQAS) and sequence-length-aware
calibration (SLAC) to enhance PTQ by considering the combined effects on
weights and activations and aligning calibration sequence lengths to target
tasks. Moreover, we introduce dINT, a hybrid data format combining integer and
denormal representations, to address the underflow issue in W4A8 quantization,
where small values are rounded to zero. Through rigorous evaluations of LLMs,
including OPT and LLaMA, we demonstrate that our techniques significantly boost
task accuracies to levels comparable with full-precision models. By developing
arithmetic units compatible with dINT, we further confirm that our methods
yield a 2$\times$ hardware efficiency improvement compared to 8-bit integer MAC
unit.",None,-1
569d55e9-13d1-49b0-93d4-d2f3bd8dd3a8,Duplex Diffusion Models Improve Speech-to-Speech Translation,0.589174,"Speech-to-speech translation is a typical sequence-to-sequence learning task
that naturally has two directions. How to effectively leverage bidirectional
supervision signals to produce high-fidelity audio for both directions?
Existing approaches either train two separate models or a multitask-learned
model with low efficiency and inferior performance. In this paper, we propose a
duplex diffusion model that applies diffusion probabilistic models to both
sides of a reversible duplex Conformer, so that either end can simultaneously
input and output a distinct language's speech. Our model enables reversible
speech translation by simply flipping the input and output ends. Experiments
show that our model achieves the first success of reversible speech translation
with significant improvements of ASR-BLEU scores compared with a list of
state-of-the-art baselines.",None,-1
2b086a0c-78a5-4de3-8f18-bd31e6d88ab8,Zero-Shot Scene Graph Generation via Triplet Calibration and Reduction,0.0780837,"Scene Graph Generation (SGG) plays a pivotal role in downstream
vision-language tasks. Existing SGG methods typically suffer from poor
compositional generalizations on unseen triplets. They are generally trained on
incompletely annotated scene graphs that contain dominant triplets and tend to
bias toward these seen triplets during inference. To address this issue, we
propose a Triplet Calibration and Reduction (T-CAR) framework in this paper. In
our framework, a triplet calibration loss is first presented to regularize the
representations of diverse triplets and to simultaneously excavate the unseen
triplets in incompletely annotated training scene graphs. Moreover, the unseen
space of scene graphs is usually several times larger than the seen space since
it contains a huge number of unrealistic compositions. Thus, we propose an
unseen space reduction loss to shift the attention of excavation to reasonable
unseen compositions to facilitate the model training. Finally, we propose a
contextual encoder to improve the compositional generalizations of unseen
triplets by explicitly modeling the relative spatial relations between subjects
and objects. Extensive experiments show that our approach achieves consistent
improvements for zero-shot SGG over state-of-the-art methods. The code is
available at https://github.com/jkli1998/T-CAR.",None,-1
cb37aa8c-293e-42dd-bc3d-843e1965ed3f,Emotionally Enhanced Talking Face Generation,0.609572,"Several works have developed end-to-end pipelines for generating lip-synced
talking faces with various real-world applications, such as teaching and
language translation in videos. However, these prior works fail to create
realistic-looking videos since they focus little on people's expressions and
emotions. Moreover, these methods' effectiveness largely depends on the faces
in the training dataset, which means they may not perform well on unseen faces.
To mitigate this, we build a talking face generation framework conditioned on a
categorical emotion to generate videos with appropriate expressions, making
them more realistic and convincing. With a broad range of six emotions, i.e.,
\emph{happiness}, \emph{sadness}, \emph{fear}, \emph{anger}, \emph{disgust},
and \emph{neutral}, we show that our model can adapt to arbitrary identities,
emotions, and languages. Our proposed framework is equipped with a
user-friendly web interface with a real-time experience for talking face
generation with emotions. We also conduct a user study for subjective
evaluation of our interface's usability, design, and functionality. Project
page: https://midas.iiitd.edu.in/emo/",None,-1
7586da71-542f-4f31-a239-54aab8b2ed12,Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents,0.999977,"In this paper, we present a novel framework for enhancing the capabilities of
large language models (LLMs) by leveraging the power of multi-agent systems.
Our framework introduces a collaborative environment where multiple intelligent
agent components, each with distinctive attributes and roles, work together to
handle complex tasks more efficiently and effectively. We demonstrate the
practicality and versatility of our framework through case studies in
artificial general intelligence (AGI), specifically focusing on the Auto-GPT
and BabyAGI models. We also examine the ""Gorilla"" model, which integrates
external APIs into the LLM. Our framework addresses limitations and challenges
such as looping issues, security risks, scalability, system evaluation, and
ethical considerations. By modeling various domains such as courtroom
simulations and software development scenarios, we showcase the potential
applications and benefits of our proposed multi-agent system. Our framework
provides an avenue for advancing the capabilities and performance of LLMs
through collaboration and knowledge exchange among intelligent agents.",None,-1
270d7a69-dcd0-48c2-8d8f-d7b767716345,Structured Epipolar Matcher for Local Feature Matching,0.168567,"Local feature matching is challenging due to textureless and repetitive
patterns. Existing methods focus on using appearance features and global
interaction and matching, while the importance of geometry priors in local
feature matching has not been fully exploited. Different from these methods, in
this paper, we delve into the importance of geometry prior and propose
Structured Epipolar Matcher (SEM) for local feature matching, which can
leverage the geometric information in an iterative matching way. The proposed
model enjoys several merits. First, our proposed Structured Feature Extractor
can model the relative positional relationship between pixels and
high-confidence anchor points. Second, our proposed Epipolar Attention and
Matching can filter out irrelevant areas by utilizing the epipolar constraint.
Extensive experimental results on five standard benchmarks demonstrate the
superior performance of our SEM compared to state-of-the-art methods. Project
page: https://sem2023.github.io.",None,-1
1154f9ff-2b8d-46bc-90db-3476421b60a7,NovPhy: A Testbed for Physical Reasoning in Open-world Environments,0.0457542,"Due to the emergence of AI systems that interact with the physical
environment, there is an increased interest in incorporating physical reasoning
capabilities into those AI systems. But is it enough to only have physical
reasoning capabilities to operate in a real physical environment? In the real
world, we constantly face novel situations we have not encountered before. As
humans, we are competent at successfully adapting to those situations.
Similarly, an agent needs to have the ability to function under the impact of
novelties in order to properly operate in an open-world physical environment.
To facilitate the development of such AI systems, we propose a new testbed,
NovPhy, that requires an agent to reason about physical scenarios in the
presence of novelties and take actions accordingly. The testbed consists of
tasks that require agents to detect and adapt to novelties in physical
scenarios. To create tasks in the testbed, we develop eight novelties
representing a diverse novelty space and apply them to five commonly
encountered scenarios in a physical environment. According to our testbed
design, we evaluate two capabilities of an agent: the performance on a novelty
when it is applied to different physical scenarios and the performance on a
physical scenario when different novelties are applied to it. We conduct a
thorough evaluation with human players, learning agents, and heuristic agents.
Our evaluation shows that humans' performance is far beyond the agents'
performance. Some agents, even with good normal task performance, perform
significantly worse when there is a novelty, and the agents that can adapt to
novelties typically adapt slower than humans. We promote the development of
intelligent agents capable of performing at the human level or above when
operating in open-world physical environments. Testbed website:
https://github.com/phy-q/novphy",None,-1
8460f0ae-5491-445c-9515-1726fb4e3be7,FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models,0.492267,"Large language models (LLMs) have demonstrated exceptional performance in
various natural language processing tasks, yet their efficacy in more
challenging and domain-specific tasks remains largely unexplored. This paper
presents FinEval, a benchmark specifically designed for the financial domain
knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice
questions covering Finance, Economy, Accounting, and Certificate. It includes
4,661 questions spanning 34 different academic subjects. To ensure a
comprehensive model performance evaluation, FinEval employs a range of prompt
types, including zero-shot and few-shot prompts, as well as answer-only and
chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs
on FinEval, the results show that only GPT-4 achieved an accuracy close to 70%
in different prompt settings, indicating significant growth potential for LLMs
in the financial domain knowledge. Our work offers a more comprehensive
financial knowledge evaluation benchmark, utilizing data of mock exams and
covering a wide range of evaluated LLMs.",None,-1
ff6a49af-edfe-4e3c-9165-c6bcbff37bcb,llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology,0.0381973,"This study constructed a Japanese chat dataset for tuning large language
models (LLMs), which consist of about 8.4 million records. Recently, LLMs have
been developed and gaining popularity. However, high-performing LLMs are
usually mainly for English. There are two ways to support languages other than
English by those LLMs: constructing LLMs from scratch or tuning existing
models. However, in both ways, datasets are necessary parts. In this study, we
focused on supporting Japanese in those LLMs and making a dataset for training
or tuning LLMs in Japanese. The dataset we constructed consisted of various
tasks, such as translation and knowledge tasks. In our experiment, we tuned an
existing LLM using our dataset and evaluated the performance qualitatively. The
results suggest that our dataset is possibly beneficial for LLMs. However, we
also revealed some difficulties in constructing LLMs in languages other than
English.",None,-1
8ef3a02d-a468-4dc2-90ff-9374dbc44cfa,Bipartite Graph Diffusion Model for Human Interaction Generation,0.601755,"The generation of natural human motion interactions is a hot topic in
computer vision and computer animation. It is a challenging task due to the
diversity of possible human motion interactions. Diffusion models, which have
already shown remarkable generative capabilities in other domains, are a good
candidate for this task. In this paper, we introduce a novel bipartite graph
diffusion method (BiGraphDiff) to generate human motion interactions between
two persons. Specifically, bipartite node sets are constructed to model the
inherent geometric constraints between skeleton nodes during interactions. The
interaction graph diffusion model is transformer-based, combining some
state-of-the-art motion methods. We show that the proposed achieves new
state-of-the-art results on leading benchmarks for the human interaction
generation task.",None,-1
9fcbc14c-ace9-48a4-91ac-d8ce961ad59a,Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES,0.375919,"As part of the WMT-2023 ""Test suites"" shared task, in this paper we summarize
the results of two test suites evaluations: MuST-SHE-WMT23 and INES. By
focusing on the en-de and de-en language pairs, we rely on these newly created
test suites to investigate systems' ability to translate feminine and masculine
gender and produce gender-inclusive translations. Furthermore we discuss
metrics associated with our test suites and validate them by means of human
evaluations. Our results indicate that systems achieve reasonable and
comparable performance in correctly translating both feminine and masculine
gender forms for naturalistic gender phenomena. Instead, the generation of
inclusive language forms in translation emerges as a challenging task for all
the evaluated MT models, indicating room for future improvements and research
on the topic.",None,-1
22789fd9-e4f8-4bd9-9b97-6fb0d8f1f887,Perspectives on the State and Future of Deep Learning - 2023,0.0858481,"The goal of this series is to chronicle opinions and issues in the field of
machine learning as they stand today and as they change over time. The plan is
to host this survey periodically until the AI singularity
paperclip-frenzy-driven doomsday, keeping an updated list of topical questions
and interviewing new community members for each edition. In this issue, we
probed people's opinions on interpretable AI, the value of benchmarking in
modern NLP, the state of progress towards understanding deep learning, and the
future of academia.",None,-1
fe0b1af2-7dc1-41ab-b69a-e2d829fee36f,Iterative Geometry Encoding Volume for Stereo Matching,0.99678,"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in
matching tasks. However, all-pairs correlations lack non-local geometry
knowledge and have difficulties tackling local ambiguities in ill-posed
regions. In this paper, we propose Iterative Geometry Encoding Volume
(IGEV-Stereo), a new deep network architecture for stereo matching. The
proposed IGEV-Stereo builds a combined geometry encoding volume that encodes
geometry and context information as well as local matching details, and
iteratively indexes it to update the disparity map. To speed up the
convergence, we exploit GEV to regress an accurate starting point for ConvGRUs
iterations. Our IGEV-Stereo ranks $1^{st}$ on KITTI 2015 and 2012 (Reflective)
among all published methods and is the fastest among the top 10 methods. In
addition, IGEV-Stereo has strong cross-dataset generalization as well as high
inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e.
IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is
available at https://github.com/gangweiX/IGEV.",None,-1
516f64c1-6ba8-4c77-a87e-6bfe58c77374,Causal Document-Grounded Dialogue Pre-training,0.426382,"The goal of document-grounded dialogue (DocGD) is to generate a response by
grounding the evidence in a supporting document in accordance with the dialogue
context. This process involves four variables that are causally connected.
Recently, task-specific pre-training has greatly boosted performances on many
downstream tasks. Existing DocGD methods, however, continue to rely on general
pre-trained language models without a specifically tailored pre-training
approach that explicitly captures the causal relationships. To tackle this
issue, we are the first to present a causally-complete dataset construction
strategy for building million-level DocGD pre-training corpora. To better
capture causality, we further propose a causally-perturbed pre-training
strategy, which introduces causal perturbations on the variables and optimizes
the overall causal effect. Experiments on three benchmark datasets demonstrate
that our causal pre-training achieves considerable and consistent improvements
under fully-supervised, low-resource, few-shot, and zero-shot settings.",None,-1
41608637-07e6-42df-9483-8b32f9eb8957,Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration,0.230302,"The current approach for testing the robustness of object detectors suffers
from serious deficiencies such as improper methods of performing
out-of-distribution detection and using calibration metrics which do not
consider both localisation and classification quality. In this work, we address
these issues, and introduce the Self-Aware Object Detection (SAOD) task, a
unified testing framework which respects and adheres to the challenges that
object detectors face in safety-critical environments such as autonomous
driving. Specifically, the SAOD task requires an object detector to be: robust
to domain shift; obtain reliable uncertainty estimates for the entire scene;
and provide calibrated confidence scores for the detections. We extensively use
our framework, which introduces novel metrics and large scale test datasets, to
test numerous object detectors in two different use-cases, allowing us to
highlight critical insights into their robustness performance. Finally, we
introduce a simple baseline for the SAOD task, enabling researchers to
benchmark future proposed methods and move towards robust object detectors
which are fit for purpose. Code is available at https://github.com/fiveai/saod",None,-1
88007850-a64a-4afc-9a60-6862efb63b06,Design Booster: A Text-Guided Diffusion Model for Image Translation with Spatial Layout Preservation,0.158745,"Diffusion models are able to generate photorealistic images in arbitrary
scenes. However, when applying diffusion models to image translation, there
exists a trade-off between maintaining spatial structure and high-quality
content. Besides, existing methods are mainly based on test-time optimization
or fine-tuning model for each input image, which are extremely time-consuming
for practical applications. To address these issues, we propose a new approach
for flexible image translation by learning a layout-aware image condition
together with a text condition. Specifically, our method co-encodes images and
text into a new domain during the training phase. In the inference stage, we
can choose images/text or both as the conditions for each time step, which
gives users more flexible control over layout and content. Experimental
comparisons of our method with state-of-the-art methods demonstrate our model
performs best in both style image translation and semantic image translation
and took the shortest time.",None,-1
8118a535-0710-4fa9-8c35-e4050a8cb19d,ZeroPose: CAD-Model-based Zero-Shot Pose Estimation,0.540853,"In this paper, we present a CAD model-based zero-shot pose estimation
pipeline called ZeroPose. Existing pose estimation methods remain to require
expensive training when applied to an unseen object, which greatly hinders
their scalability in the practical application of industry. In contrast, the
proposed method enables the accurate estimation of pose parameters for
previously unseen objects without the need for training. Specifically, we
design a two-step pipeline consisting of CAD model-based zero-shot instance
segmentation and a zero-shot pose estimator. For the first step, there is a
simple but effective way to leverage CAD models and visual foundation models
SAM and Imagebind to segment the interest unseen object at the instance level.
For the second step, we based on the intensive geometric information in the CAD
model of the rigid object to propose a lightweight hierarchical geometric
structure matching mechanism achieving zero-shot pose estimation. Extensive
experimental results on the seven core datasets on the BOP challenge show that
the proposed zero-shot instance segmentation methods achieve comparable
performance with supervised MaskRCNN and the zero-shot pose estimation results
outperform the SOTA pose estimators with better efficiency.",None,-1
4187c73e-0e56-4590-9283-043e809e0531,ICICLE: Interpretable Class Incremental Continual Learning,0.676481,"Continual learning enables incremental learning of new tasks without
forgetting those previously learned, resulting in positive knowledge transfer
that can enhance performance on both new and old tasks. However, continual
learning poses new challenges for interpretability, as the rationale behind
model predictions may change over time, leading to interpretability concept
drift. We address this problem by proposing Interpretable Class-InCremental
LEarning (ICICLE), an exemplar-free approach that adopts a prototypical
part-based approach. It consists of three crucial novelties: interpretability
regularization that distills previously learned concepts while preserving
user-friendly positive reasoning; proximity-based prototype initialization
strategy dedicated to the fine-grained setting; and task-recency bias
compensation devoted to prototypical parts. Our experimental results
demonstrate that ICICLE reduces the interpretability concept drift and
outperforms the existing exemplar-free methods of common class-incremental
learning when applied to concept-based models.",None,-1
6837b85f-2b6d-4a7f-b10d-0336e5725095,A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding,0.741395,"Zero-shot dialogue understanding aims to enable dialogue to track the user's
needs without any training data, which has gained increasing attention. In this
work, we investigate the understanding ability of ChatGPT for zero-shot
dialogue understanding tasks including spoken language understanding (SLU) and
dialogue state tracking (DST). Experimental results on four popular benchmarks
reveal the great potential of ChatGPT for zero-shot dialogue understanding. In
addition, extensive analysis shows that ChatGPT benefits from the multi-turn
interactive prompt in the DST task but struggles to perform slot filling for
SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue
understanding tasks, hoping to provide some insights for future research on
building zero-shot dialogue understanding systems with Large Language Models
(LLMs).",None,-1
6f756058-8e17-47b6-9425-ed8ef1ab7436,EventCLIP: Adapting CLIP for Event-based Object Recognition,0.644146,"Recent advances in zero-shot and few-shot classification heavily rely on the
success of pre-trained vision-language models (VLMs) such as CLIP. Due to a
shortage of large-scale datasets, training such models for event camera data
remains infeasible. Thus, adapting existing VLMs across modalities to event
vision is an important research challenge. In this work, we introduce
EventCLIP, a novel approach that utilizes CLIP for zero-shot and few-shot
event-based object recognition. We first generalize CLIP's image encoder to
event data by converting raw events to 2D grid-based representations. To
further enhance performance, we propose a feature adapter to aggregate temporal
information over event frames and refine text embeddings to better align with
the visual inputs. We evaluate EventCLIP on N-Caltech, N-Cars, and N-ImageNet
datasets, achieving state-of-the-art few-shot performance. When fine-tuned on
the entire dataset, our method outperforms all existing event classifiers.
Moreover, we explore practical applications of EventCLIP including robust event
classification and label-free event recognition, where our approach surpasses
previous baselines designed specifically for these tasks.",None,-1
0c3d1fef-30c2-4143-a1de-ad43b717fc49,Unsupervised Semantic Correspondence Using Stable Diffusion,0.845484,"Text-to-image diffusion models are now capable of generating images that are
often indistinguishable from real images. To generate such images, these models
must understand the semantics of the objects they are asked to generate. In
this work we show that, without any training, one can leverage this semantic
knowledge within diffusion models to find semantic correspondences - locations
in multiple images that have the same semantic meaning. Specifically, given an
image, we optimize the prompt embeddings of these models for maximum attention
on the regions of interest. These optimized embeddings capture semantic
information about the location, which can then be transferred to another image.
By doing so we obtain results on par with the strongly supervised state of the
art on the PF-Willow dataset and significantly outperform (20.9% relative for
the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow,
CUB-200 and SPair-71k datasets.",None,-1
8b3fba75-57f9-497e-9d44-e89d599815f0,The language of sounds unheard: Exploring musical timbre semantics of large language models,0.669247,"Semantic dimensions of sound have been playing a central role in
understanding the nature of auditory sensory experience as well as the broader
relation between perception, language, and meaning. Accordingly, and given the
recent proliferation of large language models (LLMs), here we asked whether
such models exhibit an organisation of perceptual semantics similar to those
observed in humans. Specifically, we prompted ChatGPT, a chatbot based on a
state-of-the-art LLM, to rate musical instrument sounds on a set of 20 semantic
scales. We elicited multiple responses in separate chats, analogous to having
multiple human raters. ChatGPT generated semantic profiles that only partially
correlated with human ratings, yet showed robust agreement along well-known
psychophysical dimensions of musical sounds such as brightness (bright-dark)
and pitch height (deep-high). Exploratory factor analysis suggested the same
dimensionality but different spatial configuration of a latent factor space
between the chatbot and human ratings. Unexpectedly, the chatbot showed degrees
of internal variability that were comparable in magnitude to that of human
ratings. Our work highlights the potential of LLMs to capture salient
dimensions of human sensory experience.",None,-1
1c8f44d6-329b-4864-8007-10b40c8d1455,SoccerNet 2023 Tracking Challenge -- 3rd place MOT4MOT Team Technical Report,0.705425,"The SoccerNet 2023 tracking challenge requires the detection and tracking of
soccer players and the ball. In this work, we present our approach to tackle
these tasks separately. We employ a state-of-the-art online multi-object
tracker and a contemporary object detector for player tracking. To overcome the
limitations of our online approach, we incorporate a post-processing stage
using interpolation and appearance-free track merging. Additionally, an
appearance-based track merging technique is used to handle the termination and
creation of tracks far from the image boundaries. Ball tracking is formulated
as single object detection, and a fine-tuned YOLOv8l detector with proprietary
filtering improves the detection precision. Our method achieves 3rd place on
the SoccerNet 2023 tracking challenge with a HOTA score of 66.27.",None,-1
0a968261-697a-49d2-98f1-51851aa939eb,PMatch: Paired Masked Image Modeling for Dense Geometric Matching,0.772557,"Dense geometric matching determines the dense pixel-wise correspondence
between a source and support image corresponding to the same 3D structure.
Prior works employ an encoder of transformer blocks to correlate the two-frame
features. However, existing monocular pretraining tasks, e.g., image
classification, and masked image modeling (MIM), can not pretrain the
cross-frame module, yielding less optimal performance. To resolve this, we
reformulate the MIM from reconstructing a single masked image to reconstructing
a pair of masked images, enabling the pretraining of transformer module.
Additionally, we incorporate a decoder into pretraining for improved upsampling
results. Further, to be robust to the textureless area, we propose a novel
cross-frame global matching module (CFGM). Since the most textureless area is
planar surfaces, we propose a homography loss to further regularize its
learning. Combined together, we achieve the State-of-The-Art (SoTA) performance
on geometric matching. Codes and models are available at
https://github.com/ShngJZ/PMatch.",None,-1
dc53935e-1372-4d32-98c4-cb1b580c9d64,In-Rack Test Tube Pose Estimation Using RGB-D Data,0.488279,"Accurate robotic manipulation of test tubes in biology and medical industries
is becoming increasingly important to address workforce shortages and improve
worker safety. The detection and localization of test tubes are essential for
the robots to successfully manipulate test tubes. In this paper, we present a
framework to detect and estimate poses for the in-rack test tubes using color
and depth data. The methodology involves the utilization of a YOLO object
detector to effectively classify and localize both the test tubes and the tube
racks within the provided image data. Subsequently, the pose of the tube rack
is estimated through point cloud registration techniques. During the process of
estimating the poses of the test tubes, we capitalize on constraints derived
from the arrangement of rack slots. By employing an optimization-based
algorithm, we effectively evaluate and refine the pose of the test tubes. This
strategic approach ensures the robustness of pose estimation, even when
confronted with noisy and incomplete point cloud data.",None,-1
f6581a67-c8a8-4d42-89cf-b7ff542a9f93,Learning Dense UV Completion for Human Mesh Recovery,0.402917,"Human mesh reconstruction from a single image is challenging in the presence
of occlusion, which can be caused by self, objects, or other humans. Existing
methods either fail to separate human features accurately or lack proper
supervision for feature completion. In this paper, we propose Dense Inpainting
Human Mesh Recovery (DIMR), a two-stage method that leverages dense
correspondence maps to handle occlusion. Our method utilizes a dense
correspondence map to separate visible human features and completes human
features on a structured UV map dense human with an attention-based feature
completion module. We also design a feature inpainting training procedure that
guides the network to learn from unoccluded features. We evaluate our method on
several datasets and demonstrate its superior performance under heavily
occluded scenarios compared to other methods. Extensive experiments show that
our method obviously outperforms prior SOTA methods on heavily occluded images
and achieves comparable results on the standard benchmarks (3DPW).",None,-1
8e53d044-0533-4f11-98bc-91ecace9eaf6,Replicating Complex Dialogue Policy of Humans via Offline Imitation Learning with Supervised Regularization,0.0566834,"Policy learning (PL) is a module of a task-oriented dialogue system that
trains an agent to make actions in each dialogue turn. Imitating human action
is a fundamental problem of PL. However, both supervised learning (SL) and
reinforcement learning (RL) frameworks cannot imitate humans well. Training RL
models require online interactions with user simulators, while simulating
complex human policy is hard. Performances of SL-based models are restricted
because of the covariate shift problem. Specifically, a dialogue is a
sequential decision-making process where slight differences in current
utterances and actions will cause significant differences in subsequent
utterances. Therefore, the generalize ability of SL models is restricted
because statistical characteristics of training and testing dialogue data
gradually become different. This study proposed an offline imitation learning
model that learns policy from real dialogue datasets and does not require user
simulators. It also utilizes state transition information, which alleviates the
influence of the covariate shift problem. We introduced a regularization trick
to make our model can be effectively optimized. We investigated the performance
of our model on four independent public dialogue datasets. The experimental
result showed that our model performed better in the action prediction task.",None,-1
b79a7e0d-45f6-4445-8a9d-4f0641e52879,Causal Reasoning of Entities and Events in Procedural Texts,0.318552,"Entities and events are crucial to natural language reasoning and common in
procedural texts. Existing work has focused either exclusively on entity state
tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one
would burn themselves by touching the pan), while these two tasks are often
causally related. We propose CREPE, the first benchmark on causal reasoning of
event plausibility and entity states. We show that most language models,
including GPT-3, perform close to chance at .35 F1, lagging far behind human at
.87 F1. We boost model performance to .59 F1 by creatively representing events
as programming languages while prompting language models pretrained on code. By
injecting the causal relations between entities and events as intermediate
reasoning steps in our representation, we further boost the performance to .67
F1. Our findings indicate not only the challenge that CREPE brings for language
models, but also the efficacy of code-like prompting combined with
chain-of-thought prompting for multihop event reasoning.",None,-1
57c82be7-1a2f-4817-a7ce-05097f58dfe7,Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning,0.210933,"We propose Pgx, a suite of board game reinforcement learning (RL)
environments written in JAX and optimized for GPU/TPU accelerators. By
leveraging JAX's auto-vectorization and parallelization over accelerators, Pgx
can efficiently scale to thousands of simultaneous simulations over
accelerators. In our experiments on a DGX-A100 workstation, we discovered that
Pgx can simulate RL environments 10-100x faster than existing implementations
available in Python. Pgx includes RL environments commonly used as benchmarks
in RL research, such as backgammon, chess, shogi, and Go. Additionally, Pgx
offers miniature game sets and baseline models to facilitate rapid research
cycles. We demonstrate the efficient training of the Gumbel AlphaZero algorithm
with Pgx environments. Overall, Pgx provides high-performance environment
simulators for researchers to accelerate their RL experiments. Pgx is available
at http://github.com/sotetsuk/pgx.",None,-1
1052e758-8531-4ebf-999d-41c63d8e79cc,Removing RLHF Protections in GPT-4 via Fine-Tuning,0.837009,"As large language models (LLMs) have increased in their capabilities, so does
their potential for dual use. To reduce harmful outputs, produces and vendors
of LLMs have used reinforcement learning with human feedback (RLHF). In tandem,
LLM vendors have been increasingly enabling fine-tuning of their most powerful
models. However, concurrent work has shown that fine-tuning can remove RLHF
protections. We may expect that the most powerful models currently available
(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the
contrary: fine-tuning allows attackers to remove RLHF protections with as few
as 340 examples and a 95% success rate. These training examples can be
automatically generated with weaker models. We further show that removing RLHF
protections does not decrease usefulness on non-censored outputs, providing
evidence that our fine-tuning strategy does not decrease usefulness despite
using weaker models to generate training data. Our results show the need for
further research on protections on LLMs.",None,-1
11bb9573-fac6-41c5-9c4f-99014ac61653,Argumentation Element Annotation Modeling using XLNet,0.773344,"This study demonstrates the effectiveness of XLNet, a transformer-based
language model, for annotating argumentative elements in persuasive essays.
XLNet's architecture incorporates a recurrent mechanism that allows it to model
long-term dependencies in lengthy texts. Fine-tuned XLNet models were applied
to three datasets annotated with different schemes - a proprietary dataset
using the Annotations for Revisions and Reflections on Writing (ARROW) scheme,
the PERSUADE corpus, and the Argument Annotated Essays (AAE) dataset. The XLNet
models achieved strong performance across all datasets, even surpassing human
agreement levels in some cases. This shows XLNet capably handles diverse
annotation schemes and lengthy essays. Comparisons between the model outputs on
different datasets also revealed insights into the relationships between the
annotation tags. Overall, XLNet's strong performance on modeling argumentative
structures across diverse datasets highlights its suitability for providing
automated feedback on essay organization.",None,-1
1aeaa12d-7982-4763-9396-7f48ae25b1ce,ACC-UNet: A Completely Convolutional UNet model for the 2020s,0.726884,"This decade is marked by the introduction of Vision Transformer, a radical
paradigm shift in broad computer vision. A similar trend is followed in medical
imaging, UNet, one of the most influential architectures, has been redesigned
with transformers. Recently, the efficacy of convolutional models in vision is
being reinvestigated by seminal works such as ConvNext, which elevates a ResNet
to Swin Transformer level. Deriving inspiration from this, we aim to improve a
purely convolutional UNet model so that it can be on par with the
transformer-based models, e.g, Swin-Unet or UCTransNet. We examined several
advantages of the transformer-based UNet models, primarily long-range
dependencies and cross-level skip connections. We attempted to emulate them
through convolution operations and thus propose, ACC-UNet, a completely
convolutional UNet model that brings the best of both worlds, the inherent
inductive biases of convnets with the design decisions of transformers.
ACC-UNet was evaluated on 5 different medical image segmentation benchmarks and
consistently outperformed convnets, transformers, and their hybrids. Notably,
ACC-UNet outperforms state-of-the-art models Swin-Unet and UCTransNet by $2.64
\pm 2.54\%$ and $0.45 \pm 1.61\%$ in terms of dice score, respectively, while
using a fraction of their parameters ($59.26\%$ and $24.24\%$). Our codes are
available at https://github.com/kiharalab/ACC-UNet.",None,-1
11901ac9-03cb-4b75-a369-d7694bb50992,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,0.897062,"As large language models (LLMs) have become the norm in NLP, demonstrating
good performance in generation and reasoning tasks, one of its most fatal
disadvantages is the lack of factual correctness. Generating unfactual texts
not only leads to lower performances but also degrades the trust and validity
of their applications. Chain-of-Thought (CoT) prompting improves trust and
model performance on complex reasoning tasks by generating interpretable
reasoning chains, but still suffers from factuality concerns in
knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit
framework for CoT prompting, which seeks to increase prediction factuality by
post-editing reasoning chains according to external knowledge. Building on top
of GPT-3, our framework lead to accuracy improvements in multiple open-domain
question-answering tasks.",None,-1
7b87ddc7-0f64-47c7-a2bf-a6daf05013b7,Unsupervised Improvement of Factual Knowledge in Language Models,0.0131493,"Masked language modeling (MLM) plays a key role in pretraining large language
models. But the MLM objective is often dominated by high-frequency words that
are sub-optimal for learning factual knowledge. In this work, we propose an
approach for influencing MLM pretraining in a way that can improve language
model performance on a variety of knowledge-intensive tasks. We force the
language model to prioritize informative words in a fully unsupervised way.
Experiments demonstrate that the proposed approach can significantly improve
the performance of pretrained language models on tasks such as factual recall,
question answering, sentiment analysis, and natural language inference in a
closed-book setting.",None,-1
2f8be666-2de4-4b81-a120-57d4b22890cd,Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory,0.999999,"The captivating realm of Minecraft has attracted substantial research
interest in recent years, serving as a rich platform for developing intelligent
agents capable of functioning in open-world environments. However, the current
research landscape predominantly focuses on specific objectives, such as the
popular ""ObtainDiamond"" task, and has not yet shown effective generalization to
a broader spectrum of tasks. Furthermore, the current leading success rate for
the ""ObtainDiamond"" task stands at around 20%, highlighting the limitations of
Reinforcement Learning (RL) based controllers used in existing methods. To
tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel
framework integrates Large Language Models (LLMs) with text-based knowledge and
memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These
agents, equipped with the logic and common sense capabilities of LLMs, can
skillfully navigate complex, sparse-reward environments with text-based
interactions. We develop a set of structured actions and leverage LLMs to
generate action plans for the agents to execute. The resulting LLM-based agent
markedly surpasses previous methods, achieving a remarkable improvement of
+47.5% in success rate on the ""ObtainDiamond"" task, demonstrating superior
robustness compared to traditional RL-based controllers. Notably, our agent is
the first to procure all items in the Minecraft Overworld technology tree,
demonstrating its extensive capabilities. GITM does not need any GPU for
training, but a single CPU node with 32 CPU cores is enough. This research
shows the potential of LLMs in developing capable agents for handling
long-horizon, complex tasks and adapting to uncertainties in open-world
environments. See the project website at https://github.com/OpenGVLab/GITM.",None,-1
41f41089-eb69-4c1f-b712-f6fff0689e88,SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark,0.411183,"Large language models (LLMs) have shown the potential to be integrated into
human daily lives. Therefore, user preference is the most critical criterion
for assessing LLMs' performance in real-world scenarios. However, existing
benchmarks mainly focus on measuring models' accuracy using multi-choice
questions, which limits the understanding of their capabilities in real
applications. We fill this gap by proposing a comprehensive Chinese benchmark
SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE
encompasses three sub-tasks: actual users' queries and ratings derived from an
LLM battle platform (CArena), open-ended questions with single and
multiple-turn dialogues (OPEN), and closed-ended questions with the same stems
as open-ended single-turn ones (CLOSE). Our study shows that accuracy on
closed-ended questions is insufficient to reflect human preferences achieved on
open-ended ones. At the same time, they can complement each other to predict
actual user preferences. We also demonstrate that GPT-4 is a reliable judge to
automatically evaluate human preferences on open-ended questions in a Chinese
context. Our benchmark will be released at https://www.CLUEbenchmarks.com",None,-1
0d7528e5-88e9-4903-ac0f-16b329e178af,Unsupervised Cross-domain Pulmonary Nodule Detection without Source Data,0.332567,"Cross domain pulmonary nodule detection suffers from performance degradation
due to large shift of data distributions between the source and target domain.
Besides, considering the high cost of medical data annotation, it is often
assumed that the target images are unlabeled. Existing approaches have made
much progress for this unsupervised domain adaptation setting. However, this
setting is still rarely plausible in the medical application since the source
medical data are often not accessible due to the privacy concerns. This
motivates us to propose a Source-free Unsupervised cross-domain method for
Pulmonary nodule detection (SUP). It first adapts the source model to the
target domain by utilizing instance-level contrastive learning. Then the
adapted model is trained in a teacher-student interaction manner, and a
weighted entropy loss is incorporated to further improve the accuracy.
Extensive experiments by adapting a pre-trained source model to three popular
pulmonary nodule datasets demonstrate the effectiveness of our method.",None,-1
1e65779c-4cab-4014-8dcc-28ae427c084b,Collaborative Blind Image Deblurring,0.127887,"Blurry images usually exhibit similar blur at various locations across the
image domain, a property barely captured in nowadays blind deblurring neural
networks. We show that when extracting patches of similar underlying blur is
possible, jointly processing the stack of patches yields superior accuracy than
handling them separately. Our collaborative scheme is implemented in a neural
architecture with a pooling layer on the stack dimension. We present three
practical patch extraction strategies for image sharpening, camera shake
removal and optical aberration correction, and validate the proposed approach
on both synthetic and real-world benchmarks. For each blur instance, the
proposed collaborative strategy yields significant quantitative and qualitative
improvements.",None,-1
fe72b49f-7493-44b1-89a5-fcfb88d33e0a,Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation,0.946985,"News summary generation is an important task in the field of intelligence
analysis, which can provide accurate and comprehensive information to help
people better understand and respond to complex real-world events. However,
traditional news summary generation methods face some challenges, which are
limited by the model itself and the amount of training data, as well as the
influence of text noise, making it difficult to generate reliable information
accurately. In this paper, we propose a new paradigm for news summary
generation using LLM with powerful natural language understanding and
generative capabilities. We use LLM to extract multiple structured event
patterns from the events contained in news paragraphs, evolve the event pattern
population with genetic algorithm, and select the most adaptive event pattern
to input into the LLM to generate news summaries. A News Summary Generator
(NSG) is designed to select and evolve the event pattern populations and
generate news summaries. The experimental results show that the news summary
generator is able to generate accurate and reliable news summaries with some
generalization ability.",None,-1
380d0ef5-9959-47b0-8dc5-38725a6e3982,Transmission-Guided Bayesian Generative Model for Smoke Segmentation,0.209737,"Smoke segmentation is essential to precisely localize wildfire so that it can
be extinguished in an early phase. Although deep neural networks have achieved
promising results on image segmentation tasks, they are prone to be
overconfident for smoke segmentation due to its non-rigid shape and transparent
appearance. This is caused by both knowledge level uncertainty due to limited
training data for accurate smoke segmentation and labeling level uncertainty
representing the difficulty in labeling ground-truth. To effectively model the
two types of uncertainty, we introduce a Bayesian generative model to
simultaneously estimate the posterior distribution of model parameters and its
predictions. Further, smoke images suffer from low contrast and ambiguity,
inspired by physics-based image dehazing methods, we design a
transmission-guided local coherence loss to guide the network to learn
pair-wise relationships based on pixel distance and the transmission feature.
To promote the development of this field, we also contribute a high-quality
smoke segmentation dataset, SMOKE5K, consisting of 1,400 real and 4,000
synthetic images with pixel-wise annotation. Experimental results on benchmark
testing datasets illustrate that our model achieves both accurate predictions
and reliable uncertainty maps representing model ignorance about its
prediction. Our code and dataset are publicly available at:
https://github.com/redlessme/Transmission-BVM.",None,-1
f6acfe0b-4596-40a1-aa0b-038c4a7856ef,Multimodal Chain-of-Thought Reasoning in Language Models,0.964243,"Large language models (LLMs) have shown impressive performance on complex
reasoning by leveraging chain-of-thought (CoT) prompting to generate
intermediate reasoning chains as the rationale to infer the answer. However,
existing CoT studies have primarily focused on the language modality. We
propose Multimodal-CoT that incorporates language (text) and vision (images)
modalities into a two-stage framework that separates rationale generation and
answer inference. In this way, answer inference can leverage better generated
rationales that are based on multimodal information. Experimental results on
ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed
approach. With Multimodal-CoT, our model under 1 billion parameters achieves
state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates
that Multimodal-CoT offers the advantages of mitigating hallucination and
enhancing convergence speed. Code is publicly available at
https://github.com/amazon-science/mm-cot.",None,-1
7d3dcc83-f134-4095-8083-de722a581753,HeGeL: A Novel Dataset for Geo-Location from Hebrew Text,0.620955,"The task of textual geolocation - retrieving the coordinates of a place based
on a free-form language description - calls for not only grounding but also
natural language understanding and geospatial reasoning. Even though there are
quite a few datasets in English used for geolocation, they are currently based
on open-source data (Wikipedia and Twitter), where the location of the
described place is mostly implicit, such that the location retrieval resolution
is limited. Furthermore, there are no datasets available for addressing the
problem of textual geolocation in morphologically rich and resource-poor
languages, such as Hebrew. In this paper, we present the Hebrew Geo-Location
(HeGeL) corpus, designed to collect literal place descriptions and analyze
lingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place
descriptions of various place types in three cities in Israel. Qualitative and
empirical analysis show that the data exhibits abundant use of geospatial
reasoning and requires a novel environmental representation.",None,-1
58246cc4-a523-4a6d-8bd6-5b84a386cc89,Chain of Thought Prompting Elicits Knowledge Augmentation,0.754477,"The knowledge-augmented deep learning paradigm refers to a paradigm in which
domain knowledge is identified and integrated into deep models. Conventional
methods typically employ task-specific approaches to gather external knowledge
from various sources. In contrast, large language models are extensively
pre-trained and can serve as a comprehensive source of external knowledge. In
this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments
knowledge for deep learning. CoT-KA avoids the need for additional knowledge
retrieval or knowledge reasoning models, as required in conventional
augmentation methods. Our results demonstrate that CoT-KA outperforms both pure
CoT-based methods and the non-augmented method across the majority of eleven
publicly available benchmarks for various reasoning tasks.",None,-1
2eb31c42-c179-446c-bf77-2d388058da7f,History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System,0.74007,"With the evolution of pre-trained language models, current open-domain
dialogue systems have achieved great progress in conducting one-session
conversations. In contrast, Multi-Session Conversation (MSC), which consists of
multiple sessions over a long term with the same user, is under-investigated.
In this paper, we propose History-Aware Hierarchical Transformer (HAHT) for
multi-session open-domain dialogue. HAHT maintains a long-term memory of
history conversations and utilizes history information to understand current
conversation context and generate well-informed and context-relevant responses.
Specifically, HAHT first encodes history conversation sessions hierarchically
into a history memory. Then, HAHT leverages historical information to
facilitate the understanding of the current conversation context by encoding
the history memory together with the current context with attention-based
mechanisms. Finally, to explicitly utilize historical information, HAHT uses a
history-aware response generator that switches between a generic vocabulary and
a history-aware vocabulary. Experimental results on a large-scale MSC dataset
suggest that the proposed HAHT model consistently outperforms baseline models.
Human evaluation results support that HAHT generates more human-like,
context-relevant and history-relevant responses than baseline models.",None,-1
c215ba21-c892-4c22-8414-d9470153826a,Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines,0.613259,"Polarization and the marketplace for impressions have conspired to make
navigating information online difficult for users, and while there has been a
significant effort to detect false or misleading text, multimodal datasets have
received considerably less attention. To complement existing resources, we
present multimodal Video Misleading Headline (VMH), a dataset that consists of
videos and whether annotators believe the headline is representative of the
video's contents. After collecting and annotating this dataset, we analyze
multimodal baselines for detecting misleading headlines. Our annotation process
also focuses on why annotators view a video as misleading, allowing us to
better understand the interplay of annotators' background and the content of
the videos.",None,-1
47c0be0b-d5ce-4152-b6be-94eaf7d16c6c,Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,0.212567,"Regular group convolutional neural networks (G-CNNs) have been shown to
increase model performance and improve equivariance to different geometrical
symmetries. This work addresses the problem of SE(3), i.e., roto-translation
equivariance, on volumetric data. Volumetric image data is prevalent in many
medical settings. Motivated by the recent work on separable group convolutions,
we devise a SE(3) group convolution kernel separated into a continuous SO(3)
(rotation) kernel and a spatial kernel. We approximate equivariance to the
continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel
is parameterized via RBF interpolation on similarly uniform grids. We
demonstrate the advantages of our approach in volumetric medical image
analysis. Our SE(3) equivariant models consistently outperform CNNs and regular
discrete G-CNNs on challenging medical classification tasks and show
significantly improved generalization capabilities. Our approach achieves up to
a 16.5% gain in accuracy over regular CNNs.",None,-1
e3718841-4d72-47ad-b183-39430aea0956,AI-assisted coding: Experiments with GPT-4,0.715421,"Artificial intelligence (AI) tools based on large language models have
acheived human-level performance on some computer programming tasks. We report
several experiments using GPT-4 to generate computer code. These experiments
demonstrate that AI code generation using the current generation of tools,
while powerful, requires substantial human validation to ensure accurate
performance. We also demonstrate that GPT-4 refactoring of existing code can
significantly improve that code along several established metrics for code
quality, and we show that GPT-4 can generate tests with substantial coverage,
but that many of the tests fail when applied to the associated code. These
findings suggest that while AI coding tools are very powerful, they still
require humans in the loop to ensure validity and accuracy of the results.",None,-1
46a1809a-905c-49e5-bae0-fa1ef052bfa8,Efficient GPT Model Pre-training using Tensor Train Matrix Representation,0.00689648,"Large-scale transformer models have shown remarkable performance in language
modelling tasks. However, such models feature billions of parameters, leading
to difficulties in their deployment and prohibitive training costs from
scratch. To reduce the number of the parameters in the GPT-2 architecture, we
replace the matrices of fully-connected layers with the corresponding Tensor
Train Matrix~(TTM) structure. Finally, we customize forward and backward
operations through the TTM-based layer for simplicity and the stableness of
further training. % The resulting GPT-2-based model stores up to 40% fewer
parameters, showing the perplexity comparable to the original model. On the
downstream tasks, including language understanding and text summarization, the
model performs similarly to the original GPT-2 model. The proposed tensorized
layers could be used to efficiently pre-training other Transformer models.",None,-1
c308d234-b96e-465a-9cdb-60e78191350f,Large AI Model-Based Semantic Communications,0.962695,"Semantic communication (SC) is an emerging intelligent paradigm, offering
solutions for various future applications like metaverse, mixed-reality, and
the Internet of everything. However, in current SC systems, the construction of
the knowledge base (KB) faces several issues, including limited knowledge
representation, frequent knowledge updates, and insecure knowledge sharing.
Fortunately, the development of the large AI model provides new solutions to
overcome above issues. Here, we propose a large AI model-based SC framework
(LAM-SC) specifically designed for image data, where we first design the
segment anything model (SAM)-based KB (SKB) that can split the original image
into different semantic segments by universal semantic knowledge. Then, we
present an attention-based semantic integration (ASI) to weigh the semantic
segments generated by SKB without human participation and integrate them as the
semantic-aware image. Additionally, we propose an adaptive semantic compression
(ASC) encoding to remove redundant information in semantic features, thereby
reducing communication overhead. Finally, through simulations, we demonstrate
the effectiveness of the LAM-SC framework and the significance of the large AI
model-based KB development in future SC paradigms.",None,-1
628c55b0-ef3a-453d-91a9-49cf2d38d8ce,Multi-modal Hate Speech Detection using Machine Learning,0.409784,"With the continuous growth of internet users and media content, it is very
hard to track down hateful speech in audio and video. Converting video or audio
into text does not detect hate speech accurately as human sometimes uses
hateful words as humorous or pleasant in sense and also uses different voice
tones or show different action in the video. The state-ofthe-art hate speech
detection models were mostly developed on a single modality. In this research,
a combined approach of multimodal system has been proposed to detect hate
speech from video contents by extracting feature images, feature values
extracted from the audio, text and used machine learning and Natural language
processing.",None,-1
871eb78a-ed8b-4e10-a732-7ce4548f8241,Language Model Tokenizers Introduce Unfairness Between Languages,0.718888,"Recent language models have shown impressive multilingual performance, even
when not explicitly trained for it. Despite this, there are concerns about the
quality of their outputs across different languages. In this paper, we show how
disparity in the treatment of different languages arises at the tokenization
stage, well before a model is even invoked. The same text translated into
different languages can have drastically different tokenization lengths, with
differences up to 15 times in some cases. These disparities persist even for
tokenizers that are intentionally trained for multilingual support.
Character-level and byte-level models also exhibit over 4 times the difference
in the encoding length for some language pairs. This induces unfair treatment
for some language communities in regard to the cost of accessing commercial
language services, the processing time and latency, as well as the amount of
content that can be provided as context to the models. Therefore, we make the
case that we should train future language models using multilingually fair
subword tokenizers.",None,-1
b4d4f21f-643a-4cd5-a076-56145c225a67,A Meta-heuristic Approach to Estimate and Explain Classifier Uncertainty,0.0685311,"Trust is a crucial factor affecting the adoption of machine learning (ML)
models. Qualitative studies have revealed that end-users, particularly in the
medical domain, need models that can express their uncertainty in
decision-making allowing users to know when to ignore the model's
recommendations. However, existing approaches for quantifying decision-making
uncertainty are not model-agnostic, or they rely on complex statistical
derivations that are not easily understood by laypersons or end-users, making
them less useful for explaining the model's decision-making process. This work
proposes a set of class-independent meta-heuristics that can characterize the
complexity of an instance in terms of factors are mutually relevant to both
human and ML decision-making. The measures are integrated into a meta-learning
framework that estimates the risk of misclassification. The proposed framework
outperformed predicted probabilities in identifying instances at risk of being
misclassified. The proposed measures and framework hold promise for improving
model development for more complex instances, as well as providing a new means
of model abstention and explanation.",None,-1
82d3c84f-5c90-4349-84c4-b00d74ee512e,Multi-Dimensional Evaluation of Text Summarization with In-Context Learning,0.842946,"Evaluation of natural language generation (NLG) is complex and
multi-dimensional. Generated text can be evaluated for fluency, coherence,
factuality, or any other dimensions of interest. Most frameworks that perform
such multi-dimensional evaluation require training on large manually or
synthetically generated datasets. In this paper, we study the efficacy of large
language models as multi-dimensional evaluators using in-context learning,
obviating the need for large training datasets. Our experiments show that
in-context learning-based evaluators are competitive with learned evaluation
frameworks for the task of text summarization, establishing state-of-the-art on
dimensions such as relevance and factual consistency. We then analyze the
effects of factors such as the selection and number of in-context examples on
performance. Finally, we study the efficacy of in-context learning based
evaluators in evaluating zero-shot summaries written by large language models
such as GPT-3.",None,-1
0efa094b-238f-4eef-ac08-b8834f0c95a4,Gemini Pro Defeated by GPT-4V: Evidence from Education,0.893935,"This study compared the classification performance of Gemini Pro and GPT-4V
in educational settings. Employing visual question answering (VQA) techniques,
the study examined both models' abilities to read text-based rubrics and then
automatically score student-drawn models in science education. We employed both
quantitative and qualitative analyses using a dataset derived from
student-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics
for Image Feedback) prompting methods. The findings reveal that GPT-4V
significantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic
Weighted Kappa. The qualitative analysis reveals that the differences may be
due to the models' ability to process fine-grained texts in images and overall
image classification performance. Even adapting the NERIF approach by further
de-sizing the input images, Gemini Pro seems not able to perform as well as
GPT-4V. The findings suggest GPT-4V's superior capability in handling complex
multimodal educational tasks. The study concludes that while both models
represent advancements in AI, GPT-4V's higher performance makes it a more
suitable tool for educational applications involving multimodal data
interpretation.",None,-1
85c3ae6f-e70e-4de0-8521-5e4c7e4bc413,Likelihood-Based Generative Radiance Field with Latent Space Energy-Based Model for 3D-Aware Disentangled Image Representation,0.216564,"We propose the NeRF-LEBM, a likelihood-based top-down 3D-aware 2D image
generative model that incorporates 3D representation via Neural Radiance Fields
(NeRF) and 2D imaging process via differentiable volume rendering. The model
represents an image as a rendering process from 3D object to 2D image and is
conditioned on some latent variables that account for object characteristics
and are assumed to follow informative trainable energy-based prior models. We
propose two likelihood-based learning frameworks to train the NeRF-LEBM: (i)
maximum likelihood estimation with Markov chain Monte Carlo-based inference and
(ii) variational inference with the reparameterization trick. We study our
models in the scenarios with both known and unknown camera poses. Experiments
on several benchmark datasets demonstrate that the NeRF-LEBM can infer 3D
object structures from 2D images, generate 2D images with novel views and
objects, learn from incomplete 2D images, and learn from 2D images with known
or unknown camera poses.",None,-1
6d66abce-911e-46f1-990d-17a47e61900b,Learning Coordination Policies over Heterogeneous Graphs for Human-Robot Teams via Recurrent Neural Schedule Propagation,0.32682,"As human-robot collaboration increases in the workforce, it becomes essential
for human-robot teams to coordinate efficiently and intuitively. Traditional
approaches for human-robot scheduling either utilize exact methods that are
intractable for large-scale problems and struggle to account for stochastic,
time varying human task performance, or application-specific heuristics that
require expert domain knowledge to develop. We propose a deep learning-based
framework, called HybridNet, combining a heterogeneous graph-based encoder with
a recurrent schedule propagator for scheduling stochastic human-robot teams
under upper- and lower-bound temporal constraints. The HybridNet's encoder
leverages Heterogeneous Graph Attention Networks to model the initial
environment and team dynamics while accounting for the constraints. By
formulating task scheduling as a sequential decision-making process, the
HybridNet's recurrent neural schedule propagator leverages Long Short-Term
Memory (LSTM) models to propagate forward consequences of actions to carry out
fast schedule generation, removing the need to interact with the environment
between every task-agent pair selection. The resulting scheduling policy
network provides a computationally lightweight yet highly expressive model that
is end-to-end trainable via Reinforcement Learning algorithms. We develop a
virtual task scheduling environment for mixed human-robot teams in a
multi-round setting, capable of modeling the stochastic learning behaviors of
human workers. Experimental results showed that HybridNet outperformed other
human-robot scheduling solutions across problem sizes for both deterministic
and stochastic human performance, with faster runtime compared to
pure-GNN-based schedulers.",None,-1
8b2979a2-d946-4a24-811d-2e3d39909764,Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses,0.917285,"In the field of Japanese-Chinese translation linguistics, the issue of
correctly translating attributive clauses has persistently proven to be
challenging. Present-day machine translation tools often fail to accurately
translate attributive clauses from Japanese to Chinese. In light of this, this
paper investigates the linguistic problem underlying such difficulties, namely
how does the semantic role of the modified noun affect the selection of
translation patterns for attributive clauses, from a linguistic perspective. To
ad-dress these difficulties, a pre-edit scheme is proposed, which aims to
enhance the accuracy of translation. Furthermore, we propose a novel two-step
prompt strategy, which combines this pre-edit scheme with ChatGPT, currently
the most widely used large language model. This prompt strategy is capable of
optimizing translation input in zero-shot scenarios and has been demonstrated
to improve the average translation accuracy score by over 35%.",None,-1
e9041a98-9cb1-478f-b4f4-563365b19690,Dual-level Interaction for Domain Adaptive Semantic Segmentation,0.146989,"Self-training approach recently secures its position in domain adaptive
semantic segmentation, where a model is trained with target domain
pseudo-labels. Current advances have mitigated noisy pseudo-labels resulting
from the domain gap. However, they still struggle with erroneous pseudo-labels
near the boundaries of the semantic classifier. In this paper, we tackle this
issue by proposing a dual-level interaction for domain adaptation (DIDA) in
semantic segmentation. Explicitly, we encourage the different augmented views
of the same pixel to have not only similar class prediction (semantic-level)
but also akin similarity relationship with respect to other pixels
(instance-level). As it's impossible to keep features of all pixel instances
for a dataset, we, therefore, maintain a labeled instance bank with dynamic
updating strategies to selectively store the informative features of instances.
Further, DIDA performs cross-level interaction with scattering and gathering
techniques to regenerate more reliable pseudo-labels. Our method outperforms
the state-of-the-art by a notable margin, especially on confusing and
long-tailed classes. Code is available at
\href{https://github.com/RainJamesY/DIDA}",None,-1
f9306627-fd76-4f00-9b41-882f621955d2,Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models,0.38972,"While language models (LMs) have shown potential across a range of
decision-making tasks, their reliance on simple acting processes limits their
broad deployment as autonomous agents. In this paper, we introduce Language
Agent Tree Search (LATS) -- the first general framework that synergizes the
capabilities of LMs in reasoning, acting, and planning. By leveraging the
in-context learning ability of LMs, we integrate Monte Carlo Tree Search into
LATS to enable LMs as agents, along with LM-powered value functions and
self-reflections for proficient exploration and enhanced decision-making. A key
feature of our approach is the incorporation of an environment for external
feedback, which offers a more deliberate and adaptive problem-solving mechanism
that surpasses the constraints of existing techniques. Our experimental
evaluation across diverse domains, including programming, interactive
question-answering (QA), web navigation, and math, validates the effectiveness
and generality of LATS in decision-making while maintaining competitive or
improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1
accuracy (92.7%) for programming on HumanEval with GPT-4 and demonstrates
gradient-free performance (average score of 75.9) comparable to gradient-based
fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at
https://github.com/lapisrocks/LanguageAgentTreeSearch",None,-1
4eac969a-7ab8-4ff7-9bd9-d0845eb80c2b,SeMAnD: Self-Supervised Anomaly Detection in Multimodal Geospatial Datasets,0.132646,"We propose a Self-supervised Anomaly Detection technique, called SeMAnD, to
detect geometric anomalies in Multimodal geospatial datasets. Geospatial data
comprises of acquired and derived heterogeneous data modalities that we
transform to semantically meaningful, image-like tensors to address the
challenges of representation, alignment, and fusion of multimodal data. SeMAnD
is comprised of (i) a simple data augmentation strategy, called
RandPolyAugment, capable of generating diverse augmentations of vector
geometries, and (ii) a self-supervised training objective with three components
that incentivize learning representations of multimodal data that are
discriminative to local changes in one modality which are not corroborated by
the other modalities. Detecting local defects is crucial for geospatial anomaly
detection where even small anomalies (e.g., shifted, incorrectly connected,
malformed, or missing polygonal vector geometries like roads, buildings,
landcover, etc.) are detrimental to the experience and safety of users of
geospatial applications like mapping, routing, search, and recommendation
systems. Our empirical study on test sets of different types of real-world
geometric geospatial anomalies across 3 diverse geographical regions
demonstrates that SeMAnD is able to detect real-world defects and outperforms
domain-agnostic anomaly detection strategies by 4.8-19.7% as measured using
anomaly classification AUC. We also show that model performance increases (i)
up to 20.4% as the number of input modalities increase and (ii) up to 22.9% as
the diversity and strength of training data augmentations increase.",None,-1
ef183206-51a9-4292-99f3-c690fa2fde19,"Vision, Deduction and Alignment: An Empirical Study on Multi-modal Knowledge Graph Alignment",0.758037,"Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in
knowledge engineering. Existing EA methods mostly focus on utilizing the graph
structures and entity attributes (including literals), but ignore images that
are common in modern multi-modal KGs. In this study we first constructed
Multi-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then
evaluated some existing embedding-based methods for utilizing images. In view
of the complementary nature of visual modal information and logical deduction,
we further developed a new multi-modal EA method named LODEME using logical
deduction and multi-modal KG embedding, with state-of-the-art performance
achieved on Multi-OpenEA and other existing multi-modal EA benchmarks.",None,-1
1f88ea5e-1359-459d-9398-390226afc49a,ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition,0.538763,"Class imbalance is a common challenge in real-world recognition tasks, where
the majority of classes have few samples, also known as tail classes. We
address this challenge with the perspective of generalization and empirically
find that the promising Sharpness-Aware Minimization (SAM) fails to address
generalization issues under the class-imbalanced setting. Through investigating
this specific type of task, we identify that its generalization bottleneck
primarily lies in the severe overfitting for tail classes with limited training
data. To overcome this bottleneck, we leverage class priors to restrict the
generalization scope of the class-agnostic SAM and propose a class-aware
smoothness optimization algorithm named Imbalanced-SAM (ImbSAM). With the
guidance of class priors, our ImbSAM specifically improves generalization
targeting tail classes. We also verify the efficacy of ImbSAM on two
prototypical applications of class-imbalanced recognition: long-tailed
classification and semi-supervised anomaly detection, where our ImbSAM
demonstrates remarkable performance improvements for tail classes and anomaly.
Our code implementation is available at
https://github.com/cool-xuan/Imbalanced_SAM.",None,-1
d7c1f0f8-569d-4b6a-b5e6-a754cdb50fe4,TabLib: A Dataset of 627M Tables with Context,0.607764,"It is well-established that large, diverse datasets play a pivotal role in
the performance of modern AI systems for text and image modalities. However,
there are no datasets for tabular data of comparable size and diversity to
those available for text and images. Thus we present ""TabLib'', a compilation
of 627 million tables totaling 69 TiB, along with 867B tokens of context.
TabLib was extracted from numerous file formats, including CSV, HTML, SQLite,
PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and
diversity of TabLib offer considerable promise in the table modality,
reminiscent of the original promise of foundational datasets for text and
images, such as The Pile and LAION.",None,-1
ed525ae0-63d8-40c7-8266-1835657ce18e,SE-ORNet: Self-Ensembling Orientation-aware Network for Unsupervised Point Cloud Shape Correspondence,0.662247,"Unsupervised point cloud shape correspondence aims to obtain dense
point-to-point correspondences between point clouds without manually annotated
pairs. However, humans and some animals have bilateral symmetry and various
orientations, which lead to severe mispredictions of symmetrical parts.
Besides, point cloud noise disrupts consistent representations for point cloud
and thus degrades the shape correspondence accuracy. To address the above
issues, we propose a Self-Ensembling ORientation-aware Network termed SE-ORNet.
The key of our approach is to exploit an orientation estimation module with a
domain adaptive discriminator to align the orientations of point cloud pairs,
which significantly alleviates the mispredictions of symmetrical parts.
Additionally, we design a selfensembling framework for unsupervised point cloud
shape correspondence. In this framework, the disturbances of point cloud noise
are overcome by perturbing the inputs of the student and teacher networks with
different data augmentations and constraining the consistency of predictions.
Extensive experiments on both human and animal datasets show that our SE-ORNet
can surpass state-of-the-art unsupervised point cloud shape correspondence
methods.",None,-1
8e6086be-606e-4c25-b7ff-4481f4d933ed,"Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks",0.659174,"We explore the abstract reasoning abilities of text-only and multimodal
versions of GPT-4, using the ConceptARC benchmark [10], which is designed to
evaluate robust understanding and reasoning with core-knowledge concepts. We
extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,
one-shot prompting (rather than simple, zero-shot prompts) with text versions
of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,
on zero- and one-shot prompts using image versions of the simplest tasks. Our
experimental results support the conclusion that neither version of GPT-4 has
developed robust abstraction abilities at humanlike levels.",None,-1
6bb6c44f-2657-4ab5-aea2-b0d11087f41c,MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning,0.673595,"While large language models (LLMs) equipped with techniques like
chain-of-thought prompting have demonstrated impressive capabilities, they
still fall short in their ability to reason robustly in complex settings.
However, evaluating LLM reasoning is challenging because system capabilities
continue to grow while benchmark datasets for tasks like logical deduction have
remained static. We introduce MuSR, a dataset for evaluating language models on
multistep soft reasoning tasks specified in a natural language narrative. This
dataset has two crucial features. First, it is created through a novel
neurosymbolic synthetic-to-natural generation algorithm, enabling the
construction of complex reasoning instances that challenge GPT-4 (e.g., murder
mysteries roughly 1000 words in length) and which can be scaled further as more
capable LLMs are released. Second, our dataset instances are free text
narratives corresponding to real-world domains of reasoning; this makes it
simultaneously much more challenging than other synthetically-crafted
benchmarks while remaining realistic and tractable for human annotators to
solve with high accuracy. We evaluate a range of LLMs and prompting techniques
on this dataset and characterize the gaps that remain for techniques like
chain-of-thought to perform robust reasoning.",None,-1
e8045b65-1bb6-433e-8f86-16b1921e7da8,Enhanced Multimodal Representation Learning with Cross-modal KD,0.242593,"This paper explores the tasks of leveraging auxiliary modalities which are
only available at training to enhance multimodal representation learning
through cross-modal Knowledge Distillation (KD). The widely adopted mutual
information maximization-based objective leads to a short-cut solution of the
weak teacher, i.e., achieving the maximum mutual information by simply making
the teacher model as weak as the student model. To prevent such a weak
solution, we introduce an additional objective term, i.e., the mutual
information between the teacher and the auxiliary modality model. Besides, to
narrow down the information gap between the student and teacher, we further
propose to minimize the conditional entropy of the teacher given the student.
Novel training schemes based on contrastive learning and adversarial learning
are designed to optimize the mutual information and the conditional entropy,
respectively. Experimental results on three popular multimodal benchmark
datasets have shown that the proposed method outperforms a range of
state-of-the-art approaches for video recognition, video retrieval and emotion
classification.",None,-1
d2f76edf-38ac-4bdd-86b7-8e16cf31b627,A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram,0.395877,"Geometry problem solving (GPS) is a high-level mathematical reasoning
requiring the capacities of multi-modal fusion and geometric knowledge
application. Recently, neural solvers have shown great potential in GPS but
still be short in diagram presentation and modal fusion. In this work, we
convert diagrams into basic textual clauses to describe diagram features
effectively, and propose a new neural solver called PGPSNet to fuse multi-modal
information efficiently. Combining structural and semantic pre-training, data
augmentation and self-limited decoding, PGPSNet is endowed with rich knowledge
of geometry theorems and geometric representation, and therefore promotes
geometric understanding and reasoning. In addition, to facilitate the research
of GPS, we build a new large-scale and fine-annotated GPS dataset named PGPS9K,
labeled with both fine-grained diagram annotation and interpretable solution
program. Experiments on PGPS9K and an existing dataset Geometry3K validate the
superiority of our method over the state-of-the-art neural solvers. Our code,
dataset and appendix material are available at
\url{https://github.com/mingliangzhang2018/PGPS}.",None,-1
53cea965-3fa4-4940-b824-83d133463aab,Embedding-based Retrieval with LLM for Effective Agriculture Information Extracting from Unstructured Data,0.99998,"Pest identification is a crucial aspect of pest control in agriculture.
However, most farmers are not capable of accurately identifying pests in the
field, and there is a limited number of structured data sources available for
rapid querying. In this work, we explored using domain-agnostic general
pre-trained large language model(LLM) to extract structured data from
agricultural documents with minimal or no human intervention. We propose a
methodology that involves text retrieval and filtering using embedding-based
retrieval, followed by LLM question-answering to automatically extract entities
and attributes from the documents, and transform them into structured data. In
comparison to existing methods, our approach achieves consistently better
accuracy in the benchmark while maintaining efficiency.",None,-1
0706c55d-d31e-44ff-94ec-f91b847907ea,Generative Knowledge Selection for Knowledge-Grounded Dialogues,0.679619,"Knowledge selection is the key in knowledge-grounded dialogues (KGD), which
aims to select an appropriate knowledge snippet to be used in the utterance
based on dialogue history. Previous studies mainly employ the classification
approach to classify each candidate snippet as ""relevant"" or ""irrelevant""
independently. However, such approaches neglect the interactions between
snippets, leading to difficulties in inferring the meaning of snippets.
Moreover, they lack modeling of the discourse structure of dialogue-knowledge
interactions. We propose a simple yet effective generative approach for
knowledge selection, called GenKS. GenKS learns to select snippets by
generating their identifiers with a sequence-to-sequence model. GenKS therefore
captures intra-knowledge interaction inherently through attention mechanisms.
Meanwhile, we devise a hyperlink mechanism to model the dialogue-knowledge
interactions explicitly. We conduct experiments on three benchmark datasets,
and verify GenKS achieves the best results on both knowledge selection and
response generation.",None,-1
ffa194c8-7c3b-4962-b820-e8ef4ba2f662,Synthcity: facilitating innovative use cases of synthetic data in different data modalities,0.796039,"Synthcity is an open-source software package for innovative use cases of
synthetic data in ML fairness, privacy and augmentation across diverse tabular
data modalities, including static data, regular and irregular time series, data
with censoring, multi-source data, composite data, and more. Synthcity provides
the practitioners with a single access point to cutting edge research and tools
in synthetic data. It also offers the community a playground for rapid
experimentation and prototyping, a one-stop-shop for SOTA benchmarks, and an
opportunity for extending research impact. The library can be accessed on
GitHub (https://github.com/vanderschaarlab/synthcity) and pip
(https://pypi.org/project/synthcity/). We warmly invite the community to join
the development effort by providing feedback, reporting bugs, and contributing
code.",None,-1
7823a1f3-58fd-4e61-9ca0-ba3f59f72e17,Exploring XAI for the Arts: Explaining Latent Space in Generative Music,0.505282,"Explainable AI has the potential to support more interactive and fluid
co-creative AI systems which can creatively collaborate with people. To do
this, creative AI models need to be amenable to debugging by offering
eXplainable AI (XAI) features which are inspectable, understandable, and
modifiable. However, currently there is very little XAI for the arts. In this
work, we demonstrate how a latent variable model for music generation can be
made more explainable; specifically we extend MeasureVAE which generates
measures of music. We increase the explainability of the model by: i) using
latent space regularisation to force some specific dimensions of the latent
space to map to meaningful musical attributes, ii) providing a user interface
feedback loop to allow people to adjust dimensions of the latent space and
observe the results of these changes in real-time, iii) providing a
visualisation of the musical attributes in the latent space to help people
understand and predict the effect of changes to latent space dimensions. We
suggest that in doing so we bridge the gap between the latent space and the
generated musical outcomes in a meaningful way which makes the model and its
outputs more explainable and more debuggable.",None,-1
dbd84ac9-cd52-4b0b-8fde-32483b90863d,Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation,0.993873,"One critical challenge in 6D object pose estimation from a single RGBD image
is efficient integration of two different modalities, i.e., color and depth. In
this work, we tackle this problem by a novel Deep Fusion Transformer~(DFTr)
block that can aggregate cross-modality features for improving pose estimation.
Unlike existing fusion methods, the proposed DFTr can better model
cross-modality semantic correlation by leveraging their semantic similarity,
such that globally enhanced features from different modalities can be better
integrated for improved information extraction. Moreover, to further improve
robustness and efficiency, we introduce a novel weighted vector-wise voting
algorithm that employs a non-iterative global optimization strategy for precise
3D keypoint localization while achieving near real-time inference. Extensive
experiments show the effectiveness and strong generalization capability of our
proposed 3D keypoint voting algorithm. Results on four widely used benchmarks
also demonstrate that our method outperforms the state-of-the-art methods by
large margins.",None,-1
99011642-bab9-47ab-a743-eaa9ba92ce89,Context-Aware Transformer for 3D Point Cloud Automatic Annotation,0.434075,"3D automatic annotation has received increased attention since manually
annotating 3D point clouds is laborious. However, existing methods are usually
complicated, e.g., pipelined training for 3D foreground/background
segmentation, cylindrical object proposals, and point completion. Furthermore,
they often overlook the inter-object feature relation that is particularly
informative to hard samples for 3D annotation. To this end, we propose a simple
yet effective end-to-end Context-Aware Transformer (CAT) as an automated 3D-box
labeler to generate precise 3D box annotations from 2D boxes, trained with a
small number of human annotations. We adopt the general encoder-decoder
architecture, where the CAT encoder consists of an intra-object encoder (local)
and an inter-object encoder (global), performing self-attention along the
sequence and batch dimensions, respectively. The former models intra-object
interactions among points, and the latter extracts feature relations among
different objects, thus boosting scene-level understanding. Via local and
global encoders, CAT can generate high-quality 3D box annotations with a
streamlined workflow, allowing it to outperform existing state-of-the-art by up
to 1.79% 3D AP on the hard task of the KITTI test set.",None,-1
75ec088b-a936-4299-a64c-af207d1e28a1,CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations,0.910751,"Geo-tagged images are publicly available in large quantities, whereas labels
such as object classes are rather scarce and expensive to collect. Meanwhile,
contrastive learning has achieved tremendous success in various natural image
and language tasks with limited labeled data. However, existing methods fail to
fully leverage geospatial information, which can be paramount to distinguishing
objects that are visually similar. To directly leverage the abundant geospatial
information associated with images in pre-training, fine-tuning, and inference
stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised
learning framework for geo-tagged images. We use a dual-encoder to separately
encode the images and their corresponding geo-locations, and use contrastive
objectives to learn effective location representations from images, which can
be transferred to downstream supervised tasks such as image classification.
Experiments show that CSP can improve model performance on both iNat2018 and
fMoW datasets. Especially, on iNat2018, CSP significantly boosts the model
performance with 10-34% relative improvement with various labeled training data
sampling ratios.",None,-1
9bfee463-863b-4c08-866d-b0a325502a55,Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF,0.661089,"Deep neural networks (DNNs) have been proven extremely susceptible to
adversarial examples, which raises special safety-critical concerns for
DNN-based autonomous driving stacks (i.e., 3D object detection). Although there
are extensive works on image-level attacks, most are restricted to 2D pixel
spaces, and such attacks are not always physically realistic in our 3D world.
Here we present Adv3D, the first exploration of modeling adversarial examples
as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic
appearances and 3D accurate generation, yielding a more realistic and
realizable adversarial example. We train our adversarial NeRF by minimizing the
surrounding objects' confidence predicted by 3D detectors on the training set.
Then we evaluate Adv3D on the unseen validation set and show that it can cause
a large performance reduction when rendering NeRF in any sampled pose. To
generate physically realizable adversarial examples, we propose primitive-aware
sampling and semantic-guided regularization that enable 3D patch attacks with
camouflage adversarial texture. Experimental results demonstrate that the
trained adversarial NeRF generalizes well to different poses, scenes, and 3D
detectors. Finally, we provide a defense method to our attacks that involves
adversarial training through data augmentation. Project page:
https://len-li.github.io/adv3d-web",None,-1
257dffe1-7a21-40a0-9431-995bc5fd6b37,"Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras",0.633467,"Semantic segmentation plays a vital role in computer vision tasks, enabling
precise pixel-level understanding of images. In this paper, we present a
comprehensive library for semantic segmentation, which contains implementations
of popular segmentation models like SegNet, FCN, UNet, and PSPNet. We also
evaluate and compare these models on several datasets, offering researchers and
practitioners a powerful toolset for tackling diverse segmentation challenges.",None,-1
74158941-9df7-469d-8ee3-c380fd24449c,Fairness in Visual Clustering: A Novel Transformer Clustering Approach,0.854114,"Promoting fairness for deep clustering models in unsupervised clustering
settings to reduce demographic bias is a challenging goal. This is because of
the limitation of large-scale balanced data with well-annotated labels for
sensitive or protected attributes. In this paper, we first evaluate demographic
bias in deep clustering models from the perspective of cluster purity, which is
measured by the ratio of positive samples within a cluster to their correlation
degree. This measurement is adopted as an indication of demographic bias. Then,
a novel loss function is introduced to encourage a purity consistency for all
clusters to maintain the fairness aspect of the learned clustering model.
Moreover, we present a novel attention mechanism, Cross-attention, to measure
correlations between multiple clusters, strengthening faraway positive samples
and improving the purity of clusters during the learning process. Experimental
results on a large-scale dataset with numerous attribute settings have
demonstrated the effectiveness of the proposed approach on both clustering
accuracy and fairness enhancement on several sensitive attributes.",None,-1
b804cb98-3167-437e-8f1d-49dfa083e7b1,Large Language Models as Topological Structure Enhancers for Text-Attributed Graphs,0.607936,"The latest advancements in large language models (LLMs) have revolutionized
the field of natural language processing (NLP). Inspired by the success of LLMs
in NLP tasks, some recent work has begun investigating the potential of
applying LLMs in graph learning tasks. However, most of the existing work
focuses on utilizing LLMs as powerful node feature augmenters, leaving
employing LLMs to enhance graph topological structures an understudied problem.
In this work, we explore how to leverage the information retrieval and text
generation capabilities of LLMs to refine/enhance the topological structure of
text-attributed graphs (TAGs) under the node classification setting. First, we
propose using LLMs to help remove unreliable edges and add reliable ones in the
TAG. Specifically, we first let the LLM output the semantic similarity between
node attributes through delicate prompt designs, and then perform edge deletion
and edge addition based on the similarity. Second, we propose using
pseudo-labels generated by the LLM to improve graph topology, that is, we
introduce the pseudo-label propagation as a regularization to guide the graph
neural network (GNN) in learning proper edge weights. Finally, we incorporate
the two aforementioned LLM-based methods for graph topological refinement into
the process of GNN training, and perform extensive experiments on four
real-world datasets. The experimental results demonstrate the effectiveness of
LLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain
on public benchmarks).",None,-1
209b2caf-8234-454e-acc0-56a8efa521ba,Single Sequence Prediction over Reasoning Graphs for Multi-hop QA,0.171702,"Recent generative approaches for multi-hop question answering (QA) utilize
the fusion-in-decoder method~\cite{izacard-grave-2021-leveraging} to generate a
single sequence output which includes both a final answer and a reasoning path
taken to arrive at that answer, such as passage titles and key facts from those
passages. While such models can lead to better interpretability and high
quantitative scores, they often have difficulty accurately identifying the
passages corresponding to key entities in the context, resulting in incorrect
passage hops and a lack of faithfulness in the reasoning path. To address this,
we propose a single-sequence prediction method over a local reasoning graph
(\model)\footnote{Code/Models will be released at
\url{https://github.com/gowtham1997/SeqGraph}} that integrates a graph
structure connecting key entities in each context passage to relevant
subsequent passages for each question. We use a graph neural network to encode
this graph structure and fuse the resulting representations into the entity
representations of the model. Our experiments show significant improvements in
answer exact-match/F1 scores and faithfulness of grounding in the reasoning
path on the HotpotQA dataset and achieve state-of-the-art numbers on the
Musique dataset with only up to a 4\% increase in model parameters.",None,-1
144ff934-d72d-4b09-a88d-06627a492c49,Scaling Vision-Language Models with Sparse Mixture of Experts,0.474456,"The field of natural language processing (NLP) has made significant strides
in recent years, particularly in the development of large-scale vision-language
models (VLMs). These models aim to bridge the gap between text and visual
information, enabling a more comprehensive understanding of multimedia data.
However, as these models become larger and more complex, they also become more
challenging to train and deploy. One approach to addressing this challenge is
the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the
model into smaller, specialized sub-models that can jointly solve a task. In
this paper, we explore the effectiveness of MoE in scaling vision-language
models, demonstrating its potential to achieve state-of-the-art performance on
a range of benchmarks over dense models of equivalent computational cost. Our
research offers valuable insights into stabilizing the training of MoE models,
understanding the impact of MoE on model interpretability, and balancing the
trade-offs between compute performance when scaling VLMs. We hope our work will
inspire further research into the use of MoE for scaling large-scale
vision-language models and other multimodal machine learning applications.",None,-1
4de59377-53b8-4138-8c07-78065d3d2a1e,End-to-End Spatio-Temporal Action Localisation with Video Transformers,0.636293,"The most performant spatio-temporal action localisation models use external
person proposals and complex external memory banks. We propose a fully
end-to-end, purely-transformer based model that directly ingests an input
video, and outputs tubelets -- a sequence of bounding boxes and the action
classes at each frame. Our flexible model can be trained with either sparse
bounding-box supervision on individual frames, or full tubelet annotations. And
in both cases, it predicts coherent tubelets as the output. Moreover, our
end-to-end model requires no additional pre-processing in the form of
proposals, or post-processing in terms of non-maximal suppression. We perform
extensive ablation experiments, and significantly advance the state-of-the-art
results on four different spatio-temporal action localisation benchmarks with
both sparse keyframes and full tubelet annotations.",None,-1
0301a27f-6c96-4083-98b3-d6bdda7efe13,Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting,0.394309,"Most existing stylistic text rewriting methods and evaluation metrics operate
on a sentence level, but ignoring the broader context of the text can lead to
preferring generic, ambiguous, and incoherent rewrites. In this paper, we
investigate integrating the preceding textual context into both the
$\textit{rewriting}$ and $\textit{evaluation}$ stages of stylistic text
rewriting, and introduce a new composite contextual evaluation metric
$\texttt{CtxSimFit}$ that combines similarity to the original sentence with
contextual cohesiveness. We comparatively evaluate non-contextual and
contextual rewrites in formality, toxicity, and sentiment transfer tasks. Our
experiments show that humans significantly prefer contextual rewrites as more
fitting and natural over non-contextual ones, yet existing sentence-level
automatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences
($\rho$=0--0.3). In contrast, human preferences are much better reflected by
both our novel $\texttt{CtxSimFit}$ ($\rho$=0.7--0.9) as well as proposed
context-infused versions of common metrics ($\rho$=0.4--0.7). Overall, our
findings highlight the importance of integrating context into the generation
and especially the evaluation stages of stylistic text rewriting.",None,-1
21cc53f5-4599-455a-a84e-95a48c959aa2,GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering,0.251063,"In this work, we present an end-to-end Knowledge Graph Question Answering
(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text
pre-trained language model. The model takes a question in natural language as
input and produces a simpler form of the intended SPARQL query. In the simpler
form, the model does not directly produce entity and relation IDs. Instead, it
produces corresponding entity and relation labels. The labels are grounded to
KG entity and relation IDs in a subsequent step. To further improve the
results, we instruct the model to produce a truncated version of the KG
embedding for each entity. The truncated KG embedding enables a finer search
for disambiguation purposes. We find that T5 is able to learn the truncated KG
embeddings without any change of loss function, improving KGQA performance. As
a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata
datasets on end-to-end KGQA over Wikidata.",None,-1
f862b735-40fd-4b82-8224-faf6ac4bf9af,Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission,0.291995,"Sequential labeling is a task predicting labels for each token in a sequence,
such as Named Entity Recognition (NER). NER tasks aim to extract entities and
predict their labels given a text, which is important in information
extraction. Although previous works have shown great progress in improving NER
performance, uncertainty estimation on NER (UE-NER) is still underexplored but
essential. This work focuses on UE-NER, which aims to estimate uncertainty
scores for the NER predictions. Previous uncertainty estimation models often
overlook two unique characteristics of NER: the connection between entities
(i.e., one entity embedding is learned based on the other ones) and wrong span
cases in the entity extraction subtask. Therefore, we propose a Sequential
Labeling Posterior Network (SLPN) to estimate uncertainty scores for the
extracted entities, considering uncertainty transmitted from other tokens.
Moreover, we have defined an evaluation strategy to address the specificity of
wrong-span cases. Our SLPN has achieved significant improvements on three
datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant
dataset. Our code is available at
\url{https://github.com/he159ok/UncSeqLabeling_SLPN}.",None,-1
ba9feb20-0340-4a3d-a40a-b388b77f34f4,Privacy Measurement in Tabular Synthetic Data: State of the Art and Future Research Directions,0.391416,"Synthetic data (SD) have garnered attention as a privacy enhancing
technology. Unfortunately, there is no standard for quantifying their degree of
privacy protection. In this paper, we discuss proposed quantification
approaches. This contributes to the development of SD privacy standards;
stimulates multi-disciplinary discussion; and helps SD researchers make
informed modeling and evaluation decisions.",None,-1
a06aa00b-bb86-418d-b53e-7087d4e7d493,Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding,0.678847,"To tackle the high inference latency exhibited by autoregressive language
models, previous studies have proposed an early-exiting framework that
allocates adaptive computation paths for each token based on the complexity of
generating the subsequent token. However, we observed several shortcomings,
including performance degradation caused by a state copying mechanism or
numerous exit paths, and sensitivity to exit confidence thresholds.
Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework,
which incorporates a shallow-deep module and a synchronized parallel decoding.
Our framework enables faster inference by synchronizing the decoding process of
the current token with previously stacked early-exited tokens. Furthermore, as
parallel decoding allows us to observe predictions from both shallow and deep
models, we present a novel adaptive threshold estimator that exploits a Beta
mixture model to determine suitable confidence thresholds. We empirically
demonstrated the superiority of our proposed framework on extensive generation
tasks.",None,-1
5f158910-9c99-47e1-bcf9-500ab58c2dcc,OptiMUS: Optimization Modeling Using MIP Solvers and large language models,0.25695,"Optimization problems are pervasive across various sectors, from
manufacturing and distribution to healthcare. However, most such problems are
still solved heuristically by hand rather than optimally by state-of-the-art
solvers, as the expertise required to formulate and solve these problems limits
the widespread adoption of optimization tools and techniques. We introduce
OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and
solve MILP problems from their natural language descriptions. OptiMUS is
capable of developing mathematical models, writing and debugging solver code,
developing tests, and checking the validity of generated solutions. To
benchmark our agent, we present NLP4LP, a novel dataset of linear programming
(LP) and mixed integer linear programming (MILP) problems. Our experiments
demonstrate that OptiMUS solves nearly twice as many problems as a basic LLM
prompting strategy. OptiMUS code and NLP4LP dataset are available at
\href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}",None,-1
96155284-1232-4e1b-828e-a4823c284314,Deformation measurement of a soil mixing retaining wall using terrestrial laser scanning,0.49269,"Retaining walls are often built to prevent excessive lateral movements of the
ground surrounding an excavation site. During an excavation, failure of
retaining walls could cause catastrophic accidents and hence their lateral
deformations are monitored regularly. Laser scanning can rapidly acquire the
spatial data of a relatively large area at fine spatial resolutions, which is
ideal for monitoring retaining walls' deformations. This paper attempts to
apply laser scanning to measurements of the lateral deformations of a soil
mixing retaining wall at an ongoing excavation site. Reference measurements by
total station and inclinometer were also conducted to verify those from the
laser scanning. The deformations derived using laser scanning data were
consistent with the reference measurements at the top part of the retaining
wall (i.e., mainly the ring beam of the wall). This research also shows that
the multi-scale-model-to-model method was the most accurate deformation
estimation method on the research data.",None,-1
e96baa38-46e4-46ce-9c8d-3765515e6449,ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts,0.588705,"We introduce ProtoSeg, a novel model for interpretable semantic image
segmentation, which constructs its predictions using similar patches from the
training set. To achieve accuracy comparable to baseline methods, we adapt the
mechanism of prototypical parts and introduce a diversity loss function that
increases the variety of prototypes within each class. We show that ProtoSeg
discovers semantic concepts, in contrast to standard segmentation models.
Experiments conducted on Pascal VOC and Cityscapes datasets confirm the
precision and transparency of the presented method.",None,-1
72cefe7b-69ee-47cf-8a06-c41332651e39,FedMAE: Federated Self-Supervised Learning with One-Block Masked Auto-Encoder,0.053655,"Latest federated learning (FL) methods started to focus on how to use
unlabeled data in clients for training due to users' privacy concerns, high
labeling costs, or lack of expertise. However, current Federated
Semi-Supervised/Self-Supervised Learning (FSSL) approaches fail to learn
large-scale images because of the limited computing resources of local clients.
In this paper, we introduce a new framework FedMAE, which stands for Federated
Masked AutoEncoder, to address the problem of how to utilize unlabeled
large-scale images for FL. Specifically, FedMAE can pre-train one-block Masked
AutoEncoder (MAE) using large images in lightweight client devices, and then
cascades multiple pre-trained one-block MAEs in the server to build a
multi-block ViT backbone for downstream tasks. Theoretical analysis and
experimental results on image reconstruction and classification show that our
FedMAE achieves superior performance compared to the state-of-the-art FSSL
methods.",None,-1
90ceea1d-e48c-43cd-a0b0-1cf7d78d36b5,Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization,0.843981,"Backdoor defense, which aims to detect or mitigate the effect of malicious
triggers introduced by attackers, is becoming increasingly critical for machine
learning security and integrity. Fine-tuning based on benign data is a natural
defense to erase the backdoor effect in a backdoored model. However, recent
studies show that, given limited benign data, vanilla fine-tuning has poor
defense performance. In this work, we provide a deep study of fine-tuning the
backdoored model from the neuron perspective and find that backdoorrelated
neurons fail to escape the local minimum in the fine-tuning process. Inspired
by observing that the backdoorrelated neurons often have larger norms, we
propose FTSAM, a novel backdoor defense paradigm that aims to shrink the norms
of backdoor-related neurons by incorporating sharpness-aware minimization with
fine-tuning. We demonstrate the effectiveness of our method on several
benchmark datasets and network architectures, where it achieves
state-of-the-art defense performance. Overall, our work provides a promising
avenue for improving the robustness of machine learning models against backdoor
attacks.",None,-1
a11fae81-57f2-4fb2-a20a-8aa338c33daf,Gradient-Based Automated Iterative Recovery for Parameter-Efficient Tuning,0.620888,"Pretrained large language models (LLMs) are able to solve a wide variety of
tasks through transfer learning. Various explainability methods have been
developed to investigate their decision making process. TracIn (Pruthi et al.,
2020) is one such gradient-based method which explains model inferences based
on the influence of training examples. In this paper, we explore the use of
TracIn to improve model performance in the parameter-efficient tuning (PET)
setting. We develop conversational safety classifiers via the prompt-tuning PET
method and show how the unique characteristics of the PET regime enable TracIn
to identify the cause for certain misclassifications by LLMs. We develop a new
methodology for using gradient-based explainability techniques to improve model
performance, G-BAIR: gradient-based automated iterative recovery. We show that
G-BAIR can recover LLM performance on benchmarks after manually corrupting
training labels. This suggests that influence methods like TracIn can be used
to automatically perform data cleaning, and introduces the potential for
interactive debugging and relabeling for PET-based transfer learning methods.",None,-1
d00b2689-2beb-45f7-817e-01deecc3c1b7,Enhancing Programming eTextbooks with ChatGPT Generated Counterfactual-Thinking-Inspired Questions,0.152213,"Digital textbooks have become an integral part of everyday learning tasks. In
this work, we consider the use of digital textbooks for programming classes.
Generally, students struggle with utilizing textbooks on programming to the
maximum, with a possible reason being that the example programs provided as
illustration of concepts in these textbooks don't offer sufficient
interactivity for students, and thereby not sufficiently motivating to explore
or understand these programming examples better. In our work, we explore the
idea of enhancing the navigability of intelligent textbooks with the use of
``counterfactual'' questions, to make students think critically about these
programs and enhance possible program comprehension. Inspired from previous
works on nudging students on counter factual thinking, we present the
possibility to enhance digital textbooks with questions generated using GPT.",None,-1
6d3687aa-f587-44c2-901e-84553a2dd335,With a Little Help from the Authors: Reproducing Human Evaluation of an MT Error Detector,0.628339,"This work presents our efforts to reproduce the results of the human
evaluation experiment presented in the paper of Vamvas and Sennrich (2022),
which evaluated an automatic system detecting over- and undertranslations
(translations containing more or less information than the original) in machine
translation (MT) outputs. Despite the high quality of the documentation and
code provided by the authors, we discuss some problems we found in reproducing
the exact experimental setup and offer recommendations for improving
reproducibility. Our replicated results generally confirm the conclusions of
the original study, but in some cases, statistically significant differences
were observed, suggesting a high variability of human annotation.",None,-1
c89f16d1-be26-42c1-aae6-25fbe0a0a404,Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs,0.864471,"The performance of large language models (LLMs) has recently improved to the
point where the models can perform well on many language tasks. We show here
that for the first time, the models can also generate coherent and valid formal
analyses of linguistic data and illustrate the vast potential of large language
models for analyses of their metalinguistic abilities. LLMs are primarily
trained on language data in the form of text; analyzing and evaluating their
metalinguistic abilities improves our understanding of their general
capabilities and sheds new light on theoretical models in linguistics. In this
paper, we probe into GPT-4's metalinguistic capabilities by focusing on three
subfields of formal linguistics: syntax, phonology, and semantics. We outline a
research program for metalinguistic analyses of large language models, propose
experimental designs, provide general guidelines, discuss limitations, and
offer future directions for this line of research. This line of inquiry also
exemplifies behavioral interpretability of deep learning, where models'
representations are accessed by explicit prompting rather than internal
representations.",None,-1
9bffc9c8-074c-4d69-99f8-56f1bc80eaf8,Injecting Logical Constraints into Neural Networks via Straight-Through Estimators,0.986575,"Injecting discrete logical constraints into neural network learning is one of
the main challenges in neuro-symbolic AI. We find that a
straight-through-estimator, a method introduced to train binary neural
networks, could effectively be applied to incorporate logical constraints into
neural network learning. More specifically, we design a systematic way to
represent discrete logical constraints as a loss function; minimizing this loss
using gradient descent via a straight-through-estimator updates the neural
network's weights in the direction that the binarized outputs satisfy the
logical constraints. The experimental results show that by leveraging GPUs and
batch training, this method scales significantly better than existing
neuro-symbolic methods that require heavy symbolic computation for computing
gradients. Also, we demonstrate that our method applies to different types of
neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer
labeled data by learning directly from known constraints.",None,-1
f89595fa-9933-44d3-b78e-58679b95ce82,Use neural networks to recognize students' handwritten letters and incorrect symbols,0.500889,"Correcting students' multiple-choice answers is a repetitive and mechanical
task that can be considered an image multi-classification task. Assuming
possible options are 'abcd' and the correct option is one of the four, some
students may write incorrect symbols or options that do not exist. In this
paper, five classifications were set up - four for possible correct options and
one for other incorrect writing. This approach takes into account the
possibility of non-standard writing options.",None,-1
7198d1ea-6167-4441-8ca5-f97fc5422920,KGTrust: Evaluating Trustworthiness of SIoT via Knowledge Enhanced Graph Neural Networks,0.506885,"Social Internet of Things (SIoT), a promising and emerging paradigm that
injects the notion of social networking into smart objects (i.e., things),
paving the way for the next generation of Internet of Things. However, due to
the risks and uncertainty, a crucial and urgent problem to be settled is
establishing reliable relationships within SIoT, that is, trust evaluation.
Graph neural networks for trust evaluation typically adopt a straightforward
way such as one-hot or node2vec to comprehend node characteristics, which
ignores the valuable semantic knowledge attached to nodes. Moreover, the
underlying structure of SIoT is usually complex, including both the
heterogeneous graph structure and pairwise trust relationships, which renders
hard to preserve the properties of SIoT trust during information propagation.
To address these aforementioned problems, we propose a novel knowledge-enhanced
graph neural network (KGTrust) for better trust evaluation in SIoT.
Specifically, we first extract useful knowledge from users' comment behaviors
and external structured triples related to object descriptions, in order to
gain a deeper insight into the semantics of users and objects. Furthermore, we
introduce a discriminative convolutional layer that utilizes heterogeneous
graph structure, node semantics, and augmented trust relationships to learn
node embeddings from the perspective of a user as a trustor or a trustee,
effectively capturing multi-aspect properties of SIoT trust during information
propagation. Finally, a trust prediction layer is developed to estimate the
trust relationships between pairwise nodes. Extensive experiments on three
public datasets illustrate the superior performance of KGTrust over
state-of-the-art methods.",None,-1
d103860c-8dfd-4033-93ee-e653d18b616d,Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes,0.979713,"Increasingly powerful Large Language Model (LLM) based chatbots, like ChatGPT
and Bard, are becoming available to users that have the potential to
revolutionize the quality of decision-making achieved by the public. In this
context, we set out to investigate how such systems perform in the personal
finance domain, where financial inclusion has been an overarching stated aim of
banks for decades. We asked 13 questions representing banking products in
personal finance: bank account, credit card, and certificate of deposits and
their inter-product interactions, and decisions related to high-value
purchases, payment of bank dues, and investment advice, and in different
dialects and languages (English, African American Vernacular English, and
Telugu). We find that although the outputs of the chatbots are fluent and
plausible, there are still critical gaps in providing accurate and reliable
financial information using LLM-based chatbots.",None,-1
671ab3ae-336f-4364-8914-488facd24631,Bright Channel Prior Attention for Multispectral Pedestrian Detection,0.53994,"Multispectral methods have gained considerable attention due to their
promising performance across various fields. However, most existing methods
cannot effectively utilize information from two modalities while optimizing
time efficiency. These methods often prioritize accuracy or time efficiency,
leaving room for improvement in their performance. To this end, we propose a
new method bright channel prior attention for enhancing pedestrian detection in
low-light conditions by integrating image enhancement and detection within a
unified framework. The method uses the V-channel of the HSV image of the
thermal image as an attention map to trigger the unsupervised auto-encoder for
visible light images, which gradually emphasizes pedestrian features across
layers. Moreover, we utilize unsupervised bright channel prior algorithms to
address light compensation in low light images. The proposed method includes a
self-attention enhancement module and a detection module, which work together
to improve object detection. An initial illumination map is estimated using the
BCP, guiding the learning of the self-attention map from the enhancement
network to obtain more informative representation focused on pedestrians. The
extensive experiments show effectiveness of the proposed method is demonstrated
through.",None,-1
ff70093d-cffc-45cc-ba18-0ba25f65eedd,BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks,0.219454,"Recently, the Segment Anything Model (SAM) has gained significant attention
as an image segmentation foundation model due to its strong performance on
various downstream tasks. However, it has been found that SAM does not always
perform satisfactorily when faced with challenging downstream tasks. This has
led downstream users to demand a customized SAM model that can be adapted to
these downstream tasks. In this paper, we present BadSAM, the first backdoor
attack on the image segmentation foundation model. Our preliminary experiments
on the CAMO dataset demonstrate the effectiveness of BadSAM.",None,-1
67a26473-05ec-4183-b191-cc4fca8c9c6d,DiscoPrompt: Path Prediction Prompt Tuning for Implicit Discourse Relation Recognition,0.989553,"Implicit Discourse Relation Recognition (IDRR) is a sophisticated and
challenging task to recognize the discourse relations between the arguments
with the absence of discourse connectives. The sense labels for each discourse
relation follow a hierarchical classification scheme in the annotation process
(Prasad et al., 2008), forming a hierarchy structure. Most existing works do
not well incorporate the hierarchy structure but focus on the syntax features
and the prior knowledge of connectives in the manner of pure text
classification. We argue that it is more effective to predict the paths inside
the hierarchical tree (e.g., ""Comparison -> Contrast -> however"") rather than
flat labels (e.g., Contrast) or connectives (e.g., however). We propose a
prompt-based path prediction method to utilize the interactive information and
intrinsic senses among the hierarchy in IDRR. This is the first work that
injects such structure information into pre-trained language models via prompt
tuning, and the performance of our solution shows significant and consistent
improvement against competitive baselines.",None,-1
8709bb69-783b-446c-a99e-fb78fae4da30,DynamicDet: A Unified Dynamic Architecture for Object Detection,0.406882,"Dynamic neural network is an emerging research topic in deep learning. With
adaptive inference, dynamic models can achieve remarkable accuracy and
computational efficiency. However, it is challenging to design a powerful
dynamic detector, because of no suitable dynamic architecture and exiting
criterion for object detection. To tackle these difficulties, we propose a
dynamic framework for object detection, named DynamicDet. Firstly, we carefully
design a dynamic architecture based on the nature of the object detection task.
Then, we propose an adaptive router to analyze the multi-scale information and
to decide the inference route automatically. We also present a novel
optimization strategy with an exiting criterion based on the detection losses
for our dynamic detectors. Last, we present a variable-speed inference
strategy, which helps to realize a wide range of accuracy-speed trade-offs with
only one dynamic detector. Extensive experiments conducted on the COCO
benchmark demonstrate that the proposed DynamicDet achieves new
state-of-the-art accuracy-speed trade-offs. For instance, with comparable
accuracy, the inference speed of our dynamic detector Dy-YOLOv7-W6 surpasses
YOLOv7-E6 by 12%, YOLOv7-D6 by 17%, and YOLOv7-E6E by 39%. The code is
available at https://github.com/VDIGPKU/DynamicDet.",None,-1
7ff4e1aa-0db1-46b5-a388-f316a0244d3f,ExpertPrompting: Instructing Large Language Models to be Distinguished Experts,0.39323,"The answering quality of an aligned large language model (LLM) can be
drastically improved if treated with proper crafting of prompts. In this paper,
we propose ExpertPrompting to elicit the potential of LLMs to answer as
distinguished experts. We first utilize In-Context Learning to automatically
synthesize detailed and customized descriptions of the expert identity for each
specific instruction, and then ask LLMs to provide answer conditioned on such
agent background. Based on this augmented prompting strategy, we produce a new
set of instruction-following data using GPT-3.5, and train a competitive
open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation
to show that 1) the expert data is of significantly higher quality than vanilla
answers, and 2) ExpertLLaMA outperforms existing open-source opponents and
achieves 96\% of the original ChatGPT's capability. All data and the
ExpertLLaMA model will be made publicly available at
\url{https://github.com/OFA-Sys/ExpertLLaMA}.",None,-1
c3645b92-f85a-4c04-9a76-d5a825bcf4ff,Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements,0.973528,"Despite the much discussed capabilities of today's language models, they are
still prone to silly and unexpected commonsense failures. We consider a
retrospective verification approach that reflects on the correctness of LM
outputs, and introduce Vera, a general-purpose model that estimates the
plausibility of declarative statements based on commonsense knowledge. Trained
on ~7M commonsense statements created from 19 QA datasets and two large-scale
knowledge bases, and with a combination of three training objectives, Vera is a
versatile model that effectively separates correct from incorrect statements
across diverse commonsense domains. When applied to solving commonsense
problems in the verification format, Vera substantially outperforms existing
models that can be repurposed for commonsense verification, and it further
exhibits generalization capabilities to unseen tasks and provides
well-calibrated outputs. We find that Vera excels at filtering LM-generated
commonsense knowledge and is useful in detecting erroneous commonsense
statements generated by models like ChatGPT in real-world settings.",None,-1
7d048acd-847d-4271-b87a-bf8974f00c0c,InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion,0.999822,"This paper addresses a novel task of anticipating 3D human-object
interactions (HOIs). Most existing research on HOI synthesis lacks
comprehensive whole-body interactions with dynamic objects, e.g., often limited
to manipulating small or static objects. Our task is significantly more
challenging, as it requires modeling dynamic objects with various shapes,
capturing whole-body motion, and ensuring physically valid interactions. To
this end, we propose InterDiff, a framework comprising two key steps: (i)
interaction diffusion, where we leverage a diffusion model to encode the
distribution of future human-object interactions; (ii) interaction correction,
where we introduce a physics-informed predictor to correct denoised HOIs in a
diffusion step. Our key insight is to inject prior knowledge that the
interactions under reference with respect to contact points follow a simple
pattern and are easily predictable. Experiments on multiple human-object
interaction datasets demonstrate the effectiveness of our method for this task,
capable of producing realistic, vivid, and remarkably long-term 3D HOI
predictions.",None,-1
4a24f2b2-13db-4a57-a435-27613e83ebed,Improving Language Models via Plug-and-Play Retrieval Feedback,0.725941,"Large language models (LLMs) exhibit remarkable performance across various
NLP tasks. However, they often generate incorrect or hallucinated information,
which hinders their practical applicability in real-world scenarios. Human
feedback has been shown to effectively enhance the factuality and quality of
generated content, addressing some of these limitations. However, this approach
is resource-intensive, involving manual input and supervision, which can be
time-consuming and expensive. Moreover, it cannot be provided during inference,
further limiting its practical utility in dynamic and interactive applications.
In this paper, we introduce ReFeed, a novel pipeline designed to enhance LLMs
by providing automatic retrieval feedback in a plug-and-play framework without
the need for expensive fine-tuning. ReFeed first generates initial outputs,
then utilizes a retrieval model to acquire relevant information from large
document collections, and finally incorporates the retrieved information into
the in-context demonstration for output refinement, thereby addressing the
limitations of LLMs in a more efficient and cost-effective manner. Experiments
on four knowledge-intensive benchmark datasets demonstrate our proposed ReFeed
could improve over +6.0% under zero-shot setting and +2.5% under few-shot
setting, compared to baselines without using retrieval feedback.",None,-1
59d58f31-fe00-4056-8d6c-a4fba1c5e7d5,Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping,0.340702,"Identifying disease phenotypes from electronic health records (EHRs) is
critical for numerous secondary uses. Manually encoding physician knowledge
into rules is particularly challenging for rare diseases due to inadequate EHR
coding, necessitating review of clinical notes. Large language models (LLMs)
offer promise in text understanding but may not efficiently handle real-world
clinical documentation. We propose a zero-shot LLM-based method enriched by
retrieval-augmented generation and MapReduce, which pre-identifies
disease-related text snippets to be used in parallel as queries for the LLM to
establish diagnosis. We show that this method as applied to pulmonary
hypertension (PH), a rare disease characterized by elevated arterial pressures
in the lungs, significantly outperforms physician logic rules ($F_1$ score of
0.62 vs. 0.75). This method has the potential to enhance rare disease cohort
identification, expanding the scope of robust clinical research and care gap
identification.",None,-1
550f95ed-ae9e-487f-9f6f-1eb031838c72,Egocentric Auditory Attention Localization in Conversations,0.801655,"In a noisy conversation environment such as a dinner party, people often
exhibit selective auditory attention, or the ability to focus on a particular
speaker while tuning out others. Recognizing who somebody is listening to in a
conversation is essential for developing technologies that can understand
social behavior and devices that can augment human hearing by amplifying
particular sound sources. The computer vision and audio research communities
have made great strides towards recognizing sound sources and speakers in
scenes. In this work, we take a step further by focusing on the problem of
localizing auditory attention targets in egocentric video, or detecting who in
a camera wearer's field of view they are listening to. To tackle the new and
challenging Selective Auditory Attention Localization problem, we propose an
end-to-end deep learning approach that uses egocentric video and multichannel
audio to predict the heatmap of the camera wearer's auditory attention. Our
approach leverages spatiotemporal audiovisual features and holistic reasoning
about the scene to make predictions, and outperforms a set of baselines on a
challenging multi-speaker conversation dataset. Project page:
https://fkryan.github.io/saal",None,-1
a3b23d96-3372-422f-9c0e-c0ce45643a7d,Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features,0.831646,"Recent studies have demonstrated the susceptibility of deep neural networks
to backdoor attacks. Given a backdoored model, its prediction of a poisoned
sample with trigger will be dominated by the trigger information, though
trigger information and benign information coexist. Inspired by the mechanism
of the optical polarizer that a polarizer could pass light waves with
particular polarizations while filtering light waves with other polarizations,
we propose a novel backdoor defense method by inserting a learnable neural
polarizer into the backdoored model as an intermediate layer, in order to
purify the poisoned sample via filtering trigger information while maintaining
benign information. The neural polarizer is instantiated as one lightweight
linear transformation layer, which is learned through solving a well designed
bi-level optimization problem, based on a limited clean dataset. Compared to
other fine-tuning-based defense methods which often adjust all parameters of
the backdoored model, the proposed method only needs to learn one additional
layer, such that it is more efficient and requires less clean data. Extensive
experiments demonstrate the effectiveness and efficiency of our method in
removing backdoors across various neural network architectures and datasets,
especially in the case of very limited clean data.",None,-1
9228d6e3-025e-4da0-86ec-182db8215e32,ControversialQA: Exploring Controversy in Question Answering,0.0215258,"Controversy is widespread online. Previous studies mainly define controversy
based on vague assumptions of its relation to sentiment such as hate speech and
offensive words. This paper introduces the first question-answering dataset
that defines content controversy by user perception, i.e., votes from plenty of
users. It contains nearly 10K questions, and each question has a best answer
and a most controversial answer. Experimental results reveal that controversy
detection in question answering is essential and challenging, and there is no
strong correlation between controversy and sentiment tasks.",None,-1
3c6f735f-2f1b-40bf-b670-084700dc3448,"Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language",0.044366,"The Free Energy Principle (FEP) is a theoretical framework for describing how
(intelligent) systems self-organise into coherent, stable structures by
minimising a free energy functional. Active Inference (AIF) is a corollary of
the FEP that specifically details how systems that are able to plan for the
future (agents) function by minimising particular free energy functionals that
incorporate information seeking components. This paper is the first in a series
of two where we derive a synthetic version of AIF on free form factor graphs.
The present paper focuses on deriving a local version of the free energy
functionals used for AIF. This enables us to construct a version of AIF which
applies to arbitrary graphical models and interfaces with prior work on message
passing algorithms. The resulting messages are derived in our companion paper.
We also identify a gap in the graphical notation used for factor graphs. While
factor graphs are great at expressing a generative model, they have so far been
unable to specify the full optimisation problem including constraints. To solve
this problem we develop Constrained Forney-style Factor Graph (CFFG) notation
which permits a fully graphical description of variational inference
objectives. We then proceed to show how CFFG's can be used to reconstruct prior
algorithms for AIF as well as derive new ones. The latter is demonstrated by
deriving an algorithm that permits direct policy inference for AIF agents,
circumventing a long standing scaling issue that has so far hindered the
application of AIF in industrial settings. We demonstrate our algorithm on the
classic T-maze task and show that it reproduces the information seeking
behaviour that is a hallmark feature of AIF.",None,-1
bda6f0c6-55d1-4f9b-b3e0-fbfce7750201,Conceptual Views on Tree Ensemble Classifiers,0.334503,"Random Forests and related tree-based methods are popular for supervised
learning from table based data. Apart from their ease of parallelization, their
classification performance is also superior. However, this performance,
especially parallelizability, is offset by the loss of explainability.
Statistical methods are often used to compensate for this disadvantage. Yet,
their ability for local explanations, and in particular for global
explanations, is limited. In the present work we propose an algebraic method,
rooted in lattice theory, for the (global) explanation of tree ensembles. In
detail, we introduce two novel conceptual views on tree ensemble classifiers
and demonstrate their explanatory capabilities on Random Forests that were
trained with standard parameters.",None,-1
2771e076-3217-47ff-a0f5-9308d935726f,Mark My Words: Dangers of Watermarked Images in ImageNet,0.0686769,"The utilization of pre-trained networks, especially those trained on
ImageNet, has become a common practice in Computer Vision. However, prior
research has indicated that a significant number of images in the ImageNet
dataset contain watermarks, making pre-trained networks susceptible to learning
artifacts such as watermark patterns within their latent spaces. In this paper,
we aim to assess the extent to which popular pre-trained architectures display
such behavior and to determine which classes are most affected. Additionally,
we examine the impact of watermarks on the extracted features. Contrary to the
popular belief that the Chinese logographic watermarks impact the ""carton""
class only, our analysis reveals that a variety of ImageNet classes, such as
""monitor"", ""broom"", ""apron"" and ""safe"" rely on spurious correlations. Finally,
we propose a simple approach to mitigate this issue in fine-tuned networks by
ignoring the encodings from the feature-extractor layer of ImageNet pre-trained
networks that are most susceptible to watermark imprints.",None,-1
45727224-766c-4bd0-9fe3-6411e4f67aa5,Elementary Sets for Logic Programs,0.390698,"By introducing the concepts of a loop and a loop formula, Lin and Zhao showed
that the answer sets of a nondisjunctive logic program are exactly the models
of its Clark's completion that satisfy the loop formulas of all loops.
Recently, Gebser and Schaub showed that the Lin-Zhao theorem remains correct
even if we restrict loop formulas to a special class of loops called
``elementary loops.'' In this paper, we simplify and generalize the notion of
an elementary loop, and clarify its role. We propose the notion of an
elementary set, which is almost equivalent to the notion of an elementary loop
for nondisjunctive programs, but is simpler, and, unlike elementary loops, can
be extended to disjunctive programs without producing unintuitive results. We
show that the maximal unfounded elementary sets for the ``relevant'' part of a
program are exactly the minimal sets among the nonempty unfounded sets. We also
present a graph-theoretic characterization of elementary sets for
nondisjunctive programs, which is simpler than the one proposed in (Gebser &
Schaub 2005). Unlike the case of nondisjunctive programs, we show that the
problem of deciding an elementary set is coNP-complete for disjunctive
programs.",None,-1
35137f42-dc42-432d-9b15-71e0e862f090,UNICORN: A Unified Backdoor Trigger Inversion Framework,0.765972,"The backdoor attack, where the adversary uses inputs stamped with triggers
(e.g., a patch) to activate pre-planted malicious behaviors, is a severe threat
to Deep Neural Network (DNN) models. Trigger inversion is an effective way of
identifying backdoor models and understanding embedded adversarial behaviors. A
challenge of trigger inversion is that there are many ways of constructing the
trigger. Existing methods cannot generalize to various types of triggers by
making certain assumptions or attack-specific constraints. The fundamental
reason is that existing work does not consider the trigger's design space in
their formulation of the inversion problem. This work formally defines and
analyzes the triggers injected in different spaces and the inversion problem.
Then, it proposes a unified framework to invert backdoor triggers based on the
formalization of triggers and the identified inner behaviors of backdoor models
from our analysis. Our prototype UNICORN is general and effective in inverting
backdoor triggers in DNNs. The code can be found at
https://github.com/RU-System-Software-and-Security/UNICORN.",None,-1
2e02f4a6-8e47-4aca-a1da-a31927395dad,When to Trust AI: Advances and Challenges for Certification of Neural Networks,0.906671,"Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges.",None,-1
7405b93c-e79a-44ca-8047-04d31b419377,SparseVSR: Lightweight and Noise Robust Visual Speech Recognition,0.333421,"Recent advances in deep neural networks have achieved unprecedented success
in visual speech recognition. However, there remains substantial disparity
between current methods and their deployment in resource-constrained devices.
In this work, we explore different magnitude-based pruning techniques to
generate a lightweight model that achieves higher performance than its dense
model equivalent, especially under the presence of visual noise. Our sparse
models achieve state-of-the-art results at 10% sparsity on the LRS3 dataset and
outperform the dense equivalent up to 70% sparsity. We evaluate our 50% sparse
model on 7 different visual noise types and achieve an overall absolute
improvement of more than 2% WER compared to the dense equivalent. Our results
confirm that sparse networks are more resistant to noise than dense networks.",None,-1
53d22f47-2f92-4a88-9bfa-356eceff48d0,A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets,0.562199,"Low-quality data can cause downstream problems in high-stakes applications.
Data-centric approach emphasizes on improving dataset quality to enhance model
performance. High-quality datasets are needed for general-purpose Large
Language Models (LLMs) training, as well as for domain-specific models, which
are usually small in size as it is costly to engage a large number of domain
experts for their creation. Thus, it is vital to ensure high-quality
domain-specific training data. In this paper, we propose a framework for
enhancing the data quality of original datasets. We applied the proposed
framework to four biomedical datasets and showed relative improvement of up to
33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when
using back translation to enhance the original dataset quality.",None,-1
35f45b0b-3eee-47a8-8ecb-67b6729f923b,When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset,0.272108,"Annotators are not fungible. Their demographics, life experiences, and
backgrounds all contribute to how they label data. However, NLP has only
recently considered how annotator identity might influence their decisions.
Here, we present POPQUORN (the POtato-Prolific dataset for QUestion-Answering,
Offensiveness, text Rewriting, and politeness rating with demographic Nuance).
POPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a
representative sample regarding sex, age, and race as the US population.
Through a series of analyses, we show that annotators' background plays a
significant role in their judgments. Further, our work shows that backgrounds
not previously considered in NLP (e.g., education), are meaningful and should
be considered. Our study suggests that understanding the background of
annotators and collecting labels from a demographically balanced pool of crowd
workers is important to reduce the bias of datasets. The dataset, annotator
background, and annotation interface are available at
https://github.com/Jiaxin-Pei/potato-prolific-dataset .",None,-1
c5b7e139-9d2c-4379-b514-5bca98fd9c22,Explainable Depression Detection via Head Motion Patterns,0.106086,"While depression has been studied via multimodal non-verbal behavioural cues,
head motion behaviour has not received much attention as a biomarker. This
study demonstrates the utility of fundamental head-motion units, termed
\emph{kinemes}, for depression detection by adopting two distinct approaches,
and employing distinctive features: (a) discovering kinemes from head motion
data corresponding to both depressed patients and healthy controls, and (b)
learning kineme patterns only from healthy controls, and computing statistics
derived from reconstruction errors for both the patient and control classes.
Employing machine learning methods, we evaluate depression classification
performance on the \emph{BlackDog} and \emph{AVEC2013} datasets. Our findings
indicate that: (1) head motion patterns are effective biomarkers for detecting
depressive symptoms, and (2) explanatory kineme patterns consistent with prior
findings can be observed for the two classes. Overall, we achieve peak F1
scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 for binary
classification over episodic \emph{thin-slices}, and a peak F1 of 0.72 over
videos for AVEC2013.",None,-1
479349b3-ce5c-4c2f-a298-c79d6285873d,NeuroX Library for Neuron Analysis of Deep NLP Models,0.741346,"Neuron analysis provides insights into how knowledge is structured in
representations and discovers the role of neurons in the network. In addition
to developing an understanding of our models, neuron analysis enables various
applications such as debiasing, domain adaptation and architectural search. We
present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis
of natural language processing models. It implements various interpretation
methods under a unified API, and provides a framework for data processing and
evaluation, thus making it easier for researchers and practitioners to perform
neuron analysis. The Python toolkit is available at
https://www.github.com/fdalvi/NeuroX. Demo Video available at
https://youtu.be/mLhs2YMx4u8.",None,-1
04e664ce-07c7-40ff-ba3b-3632296bd709,Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),0.809814,"Recently, there has been a remarkable increase in the interest towards
skeleton-based action recognition within the research community, owing to its
various advantageous features, including computational efficiency,
representative features, and illumination invariance. Despite this, researchers
continue to explore and investigate the most optimal way to represent human
actions through skeleton representation and the extracted features. As a
result, the growth and availability of human action recognition datasets have
risen substantially. In addition, deep learning-based algorithms have gained
widespread popularity due to the remarkable advancements in various computer
vision tasks. Most state-of-the-art contributions in skeleton-based action
recognition incorporate a Graph Neural Network (GCN) architecture for
representing the human body and extracting features. Our research demonstrates
that Convolutional Neural Networks (CNNs) can attain comparable results to GCN,
provided that the proper training techniques, augmentations, and optimizers are
applied. Our approach has been rigorously validated, and we have achieved a
score of 95% on the NTU-60 dataset",None,-1
279b5d67-3c60-485f-9a9e-db1bf4ef984d,Does Federated Learning Really Need Backpropagation?,0.510565,"Federated learning (FL) is a general principle for decentralized clients to
train a server model collectively without sharing local data. FL is a promising
framework with practical applications, but its standard training paradigm
requires the clients to backpropagate through the model to compute gradients.
Since these clients are typically edge devices and not fully trusted, executing
backpropagation on them incurs computational and storage overhead as well as
white-box vulnerability. In light of this, we develop backpropagation-free
federated learning, dubbed BAFFLE, in which backpropagation is replaced by
multiple forward processes to estimate gradients. BAFFLE is 1) memory-efficient
and easily fits uploading bandwidth; 2) compatible with inference-only hardware
optimization and model quantization or pruning; and 3) well-suited to trusted
execution environments, because the clients in BAFFLE only execute forward
propagation and return a set of scalars to the server. Empirically we use
BAFFLE to train deep models from scratch or to finetune pretrained models,
achieving acceptable results. Code is available in
https://github.com/FengHZ/BAFFLE.",None,-1
7b73b726-dc7e-4928-96a3-0b00d9043d8a,Construction of Decision Trees and Acyclic Decision Graphs from Decision Rule Systems,0.0880152,"Decision trees and systems of decision rules are widely used as classifiers,
as a means for knowledge representation, and as algorithms. They are among the
most interpretable models for data analysis. The study of the relationships
between these two models can be seen as an important task of computer science.
Methods for transforming decision trees into systems of decision rules are
simple and well-known. In this paper, we consider the inverse transformation
problem, which is not trivial. We study the complexity of constructing decision
trees and acyclic decision graphs representing decision trees from decision
rule systems, and we discuss the possibility of not building the entire
decision tree, but describing the computation path in this tree for the given
input.",None,-1
e8f2761b-1cc0-40e1-9835-9bd63d8c63ad,Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals,0.0665215,"We present a human-in-the-loop dashboard tailored to diagnosing potential
spurious features that NLI models rely on for predictions. The dashboard
enables users to generate diverse and challenging examples by drawing
inspiration from GPT-3 suggestions. Additionally, users can receive feedback
from a trained NLI model on how challenging the newly created example is and
make refinements based on the feedback. Through our investigation, we discover
several categories of spurious correlations that impact the reasoning of NLI
models, which we group into three categories: Semantic Relevance, Logical
Fallacies, and Bias. Based on our findings, we identify and describe various
research opportunities, including diversifying training data and assessing NLI
models' robustness by creating adversarial test suites.",None,-1
fe992074-ca9e-45e0-b2e6-062891cf0c49,PhoGPT: Generative Pre-training for Vietnamese,0.202886,"We open-source a state-of-the-art 4B-parameter generative model series for
Vietnamese, which includes the base pre-trained monolingual model PhoGPT-4B and
its chat variant, PhoGPT-4B-Chat. The base model, PhoGPT-4B, with exactly 3.7B
parameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens,
with an 8192 context length, employing a vocabulary of 20480 token types. The
chat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning
PhoGPT-4B on a dataset of 70K instructional prompts and their responses, along
with an additional 290K conversations. In addition, we also demonstrate its
superior performance compared to previous open-source models. Our PhoGPT models
are available at: https://github.com/VinAIResearch/PhoGPT",None,-1
39e3af23-40b6-4e85-aec4-24757c7e26ea,InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators,0.704249,"Instruction-based language modeling has received significant attention in
pretrained language models. However, the efficiency of instruction engineering
remains low and hinders the development of instruction studies. Recent studies
have focused on automating instruction generation, but they primarily aim to
improve performance without considering other crucial objectives that impact
instruction quality, such as instruction length and perplexity. Therefore, we
propose a novel approach (i.e., InstOptima) that treats instruction generation
as an evolutionary multi-objective optimization problem. In contrast to text
edition-based methods, our approach utilizes a large language model (LLM) to
simulate instruction operators, including mutation and crossover. Furthermore,
we introduce an objective-guided mechanism for these operators, allowing the
LLM to comprehend the objectives and enhance the quality of the generated
instructions. Experimental results demonstrate improved fine-tuning performance
and the generation of a diverse set of high-quality instructions.",None,-1
5ca1d7cf-5106-478c-80c9-3227d07b926d,Event Causality Extraction with Event Argument Correlations,0.720348,"Event Causality Identification (ECI), which aims to detect whether a
causality relation exists between two given textual events, is an important
task for event causality understanding. However, the ECI task ignores crucial
event structure and cause-effect causality component information, making it
struggle for downstream applications. In this paper, we explore a novel task,
namely Event Causality Extraction (ECE), aiming to extract the cause-effect
event causality pairs with their structured event information from plain texts.
The ECE task is more challenging since each event can contain multiple event
arguments, posing fine-grained correlations between events to decide the
causeeffect event pair. Hence, we propose a method with a dual grid tagging
scheme to capture the intra- and inter-event argument correlations for ECE.
Further, we devise a event type-enhanced model architecture to realize the dual
grid tagging scheme. Experiments demonstrate the effectiveness of our method,
and extensive analyses point out several future directions for ECE.",None,-1
413f868c-d1e0-4032-bfba-bd26aedce3a9,Cyclic-Bootstrap Labeling for Weakly Supervised Object Detection,0.375434,"Recent progress in weakly supervised object detection is featured by a
combination of multiple instance detection networks (MIDN) and ordinal online
refinement. However, with only image-level annotation, MIDN inevitably assigns
high scores to some unexpected region proposals when generating pseudo labels.
These inaccurate high-scoring region proposals will mislead the training of
subsequent refinement modules and thus hamper the detection performance. In
this work, we explore how to ameliorate the quality of pseudo-labeling in MIDN.
Formally, we devise Cyclic-Bootstrap Labeling (CBL), a novel weakly supervised
object detection pipeline, which optimizes MIDN with rank information from a
reliable teacher network. Specifically, we obtain this teacher network by
introducing a weighted exponential moving average strategy to take advantage of
various refinement modules. A novel class-specific ranking distillation
algorithm is proposed to leverage the output of weighted ensembled teacher
network for distilling MIDN with rank information. As a result, MIDN is guided
to assign higher scores to accurate proposals among their neighboring ones,
thus benefiting the subsequent pseudo labeling. Extensive experiments on the
prevalent PASCAL VOC 2007 \& 2012 and COCO datasets demonstrate the superior
performance of our CBL framework. Code will be available at
https://github.com/Yinyf0804/WSOD-CBL/.",None,-1
991c6476-8572-4a83-aafd-92cb7c7dbfa5,Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning,0.254425,"Neural MMO 2.0 is a massively multi-agent environment for reinforcement
learning research. The key feature of this new version is a flexible task
system that allows users to define a broad range of objectives and reward
signals. We challenge researchers to train agents capable of generalizing to
tasks, maps, and opponents never seen during training. Neural MMO features
procedurally generated maps with 128 agents in the standard setting and support
for up to. Version 2.0 is a complete rewrite of its predecessor with three-fold
improved performance and compatibility with CleanRL. We release the platform as
free and open-source software with comprehensive documentation available at
neuralmmo.github.io and an active community Discord. To spark initial research
on this new platform, we are concurrently running a competition at NeurIPS
2023.",None,-1
020a297e-a572-40b7-a7ba-984fd5e43658,International Governance of Civilian AI: A Jurisdictional Certification Approach,0.939734,"This report describes trade-offs in the design of international governance
arrangements for civilian artificial intelligence (AI) and presents one
approach in detail. This approach represents the extension of a standards,
licensing, and liability regime to the global level. We propose that states
establish an International AI Organization (IAIO) to certify state
jurisdictions (not firms or AI projects) for compliance with international
oversight standards. States can give force to these international standards by
adopting regulations prohibiting the import of goods whose supply chains embody
AI from non-IAIO-certified jurisdictions. This borrows attributes from models
of existing international organizations, such as the International Civilian
Aviation Organization (ICAO), the International Maritime Organization (IMO),
and the Financial Action Task Force (FATF). States can also adopt multilateral
controls on the export of AI product inputs, such as specialized hardware, to
non-certified jurisdictions. Indeed, both the import and export standards could
be required for certification. As international actors reach consensus on risks
of and minimum standards for advanced AI, a jurisdictional certification regime
could mitigate a broad range of potential harms, including threats to public
safety.",None,-1
b25f1a29-3e74-4d31-8440-b771ca4291f2,Exploration with Principles for Diverse AI Supervision,0.0936138,"Training large transformers using next-token prediction has given rise to
groundbreaking advancements in AI. While this generative AI approach has
produced impressive results, it heavily leans on human supervision. Even
state-of-the-art AI models like ChatGPT depend on fine-tuning through human
demonstrations, demanding extensive human input and domain expertise. This
strong reliance on human oversight poses a significant hurdle to the
advancement of AI innovation. To address this limitation, we propose a novel
paradigm termed Exploratory AI (EAI) aimed at autonomously generating
high-quality training data. Drawing inspiration from unsupervised reinforcement
learning (RL) pretraining, EAI achieves exploration within the natural language
space. We accomplish this by harnessing large language models to assess the
novelty of generated content. Our approach employs two key components: an actor
that generates novel content following exploration principles and a critic that
evaluates the generated content, offering critiques to guide the actor.
Empirical evaluations demonstrate that EAI significantly boosts model
performance on complex reasoning tasks, addressing the limitations of
human-intensive supervision.",None,-1
12e3faa6-06ce-4d2b-8701-54bd9476c8a5,Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting,0.772749,"Automatically generated reports from medical images promise to improve the
workflow of radiologists. Existing methods consider an image-to-report modeling
task by directly generating a fully-fledged report from an image. However, this
conflates the content of the report (e.g., findings and their attributes) with
its style (e.g., format and choice of words), which can lead to clinically
inaccurate reports. To address this, we propose a two-step approach for
radiology report generation. First, we extract the content from an image; then,
we verbalize the extracted content into a report that matches the style of a
specific radiologist. For this, we leverage RadGraph -- a graph representation
of reports -- together with large language models (LLMs). In our quantitative
evaluations, we find that our approach leads to beneficial performance. Our
human evaluation with clinical raters highlights that the AI-generated reports
are indistinguishably tailored to the style of individual radiologist despite
leveraging only a few examples as context.",None,-1
09a8aa55-a520-4da1-9247-dbb53ac26799,Towards Fine-Grained Information: Identifying the Type and Location of Translation Errors,0.279756,"Fine-grained information on translation errors is helpful for the translation
evaluation community. Existing approaches can not synchronously consider error
position and type, failing to integrate the error information of both. In this
paper, we propose Fine-Grained Translation Error Detection (FG-TED) task,
aiming at identifying both the position and the type of translation errors on
given source-hypothesis sentence pairs. Besides, we build an FG-TED model to
predict the \textbf{addition} and \textbf{omission} errors -- two typical
translation accuracy errors. First, we use a word-level classification paradigm
to form our model and use the shortcut learning reduction to relieve the
influence of monolingual features. Besides, we construct synthetic datasets for
model training, and relieve the disagreement of data labeling in authoritative
datasets, making the experimental benchmark concordant. Experiments show that
our model can identify both error type and position concurrently, and gives
state-of-the-art results on the restored dataset. Our model also delivers more
reliable predictions on low-resource and transfer scenarios than existing
baselines. The related datasets and the source code will be released in the
future.",None,-1
757cf691-43db-4921-8fac-335899298882,Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection,0.999987,"In this paper, we propose a long-sequence modeling framework, named
StreamPETR, for multi-view 3D object detection. Built upon the sparse query
design in the PETR series, we systematically develop an object-centric temporal
mechanism. The model is performed in an online manner and the long-term
historical information is propagated through object queries frame by frame.
Besides, we introduce a motion-aware layer normalization to model the movement
of the objects. StreamPETR achieves significant performance improvements only
with negligible computation cost, compared to the single-frame baseline. On the
standard nuScenes benchmark, it is the first online multi-view method that
achieves comparable performance (67.6% NDS & 65.3% AMOTA) with lidar-based
methods. The lightweight version realizes 45.0% mAP and 31.7 FPS, outperforming
the state-of-the-art method (SOLOFusion) by 2.3% mAP and 1.8x faster FPS. Code
has been available at https://github.com/exiawsh/StreamPETR.git.",None,-1
f42fbe1c-5477-4d3c-bc52-cd194a612f59,Multilingual Word Error Rate Estimation: e-WER3,0.595366,"The success of the multilingual automatic speech recognition systems
empowered many voice-driven applications. However, measuring the performance of
such systems remains a major challenge, due to its dependency on manually
transcribed speech data in both mono- and multilingual scenarios. In this
paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on
acoustic and lexical representation to estimate word error rate. We demonstrate
the effectiveness of eWER3 to (i) predict WER without using any internal states
from the ASR and (ii) use the multilingual shared latent space to push the
performance of the close-related languages. We show our proposed multilingual
model outperforms the previous monolingual word error rate estimation method
(eWER2) by an absolute 9\% increase in Pearson correlation coefficient (PCC),
with better overall estimation between the predicted and reference WER.",None,-1
e0264538-cf35-4a74-b27d-dafd13847110,Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains,0.511877,"As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following' by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models'
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENeralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.",None,-1
4c5d6f6d-0d93-4084-8a7c-b8f55ed72493,nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla Sentiment Analysis,0.559276,"In this paper, we discuss the nlpBDpatriots entry to the shared task on
Sentiment Analysis of Bangla Social Media Posts organized at the first workshop
on Bangla Language Processing (BLP) co-located with EMNLP. The main objective
of this task is to identify the polarity of social media content using a Bangla
dataset annotated with positive, neutral, and negative labels provided by the
shared task organizers. Our best system for this task is a transfer learning
approach with data augmentation which achieved a micro F1 score of 0.71. Our
best system ranked 12th among 30 teams that participated in the competition.",None,-1
5ffb6005-0194-46f9-8387-eb92fa897003,Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed Hindi-English Memes,0.946003,"Memes are the new-age conveyance mechanism for humor on social media sites.
Memes often include an image and some text. Memes can be used to promote
disinformation or hatred, thus it is crucial to investigate in details. We
introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other
prevalent datasets in the domain, including prior iterations of Memotion,
Memotion 3 introduces Hindi-English Codemixed memes while prior works in the
area were limited to only the English memes. We describe the Memotion task, the
data collection and the dataset creation methodologies. We also provide a
baseline for the task. The baseline code and dataset will be made available at
https://github.com/Shreyashm16/Memotion-3.0",None,-1
c570768f-2ebe-42a9-977f-ddf75e6de197,MoSFPAD: An end-to-end Ensemble of MobileNet and Support Vector Classifier for Fingerprint Presentation Attack Detection,0.756462,"Automatic fingerprint recognition systems are the most extensively used
systems for person authentication although they are vulnerable to Presentation
attacks. Artificial artifacts created with the help of various materials are
used to deceive these systems causing a threat to the security of
fingerprint-based applications. This paper proposes a novel end-to-end model to
detect fingerprint Presentation attacks. The proposed model incorporates
MobileNet as a feature extractor and a Support Vector Classifier as a
classifier to detect presentation attacks in cross-material and cross-sensor
paradigms. The feature extractor's parameters are learned with the loss
generated by the support vector classifier. The proposed model eliminates the
need for intermediary data preparation procedures, unlike other static hybrid
architectures. The performance of the proposed model has been validated on
benchmark LivDet 2011, 2013, 2015, 2017, and 2019 databases, and overall
accuracy of 98.64%, 99.50%, 97.23%, 95.06%, and 95.20% is achieved on these
databases, respectively. The performance of the proposed model is compared with
state-of-the-art methods and the proposed method outperforms in cross-material
and cross-sensor paradigms in terms of average classification error.",None,-1
3af3d6cd-aa26-4085-9d1b-7879c68cde09,Distracting Downpour: Adversarial Weather Attacks for Motion Estimation,0.460798,"Current adversarial attacks on motion estimation, or optical flow, optimize
small per-pixel perturbations, which are unlikely to appear in the real world.
In contrast, adverse weather conditions constitute a much more realistic threat
scenario. Hence, in this work, we present a novel attack on motion estimation
that exploits adversarially optimized particles to mimic weather effects like
snowflakes, rain streaks or fog clouds. At the core of our attack framework is
a differentiable particle rendering system that integrates particles (i)
consistently over multiple time steps (ii) into the 3D space (iii) with a
photo-realistic appearance. Through optimization, we obtain adversarial weather
that significantly impacts the motion estimation. Surprisingly, methods that
previously showed good robustness towards small per-pixel perturbations are
particularly vulnerable to adversarial weather. At the same time, augmenting
the training with non-optimized weather increases a method's robustness towards
weather effects and improves generalizability at almost no additional cost. Our
code will be available at https://github.com/cv-stuttgart/DistractingDownpour.",None,-1
c2cd6d76-fcda-4845-9906-23fc04d014b7,The Confidence-Competence Gap in Large Language Models: A Cognitive Study,0.0380692,"Large Language Models (LLMs) have acquired ubiquitous attention for their
performances across diverse domains. Our study here searches through LLMs'
cognitive abilities and confidence dynamics. We dive deep into understanding
the alignment between their self-assessed confidence and actual performance. We
exploit these models with diverse sets of questionnaires and real-world
scenarios and extract how LLMs exhibit confidence in their responses. Our
findings reveal intriguing instances where models demonstrate high confidence
even when they answer incorrectly. This is reminiscent of the Dunning-Kruger
effect observed in human psychology. In contrast, there are cases where models
exhibit low confidence with correct answers revealing potential underestimation
biases. Our results underscore the need for a deeper understanding of their
cognitive processes. By examining the nuances of LLMs' self-assessment
mechanism, this investigation provides noteworthy revelations that serve to
advance the functionalities and broaden the potential applications of these
formidable language models.",None,-1
16adf5ab-6244-4688-9d91-dd6a18bb4b83,TwistList: Resources and Baselines for Tongue Twister Generation,0.999934,"Previous work in phonetically-grounded language generation has mainly focused
on domains such as lyrics and poetry. In this paper, we present work on the
generation of tongue twisters - a form of language that is required to be
phonetically conditioned to maximise sound overlap, whilst maintaining semantic
consistency with an input topic, and still being grammatically correct. We
present \textbf{TwistList}, a large annotated dataset of tongue twisters,
consisting of 2.1K+ human-authored examples. We additionally present several
benchmark systems (referred to as TwisterMisters) for the proposed task of
tongue twister generation, including models that both do and do not require
training on in-domain data. We present the results of automatic and human
evaluation to demonstrate the performance of existing mainstream pre-trained
models in this task with limited (or no) task specific training and data, and
no explicit phonetic knowledge. We find that the task of tongue twister
generation is challenging for models under these conditions, yet some models
are still capable of generating acceptable examples of this language type.",None,-1
1af141ec-7db8-47e3-8db0-9e2d89dfa6d5,Recent Advances in Direct Speech-to-text Translation,0.908992,"Recently, speech-to-text translation has attracted more and more attention
and many studies have emerged rapidly. In this paper, we present a
comprehensive survey on direct speech translation aiming to summarize the
current state-of-the-art techniques. First, we categorize the existing research
work into three directions based on the main challenges -- modeling burden,
data scarcity, and application issues. To tackle the problem of modeling
burden, two main structures have been proposed, encoder-decoder framework
(Transformer and the variants) and multitask frameworks. For the challenge of
data scarcity, recent work resorts to many sophisticated techniques, such as
data augmentation, pre-training, knowledge distillation, and multilingual
modeling. We analyze and summarize the application issues, which include
real-time, segmentation, named entity, gender bias, and code-switching.
Finally, we discuss some promising directions for future work.",None,-1
2fa2bd7a-0355-4fea-a2d3-1cd162783cf5,Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning,0.569656,"Clinical note classification is a common clinical NLP task. However,
annotated data-sets are scarse. Prompt-based learning has recently emerged as
an effective method to adapt pre-trained models for text classification using
only few training examples. A critical component of prompt design is the
definition of the template (i.e. prompt text). The effect of template position,
however, has been insufficiently investigated. This seems particularly
important in the clinical setting, where task-relevant information is usually
sparse in clinical notes. In this study we develop a keyword-optimized template
insertion method (KOTI) and show how optimizing position can improve
performance on several clinical tasks in a zero-shot and few-shot training
setting.",None,-1
fbc5e1df-d440-4138-9c6e-ee5b53abe471,LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding,0.616426,"Visually-rich Document Understanding (VrDU) has attracted much research
attention over the past years. Pre-trained models on a large number of document
images with transformer-based backbones have led to significant performance
gains in this field. The major challenge is how to fusion the different
modalities (text, layout, and image) of the documents in a unified model with
different pre-training tasks. This paper focuses on improving text-layout
interactions and proposes a novel multi-modal pre-training model, LayoutMask.
LayoutMask uses local 1D position, instead of global 1D position, as layout
input and has two pre-training objectives: (1) Masked Language Modeling:
predicting masked tokens with two novel masking strategies; (2) Masked Position
Modeling: predicting masked 2D positions to improve layout representation
learning. LayoutMask can enhance the interactions between text and layout
modalities in a unified model and produce adaptive and robust multi-modal
representations for downstream tasks. Experimental results show that our
proposed method can achieve state-of-the-art results on a wide variety of VrDU
problems, including form understanding, receipt understanding, and document
image classification.",None,-1
b54b930b-74af-4b8f-8b7a-65b38598b57f,The Capability of Large Language Models to Measure Psychiatric Functioning,0.994716,"The current work investigates the capability of Large language models (LLMs)
that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2)
to predict psychiatric functioning from patient interviews and clinical
descriptions without being trained to do so. To assess this, n = 145 depression
and n =115 PTSD assessments and n = 46 clinical case studies across high
prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma
and stress, Addictive disorders) were analyzed using prompts to extract
estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is
capable of assessing psychiatric functioning across a range of psychiatric
conditions with the strongest performance being the prediction of depression
scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which
were statistically indistinguishable from human clinical raters t(1,144) =
1.20; p = 0.23. Results show the potential for general clinical language models
to flexibly predict psychiatric risk based on free descriptions of functioning
from both patients and clinicians.",None,-1
3f58e23e-1d7b-4dc5-a823-b3cefc198880,"Industrial Segment Anything -- a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul",0.538005,"Deploying deep learning-based applications in specialized domains like the
aircraft production industry typically suffers from the training data
availability problem. Only a few datasets represent non-everyday objects,
situations, and tasks. Recent advantages in research around Vision Foundation
Models (VFM) opened a new area of tasks and models with high generalization
capabilities in non-semantic and semantic predictions. As recently demonstrated
by the Segment Anything Project, exploiting VFM's zero-shot capabilities is a
promising direction in tackling the boundaries spanned by data, context, and
sensor variety. Although, investigating its application within specific domains
is subject to ongoing research. This paper contributes here by surveying
applications of the SAM in aircraft production-specific use cases. We include
manufacturing, intralogistics, as well as maintenance, repair, and overhaul
processes, also representing a variety of other neighboring industrial domains.
Besides presenting the various use cases, we further discuss the injection of
domain knowledge.",None,-1
cf9e5bc2-bb9c-4ccc-ae4f-4a4df22a51d6,Vision Transformer for Action Units Detection,0.963505,"Facial Action Units detection (FAUs) represents a fine-grained classification
problem that involves identifying different units on the human face, as defined
by the Facial Action Coding System. In this paper, we present a simple yet
efficient Vision Transformer-based approach for addressing the task of Action
Units (AU) detection in the context of Affective Behavior Analysis in-the-wild
(ABAW) competition. We employ the Video Vision Transformer(ViViT) Network to
capture the temporal facial change in the video. Besides, to reduce massive
size of the Vision Transformers model, we replace the ViViT feature extraction
layers with the CNN backbone (Regnet). Our model outperform the baseline model
of ABAW 2023 challenge, with a notable 14% difference in result. Furthermore,
the achieved results are comparable to those of the top three teams in the
previous ABAW 2022 challenge.",None,-1
6637aaf7-46e6-4bf0-8b5a-84fe609143ba,Orca 2: Teaching Small Language Models How to Reason,0.846302,"Orca 1 learns from rich signals, such as explanation traces, allowing it to
outperform conventional instruction-tuned models on benchmarks like BigBench
Hard and AGIEval. In Orca 2, we continue exploring how improved training
signals can enhance smaller LMs' reasoning abilities. Research on training
small LMs has often relied on imitation learning to replicate the output of
more capable models. We contend that excessive emphasis on imitation may
restrict the potential of smaller models. We seek to teach small LMs to employ
different solution strategies for different tasks, potentially different from
the one used by the larger model. For example, while larger models might
provide a direct answer to a complex task, smaller models may not have the same
capacity. In Orca 2, we teach the model various reasoning techniques
(step-by-step, recall then generate, recall-reason-generate, direct answer,
etc.). More crucially, we aim to help the model learn to determine the most
effective solution strategy for each task. We evaluate Orca 2 using a
comprehensive set of 15 diverse benchmarks (corresponding to approximately 100
tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of
similar size and attains performance levels similar or better to those of
models 5-10x larger, as assessed on complex tasks that test advanced reasoning
abilities in zero-shot settings. make Orca 2 weights publicly available at
aka.ms/orca-lm to support research on the development, evaluation, and
alignment of smaller LMs",None,-1
ab23df95-9ce9-4b53-aa5a-2933ab8a4590,Using Text Injection to Improve Recognition of Personal Identifiers in Speech,0.678188,"Accurate recognition of specific categories, such as persons' names, dates or
other identifiers is critical in many Automatic Speech Recognition (ASR)
applications. As these categories represent personal information, ethical use
of this data including collection, transcription, training and evaluation
demands special care. One way of ensuring the security and privacy of
individuals is to redact or eliminate Personally Identifiable Information (PII)
from collection altogether. However, this results in ASR models that tend to
have lower recognition accuracy of these categories. We use text-injection to
improve the recognition of PII categories by including fake textual substitutes
of PII categories in the training data using a text injection method. We
demonstrate substantial improvement to Recall of Names and Dates in medical
notes while improving overall WER. For alphanumeric digit sequences we show
improvements to Character Error Rate and Sentence Accuracy.",None,-1
381acd67-de1a-41c4-be2a-758b2130b511,Uncertainty-guided Boundary Learning for Imbalanced Social Event Detection,0.379843,"Real-world social events typically exhibit a severe class-imbalance
distribution, which makes the trained detection model encounter a serious
generalization challenge. Most studies solve this problem from the frequency
perspective and emphasize the representation or classifier learning for tail
classes. While in our observation, compared to the rarity of classes, the
calibrated uncertainty estimated from well-trained evidential deep learning
networks better reflects model performance. To this end, we propose a novel
uncertainty-guided class imbalance learning framework - UCL$_{SED}$, and its
variant - UCL-EC$_{SED}$, for imbalanced social event detection tasks. We aim
to improve the overall model performance by enhancing model generalization to
those uncertain classes. Considering performance degradation usually comes from
misclassifying samples as their confusing neighboring classes, we focus on
boundary learning in latent space and classifier learning with high-quality
uncertainty estimation. First, we design a novel uncertainty-guided contrastive
learning loss, namely UCL and its variant - UCL-EC, to manipulate
distinguishable representation distribution for imbalanced data. During
training, they force all classes, especially uncertain ones, to adaptively
adjust a clear separable boundary in the feature space. Second, to obtain more
robust and accurate class uncertainty, we combine the results of multi-view
evidential classifiers via the Dempster-Shafer theory under the supervision of
an additional calibration method. We conduct experiments on three severely
imbalanced social event datasets including Events2012\_100, Events2018\_100,
and CrisisLexT\_7. Our model significantly improves social event representation
and classification tasks in almost all classes, especially those uncertain
ones.",None,-1
31e4337e-25e8-4e28-bdfb-298795db5ae2,UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video,0.404555,"We show how to build a model that allows realistic, free-viewpoint renderings
of a scene under novel lighting conditions from video. Our method -- UrbanIR:
Urban Scene Inverse Rendering -- computes an inverse graphics representation
from the video. UrbanIR jointly infers shape, albedo, visibility, and sun and
sky illumination from a single video of unbounded outdoor scenes with unknown
lighting. UrbanIR uses videos from cameras mounted on cars (in contrast to many
views of the same points in typical NeRF-style estimation). As a result,
standard methods produce poor geometry estimates (for example, roofs), and
there are numerous ''floaters''. Errors in inverse graphics inference can
result in strong rendering artifacts. UrbanIR uses novel losses to control
these and other sources of error. UrbanIR uses a novel loss to make very good
estimates of shadow volumes in the original scene. The resulting
representations facilitate controllable editing, delivering photorealistic
free-viewpoint renderings of relit scenes and inserted objects. Qualitative
evaluation demonstrates strong improvements over the state-of-the-art.",None,-1
8a851c61-cd29-49a2-af61-0ca411479a84,Give and Take: Federated Transfer Learning for Industrial IoT Network Intrusion Detection,0.958766,"The rapid growth in Internet of Things (IoT) technology has become an
integral part of today's industries forming the Industrial IoT (IIoT)
initiative, where industries are leveraging IoT to improve communication and
connectivity via emerging solutions like data analytics and cloud computing.
Unfortunately, the rapid use of IoT has made it an attractive target for
cybercriminals. Therefore, protecting these systems is of utmost importance. In
this paper, we propose a federated transfer learning (FTL) approach to perform
IIoT network intrusion detection. As part of the research, we also propose a
combinational neural network as the centerpiece for performing FTL. The
proposed technique splits IoT data between the client and server devices to
generate corresponding models, and the weights of the client models are
combined to update the server model. Results showcase high performance for the
FTL setup between iterations on both the IIoT clients and the server.
Additionally, the proposed FTL setup achieves better overall performance than
contemporary machine learning algorithms at performing network intrusion
detection.",None,-1
15388d24-49f8-457e-bee3-128304274958,A Computational Analysis of Vagueness in Revisions of Instructional Texts,0.694919,"WikiHow is an open-domain repository of instructional articles for a variety
of tasks, which can be revised by users. In this paper, we extract pairwise
versions of an instruction before and after a revision was made. Starting from
a noisy dataset of revision histories, we specifically extract and analyze
edits that involve cases of vagueness in instructions. We further investigate
the ability of a neural model to distinguish between two versions of an
instruction in our data by adopting a pairwise ranking task from previous work
and showing improvements over existing baselines.",None,-1
2c8223b6-e56e-4851-8a47-8125eb8bda61,Label-Aware Automatic Verbalizer for Few-Shot Text Classification,0.102669,"Prompt-based learning has shown its effectiveness in few-shot text
classification. One important factor in its success is a verbalizer, which
translates output from a language model into a predicted class. Notably, the
simplest and widely acknowledged verbalizer employs manual labels to represent
the classes. However, manual selection does not guarantee the optimality of the
selected words when conditioned on the chosen language model. Therefore, we
propose Label-Aware Automatic Verbalizer (LAAV), effectively augmenting the
manual labels to achieve better few-shot classification results. Specifically,
we use the manual labels along with the conjunction ""and"" to induce the model
to generate more effective words for the verbalizer. The experimental results
on five datasets across five languages demonstrate that LAAV significantly
outperforms existing verbalizers. Furthermore, our analysis reveals that LAAV
suggests more relevant words compared to similar approaches, especially in
mid-to-low resource languages.",None,-1
4015f858-7ecc-4918-894e-fc73b4b60e71,Harmonizing Flows: Unsupervised MR harmonization based on normalizing flows,0.28845,"In this paper, we propose an unsupervised framework based on normalizing
flows that harmonizes MR images to mimic the distribution of the source domain.
The proposed framework consists of three steps. First, a shallow harmonizer
network is trained to recover images of the source domain from their augmented
versions. A normalizing flow network is then trained to learn the distribution
of the source domain. Finally, at test time, a harmonizer network is modified
so that the output images match the source domain's distribution learned by the
normalizing flow model. Our unsupervised, source-free and task-independent
approach is evaluated on cross-domain brain MRI segmentation using data from
four different sites. Results demonstrate its superior performance compared to
existing methods.",None,-1
14188bfa-3f84-4e41-a6fa-7cc511b5a0fe,POE: Process of Elimination for Multiple Choice Reasoning,0.210702,"Language models (LMs) are capable of conducting in-context learning for
multiple choice reasoning tasks, but the options in these tasks are treated
equally. As humans often first eliminate wrong options before picking the final
correct answer, we argue a similar two-step strategy can make LMs better at
these tasks. To this end, we present the Process of Elimination (POE), a
two-step scoring method. In the first step, POE scores each option, and
eliminates seemingly wrong options. In the second step, POE masks these wrong
options, and makes the final prediction from the remaining options. Zero-shot
experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a
following analysis finds our method to be especially performant on logical
reasoning tasks. We further analyze the effect of masks, and show that POE
applies to few-shot settings and large language models (LLMs) like ChatGPT.",None,-1
2f3be74c-3a2c-48e4-97ed-79cef113010b,Reducing Discretization Error in the Frank-Wolfe Method,0.113771,"The Frank-Wolfe algorithm is a popular method in structurally constrained
machine learning applications, due to its fast per-iteration complexity.
However, one major limitation of the method is a slow rate of convergence that
is difficult to accelerate due to erratic, zig-zagging step directions, even
asymptotically close to the solution. We view this as an artifact of
discretization; that is to say, the Frank-Wolfe \emph{flow}, which is its
trajectory at asymptotically small step sizes, does not zig-zag, and reducing
discretization error will go hand-in-hand in producing a more stabilized
method, with better convergence properties. We propose two improvements: a
multistep Frank-Wolfe method that directly applies optimized higher-order
discretization schemes; and an LMO-averaging scheme with reduced discretization
error, and whose local convergence rate over general convex sets accelerates
from a rate of $O(1/k)$ to up to $O(1/k^{3/2})$.",None,-1
327ec9f8-10c0-4409-ac7d-5d090ab22af0,Token-Scaled Logit Distillation for Ternary Weight Generative Language Models,0.259429,"Generative Language Models (GLMs) have shown impressive performance in tasks
such as text generation, understanding, and reasoning. However, the large model
size poses challenges for practical deployment. To solve this problem,
Quantization-Aware Training (QAT) has become increasingly popular. However,
current QAT methods for generative models have resulted in a noticeable loss of
accuracy. To counteract this issue, we propose a novel knowledge distillation
method specifically designed for GLMs. Our method, called token-scaled logit
distillation, prevents overfitting and provides superior learning from the
teacher model and ground truth. This research marks the first evaluation of
ternary weight quantization-aware training of large-scale GLMs with less than
1.0 degradation in perplexity and achieves enhanced accuracy in tasks like
common-sense QA and arithmetic reasoning as well as natural language
understanding. Our code is available at https://github.com/aiha-lab/TSLD.",None,-1
c73356a8-f1b3-4a5d-be17-34ec95f3ed3e,Empirical study of the modulus as activation function in computer vision applications,0.509669,"In this work we propose a new non-monotonic activation function: the modulus.
The majority of the reported research on nonlinearities is focused on monotonic
functions. We empirically demonstrate how by using the modulus activation
function on computer vision tasks the models generalize better than with other
nonlinearities - up to a 15% accuracy increase in CIFAR100 and 4% in CIFAR10,
relative to the best of the benchmark activations tested. With the proposed
activation function the vanishing gradient and dying neurons problems
disappear, because the derivative of the activation function is always 1 or -1.
The simplicity of the proposed function and its derivative make this solution
specially suitable for TinyML and hardware applications.",None,-1
259d6181-e9e1-4178-95bc-2a5479743a8b,Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning,0.436515,"How can we perform computations over natural language representations to
solve tasks that require symbolic and numeric reasoning? We propose natural
language embedded programs (NLEP) as a unifying framework for addressing
math/symbolic reasoning, natural language understanding, and instruction
following tasks. Our approach prompts a language model to generate full Python
programs that define functions over data structures which contain natural
language representations of structured knowledge. A Python interpreter then
executes the generated code and prints the output. Despite using a task-general
prompt, we find that this approach can improve upon strong baselines across a
range of different tasks including math and symbolic reasoning, text
classification, question answering, and instruction following. We found that
the generated programs are interpretable since they outline the exact reasoning
process followed by the program interpreter.",None,-1
516fa994-9136-4539-abaa-b5ec7c35c491,OpenContrails: Benchmarking Contrail Detection on GOES-16 ABI,0.27567,"Contrails (condensation trails) are line-shaped ice clouds caused by aircraft
and are likely the largest contributor of aviation-induced climate change.
Contrail avoidance is potentially an inexpensive way to significantly reduce
the climate impact of aviation. An automated contrail detection system is an
essential tool to develop and evaluate contrail avoidance systems. In this
paper, we present a human-labeled dataset named OpenContrails to train and
evaluate contrail detection models based on GOES-16 Advanced Baseline Imager
(ABI) data. We propose and evaluate a contrail detection model that
incorporates temporal context for improved detection accuracy. The human
labeled dataset and the contrail detection outputs are publicly available on
Google Cloud Storage at gs://goes_contrails_dataset.",None,-1
ad66f9b5-181e-479d-a106-f63d0f5af6cb,ODOR: The ICPR2022 ODeuropa Challenge on Olfactory Object Recognition,0.293608,"The Odeuropa Challenge on Olfactory Object Recognition aims to foster the
development of object detection in the visual arts and to promote an olfactory
perspective on digital heritage. Object detection in historical artworks is
particularly challenging due to varying styles and artistic periods. Moreover,
the task is complicated due to the particularity and historical variance of
predefined target objects, which exhibit a large intra-class variance, and the
long tail distribution of the dataset labels, with some objects having only
very few training examples. These challenges should encourage participants to
create innovative approaches using domain adaptation or few-shot learning. We
provide a dataset of 2647 artworks annotated with 20 120 tightly fit bounding
boxes that are split into a training and validation set (public). A test set
containing 1140 artworks and 15 480 annotations is kept private for the
challenge evaluation.",None,-1
d141b1cf-4a46-4648-8fb2-ed23fccbee07,CitySpec with Shield: A Secure Intelligent Assistant for Requirement Formalization,0.168808,"An increasing number of monitoring systems have been developed in smart
cities to ensure that the real-time operations of a city satisfy safety and
performance requirements. However, many existing city requirements are written
in English with missing, inaccurate, or ambiguous information. There is a high
demand for assisting city policymakers in converting human-specified
requirements to machine-understandable formal specifications for monitoring
systems. To tackle this limitation, we build CitySpec, the first intelligent
assistant system for requirement specification in smart cities. To create
CitySpec, we first collect over 1,500 real-world city requirements across
different domains (e.g., transportation and energy) from over 100 cities and
extract city-specific knowledge to generate a dataset of city vocabulary with
3,061 words. We also build a translation model and enhance it through
requirement synthesis and develop a novel online learning framework with
shielded validation. The evaluation results on real-world city requirements
show that CitySpec increases the sentence-level accuracy of requirement
specification from 59.02% to 86.64%, and has strong adaptability to a new city
and a new domain (e.g., the F1 score for requirements in Seattle increases from
77.6% to 93.75% with online learning). After the enhancement from the shield
function, CitySpec is now immune to most known textual adversarial inputs
(e.g., the attack success rate of DeepWordBug after the shield function is
reduced to 0% from 82.73%). We test the CitySpec with 18 participants from
different domains. CitySpec shows its strong usability and adaptability to
different domains, and also its robustness to malicious inputs.",None,-1
dd02a26a-e017-43c3-99b8-f2ac182506ce,Dynamic Scenario Representation Learning for Motion Forecasting with Heterogeneous Graph Convolutional Recurrent Networks,0.420024,"Due to the complex and changing interactions in dynamic scenarios, motion
forecasting is a challenging problem in autonomous driving. Most existing works
exploit static road graphs to characterize scenarios and are limited in
modeling evolving spatio-temporal dependencies in dynamic scenarios. In this
paper, we resort to dynamic heterogeneous graphs to model the scenario. Various
scenario components including vehicles (agents) and lanes, multi-type
interactions, and their changes over time are jointly encoded. Furthermore, we
design a novel heterogeneous graph convolutional recurrent network, aggregating
diverse interaction information and capturing their evolution, to learn to
exploit intrinsic spatio-temporal dependencies in dynamic graphs and obtain
effective representations of dynamic scenarios. Finally, with a motion
forecasting decoder, our model predicts realistic and multi-modal future
trajectories of agents and outperforms state-of-the-art published works on
several motion forecasting benchmarks.",None,-1
720c0ce8-3498-4fc9-b60c-0d263941eb7d,Deep Integrated Explanations,0.198232,"This paper presents Deep Integrated Explanations (DIX) - a universal method
for explaining vision models. DIX generates explanation maps by integrating
information from the intermediate representations of the model, coupled with
their corresponding gradients. Through an extensive array of both objective and
subjective evaluations spanning diverse tasks, datasets, and model
configurations, we showcase the efficacy of DIX in generating faithful and
accurate explanation maps, while surpassing current state-of-the-art methods.",None,-1
4350cd85-ed64-48f4-8f5a-1bae761f0547,Long Horizon Temperature Scaling,0.568984,"Temperature scaling is a popular technique for tuning the sharpness of a
model distribution. It is used extensively for sampling likely generations and
calibrating model uncertainty, and even features as a controllable parameter to
many large language models in deployment. However, autoregressive models rely
on myopic temperature scaling that greedily optimizes the next token. To
address this, we propose Long Horizon Temperature Scaling (LHTS), a novel
approach for sampling from temperature-scaled joint distributions. LHTS is
compatible with all likelihood-based models, and optimizes for the long horizon
likelihood of samples. We derive a temperature-dependent LHTS objective, and
show that finetuning a model on a range of temperatures produces a single model
capable of generation with a controllable long horizon temperature parameter.
We experiment with LHTS on image diffusion models and character/language
autoregressive models, demonstrating advantages over myopic temperature scaling
in likelihood and sample quality, and showing improvements in accuracy on a
multiple choice analogy task by $10\%$.",None,-1
1bdda410-c25c-4301-9a06-2647c97b6e04,Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach,0.313559,"Large language models (LLMs) encode a vast amount of world knowledge acquired
from massive text datasets. Recent studies have demonstrated that LLMs can
assist an embodied agent in solving complex sequential decision making tasks by
providing high-level instructions. However, interactions with LLMs can be
time-consuming. In many practical scenarios, it requires a significant amount
of storage space that can only be deployed on remote cloud servers.
Additionally, using commercial LLMs can be costly since they may charge based
on usage frequency. In this paper, we explore how to enable intelligent
cost-effective interactions between a down stream task oriented agent and an
LLM. We find that this problem can be naturally formulated by a Markov decision
process (MDP), and propose When2Ask, a reinforcement learning based approach
that learns when it is necessary to query LLMs for high-level instructions to
accomplish a target task. On one side, When2Ask discourages unnecessary
redundant interactions, while on the other side, it enables the agent to
identify and follow useful instructions from the LLM. This enables the agent to
halt an ongoing plan and transition to a more suitable one based on new
environmental observations. Experiments on MiniGrid and Habitat environments
that entail planning sub-goals demonstrate that When2Ask learns to solve target
tasks with only a few necessary interactions with the LLM, significantly
reducing interaction costs in testing environments compared with baseline
methods. Our code is available at: https://github.com/ZJLAB-AMMI/LLM4RL.",None,-1
57aa06f7-db25-439b-afcf-9d48c43c13d0,Zero-Shot Co-salient Object Detection Framework,0.777138,"Co-salient Object Detection (CoSOD) endeavors to replicate the human visual
system's capacity to recognize common and salient objects within a collection
of images. Despite recent advancements in deep learning models, these models
still rely on training with well-annotated CoSOD datasets. The exploration of
training-free zero-shot CoSOD frameworks has been limited. In this paper,
taking inspiration from the zero-shot transfer capabilities of foundational
computer vision models, we introduce the first zero-shot CoSOD framework that
harnesses these models without any training process. To achieve this, we
introduce two novel components in our proposed framework: the group prompt
generation (GPG) module and the co-saliency map generation (CMP) module. We
evaluate the framework's performance on widely-used datasets and observe
impressive results. Our approach surpasses existing unsupervised methods and
even outperforms fully supervised methods developed before 2020, while
remaining competitive with some fully supervised methods developed before 2022.",None,-1
4e2090ab-2e70-4d84-9a40-9b8bc6e88787,Bridging the Transparency Gap: What Can Explainable AI Learn From the AI Act?,0.341543,"The European Union has proposed the Artificial Intelligence Act which
introduces detailed requirements of transparency for AI systems. Many of these
requirements can be addressed by the field of explainable AI (XAI), however,
there is a fundamental difference between XAI and the Act regarding what
transparency is. The Act views transparency as a means that supports wider
values, such as accountability, human rights, and sustainable innovation. In
contrast, XAI views transparency narrowly as an end in itself, focusing on
explaining complex algorithmic properties without considering the
socio-technical context. We call this difference the ``transparency gap''.
Failing to address the transparency gap, XAI risks leaving a range of
transparency issues unaddressed. To begin to bridge this gap, we overview and
clarify the terminology of how XAI and European regulation -- the Act and the
related General Data Protection Regulation (GDPR) -- view basic definitions of
transparency. By comparing the disparate views of XAI and regulation, we arrive
at four axes where practical work could bridge the transparency gap: defining
the scope of transparency, clarifying the legal status of XAI, addressing
issues with conformity assessment, and building explainability for datasets.",None,-1
87a904e1-08a9-4c28-86da-3053700278c8,W2SAT: Learning to generate SAT instances from Weighted Literal Incidence Graphs,0.384066,"The Boolean Satisfiability (SAT) problem stands out as an attractive
NP-complete problem in theoretic computer science and plays a central role in a
broad spectrum of computing-related applications. Exploiting and tuning SAT
solvers under numerous scenarios require massive high-quality industry-level
SAT instances, which unfortunately are quite limited in the real world. To
address the data insufficiency issue, in this paper, we propose W2SAT, a
framework to generate SAT formulas by learning intrinsic structures and
properties from given real-world/industrial instances in an implicit fashion.
To this end, we introduce a novel SAT representation called Weighted Literal
Incidence Graph (WLIG), which exhibits strong representation ability and
generalizability against existing counterparts, and can be efficiently
generated via a specialized learning-based graph generative model. Decoding
from WLIGs into SAT problems is then modeled as finding overlapping cliques
with a novel hill-climbing optimization method termed Optimal Weight Coverage
(OWC). Experiments demonstrate the superiority of our WLIG-induced approach in
terms of graph metrics, efficiency, and scalability in comparison to previous
methods. Additionally, we discuss the limitations of graph-based SAT generation
for real-world applications, especially when utilizing generated instances for
SAT solver parameter-tuning, and pose some potential directions.",None,-1
63c8c4f8-f25d-4969-aadb-d98939251abc,VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style Transfer,0.783639,"Current talking face generation methods mainly focus on speech-lip
synchronization. However, insufficient investigation on the facial talking
style leads to a lifeless and monotonous avatar. Most previous works fail to
imitate expressive styles from arbitrary video prompts and ensure the
authenticity of the generated video. This paper proposes an unsupervised
variational style transfer model (VAST) to vivify the neutral photo-realistic
avatars. Our model consists of three key components: a style encoder that
extracts facial style representations from the given video prompts; a hybrid
facial expression decoder to model accurate speech-related movements; a
variational style enhancer that enhances the style space to be highly
expressive and meaningful. With our essential designs on facial style learning,
our model is able to flexibly capture the expressive facial style from
arbitrary video prompts and transfer it onto a personalized image renderer in a
zero-shot manner. Experimental results demonstrate the proposed approach
contributes to a more vivid talking avatar with higher authenticity and richer
expressiveness.",None,-1
e047e2e1-5655-41a6-8b96-8b37bafd1f11,Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts,0.50325,"Recent studies have demonstrated that natural-language prompts can help to
leverage the knowledge learned by pre-trained language models for the binary
sentence-level sentiment classification task. Specifically, these methods
utilize few-shot learning settings to fine-tune the sentiment classification
model using manual or automatically generated prompts. However, the performance
of these methods is sensitive to the perturbations of the utilized prompts.
Furthermore, these methods depend on a few labeled instances for automatic
prompt generation and prompt ranking. This study aims to find high-quality
prompts for the given task in a zero-shot setting. Given a base prompt, our
proposed approach automatically generates multiple prompts similar to the base
prompt employing positional, reasoning, and paraphrasing techniques and then
ranks the prompts using a novel metric. We empirically demonstrate that the
top-ranked prompts are high-quality and significantly outperform the base
prompt and the prompts generated using few-shot learning for the binary
sentence-level sentiment classification task.",None,-1
b9e23330-9182-48aa-a797-3077b019873d,Imbalanced Node Classification Beyond Homophilic Assumption,0.517457,"Imbalanced node classification widely exists in real-world networks where
graph neural networks (GNNs) are usually highly inclined to majority classes
and suffer from severe performance degradation on classifying minority class
nodes. Various imbalanced node classification methods have been proposed
recently which construct synthetic nodes and edges w.r.t. minority classes to
balance the label and topology distribution. However, they are all based on the
homophilic assumption that nodes of the same label tend to connect despite the
wide existence of heterophilic edges in real-world graphs. Thus, they uniformly
aggregate features from both homophilic and heterophilic neighbors and rely on
feature similarity to generate synthetic edges, which cannot be applied to
imbalanced graphs in high heterophily. To address this problem, we propose a
novel GraphSANN for imbalanced node classification on both homophilic and
heterophilic graphs. Firstly, we propose a unified feature mixer to generate
synthetic nodes with both homophilic and heterophilic interpolation in a
unified way. Next, by randomly sampling edges between synthetic nodes and
existing nodes as candidate edges, we design an adaptive subgraph extractor to
adaptively extract the contextual subgraphs of candidate edges with flexible
ranges. Finally, we develop a multi-filter subgraph encoder that constructs
different filter channels to discriminatively aggregate neighbor's information
along the homophilic and heterophilic edges. Extensive experiments on eight
datasets demonstrate the superiority of our model for imbalanced node
classification on both homophilic and heterophilic graphs.",None,-1
ed20dd08-0faa-41c1-965b-eca7d02e66a8,Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning,0.637571,"To advance argumentative stance prediction as a multimodal problem, the First
Shared Task in Multimodal Argument Mining hosted stance prediction in crucial
social topics of gun control and abortion. Our exploratory study attempts to
evaluate the necessity of images for stance prediction in tweets and compare
out-of-the-box text-based large-language models (LLM) in few-shot settings
against fine-tuned unimodal and multimodal models. Our work suggests an
ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms
both the multimodal (0.677 F1-score) and text-based few-shot prediction using a
recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in
performance, our findings suggest that the multimodal models tend to perform
better when image content is summarized as natural language over their native
pixel structure and, using in-context examples improves few-shot performance of
LLMs.",None,-1
e1b108fe-4943-45c8-bf80-ccf7096c8f40,Optimizing delegation between human and AI collaborative agents,0.594539,"In the context of humans operating with artificial or autonomous agents in a
hybrid team, it is essential to accurately identify when to authorize those
team members to perform actions. Given past examples where humans and
autonomous systems can either succeed or fail at tasks, we seek to train a
delegating manager agent to make delegation decisions with respect to these
potential performance deficiencies. Additionally, we cannot always expect the
various agents to operate within the same underlying model of the environment.
It is possible to encounter cases where the actions and transitions would vary
between agents. Therefore, our framework provides a manager model which learns
through observations of team performance without restricting agents to matching
dynamics. Our results show our manager learns to perform delegation decisions
with teams of agents operating under differing representations of the
environment, significantly outperforming alternative methods to manage the
team.",None,-1
1f46ab76-1a9d-454a-b927-a78ab2fd8f84,Intelligent System for Assessing University Student Personality Development and Career Readiness,0.0593644,"While academic metrics such as transcripts and GPA are commonly used to
evaluate students' knowledge acquisition, there is a lack of comprehensive
metrics to measure their preparedness for the challenges of post-graduation
life. This research paper explores the impact of various factors on university
students' readiness for change and transition, with a focus on their
preparedness for careers. The methodology employed in this study involves
designing a survey based on Paul J. Mayer's ""The Balance Wheel"" to capture
students' sentiments on various life aspects, including satisfaction with the
educational process and expectations of salary. The collected data from a KBTU
student survey (n=47) were processed through machine learning models: Linear
Regression, Support Vector Regression (SVR), Random Forest Regression.
Subsequently, an intelligent system was built using these models and fuzzy
sets. The system is capable of evaluating graduates' readiness for their future
careers and demonstrates a high predictive power. The findings of this research
have practical implications for educational institutions. Such an intelligent
system can serve as a valuable tool for universities to assess and enhance
students' preparedness for post-graduation challenges. By recognizing the
factors contributing to students' readiness for change, universities can refine
curricula and processes to better prepare students for their career journeys.",None,-1
b8e5cbeb-0369-4028-b98f-31a6dfd80484,An Image Processing Pipeline for Autonomous Deep-Space Optical Navigation,0.440582,"A new era of space exploration and exploitation is fast approaching. A
multitude of spacecraft will flow in the future decades under the propulsive
momentum of the new space economy. Yet, the flourishing proliferation of
deep-space assets will make it unsustainable to pilot them from ground with
standard radiometric tracking. The adoption of autonomous navigation
alternatives is crucial to overcoming these limitations. Among these, optical
navigation is an affordable and fully ground-independent approach. Probes can
triangulate their position by observing visible beacons, e.g., planets or
asteroids, by acquiring their line-of-sight in deep space. To do so, developing
efficient and robust image processing algorithms providing information to
navigation filters is a necessary action. This paper proposes an innovative
pipeline for unresolved beacon recognition and line-of-sight extraction from
images for autonomous interplanetary navigation. The developed algorithm
exploits the k-vector method for the non-stellar object identification and
statistical likelihood to detect whether any beacon projection is visible in
the image. Statistical results show that the accuracy in detecting the planet
position projection is independent of the spacecraft position uncertainty.
Whereas, the planet detection success rate is higher than 95% when the
spacecraft position is known with a 3sigma accuracy up to 10^5 km.",None,-1
3a19c674-bb1d-42c6-b0bb-22697abf95a7,Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks,0.605689,"Large Language Models (LLMs) have demonstrated impressive text generation
capabilities, prompting us to reconsider the future of human-AI co-creation and
how humans interact with LLMs. In this paper, we present a spectrum of content
generation tasks and their corresponding human-AI interaction patterns. These
tasks include: 1) fixed-scope content curation tasks with minimal human-AI
interactions, 2) independent creative tasks with precise human-AI interactions,
and 3) complex and interdependent creative tasks with iterative human-AI
interactions. We encourage the generative AI and HCI research communities to
focus on the more complex and interdependent tasks, which require greater
levels of human involvement.",None,-1
d31f071b-a10f-433a-b141-6f0aacf9e52b,Efficient Computation of Counterfactual Bounds,0.27069,"We assume to be given structural equations over discrete variables inducing a
directed acyclic graph, namely, a structural causal model, together with data
about its internal nodes. The question we want to answer is how we can compute
bounds for partially identifiable counterfactual queries from such an input. We
start by giving a map from structural casual models to credal networks. This
allows us to compute exact counterfactual bounds via algorithms for credal nets
on a subclass of structural causal models. Exact computation is going to be
inefficient in general given that, as we show, causal inference is NP-hard even
on polytrees. We target then approximate bounds via a causal EM scheme. We
evaluate their accuracy by providing credible intervals on the quality of the
approximation; we show through a synthetic benchmark that the EM scheme
delivers accurate results in a fair number of runs. In the course of the
discussion, we also point out what seems to be a neglected limitation to the
trending idea that counterfactual bounds can be computed without knowledge of
the structural equations. We also present a real case study on palliative care
to show how our algorithms can readily be used for practical purposes.",None,-1
1abbd963-e043-4012-9bf4-b7ab317d3c0c,HistRED: A Historical Document-Level Relation Extraction Dataset,0.740519,"Despite the extensive applications of relation extraction (RE) tasks in
various domains, little has been explored in the historical context, which
contains promising data across hundreds and thousands of years. To promote the
historical RE research, we present HistRED constructed from Yeonhaengnok.
Yeonhaengnok is a collection of records originally written in Hanja, the
classical Chinese writing, which has later been translated into Korean. HistRED
provides bilingual annotations such that RE can be performed on Korean and
Hanja texts. In addition, HistRED supports various self-contained subtexts with
different lengths, from a sentence level to a document level, supporting
diverse context settings for researchers to evaluate the robustness of their RE
models. To demonstrate the usefulness of our dataset, we propose a bilingual RE
model that leverages both Korean and Hanja contexts to predict relations
between entities. Our model outperforms monolingual baselines on HistRED,
showing that employing multiple language contexts supplements the RE
predictions. The dataset is publicly available at:
https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.",None,-1
3907b192-b092-4185-b7c7-643ca14e5255,PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment,0.986745,"Camera pose estimation is a long-standing computer vision problem that to
date often relies on classical methods, such as handcrafted keypoint matching,
RANSAC and bundle adjustment. In this paper, we propose to formulate the
Structure from Motion (SfM) problem inside a probabilistic diffusion framework,
modelling the conditional distribution of camera poses given input images. This
novel view of an old problem has several advantages. (i) The nature of the
diffusion framework mirrors the iterative procedure of bundle adjustment. (ii)
The formulation allows a seamless integration of geometric constraints from
epipolar geometry. (iii) It excels in typically difficult scenarios such as
sparse views with wide baselines. (iv) The method can predict intrinsics and
extrinsics for an arbitrary amount of images. We demonstrate that our method
PoseDiffusion significantly improves over the classic SfM pipelines and the
learned approaches on two real-world datasets. Finally, it is observed that our
method can generalize across datasets without further training. Project page:
https://posediffusion.github.io/",None,-1
ec28b16e-8fba-4dbf-8ee6-9fc22c934876,Human Pose as Compositional Tokens,0.892381,"Human pose is typically represented by a coordinate vector of body joints or
their heatmap embeddings. While easy for data processing, unrealistic pose
estimates are admitted due to the lack of dependency modeling between the body
joints. In this paper, we present a structured representation, named Pose as
Compositional Tokens (PCT), to explore the joint dependency. It represents a
pose by M discrete tokens with each characterizing a sub-structure with several
interdependent joints. The compositional design enables it to achieve a small
reconstruction error at a low cost. Then we cast pose estimation as a
classification task. In particular, we learn a classifier to predict the
categories of the M tokens from an image. A pre-learned decoder network is used
to recover the pose from the tokens without further post-processing. We show
that it achieves better or comparable pose estimation results as the existing
methods in general scenarios, yet continues to work well when occlusion occurs,
which is ubiquitous in practice. The code and models are publicly available at
https://github.com/Gengzigang/PCT.",None,-1
555d7f72-dc0a-4b78-8d4c-dcadb42c966e,Weakly Supervised 3D Instance Segmentation without Instance-level Annotations,0.208335,"3D semantic scene understanding tasks have achieved great success with the
emergence of deep learning, but often require a huge amount of manually
annotated training data. To alleviate the annotation cost, we propose the first
weakly-supervised 3D instance segmentation method that only requires
categorical semantic labels as supervision, and we do not need instance-level
labels. The required semantic annotations can be either dense or extreme sparse
(e.g. 0.02% of total points). Even without having any instance-related
ground-truth, we design an approach to break point clouds into raw fragments
and find the most confident samples for learning instance centroids.
Furthermore, we construct a recomposed dataset using pseudo instances, which is
used to learn our defined multilevel shape-aware objectness signal. An
asymmetrical object inference algorithm is followed to process core points and
boundary points with different strategies, and generate high-quality pseudo
instance labels to guide iterative training. Experiments demonstrate that our
method can achieve comparable results with recent fully supervised methods. By
generating pseudo instance labels from categorical semantic labels, our
designed approach can also assist existing methods for learning 3D instance
segmentation at reduced annotation cost.",None,-1
d8d6daf3-3f14-4bd8-ad28-80b0f752d962,Explainable Goal Recognition: A Framework Based on Weight of Evidence,0.231949,"We introduce and evaluate an eXplainable Goal Recognition (XGR) model that
uses the Weight of Evidence (WoE) framework to explain goal recognition
problems. Our model provides human-centered explanations that answer why? and
why not? questions. We computationally evaluate the performance of our system
over eight different domains. Using a human behavioral study to obtain the
ground truth from human annotators, we further show that the XGR model can
successfully generate human-like explanations. We then report on a study with
60 participants who observe agents playing Sokoban game and then receive
explanations of the goal recognition output. We investigate participants'
understanding obtained by explanations through task prediction, explanation
satisfaction, and trust.",None,-1
d251f801-0c55-405d-8caa-cb63817386b4,MAP: Domain Generalization via Meta-Learning on Anatomy-Consistent Pseudo-Modalities,0.746767,"Deep models suffer from limited generalization capability to unseen domains,
which has severely hindered their clinical applicability. Specifically for the
retinal vessel segmentation task, although the model is supposed to learn the
anatomy of the target, it can be distracted by confounding factors like
intensity and contrast. We propose Meta learning on Anatomy-consistent
Pseudo-modalities (MAP), a method that improves model generalizability by
learning structural features. We first leverage a feature extraction network to
generate three distinct pseudo-modalities that share the vessel structure of
the original image. Next, we use the episodic learning paradigm by selecting
one of the pseudo-modalities as the meta-train dataset, and perform
meta-testing on a continuous augmented image space generated through Dirichlet
mixup of the remaining pseudo-modalities. Further, we introduce two loss
functions that facilitate the model's focus on shape information by clustering
the latent vectors obtained from images featuring identical vasculature. We
evaluate our model on seven public datasets of various retinal imaging
modalities and we conclude that MAP has substantially better generalizability.
Our code is publically available at https://github.com/DeweiHu/MAP.",None,-1
90ff4d9c-1309-410a-aa41-af27d330f977,Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers,0.240066,"Vision Transformers achieve impressive accuracy across a range of visual
recognition tasks. Unfortunately, their accuracy frequently comes with high
computational costs. This is a particular issue in video recognition, where
models are often applied repeatedly across frames or temporal chunks. In this
work, we exploit temporal redundancy between subsequent inputs to reduce the
cost of Transformers for video processing. We describe a method for identifying
and re-processing only those tokens that have changed significantly over time.
Our proposed family of models, Eventful Transformers, can be converted from
existing Transformers (often without any re-training) and give adaptive control
over the compute cost at runtime. We evaluate our method on large-scale
datasets for video object detection (ImageNet VID) and action recognition
(EPIC-Kitchens 100). Our approach leads to significant computational savings
(on the order of 2-4x) with only minor reductions in accuracy.",None,-1
dac56b9e-478f-41b4-b813-76abd581e593,The Hardness of Reasoning about Probabilities and Causality,0.77687,"We study formal languages which are capable of fully expressing quantitative
probabilistic reasoning and do-calculus reasoning for causal effects, from a
computational complexity perspective. We focus on satisfiability problems whose
instance formulas allow expressing many tasks in probabilistic and causal
inference. The main contribution of this work is establishing the exact
computational complexity of these satisfiability problems. We introduce a new
natural complexity class, named succ$\exists$R, which can be viewed as a
succinct variant of the well-studied class $\exists$R, and show that the
problems we consider are complete for succ$\exists$R. Our results imply even
stronger algorithmic limitations than were proven by Fagin, Halpern, and
Megiddo (1990) and Moss\'{e}, Ibeling, and Icard (2022) for some variants of
the standard languages used commonly in probabilistic and causal inference.",None,-1
e31114be-92f8-4663-ba7c-cd4318567ede,Accurate and Structured Pruning for Efficient Automatic Speech Recognition,0.531468,"Automatic Speech Recognition (ASR) has seen remarkable advancements with deep
neural networks, such as Transformer and Conformer. However, these models
typically have large model sizes and high inference costs, posing a challenge
to deploy on resource-limited devices. In this paper, we propose a novel
compression strategy that leverages structured pruning and knowledge
distillation to reduce the model size and inference cost of the Conformer model
while preserving high recognition performance. Our approach utilizes a set of
binary masks to indicate whether to retain or prune each Conformer module, and
employs L0 regularization to learn the optimal mask values. To further enhance
pruning performance, we use a layerwise distillation strategy to transfer
knowledge from unpruned to pruned models. Our method outperforms all pruning
baselines on the widely used LibriSpeech benchmark, achieving a 50% reduction
in model size and a 28% reduction in inference cost with minimal performance
loss.",None,-1
7b56e67f-89d3-4ecb-9bf5-383f3f9ea3a5,ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT,0.879127,"In this paper, we investigate the use of data obtained from prompting a large
generative language model, ChatGPT, to generate synthetic training data with
the aim of augmenting data in low resource scenarios. We show that with
appropriate task-specific ChatGPT prompts, we outperform the most popular
existing approaches for such data augmentation. Furthermore, we investigate
methodologies for evaluating the similarity of the augmented data generated
from ChatGPT with the aim of validating and assessing the quality of the data
generated.",None,-1
a180b9e7-f0ec-4b47-954a-a6f68e2a250f,GameEval: Evaluating LLMs on Conversational Games,0.701052,"The rapid advancements in large language models (LLMs) have presented
challenges in evaluating those models. Existing evaluation methods are either
reference-based or preference based, which inevitably need human intervention
or introduce test bias caused by evaluator models. In this paper, we propose
GameEval, a novel approach to evaluating LLMs through goal-driven
conversational games, overcoming the limitations of previous methods. GameEval
treats LLMs as game players and assigns them distinct roles with specific goals
achieved by launching conversations of various forms, including discussion,
question answering, and voting. We design three unique games with cooperative
or adversarial objectives, accompanied by corresponding evaluation metrics, to
show how this new paradigm comprehensively evaluates model performance.Through
extensive experiments, we show that GameEval can effectively differentiate the
capabilities of various LLMs, providing a comprehensive assessment of their
integrated abilities to solve complex problems. Our public anonymous code is
available at https://github.com/GameEval/GameEval.",None,-1
5d52b646-d9eb-4092-b992-66f9f3e4b33e,Independent Component Alignment for Multi-Task Learning,0.591255,"In a multi-task learning (MTL) setting, a single model is trained to tackle a
diverse set of tasks jointly. Despite rapid progress in the field, MTL remains
challenging due to optimization issues such as conflicting and dominating
gradients. In this work, we propose using a condition number of a linear system
of gradients as a stability criterion of an MTL optimization. We theoretically
demonstrate that a condition number reflects the aforementioned optimization
issues. Accordingly, we present Aligned-MTL, a novel MTL optimization approach
based on the proposed criterion, that eliminates instability in the training
process by aligning the orthogonal components of the linear system of
gradients. While many recent MTL approaches guarantee convergence to a minimum,
task trade-offs cannot be specified in advance. In contrast, Aligned-MTL
provably converges to an optimal point with pre-defined task-specific weights,
which provides more control over the optimization result. Through experiments,
we show that the proposed approach consistently improves performance on a
diverse set of MTL benchmarks, including semantic and instance segmentation,
depth estimation, surface normal estimation, and reinforcement learning. The
source code is publicly available at https://github.com/SamsungLabs/MTL .",None,-1
4a278fa7-4136-440f-810e-fea7ed0c9e12,Masked Image Modeling with Local Multi-Scale Reconstruction,0.930366,"Masked Image Modeling (MIM) achieves outstanding success in self-supervised
representation learning. Unfortunately, MIM models typically have huge
computational burden and slow learning process, which is an inevitable obstacle
for their industrial applications. Although the lower layers play the key role
in MIM, existing MIM models conduct reconstruction task only at the top layer
of encoder. The lower layers are not explicitly guided and the interaction
among their patches is only used for calculating new activations. Considering
the reconstruction task requires non-trivial inter-patch interactions to reason
target signals, we apply it to multiple local layers including lower and upper
layers. Further, since the multiple layers expect to learn the information of
different scales, we design local multi-scale reconstruction, where the lower
and upper layers reconstruct fine-scale and coarse-scale supervision signals
respectively. This design not only accelerates the representation learning
process by explicitly guiding multiple layers, but also facilitates multi-scale
semantical understanding to the input. Extensive experiments show that with
significantly less pre-training burden, our model achieves comparable or better
performance on classification, detection and segmentation tasks than existing
MIM models.",None,-1
c8fc1179-f24c-446b-84e5-ea5c6df0da3d,Biomedical Entity Linking with Triple-aware Pre-Training,0.432897,"Linking biomedical entities is an essential aspect in biomedical natural
language processing tasks, such as text mining and question answering. However,
a difficulty of linking the biomedical entities using current large language
models (LLM) trained on a general corpus is that biomedical entities are
scarcely distributed in texts and therefore have been rarely seen during
training by the LLM. At the same time, those LLMs are not aware of high level
semantic connection between different biomedical entities, which are useful in
identifying similar concepts in different textual contexts. To cope with
aforementioned problems, some recent works focused on injecting knowledge graph
information into LLMs. However, former methods either ignore the relational
knowledge of the entities or lead to catastrophic forgetting. Therefore, we
propose a novel framework to pre-train the powerful generative LLM by a corpus
synthesized from a KG. In the evaluations we are unable to confirm the benefit
of including synonym, description or relational information.",None,-1
59dc8b6f-35bc-4326-b33f-bf6463a303d2,LARD -- Landing Approach Runway Detection -- Dataset for Vision Based Landing,0.583275,"As the interest in autonomous systems continues to grow, one of the major
challenges is collecting sufficient and representative real-world data. Despite
the strong practical and commercial interest in autonomous landing systems in
the aerospace field, there is a lack of open-source datasets of aerial images.
To address this issue, we present a dataset-lard-of high-quality aerial images
for the task of runway detection during approach and landing phases. Most of
the dataset is composed of synthetic images but we also provide manually
labelled images from real landing footages, to extend the detection task to a
more realistic setting. In addition, we offer the generator which can produce
such synthetic front-view images and enables automatic annotation of the runway
corners through geometric transformations. This dataset paves the way for
further research such as the analysis of dataset quality or the development of
models to cope with the detection tasks. Find data, code and more up-to-date
information at https://github.com/deel-ai/LARD",None,-1
df759dd6-1aa5-4f18-9dd6-b86e2feccc0d,DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles,0.434178,"We propose a new object-centric video prediction algorithm based on the deep
latent particle (DLP) representation. In comparison to existing slot- or
patch-based representations, DLPs model the scene using a set of keypoints with
learned parameters for properties such as position and size, and are both
efficient and interpretable. Our method, deep dynamic latent particles (DDLP),
yields state-of-the-art object-centric video prediction results on several
challenging datasets. The interpretable nature of DDLP allows us to perform
``what-if'' generation -- predict the consequence of changing properties of
objects in the initial frames, and DLP's compact structure enables efficient
diffusion-based unconditional video generation. Videos, code and pre-trained
models are available: https://taldatech.github.io/ddlp-web",None,-1
d91dd95a-cee8-41ee-a139-bcee9c6f3721,Controllable Mind Visual Diffusion Model,0.827786,"Brain signal visualization has emerged as an active research area, serving as
a critical interface between the human visual system and computer vision
models. Although diffusion models have shown promise in analyzing functional
magnetic resonance imaging (fMRI) data, including reconstructing high-quality
images consistent with original visual stimuli, their accuracy in extracting
semantic and silhouette information from brain signals remains limited. In this
regard, we propose a novel approach, referred to as Controllable Mind Visual
Diffusion Model (CMVDM). CMVDM extracts semantic and silhouette information
from fMRI data using attribute alignment and assistant networks. Additionally,
a residual block is incorporated to capture information beyond semantic and
silhouette features. We then leverage a control model to fully exploit the
extracted information for image synthesis, resulting in generated images that
closely resemble the visual stimuli in terms of semantics and silhouette.
Through extensive experimentation, we demonstrate that CMVDM outperforms
existing state-of-the-art methods both qualitatively and quantitatively.",None,-1
2f4cf78e-13b6-4202-8af8-b20856a1c725,Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN,0.885462,"Performant Convolutional Neural Network (CNN) architectures must be tailored
to specific tasks in order to consider the length, resolution, and
dimensionality of the input data. In this work, we tackle the need for
problem-specific CNN architectures. We present the Continuous Convolutional
Neural Network (CCNN): a single CNN able to process data of arbitrary
resolution, dimensionality and length without any structural changes. Its key
component are its continuous convolutional kernels which model long-range
dependencies at every layer, and thus remove the need of current CNN
architectures for task-dependent downsampling and depths. We showcase the
generality of our method by using the same architecture for tasks on sequential
($1{\rm D}$), visual ($2{\rm D}$) and point-cloud ($3{\rm D}$) data. Our CCNN
matches and often outperforms the current state-of-the-art across all tasks
considered.",None,-1
926247f7-30a1-48bc-97a9-446aa8471384,Normalizing Flow based Feature Synthesis for Outlier-Aware Object Detection,0.791661,"Real-world deployment of reliable object detectors is crucial for
applications such as autonomous driving. However, general-purpose object
detectors like Faster R-CNN are prone to providing overconfident predictions
for outlier objects. Recent outlier-aware object detection approaches estimate
the density of instance-wide features with class-conditional Gaussians and
train on synthesized outlier features from their low-likelihood regions.
However, this strategy does not guarantee that the synthesized outlier features
will have a low likelihood according to the other class-conditional Gaussians.
We propose a novel outlier-aware object detection framework that distinguishes
outliers from inlier objects by learning the joint data distribution of all
inlier classes with an invertible normalizing flow. The appropriate sampling of
the flow model ensures that the synthesized outliers have a lower likelihood
than inliers of all object classes, thereby modeling a better decision boundary
between inlier and outlier objects. Our approach significantly outperforms the
state-of-the-art for outlier-aware object detection on both image and video
datasets. Code available at https://github.com/nish03/FFS",None,-1
0a4ed4a4-f3c7-4ae6-b412-3a85a4f3e5d2,Empowering Wildlife Guardians: An Equitable Digital Stewardship and Reward System for Biodiversity Conservation using Deep Learning and 3/4G Camera Traps,0.744144,"The biodiversity of our planet is under threat, with approximately one
million species expected to become extinct within decades. The reason; negative
human actions, which include hunting, overfishing, pollution, and the
conversion of land for urbanisation and agricultural purposes. Despite
significant investment from charities and governments for activities that
benefit nature, global wildlife populations continue to decline. Local wildlife
guardians have historically played a critical role in global conservation
efforts and have shown their ability to achieve sustainability at various
levels. In 2021, COP26 recognised their contributions and pledged US$1.7
billion per year; however, this is a fraction of the global biodiversity budget
available (between US$124 billion and US$143 billion annually) given they
protect 80% of the planets biodiversity. This paper proposes a radical new
solution based on ""Interspecies Money,"" where animals own their own money.
Creating a digital twin for each species allows animals to dispense funds to
their guardians for the services they provide. For example, a rhinoceros may
release a payment to its guardian each time it is detected in a camera trap as
long as it remains alive and well. To test the efficacy of this approach 27
camera traps were deployed over a 400km2 area in Welgevonden Game Reserve in
Limpopo Province in South Africa. The motion-triggered camera traps were
operational for ten months and, using deep learning, we managed to capture
images of 12 distinct animal species. For each species, a makeshift bank
account was set up and credited with {\pounds}100. Each time an animal was
captured in a camera and successfully classified, 1 penny (an arbitrary amount
- mechanisms still need to be developed to determine the real value of species)
was transferred from the animal account to its associated guardian.",None,-1
b54f4866-5dd3-44f8-b92b-8a6ecc28d8e8,The Hidden Linear Structure in Score-Based Models and its Application,0.0613111,"Score-based models have achieved remarkable results in the generative
modeling of many domains. By learning the gradient of smoothed data
distribution, they can iteratively generate samples from complex distribution
e.g. natural images. However, is there any universal structure in the gradient
field that will eventually be learned by any neural network? Here, we aim to
find such structures through a normative analysis of the score function. First,
we derived the closed-form solution to the scored-based model with a Gaussian
score. We claimed that for well-trained diffusion models, the learned score at
a high noise scale is well approximated by the linear score of Gaussian. We
demonstrated this through empirical validation of pre-trained images diffusion
model and theoretical analysis of the score function. This finding enabled us
to precisely predict the initial diffusion trajectory using the analytical
solution and to accelerate image sampling by 15-30\% by skipping the initial
phase without sacrificing image quality. Our finding of the linear structure in
the score-based model has implications for better model design and data
pre-processing.",None,-1
4d827d46-820c-4a3a-ac34-e3d3c8ad7a25,Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity,0.736049,"A widespread view is that Artificial Intelligence cannot be creative. We
tested this assumption by comparing human-generated ideas with those generated
by six Generative Artificial Intelligence (GAI) chatbots: $alpa.\!ai$,
$Copy.\!ai$, ChatGPT (versions 3 and 4), $Studio.\!ai$, and YouChat. Humans and
a specifically trained AI independently assessed the quality and quantity of
ideas. We found no qualitative difference between AI and human-generated
creativity, although there are differences in how ideas are generated.
Interestingly, 9.4 percent of humans were more creative than the most creative
GAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the
creative process. Continued research and development of GAI in creative tasks
is crucial to fully understand this technology's potential benefits and
drawbacks in shaping the future of creativity. Finally, we discuss the question
of whether GAIs are capable of being truly creative.",None,-1
779e2940-8083-4723-9dd4-58ea2f1fad65,Understanding Client Reactions in Online Mental Health Counseling,0.904181,"Communication success relies heavily on reading participants' reactions. Such
feedback is especially important for mental health counselors, who must
carefully consider the client's progress and adjust their approach accordingly.
However, previous NLP research on counseling has mainly focused on studying
counselors' intervention strategies rather than their clients' reactions to the
intervention. This work aims to fill this gap by developing a theoretically
grounded annotation framework that encompasses counselors' strategies and
client reaction behaviors. The framework has been tested against a large-scale,
high-quality text-based counseling dataset we collected over the past two years
from an online welfare counseling platform. Our study shows how clients react
to counselors' strategies, how such reactions affect the final counseling
outcomes, and how counselors can adjust their strategies in response to these
reactions. We also demonstrate that this study can help counselors
automatically predict their clients' states.",None,-1
4bfdfc3b-8b41-4781-86bf-8790ac98d816,Representation Disparity-aware Distillation for 3D Object Detection,0.182522,"In this paper, we focus on developing knowledge distillation (KD) for compact
3D detectors. We observe that off-the-shelf KD methods manifest their efficacy
only when the teacher model and student counterpart share similar intermediate
feature representations. This might explain why they are less effective in
building extreme-compact 3D detectors where significant representation
disparity arises due primarily to the intrinsic sparsity and irregularity in 3D
point clouds. This paper presents a novel representation disparity-aware
distillation (RDD) method to address the representation disparity issue and
reduce performance gap between compact students and over-parameterized
teachers. This is accomplished by building our RDD from an innovative
perspective of information bottleneck (IB), which can effectively minimize the
disparity of proposal region pairs from student and teacher in features and
logits. Extensive experiments are performed to demonstrate the superiority of
our RDD over existing KD methods. For example, our RDD increases mAP of
CP-Voxel-S to 57.1% on nuScenes dataset, which even surpasses teacher
performance while taking up only 42% FLOPs.",None,-1
67281b7a-f728-4d79-9114-04a9e266f246,Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics,0.183641,"Automatic assessment of reading fluency using automatic speech recognition
(ASR) holds great potential for early detection of reading difficulties and
subsequent timely intervention. Precise assessment tools are required,
especially for languages other than English. In this study, we evaluate six
state-of-the-art ASR-based systems for automatically assessing Dutch oral
reading accuracy using Kaldi and Whisper. Results show our most successful
system reached substantial agreement with human evaluations (MCC = .63). The
same system reached the highest correlation between forced decoding confidence
scores and word correctness (r = .45). This system's language model (LM)
consisted of manual orthographic transcriptions and reading prompts of the test
data, which shows that including reading errors in the LM improves assessment
performance. We discuss the implications for developing automatic assessment
systems and identify possible avenues of future research.",None,-1
5a7a8cef-23ca-4827-9442-25fede035853,Inter-Annotator Agreement in the Wild: Uncovering Its Emerging Roles and Considerations in Real-World Scenarios,0.388692,"Inter-Annotator Agreement (IAA) is commonly used as a measure of label
consistency in natural language processing tasks. However, in real-world
scenarios, IAA has various roles and implications beyond its traditional usage.
In this paper, we not only consider IAA as a measure of consistency but also as
a versatile tool that can be effectively utilized in practical applications.
Moreover, we discuss various considerations and potential concerns when
applying IAA and suggest strategies for effectively navigating these
challenges.",None,-1
d894a3c0-f3f2-482b-b33f-906ffaaccf7a,DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase,0.238229,"In-Context Learning (ICL) combined with pre-trained large language models has
achieved promising results on various NLP tasks. However, ICL requires
high-quality annotated demonstrations which might not be available in
real-world scenarios. To overcome this limitation, we propose \textbf{D}ata
\textbf{A}ugmentation for \textbf{I}n-Context \textbf{L}earning
(\textbf{DAIL}). DAIL leverages the intuition that large language models are
more familiar with the content generated by themselves. It first utilizes the
language model to generate paraphrases of the test sample and employs majority
voting to determine the final result based on individual predictions. Our
extensive empirical evaluation shows that DAIL outperforms the standard ICL
method and other ensemble-based methods in the low-resource scenario.
Additionally, we explore the use of voting consistency as a confidence score of
the model when the logits of predictions are inaccessible. We believe our work
will stimulate further research on ICL in low-resource settings.",None,-1
3523b203-40d0-4ccf-bc02-727adabef32f,Going faster to see further: GPU-accelerated value iteration and simulation for perishable inventory control using JAX,0.396407,"Value iteration can find the optimal replenishment policy for a perishable
inventory problem, but is computationally demanding due to the large state
spaces that are required to represent the age profile of stock. The parallel
processing capabilities of modern GPUs can reduce the wall time required to run
value iteration by updating many states simultaneously. The adoption of
GPU-accelerated approaches has been limited in operational research relative to
other fields like machine learning, in which new software frameworks have made
GPU programming widely accessible. We used the Python library JAX to implement
value iteration and simulators of the underlying Markov decision processes in a
high-level API, and relied on this library's function transformations and
compiler to efficiently utilize GPU hardware. Our method can extend use of
value iteration to settings that were previously considered infeasible or
impractical. We demonstrate this on example scenarios from three recent studies
which include problems with over 16 million states and additional problem
features, such as substitution between products, that increase computational
complexity. We compare the performance of the optimal replenishment policies to
heuristic policies, fitted using simulation optimization in JAX which allowed
the parallel evaluation of multiple candidate policy parameters on thousands of
simulated years. The heuristic policies gave a maximum optimality gap of 2.49%.
Our general approach may be applicable to a wide range of problems in
operational research that would benefit from large-scale parallel computation
on consumer-grade GPU hardware.",None,-1
d812d48c-750a-4573-a0e4-c1b5ee1e734d,Learning Universal Policies via Text-Guided Video Generation,0.975186,"A goal of artificial intelligence is to construct an agent that can solve a
wide variety of tasks. Recent progress in text-guided image synthesis has
yielded models with an impressive ability to generate complex novel images,
exhibiting combinatorial generalization across domains. Motivated by this
success, we investigate whether such tools can be used to construct more
general-purpose agents. Specifically, we cast the sequential decision making
problem as a text-conditioned video generation problem, where, given a
text-encoded specification of a desired goal, a planner synthesizes a set of
future frames depicting its planned actions in the future, after which control
actions are extracted from the generated video. By leveraging text as the
underlying goal specification, we are able to naturally and combinatorially
generalize to novel goals. The proposed policy-as-video formulation can further
represent environments with different state and action spaces in a unified
space of images, which, for example, enables learning and generalization across
a variety of robot manipulation tasks. Finally, by leveraging pretrained
language embeddings and widely available videos from the internet, the approach
enables knowledge transfer through predicting highly realistic video plans for
real robots.",None,-1
dab3c17b-5ac2-49ec-9665-310a4a2a77d8,Introducing Tales of Tribute AI Competition,0.719569,"This paper presents a new AI challenge, the Tales of Tribute AI Competition
(TOTAIC), based on a two-player deck-building card game released with the High
Isle chapter of The Elder Scrolls Online. Currently, there is no other AI
competition covering Collectible Card Games (CCG) genre, and there has never
been one that targets a deck-building game. Thus, apart from usual CCG-related
obstacles to overcome, like randomness, hidden information, and large branching
factor, the successful approach additionally requires long-term planning and
versatility. The game can be tackled with multiple approaches, including
classic adversarial search, single-player planning, and Neural Networks-based
algorithms. This paper introduces the competition framework, describes the
rules of the game, and presents the results of a tournament between sample AI
agents.",None,-1
d9aecd25-fdc5-45ed-822d-a6dc0657745c,Witscript 3: A Hybrid AI System for Improvising Jokes in a Conversation,0.173375,"Previous papers presented Witscript and Witscript 2, AI systems for
improvising jokes in a conversation. Witscript generates jokes that rely on
wordplay, whereas the jokes generated by Witscript 2 rely on common sense. This
paper extends that earlier work by presenting Witscript 3, which generates joke
candidates using three joke production mechanisms and then selects the best
candidate to output. Like Witscript and Witscript 2, Witscript 3 is based on
humor algorithms created by an expert comedy writer. Human evaluators judged
Witscript 3's responses to input sentences to be jokes 44% of the time. This is
evidence that Witscript 3 represents another step toward giving a chatbot a
humanlike sense of humor.",None,-1
826c8b22-0d54-49f5-be16-86bce570b016,Building Manufacturing Deep Learning Models with Minimal and Imbalanced Training Data Using Domain Adaptation and Data Augmentation,0.144822,"Deep learning (DL) techniques are highly effective for defect detection from
images. Training DL classification models, however, requires vast amounts of
labeled data which is often expensive to collect. In many cases, not only the
available training data is limited but may also imbalanced. In this paper, we
propose a novel domain adaptation (DA) approach to address the problem of
labeled training data scarcity for a target learning task by transferring
knowledge gained from an existing source dataset used for a similar learning
task. Our approach works for scenarios where the source dataset and the dataset
available for the target learning task have same or different feature spaces.
We combine our DA approach with an autoencoder-based data augmentation approach
to address the problem of imbalanced target datasets. We evaluate our combined
approach using image data for wafer defect prediction. The experiments show its
superior performance against other algorithms when the number of labeled
samples in the target dataset is significantly small and the target dataset is
imbalanced.",None,-1
455cc4d7-e010-4e85-8e35-6054bb6180e3,A Fully First-Order Method for Stochastic Bilevel Optimization,0.978149,"We consider stochastic unconstrained bilevel optimization problems when only
the first-order gradient oracles are available. While numerous optimization
methods have been proposed for tackling bilevel problems, existing methods
either tend to require possibly expensive calculations regarding Hessians of
lower-level objectives, or lack rigorous finite-time performance guarantees. In
this work, we propose a Fully First-order Stochastic Approximation (F2SA)
method, and study its non-asymptotic convergence properties. Specifically, we
show that F2SA converges to an $\epsilon$-stationary solution of the bilevel
problem after $\epsilon^{-7/2}, \epsilon^{-5/2}$, and $\epsilon^{-3/2}$
iterations (each iteration using $O(1)$ samples) when stochastic noises are in
both level objectives, only in the upper-level objective, and not present
(deterministic settings), respectively. We further show that if we employ
momentum-assisted gradient estimators, the iteration complexities can be
improved to $\epsilon^{-5/2}, \epsilon^{-4/2}$, and $\epsilon^{-3/2}$,
respectively. We demonstrate even superior practical performance of the
proposed method over existing second-order based approaches on MNIST
data-hypercleaning experiments.",None,-1
e1cd1960-b5cc-407d-ad8a-d44cb8d8feb3,TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications,0.67874,"We introduce TabRepo, a new dataset of tabular model evaluations and
predictions. TabRepo contains the predictions and metrics of 1310 models
evaluated on 200 classification and regression datasets. We illustrate the
benefit of our dataset in multiple ways. First, we show that it allows to
perform analysis such as comparing Hyperparameter Optimization against current
AutoML systems while also considering ensembling at marginal cost by using
precomputed model predictions. Second, we show that our dataset can be readily
leveraged to perform transfer-learning. In particular, we show that applying
standard transfer-learning techniques allows to outperform current
state-of-the-art tabular systems in accuracy, runtime and latency.",None,-1
7b6bfb2f-be0a-4590-b58d-990712929c73,Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need,0.926625,"The core of out-of-distribution (OOD) detection is to learn the
in-distribution (ID) representation, which is distinguishable from OOD samples.
Previous work applied recognition-based methods to learn the ID features, which
tend to learn shortcuts instead of comprehensive representations. In this work,
we find surprisingly that simply using reconstruction-based methods could boost
the performance of OOD detection significantly. We deeply explore the main
contributors of OOD detection and find that reconstruction-based pretext tasks
have the potential to provide a generally applicable and efficacious prior,
which benefits the model in learning intrinsic data distributions of the ID
dataset. Specifically, we take Masked Image Modeling as a pretext task for our
OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms
previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by
3.0%, and near-distribution OOD detection by 2.1%. It even defeats the
10-shot-per-class outlier exposure OOD detection, although we do not include
any OOD samples for our detection",None,-1
59558f8b-59a5-4e5a-829d-12ad58f46a66,VL-Fields: Towards Language-Grounded Neural Implicit Spatial Representations,0.513472,"We present Visual-Language Fields (VL-Fields), a neural implicit spatial
representation that enables open-vocabulary semantic queries. Our model encodes
and fuses the geometry of a scene with vision-language trained latent features
by distilling information from a language-driven segmentation model. VL-Fields
is trained without requiring any prior knowledge of the scene object classes,
which makes it a promising representation for the field of robotics. Our model
outperformed the similar CLIP-Fields model in the task of semantic segmentation
by almost 10%.",None,-1
b88f11c9-094e-466a-a4cc-889727b053e9,Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature,0.824431,"Large language models (LLMs) have shown the ability to produce fluent and
cogent content, presenting both productivity opportunities and societal risks.
To build trustworthy AI systems, it is imperative to distinguish between
machine-generated and human-authored content. The leading zero-shot detector,
DetectGPT, showcases commendable performance but is marred by its intensive
computational costs. In this paper, we introduce the concept of conditional
probability curvature to elucidate discrepancies in word choices between LLMs
and humans within a given context. Utilizing this curvature as a foundational
metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which
substitutes DetectGPT's perturbation step with a more efficient sampling step.
Our evaluations on various datasets, source models, and test conditions
indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around
75% in both the white-box and black-box settings but also accelerates the
detection process by a factor of 340, as detailed in Table 1. See
\url{https://github.com/baoguangsheng/fast-detect-gpt} for code, data, and
results.",None,-1
4dd8acff-eb6c-4e6e-97a4-0031908db0eb,GLADIS: A General and Large Acronym Disambiguation Benchmark,0.543882,"Acronym Disambiguation (AD) is crucial for natural language understanding on
various sources, including biomedical reports, scientific papers, and search
engine queries. However, existing acronym disambiguation benchmarks and tools
are limited to specific domains, and the size of prior benchmarks is rather
small. To accelerate the research on acronym disambiguation, we construct a new
benchmark named GLADIS with three components: (1) a much larger acronym
dictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus
with 160 million sentences; (3) three datasets that cover the general,
scientific, and biomedical domains. We then pre-train a language model,
\emph{AcroBERT}, on our constructed corpus for general acronym disambiguation,
and show the challenges and values of our new benchmark.",None,-1
de322303-4dc8-4bdb-9b13-9b2f6d61939b,Sample-Efficient Learning of Novel Visual Concepts,0.214324,"Despite the advances made in visual object recognition, state-of-the-art deep
learning models struggle to effectively recognize novel objects in a few-shot
setting where only a limited number of examples are provided. Unlike humans who
excel at such tasks, these models often fail to leverage known relationships
between entities in order to draw conclusions about such objects. In this work,
we show that incorporating a symbolic knowledge graph into a state-of-the-art
recognition model enables a new approach for effective few-shot classification.
In our proposed neuro-symbolic architecture and training methodology, the
knowledge graph is augmented with additional relationships extracted from a
small set of examples, improving its ability to recognize novel objects by
considering the presence of interconnected entities. Unlike existing few-shot
classifiers, we show that this enables our model to incorporate not only
objects but also abstract concepts and affordances. The existence of the
knowledge graph also makes this approach amenable to interpretability through
analysis of the relationships contained within it. We empirically show that our
approach outperforms current state-of-the-art few-shot multi-label
classification methods on the COCO dataset and evaluate the addition of
abstract concepts and affordances on the Visual Genome dataset.",None,-1
38114a4f-8ba7-4a77-9bb7-3299cfcf7add,ETran: Energy-Based Transferability Estimation,0.738163,"This paper addresses the problem of ranking pre-trained models for object
detection and image classification. Selecting the best pre-trained model by
fine-tuning is an expensive and time-consuming task. Previous works have
proposed transferability estimation based on features extracted by the
pre-trained models. We argue that quantifying whether the target dataset is
in-distribution (IND) or out-of-distribution (OOD) for the pre-trained model is
an important factor in the transferability estimation. To this end, we propose
ETran, an energy-based transferability assessment metric, which includes three
scores: 1) energy score, 2) classification score, and 3) regression score. We
use energy-based models to determine whether the target dataset is OOD or IND
for the pre-trained model. In contrast to the prior works, ETran is applicable
to a wide range of tasks including classification, regression, and object
detection (classification+regression). This is the first work that proposes
transferability estimation for object detection task. Our extensive experiments
on four benchmarks and two tasks show that ETran outperforms previous works on
object detection and classification benchmarks by an average of 21% and 12%,
respectively, and achieves SOTA in transferability assessment.",None,-1
24af0096-2348-4ebd-8e82-afaceab6bf23,ACI-BENCH: a Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation,0.947131,"Recent immense breakthroughs in generative models such as in GPT4 have
precipitated re-imagined ubiquitous usage of these models in all applications.
One area that can benefit by improvements in artificial intelligence (AI) is
healthcare. The note generation task from doctor-patient encounters, and its
associated electronic medical record documentation, is one of the most arduous
time-consuming tasks for physicians. It is also a natural prime potential
beneficiary to advances in generative models. However with such advances,
benchmarking is more critical than ever. Whether studying model weaknesses or
developing new evaluation metrics, shared open datasets are an imperative part
of understanding the current state-of-the-art. Unfortunately as clinic
encounter conversations are not routinely recorded and are difficult to
ethically share due to patient confidentiality, there are no sufficiently large
clinic dialogue-note datasets to benchmark this task. Here we present the
Ambient Clinical Intelligence Benchmark (ACI-BENCH) corpus, the largest dataset
to date tackling the problem of AI-assisted note generation from visit
dialogue. We also present the benchmark performances of several common
state-of-the-art approaches.",None,-1
2b053d90-a550-41c2-b9c2-2cdacb89b55c,"SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation",0.797587,"Reliable automatic evaluation of summarization systems is challenging due to
the multifaceted and subjective nature of the task. This is especially the case
for languages other than English, where human evaluations are scarce. In this
work, we introduce SEAHORSE, a dataset for multilingual, multifaceted
summarization evaluation. SEAHORSE consists of 96K summaries with human ratings
along 6 dimensions of text quality: comprehensibility, repetition, grammar,
attribution, main ideas, and conciseness, covering 6 languages, 9 systems and 4
datasets. As a result of its size and scope, SEAHORSE can serve both as a
benchmark to evaluate learnt metrics, as well as a large-scale resource for
training such metrics. We show that metrics trained with SEAHORSE achieve
strong performance on the out-of-domain meta-evaluation benchmarks TRUE
(Honovich et al., 2022) and mFACE (Aharoni et al., 2022). We make the SEAHORSE
dataset and metrics publicly available for future research on multilingual and
multifaceted summarization evaluation.",None,-1
716f2917-5a41-4971-b880-2147b59afda7,CTRAN: CNN-Transformer-based Network for Natural Language Understanding,0.30737,"Intent-detection and slot-filling are the two main tasks in natural language
understanding. In this study, we propose CTRAN, a novel encoder-decoder
CNN-Transformer-based architecture for intent-detection and slot-filling. In
the encoder, we use BERT, followed by several convolutional layers, and
rearrange the output using window feature sequence. We use stacked Transformer
encoders after the window feature sequence. For the intent-detection decoder,
we utilize self-attention followed by a linear layer. In the slot-filling
decoder, we introduce the aligned Transformer decoder, which utilizes a zero
diagonal mask, aligning output tags with input tokens. We apply our network on
ATIS and SNIPS, and surpass the current state-of-the-art in slot-filling on
both datasets. Furthermore, we incorporate the language model as word
embeddings, and show that this strategy yields a better result when compared to
the language model as an encoder.",None,-1
ce4eb6fc-799c-47f8-8166-20a30e19dfcf,Integrating Graphs with Large Language Models: Methods and Prospects,0.569045,"Large language models (LLMs) such as GPT-4 have emerged as frontrunners,
showcasing unparalleled prowess in diverse applications, including answering
queries, code generation, and more. Parallelly, graph-structured data, an
intrinsic data type, is pervasive in real-world scenarios. Merging the
capabilities of LLMs with graph-structured data has been a topic of keen
interest. This paper bifurcates such integrations into two predominant
categories. The first leverages LLMs for graph learning, where LLMs can not
only augment existing graph algorithms but also stand as prediction models for
various graph tasks. Conversely, the second category underscores the pivotal
role of graphs in advancing LLMs. Mirroring human cognition, we solve complex
tasks by adopting graphs in either reasoning or collaboration. Integrating with
such structures can significantly boost the performance of LLMs in various
complicated tasks. We also discuss and propose open questions for integrating
LLMs with graph-structured data for the future direction of the field.",None,-1
13f4d823-91d2-4ae9-a0f1-6c33d7decc66,Document-Level Language Models for Machine Translation,0.610523,"Despite the known limitations, most machine translation systems today still
operate on the sentence-level. One reason for this is, that most parallel
training data is only sentence-level aligned, without document-level meta
information available. In this work, we set out to build context-aware
translation systems utilizing document-level monolingual data instead. This can
be achieved by combining any existing sentence-level translation model with a
document-level language model. We improve existing approaches by leveraging
recent advancements in model combination. Additionally, we propose novel
weighting techniques that make the system combination more flexible and
significantly reduce computational overhead. In a comprehensive evaluation on
four diverse translation tasks, we show that our extensions improve
document-targeted scores substantially and are also computationally more
efficient. However, we also find that in most scenarios, back-translation gives
even better results, at the cost of having to re-train the translation system.
Finally, we explore language model fusion in the light of recent advancements
in large language models. Our findings suggest that there might be strong
potential in utilizing large language models via model combination.",None,-1
040142c5-11ed-4c8f-9aaf-ba6219d5a9af,Chinese Spelling Correction as Rephrasing Language Model,0.75813,"This paper studies Chinese Spelling Correction (CSC), which aims to detect
and correct the potential spelling errors in a given sentence. Current
state-of-the-art methods regard CSC as a sequence tagging task and fine-tune
BERT-based models on sentence pairs. However, we note a critical flaw in the
process of tagging one character to another, that the correction is excessively
conditioned on the error. This is opposite from human mindset, where
individuals rephrase the complete sentence based on its semantics, rather than
solely on the error patterns memorized before. Such a counter-intuitive
learning process results in the bottleneck of generalizability and
transferability of machine spelling correction. To address this, we propose
Rephrasing Language Model (ReLM), where the model is trained to rephrase the
entire sentence by infilling additional slots, instead of
character-to-character tagging. This novel training paradigm achieves the new
state-of-the-art results across fine-tuned and zero-shot CSC benchmarks,
outperforming previous counterparts by a large margin. Our method also learns
transferable language representation when CSC is jointly trained with other
tasks.",None,-1
6840e3f3-2b24-45dc-afd2-e933ad3d9a9c,Linking Alternative Fuel Vehicles Adoption with Socioeconomic Status and Air Quality Index,0.550671,"This is a study on the potential widespread usage of alternative fuel
vehicles, linking them with the socio-economic status of the respective
consumers as well as the impact on the resulting air quality index. Research in
this area aims to leverage machine learning techniques in order to promote
appropriate policies for the proliferation of alternative fuel vehicles such as
electric vehicles with due justice to different population groups. Pearson
correlation coefficient is deployed in the modeling the relationships between
socio-economic data, air quality index and data on alternative fuel vehicles.
Linear regression is used to conduct predictive modeling on air quality index
as per the adoption of alternative fuel vehicles, based on socio-economic
factors. This work exemplifies artificial intelligence for social good.",None,-1
5a7fb0a7-36a1-4bae-911f-ca1e520e1496,Task-Driven Graph Attention for Hierarchical Relational Object Navigation,0.689056,"Embodied AI agents in large scenes often need to navigate to find objects. In
this work, we study a naturally emerging variant of the object navigation task,
hierarchical relational object navigation (HRON), where the goal is to find
objects specified by logical predicates organized in a hierarchical structure -
objects related to furniture and then to rooms - such as finding an apple on
top of a table in the kitchen. Solving such a task requires an efficient
representation to reason about object relations and correlate the relations in
the environment and in the task goal. HRON in large scenes (e.g. homes) is
particularly challenging due to its partial observability and long horizon,
which invites solutions that can compactly store the past information while
effectively exploring the scene. We demonstrate experimentally that scene
graphs are the best-suited representation compared to conventional
representations such as images or 2D maps. We propose a solution that uses
scene graphs as part of its input and integrates graph neural networks as its
backbone, with an integrated task-driven attention mechanism, and demonstrate
its better scalability and learning efficiency than state-of-the-art baselines.",None,-1
57deae5b-c6ac-4e80-b9bc-40c17e3689a3,Conversational Semantic Parsing using Dynamic Context Graphs,0.353777,"In this paper we consider the task of conversational semantic parsing over
general purpose knowledge graphs (KGs) with millions of entities, and thousands
of relation-types. We focus on models which are capable of interactively
mapping user utterances into executable logical forms (e.g., Sparql) in the
context of the conversational history. Our key idea is to represent information
about an utterance and its context via a subgraph which is created dynamically,
i.e., the number of nodes varies per utterance. Rather than treating the
subgraph as a sequence, we exploit its underlying structure and encode it with
a graph neural network which further allows us to represent a large number of
(unseen) nodes. Experimental results show that dynamic context modeling is
superior to static approaches, delivering performance improvements across the
board (i.e., for simple and complex questions). Our results further confirm
that modeling the structure of context is better at processing discourse
information, (i.e., at handling ellipsis and resolving coreference) and longer
interactions.",None,-1
4c6144e2-f1f7-48ed-8ac4-30325b18e9ed,Weakly supervised marine animal detection from remote sensing images using vector-quantized variational autoencoder,0.63378,"This paper studies a reconstruction-based approach for weakly-supervised
animal detection from aerial images in marine environments. Such an approach
leverages an anomaly detection framework that computes metrics directly on the
input space, enhancing interpretability and anomaly localization compared to
feature embedding methods. Building upon the success of Vector-Quantized
Variational Autoencoders in anomaly detection on computer vision datasets, we
adapt them to the marine animal detection domain and address the challenge of
handling noisy data. To evaluate our approach, we compare it with existing
methods in the context of marine animal detection from aerial image data.
Experiments conducted on two dedicated datasets demonstrate the superior
performance of the proposed method over recent studies in the literature. Our
framework offers improved interpretability and localization of anomalies,
providing valuable insights for monitoring marine ecosystems and mitigating the
impact of human activities on marine animals.",None,-1
1a667b77-157b-420b-bcb3-87ed5276fd1c,"3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement",0.729421,"Disentangling uncorrelated information in speech utterances is a crucial
research topic within speech community. Different speech-related tasks focus on
extracting distinct speech representations while minimizing the affects of
other uncorrelated information. We present a large-scale speech corpus to
facilitate the research of speech representation disentanglement. 3D-Speaker
contains over 10,000 speakers, each of whom are simultaneously recorded by
multiple Devices, locating at different Distances, and some speakers are
speaking multiple Dialects. The controlled combinations of multi-dimensional
audio data yield a matrix of a diverse blend of speech representation
entanglement, thereby motivating intriguing methods to untangle them. The
multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate
large universal speech models and experiment methods of out-of-domain learning
and self-supervised learning. https://3dspeaker.github.io/",None,-1
3c4b6c29-4101-424f-8610-0518d5ad506d,PSYCHIC: A Neuro-Symbolic Framework for Knowledge Graph Question-Answering Grounding,0.101854,"The Scholarly Question Answering over Linked Data (Scholarly QALD) at The
International Semantic Web Conference (ISWC) 2023 challenge presents two
sub-tasks to tackle question answering (QA) over knowledge graphs (KGs). We
answer the KGQA over DBLP (DBLP-QUAD) task by proposing a neuro-symbolic (NS)
framework based on PSYCHIC, an extractive QA model capable of identifying the
query and entities related to a KG question. Our system achieved a F1 score of
00.18% on question answering and came in third place for entity linking (EL)
with a score of 71.00%.",None,-1
baf227fb-0cec-413a-9575-8260d6de821b,Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media,0.264947,"Stance detection predicts attitudes towards targets in texts and has gained
attention with the rise of social media. Traditional approaches include
conventional machine learning, early deep neural networks, and pre-trained
fine-tuning models. However, with the evolution of very large pre-trained
language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face
deployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not
requiring backpropagation training, has emerged as a promising alternative.
This paper examines CoT's effectiveness in stance detection tasks,
demonstrating its superior accuracy and discussing associated challenges.",None,-1
7adc7092-2486-4582-8781-caaaaab56a1d,PromptST: Prompt-Enhanced Spatio-Temporal Multi-Attribute Prediction,0.73799,"In the era of information explosion, spatio-temporal data mining serves as a
critical part of urban management. Considering the various fields demanding
attention, e.g., traffic state, human activity, and social event, predicting
multiple spatio-temporal attributes simultaneously can alleviate regulatory
pressure and foster smart city construction. However, current research can not
handle the spatio-temporal multi-attribute prediction well due to the complex
relationships between diverse attributes. The key challenge lies in how to
address the common spatio-temporal patterns while tackling their distinctions.
In this paper, we propose an effective solution for spatio-temporal
multi-attribute prediction, PromptST. We devise a spatio-temporal transformer
and a parameter-sharing training scheme to address the common knowledge among
different spatio-temporal attributes. Then, we elaborate a spatio-temporal
prompt tuning strategy to fit the specific attributes in a lightweight manner.
Through the pretrain and prompt tuning phases, our PromptST is able to enhance
the specific spatio-temoral characteristic capture by prompting the backbone
model to fit the specific target attribute while maintaining the learned common
knowledge. Extensive experiments on real-world datasets verify that our
PromptST attains state-of-the-art performance. Furthermore, we also prove
PromptST owns good transferability on unseen spatio-temporal attributes, which
brings promising application potential in urban computing. The implementation
code is available to ease reproducibility.",None,-1
082f5975-8e72-438e-941b-c7247b115104,Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge,0.546039,"In empathetic conversations, individuals express their empathy towards
others. Previous work has mainly focused on generating empathetic responses by
utilizing the speaker's emotion. Besides, external commonsense knowledge has
been applied to enhance the system's understandings of the speaker's situation.
However, given an event, commonsense knowledge base contains various relations,
potentially leading to confusion for the dialogue system. Consequently,
inconsistencies arise among the emotion, generated response and speaker's
contextual information. To this end, we propose a novel approach for empathetic
response generation, which incorporates an adaptive module for commonsense
knowledge selection to ensure consistency between the generated empathetic
responses and the speaker's situation. This selected knowledge is used to
refine the commonsense cognition and empathy expression for generated
responses. Experimental results show that our approach significantly
outperforms baseline models in both automatic and human evaluations, exhibiting
the generation of more coherent and empathetic responses. Moreover, case
studies highlight the interpretability of knowledge selection in the responses
and the effectiveness of adaptive module in our model. Code:
https://github.com/Hanscal/DCKS.",None,-1
0f451b0b-3900-4215-82de-f61de3324439,Real-Aug: Realistic Scene Synthesis for LiDAR Augmentation in 3D Object Detection,0.462139,"Data and model are the undoubtable two supporting pillars for LiDAR object
detection. However, data-centric works have fallen far behind compared with the
ever-growing list of fancy new models. In this work, we systematically study
the synthesis-based LiDAR data augmentation approach (so-called GT-Aug) which
offers maxium controllability over generated data samples. We pinpoint the main
shortcoming of existing works is introducing unrealistic LiDAR scan patterns
during GT-Aug. In light of this finding, we propose Real-Aug, a synthesis-based
augmentation method which prioritizes on generating realistic LiDAR scans. Our
method consists a reality-conforming scene composition module which handles the
details of the composition and a real-synthesis mixing up training strategy
which gradually adapts the data distribution from synthetic data to the real
one. To verify the effectiveness of our methods, we conduct extensive ablation
studies and validate the proposed Real-Aug on a wide combination of detectors
and datasets. We achieve a state-of-the-art 0.744 NDS and 0.702 mAP on nuScenes
test set. The code shall be released soon.",None,-1
84d3158a-9f9a-4ab6-94f5-b21de6e7617f,Measuring Faithfulness in Chain-of-Thought Reasoning,0.968247,"Large language models (LLMs) perform better when they produce step-by-step,
""Chain-of-Thought"" (CoT) reasoning before answering a question, but it is
unclear if the stated reasoning is a faithful explanation of the model's actual
reasoning (i.e., its process for answering the question). We investigate
hypotheses for how CoT reasoning may be unfaithful, by examining how the model
predictions change when we intervene on the CoT (e.g., by adding mistakes or
paraphrasing it). Models show large variation across tasks in how strongly they
condition on the CoT when predicting their answer, sometimes relying heavily on
the CoT and other times primarily ignoring it. CoT's performance boost does not
seem to come from CoT's added test-time compute alone or from information
encoded via the particular phrasing of the CoT. As models become larger and
more capable, they produce less faithful reasoning on most tasks we study.
Overall, our results suggest that CoT can be faithful if the circumstances such
as the model size and task are carefully chosen.",None,-1
0a0f2815-f4db-4f47-83fe-d2dfdde235f8,Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark,0.594047,"Recent model editing techniques promise to mitigate the problem of memorizing
false or outdated associations during LLM training. However, we show that these
techniques can introduce large unwanted side effects which are not detected by
existing specificity benchmarks. We extend the existing CounterFact benchmark
to include a dynamic component and dub our benchmark CounterFact+.
Additionally, we extend the metrics used for measuring specificity by a
principled KL divergence-based metric. We use this improved benchmark to
evaluate recent model editing techniques and find that they suffer from low
specificity. Our findings highlight the need for improved specificity
benchmarks that identify and prevent unwanted side effects.",None,-1
83c613d1-7687-477a-b1be-8172f58065c8,Spatially and Spectrally Consistent Deep Functional Maps,0.937201,"Cycle consistency has long been exploited as a powerful prior for jointly
optimizing maps within a collection of shapes. In this paper, we investigate
its utility in the approaches of Deep Functional Maps, which are considered
state-of-the-art in non-rigid shape matching. We first justify that under
certain conditions, the learned maps, when represented in the spectral domain,
are already cycle consistent. Furthermore, we identify the discrepancy that
spectrally consistent maps are not necessarily spatially, or point-wise,
consistent. In light of this, we present a novel design of unsupervised Deep
Functional Maps, which effectively enforces the harmony of learned maps under
the spectral and the point-wise representation. By taking advantage of cycle
consistency, our framework produces state-of-the-art results in mapping shapes
even under significant distortions. Beyond that, by independently estimating
maps in both spectral and spatial domains, our method naturally alleviates
over-fitting in network training, yielding superior generalization performance
and accuracy within an array of challenging tests for both near-isometric and
non-isometric datasets. Codes are available at
https://github.com/rqhuang88/Spatiallyand-Spectrally-Consistent-Deep-Functional-Maps.",None,-1
8ddf40e2-c414-4c65-9163-8da9b578ae82,Morphosyntactic probing of multilingual BERT models,0.352921,"We introduce an extensive dataset for multilingual probing of morphological
information in language models (247 tasks across 42 languages from 10
families), each consisting of a sentence with a target word and a morphological
tag as the desired label, derived from the Universal Dependencies treebanks. We
find that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features
that attain strong performance across these tasks. We then apply two methods to
locate, for each probing task, where the disambiguating information resides in
the input. The first is a new perturbation method that masks various parts of
context; the second is the classical method of Shapley values. The most
intriguing finding that emerges is a strong tendency for the preceding context
to hold more information relevant to the prediction than the following context.",None,-1
14dbe66c-4c32-4504-83ea-42904882f32e,Spatial-temporal Transformer for Affective Behavior Analysis,0.625558,"The in-the-wild affective behavior analysis has been an important study. In
this paper, we submit our solutions for the 5th Workshop and Competition on
Affective Behavior Analysis in-the-wild (ABAW), which includes V-A Estimation,
Facial Expression Classification and AU Detection Sub-challenges. We propose a
Transformer Encoder with Multi-Head Attention framework to learn the
distribution of both the spatial and temporal features. Besides, there are
virious effective data augmentation strategies employed to alleviate the
problems of sample imbalance during model training. The results fully
demonstrate the effectiveness of our proposed model based on the Aff-Wild2
dataset.",None,-1
0e0b41f2-da1a-45be-b216-211ce33452b7,How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives,0.499647,"Recently, various intermediate layer distillation (ILD) objectives have been
shown to improve compression of BERT models via Knowledge Distillation (KD).
However, a comprehensive evaluation of the objectives in both task-specific and
task-agnostic settings is lacking. To the best of our knowledge, this is the
first work comprehensively evaluating distillation objectives in both settings.
We show that attention transfer gives the best performance overall. We also
study the impact of layer choice when initializing the student from the teacher
layers, finding a significant impact on the performance in task-specific
distillation. For vanilla KD and hidden states transfer, initialisation with
lower layers of the teacher gives a considerable improvement over higher
layers, especially on the task of QNLI (up to an absolute percentage change of
17.8 in accuracy). Attention transfer behaves consistently under different
initialisation settings. We release our code as an efficient transformer-based
model distillation framework for further studies.",None,-1
4c309588-2ca5-48c2-9cc6-3890b5114886,Randomized Adversarial Training via Taylor Expansion,0.552244,"In recent years, there has been an explosion of research into developing more
robust deep neural networks against adversarial examples. Adversarial training
appears as one of the most successful methods. To deal with both the robustness
against adversarial examples and the accuracy over clean examples, many works
develop enhanced adversarial training methods to achieve various trade-offs
between them. Leveraging over the studies that smoothed update on weights
during training may help find flat minima and improve generalization, we
suggest reconciling the robustness-accuracy trade-off from another perspective,
i.e., by adding random noise into deterministic weights. The randomized weights
enable our design of a novel adversarial training method via Taylor expansion
of a small Gaussian noise, and we show that the new adversarial training method
can flatten loss landscape and find flat minima. With PGD, CW, and Auto
Attacks, an extensive set of experiments demonstrate that our method enhances
the state-of-the-art adversarial training methods, boosting both robustness and
clean accuracy. The code is available at
https://github.com/Alexkael/Randomized-Adversarial-Training.",None,-1
c2bbf860-6e0f-45c5-95a0-0c6567d840a7,K-ESConv: Knowledge Injection for Emotional Support Dialogue Systems via Prompt Learning,0.611104,"Automatic psychological counseling requires mass of professional knowledge
that can be found in online counseling forums. Motivated by this, we propose
K-ESConv, a novel prompt learning based knowledge injection method for
emotional support dialogue system, transferring forum knowledge to response
generation. We evaluate our model on an emotional support dataset ESConv, where
the model retrieves and incorporates knowledge from external professional
emotional Q\&A forum. Experiment results show that the proposed method
outperforms existing baselines on both automatic evaluation and human
evaluation, which shows that our approach significantly improves the
correlation and diversity of responses and provides more comfort and better
suggestion for the seeker.",None,-1
3ac707ef-a8c7-4723-9a78-d4a52bc3d071,Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios,0.290366,"Current pre-trained language models have enabled remarkable improvements in
downstream tasks, but it remains difficult to distinguish effects of
statistical correlation from more systematic logical reasoning grounded on the
understanding of real world. We tease these factors apart by leveraging
counterfactual conditionals, which force language models to predict unusual
consequences based on hypothetical propositions. We introduce a set of tests
from psycholinguistic experiments, as well as larger-scale controlled datasets,
to probe counterfactual predictions from five pre-trained language models. We
find that models are consistently able to override real-world knowledge in
counterfactual scenarios, and that this effect is more robust in case of
stronger baseline world knowledge -- however, we also find that for most models
this effect appears largely to be driven by simple lexical cues. When we
mitigate effects of both world knowledge and lexical cues to test knowledge of
linguistic nuances of counterfactuals, we find that only GPT-3 shows
sensitivity to these nuances, though this sensitivity is also non-trivially
impacted by lexical associative factors.",None,-1
a97aa2b2-a84a-480d-947e-14fa0674c847,ATHENA: Mathematical Reasoning with Thought Expansion,0.0822764,"Solving math word problems depends on how to articulate the problems, the
lens through which models view human linguistic expressions. Real-world
settings count on such a method even more due to the diverse practices of the
same mathematical operations. Earlier works constrain available thinking
processes by limited prediction strategies without considering their
significance in acquiring mathematical knowledge. We introduce Attention-based
THought Expansion Network Architecture (ATHENA) to tackle the challenges of
real-world practices by mimicking human thought expansion mechanisms in the
form of neural network propagation. A thought expansion recurrently generates
the candidates carrying the thoughts of possible math expressions driven from
the previous step and yields reasonable thoughts by selecting the valid
pathways to the goal. Our experiments show that ATHENA achieves a new
state-of-the-art stage toward the ideal model that is compelling in variant
questions even when the informativeness in training examples is restricted.",None,-1
50b9b174-9574-41fe-a63a-e154449b62ba,Improving Adversarial Robustness with Hypersphere Embedding and Angular-based Regularizations,0.257591,"Adversarial training (AT) methods have been found to be effective against
adversarial attacks on deep neural networks. Many variants of AT have been
proposed to improve its performance. Pang et al. [1] have recently shown that
incorporating hypersphere embedding (HE) into the existing AT procedures
enhances robustness. We observe that the existing AT procedures are not
designed for the HE framework, and thus fail to adequately learn the angular
discriminative information available in the HE framework. In this paper, we
propose integrating HE into AT with regularization terms that exploit the rich
angular information available in the HE framework. Specifically, our method,
termed angular-AT, adds regularization terms to AT that explicitly enforce
weight-feature compactness and inter-class separation; all expressed in terms
of angular features. Experimental results show that angular-AT further improves
adversarial robustness.",None,-1
cbad1145-c0fe-4636-b05f-64b4c922f79b,Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement,0.624692,"Mentions of new concepts appear regularly in texts and require automated
approaches to harvest and place them into Knowledge Bases (KB), e.g.,
ontologies and taxonomies. Existing datasets suffer from three issues, (i)
mostly assuming that a new concept is pre-discovered and cannot support
out-of-KB mention discovery; (ii) only using the concept label as the input
along with the KB and thus lacking the contexts of a concept label; and (iii)
mostly focusing on concept placement w.r.t a taxonomy of atomic concepts,
instead of complex concepts, i.e., with logical operators. To address these
issues, we propose a new benchmark, adapting MedMentions dataset (PubMed
abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases
sub-category and the broader categories of Clinical finding, Procedure, and
Pharmaceutical / biologic product. We provide usage on the evaluation with the
dataset for out-of-KB mention discovery and concept placement, adapting recent
Large Language Model based methods.",None,-1
80039060-cf16-425a-b610-564eb977ea9d,Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge,0.629679,"Pre-trained language models (LMs) are used for knowledge intensive tasks like
question answering, but their knowledge gets continuously outdated as the world
changes. Prior work has studied targeted updates to LMs, injecting individual
facts and evaluating whether the model learns these facts while not changing
predictions on other contexts. We take a step forward and study LMs' abilities
to make inferences based on injected facts (or propagate those facts): for
example, after learning that something is a TV show, does an LM predict that
you can watch it? We study this with two cloze-style tasks: an existing dataset
of real-world sentences about novel entities (ECBD) as well as a new controlled
benchmark with manually designed templates requiring varying levels of
inference about injected knowledge. Surprisingly, we find that existing methods
for updating knowledge (gradient-based fine-tuning and modifications of this
approach) show little propagation of injected knowledge. These methods improve
performance on cloze instances only when there is lexical overlap between
injected facts and target inferences. Yet, prepending entity definitions in an
LM's context improves performance across all settings, suggesting that there is
substantial headroom for parameter-updating approaches for knowledge injection.",None,-1
afef3cd8-661e-4710-91c2-e22bc8d8770a,D2NT: A High-Performing Depth-to-Normal Translator,0.396987,"Surface normal holds significant importance in visual environmental
perception, serving as a source of rich geometric information. However, the
state-of-the-art (SoTA) surface normal estimators (SNEs) generally suffer from
an unsatisfactory trade-off between efficiency and accuracy. To resolve this
dilemma, this paper first presents a superfast depth-to-normal translator
(D2NT), which can directly translate depth images into surface normal maps
without calculating 3D coordinates. We then propose a discontinuity-aware
gradient (DAG) filter, which adaptively generates gradient convolution kernels
to improve depth gradient estimation. Finally, we propose a surface normal
refinement module that can easily be integrated into any depth-to-normal SNEs,
substantially improving the surface normal estimation accuracy. Our proposed
algorithm demonstrates the best accuracy among all other existing real-time
SNEs and achieves the SoTA trade-off between efficiency and accuracy.",None,-1
35211d4b-aca0-4cbb-a3fc-470db90ab96a,A Novel Self-training Approach for Low-resource Speech Recognition,0.88048,"In this paper, we propose a self-training approach for automatic speech
recognition (ASR) for low-resource settings. While self-training approaches
have been extensively developed and evaluated for high-resource languages such
as English, their applications to low-resource languages like Punjabi have been
limited, despite the language being spoken by millions globally. The scarcity
of annotated data has hindered the development of accurate ASR systems,
especially for low-resource languages (e.g., Punjabi and M\=aori languages). To
address this issue, we propose an effective self-training approach that
generates highly accurate pseudo-labels for unlabeled low-resource speech. Our
experimental analysis demonstrates that our approach significantly improves
word error rate, achieving a relative improvement of 14.94% compared to a
baseline model across four real speech datasets. Further, our proposed approach
reports the best results on the Common Voice Punjabi dataset.",None,-1
ddf30d03-6964-4f03-b5cc-cea043f82971,Think Rationally about What You See: Continuous Rationale Extraction for Relation Extraction,0.782246,"Relation extraction (RE) aims to extract potential relations according to the
context of two entities, thus, deriving rational contexts from sentences plays
an important role. Previous works either focus on how to leverage the entity
information (e.g., entity types, entity verbalization) to inference relations,
but ignore context-focused content, or use counterfactual thinking to remove
the model's bias of potential relations in entities, but the relation reasoning
process will still be hindered by irrelevant content. Therefore, how to
preserve relevant content and remove noisy segments from sentences is a crucial
task. In addition, retained content needs to be fluent enough to maintain
semantic coherence and interpretability. In this work, we propose a novel
rationale extraction framework named RE2, which leverages two continuity and
sparsity factors to obtain relevant and coherent rationales from sentences. To
solve the problem that the gold rationales are not labeled, RE2 applies an
optimizable binary mask to each token in the sentence, and adjust the
rationales that need to be selected according to the relation label.
Experiments on four datasets show that RE2 surpasses baselines.",None,-1
4874b6b4-6e03-4178-9427-8c0cd872cc5d,Parmesan: mathematical concept extraction for education,0.507478,"Mathematics is a highly specialized domain with its own unique set of
challenges that has seen limited study in natural language processing. However,
mathematics is used in a wide variety of fields and multidisciplinary research
in many different domains often relies on an understanding of mathematical
concepts. To aid researchers coming from other fields, we develop a prototype
system for searching for and defining mathematical concepts in context,
focusing on the field of category theory. This system, Parmesan, depends on
natural language processing components including concept extraction, relation
extraction, definition extraction, and entity linking. In developing this
system, we show that existing techniques cannot be applied directly to the
category theory domain, and suggest hybrid techniques that do perform well,
though we expect the system to evolve over time. We also provide two cleaned
mathematical corpora that power the prototype system, which are based on
journal articles and wiki pages, respectively. The corpora have been annotated
with dependency trees, lemmas, and part-of-speech tags.",None,-1
b24da6c0-62fa-4227-8c82-8aec08efcd79,MemeFier: Dual-stage Modality Fusion for Image Meme Classification,0.34106,"Hate speech is a societal problem that has significantly grown through the
Internet. New forms of digital content such as image memes have given rise to
spread of hate using multimodal means, being far more difficult to analyse and
detect compared to the unimodal case. Accurate automatic processing, analysis
and understanding of this kind of content will facilitate the endeavor of
hindering hate speech proliferation through the digital world. To this end, we
propose MemeFier, a deep learning-based architecture for fine-grained
classification of Internet image memes, utilizing a dual-stage modality fusion
module. The first fusion stage produces feature vectors containing modality
alignment information that captures non-trivial connections between the text
and image of a meme. The second fusion stage leverages the power of a
Transformer encoder to learn inter-modality correlations at the token level and
yield an informative representation. Additionally, we consider external
knowledge as an additional input, and background image caption supervision as a
regularizing component. Extensive experiments on three widely adopted
benchmarks, i.e., Facebook Hateful Memes, Memotion7k and MultiOFF, indicate
that our approach competes and in some cases surpasses state-of-the-art. Our
code is available on https://github.com/ckoutlis/memefier.",None,-1
d6f9a11a-b90b-43a6-86cf-6ebc86f25113,Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning,0.917915,"Planning for goal-oriented dialogue often requires simulating future dialogue
interactions and estimating task progress. Many approaches thus consider
training neural networks to perform look-ahead search algorithms such as A*
search and Monte Carlo Tree Search (MCTS). However, this training often
requires abundant annotated data, which creates challenges when faced with
noisy annotations or low-resource settings. We introduce GDP-Zero, an approach
using Open-Loop MCTS to perform goal-oriented dialogue policy planning without
any model training. GDP-Zero prompts a large language model to act as a policy
prior, value function, user simulator, and system model during the tree search.
We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that
its responses are preferred over ChatGPT up to 59.32% of the time, and are
rated more persuasive than ChatGPT during interactive evaluations.",None,-1
a34b3ee9-13a2-45de-8dd0-7369ef40924d,Task Weighting in Meta-learning with Trajectory Optimisation,0.128502,"Developing meta-learning algorithms that are un-biased toward a subset of
training tasks often requires hand-designed criteria to weight tasks,
potentially resulting in sub-optimal solutions. In this paper, we introduce a
new principled and fully-automated task-weighting algorithm for meta-learning
methods. By considering the weights of tasks within the same mini-batch as an
action, and the meta-parameter of interest as the system state, we cast the
task-weighting meta-learning problem to a trajectory optimisation and employ
the iterative linear quadratic regulator to determine the optimal action or
weights of tasks. We theoretically show that the proposed algorithm converges
to an $\epsilon_{0}$-stationary point, and empirically demonstrate that the
proposed approach out-performs common hand-engineering weighting methods in two
few-shot learning benchmarks.",None,-1
052f3848-176e-4c6a-8485-abf395884668,LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using XLM-RoBERTa,0.880884,"Named Entity Recognition(NER) is a task of recognizing entities at a token
level in a sentence. This paper focuses on solving NER tasks in a multilingual
setting for complex named entities. Our team, LLM-RM participated in the
recently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual
Complex Named Entity Recognition. We approach the problem by leveraging
cross-lingual representation provided by fine-tuning XLM-Roberta base model on
datasets of all of the 12 languages provided -- Bangla, Chinese, English,
Farsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and
Ukrainian",None,-1
536e9ac2-0bb3-456d-9d0c-08cdf1dea6c8,Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models,0.98266,"Current dialogue research primarily studies pairwise (two-party)
conversations, and does not address the everyday setting where more than two
speakers converse together. In this work, we both collect and evaluate
multi-party conversations to study this more general case. We use the LIGHT
environment to construct grounded conversations, where each participant has an
assigned character to role-play. We thus evaluate the ability of language
models to act as one or more characters in such conversations. Models require
two skills that pairwise-trained models appear to lack: (1) being able to
decide when to talk; (2) producing coherent utterances grounded on multiple
characters. We compare models trained on our new dataset to existing
pairwise-trained dialogue models, as well as large language models with
few-shot prompting. We find that our new dataset, MultiLIGHT, which we will
publicly release, can help bring significant improvements in the group setting.",None,-1
61ac6b42-eabe-41ca-8c0e-1d85319e4098,StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding,0.999778,"Analogy-making between narratives is crucial for human reasoning. In this
paper, we evaluate the ability to identify and generate analogies by
constructing a first-of-its-kind large-scale story-level analogy corpus,
\textsc{StoryAnalogy}, which contains 24K story pairs from diverse domains with
human annotations on two similarities from the extended Structure-Mapping
Theory. We design a set of tests on \textsc{StoryAnalogy}, presenting the first
evaluation of story-level analogy identification and generation. Interestingly,
we find that the analogy identification tasks are incredibly difficult not only
for sentence embedding models but also for the recent large language models
(LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around
30% accuracy in multiple-choice questions (compared to over 85% accuracy for
humans). Furthermore, we observe that the data in \textsc{StoryAnalogy} can
improve the quality of analogy generation in LLMs, where a fine-tuned
FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.",None,-1
22348d2c-17a1-4c78-84a8-1ef6db3d0027,Financial News Analytics Using Fine-Tuned Llama 2 GPT Model,0.995654,"The paper considers the possibility to fine-tune Llama 2 GPT large language
model (LLM) for the multitask analysis of financial news. For fine-tuning, the
PEFT/LoRA based approach was used. In the study, the model was fine-tuned for
the following tasks: analysing a text from financial market perspectives,
highlighting main points of a text, summarizing a text and extracting named
entities with appropriate sentiments. The obtained results show that the
fine-tuned Llama 2 model can perform a multitask financial news analysis with a
specified structure of response, part of response can be a structured text and
another part of data can have JSON format for further processing. Extracted
sentiments for named entities can be considered as predictive features in
supervised machine learning models with quantitative target variables.",None,-1
f90dda6f-4c90-4f4a-9d7c-1a0df85ddc27,Partial Network Cloning,0.83801,"In this paper, we study a novel task that enables partial knowledge transfer
from pre-trained models, which we term as Partial Network Cloning (PNC). Unlike
prior methods that update all or at least part of the parameters in the target
network throughout the knowledge transfer process, PNC conducts partial
parametric ""cloning"" from a source network and then injects the cloned module
to the target, without modifying its parameters. Thanks to the transferred
module, the target network is expected to gain additional functionality, such
as inference on new classes; whenever needed, the cloned module can be readily
removed from the target, with its original parameters and competence kept
intact. Specifically, we introduce an innovative learning scheme that allows us
to identify simultaneously the component to be cloned from the source and the
position to be inserted within the target network, so as to ensure the optimal
performance. Experimental results on several datasets demonstrate that, our
method yields a significant improvement of 5% in accuracy and 50% in locality
when compared with parameter-tuning based methods. Our code is available at
https://github.com/JngwenYe/PNCloning.",None,-1
af32bf6b-ee42-49f7-89a6-7c30532ff01c,ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text,0.848816,"We present an overview of the ArAIEval shared task, organized as part of the
first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two
tasks over Arabic text: (i) persuasion technique detection, focusing on
identifying persuasion techniques in tweets and news articles, and (ii)
disinformation detection in binary and multiclass setups over tweets. A total
of 20 teams participated in the final evaluation phase, with 14 and 16 teams
participating in Tasks 1 and 2, respectively. Across both tasks, we observed
that fine-tuning transformer models such as AraBERT was at the core of the
majority of the participating systems. We provide a description of the task
setup, including a description of the dataset construction and the evaluation
setup. We further give a brief overview of the participating systems. All
datasets and evaluation scripts from the shared task are released to the
research community. (https://araieval.gitlab.io/) We hope this will enable
further research on these important tasks in Arabic.",None,-1
397df104-af19-4c4e-b27f-60d6d316a37a,Meaningful human command: Advance control directives as a method to enable moral and legal responsibility for autonomous weapons systems,0.174369,"21st Century war is increasing in speed, with conventional forces combined
with massed use of autonomous systems and human-machine integration. However, a
significant challenge is how humans can ensure moral and legal responsibility
for systems operating outside of normal temporal parameters. This chapter
considers whether humans can stand outside of real time and authorise actions
for autonomous systems by the prior establishment of a contract, for actions to
occur in a future context particularly in faster than real time or in very slow
operations where human consciousness and concentration could not remain well
informed. The medical legal precdent found in 'advance care directives'
suggests how the time-consuming, deliberative process required for
accountability and responsibility of weapons systems may be achievable outside
real time captured in an 'advance control driective' (ACD). The chapter
proposes 'autonomy command' scaffolded and legitimised through the construction
of ACD ahead of the deployment of autonomous systems.",None,-1
f0be3e87-e02b-4796-bdc7-a513a8f1df6c,DCFace: Synthetic Face Generation with Dual Condition Diffusion Model,0.993007,"Generating synthetic datasets for training face recognition models is
challenging because dataset generation entails more than creating high fidelity
images. It involves generating multiple images of same subjects under different
factors (\textit{e.g.}, variations in pose, illumination, expression, aging and
occlusion) which follows the real image conditional distribution. Previous
works have studied the generation of synthetic datasets using GAN or 3D models.
In this work, we approach the problem from the aspect of combining subject
appearance (ID) and external factor (style) conditions. These two conditions
provide a direct way to control the inter-class and intra-class variations. To
this end, we propose a Dual Condition Face Generator (DCFace) based on a
diffusion model. Our novel Patch-wise style extractor and Time-step dependent
ID loss enables DCFace to consistently produce face images of the same subject
under different styles with precise control. Face recognition models trained on
synthetic images from the proposed DCFace provide higher verification
accuracies compared to previous works by $6.11\%$ on average in $4$ out of $5$
test datasets, LFW, CFP-FP, CPLFW, AgeDB and CALFW. Code is available at
https://github.com/mk-minchul/dcface",None,-1
0ef1d2de-2c85-4f5c-afb7-f5dbab947aa0,Semi-Supervised SAR ATR Framework with Transductive Auxiliary Segmentation,0.999866,"Convolutional neural networks (CNNs) have achieved high performance in
synthetic aperture radar (SAR) automatic target recognition (ATR). However, the
performance of CNNs depends heavily on a large amount of training data. The
insufficiency of labeled training SAR images limits the recognition performance
and even invalidates some ATR methods. Furthermore, under few labeled training
data, many existing CNNs are even ineffective. To address these challenges, we
propose a Semi-supervised SAR ATR Framework with transductive Auxiliary
Segmentation (SFAS). The proposed framework focuses on exploiting the
transductive generalization on available unlabeled samples with an auxiliary
loss serving as a regularizer. Through auxiliary segmentation of unlabeled SAR
samples and information residue loss (IRL) in training, the framework can
employ the proposed training loop process and gradually exploit the information
compilation of recognition and segmentation to construct a helpful inductive
bias and achieve high performance. Experiments conducted on the MSTAR dataset
have shown the effectiveness of our proposed SFAS for few-shot learning. The
recognition performance of 94.18\% can be achieved under 20 training samples in
each class with simultaneous accurate segmentation results. Facing variances of
EOCs, the recognition ratios are higher than 88.00\% when 10 training samples
each class.",None,-1
43597e7b-b5ee-4a21-adef-1f6fd5192e85,GaitRef: Gait Recognition with Refined Sequential Skeletons,0.661168,"Identifying humans with their walking sequences, known as gait recognition,
is a useful biometric understanding task as it can be observed from a long
distance and does not require cooperation from the subject. Two common
modalities used for representing the walking sequence of a person are
silhouettes and joint skeletons. Silhouette sequences, which record the
boundary of the walking person in each frame, may suffer from the variant
appearances from carried-on objects and clothes of the person. Framewise joint
detections are noisy and introduce some jitters that are not consistent with
sequential detections. In this paper, we combine the silhouettes and skeletons
and refine the framewise joint predictions for gait recognition. With temporal
information from the silhouette sequences, we show that the refined skeletons
can improve gait recognition performance without extra annotations. We compare
our methods on four public datasets, CASIA-B, OUMVLP, Gait3D and GREW, and show
state-of-the-art performance.",None,-1
fb9b9b64-7d6a-48c2-b13e-d60baf507c11,Language Embeddings Sometimes Contain Typological Generalizations,0.154159,"To what extent can neural network models learn generalizations about language
structure, and how do we find out what they have learned? We explore these
questions by training neural models for a range of natural language processing
tasks on a massively multilingual dataset of Bible translations in 1295
languages. The learned language representations are then compared to existing
typological databases as well as to a novel set of quantitative syntactic and
morphological features obtained through annotation projection. We conclude that
some generalizations are surprisingly close to traditional features from
linguistic typology, but that most of our models, as well as those of previous
work, do not appear to have made linguistically meaningful generalizations.
Careful attention to details in the evaluation turns out to be essential to
avoid false positives. Furthermore, to encourage continued work in this field,
we release several resources covering most or all of the languages in our data:
(i) multiple sets of language representations, (ii) multilingual word
embeddings, (iii) projected and predicted syntactic and morphological features,
(iv) software to provide linguistically sound evaluations of language
representations.",None,-1
effb8d40-80f5-4ff7-9a86-d3e67aaadfd4,Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages,0.534582,"Connectionist Temporal Classification (CTC) models are popular for their
balance between speed and performance for Automatic Speech Recognition (ASR).
However, these CTC models still struggle in other areas, such as
personalization towards custom words. A recent approach explores Contextual
Adapters, wherein an attention-based biasing model for CTC is used to improve
the recognition of custom entities. While this approach works well with enough
data, we showcase that it isn't an effective strategy for low-resource
languages. In this work, we propose a supervision loss for smoother training of
the Contextual Adapters. Further, we explore a multilingual strategy to improve
performance with limited training data. Our method achieves 48% F1 improvement
in retrieving unseen custom entities for a low-resource language.
Interestingly, as a by-product of training the Contextual Adapters, we see a
5-11% Word Error Rate (WER) reduction in the performance of the base CTC model
as well.",None,-1
4299e831-a6ac-4f27-ba58-5dcdfd21408e,Scalable 3D Captioning with Pretrained Models,0.902648,"We introduce Cap3D, an automatic approach for generating descriptive text for
3D objects. This approach utilizes pretrained models from image captioning,
image-text alignment, and LLM to consolidate captions from multiple views of a
3D asset, completely side-stepping the time-consuming and costly process of
manual annotation. We apply Cap3D to the recently introduced large-scale 3D
dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted
using 41k human annotations from the same dataset, demonstrates that Cap3D
surpasses human-authored descriptions in terms of quality, cost, and speed.
Through effective prompt engineering, Cap3D rivals human performance in
generating geometric descriptions on 17k collected annotations from the ABO
dataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,
and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,
and DreamFusion.",None,-1
8ce70afe-3351-43c4-a665-f6f3ef563266,Annotating Ambiguous Images: General Annotation Strategy for High-Quality Data with Real-World Biomedical Validation,0.045325,"In the field of image classification, existing methods often struggle with
biased or ambiguous data, a prevalent issue in real-world scenarios. Current
strategies, including semi-supervised learning and class blending, offer
partial solutions but lack a definitive resolution. Addressing this gap, our
paper introduces a novel strategy for generating high-quality labels in
challenging datasets. Central to our approach is a clearly designed flowchart,
based on a broad literature review, which enables the creation of reliable
labels. We validate our methodology through a rigorous real-world test case in
the biomedical field, specifically in deducing height reduction from vertebral
imaging. Our empirical study, leveraging over 250,000 annotations, demonstrates
the effectiveness of our strategies decisions compared to their alternatives.",None,-1
84ac5bfe-7d10-4cb2-a239-ae0266c17110,Tracing and Manipulating Intermediate Values in Neural Math Problem Solvers,0.0693578,"How language models process complex input that requires multiple steps of
inference is not well understood. Previous research has shown that information
about intermediate values of these inputs can be extracted from the activations
of the models, but it is unclear where that information is encoded and whether
that information is indeed used during inference. We introduce a method for
analyzing how a Transformer model processes these inputs by focusing on simple
arithmetic problems and their intermediate values. To trace where information
about intermediate values is encoded, we measure the correlation between
intermediate values and the activations of the model using principal component
analysis (PCA). Then, we perform a causal intervention by manipulating model
weights. This intervention shows that the weights identified via tracing are
not merely correlated with intermediate values, but causally related to model
predictions. Our findings show that the model has a locality to certain
intermediate values, and this is useful for enhancing the interpretability of
the models.",None,-1
7266c801-5759-4103-930b-b645eb3dfc37,Multi-Granularity Prompts for Topic Shift Detection in Dialogue,0.637737,"The goal of dialogue topic shift detection is to identify whether the current
topic in a conversation has changed or needs to change. Previous work focused
on detecting topic shifts using pre-trained models to encode the utterance,
failing to delve into the various levels of topic granularity in the dialogue
and understand dialogue contents. To address the above issues, we take a
prompt-based approach to fully extract topic information from dialogues at
multiple-granularity, i.e., label, turn, and topic. Experimental results on our
annotated Chinese Natural Topic Dialogue dataset CNTD and the publicly
available English TIAGE dataset show that the proposed model outperforms the
baselines. Further experiments show that the information extracted at different
levels of granularity effectively helps the model comprehend the conversation
topics.",None,-1
a2b06362-ba52-4ae2-8f4e-f68e76d9b1dc,Decomposing Complex Queries for Tip-of-the-tongue Retrieval,0.418106,"When re-finding items, users who forget or are uncertain about identifying
details often rely on creative strategies for expressing their information
needs -- complex queries that describe content elements (e.g., book characters
or events), information beyond the document text (e.g., descriptions of book
covers), or personal context (e.g., when they read a book). This retrieval
setting, called tip of the tongue (TOT), is especially challenging for models
heavily reliant on lexical and semantic overlap between query and document
text. In this work, we introduce a simple yet effective framework for handling
such complex queries by decomposing the query into individual clues, routing
those as sub-queries to specialized retrievers, and ensembling the results.
This approach allows us to take advantage of off-the-shelf retrievers (e.g.,
CLIP for retrieving images of book covers) or incorporate retriever-specific
logic (e.g., date constraints). We show that our framework incorportating query
decompositions into retrievers can improve gold book recall up to 7% relative
again for Recall@5 on a new collection of 14,441 real-world query-book pairs
from an online community for resolving TOT inquiries.",None,-1
4e53c884-6ff4-42e1-adfc-bb751c041102,Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions,0.343994,"Occlusion is a common problem with biometric recognition in the wild. The
generalization ability of CNNs greatly decreases due to the adverse effects of
various occlusions. To this end, we propose a novel unified framework
integrating the merits of both CNNs and graph models to overcome occlusion
problems in biometric recognition, called multiscale dynamic graph
representation (MS-DGR). More specifically, a group of deep features reflected
on certain subregions is recrafted into a feature graph (FG). Each node inside
the FG is deemed to characterize a specific local region of the input sample,
and the edges imply the co-occurrence of non-occluded regions. By analyzing the
similarities of the node representations and measuring the topological
structures stored in the adjacent matrix, the proposed framework leverages
dynamic graph matching to judiciously discard the nodes corresponding to the
occluded parts. The multiscale strategy is further incorporated to attain more
diverse nodes representing regions of various sizes. Furthermore, the proposed
framework exhibits a more illustrative and reasonable inference by showing the
paired nodes. Extensive experiments demonstrate the superiority of the proposed
framework, which boosts the accuracy in both natural and occlusion-simulated
cases by a large margin compared with that of baseline methods.",None,-1
3e3b870f-8535-4e36-8f15-fdf003dadb5b,Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning,0.103528,"Large Language Models (LLMs) are known to memorize significant portions of
their training data. Parts of this memorized content have been shown to be
extractable by simply querying the model, which poses a privacy risk. We
present a novel approach which uses prompt-tuning to control the extraction
rates of memorized content in LLMs. We present two prompt training strategies
to increase and decrease extraction rates, which correspond to an attack and a
defense, respectively. We demonstrate the effectiveness of our techniques by
using models from the GPT-Neo family on a public benchmark. For the 1.3B
parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in
extraction rate compared to our baseline. Our defense can be tuned to achieve
different privacy-utility trade-offs by a user-specified hyperparameter. We
achieve an extraction rate reduction of up to 97.7% relative to our baseline,
with a perplexity increase of 16.9%.",None,-1
543054c0-7105-46a8-89c1-9663f024700a,Single-Stage Heavy-Tailed Food Classification,0.870618,"Deep learning based food image classification has enabled more accurate
nutrition content analysis for image-based dietary assessment by predicting the
types of food in eating occasion images. However, there are two major obstacles
to apply food classification in real life applications. First, real life food
images are usually heavy-tailed distributed, resulting in severe
class-imbalance issue. Second, it is challenging to train a single-stage (i.e.
end-to-end) framework under heavy-tailed data distribution, which cause the
over-predictions towards head classes with rich instances and under-predictions
towards tail classes with rare instance. In this work, we address both issues
by introducing a novel single-stage heavy-tailed food classification framework.
Our method is evaluated on two heavy-tailed food benchmark datasets, Food101-LT
and VFN-LT, and achieves the best performance compared to existing work with
over 5% improvements for top-1 accuracy.",None,-1
a5a9ed73-51a4-4925-b862-80dc341e2747,Knowledge Transfer via Multi-Head Feature Adaptation for Whole Slide Image Classification,0.181138,"Transferring prior knowledge from a source domain to the same or similar
target domain can greatly enhance the performance of models on the target
domain. However, it is challenging to directly leverage the knowledge from the
source domain due to task discrepancy and domain shift. To bridge the gaps
between different tasks and domains, we propose a Multi-Head Feature Adaptation
module, which projects features in the source feature space to a new space that
is more similar to the target space. Knowledge transfer is particularly
important in Whole Slide Image (WSI) classification since the number of WSIs in
one dataset might be too small to achieve satisfactory performance. Therefore,
WSI classification is an ideal testbed for our method, and we adapt multiple
knowledge transfer methods for WSI classification. The experimental results
show that models with knowledge transfer outperform models that are trained
from scratch by a large margin regardless of the number of WSIs in the
datasets, and our method achieves state-of-the-art performances among other
knowledge transfer methods on multiple datasets, including TCGA-RCC,
TCGA-NSCLC, and Camelyon16 datasets.",None,-1
5107cfa5-73e0-444f-a6f3-94f4ae6b7d56,MatFuse: Controllable Material Generation with Diffusion Models,0.877781,"Creating high-quality materials in computer graphics is a challenging and
time-consuming task, which requires great expertise. To simplify this process,
we introduce MatFuse, a unified approach that harnesses the generative power of
diffusion models for creation and editing of 3D materials. Our method
integrates multiple sources of conditioning, including color palettes,
sketches, text, and pictures, enhancing creative possibilities and granting
fine-grained control over material synthesis. Additionally, MatFuse enables
map-level material editing capabilities through latent manipulation by means of
a multi-encoder compression model which learns a disentangled latent
representation for each map. We demonstrate the effectiveness of MatFuse under
multiple conditioning settings and explore the potential of material editing.
Finally, we assess the quality of the generated materials both quantitatively
in terms of CLIP-IQA and FID scores and qualitatively by conducting a user
study. Source code for training MatFuse and supplemental materials are publicly
available at https://gvecchio.com/matfuse.",None,-1
87cc90cc-17c2-47c3-9b0a-fa6953dcfaf4,Diffusion Action Segmentation,0.650794,"Temporal action segmentation is crucial for understanding long-form videos.
Previous works on this task commonly adopt an iterative refinement paradigm by
using multi-stage models. We propose a novel framework via denoising diffusion
models, which nonetheless shares the same inherent spirit of such iterative
refinement. In this framework, action predictions are iteratively generated
from random noise with input video features as conditions. To enhance the
modeling of three striking characteristics of human actions, including the
position prior, the boundary ambiguity, and the relational dependency, we
devise a unified masking strategy for the conditioning inputs in our framework.
Extensive experiments on three benchmark datasets, i.e., GTEA, 50Salads, and
Breakfast, are performed and the proposed method achieves superior or
comparable results to state-of-the-art methods, showing the effectiveness of a
generative approach for action segmentation.",None,-1
1bbc667e-8625-4b8b-bbbd-bb29501c6aab,EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras,0.559424,"Falls are significant and often fatal for vulnerable populations such as the
elderly. Previous works have addressed the detection of falls by relying on
data capture by a single sensor, images or accelerometers. In this work, we
rely on multimodal descriptors extracted from videos captured by egocentric
cameras. Our proposed method includes a late decision fusion layer that builds
on top of the extracted descriptors. Furthermore, we collect a new dataset on
which we assess our proposed approach. We believe this is the first public
dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects.
We conducted ablation experiments to assess the performance of individual
feature extractors, fusion of visual information, and fusion of both visual and
audio information. Moreover, we experimented with internal and external
cross-validation. Our results demonstrate that the fusion of audio and visual
information through late decision fusion improves detection performance, making
it a promising tool for fall prevention and mitigation.",None,-1
13aa8ab5-09cb-45d6-8834-e6310a01e9b9,TS-RGBD Dataset: a Novel Dataset for Theatre Scenes Description for People with Visual Impairments,0.0786916,"Computer vision was long a tool used for aiding visually impaired people to
move around their environment and avoid obstacles and falls. Solutions are
limited to either indoor or outdoor scenes, which limits the kind of places and
scenes visually disabled people can be in, including entertainment places such
as theatres. Furthermore, most of the proposed computer-vision-based methods
rely on RGB benchmarks to train their models resulting in a limited performance
due to the absence of the depth modality.
  In this paper, we propose a novel RGB-D dataset containing theatre scenes
with ground truth human actions and dense captions annotations for image
captioning and human action recognition: TS-RGBD dataset. It includes three
types of data: RGB, depth, and skeleton sequences, captured by Microsoft
Kinect.
  We test image captioning models on our dataset as well as some skeleton-based
human action recognition models in order to extend the range of environment
types where a visually disabled person can be, by detecting human actions and
textually describing appearances of regions of interest in theatre scenes.",None,-1
bf02a055-53d0-42a2-9f20-876fb9a39395,Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation,0.762063,"Translating images from a source domain to a target domain for learning
target models is one of the most common strategies in domain adaptive semantic
segmentation (DASS). However, existing methods still struggle to preserve
semantically-consistent local details between the original and translated
images. In this work, we present an innovative approach that addresses this
challenge by using source-domain labels as explicit guidance during image
translation. Concretely, we formulate cross-domain image translation as a
denoising diffusion process and utilize a novel Semantic Gradient Guidance
(SGG) method to constrain the translation process, conditioning it on the
pixel-wise source labels. Additionally, a Progressive Translation Learning
(PTL) strategy is devised to enable the SGG method to work reliably across
domains with large gaps. Extensive experiments demonstrate the superiority of
our approach over state-of-the-art methods.",None,-1
cead4ab6-ea02-4376-ab97-bba910ab823c,Fine-Tuning BERT with Character-Level Noise for Zero-Shot Transfer to Dialects and Closely-Related Languages,0.46841,"In this work, we induce character-level noise in various forms when
fine-tuning BERT to enable zero-shot cross-lingual transfer to unseen dialects
and languages. We fine-tune BERT on three sentence-level classification tasks
and evaluate our approach on an assortment of unseen dialects and languages. We
find that character-level noise can be an extremely effective agent of
cross-lingual transfer under certain conditions, while it is not as helpful in
others. Specifically, we explore these differences in terms of the nature of
the task and the relationships between source and target languages, finding
that introduction of character-level noise during fine-tuning is particularly
helpful when a task draws on surface level cues and the source-target
cross-lingual pair has a relatively high lexical overlap with shorter (i.e.,
less meaningful) unseen tokens on average.",None,-1
1a372e6e-eca0-4594-8276-001de39cb736,A computational framework of human values for ethical AI,0.21032,"In the diverse array of work investigating the nature of human values from
psychology, philosophy and social sciences, there is a clear consensus that
values guide behaviour. More recently, a recognition that values provide a
means to engineer ethical AI has emerged. Indeed, Stuart Russell proposed
shifting AI's focus away from simply ``intelligence'' towards intelligence
``provably aligned with human values''. This challenge -- the value alignment
problem -- with others including an AI's learning of human values, aggregating
individual values to groups, and designing computational mechanisms to reason
over values, has energised a sustained research effort. Despite this, no
formal, computational definition of values has yet been proposed. We address
this through a formal conceptual framework rooted in the social sciences, that
provides a foundation for the systematic, integrated and interdisciplinary
investigation into how human values can support designing ethical AI.",None,-1
43642c09-dddc-492d-8e31-1a7aa7a8faf4,Open-TI: Open Traffic Intelligence with Augmented Language Model,0.791498,"Transportation has greatly benefited the cities' development in the modern
civilization process. Intelligent transportation, leveraging advanced computer
algorithms, could further increase people's daily commuting efficiency.
However, intelligent transportation, as a cross-discipline, often requires
practitioners to comprehend complicated algorithms and obscure neural networks,
bringing a challenge for the advanced techniques to be trusted and deployed in
practical industries. Recognizing the expressiveness of the pre-trained large
language models, especially the potential of being augmented with abilities to
understand and execute intricate commands, we introduce Open-TI. Serving as a
bridge to mitigate the industry-academic gap, Open-TI is an innovative model
targeting the goal of Turing Indistinguishable Traffic Intelligence, it is
augmented with the capability to harness external traffic analysis packages
based on existing conversations. Marking its distinction, Open-TI is the first
method capable of conducting exhaustive traffic analysis from scratch -
spanning from map data acquisition to the eventual execution in complex
simulations. Besides, Open-TI is able to conduct task-specific embodiment like
training and adapting the traffic signal control policies (TSC), explore demand
optimizations, etc. Furthermore, we explored the viability of LLMs directly
serving as control agents, by understanding the expected intentions from
Open-TI, we designed an agent-to-agent communication mode to support Open-TI
conveying messages to ChatZero (control agent), and then the control agent
would choose from the action space to proceed the execution. We eventually
provide the formal implementation structure, and the open-ended design invites
further community-driven enhancements.",None,-1
10886ace-3c1b-4fbb-8c16-3a615bea57d5,ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding,0.837592,"We introduce ZeroSCROLLS, a zero-shot benchmark for natural language
understanding over long texts, which contains only test and small validation
sets, without training data. We adapt six tasks from the SCROLLS benchmark, and
add four new datasets, including two novel information fusing tasks, such as
aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a
comprehensive evaluation of both open-source and closed large language models,
finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest
average score. However, there is still room for improvement on multiple open
challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to
pass the naive baseline. As the state of the art is a moving target, we invite
researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.",None,-1
3fe9a8c2-7df5-4e59-a3a2-5c9a63503f82,Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task,0.850962,"Large-scale language models (LLMs) has shown remarkable capability in various
of Natural Language Processing (NLP) tasks and attracted lots of attention
recently. However, some studies indicated that large language models fail to
achieve promising result beyond the state-of-the-art models in English
grammatical error correction (GEC) tasks. In this report, we aim to explore the
how large language models perform on Chinese grammatical error correction tasks
and provide guidance for future work. We conduct experiments with 3 different
LLMs of different model scale on 4 Chinese GEC dataset. Our experimental
results indicate that the performances of LLMs on automatic evaluation metrics
falls short of the previous sota models because of the problem of
over-correction. Furthermore, we also discover notable variations in the
performance of LLMs when evaluated on different data distributions. Our
findings demonstrates that further investigation is required for the
application of LLMs on Chinese GEC task.",None,-1
84c30437-ce77-4156-b865-1908de632cb0,KGS: Causal Discovery Using Knowledge-guided Greedy Equivalence Search,0.0938464,"Learning causal relationships solely from observational data provides
insufficient information about the underlying causal mechanism and the search
space of possible causal graphs. As a result, often the search space can grow
exponentially for approaches such as Greedy Equivalence Search (GES) that uses
a score-based approach to search the space of equivalence classes of graphs.
Prior causal information such as the presence or absence of a causal edge can
be leveraged to guide the discovery process towards a more restricted and
accurate search space. In this study, we present KGS, a knowledge-guided greedy
score-based causal discovery approach that uses observational data and
structural priors (causal edges) as constraints to learn the causal graph. KGS
is a novel application of knowledge constraints that can leverage any of the
following prior edge information between any two variables: the presence of a
directed edge, the absence of an edge, and the presence of an undirected edge.
We extensively evaluate KGS across multiple settings in both synthetic and
benchmark real-world datasets. Our experimental results demonstrate that
structural priors of any type and amount are helpful and guide the search
process towards an improved performance and early convergence.",None,-1
56998700-3cd0-47cb-aa7d-e1c04b6e1f84,Using and Abusing Equivariance,0.322714,"In this paper we show how Group Equivariant Convolutional Neural Networks use
subsampling to learn to break equivariance to their symmetries. We focus on 2D
rotations and reflections and investigate the impact of broken equivariance on
network performance. We show that a change in the input dimension of a network
as small as a single pixel can be enough for commonly used architectures to
become approximately equivariant, rather than exactly. We investigate the
impact of networks not being exactly equivariant and find that approximately
equivariant networks generalise significantly worse to unseen symmetries
compared to their exactly equivariant counterparts. However, when the
symmetries in the training data are not identical to the symmetries of the
network, we find that approximately equivariant networks are able to relax
their own equivariant constraints, causing them to match or outperform exactly
equivariant networks on common benchmark datasets.",None,-1
c963b73c-f2af-43c5-a1aa-2bdc4d81dc12,FV-MgNet: Fully Connected V-cycle MgNet for Interpretable Time Series Forecasting,0.19287,"By investigating iterative methods for a constrained linear model, we propose
a new class of fully connected V-cycle MgNet for long-term time series
forecasting, which is one of the most difficult tasks in forecasting. MgNet is
a CNN model that was proposed for image classification based on the multigrid
(MG) methods for solving discretized partial differential equations (PDEs). We
replace the convolutional operations with fully connected operations in the
existing MgNet and then apply them to forecasting problems. Motivated by the
V-cycle structure in MG, we further propose the FV-MgNet, a V-cycle version of
the fully connected MgNet, to extract features hierarchically. By evaluating
the performance of FV-MgNet on popular data sets and comparing it with
state-of-the-art models, we show that the FV-MgNet achieves better results with
less memory usage and faster inference speed. In addition, we develop ablation
experiments to demonstrate that the structure of FV-MgNet is the best choice
among the many variants.",None,-1
6851d828-4151-492a-a41f-c8d7e2515623,A store-and-forward cloud-based telemonitoring system for automatic assessing dysarthria evolution in neurological diseases from video-recording analysis,0.900509,"Background and objectives: Patients suffering from neurological diseases may
develop dysarthria, a motor speech disorder affecting the execution of speech.
Close and quantitative monitoring of dysarthria evolution is crucial for
enabling clinicians to promptly implement patient management strategies and
maximizing effectiveness and efficiency of communication functions in term of
restoring, compensating or adjusting. In the clinical assessment of orofacial
structures and functions, at rest condition or during speech and non-speech
movements, a qualitative evaluation is usually performed, throughout visual
observation. Methods: To overcome limitations posed by qualitative assessments,
this work presents a store-and-forward self-service telemonitoring system that
integrates, within its cloud architecture, a convolutional neural network (CNN)
for analyzing video recordings acquired by individuals with dysarthria. This
architecture, called facial landmark Mask RCNN, aims at locating facial
landmarks as a prior for assessing the orofacial functions related to speech
and examining dysarthria evolution in neurological diseases. Results: When
tested on the Toronto NeuroFace dataset, a publicly available annotated dataset
of video recordings from patients with amyotrophic lateral sclerosis (ALS) and
stroke, the proposed CNN achieved a normalized mean error equal to 1.79 on
localizing the facial landmarks. We also tested our system in a real-life
scenario on 11 bulbar-onset ALS subjects, obtaining promising outcomes in terms
of facial landmark position estimation. Discussion and conclusions: This
preliminary study represents a relevant step towards the use of remote tools to
support clinicians in monitoring the evolution of dysarthria.",None,-1
8e55b149-50b9-4927-9622-1c4130cf3021,Language Models are Few-shot Learners for Prognostic Prediction,0.967815,"Clinical prediction is an essential task in the healthcare industry. However,
the recent success of transformers, on which large language models are built,
has not been extended to this domain. In this research, we explore the use of
transformers and language models in prognostic prediction for immunotherapy
using real-world patients' clinical data and molecular profiles. This paper
investigates the potential of transformers to improve clinical prediction
compared to conventional machine learning approaches and addresses the
challenge of few-shot learning in predicting rare disease areas. The study
benchmarks the efficacy of baselines and language models on prognostic
prediction across multiple cancer types and investigates the impact of
different pretrained language models under few-shot regimes. The results
demonstrate significant improvements in accuracy and highlight the potential of
NLP in clinical research to improve early detection and intervention for
different diseases.",None,-1
577eb3b1-4432-4c40-b84d-4522c28de2ac,SURE-Val: Safe Urban Relevance Extension and Validation,0.147519,"To evaluate perception components of an automated driving system, it is
necessary to define the relevant objects. While the urban domain is popular
among perception datasets, relevance is insufficiently specified for this
domain. Therefore, this work adopts an existing method to define relevance in
the highway domain and expands it to the urban domain. While different
conceptualizations and definitions of relevance are present in literature,
there is a lack of methods to validate these definitions. Therefore, this work
presents a novel relevance validation method leveraging a motion prediction
component. The validation leverages the idea that removing irrelevant objects
should not influence a prediction component which reflects human driving
behavior. The influence on the prediction is quantified by considering the
statistical distribution of prediction performance across a large-scale
dataset. The validation procedure is verified using criteria specifically
designed to exclude relevant objects. The validation method is successfully
applied to the relevance criteria from this work, thus supporting their
validity.",None,-1
ca9b884d-e10e-49a1-80be-402056e4a109,xDeepInt: a hybrid architecture for modeling the vector-wise and bit-wise feature interactions,0.165554,"Learning feature interactions is the key to success for the large-scale CTR
prediction and recommendation. In practice, handcrafted feature engineering
usually requires exhaustive searching. In order to reduce the high cost of
human efforts in feature engineering, researchers propose several deep neural
networks (DNN)-based approaches to learn the feature interactions in an
end-to-end fashion. However, existing methods either do not learn both
vector-wise interactions and bit-wise interactions simultaneously, or fail to
combine them in a controllable manner. In this paper, we propose a new model,
xDeepInt, based on a novel network architecture called polynomial interaction
network (PIN) which learns higher-order vector-wise interactions recursively.
By integrating subspace-crossing mechanism, we enable xDeepInt to balance the
mixture of vector-wise and bit-wise feature interactions at a bounded order.
Based on the network architecture, we customize a combined optimization
strategy to conduct feature selection and interaction selection. We implement
the proposed model and evaluate the model performance on three real-world
datasets. Our experiment results demonstrate the efficacy and effectiveness of
xDeepInt over state-of-the-art models. We open-source the TensorFlow
implementation of xDeepInt: https://github.com/yanyachen/xDeepInt.",None,-1
b83f21bb-e90f-45fe-b548-e7d055f45faf,Neural Refinement for Absolute Pose Regression with Feature Synthesis,0.42724,"Absolute Pose Regression (APR) methods use deep neural networks to directly
regress camera poses from RGB images. However, the predominant APR
architectures only rely on 2D operations during inference, resulting in limited
accuracy of pose estimation due to the lack of 3D geometry constraints or
priors. In this work, we propose a test-time refinement pipeline that leverages
implicit geometric constraints using a robust feature field to enhance the
ability of APR methods to use 3D information during inference. We also
introduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3D
geometric features during training and directly renders dense novel view
features at test time to refine APR methods. To enhance the robustness of our
model, we introduce a feature fusion module and a progressive training
strategy. Our proposed method achieves state-of-the-art single-image APR
accuracy on indoor and outdoor datasets.",None,-1
611c4edd-2ada-42da-bd6d-2889aadbb062,On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective,0.990781,"ChatGPT is a recent chatbot service released by OpenAI and is receiving
increasing attention over the past few months. While evaluations of various
aspects of ChatGPT have been done, its robustness, i.e., the performance to
unexpected inputs, is still unclear to the public. Robustness is of particular
concern in responsible AI, especially for safety-critical applications. In this
paper, we conduct a thorough evaluation of the robustness of ChatGPT from the
adversarial and out-of-distribution (OOD) perspective. To do so, we employ the
AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart
review and DDXPlus medical diagnosis datasets for OOD evaluation. We select
several popular foundation models as baselines. Results show that ChatGPT shows
consistent advantages on most adversarial and OOD classification and
translation tasks. However, the absolute performance is far from perfection,
which suggests that adversarial and OOD robustness remains a significant threat
to foundation models. Moreover, ChatGPT shows astounding performance in
understanding dialogue-related texts and we find that it tends to provide
informal suggestions for medical tasks instead of definitive answers. Finally,
we present in-depth discussions of possible research directions.",None,-1
4414a9fe-1c77-4925-b3c4-8a7cedf71da1,Ensemble Distillation for Unsupervised Constituency Parsing,0.69139,"We investigate the unsupervised constituency parsing task, which organizes
words and phrases of a sentence into a hierarchical structure without using
linguistically annotated data. We observe that existing unsupervised parsers
capture differing aspects of parsing structures, which can be leveraged to
enhance unsupervised parsing performance. To this end, we propose a notion of
""tree averaging,"" based on which we further propose a novel ensemble method for
unsupervised parsing. To improve inference efficiency, we further distill the
ensemble knowledge into a student model; such an ensemble-then-distill process
is an effective approach to mitigate the over-smoothing problem existing in
common multi-teacher distilling methods. Experiments show that our method
surpasses all previous approaches, consistently demonstrating its effectiveness
and robustness across various runs, with different ensemble components, and
under domain-shift conditions.",None,-1
3921717e-78be-4a76-a11f-055358955bfa,Self-supervised 3D Human Pose Estimation from a Single Image,0.252976,"We propose a new self-supervised method for predicting 3D human body pose
from a single image. The prediction network is trained from a dataset of
unlabelled images depicting people in typical poses and a set of unpaired 2D
poses. By minimising the need for annotated data, the method has the potential
for rapid application to pose estimation of other articulated structures (e.g.
animals). The self-supervision comes from an earlier idea exploiting
consistency between predicted pose under 3D rotation. Our method is a
substantial advance on state-of-the-art self-supervised methods in training a
mapping directly from images, without limb articulation constraints or any 3D
empirical pose prior. We compare performance with state-of-the-art
self-supervised methods using benchmark datasets that provide images and
ground-truth 3D pose (Human3.6M, MPI-INF-3DHP). Despite the reduced requirement
for annotated data, we show that the method outperforms on Human3.6M and
matches performance on MPI-INF-3DHP. Qualitative results on a dataset of human
hands show the potential for rapidly learning to predict 3D pose for
articulated structures other than the human body.",None,-1
1d3c724b-80ae-4e5f-8430-4362df6b0155,Both eyes open: Vigilant Incentives help Regulatory Markets improve AI Safety,0.184282,"In the context of rapid discoveries by leaders in AI, governments must
consider how to design regulation that matches the increasing pace of new AI
capabilities. Regulatory Markets for AI is a proposal designed with
adaptability in mind. It involves governments setting outcome-based targets for
AI companies to achieve, which they can show by purchasing services from a
market of private regulators. We use an evolutionary game theory model to
explore the role governments can play in building a Regulatory Market for AI
systems that deters reckless behaviour. We warn that it is alarmingly easy to
stumble on incentives which would prevent Regulatory Markets from achieving
this goal. These 'Bounty Incentives' only reward private regulators for
catching unsafe behaviour. We argue that AI companies will likely learn to
tailor their behaviour to how much effort regulators invest, discouraging
regulators from innovating. Instead, we recommend that governments always
reward regulators, except when they find that those regulators failed to detect
unsafe behaviour that they should have. These 'Vigilant Incentives' could
encourage private regulators to find innovative ways to evaluate cutting-edge
AI systems.",None,-1
02242fb1-9309-4ed9-bfa8-c36c8e347cd6,Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model,0.0154318,"Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE)
have proven effective in scaling up Transformers model size for
\textit{pretraining} large language models. By only activating part of the FFN
parameters conditioning on input, S-FFN improves generalization performance
while keeping training and inference costs (in FLOPs) fixed. In this work, we
analyzed two major design choices of S-FFN: the memory block (a.k.a. expert)
size and the memory block selection method under a general conceptual framework
of sparse neural memory. Using this unified framework, we compare several S-FFN
architectures for language modeling and provide insights into their relative
efficacy and efficiency. We found a simpler selection method --
\textbf{\texttt{Avg-K}} that selects blocks through their mean aggregated
hidden states, achieving lower perplexity in language model pretraining
compared to existing MoE architectures including Switch Transformer (Fedus et
al., 2021) and HashLayer (Roller et al., 2021).",None,-1
d93c7aa8-f097-4a32-9e6b-aa244e833e93,SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation,0.552713,"Detecting objects and estimating their 6D poses is essential for automated
systems to interact safely with the environment. Most 6D pose estimators,
however, rely on a single camera frame and suffer from occlusions and
ambiguities due to object symmetries. We overcome this issue by presenting a
novel symmetry-aware multi-view 6D pose estimator called SyMFM6D. Our approach
efficiently fuses the RGB-D frames from multiple perspectives in a deep
multi-directional fusion network and predicts predefined keypoints for all
objects in the scene simultaneously. Based on the keypoints and an instance
semantic segmentation, we efficiently compute the 6D poses by least-squares
fitting. To address the ambiguity issues for symmetric objects, we propose a
novel training procedure for symmetry-aware keypoint detection including a new
objective function. Our SyMFM6D network significantly outperforms the
state-of-the-art in both single-view and multi-view 6D pose estimation. We
furthermore show the effectiveness of our symmetry-aware training procedure and
demonstrate that our approach is robust towards inaccurate camera calibration
and dynamic camera setups.",None,-1
139907fd-a32e-46b1-92eb-acc4a332cb7b,GET3D--: Learning GET3D from Unconstrained Image Collections,0.078035,"The demand for efficient 3D model generation techniques has grown
exponentially, as manual creation of 3D models is time-consuming and requires
specialized expertise. While generative models have shown potential in creating
3D textured shapes from 2D images, their applicability in 3D industries is
limited due to the lack of a well-defined camera distribution in real-world
scenarios, resulting in low-quality shapes. To overcome this limitation, we
propose GET3D--, the first method that directly generates textured 3D shapes
from 2D images with unknown pose and scale. GET3D-- comprises a 3D shape
generator and a learnable camera sampler that captures the 6D external changes
on the camera. In addition, We propose a novel training schedule to stably
optimize both the shape generator and camera sampler in a unified framework. By
controlling external variations using the learnable camera sampler, our method
can generate aligned shapes with clear textures. Extensive experiments
demonstrate the efficacy of GET3D--, which precisely fits the 6D camera pose
distribution and generates high-quality shapes on both synthetic and realistic
unconstrained datasets.",None,-1
54e16218-ceb2-46a6-991f-4b6634f03e65,Deep Task-specific Bottom Representation Network for Multi-Task Recommendation,0.147916,"Neural-based multi-task learning (MTL) has gained significant improvement,
and it has been successfully applied to recommendation system (RS). Recent deep
MTL methods for RS (e.g. MMoE, PLE) focus on designing soft gating-based
parameter-sharing networks that implicitly learn a generalized representation
for each task. However, MTL methods may suffer from performance degeneration
when dealing with conflicting tasks, as negative transfer effects can occur on
the task-shared bottom representation. This can result in a reduced capacity
for MTL methods to capture task-specific characteristics, ultimately impeding
their effectiveness and hindering the ability to generalize well on all tasks.
In this paper, we focus on the bottom representation learning of MTL in RS and
propose the Deep Task-specific Bottom Representation Network (DTRN) to
alleviate the negative transfer problem. DTRN obtains task-specific bottom
representation explicitly by making each task have its own representation
learning network in the bottom representation modeling stage. Specifically, it
extracts the user's interests from multiple types of behavior sequences for
each task through the parameter-efficient hypernetwork. To further obtain the
dedicated representation for each task, DTRN refines the representation of each
feature by employing a SENet-like network for each task. The two proposed
modules can achieve the purpose of getting task-specific bottom representation
to relieve tasks' mutual interference. Moreover, the proposed DTRN is flexible
to combine with existing MTL methods. Experiments on one public dataset and one
industrial dataset demonstrate the effectiveness of the proposed DTRN.",None,-1
2bc8ad9e-6db6-4e2f-a60f-1c463a4fb8b4,Evolutionary approaches to explainable machine learning,0.202049,"Machine learning models are increasingly being used in critical sectors, but
their black-box nature has raised concerns about accountability and trust. The
field of explainable artificial intelligence (XAI) or explainable machine
learning (XML) has emerged in response to the need for human understanding of
these models. Evolutionary computing, as a family of powerful optimization and
learning tools, has significant potential to contribute to XAI/XML. In this
chapter, we provide a brief introduction to XAI/XML and review various
techniques in current use for explaining machine learning models. We then focus
on how evolutionary computing can be used in XAI/XML, and review some
approaches which incorporate EC techniques. We also discuss some open
challenges in XAI/XML and opportunities for future research in this field using
EC. Our aim is to demonstrate that evolutionary computing is well-suited for
addressing current problems in explainability, and to encourage further
exploration of these methods to contribute to the development of more
transparent, trustworthy and accountable machine learning models.",None,-1
fc16488a-9d8d-41e8-a6e2-3969e5c0303a,CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models,0.611815,"We propose CHiLL (Crafting High-Level Latents), an approach for
natural-language specification of features for linear models. CHiLL prompts
LLMs with expert-crafted queries to generate interpretable features from health
records. The resulting noisy labels are then used to train a simple linear
classifier. Generating features based on queries to an LLM can empower
physicians to use their domain expertise to craft features that are clinically
meaningful for a downstream task of interest, without having to manually
extract these from raw EHR. We are motivated by a real-world risk prediction
task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and
standard predictive tasks (e.g., 30-day readmission) to evaluate this approach.
We find that linear models using automatically extracted features are
comparably performant to models using reference features, and provide greater
interpretability than linear models using ""Bag-of-Words"" features. We verify
that learned feature weights align well with clinical expectations.",None,-1
0103746f-b70d-4920-8a3d-b41bb58ea759,Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA,0.807384,"Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with factual information such as entities and relations in KBs.
However, traditional Pre-trained Language Models (PLMs) are directly
pre-trained on large-scale natural language corpus, which poses challenges for
them in understanding and representing complex subgraphs in structured KBs. To
bridge the gap between texts and structured KBs, we propose a Structured
Knowledge-aware Pre-training method (SKP). In the pre-training stage, we
introduce two novel structured knowledge-aware tasks, guiding the model to
effectively learn the implicit relationship and better representations of
complex subgraphs. In downstream KBQA task, we further design an efficient
linearization strategy and an interval attention mechanism, which assist the
model to better encode complex subgraphs and shield the interference of
irrelevant subgraphs during reasoning respectively. Detailed experiments and
analyses on WebQSP verify the effectiveness of SKP, especially the significant
improvement in subgraph retrieval (+4.08% H@10).",None,-1
cccf105f-304f-4150-98d5-cc66f9eb913a,Interpreting Pretrained Language Models via Concept Bottlenecks,0.220251,"Pretrained language models (PLMs) have made significant strides in various
natural language processing tasks. However, the lack of interpretability due to
their ``black-box'' nature poses challenges for responsible implementation.
Although previous studies have attempted to improve interpretability by using,
e.g., attention weights in self-attention layers, these weights often lack
clarity, readability, and intuitiveness. In this research, we propose a novel
approach to interpreting PLMs by employing high-level, meaningful concepts that
are easily understandable for humans. For example, we learn the concept of
``Food'' and investigate how it influences the prediction of a model's
sentiment towards a restaurant review. We introduce C$^3$M, which combines
human-annotated and machine-generated concepts to extract hidden neurons
designed to encapsulate semantically meaningful and task-specific concepts.
Through empirical evaluations on real-world datasets, we manifest that our
approach offers valuable insights to interpret PLM behavior, helps diagnose
model failures, and enhances model robustness amidst noisy concept labels.",None,-1
fcdc2374-a268-4527-9360-ccf226680ab9,No Pitch Left Behind: Addressing Gender Unbalance in Automatic Speech Recognition through Pitch Manipulation,0.321212,"Automatic speech recognition (ASR) systems are known to be sensitive to the
sociolinguistic variability of speech data, in which gender plays a crucial
role. This can result in disparities in recognition accuracy between male and
female speakers, primarily due to the under-representation of the latter group
in the training data. While in the context of hybrid ASR models several
solutions have been proposed, the gender bias issue has not been explicitly
addressed in end-to-end neural architectures. To fill this gap, we propose a
data augmentation technique that manipulates the fundamental frequency (f0) and
formants. This technique reduces the data unbalance among genders by simulating
voices of the under-represented female speakers and increases the variability
within each gender group. Experiments on spontaneous English speech show that
our technique yields a relative WER improvement up to 9.87% for utterances by
female speakers, with larger gains for the least-represented f0 ranges.",None,-1
d6e278e1-af78-4023-9717-0ac9f13d2a76,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,0.0805024,"The StyleGAN family succeed in high-fidelity image generation and allow for
flexible and plausible editing of generated images by manipulating the
semantic-rich latent style space.However, projecting a real image into its
latent space encounters an inherent trade-off between inversion quality and
editability. Existing encoder-based or optimization-based StyleGAN inversion
methods attempt to mitigate the trade-off but see limited performance. To
fundamentally resolve this problem, we propose a novel two-phase framework by
designating two separate networks to tackle editing and reconstruction
respectively, instead of balancing the two. Specifically, in Phase I, a
W-space-oriented StyleGAN inversion network is trained and used to perform
image inversion and editing, which assures the editability but sacrifices
reconstruction quality. In Phase II, a carefully designed rectifying network is
utilized to rectify the inversion errors and perform ideal reconstruction.
Experimental results show that our approach yields near-perfect reconstructions
without sacrificing the editability, thus allowing accurate manipulation of
real images. Further, we evaluate the performance of our rectifying network,
and see great generalizability towards unseen manipulation types and
out-of-domain images.",None,-1
baee610c-6fcd-43cf-ba97-582e75d0a995,Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games,0.663078,"We introduce a class of networked Markov potential games in which agents are
associated with nodes in a network. Each agent has its own local potential
function, and the reward of each agent depends only on the states and actions
of the agents within a neighborhood. In this context, we propose a localized
actor-critic algorithm. The algorithm is scalable since each agent uses only
local information and does not need access to the global state. Further, the
algorithm overcomes the curse of dimensionality through the use of function
approximation. Our main results provide finite-sample guarantees up to a
localization error and a function approximation error. Specifically, we achieve
an $\tilde{\mathcal{O}}(\tilde{\epsilon}^{-4})$ sample complexity measured by
the averaged Nash regret. This is the first finite-sample bound for multi-agent
competitive games that does not depend on the number of agents.",None,-1
72bbd099-c68c-4124-9287-a77d473fc11f,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,0.624026,"Recent learning-based video quality assessment (VQA) algorithms are expensive
to implement due to the cost of data collection of human quality opinions, and
are less robust across various scenarios due to the biases of these opinions.
This motivates our exploration on opinion-unaware (a.k.a zero-shot) VQA
approaches. Existing approaches only considers low-level naturalness in spatial
or temporal domain, without considering impacts from high-level semantics. In
this work, we introduce an explicit semantic affinity index for opinion-unaware
VQA using text-prompts in the contrastive language-image pre-training (CLIP)
model. We also aggregate it with different traditional low-level naturalness
indexes through gaussian normalization and sigmoid rescaling strategies.
Composed of aggregated semantic and technical metrics, the proposed Blind
Unified Opinion-Unaware Video Quality Index via Semantic and Technical Metric
Aggregation (BUONA-VISTA) outperforms existing opinion-unaware VQA methods by
at least 20% improvements, and is more robust than opinion-aware approaches.",None,-1
3850aa9e-9080-4efd-92ca-54dadc673df9,Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision,0.212929,"We address the challenging problem of jointly inferring the 3D flow and
volumetric densities moving in a fluid from a monocular input video with a deep
neural network. Despite the complexity of this task, we show that it is
possible to train the corresponding networks without requiring any 3D ground
truth for training. In the absence of ground truth data we can train our model
with observations from real-world capture setups instead of relying on
synthetic reconstructions. We make this unsupervised training approach possible
by first generating an initial prototype volume which is then moved and
transported over time without the need for volumetric supervision. Our approach
relies purely on image-based losses, an adversarial discriminator network, and
regularization. Our method can estimate long-term sequences in a stable manner,
while achieving closely matching targets for inputs such as rising smoke
plumes.",None,-1
9a9fc235-a21c-4b63-bdd5-1fd314e16dc8,Evaluating Self-Supervised Speech Representations for Indigenous American Languages,0.618935,"The application of self-supervision to speech representation learning has
garnered significant interest in recent years, due to its scalability to large
amounts of unlabeled data. However, much progress, both in terms of
pre-training and downstream evaluation, has remained concentrated in
monolingual models that only consider English. Few models consider other
languages, and even fewer consider indigenous ones. In our submission to the
New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR
corpus for Quechua, an indigenous South American Language. We benchmark the
efficacy of large SSL models on Quechua, along with 6 other indigenous
languages such as Guarani and Bribri, on low-resource ASR. Our results show
surprisingly strong performance by state-of-the-art SSL models, showing the
potential generalizability of large-scale models to real-world data.",None,-1
bf221179-b501-440b-a3d9-0cbdb34c42bf,Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation,0.980614,"Self-supervised and language-supervised image models contain rich knowledge
of the world that is important for generalization. Many robotic tasks, however,
require a detailed understanding of 3D geometry, which is often lacking in 2D
image features. This work bridges this 2D-to-3D gap for robotic manipulation by
leveraging distilled feature fields to combine accurate 3D geometry with rich
semantics from 2D foundation models. We present a few-shot learning method for
6-DOF grasping and placing that harnesses these strong spatial and semantic
priors to achieve in-the-wild generalization to unseen objects. Using features
distilled from a vision-language model, CLIP, we present a way to designate
novel objects for manipulation via free-text natural language, and demonstrate
its ability to generalize to unseen expressions and novel categories of
objects.",None,-1
742db1b7-68b1-4dbb-aa96-07ed5e4eac59,Controlling Pre-trained Language Models for Grade-Specific Text Simplification,0.0745011,"Text simplification (TS) systems rewrite text to make it more readable while
preserving its content. However, what makes a text easy to read depends on the
intended readers. Recent work has shown that pre-trained language models can
simplify text using a wealth of techniques to control output simplicity,
ranging from specifying only the desired reading grade level, to directly
specifying low-level edit operations. Yet it remains unclear how to set these
control parameters in practice. Existing approaches set them at the corpus
level, disregarding the complexity of individual inputs and considering only
one level of output complexity. In this work, we conduct an empirical study to
understand how different control mechanisms impact the adequacy and simplicity
of text simplification systems. Based on these insights, we introduce a simple
method that predicts the edit operations required for simplifying a text for a
specific grade level on an instance-per-instance basis. This approach improves
the quality of the simplified outputs over corpus-level search-based
heuristics.",None,-1
32fe5634-3f1c-45f6-a881-53023e8197f7,Refining 3D Human Texture Estimation from a Single Image,0.158128,"Estimating 3D human texture from a single image is essential in graphics and
vision. It requires learning a mapping function from input images of humans
with diverse poses into the parametric (UV) space and reasonably hallucinating
invisible parts. To achieve a high-quality 3D human texture estimation, we
propose a framework that adaptively samples the input by a deformable
convolution where offsets are learned via a deep neural network. Additionally,
we describe a novel cycle consistency loss that improves view generalization.
We further propose to train our framework with an uncertainty-based pixel-level
image reconstruction loss, which enhances color fidelity. We compare our method
against the state-of-the-art approaches and show significant qualitative and
quantitative improvements.",None,-1
a9b0173d-2266-4007-80bc-2e6cfd20629e,"What's ""up"" with vision-language models? Investigating their struggle with spatial reasoning",0.527831,"Recent vision-language (VL) models are powerful, but can they reliably
distinguish ""right"" from ""left""? We curate three new corpora to quantify model
comprehension of such basic spatial relations. These tests isolate spatial
reasoning more precisely than existing datasets like VQAv2, e.g., our What'sUp
benchmark contains sets of photographs varying only the spatial relations of
objects, keeping their identity fixed (see Figure 1: models must comprehend not
only the usual case of a dog under a table, but also, the same dog on top of
the same table). We evaluate 18 VL models, finding that all perform poorly,
e.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56%
accuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of
this surprising behavior, finding: 1) that popular vision-language pretraining
corpora like LAION-2B contain little reliable data for learning spatial
relationships; and 2) that basic modeling interventions like up-weighting
preposition-containing instances or fine-tuning on our corpora are not
sufficient to address the challenges our benchmarks pose. We are hopeful that
these corpora will facilitate further research, and we release our data and
code at https://github.com/amitakamath/whatsup_vlms.",None,-1
03764a62-e430-40f6-b73b-dca0d9201883,UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View,0.894304,"In the field of 3D object detection for autonomous driving, the sensor
portfolio including multi-modality and single-modality is diverse and complex.
Since the multi-modal methods have system complexity while the accuracy of
single-modal ones is relatively low, how to make a tradeoff between them is
difficult. In this work, we propose a universal cross-modality knowledge
distillation framework (UniDistill) to improve the performance of
single-modality detectors. Specifically, during training, UniDistill projects
the features of both the teacher and the student detector into Bird's-Eye-View
(BEV), which is a friendly representation for different modalities. Then, three
distillation losses are calculated to sparsely align the foreground features,
helping the student learn from the teacher without introducing additional cost
during inference. Taking advantage of the similar detection paradigm of
different detectors in BEV, UniDistill easily supports LiDAR-to-camera,
camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths.
Furthermore, the three distillation losses can filter the effect of misaligned
background information and balance between objects of different sizes,
improving the distillation effectiveness. Extensive experiments on nuScenes
demonstrate that UniDistill effectively improves the mAP and NDS of student
detectors by 2.0%~3.2%.",None,-1
1331d77f-f1fc-4fd7-b9f5-be58ce65c247,Mixing Backward- with Forward-Chaining for Metacognitive Skill Acquisition and Transfer,0.978073,"Metacognitive skills have been commonly associated with preparation for
future learning in deductive domains. Many researchers have regarded strategy-
and time-awareness as two metacognitive skills that address how and when to use
a problem-solving strategy, respectively. It was shown that students who are
both strategy-and time-aware (StrTime) outperformed their nonStrTime peers
across deductive domains. In this work, students were trained on a logic tutor
that supports a default forward-chaining (FC) and a backward-chaining (BC)
strategy. We investigated the impact of mixing BC with FC on teaching strategy-
and time-awareness for nonStrTime students. During the logic instruction, the
experimental students (Exp) were provided with two BC worked examples and some
problems in BC to practice how and when to use BC. Meanwhile, their control
(Ctrl) and StrTime peers received no such intervention. Six weeks later, all
students went through a probability tutor that only supports BC to evaluate
whether the acquired metacognitive skills are transferred from logic. Our
results show that on both tutors, Exp outperformed Ctrl and caught up with
StrTime.",None,-1
160ebf9f-7ae2-4a67-93d9-10f2777c305e,Emergent Communication with Attention,0.437692,"To develop computational agents that better communicate using their own
emergent language, we endow the agents with an ability to focus their attention
on particular concepts in the environment. Humans often understand an object or
scene as a composite of concepts and those concepts are further mapped onto
words. We implement this intuition as cross-modal attention mechanisms in
Speaker and Listener agents in a referential game and show attention leads to
more compositional and interpretable emergent language. We also demonstrate how
attention aids in understanding the learned communication protocol by
investigating the attention weights associated with each message symbol and the
alignment of attention weights between Speaker and Listener agents. Overall,
our results suggest that attention is a promising mechanism for developing more
human-like emergent language.",None,-1
027e1e99-0b7c-458d-8a47-37e6be0f6514,Linguistic Properties of Truthful Response,0.0515015,"We investigate the phenomenon of an LLM's untruthful response using a large
set of 220 handcrafted linguistic features. We focus on GPT-3 models and find
that the linguistic profiles of responses are similar across model sizes. That
is, how varying-sized LLMs respond to given prompts stays similar on the
linguistic properties level. We expand upon this finding by training support
vector machines that rely only upon the stylistic components of model responses
to classify the truthfulness of statements. Though the dataset size limits our
current findings, we show the possibility that truthfulness detection is
possible without evaluating the content itself. But at the same time, the
limited scope of our experiments must be taken into account in interpreting the
results.",None,-1
b34e937b-ac3c-470f-95c6-4e5778595921,Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs,0.924968,"Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and
can solve different tasks due to their emergent ability and generalizability.
However, LLMs sometimes lack domain-specific knowledge to perform tasks, which
would also cause hallucination during inference. In some previous works,
additional modules like graph neural networks (GNNs) are trained on retrieved
knowledge from external knowledge bases, aiming to mitigate the problem of
lacking domain-specific knowledge. However, incorporating additional modules:
1) would need retraining additional modules when encountering novel domains; 2)
would become a bottleneck since LLMs' strong abilities are not fully utilized
for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver
(KSL), to teach LLMs to search for essential knowledge from external knowledge
bases by harnessing their own strong generalizability. Specifically, we design
a simple yet effective prompt to transform retrieval into a multi-hop decision
sequence, which empowers LLMs with searching knowledge ability in zero-shot
manner. Additionally, KSL is able to provide complete retrieval paths and
therefore increase explainability of LLMs' reasoning processes. We conduct
experiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and
found that our approach improves LLM baseline performance by a relatively large
margin.",None,-1
8891452f-2252-4b88-9f7f-e64a3e383414,FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios,0.900563,"The emergence of generative pre-trained models has facilitated the synthesis
of high-quality text, but it has also posed challenges in identifying factual
errors in the generated text. In particular: (1) A wider range of tasks now
face an increasing risk of containing factual errors when handled by generative
models. (2) Generated texts tend to be lengthy and lack a clearly defined
granularity for individual facts. (3) There is a scarcity of explicit evidence
available during the process of fact checking. With the above challenges in
mind, in this paper, we propose FacTool, a task and domain agnostic framework
for detecting factual errors of texts generated by large language models (e.g.,
ChatGPT). Experiments on four different tasks (knowledge-based QA, code
generation, mathematical reasoning, and scientific literature review) show the
efficacy of the proposed method. We release the code of FacTool associated with
ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",None,-1
122ca469-f714-4c75-aef8-b88dbf46b586,Compositional 3D Human-Object Neural Animation,0.486507,"Human-object interactions (HOIs) are crucial for human-centric scene
understanding applications such as human-centric visual generation, AR/VR, and
robotics. Since existing methods mainly explore capturing HOIs, rendering HOI
remains less investigated. In this paper, we address this challenge in HOI
animation from a compositional perspective, i.e., animating novel HOIs
including novel interaction, novel human and/or novel object driven by a novel
pose sequence. Specifically, we adopt neural human-object deformation to model
and render HOI dynamics based on implicit neural representations. To enable the
interaction pose transferring among different persons and objects, we then
devise a new compositional conditional neural radiance field (or CC-NeRF),
which decomposes the interdependence between human and object using latent
codes to enable compositionally animation control of novel HOIs. Experiments
show that the proposed method can generalize well to various novel HOI
animation settings. Our project page is https://zhihou7.github.io/CHONA/",None,-1
79a1a327-edc4-494a-8a0f-edeba964bb7e,Boosting Video Object Segmentation via Space-time Correspondence Learning,0.66963,"Current top-leading solutions for video object segmentation (VOS) typically
follow a matching-based regime: for each query frame, the segmentation mask is
inferred according to its correspondence to previously processed and the first
annotated frames. They simply exploit the supervisory signals from the
groundtruth masks for learning mask prediction only, without posing any
constraint on the space-time correspondence matching, which, however, is the
fundamental building block of such regime. To alleviate this crucial yet
commonly ignored issue, we devise a correspondence-aware training framework,
which boosts matching-based VOS solutions by explicitly encouraging robust
correspondence matching during network learning. Through comprehensively
exploring the intrinsic coherence in videos on pixel and object levels, our
algorithm reinforces the standard, fully supervised training of mask
segmentation with label-free, contrastive correspondence learning. Without
neither requiring extra annotation cost during training, nor causing speed
delay during deployment, nor incurring architectural modification, our
algorithm provides solid performance gains on four widely used benchmarks,
i.e., DAVIS2016&2017, and YouTube-VOS2018&2019, on the top of famous
matching-based VOS solutions.",None,-1
a8179bf9-0f01-4fc6-a4e6-33f6ff5f5a62,Modelling Temporal Document Sequences for Clinical ICD Coding,0.499543,"Past studies on the ICD coding problem focus on predicting clinical codes
primarily based on the discharge summary. This covers only a small fraction of
the notes generated during each hospital stay and leaves potential for
improving performance by analysing all the available clinical notes. We propose
a hierarchical transformer architecture that uses text across the entire
sequence of clinical notes in each hospital stay for ICD coding, and
incorporates embeddings for text metadata such as their position, time, and
type of note. While using all clinical notes increases the quantity of data
substantially, superconvergence can be used to reduce training costs. We
evaluate the model on the MIMIC-III dataset. Our model exceeds the prior
state-of-the-art when using only discharge summaries as input, and achieves
further performance improvements when all clinical notes are used as input.",None,-1
bd49f794-5ade-447d-b80c-6260086bb9a4,TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering,0.0823187,"We present a new pipeline for acquiring a textured mesh in the wild with a
single smartphone which offers access to images, depth maps, and valid poses.
Our method first introduces an RGBD-aided structure from motion, which can
yield filtered depth maps and refines camera poses guided by corresponding
depth. Then, we adopt the neural implicit surface reconstruction method, which
allows for high-quality mesh and develops a new training process for applying a
regularization provided by classical multi-view stereo methods. Moreover, we
apply a differentiable rendering to fine-tune incomplete texture maps and
generate textures which are perceptually closer to the original scene. Our
pipeline can be applied to any common objects in the real world without the
need for either in-the-lab environments or accurate mask images. We demonstrate
results of captured objects with complex shapes and validate our method
numerically against existing 3D reconstruction and texture mapping methods.",None,-1
2be0d596-faf1-44d6-994d-990d991d0b3b,Back Translation for Speech-to-text Translation Without Transcripts,0.896315,"The success of end-to-end speech-to-text translation (ST) is often achieved
by utilizing source transcripts, e.g., by pre-training with automatic speech
recognition (ASR) and machine translation (MT) tasks, or by introducing
additional ASR and MT data. Unfortunately, transcripts are only sometimes
available since numerous unwritten languages exist worldwide. In this paper, we
aim to utilize large amounts of target-side monolingual data to enhance ST
without transcripts. Motivated by the remarkable success of back translation in
MT, we develop a back translation algorithm for ST (BT4ST) to synthesize pseudo
ST data from monolingual target data. To ease the challenges posed by
short-to-long generation and one-to-many mapping, we introduce self-supervised
discrete units and achieve back translation by cascading a target-to-unit model
and a unit-to-speech model. With our synthetic ST data, we achieve an average
boost of 2.3 BLEU on MuST-C En-De, En-Fr, and En-Es datasets. More experiments
show that our method is especially effective in low-resource scenarios.",None,-1
cda470f7-dd54-4ac4-8132-ce1b25018c25,Motif: Intrinsic Motivation from Artificial Intelligence Feedback,0.548875,"Exploring rich environments and evaluating one's actions without prior
knowledge is immensely challenging. In this paper, we propose Motif, a general
method to interface such prior knowledge from a Large Language Model (LLM) with
an agent. Motif is based on the idea of grounding LLMs for decision-making
without requiring them to interact with the environment: it elicits preferences
from an LLM over pairs of captions to construct an intrinsic reward, which is
then used to train agents with reinforcement learning. We evaluate Motif's
performance and behavior on the challenging, open-ended and
procedurally-generated NetHack game. Surprisingly, by only learning to maximize
its intrinsic reward, Motif achieves a higher game score than an algorithm
directly trained to maximize the score itself. When combining Motif's intrinsic
reward with the environment reward, our method significantly outperforms
existing approaches and makes progress on tasks where no advancements have ever
been made without demonstrations. Finally, we show that Motif mostly generates
intuitive human-aligned behaviors which can be steered easily through prompt
modifications, while scaling well with the LLM size and the amount of
information given in the prompt.",None,-1
0ed7d332-8eb8-48e8-9bf0-625fa92d5d61,InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent,0.882771,"This research paper delves into the integration of OpenAI's ChatGPT into
embodied agent systems, evaluating its influence on interactive decision-making
benchmark. Drawing a parallel to the concept of people assuming roles according
to their unique strengths, we introduce InterAct. In this approach, we feed
ChatGPT with varied prompts, assigning it a numerous roles like a checker and a
sorter, then integrating them with the original language model. Our research
shows a remarkable success rate of 98% in AlfWorld, which consists of 6
different tasks in a simulated household environment, emphasizing the
significance of proficient prompt engineering. The results highlight ChatGPT's
competence in comprehending and performing intricate tasks effectively in
real-world settings, thus paving the way for further advancements in task
planning.",None,-1
52e7f00d-28e7-43b9-824c-3ae0507df10c,Mitigating Bias in Visual Transformers via Targeted Alignment,0.142365,"As transformer architectures become increasingly prevalent in computer
vision, it is critical to understand their fairness implications. We perform
the first study of the fairness of transformers applied to computer vision and
benchmark several bias mitigation approaches from prior work. We visualize the
feature space of the transformer self-attention modules and discover that a
significant portion of the bias is encoded in the query matrix. With this
knowledge, we propose TADeT, a targeted alignment strategy for debiasing
transformers that aims to discover and remove bias primarily from query matrix
features. We measure performance using Balanced Accuracy and Standard Accuracy,
and fairness using Equalized Odds and Balanced Accuracy Difference. TADeT
consistently leads to improved fairness over prior work on multiple attribute
prediction tasks on the CelebA dataset, without compromising performance.",None,-1
2c8f002d-b17d-46db-92a3-eb0bd8e4a06a,"Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers",0.121976,"The rapid growth of scientific publications, particularly during the COVID-19
pandemic, emphasizes the need for tools to help researchers efficiently
comprehend the latest advancements. One essential part of understanding
scientific literature is research aspect classification, which categorizes
sentences in abstracts to Background, Purpose, Method, and Finding. In this
study, we investigate the impact of different datasets on model performance for
the crowd-annotated CODA-19 research aspect classification task. Specifically,
we explore the potential benefits of using the large, automatically curated
PubMed 200K RCT dataset and evaluate the effectiveness of large language models
(LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that
using the PubMed 200K RCT dataset does not improve performance for the CODA-19
task. We also observe that while GPT-4 performs well, it does not outperform
the SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance
of a dedicated and task-aligned datasets dataset for the target task. Our code
is available at https://github.com/Crowd-AI-Lab/CODA-19-exp.",None,-1
bfaaf3f5-4c45-4a06-8917-562fb017cc69,Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations,0.999941,"In this paper, we present an innovative process-oriented math process reward
model called \textbf{Math-Shepherd}, which assigns a reward score to each step
of math problem solutions. The training of Math-Shepherd is achieved using
automatically constructed process-wise supervision data, breaking the
bottleneck of heavy reliance on manual annotation in existing work. We explore
the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}:
Math-Shepherd is utilized for reranking multiple outputs generated by Large
Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is
employed to reinforce LLMs with step-by-step Proximal Policy Optimization
(PPO). With Math-Shepherd, a series of open-source LLMs demonstrates
exceptional performance. For instance, the step-by-step PPO with Math-Shepherd
significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K
and 28.6\%$\to$33.0\% on MATH). The accuracy can be further enhanced to 89.1\%
and 43.5\% on GSM8K and MATH with the verification of Math-Shepherd,
respectively. We believe that automatic process supervision holds significant
potential for the future evolution of LLMs.",None,-1
1057776b-5565-423c-a61f-dbe49e6148bc,LogAI: A Library for Log Analytics and Intelligence,0.172941,"Software and System logs record runtime information about processes executing
within a system. These logs have become the most critical and ubiquitous forms
of observability data that help developers understand system behavior, monitor
system health and resolve issues. However, the volume of logs generated can be
humongous (of the order of petabytes per day) especially for complex
distributed systems, such as cloud, search engine, social media, etc. This has
propelled a lot of research on developing AI-based log based analytics and
intelligence solutions that can process huge volume of raw logs and generate
insights. In order to enable users to perform multiple types of AI-based log
analysis tasks in a uniform manner, we introduce LogAI
(https://github.com/salesforce/logai), a one-stop open source library for log
analytics and intelligence. LogAI supports tasks such as log summarization, log
clustering and log anomaly detection. It adopts the OpenTelemetry data model,
to enable compatibility with different log management platforms. LogAI provides
a unified model interface and provides popular time-series, statistical
learning and deep learning models. Alongside this, LogAI also provides an
out-of-the-box GUI for users to conduct interactive analysis. With LogAI, we
can also easily benchmark popular deep learning algorithms for log anomaly
detection without putting in redundant effort to process the logs. We have
opensourced LogAI to cater to a wide range of applications benefiting both
academic research and industrial prototyping.",None,-1
bff4cb4c-5613-4648-8c9a-11d13be18713,Distributed Marker Representation for Ambiguous Discourse Markers and Entangled Relations,0.55298,"Discourse analysis is an important task because it models intrinsic semantic
structures between sentences in a document. Discourse markers are natural
representations of discourse in our daily language. One challenge is that the
markers as well as pre-defined and human-labeled discourse relations can be
ambiguous when describing the semantics between sentences. We believe that a
better approach is to use a contextual-dependent distribution over the markers
to express discourse information. In this work, we propose to learn a
Distributed Marker Representation (DMR) by utilizing the (potentially)
unlimited discourse marker data with a latent discourse sense, thereby bridging
markers with sentence pairs. Such representations can be learned automatically
from data without supervision, and in turn provide insights into the data
itself. Experiments show the SOTA performance of our DMR on the implicit
discourse relation recognition task and strong interpretability. Our method
also offers a valuable tool to understand complex ambiguity and entanglement
among discourse markers and manually defined discourse relations.",None,-1
fada50da-e180-4d92-8cd7-4eb751760167,Discovering Universal Geometry in Embeddings with ICA,0.760512,"This study utilizes Independent Component Analysis (ICA) to unveil a
consistent semantic structure within embeddings of words or images. Our
approach extracts independent semantic components from the embeddings of a
pre-trained model by leveraging anisotropic information that remains after the
whitening process in Principal Component Analysis (PCA). We demonstrate that
each embedding can be expressed as a composition of a few intrinsic
interpretable axes and that these semantic axes remain consistent across
different languages, algorithms, and modalities. The discovery of a universal
semantic structure in the geometric patterns of embeddings enhances our
understanding of the representations in embeddings.",None,-1
1cb5997a-0653-400b-bfa7-20b9a570d7b2,VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution,0.319419,"Since the introduction of deep learning, a wide scope of representation
properties, such as decorrelation, whitening, disentanglement, rank, isotropy,
and mutual information, have been studied to improve the quality of
representation. However, manipulating such properties can be challenging in
terms of implementational effectiveness and general applicability. To address
these limitations, we propose to regularize von Neumann entropy~(VNE) of
representation. First, we demonstrate that the mathematical formulation of VNE
is superior in effectively manipulating the eigenvalues of the representation
autocorrelation matrix. Then, we demonstrate that it is widely applicable in
improving state-of-the-art algorithms or popular benchmark algorithms by
investigating domain-generalization, meta-learning, self-supervised learning,
and generative models. In addition, we formally establish theoretical
connections with rank, disentanglement, and isotropy of representation.
Finally, we provide discussions on the dimension control of VNE and the
relationship with Shannon entropy. Code is available at:
https://github.com/jaeill/CVPR23-VNE.",None,-1
03079f18-bffc-4c8c-aeb1-3d220dac6ffd,Efficient Generator of Mathematical Expressions for Symbolic Regression,0.123915,"We propose an approach to symbolic regression based on a novel variational
autoencoder for generating hierarchical structures, HVAE. It combines simple
atomic units with shared weights to recursively encode and decode the
individual nodes in the hierarchy. Encoding is performed bottom-up and decoding
top-down. We empirically show that HVAE can be trained efficiently with small
corpora of mathematical expressions and can accurately encode expressions into
a smooth low-dimensional latent space. The latter can be efficiently explored
with various optimization methods to address the task of symbolic regression.
Indeed, random search through the latent space of HVAE performs better than
random search through expressions generated by manually crafted probabilistic
grammars for mathematical expressions. Finally, EDHiE system for symbolic
regression, which applies an evolutionary algorithm to the latent space of
HVAE, reconstructs equations from a standard symbolic regression benchmark
better than a state-of-the-art system based on a similar combination of deep
learning and evolutionary algorithms.\v{z}",None,-1
2c8537cd-8106-425a-8b37-e268af0b113c,Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain,0.523763,"Many NLP tasks, although well-resolved for general English, face challenges
in specific domains like fantasy literature. This is evident in Named Entity
Recognition (NER), which detects and categorizes entities in text. We analyzed
10 NER models on 7 Dungeons and Dragons (D&D) adventure books to assess
domain-specific performance. Using open-source Large Language Models, we
annotated named entities in these books and evaluated each model's precision.
Our findings indicate that, without modifications, Flair, Trankit, and Spacy
outperform others in identifying named entities in the D&D context.",None,-1
432fc815-8193-4892-a05e-b05ce14450f7,AIMS: All-Inclusive Multi-Level Segmentation,0.191332,"Despite the progress of image segmentation for accurate visual entity
segmentation, completing the diverse requirements of image editing applications
for different-level region-of-interest selections remains unsolved. In this
paper, we propose a new task, All-Inclusive Multi-Level Segmentation (AIMS),
which segments visual regions into three levels: part, entity, and relation
(two entities with some semantic relationships). We also build a unified AIMS
model through multi-dataset multi-task training to address the two major
challenges of annotation inconsistency and task correlation. Specifically, we
propose task complementarity, association, and prompt mask encoder for
three-level predictions. Extensive experiments demonstrate the effectiveness
and generalization capacity of our method compared to other state-of-the-art
methods on a single dataset or the concurrent work on segmenting anything. We
will make our code and training model publicly available.",None,-1
e0191b0b-9cfc-4d75-b9ed-43c4cb09de50,Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation,0.135153,"Language models have steadily increased in size over the past few years. They
achieve a high level of performance on various natural language processing
(NLP) tasks such as question answering and summarization. Large language models
(LLMs) have been used for generation and can now output human-like text. Due to
this, there are other downstream tasks in the realm of dialog that can now
harness the LLMs' language understanding capabilities. Dialog evaluation is one
task that this paper will explore. It concentrates on prompting with LLMs:
BLOOM, OPT, GPT-3, Flan-T5, InstructDial and TNLGv2. The paper shows that the
choice of datasets used for training a model contributes to how well it
performs on a task as well as on how the prompt should be structured.
Specifically, the more diverse and relevant the group of datasets that a model
is trained on, the better dialog evaluation performs. This paper also
investigates how the number of examples in the prompt and the type of example
selection used affect the model's performance.",None,-1
0c30c813-58cd-4445-ba7e-e26bd6794224,EFE: End-to-end Frame-to-Gaze Estimation,0.260899,"Despite the recent development of learning-based gaze estimation methods,
most methods require one or more eye or face region crops as inputs and produce
a gaze direction vector as output. Cropping results in a higher resolution in
the eye regions and having fewer confounding factors (such as clothing and
hair) is believed to benefit the final model performance. However, this
eye/face patch cropping process is expensive, erroneous, and
implementation-specific for different methods. In this paper, we propose a
frame-to-gaze network that directly predicts both 3D gaze origin and 3D gaze
direction from the raw frame out of the camera without any face or eye
cropping. Our method demonstrates that direct gaze regression from the raw
downscaled frame, from FHD/HD to VGA/HVGA resolution, is possible despite the
challenges of having very few pixels in the eye region. The proposed method
achieves comparable results to state-of-the-art methods in Point-of-Gaze (PoG)
estimation on three public gaze datasets: GazeCapture, MPIIFaceGaze, and EVE,
and generalizes well to extreme camera view changes.",None,-1
a660ec22-69d0-4444-9e68-99f5328efb54,Text Classification of Cancer Clinical Trial Eligibility Criteria,0.80928,"Automatic identification of clinical trials for which a patient is eligible
is complicated by the fact that trial eligibility is stated in natural
language. A potential solution to this problem is to employ text classification
methods for common types of eligibility criteria. In this study, we focus on
seven common exclusion criteria in cancer trials: prior malignancy, human
immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness,
drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase
III cancer trials with these exclusions annotated at the trial level. We
experiment with common transformer models as well as a new pre-trained clinical
trial BERT model. Our results demonstrate the feasibility of automatically
classifying common exclusion criteria. Additionally, we demonstrate the value
of a pre-trained language model specifically for clinical trials, which yields
the highest average performance across all criteria.",None,-1
671dea2b-3e32-4e17-a1c9-0c0d1f9c0f42,Overwriting Pretrained Bias with Finetuning Data,0.406082,"Transfer learning is beneficial by allowing the expressive features of models
pretrained on large-scale datasets to be finetuned for the target task of
smaller, more domain-specific datasets. However, there is a concern that these
pretrained models may come with their own biases which would propagate into the
finetuned model. In this work, we investigate bias when conceptualized as both
spurious correlations between the target task and a sensitive attribute as well
as underrepresentation of a particular group in the dataset. Under both notions
of bias, we find that (1) models finetuned on top of pretrained models can
indeed inherit their biases, but (2) this bias can be corrected for through
relatively minor interventions to the finetuning dataset, and often with a
negligible impact to performance. Our findings imply that careful curation of
the finetuning dataset is important for reducing biases on a downstream task,
and doing so can even compensate for bias in the pretrained model.",None,-1
ad8ff5cf-53e6-49ee-a30b-e3e5b2b4cdaa,TEDB System Description to a Shared Task on Euphemism Detection 2022,0.0729538,"In this report, we describe our Transformers for euphemism detection baseline
(TEDB) submissions to a shared task on euphemism detection 2022. We cast the
task of predicting euphemism as text classification. We considered
Transformer-based models which are the current state-of-the-art methods for
text classification. We explored different training schemes, pretrained models,
and model architectures. Our best result of 0.816 F1-score (0.818 precision and
0.814 recall) consists of a euphemism-detection-finetuned
TweetEval/TimeLMs-pretrained RoBERTa model as a feature extractor frontend with
a KimCNN classifier backend trained end-to-end using a cosine annealing
scheduler. We observed pretrained models on sentiment analysis and
offensiveness detection to correlate with more F1-score while pretraining on
other tasks, such as sarcasm detection, produces less F1-scores. Also, putting
more word vector channels does not improve the performance in our experiments.",None,-1
c60a4ec0-a79b-46bf-93e3-cb5b795fb07c,Theoretically Guaranteed Policy Improvement Distilled from Model-Based Planning,0.00139428,"Model-based reinforcement learning (RL) has demonstrated remarkable successes
on a range of continuous control tasks due to its high sample efficiency. To
save the computation cost of conducting planning online, recent practices tend
to distill optimized action sequences into an RL policy during the training
phase. Although the distillation can incorporate both the foresight of planning
and the exploration ability of RL policies, the theoretical understanding of
these methods is yet unclear. In this paper, we extend the policy improvement
step of Soft Actor-Critic (SAC) by developing an approach to distill from
model-based planning to the policy. We then demonstrate that such an approach
of policy improvement has a theoretical guarantee of monotonic improvement and
convergence to the maximum value defined in SAC. We discuss effective design
choices and implement our theory as a practical algorithm -- Model-based
Planning Distilled to Policy (MPDP) -- that updates the policy jointly over
multiple future time steps. Extensive experiments show that MPDP achieves
better sample efficiency and asymptotic performance than both model-free and
model-based planning algorithms on six continuous control benchmark tasks in
MuJoCo.",None,-1
cb782c73-f93b-4d82-89c3-c7a203772729,Distillation of encoder-decoder transformers for sequence labelling,0.311763,"Driven by encouraging results on a wide range of tasks, the field of NLP is
experiencing an accelerated race to develop bigger language models. This race
for bigger models has also underscored the need to continue the pursuit of
practical distillation approaches that can leverage the knowledge acquired by
these big models in a compute-efficient manner. Having this goal in mind, we
build on recent work to propose a hallucination-free framework for sequence
tagging that is especially suited for distillation. We show empirical results
of new state-of-the-art performance across multiple sequence labelling datasets
and validate the usefulness of this framework for distilling a large model in a
few-shot learning scenario.",None,-1
38819965-a3dc-4038-9123-4ed6db237a71,A comparative study of human inverse kinematics techniques for lower limbs,0.106097,"Inverse Kinematics (IK) has been an active research topic and many methods
have been introduced to provide a fast and accurate solution. However, high
computational cost and the generation of unrealistic positions constitute the
weak points in most existing IK methods. In this paper, a comparative study was
established to analyze the performance of popular IK techniques applied to the
human leg. The objective is to determine the most efficient method in terms of
computation time and to reach the desired position with a realistic human
posture while respecting the range of motion and joint comfort zones of every
joint.",None,-1
413e780f-9183-4b1e-b1c7-a0c1ace12f97,DREEAM: Guiding Attention with Evidence for Improving Document-Level Relation Extraction,0.999648,"Document-level relation extraction (DocRE) is the task of identifying all
relations between each entity pair in a document. Evidence, defined as
sentences containing clues for the relationship between an entity pair, has
been shown to help DocRE systems focus on relevant texts, thus improving
relation extraction. However, evidence retrieval (ER) in DocRE faces two major
issues: high memory consumption and limited availability of annotations. This
work aims at addressing these issues to improve the usage of ER in DocRE.
First, we propose DREEAM, a memory-efficient approach that adopts evidence
information as the supervisory signal, thereby guiding the attention modules of
the DocRE system to assign high weights to evidence. Second, we propose a
self-training strategy for DREEAM to learn ER from automatically-generated
evidence on massive data without evidence annotations. Experimental results
reveal that our approach exhibits state-of-the-art performance on the DocRED
benchmark for both DocRE and ER. To the best of our knowledge, DREEAM is the
first approach to employ ER self-training.",None,-1
d2790db7-75a5-49ec-bc2f-432930b1081e,FinnWoodlands Dataset,0.552329,"While the availability of large and diverse datasets has contributed to
significant breakthroughs in autonomous driving and indoor applications,
forestry applications are still lagging behind and new forest datasets would
most certainly contribute to achieving significant progress in the development
of data-driven methods for forest-like scenarios. This paper introduces a
forest dataset called \textit{FinnWoodlands}, which consists of RGB stereo
images, point clouds, and sparse depth maps, as well as ground truth manual
annotations for semantic, instance, and panoptic segmentation.
\textit{FinnWoodlands} comprises a total of 4226 objects manually annotated,
out of which 2562 objects (60.6\%) correspond to tree trunks classified into
three different instance categories, namely ""Spruce Tree"", ""Birch Tree"", and
""Pine Tree"". Besides tree trunks, we also annotated ""Obstacles"" objects as
instances as well as the semantic stuff classes ""Lake"", ""Ground"", and ""Track"".
Our dataset can be used in forestry applications where a holistic
representation of the environment is relevant. We provide an initial benchmark
using three models for instance segmentation, panoptic segmentation, and depth
completion, and illustrate the challenges that such unstructured scenarios
introduce.",None,-1
c8ffd456-132a-4909-9938-808b198bc6aa,Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection,0.971434,"Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning,
yet constructing them through human annotations can be costly. As a result,
various automatic methods have been proposed to construct CSKG with larger
semantic coverage. However, these unsupervised approaches introduce spurious
noise that can lower the quality of the resulting CSKG, which cannot be tackled
easily by existing denoising algorithms due to the unique characteristics of
nodes and structures in CSKGs. To address this issue, we propose Gold (Global
and Local-aware Denoising), a denoising framework for CSKGs that incorporates
entity semantic information, global rules, and local structural information
from the CSKG. Experiment results demonstrate that Gold outperforms all
baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks.
Furthermore, we show that denoising a real-world CSKG is effective and even
benefits the downstream zero-shot commonsense question-answering task.",None,-1
edc79c98-b0bf-48e4-b432-224ecbd631ef,Agent-based Collaborative Random Search for Hyper-parameter Tuning and Global Function Optimization,0.580747,"Hyper-parameter optimization is one of the most tedious yet crucial steps in
training machine learning models. There are numerous methods for this vital
model-building stage, ranging from domain-specific manual tuning guidelines
suggested by the oracles to the utilization of general-purpose black-box
optimization techniques. This paper proposes an agent-based collaborative
technique for finding near-optimal values for any arbitrary set of
hyper-parameters (or decision variables) in a machine learning model (or
general function optimization problem). The developed method forms a
hierarchical agent-based architecture for the distribution of the searching
operations at different dimensions and employs a cooperative searching
procedure based on an adaptive width-based random sampling technique to locate
the optima. The behavior of the presented model, specifically against the
changes in its design parameters, is investigated in both machine learning and
global function optimization applications, and its performance is compared with
that of two randomized tuning strategies that are commonly used in practice.
According to the empirical results, the proposed model outperformed the
compared methods in the experimented classification, regression, and
multi-dimensional function optimization tasks, notably in a higher number of
dimensions and in the presence of limited on-device computational resources.",None,-1
257d116b-56a6-435a-b98c-553df4e1ea3a,Reverse Knowledge Distillation: Training a Large Model using a Small One for Retinal Image Matching on Limited Data,0.457935,"Retinal image matching plays a crucial role in monitoring disease progression
and treatment response. However, datasets with matched keypoints between
temporally separated pairs of images are not available in abundance to train
transformer-based model. We propose a novel approach based on reverse knowledge
distillation to train large models with limited data while preventing
overfitting. Firstly, we propose architectural modifications to a CNN-based
semi-supervised method called SuperRetina that help us improve its results on a
publicly available dataset. Then, we train a computationally heavier model
based on a vision transformer encoder using the lighter CNN-based model, which
is counter-intuitive in the field knowledge-distillation research where
training lighter models based on heavier ones is the norm. Surprisingly, such
reverse knowledge distillation improves generalization even further. Our
experiments suggest that high-dimensional fitting in representation space may
prevent overfitting unlike training directly to match the final output. We also
provide a public dataset with annotations for retinal image keypoint detection
and matching to help the research community develop algorithms for retinal
image applications.",None,-1
80592ada-4b7a-41db-b2f4-9ddb0ff02874,2nd Swiss German Speech to Standard German Text Shared Task at SwissText 2022,0.198534,"We present the results and findings of the 2nd Swiss German speech to
Standard German text shared task at SwissText 2022. Participants were asked to
build a sentence-level Swiss German speech to Standard German text system
specialized on the Grisons dialect. The objective was to maximize the BLEU
score on a test set of Grisons speech. 3 teams participated, with the
best-performing system achieving a BLEU score of 70.1.",None,-1
2d35debc-2b6c-4de0-b2ab-69c0c0ab718c,DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices,0.75417,"Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap
between a labeled source dataset and an unlabelled target dataset for
regression problems. Recent works mostly focus on learning a deep feature
encoder by minimizing the discrepancy between source and target features. In
this work, we present a different perspective for the DAR problem by analyzing
the closed-form ordinary least square~(OLS) solution to the linear regressor in
the deep domain adaptation context. Rather than aligning the original feature
embedding space, we propose to align the inverse Gram matrix of the features,
which is motivated by its presence in the OLS solution and the Gram matrix's
ability to capture the feature correlations. Specifically, we propose a simple
yet effective DAR method which leverages the pseudo-inverse low-rank property
to align the scale and angle in a selected subspace generated by the
pseudo-inverse Gram matrix of the two domains. We evaluate our method on three
domain adaptation regression benchmarks. Experimental results demonstrate that
our method achieves state-of-the-art performance. Our code is available at
https://github.com/ismailnejjar/DARE-GRAM.",None,-1
da13e15a-a23c-4cbb-b0f3-5d750d4cf939,Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint,0.627672,"Clinical trials are indispensable in developing new treatments, but they face
obstacles in patient recruitment and retention, hindering the enrollment of
necessary participants. To tackle these challenges, deep learning frameworks
have been created to match patients to trials. These frameworks calculate the
similarity between patients and clinical trial eligibility criteria,
considering the discrepancy between inclusion and exclusion criteria. Recent
studies have shown that these frameworks outperform earlier approaches.
However, deep learning models may raise fairness issues in patient-trial
matching when certain sensitive groups of individuals are underrepresented in
clinical trials, leading to incomplete or inaccurate data and potential harm.
To tackle the issue of fairness, this work proposes a fair patient-trial
matching framework by generating a patient-criterion level fairness constraint.
The proposed framework considers the inconsistency between the embedding of
inclusion and exclusion criteria among patients of different sensitive groups.
The experimental results on real-world patient-trial and patient-criterion
matching tasks demonstrate that the proposed framework can successfully
alleviate the predictions that tend to be biased.",None,-1
50070c78-5b7b-4897-b0b1-30641a5debbc,Process Mining for Unstructured Data: Challenges and Research Directions,0.447108,"The application of process mining for unstructured data might significantly
elevate novel insights into disciplines where unstructured data is a common
data format. To efficiently analyze unstructured data by process mining and to
convey confidence into the analysis result, requires bridging multiple
challenges. The purpose of this paper is to discuss these challenges, present
initial solutions and describe future research directions. We hope that this
article lays the foundations for future collaboration on this topic.",None,-1
afb55724-d334-4cee-8c70-946b9f6981dd,PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents,0.214524,"Strategies such as chain-of-thought prompting improve the performance of
large language models (LLMs) on complex reasoning tasks by decomposing input
examples into intermediate steps. However, it remains unclear how to apply such
methods to reason over long input documents, in which both the decomposition
and the output of each intermediate step are non-trivial to obtain. In this
work, we propose PEARL, a prompting framework to improve reasoning over long
documents, which consists of three stages: action mining, plan formulation, and
plan execution. More specifically, given a question about a long document,
PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE,
FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain
the answer. Each stage of PEARL is implemented via zero-shot or few-shot
prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate
PEARL on a challenging subset of the QuALITY dataset, which contains questions
that require complex reasoning over long narrative texts. PEARL outperforms
zero-shot and chain-of-thought prompting on this dataset, and ablation
experiments show that each stage of PEARL is critical to its performance.
Overall, PEARL is a first step towards leveraging LLMs to reason over long
documents.",None,-1
06e28bd5-a0d0-4495-9bf3-dbc8a37009d1,DDS3D: Dense Pseudo-Labels with Dynamic Threshold for Semi-Supervised 3D Object Detection,0.815199,"In this paper, we present a simple yet effective semi-supervised 3D object
detector named DDS3D. Our main contributions have two-fold. On the one hand,
different from previous works using Non-Maximal Suppression (NMS) or its
variants for obtaining the sparse pseudo labels, we propose a dense
pseudo-label generation strategy to get dense pseudo-labels, which can retain
more potential supervision information for the student network. On the other
hand, instead of traditional fixed thresholds, we propose a dynamic threshold
manner to generate pseudo-labels, which can guarantee the quality and quantity
of pseudo-labels during the whole training process. Benefiting from these two
components, our DDS3D outperforms the state-of-the-art semi-supervised 3d
object detection with mAP of 3.1% on the pedestrian and 2.1% on the cyclist
under the same configuration of 1% samples. Extensive ablation studies on the
KITTI dataset demonstrate the effectiveness of our DDS3D. The code and models
will be made publicly available at https://github.com/hust-jy/DDS3D",None,-1
a45aa7cf-82f5-496b-9f8e-5156b01d1866,ECQED: Emotion-Cause Quadruple Extraction in Dialogs,0.573469,"The existing emotion-cause pair extraction (ECPE) task, unfortunately,
ignores extracting the emotion type and cause type, while these fine-grained
meta-information can be practically useful in real-world applications, i.e.,
chat robots and empathic dialog generation. Also the current ECPE is limited to
the scenario of single text piece, while neglecting the studies at dialog level
that should have more realistic values. In this paper, we extend the ECPE task
with a broader definition and scenario, presenting a new task, Emotion-Cause
Quadruple Extraction in Dialogs (ECQED), which requires detecting emotion-cause
utterance pairs and emotion and cause types. We present an ECQED model based on
a structural and semantic heterogeneous graph as well as a parallel grid
tagging scheme, which advances in effectively incorporating the dialog context
structure, meanwhile solving the challenging overlapped quadruple issue. Via
experiments we show that introducing the fine-grained emotion and cause
features evidently helps better dialog generation. Also our proposed ECQED
system shows exceptional superiority over baselines on both the emotion-cause
quadruple or pair extraction tasks, meanwhile being highly efficient.",None,-1
ddcf358e-abaf-4def-8095-0866631eccfe,Conformal Prediction for Time Series with Modern Hopfield Networks,0.751651,"To quantify uncertainty, conformal prediction methods are gaining
continuously more interest and have already been successfully applied to
various domains. However, they are difficult to apply to time series as the
autocorrelative structure of time series violates basic assumptions required by
conformal prediction. We propose HopCPT, a novel conformal prediction approach
for time series that not only copes with temporal structures but leverages
them. We show that our approach is theoretically well justified for time series
where temporal dependencies are present. In experiments, we demonstrate that
our new approach outperforms state-of-the-art conformal prediction methods on
multiple real-world time series datasets from four different domains.",None,-1
60865049-61c6-4af9-9ec2-6ec934d09221,LLMSTEP: LLM proofstep suggestions in Lean,0.996014,"We present LLMSTEP, a tool for integrating a language model into the Lean
proof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to
a server hosting a language model. The language model generates suggestions,
which are checked in Lean and displayed to a user in their development
environment. We provide a baseline language model, along with code for
fine-tuning and evaluation to support further development. We provide server
implementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a
step towards fast, effective language model suggestions for any user.",None,-1
555f4b64-52fb-4565-99bd-e4f4926c6484,The Change You Want to See (Now in 3D),0.322405,"The goal of this paper is to detect what has changed, if anything, between
two ""in the wild"" images of the same 3D scene acquired from different camera
positions and at different temporal instances. The open-set nature of this
problem, occlusions/dis-occlusions due to the shift in viewpoint, and the lack
of suitable training datasets, presents substantial challenges in devising a
solution.
  To address this problem, we contribute a change detection model that is
trained entirely on synthetic data and is class-agnostic, yet it is performant
out-of-the-box on real world images without requiring fine-tuning. Our solution
entails a ""register and difference"" approach that leverages self-supervised
frozen embeddings and feature differences, which allows the model to generalise
to a wide variety of scenes and domains. The model is able to operate directly
on two RGB images, without requiring access to ground truth camera intrinsics,
extrinsics, depth maps, point clouds, or additional before-after images.
Finally, we collect and release a new evaluation dataset consisting of
real-world image pairs with human-annotated differences and demonstrate the
efficacy of our method. The code, datasets and pre-trained model can be found
at: https://github.com/ragavsachdeva/CYWS-3D",None,-1
646792d1-c238-41e6-bc98-b59a12c5811c,Frustrated with Code Quality Issues? LLMs can Help!,0.167693,"As software projects progress, quality of code assumes paramount importance
as it affects reliability, maintainability and security of software. For this
reason, static analysis tools are used in developer workflows to flag code
quality issues. However, developers need to spend extra efforts to revise their
code to improve code quality based on the tool findings. In this work, we
investigate the use of (instruction-following) large language models (LLMs) to
assist developers in revising code to resolve code quality issues. We present a
tool, CORE (short for COde REvisions), architected using a pair of LLMs
organized as a duo comprised of a proposer and a ranker. Providers of static
analysis tools recommend ways to mitigate the tool warnings and developers
follow them to revise their code. The \emph{proposer LLM} of CORE takes the
same set of recommendations and applies them to generate candidate code
revisions. The candidates which pass the static quality checks are retained.
However, the LLM may introduce subtle, unintended functionality changes which
may go un-detected by the static analysis. The \emph{ranker LLM} evaluates the
changes made by the proposer using a rubric that closely follows the acceptance
criteria that a developer would enforce. CORE uses the scores assigned by the
ranker LLM to rank the candidate revisions before presenting them to the
developer. CORE could revise 59.2% Python files (across 52 quality checks) so
that they pass scrutiny by both a tool and a human reviewer. The ranker LLM is
able to reduce false positives by 25.8% in these cases. CORE produced revisions
that passed the static analysis tool in 76.8% Java files (across 10 quality
checks) comparable to 78.3% of a specialized program repair tool, with
significantly much less engineering efforts.",None,-1
b8269ac7-596a-4449-8ca1-58fcf3b9a2e1,Unpaired Translation from Semantic Label Maps to Images by Leveraging Domain-Specific Simulations,0.0903188,"Photorealistic image generation from simulated label maps are necessitated in
several contexts, such as for medical training in virtual reality. With
conventional deep learning methods, this task requires images that are paired
with semantic annotations, which typically are unavailable. We introduce a
contrastive learning framework for generating photorealistic images from
simulated label maps, by learning from unpaired sets of both. Due to
potentially large scene differences between real images and label maps,
existing unpaired image translation methods lead to artifacts of scene
modification in synthesized images. We utilize simulated images as surrogate
targets for a contrastive loss, while ensuring consistency by utilizing
features from a reverse translation network. Our method enables bidirectional
label-image translations, which is demonstrated in a variety of scenarios and
datasets, including laparoscopy, ultrasound, and driving scenes. By comparing
with state-of-the-art unpaired translation methods, our proposed method is
shown to generate realistic and scene-accurate translations.",None,-1
6a64b6f7-6f27-4c5f-ab62-070dd6fee3e8,Solar Irradiance Anticipative Transformer,0.522922,"This paper proposes an anticipative transformer-based model for short-term
solar irradiance forecasting. Given a sequence of sky images, our proposed
vision transformer encodes features of consecutive images, feeding into a
transformer decoder to predict irradiance values associated with future unseen
sky images. We show that our model effectively learns to attend only to
relevant features in images in order to forecast irradiance. Moreover, the
proposed anticipative transformer captures long-range dependencies between sky
images to achieve a forecasting skill of 21.45 % on a 15 minute ahead
prediction for a newly introduced dataset of all-sky images when compared to a
smart persistence model.",None,-1
f3a5c47f-34ad-4413-b79b-a4e0efedc439,Multi-Temporal Lip-Audio Memory for Visual Speech Recognition,0.536859,"Visual Speech Recognition (VSR) is a task to predict a sentence or word from
lip movements. Some works have been recently presented which use audio signals
to supplement visual information. However, existing methods utilize only
limited information such as phoneme-level features and soft labels of Automatic
Speech Recognition (ASR) networks. In this paper, we present a Multi-Temporal
Lip-Audio Memory (MTLAM) that makes the best use of audio signals to complement
insufficient information of lip movements. The proposed method is mainly
composed of two parts: 1) MTLAM saves multi-temporal audio features produced
from short- and long-term audio signals, and the MTLAM memorizes a
visual-to-audio mapping to load stored multi-temporal audio features from
visual features at the inference phase. 2) We design an audio temporal model to
produce multi-temporal audio features capturing the context of neighboring
words. In addition, to construct effective visual-to-audio mapping, the audio
temporal models can generate audio features time-aligned with visual features.
Through extensive experiments, we validate the effectiveness of the MTLAM
achieving state-of-the-art performances on two public VSR datasets.",None,-1
28ab206a-67ec-47e6-a126-e7033b37818d,How Efficient Are Today's Continual Learning Algorithms?,0.566887,"Supervised Continual learning involves updating a deep neural network (DNN)
from an ever-growing stream of labeled data. While most work has focused on
overcoming catastrophic forgetting, one of the major motivations behind
continual learning is being able to efficiently update a network with new
information, rather than retraining from scratch on the training dataset as it
grows over time. Despite recent continual learning methods largely solving the
catastrophic forgetting problem, there has been little attention paid to the
efficiency of these algorithms. Here, we study recent methods for incremental
class learning and illustrate that many are highly inefficient in terms of
compute, memory, and storage. Some methods even require more compute than
training from scratch! We argue that for continual learning to have real-world
applicability, the research community cannot ignore the resources used by these
algorithms. There is more to continual learning than mitigating catastrophic
forgetting.",None,-1
32e56e46-0169-4edc-b8b1-1b2753826221,Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal of Raindrops and Rain Streaks,0.76322,"In the real world, image degradations caused by rain often exhibit a
combination of rain streaks and raindrops, thereby increasing the challenges of
recovering the underlying clean image. Note that the rain streaks and raindrops
have diverse shapes, sizes, and locations in the captured image, and thus
modeling the correlation relationship between irregular degradations caused by
rain artifacts is a necessary prerequisite for image deraining. This paper aims
to present an efficient and flexible mechanism to learn and model degradation
relationships in a global view, thereby achieving a unified removal of
intricate rain scenes. To do so, we propose a Sparse Sampling Transformer based
on Uncertainty-Driven Ranking, dubbed UDR-S2Former. Compared to previous
methods, our UDR-S2Former has three merits. First, it can adaptively sample
relevant image degradation information to model underlying degradation
relationships. Second, explicit application of the uncertainty-driven ranking
strategy can facilitate the network to attend to degradation features and
understand the reconstruction process. Finally, experimental results show that
our UDR-S2Former clearly outperforms state-of-the-art methods for all
benchmarks.",None,-1
51978498-870b-446d-ae58-53bb7723da7b,Planar Object Tracking via Weighted Optical Flow,0.256042,"We propose WOFT -- a novel method for planar object tracking that estimates a
full 8 degrees-of-freedom pose, i.e. the homography w.r.t. a reference view.
The method uses a novel module that leverages dense optical flow and assigns a
weight to each optical flow correspondence, estimating a homography by weighted
least squares in a fully differentiable manner. The trained module assigns zero
weights to incorrect correspondences (outliers) in most cases, making the
method robust and eliminating the need of the typically used non-differentiable
robust estimators like RANSAC. The proposed weighted optical flow tracker
(WOFT) achieves state-of-the-art performance on two benchmarks, POT-210 and
POIC, tracking consistently well across a wide range of scenarios.",None,-1
75e051f8-5bb0-47f0-a890-bf4cffe701ce,An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback,0.479051,"While search is the predominant method of accessing information, formulating
effective queries remains a challenging task, especially for situations where
the users are not familiar with a domain, or searching for documents in other
languages, or looking for complex information such as events, which are not
easily expressible as queries. Providing example documents or passages of
interest, might be easier for a user, however, such query-by-example scenarios
are prone to concept drift, and are highly sensitive to the query generation
method. This demo illustrates complementary approaches of using LLMs
interactively, assisting and enabling the user to provide edits and feedback at
all stages of the query formulation process. The proposed Query Generation
Assistant is a novel search interface which supports automatic and interactive
query generation over a mono-linguial or multi-lingual document collection.
Specifically, the proposed assistive interface enables the users to refine the
queries generated by different LLMs, to provide feedback on the retrieved
documents or passages, and is able to incorporate the users' feedback as
prompts to generate more effective queries. The proposed interface is a
valuable experimental tool for exploring fine-tuning and prompting of LLMs for
query generation to qualitatively evaluate the effectiveness of retrieval and
ranking models, and for conducting Human-in-the-Loop (HITL) experiments for
complex search tasks where users struggle to formulate queries without such
assistance.",None,-1
d3643c8c-4f26-4193-bfe4-48816ae811bb,Elucidating STEM Concepts through Generative AI: A Multi-modal Exploration of Analogical Reasoning,0.236289,"This study explores the integration of generative artificial intelligence
(AI), specifically large language models, with multi-modal analogical reasoning
as an innovative approach to enhance science, technology, engineering, and
mathematics (STEM) education. We have developed a novel system that utilizes
the capacities of generative AI to transform intricate principles in
mathematics, physics, and programming into comprehensible metaphors. To further
augment the educational experience, these metaphors are subsequently converted
into visual form. Our study aims to enhance the learners' understanding of STEM
concepts and their learning engagement by using the visual metaphors. We
examine the efficacy of our system via a randomized A/B/C test, assessing
learning gains and motivation shifts among the learners. Our study demonstrates
the potential of applying large language models to educational practice on STEM
subjects. The results will shed light on the design of educational system in
terms of harnessing AI's potential to empower educational stakeholders.",None,-1
41588a50-ea3a-48bd-9f0c-eca8d2a759dc,Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods,0.998907,"Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Due to rapid
technological advances and their extreme versatility, LLMs nowadays have
millions of users and are at the cusp of being the main go-to technology for
information retrieval, content generation, problem-solving, etc. Therefore, it
is of great importance to thoroughly assess and scrutinize their capabilities.
Due to increasingly complex and novel behavioral patterns in current LLMs, this
can be done by treating them as participants in psychology experiments that
were originally designed to test humans. For this purpose, the paper introduces
a new field of research called ""machine psychology"". The paper outlines how
different subfields of psychology can inform behavioral tests for LLMs. It
defines methodological standards for machine psychology research, especially by
focusing on policies for prompt designs. Additionally, it describes how
behavioral patterns discovered in LLMs are to be interpreted. In sum, machine
psychology aims to discover emergent abilities in LLMs that cannot be detected
by most traditional natural language processing benchmarks.",None,-1
bcbf803a-7e88-4e3d-b2ae-de2d29713abd,Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale,0.0697933,"In this paper, we address the following problem: Given an offline
demonstration dataset from an imperfect expert, what is the best way to
leverage it to bootstrap online learning performance in MDPs. We first propose
an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline
dataset, and information about the expert's behavioral policy used to generate
the offline dataset. Its cumulative Bayesian regret goes down to zero
exponentially fast in N, the offline dataset size if the expert is competent
enough. Since this algorithm is computationally impractical, we then propose
the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm
for online RL, and imitation learning. Our empirical results show that the
proposed iRLSVI algorithm is able to achieve significant reduction in regret as
compared to two baselines: no offline data, and offline dataset but used
without information about the generative policy. Our algorithm bridges online
RL and imitation learning for the first time.",None,-1
6e40e3e7-3e00-415c-8c13-42ece462c361,Object Goal Navigation with Recursive Implicit Maps,0.790261,"Object goal navigation aims to navigate an agent to locations of a given
object category in unseen environments. Classical methods explicitly build maps
of environments and require extensive engineering while lacking semantic
information for object-oriented exploration. On the other hand, end-to-end
learning methods alleviate manual map design and predict actions using implicit
representations. Such methods, however, lack an explicit notion of geometry and
may have limited ability to encode navigation history. In this work, we propose
an implicit spatial map for object goal navigation. Our implicit map is
recursively updated with new observations at each step using a transformer. To
encourage spatial reasoning, we introduce auxiliary tasks and train our model
to reconstruct explicit maps as well as to predict visual features, semantic
labels and actions. Our method significantly outperforms the state of the art
on the challenging MP3D dataset and generalizes well to the HM3D dataset. We
successfully deploy our model on a real robot and achieve encouraging object
goal navigation results in real scenes using only a few real-world
demonstrations. Code, trained models and videos are available at
\url{https://www.di.ens.fr/willow/research/onav_rim/}.",None,-1
83ffd7f0-c5e2-4514-98dc-40e81d9a1aef,A Formal Perspective on Byte-Pair Encoding,0.739265,"Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in
NLP, despite being devised initially as a compression method. BPE appears to be
a greedy algorithm at face value, but the underlying optimization problem that
BPE seeks to solve has not yet been laid down. We formalize BPE as a
combinatorial optimization problem. Via submodular functions, we prove that the
iterative greedy version is a
$\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-approximation
of an optimal merge sequence, where ${\sigma(\boldsymbol{\mu}^\star)}$ is the
total backward curvature with respect to the optimal merge sequence
$\boldsymbol{\mu}^\star$. Empirically the lower bound of the approximation is
$\approx 0.37$.
  We provide a faster implementation of BPE which improves the runtime
complexity from $\mathcal{O}\left(N M\right)$ to $\mathcal{O}\left(N \log
M\right)$, where $N$ is the sequence length and $M$ is the merge count.
Finally, we optimize the brute-force algorithm for optimal BPE using
memoization.",None,-1
c7219d0c-7e45-4ddf-bf35-1001190a503c,Ben-ge: Extending BigEarthNet with Geographical and Environmental Data,0.355462,"Deep learning methods have proven to be a powerful tool in the analysis of
large amounts of complex Earth observation data. However, while Earth
observation data are multi-modal in most cases, only single or few modalities
are typically considered. In this work, we present the ben-ge dataset, which
supplements the BigEarthNet-MM dataset by compiling freely and globally
available geographical and environmental data. Based on this dataset, we
showcase the value of combining different data modalities for the downstream
tasks of patch-based land-use/land-cover classification and land-use/land-cover
segmentation. ben-ge is freely available and expected to serve as a test bed
for fully supervised and self-supervised Earth observation applications.",None,-1
2659ceb9-2513-4328-8407-f0856dc3af95,"QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models",0.767023,"Misinformation spreading in mainstream and social media has been misleading
users in different ways. Manual detection and verification efforts by
journalists and fact-checkers can no longer cope with the great scale and quick
spread of misleading information. This motivated research and industry efforts
to develop systems for analyzing and verifying news spreading online. The
SemEval-2023 Task 3 is an attempt to address several subtasks under this
overarching problem, targeting writing techniques used in news articles to
affect readers' opinions. The task addressed three subtasks with six languages,
in addition to three ``surprise'' test languages, resulting in 27 different
test setups. This paper describes our participating system to this task. Our
team is one of the 6 teams that successfully submitted runs for all setups. The
official results show that our system is ranked among the top 3 systems for 10
out of the 27 setups.",None,-1
7b37e3ed-3c7e-4afb-8bb1-f748b93839dc,Personalized Abstractive Summarization by Tri-agent Generation Pipeline,0.272962,"Tailoring outputs from large language models, like ChatGPT, to implicit user
preferences remains a challenge despite their impressive generative
capabilities. In this paper, we propose a tri-agent generation pipeline
comprising a generator, an instructor, and an editor to enhance output
personalization. The generator produces an initial output, the instructor
automatically generates editing instructions based on user preferences, and the
editor refines the output to align with those preferences. The inference-only
large language model (ChatGPT) serves as both the generator and editor, with a
smaller model acting as the instructor to guide output generation. We train the
instructor using editor-steered reinforcement learning, leveraging feedback
from a large-scale editor model to optimize instruction generation.
Experimental results on two abstractive summarization datasets demonstrate the
effectiveness of our approach in generating outputs that better meet user
expectations. Code is available at
\url{https://github.com/Wendy-Xiao/chatgpt_editing_summ}",None,-1
300d717d-2f32-448b-a811-491886e706f8,PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback,0.714947,"Large Language Models for Code (Code LLM) are flourishing. New and powerful
models are released on a weekly basis, demonstrating remarkable performance on
the code generation task. Various approaches have been proposed to boost the
code generation performance of pre-trained Code LLMs, such as supervised
fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we
propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework,
which can effectively and efficiently boost pre-trained large language models
for code generation. Under this framework, we present PanGu-Coder2, which
achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through
an extensive evaluation on CoderEval and LeetCode benchmarks, we show that
PanGu-Coder2 consistently outperforms all previous Code LLMs.",None,-1
c3abb3c2-0a92-4220-8d74-3dc8f72277d1,Testing Causal Models of Word Meaning in GPT-3 and -4,0.0754686,"Large Language Models (LLMs) have driven extraordinary improvements in NLP.
However, it is unclear how such models represent lexical concepts-i.e., the
meanings of the words they use. This paper evaluates the lexical
representations of GPT-3 and GPT-4 through the lens of HIPE theory, a theory of
concept representations which focuses on representations of words describing
artifacts (such as ""mop"", ""pencil"", and ""whistle""). The theory posits a causal
graph that relates the meanings of such words to the form, use, and history of
the objects to which they refer. We test LLMs using the same stimuli originally
used by Chaigneau et al. (2004) to evaluate the theory in humans, and consider
a variety of prompt designs. Our experiments concern judgements about causal
outcomes, object function, and object naming. We find no evidence that GPT-3
encodes the causal structure hypothesized by HIPE, but do find evidence that
GPT-4 encodes such structure. The results contribute to a growing body of
research characterizing the representational capacity of large language models.",None,-1
3afc9b75-17f2-4e16-9f83-56f8810330fd,"CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography",0.868589,"Current image steganography techniques are mainly focused on cover-based
methods, which commonly have the risk of leaking secret images and poor
robustness against degraded container images. Inspired by recent developments
in diffusion models, we discovered that two properties of diffusion models, the
ability to achieve translation between two images without training, and
robustness to noisy data, can be used to improve security and natural
robustness in image steganography tasks. For the choice of diffusion model, we
selected Stable Diffusion, a type of conditional diffusion model, and fully
utilized the latest tools from open-source communities, such as LoRAs and
ControlNets, to improve the controllability and diversity of container images.
In summary, we propose a novel image steganography framework, named
Controllable, Robust and Secure Image Steganography (CRoSS), which has
significant advantages in controllability, robustness, and security compared to
cover-based image steganography methods. These benefits are obtained without
additional training. To our knowledge, this is the first work to introduce
diffusion models to the field of image steganography. In the experimental
section, we conducted detailed experiments to demonstrate the advantages of our
proposed CRoSS framework in controllability, robustness, and security.",None,-1
3c2d17f5-fc30-45a6-b8f3-d0ce725312b8,`It is currently hodgepodge'': Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values,0.801903,"Recently, the AI/ML research community has indicated an urgent need to
establish Responsible AI (RAI) values and practices as part of the AI/ML
lifecycle. Several organizations and communities are responding to this call by
sharing RAI guidelines. However, there are gaps in awareness, deliberation, and
execution of such practices for multi-disciplinary ML practitioners. This work
contributes to the discussion by unpacking co-production challenges faced by
practitioners as they align their RAI values. We interviewed 23 individuals,
across 10 organizations, tasked to ship AI/ML based products while upholding
RAI norms and found that both top-down and bottom-up institutional structures
create burden for different roles preventing them from upholding RAI values, a
challenge that is further exacerbated when executing conflicted values. We
share multiple value levers used as strategies by the practitioners to resolve
their challenges. We end our paper with recommendations for inclusive and
equitable RAI value-practices, creating supportive organizational structures
and opportunities to further aid practitioners.",None,-1
df8b6e4b-84d1-4793-a331-925579450ced,ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer,0.33271,"Textual style transfer is the task of transforming stylistic properties of
text while preserving meaning. Target ""styles"" can be defined in numerous ways,
ranging from single attributes (e.g, formality) to authorship (e.g,
Shakespeare). Previous unsupervised style-transfer approaches generally rely on
significant amounts of labeled data for only a fixed set of styles or require
large language models. In contrast, we introduce a novel diffusion-based
framework for general-purpose style transfer that can be flexibly adapted to
arbitrary target styles at inference time. Our parameter-efficient approach,
ParaGuide, leverages paraphrase-conditioned diffusion models alongside
gradient-based guidance from both off-the-shelf classifiers and strong existing
style embedders to transform the style of text while preserving semantic
information. We validate the method on the Enron Email Corpus, with both human
and automatic evaluations, and find that it outperforms strong baselines on
formality, sentiment, and even authorship style transfer.",None,-1
bc8f1e38-8bd1-4530-94f3-913e16059596,Hierarchically Gated Recurrent Neural Network for Sequence Modeling,0.821246,"Transformers have surpassed RNNs in popularity due to their superior
abilities in parallel training and long-term dependency modeling. Recently,
there has been a renewed interest in using linear RNNs for efficient sequence
modeling. These linear RNNs often employ gating mechanisms in the output of the
linear recurrence layer while ignoring the significance of using forget gates
within the recurrence. In this paper, we propose a gated linear RNN model
dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes
forget gates that are lower bounded by a learnable value. The lower bound
increases monotonically when moving up layers. This allows the upper layers to
model long-term dependencies and the lower layers to model more local,
short-term dependencies. Experiments on language modeling, image
classification, and long-range arena benchmarks showcase the efficiency and
effectiveness of our proposed model. The source code is available at
https://github.com/OpenNLPLab/HGRN.",None,-1
148ad6fb-e132-4126-a161-860166f3b5a0,Affect as a proxy for literary mood,0.0952894,"We propose to use affect as a proxy for mood in literary texts. In this
study, we explore the differences in computationally detecting tone versus
detecting mood. Methodologically we utilize affective word embeddings to look
at the affective distribution in different text segments. We also present a
simple yet efficient and effective method of enhancing emotion lexicons to take
both semantic shift and the domain of the text into account producing
real-world congruent results closely matching both contemporary and modern
qualitative analyses.",None,-1
d82773f4-021a-4af3-a574-50386d5b83a6,Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering,0.168063,"Explainable question answering (XQA) aims to answer a given question and
provide an explanation why the answer is selected. Existing XQA methods focus
on reasoning on a single knowledge source, e.g., structured knowledge bases,
unstructured corpora, etc. However, integrating information from heterogeneous
knowledge sources is essential to answer complex questions. In this paper, we
propose to leverage question decomposing for heterogeneous knowledge
integration, by breaking down a complex question into simpler ones, and
selecting the appropriate knowledge source for each sub-question. To facilitate
reasoning, we propose a novel two-stage XQA framework, Reasoning over
Hierarchical Question Decomposition Tree (RoHT). First, we build the
Hierarchical Question Decomposition Tree (HQDT) to understand the semantics of
a complex question; then, we conduct probabilistic reasoning over HQDT from
root to leaves recursively, to aggregate heterogeneous knowledge at different
tree levels and search for a best solution considering the decomposing and
answering probabilities. The experiments on complex QA datasets KQA Pro and
Musique show that our framework outperforms SOTA methods significantly,
demonstrating the effectiveness of leveraging question decomposing for
knowledge integration and our RoHT framework.",None,-1
6cf1688c-2ff0-4f99-88ff-f0ae5642ca66,On the Connection between Pre-training Data Diversity and Fine-tuning Robustness,0.460475,"Pre-training has been widely adopted in deep learning to improve model
performance, especially when the training data for a target task is limited. In
our work, we seek to understand the implications of this training strategy on
the generalization properties of downstream models. More specifically, we ask
the following question: how do properties of the pre-training distribution
affect the robustness of a fine-tuned model? The properties we explore include
the label space, label semantics, image diversity, data domains, and data
quantity of the pre-training distribution. We find that the primary factor
influencing downstream effective robustness (Taori et al., 2020) is data
quantity, while other factors have limited significance. For example, reducing
the number of ImageNet pre-training classes by 4x while increasing the number
of images per class by 4x (that is, keeping total data quantity fixed) does not
impact the robustness of fine-tuned models. We demonstrate our findings on
pre-training distributions drawn from various natural and synthetic data
sources, primarily using the iWildCam-WILDS distribution shift as a test for
downstream robustness.",None,-1
d90663f8-ef08-45fd-9588-ca2cf7d5861c,CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population,0.941907,"Populating Commonsense Knowledge Bases (CSKB) is an important yet hard task
in NLP, as it tackles knowledge from external sources with unseen events and
entities. Fang et al. (2021a) proposed a CSKB Population benchmark with an
evaluation set CKBP v1. However, CKBP v1 adopts crowdsourced annotations that
suffer from a substantial fraction of incorrect answers, and the evaluation set
is not well-aligned with the external knowledge source as a result of random
sampling. In this paper, we introduce CKBP v2, a new high-quality CSKB
Population benchmark, which addresses the two mentioned problems by using
experts instead of crowd-sourced annotation and by adding diversified
adversarial samples to make the evaluation set more representative. We conduct
extensive experiments comparing state-of-the-art methods for CSKB Population on
the new evaluation set for future research comparisons. Empirical results show
that the population task is still challenging, even for large language models
(LLM) such as ChatGPT. Codes and data are available at
https://github.com/HKUST-KnowComp/CSKB-Population.",None,-1
7f425f3a-ff67-483d-a38c-656a47a7c181,Discovering Causality for Efficient Cooperation in Multi-Agent Environments,0.166109,"In cooperative Multi-Agent Reinforcement Learning (MARL) agents are required
to learn behaviours as a team to achieve a common goal. However, while learning
a task, some agents may end up learning sub-optimal policies, not contributing
to the objective of the team. Such agents are called lazy agents due to their
non-cooperative behaviours that may arise from failing to understand whether
they caused the rewards. As a consequence, we observe that the emergence of
cooperative behaviours is not necessarily a byproduct of being able to solve a
task as a team. In this paper, we investigate the applications of causality in
MARL and how it can be applied in MARL to penalise these lazy agents. We
observe that causality estimations can be used to improve the credit assignment
to the agents and show how it can be leveraged to improve independent learning
in MARL. Furthermore, we investigate how Amortized Causal Discovery can be used
to automate causality detection within MARL environments. The results
demonstrate that causality relations between individual observations and the
team reward can be used to detect and punish lazy agents, making them develop
more intelligent behaviours. This results in improvements not only in the
overall performances of the team but also in their individual capabilities. In
addition, results show that Amortized Causal Discovery can be used efficiently
to find causal relations in MARL.",None,-1
55ae226e-6607-4f42-a4e7-b5c824ea525d,RoboCLIP: One Demonstration is Enough to Learn Robot Policies,0.906173,"Reward specification is a notoriously difficult problem in reinforcement
learning, requiring extensive expert supervision to design robust reward
functions. Imitation learning (IL) methods attempt to circumvent these problems
by utilizing expert demonstrations but typically require a large number of
in-domain expert demonstrations. Inspired by advances in the field of
Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation
learning method that uses a single demonstration (overcoming the large data
requirement) in the form of a video demonstration or a textual description of
the task to generate rewards without manual reward function design.
Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like
videos of humans solving the task for reward generation, circumventing the need
to have the same demonstration and deployment domains. RoboCLIP utilizes
pretrained VLMs without any finetuning for reward generation. Reinforcement
learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher
zero-shot performance than competing imitation learning methods on downstream
robot manipulation tasks, doing so using only one video/text demonstration.",None,-1
626169f2-a250-48c9-9ba3-a1885d7f1169,TransFlow: Transformer as Flow Learner,0.974688,"Optical flow is an indispensable building block for various important
computer vision tasks, including motion estimation, object tracking, and
disparity measurement. In this work, we propose TransFlow, a pure transformer
architecture for optical flow estimation. Compared to dominant CNN-based
methods, TransFlow demonstrates three advantages. First, it provides more
accurate correlation and trustworthy matching in flow estimation by utilizing
spatial self-attention and cross-attention mechanisms between adjacent frames
to effectively capture global dependencies; Second, it recovers more
compromised information (e.g., occlusion and motion blur) in flow estimation
through long-range temporal association in dynamic scenes; Third, it enables a
concise self-learning paradigm and effectively eliminate the complex and
laborious multi-stage pre-training procedures. We achieve the state-of-the-art
results on the Sintel, KITTI-15, as well as several downstream tasks, including
video object detection, interpolation and stabilization. For its efficacy, we
hope TransFlow could serve as a flexible baseline for optical flow estimation.",None,-1
a7655a26-ba02-4927-972d-0942b46e04fe,Analyzing Intentional Behavior in Autonomous Agents under Uncertainty,0.0990424,"Principled accountability for autonomous decision-making in uncertain
environments requires distinguishing intentional outcomes from negligent
designs from actual accidents. We propose analyzing the behavior of autonomous
agents through a quantitative measure of the evidence of intentional behavior.
We model an uncertain environment as a Markov Decision Process (MDP). For a
given scenario, we rely on probabilistic model checking to compute the ability
of the agent to influence reaching a certain event. We call this the scope of
agency. We say that there is evidence of intentional behavior if the scope of
agency is high and the decisions of the agent are close to being optimal for
reaching the event. Our method applies counterfactual reasoning to
automatically generate relevant scenarios that can be analyzed to increase the
confidence of our assessment. In a case study, we show how our method can
distinguish between 'intentional' and 'accidental' traffic collisions.",None,-1
1e45f013-115f-44b0-942c-e90d259ed98f,TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play,0.632699,"Multi-agent football poses an unsolved challenge in AI research. Existing
work has focused on tackling simplified scenarios of the game, or else
leveraging expert demonstrations. In this paper, we develop a multi-agent
system to play the full 11 vs. 11 game mode, without demonstrations. This game
mode contains aspects that present major challenges to modern reinforcement
learning algorithms; multi-agent coordination, long-term planning, and
non-transitivity. To address these challenges, we present TiZero; a
self-evolving, multi-agent system that learns from scratch. TiZero introduces
several innovations, including adaptive curriculum learning, a novel self-play
strategy, and an objective that optimizes the policies of multiple agents
jointly. Experimentally, it outperforms previous systems by a large margin on
the Google Research Football environment, increasing win rates by over 30%. To
demonstrate the generality of TiZero's innovations, they are assessed on
several environments beyond football; Overcooked, Multi-agent
Particle-Environment, Tic-Tac-Toe and Connect-Four.",None,-1
9be5680e-ed10-440b-9113-51aab79cc7d6,Neuromorphic Event-based Facial Expression Recognition,0.949129,"Recently, event cameras have shown large applicability in several computer
vision fields especially concerning tasks that require high temporal
resolution. In this work, we investigate the usage of such kind of data for
emotion recognition by presenting NEFER, a dataset for Neuromorphic Event-based
Facial Expression Recognition. NEFER is composed of paired RGB and event videos
representing human faces labeled with the respective emotions and also
annotated with face bounding boxes and facial landmarks. We detail the data
acquisition process as well as providing a baseline method for RGB and event
data. The collected data captures subtle micro-expressions, which are hard to
spot with RGB data, yet emerge in the event domain. We report a double
recognition accuracy for the event-based approach, proving the effectiveness of
a neuromorphic approach for analyzing fast and hardly detectable expressions
and the emotions they conceal.",None,-1
34c435e1-6e69-41e1-9133-6630d03ff45c,Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information,0.819576,"Visual Word Sense Disambiguation (VWSD) is a task to find the image that most
accurately depicts the correct sense of the target word for the given context.
Previously, image-text matching models often suffered from recognizing
polysemous words. This paper introduces an unsupervised VWSD approach that uses
gloss information of an external lexical knowledge-base, especially the sense
definitions. Specifically, we suggest employing Bayesian inference to
incorporate the sense definitions when sense information of the answer is not
provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we
propose a context-aware definition generation with GPT-3. Experimental results
show that the VWSD performance significantly increased with our Bayesian
inference-based approach. In addition, our context-aware definition generation
achieved prominent performance improvement in OOD examples exhibiting better
performance than the existing definition generation method.",None,-1
a88a7594-f3f1-4f2a-bb93-4cb5339ff4bc,Bandwidth-efficient Inference for Neural Image Compression,0.229427,"With neural networks growing deeper and feature maps growing larger, limited
communication bandwidth with external memory (or DRAM) and power constraints
become a bottleneck in implementing network inference on mobile and edge
devices. In this paper, we propose an end-to-end differentiable bandwidth
efficient neural inference method with the activation compressed by neural data
compression method. Specifically, we propose a transform-quantization-entropy
coding pipeline for activation compression with symmetric exponential Golomb
coding and a data-dependent Gaussian entropy model for arithmetic coding.
Optimized with existing model quantization methods, low-level task of image
compression can achieve up to 19x bandwidth reduction with 6.21x energy saving.",None,-1
dd65916c-7877-402f-8abe-f133cc039737,Using Auxiliary Tasks In Multimodal Fusion Of Wav2vec 2.0 And BERT For Multimodal Emotion Recognition,0.683391,"The lack of data and the difficulty of multimodal fusion have always been
challenges for multimodal emotion recognition (MER). In this paper, we propose
to use pretrained models as upstream network, wav2vec 2.0 for audio modality
and BERT for text modality, and finetune them in downstream task of MER to cope
with the lack of data. For the difficulty of multimodal fusion, we use a
K-layer multi-head attention mechanism as a downstream fusion module. Starting
from the MER task itself, we design two auxiliary tasks to alleviate the
insufficient fusion between modalities and guide the network to capture and
align emotion-related features. Compared to the previous state-of-the-art
models, we achieve a better performance by 78.42% Weighted Accuracy (WA) and
79.71% Unweighted Accuracy (UA) on the IEMOCAP dataset.",None,-1
b8ef8437-451c-43f5-9776-658cd2e5b3f3,Probabilistic Circuits That Know What They Don't Know,0.665424,"Probabilistic circuits (PCs) are models that allow exact and tractable
probabilistic inference. In contrast to neural networks, they are often assumed
to be well-calibrated and robust to out-of-distribution (OOD) data. In this
paper, we show that PCs are in fact not robust to OOD data, i.e., they don't
know what they don't know. We then show how this challenge can be overcome by
model uncertainty quantification. To this end, we propose tractable dropout
inference (TDI), an inference procedure to estimate uncertainty by deriving an
analytical solution to Monte Carlo dropout (MCD) through variance propagation.
Unlike MCD in neural networks, which comes at the cost of multiple network
evaluations, TDI provides tractable sampling-free uncertainty estimates in a
single forward pass. TDI improves the robustness of PCs to distribution shift
and OOD data, demonstrated through a series of experiments evaluating the
classification confidence and uncertainty estimates on real-world data.",None,-1
ea99f9c4-0873-40e7-929b-1e43a0f26c40,Navigating Fairness Measures and Trade-Offs,0.770508,"In order to monitor and prevent bias in AI systems we can use a wide range of
(statistical) fairness measures. However, it is mathematically impossible to
optimize for all of these measures at the same time. In addition, optimizing a
fairness measure often greatly reduces the accuracy of the system (Kozodoi et
al, 2022). As a result, we need a substantive theory that informs us how to
make these decisions and for what reasons. I show that by using Rawls' notion
of justice as fairness, we can create a basis for navigating fairness measures
and the accuracy trade-off. In particular, this leads to a principled choice
focusing on both the most vulnerable groups and the type of fairness measure
that has the biggest impact on that group. This also helps to close part of the
gap between philosophical accounts of distributive justice and the fairness
literature that has been observed (Kuppler et al, 2021) and to operationalise
the value of fairness.",None,-1
ff0060f5-8e0c-4fb1-b4e7-0ff69813124a,Improving Mass Detection in Mammography Images: A Study of Weakly Supervised Learning and Class Activation Map Methods,0.399239,"In recent years, weakly supervised models have aided in mass detection using
mammography images, decreasing the need for pixel-level annotations. However,
most existing models in the literature rely on Class Activation Maps (CAM) as
the activation method, overlooking the potential benefits of exploring other
activation techniques. This work presents a study that explores and compares
different activation maps in conjunction with state-of-the-art methods for
weakly supervised training in mammography images. Specifically, we investigate
CAM, GradCAM, GradCAM++, XGradCAM, and LayerCAM methods within the framework of
the GMIC model for mass detection in mammography images. The evaluation is
conducted on the VinDr-Mammo dataset, utilizing the metrics Accuracy, True
Positive Rate (TPR), False Negative Rate (FNR), and False Positive Per Image
(FPPI). Results show that using different strategies of activation maps during
training and test stages leads to an improvement of the model. With this
strategy, we improve the results of the GMIC method, decreasing the FPPI value
and increasing TPR.",None,-1
9775d13f-3f49-4cdc-b6fd-ae8a40bfae3e,Epsilon-Identifiability of Causal Quantities,0.230531,"Identifying the effects of causes and causes of effects is vital in virtually
every scientific field. Often, however, the needed probabilities may not be
fully identifiable from the data sources available. This paper shows how
partial identifiability is still possible for several probabilities of
causation. We term this epsilon-identifiability and demonstrate its usefulness
in cases where the behavior of certain subpopulations can be restricted to
within some narrow bounds. In particular, we show how unidentifiable causal
effects and counterfactual probabilities can be narrowly bounded when such
allowances are made. Often those allowances are easily measured and reasonably
assumed. Finally, epsilon-identifiability is applied to the unit selection
problem.",None,-1
66e6a47c-4c08-48c2-8885-c48526ba1fa7,Transferring Procedural Knowledge across Commonsense Tasks,0.0406323,"Stories about everyday situations are an essential part of human
communication, motivating the need to develop AI agents that can reliably
understand these stories. Despite the long list of supervised methods for story
completion and procedural understanding, current AI has no mechanisms to
automatically track and explain procedures in unseen stories. To bridge this
gap, we study the ability of AI models to transfer procedural knowledge to
novel narrative tasks in a transparent manner. We design LEAP: a comprehensive
framework that integrates state-of-the-art modeling architectures, training
regimes, and augmentation strategies based on both natural and synthetic
stories. To address the lack of densely annotated training data, we devise a
robust automatic labeler based on few-shot prompting to enhance the augmented
data. Our experiments with in- and out-of-domain tasks reveal insights into the
interplay of different architectures, training regimes, and augmentation
strategies. LEAP's labeler has a clear positive impact on out-of-domain
datasets, while the resulting dense annotation provides native explainability.",None,-1
949a95e1-6b90-4d67-99bd-5c88d80b4ad9,Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages,0.553307,"Sentiment analysis (SA) systems are used in many products and hundreds of
languages. Gender and racial biases are well-studied in English SA systems, but
understudied in other languages, with few resources for such studies. To remedy
this, we build a counterfactual evaluation corpus for gender and racial/migrant
bias in four languages. We demonstrate its usefulness by answering a simple but
important question that an engineer might need to answer when deploying a
system: What biases do systems import from pre-trained models when compared to
a baseline with no pre-training? Our evaluation corpus, by virtue of being
counterfactual, not only reveals which models have less bias, but also
pinpoints changes in model bias behaviour, which enables more targeted
mitigation strategies. We release our code and evaluation corpora to facilitate
future research.",None,-1
c71a6e2d-17bc-4cbb-b765-cbad7492f368,Using Large Language Models for Cybersecurity Capture-The-Flag Challenges and Certification Questions,0.999704,"The assessment of cybersecurity Capture-The-Flag (CTF) exercises involves
participants finding text strings or ``flags'' by exploiting system
vulnerabilities. Large Language Models (LLMs) are natural-language models
trained on vast amounts of words to understand and generate text; they can
perform well on many CTF challenges. Such LLMs are freely available to
students. In the context of CTF exercises in the classroom, this raises
concerns about academic integrity. Educators must understand LLMs' capabilities
to modify their teaching to accommodate generative AI assistance. This research
investigates the effectiveness of LLMs, particularly in the realm of CTF
challenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT,
Google Bard, and Microsoft Bing. First, we assess the LLMs' question-answering
performance on five Cisco certifications with varying difficulty levels. Next,
we qualitatively study the LLMs' abilities in solving CTF challenges to
understand their limitations. We report on the experience of using the LLMs for
seven test cases in all five types of CTF challenges. In addition, we
demonstrate how jailbreak prompts can bypass and break LLMs' ethical
safeguards. The paper concludes by discussing LLM's impact on CTF exercises and
its implications.",None,-1
a504994e-ed31-4f21-8434-68acfb93644b,Understanding Difficulty-based Sample Weighting with a Universal Difficulty Measure,0.0738831,"Sample weighting is widely used in deep learning. A large number of weighting
methods essentially utilize the learning difficulty of training samples to
calculate their weights. In this study, this scheme is called difficulty-based
weighting. Two important issues arise when explaining this scheme. First, a
unified difficulty measure that can be theoretically guaranteed for training
samples does not exist. The learning difficulties of the samples are determined
by multiple factors including noise level, imbalance degree, margin, and
uncertainty. Nevertheless, existing measures only consider a single factor or
in part, but not in their entirety. Second, a comprehensive theoretical
explanation is lacking with respect to demonstrating why difficulty-based
weighting schemes are effective in deep learning. In this study, we
theoretically prove that the generalization error of a sample can be used as a
universal difficulty measure. Furthermore, we provide formal theoretical
justifications on the role of difficulty-based weighting for deep learning,
consequently revealing its positive influences on both the optimization
dynamics and generalization performance of deep models, which is instructive to
existing weighting schemes.",None,-1
3cf5329d-7dcb-4e24-8bfd-3bc02d48c5d0,"Learn over Past, Evolve for Future: Forecasting Temporal Trends for Fake News Detection",0.866978,"Fake news detection has been a critical task for maintaining the health of
the online news ecosystem. However, very few existing works consider the
temporal shift issue caused by the rapidly-evolving nature of news data in
practice, resulting in significant performance degradation when training on
past data and testing on future data. In this paper, we observe that the
appearances of news events on the same topic may display discernible patterns
over time, and posit that such patterns can assist in selecting training
instances that could make the model adapt better to future data. Specifically,
we design an effective framework FTT (Forecasting Temporal Trends), which could
forecast the temporal distribution patterns of news data and then guide the
detector to fast adapt to future distribution. Experiments on the real-world
temporally split dataset demonstrate the superiority of our proposed framework.
The code is available at https://github.com/ICTMCG/FTT-ACL23.",None,-1
27c34d7b-a7b2-4013-860d-74b6890607fd,What does CLIP know about a red circle? Visual prompt engineering for VLMs,0.723542,"Large-scale Vision-Language Models, such as CLIP, learn powerful image-text
representations that have found numerous applications, from zero-shot
classification to text-to-image generation. Despite that, their capabilities
for solving novel discriminative tasks via prompting fall behind those of large
language models, such as GPT-3. Here we explore the idea of visual prompt
engineering for solving computer vision tasks beyond classification by editing
in image space instead of text. In particular, we discover an emergent ability
of CLIP, where, by simply drawing a red circle around an object, we can direct
the model's attention to that region, while also maintaining global
information. We show the power of this simple approach by achieving
state-of-the-art in zero-shot referring expressions comprehension and strong
performance in keypoint localization tasks. Finally, we draw attention to some
potential ethical concerns of large language-vision models.",None,-1
87c1ca3e-161f-4d35-9f11-0fb23eabae35,Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale,0.0154577,"Large language models (LLMs) have achieved a milestone that undenia-bly
changed many held beliefs in artificial intelligence (AI). However, there
remains many limitations of these LLMs when it comes to true language
understanding, limitations that are a byproduct of the under-lying architecture
of deep neural networks. Moreover, and due to their subsymbolic nature,
whatever knowledge these models acquire about how language works will always be
buried in billions of microfeatures (weights), none of which is meaningful on
its own, making such models hopelessly unexplainable. To address these
limitations, we suggest com-bining the strength of symbolic representations
with what we believe to be the key to the success of LLMs, namely a successful
bottom-up re-verse engineering of language at scale. As such we argue for a
bottom-up reverse engineering of language in a symbolic setting. Hints on what
this project amounts to have been suggested by several authors, and we discuss
in some detail here how this project could be accomplished.",None,-1
ba1d1f60-5bf9-46a4-873c-c87de1cf5250,MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation,0.710772,"Weakly-supervised semantic segmentation aims to reduce labeling costs by
training semantic segmentation models using weak supervision, such as
image-level class labels. However, most approaches struggle to produce accurate
localization maps and suffer from false predictions in class-related
backgrounds (i.e., biased objects), such as detecting a railroad with the train
class. Recent methods that remove biased objects require additional supervision
for manually identifying biased objects for each problematic class and
collecting their datasets by reviewing predictions, limiting their
applicability to the real-world dataset with multiple labels and complex
relationships for biasing. Following the first observation that biased features
can be separated and eliminated by matching biased objects with backgrounds in
the same dataset, we propose a fully-automatic/model-agnostic biased removal
framework called MARS (Model-Agnostic biased object Removal without additional
Supervision), which utilizes semantically consistent features of an
unsupervised technique to eliminate biased objects in pseudo labels.
Surprisingly, we show that MARS achieves new state-of-the-art results on two
popular benchmarks, PASCAL VOC 2012 (val: 77.7%, test: 77.2%) and MS COCO 2014
(val: 49.4%), by consistently improving the performance of various WSSS models
by at least 30% without additional supervision.",None,-1
fade1260-4289-4f16-8f1b-eed60463740b,Multi-class Categorization of Reasons behind Mental Disturbance in Long Texts,0.364936,"Motivated with recent advances in inferring users' mental state in social
media posts, we identify and formulate the problem of finding causal indicators
behind mental illness in self-reported text. In the past, we witness the
presence of rule-based studies for causal explanation analysis on curated
Facebook data. The investigation on transformer-based model for multi-class
causal categorization in Reddit posts point to a problem of using long-text
which contains as many as 4000 words. Developing end-to-end transformer-based
models subject to the limitation of maximum-length in a given instance. To
handle this problem, we use Longformer and deploy its encoding on
transformer-based classifier. The experimental results show that Longformer
achieves new state-of-the-art results on M-CAMS, a publicly available dataset
with 62\% F1-score. Cause-specific analysis and ablation study prove the
effectiveness of Longformer. We believe our work facilitates causal analysis of
depression and suicide risk on social media data, and shows potential for
application on other mental health conditions.",None,-1
0b3b5d12-5ab1-4194-bc07-fa0347c54546,Scaling Laws for Associative Memories,0.968791,"Learning arguably involves the discovery and memorization of abstract rules.
The aim of this paper is to study associative memory mechanisms. Our model is
based on high-dimensional matrices consisting of outer products of embeddings,
which relates to the inner layers of transformer language models. We derive
precise scaling laws with respect to sample size and parameter size, and
discuss the statistical efficiency of different estimators, including
optimization-based algorithms. We provide extensive numerical experiments to
validate and interpret theoretical results, including fine-grained
visualizations of the stored memory associations.",None,-1
77dd2c39-fd79-47a4-b8ea-2be37c024da5,AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities,0.703748,"Accident detection and traffic analysis is a critical component of smart city
and autonomous transportation systems that can reduce accident frequency,
severity and improve overall traffic management. This paper presents a
comprehensive analysis of traffic accidents in different regions across the
United States using data from the National Highway Traffic Safety
Administration (NHTSA) Crash Report Sampling System (CRSS). To address the
challenges of accident detection and traffic analysis, this paper proposes a
framework that uses traffic surveillance cameras and action recognition systems
to detect and respond to traffic accidents spontaneously. Integrating the
proposed framework with emergency services will harness the power of traffic
cameras and machine learning algorithms to create an efficient solution for
responding to traffic accidents and reducing human errors. Advanced
intelligence technologies, such as the proposed accident detection systems in
smart cities, will improve traffic management and traffic accident severity.
Overall, this study provides valuable insights into traffic accidents in the US
and presents a practical solution to enhance the safety and efficiency of
transportation systems.",None,-1
38fc953a-a1e5-46cd-8ec4-e7e98e724bae,Natural Language Decomposition and Interpretation of Complex Utterances,0.123732,"Designing natural language interfaces has historically required collecting
supervised data to translate user requests into carefully designed intent
representations. This requires enumerating and labeling a long tail of user
requests, which is challenging. At the same time, large language models (LLMs)
encode knowledge about goals and plans that can help conversational assistants
interpret user requests requiring numerous steps to complete. We introduce an
approach to handle complex-intent-bearing utterances from a user via a process
of hierarchical natural language decomposition and interpretation. Our approach
uses a pre-trained language model to decompose a complex utterance into a
sequence of simpler natural language steps and interprets each step using the
language-to-program model designed for the interface. To test our approach, we
collect and release DeCU -- a new NL-to-program benchmark to evaluate
Decomposition of Complex Utterances. Experiments show that the proposed
approach enables the interpretation of complex utterances with almost no
complex training data, while outperforming standard few-shot prompting
approaches.",None,-1
5e7dcbe2-0098-4d1c-907d-b611188fefe4,Anomaly Detection of Command Shell Sessions based on DistilBERT: Unsupervised and Supervised Approaches,0.2822,"Anomaly detection in command shell sessions is a critical aspect of computer
security. Recent advances in deep learning and natural language processing,
particularly transformer-based models, have shown great promise for addressing
complex security challenges. In this paper, we implement a comprehensive
approach to detect anomalies in Unix shell sessions using a pretrained
DistilBERT model, leveraging both unsupervised and supervised learning
techniques to identify anomalous activity while minimizing data labeling. The
unsupervised method captures the underlying structure and syntax of Unix shell
commands, enabling the detection of session deviations from normal behavior.
Experiments on a large-scale enterprise dataset collected from production
systems demonstrate the effectiveness of our approach in detecting anomalous
behavior in Unix shell sessions. This work highlights the potential of
leveraging recent advances in transformers to address important computer
security challenges.",None,-1
fa50b24d-eda2-4ec2-b202-102f360f256c,A Unified Conditional Framework for Diffusion-based Image Restoration,0.726608,"Diffusion Probabilistic Models (DPMs) have recently shown remarkable
performance in image generation tasks, which are capable of generating highly
realistic images. When adopting DPMs for image restoration tasks, the crucial
aspect lies in how to integrate the conditional information to guide the DPMs
to generate accurate and natural output, which has been largely overlooked in
existing works. In this paper, we present a unified conditional framework based
on diffusion models for image restoration. We leverage a lightweight UNet to
predict initial guidance and the diffusion model to learn the residual of the
guidance. By carefully designing the basic module and integration module for
the diffusion model block, we integrate the guidance and other auxiliary
conditional information into every block of the diffusion model to achieve
spatially-adaptive generation conditioning. To handle high-resolution images,
we propose a simple yet effective inter-step patch-splitting strategy to
produce arbitrary-resolution images without grid artifacts. We evaluate our
conditional framework on three challenging tasks: extreme low-light denoising,
deblurring, and JPEG restoration, demonstrating its significant improvements in
perceptual quality and the generalization to restoration tasks.",None,-1
64aeb282-e729-4a37-a675-3b6aefba5464,Measuring Lexical Diversity in Texts: The Twofold Length Problem,0.0915667,"The impact of text length on the estimation of lexical diversity has captured
the attention of the scientific community for more than a century. Numerous
indices have been proposed, and many studies have been conducted to evaluate
them, but the problem remains. This methodological review provides a critical
analysis not only of the most commonly used indices in language learning
studies, but also of the length problem itself, as well as of the methodology
for evaluating the proposed solutions. The analysis of three datasets of
English language-learners' texts revealed that indices that reduce all texts to
the same length using a probabilistic or an algorithmic approach solve the
length dependency problem; however, all these indices failed to address the
second problem, which is their sensitivity to the parameter that determines the
length to which the texts are reduced. The paper concludes with recommendations
for optimizing lexical diversity analysis.",None,-1
41a2dc64-0c22-4260-bc15-90a0ba55d027,Learning Answer Generation using Supervision from Automatic Question Answering Evaluators,0.116167,"Recent studies show that sentence-level extractive QA, i.e., based on Answer
Sentence Selection (AS2), is outperformed by Generation-based QA (GenQA)
models, which generate answers using the top-k answer sentences ranked by AS2
models (a la retrieval-augmented generation style). In this paper, we propose a
novel training paradigm for GenQA using supervision from automatic QA
evaluation models (GAVA). Specifically, we propose three strategies to transfer
knowledge from these QA evaluation models to a GenQA model: (i) augmenting
training data with answers generated by the GenQA model and labelled by GAVA
(either statically, before training, or (ii) dynamically, at every training
epoch); and (iii) using the GAVA score for weighting the generator loss during
the learning of the GenQA model. We evaluate our proposed methods on two
academic and one industrial dataset, obtaining a significant improvement in
answering accuracy over the previous state of the art.",None,-1
cdbc7141-66c2-449d-ad32-74c31a289f55,Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations,0.709483,"The abundance of instructional videos and their narrations over the Internet
offers an exciting avenue for understanding procedural activities. In this
work, we propose to learn video representation that encodes both action steps
and their temporal ordering, based on a large-scale dataset of web
instructional videos and their narrations, without using human annotations. Our
method jointly learns a video representation to encode individual step
concepts, and a deep probabilistic model to capture both temporal dependencies
and immense individual variations in the step ordering. We empirically
demonstrate that learning temporal ordering not only enables new capabilities
for procedure reasoning, but also reinforces the recognition of individual
steps. Our model significantly advances the state-of-the-art results on step
classification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting
(+7.4% on COIN). Moreover, our model attains promising results in zero-shot
inference for step classification and forecasting, as well as in predicting
diverse and plausible steps for incomplete procedures. Our code is available at
https://github.com/facebookresearch/ProcedureVRL.",None,-1
19b3e1f2-dc41-4295-ad40-621e88976c64,X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs,0.725467,"Understanding when two pieces of text convey the same information is a goal
touching many subproblems in NLP, including textual entailment and
fact-checking. This problem becomes more complex when those two pieces of text
are in different languages. Here, we introduce X-PARADE (Cross-lingual
Paragraph-level Analysis of Divergences and Entailments), the first
cross-lingual dataset of paragraph-level information divergences. Annotators
label a paragraph in a target language at the span level and evaluate it with
respect to a corresponding paragraph in a source language, indicating whether a
given piece of information is the same, new, or new but can be inferred. This
last notion establishes a link with cross-language NLI. Aligned paragraphs are
sourced from Wikipedia pages in different languages, reflecting real
information divergences observed in the wild. Armed with our dataset, we
investigate a diverse set of approaches for this problem, including token
alignment from machine translation, textual entailment methods that localize
their decisions, and prompting LLMs. Our results show that these methods vary
in their capability to handle inferable information, but they all fall short of
human performance.",None,-1
c24776d1-0e25-4494-8479-fc37d8b81fcd,Knowledge Distillation for Feature Extraction in Underwater VSLAM,0.272063,"In recent years, learning-based feature detection and matching have
outperformed manually-designed methods in in-air cases. However, it is
challenging to learn the features in the underwater scenario due to the absence
of annotated underwater datasets. This paper proposes a cross-modal knowledge
distillation framework for training an underwater feature detection and
matching network (UFEN). In particular, we use in-air RGBD data to generate
synthetic underwater images based on a physical underwater imaging formation
model and employ these as the medium to distil knowledge from a teacher model
SuperPoint pretrained on in-air images. We embed UFEN into the ORB-SLAM3
framework to replace the ORB feature by introducing an additional binarization
layer. To test the effectiveness of our method, we built a new underwater
dataset with groundtruth measurements named EASI
(https://github.com/Jinghe-mel/UFEN-SLAM), recorded in an indoor water tank for
different turbidity levels. The experimental results on the existing dataset
and our new dataset demonstrate the effectiveness of our method.",None,-1
60d87f95-7d9a-4e23-9bbf-2f2813b2a387,Weakly-Supervised Scientific Document Classification via Retrieval-Augmented Multi-Stage Training,0.952331,"Scientific document classification is a critical task for a wide range of
applications, but the cost of obtaining massive amounts of human-labeled data
can be prohibitive. To address this challenge, we propose a weakly-supervised
approach for scientific document classification using label names only. In
scientific domains, label names often include domain-specific concepts that may
not appear in the document corpus, making it difficult to match labels and
documents precisely. To tackle this issue, we propose WANDER, which leverages
dense retrieval to perform matching in the embedding space to capture the
semantics of label names. We further design the label name expansion module to
enrich the label name representations. Lastly, a self-training step is used to
refine the predictions. The experiments on three datasets show that WANDER
outperforms the best baseline by 11.9% on average. Our code will be published
at https://github.com/ritaranx/wander.",None,-1
054829e9-47de-4b38-91d7-f12aaa3fba90,Building a Parallel Corpus and Training Translation Models Between Luganda and English,0.491924,"Neural machine translation (NMT) has achieved great successes with large
datasets, so NMT is more premised on high-resource languages. This continuously
underpins the low resource languages such as Luganda due to the lack of
high-quality parallel corpora, so even 'Google translate' does not serve
Luganda at the time of this writing. In this paper, we build a parallel corpus
with 41,070 pairwise sentences for Luganda and English which is based on three
different open-sourced corpora. Then, we train NMT models with hyper-parameter
search on the dataset. Experiments gave us a BLEU score of 21.28 from Luganda
to English and 17.47 from English to Luganda. Some translation examples show
high quality of the translation. We believe that our model is the first
Luganda-English NMT model. The bilingual dataset we built will be available to
the public.",None,-1
1510f3f6-c40d-4451-ba16-63777f184f4c,Summarizing Indian Languages using Multilingual Transformers based Models,0.099434,"With the advent of multilingual models like mBART, mT5, IndicBART etc.,
summarization in low resource Indian languages is getting a lot of attention
now a days. But still the number of datasets is low in number. In this work, we
(Team HakunaMatata) study how these multilingual models perform on the datasets
which have Indian languages as source and target text while performing
summarization. We experimented with IndicBART and mT5 models to perform the
experiments and report the ROUGE-1, ROUGE-2, ROUGE-3 and ROUGE-4 scores as a
performance metric.",None,-1
3a82c10b-2b30-40dc-8e42-35e141505084,Fast model inference and training on-board of Satellites,0.652435,"Artificial intelligence onboard satellites has the potential to reduce data
transmission requirements, enable real-time decision-making and collaboration
within constellations. This study deploys a lightweight foundational model
called RaVAEn on D-Orbit's ION SCV004 satellite. RaVAEn is a variational
auto-encoder (VAE) that generates compressed latent vectors from small image
tiles, enabling several downstream tasks. In this work we demonstrate the
reliable use of RaVAEn onboard a satellite, achieving an encoding time of
0.110s for tiles of a 4.8x4.8 km$^2$ area. In addition, we showcase fast
few-shot training onboard a satellite using the latent representation of data.
We compare the deployment of the model on the on-board CPU and on the available
Myriad vision processing unit (VPU) accelerator. To our knowledge, this work
shows for the first time the deployment of a multi-task model on-board a
CubeSat and the on-board training of a machine learning model.",None,-1
412790f0-d139-4c02-81d7-ca17b396a540,CoReFace: Sample-Guided Contrastive Regularization for Deep Face Recognition,0.588239,"The discriminability of feature representation is the key to open-set face
recognition. Previous methods rely on the learnable weights of the
classification layer that represent the identities. However, the evaluation
process learns no identity representation and drops the classifier from
training. This inconsistency could confuse the feature encoder in understanding
the evaluation goal and hinder the effect of identity-based methods. To
alleviate the above problem, we propose a novel approach namely Contrastive
Regularization for Face recognition (CoReFace) to apply image-level
regularization in feature representation learning. Specifically, we employ
sample-guided contrastive learning to regularize the training with the
image-image relationship directly, which is consistent with the evaluation
process. To integrate contrastive learning into face recognition, we augment
embeddings instead of images to avoid the image quality degradation. Then, we
propose a novel contrastive loss for the representation distribution by
incorporating an adaptive margin and a supervised contrastive mask to generate
steady loss values and avoid the collision with the classification supervision
signal. Finally, we discover and solve the semantically repetitive signal
problem in contrastive learning by exploring new pair coupling protocols.
Extensive experiments demonstrate the efficacy and efficiency of our CoReFace
which is highly competitive with the state-of-the-art approaches.",None,-1
25d7e407-7ab9-4990-80f1-2b2a1af6172f,Memory Efficient Diffusion Probabilistic Models via Patch-based Generation,0.0396875,"Diffusion probabilistic models have been successful in generating
high-quality and diverse images. However, traditional models, whose input and
output are high-resolution images, suffer from excessive memory requirements,
making them less practical for edge devices. Previous approaches for generative
adversarial networks proposed a patch-based method that uses positional
encoding and global content information. Nevertheless, designing a patch-based
approach for diffusion probabilistic models is non-trivial. In this paper, we
resent a diffusion probabilistic model that generates images on a
patch-by-patch basis. We propose two conditioning methods for a patch-based
generation. First, we propose position-wise conditioning using one-hot
representation to ensure patches are in proper positions. Second, we propose
Global Content Conditioning (GCC) to ensure patches have coherent content when
concatenated together. We evaluate our model qualitatively and quantitatively
on CelebA and LSUN bedroom datasets and demonstrate a moderate trade-off
between maximum memory consumption and generated image quality. Specifically,
when an entire image is divided into 2 x 2 patches, our proposed approach can
reduce the maximum memory consumption by half while maintaining comparable
image quality.",None,-1
532ea431-a48b-4e2d-a370-5fea53aaf935,Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing,0.589758,"Environmental damage has been of much concern, particularly in coastal areas
and the oceans, given climate change and the drastic effects of pollution and
extreme climate events. Our present-day analytical capabilities, along with
advancements in information acquisition techniques such as remote sensing, can
be utilised for the management and study of coral reef ecosystems. In this
paper, we present Reef-Insight, an unsupervised machine learning framework that
features advanced clustering methods and remote sensing for reef habitat
mapping. Our framework compares different clustering methods for reef habitat
mapping using remote sensing data. We evaluate four major clustering approaches
based on qualitative and visual assessments which include k-means, hierarchical
clustering, Gaussian mixture model, and density-based clustering. We utilise
remote sensing data featuring the One Tree Island reef in Australia's Southern
Great Barrier Reef. Our results indicate that clustering methods using remote
sensing data can well identify benthic and geomorphic clusters in reefs when
compared with other studies. Our results indicate that Reef-Insight can
generate detailed reef habitat maps outlining distinct reef habitats and has
the potential to enable further insights for reef restoration projects.",None,-1
37bd1dbd-2e23-4846-887e-eca13d5fd58d,Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark,0.994129,"Recent studies have demonstrated promising potential of ChatGPT for various
text annotation and classification tasks. However, ChatGPT is non-deterministic
which means that, as with human coders, identical input can lead to different
outputs. Given this, it seems appropriate to test the reliability of ChatGPT.
Therefore, this study investigates the consistency of ChatGPT's zero-shot
capabilities for text annotation and classification, focusing on different
model parameters, prompt variations, and repetitions of identical inputs. Based
on the real-world classification task of differentiating website texts into
news and not news, results show that consistency in ChatGPT's classification
output can fall short of scientific thresholds for reliability. For example,
even minor wording alterations in prompts or repeating the identical input can
lead to varying outputs. Although pooling outputs from multiple repetitions can
improve reliability, this study advises caution when using ChatGPT for
zero-shot text annotation and underscores the need for thorough validation,
such as comparison against human-annotated data. The unsupervised application
of ChatGPT for text annotation and classification is not recommended.",None,-1
1ee4f7f6-ab59-4639-8cad-2794acbd709a,MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer,0.769474,"With the increasing demand for video understanding, video moment and
highlight detection (MHD) has emerged as a critical research topic. MHD aims to
localize all moments and predict clip-wise saliency scores simultaneously.
Despite progress made by existing DETR-based methods, we observe that these
methods coarsely fuse features from different modalities, which weakens the
temporal intra-modal context and results in insufficient cross-modal
interaction. To address this issue, we propose MH-DETR (Moment and Highlight
Detection Transformer) tailored for MHD. Specifically, we introduce a simple
yet efficient pooling operator within the uni-modal encoder to capture global
intra-modal context. Moreover, to obtain temporally aligned cross-modal
features, we design a plug-and-play cross-modal interaction module between the
encoder and decoder, seamlessly integrating visual and textual features.
Comprehensive experiments on QVHighlights, Charades-STA, Activity-Net, and
TVSum datasets show that MH-DETR outperforms existing state-of-the-art methods,
demonstrating its effectiveness and superiority. Our code is available at
https://github.com/YoucanBaby/MH-DETR.",None,-1
0c7434f7-9c7b-4362-b6b7-283208fa2f36,X-ReID: Cross-Instance Transformer for Identity-Level Person Re-Identification,0.358756,"Currently, most existing person re-identification methods use Instance-Level
features, which are extracted only from a single image. However, these
Instance-Level features can easily ignore the discriminative information due to
the appearance of each identity varies greatly in different images. Thus, it is
necessary to exploit Identity-Level features, which can be shared across
different images of each identity. In this paper, we propose to promote
Instance-Level features to Identity-Level features by employing cross-attention
to incorporate information from one image to another of the same identity, thus
more unified and discriminative pedestrian information can be obtained. We
propose a novel training framework named X-ReID. Specifically, a Cross
Intra-Identity Instances module (IntraX) fuses different intra-identity
instances to transfer Identity-Level knowledge and make Instance-Level features
more compact. A Cross Inter-Identity Instances module (InterX) involves hard
positive and hard negative instances to improve the attention response to the
same identity instead of different identity, which minimizes intra-identity
variation and maximizes inter-identity variation. Extensive experiments on
benchmark datasets show the superiority of our method over existing works.
Particularly, on the challenging MSMT17, our proposed method gains 1.1% mAP
improvements when compared to the second place.",None,-1
f95f0d9d-c3c8-40df-9479-cff5b27ba040,TuPy-E: detecting hate speech in Brazilian Portuguese social media with a novel dataset and comprehensive analysis of models,0.433628,"Social media has become integral to human interaction, providing a platform
for communication and expression. However, the rise of hate speech on these
platforms poses significant risks to individuals and communities. Detecting and
addressing hate speech is particularly challenging in languages like Portuguese
due to its rich vocabulary, complex grammar, and regional variations. To
address this, we introduce TuPy-E, the largest annotated Portuguese corpus for
hate speech detection. TuPy-E leverages an open-source approach, fostering
collaboration within the research community. We conduct a detailed analysis
using advanced techniques like BERT models, contributing to both academic
understanding and practical applications",None,-1
5aaff7c1-1641-46fa-bf0a-019c967689fc,Learning to Name Classes for Vision and Language Models,0.121959,"Large scale vision and language models can achieve impressive zero-shot
recognition performance by mapping class specific text queries to image
content. Two distinct challenges that remain however, are high sensitivity to
the choice of handcrafted class names that define queries, and the difficulty
of adaptation to new, smaller datasets. Towards addressing these problems, we
propose to leverage available data to learn, for each class, an optimal word
embedding as a function of the visual content. By learning new word embeddings
on an otherwise frozen model, we are able to retain zero-shot capabilities for
new classes, easily adapt models to new datasets, and adjust potentially
erroneous, non-descriptive or ambiguous class names. We show that our solution
can easily be integrated in image classification and object detection
pipelines, yields significant performance gains in multiple scenarios and
provides insights into model biases and labelling errors.",None,-1
5546f0ce-541b-4476-8e06-48d600f715c9,SCB-dataset: A Dataset for Detecting Student Classroom Behavior,0.622878,"The use of deep learning methods for automatic detection of students'
classroom behavior is a promising approach to analyze their class performance
and enhance teaching effectiveness. However, the lack of publicly available
datasets on student behavior poses a challenge for researchers in this field.
To address this issue, we propose a Student Classroom Behavior dataset
(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248
labels and 4,003 images, with a focus on hand-raising behavior. We evaluated
the dataset using the YOLOv7 algorithm, achieving a mean average precision
(map) of up to 85.3%. We believe that our dataset can serve as a robust
foundation for future research in the field of student behavior detection and
promote further advancements in this area.Our SCB-dataset can be downloaded
from: https://github.com/Whiffe/SCB-dataset",None,-1
2656fff6-f008-40e3-b0b5-4468938f5de2,Comparing Sentence-Level Suggestions to Message-Level Suggestions in AI-Mediated Communication,0.687926,"Traditionally, writing assistance systems have focused on short or even
single-word suggestions. Recently, large language models like GPT-3 have made
it possible to generate significantly longer natural-sounding suggestions,
offering more advanced assistance opportunities. This study explores the
trade-offs between sentence- vs. message-level suggestions for AI-mediated
communication. We recruited 120 participants to act as staffers from
legislators' offices who often need to respond to large volumes of constituent
concerns. Participants were asked to reply to emails with different types of
assistance. The results show that participants receiving message-level
suggestions responded faster and were more satisfied with the experience, as
they mainly edited the suggested drafts. In addition, the texts they wrote were
evaluated as more helpful by others. In comparison, participants receiving
sentence-level assistance retained a higher sense of agency, but took longer
for the task as they needed to plan the flow of their responses and decide when
to use suggestions. Our findings have implications for designing
task-appropriate communication assistance systems.",None,-1
d5b038f7-19a3-44e3-accb-6b6b1deb99fc,Named Entity Resolution in Personal Knowledge Graphs,0.202436,"Entity Resolution (ER) is the problem of determining when two entities refer
to the same underlying entity. The problem has been studied for over 50 years,
and most recently, has taken on new importance in an era of large,
heterogeneous 'knowledge graphs' published on the Web and used widely in
domains as wide ranging as social media, e-commerce and search. This chapter
will discuss the specific problem of named ER in the context of personal
knowledge graphs (PKGs). We begin with a formal definition of the problem, and
the components necessary for doing high-quality and efficient ER. We also
discuss some challenges that are expected to arise for Web-scale data. Next, we
provide a brief literature review, with a special focus on how existing
techniques can potentially apply to PKGs. We conclude the chapter by covering
some applications, as well as promising directions for future research.",None,-1
8940aa1c-0984-4d76-8304-8bf6c2a9d2e4,Large Language Models Can Be Good Privacy Protection Learners,0.810937,"The proliferation of Large Language Models (LLMs) has driven considerable
interest in fine-tuning them with domain-specific data to create specialized
language models. Nevertheless, such domain-specific fine-tuning data often
contains sensitive personally identifiable information (PII). Direct
fine-tuning LLMs on this data without privacy protection poses a risk of
leakage. To address this challenge, we introduce Privacy Protection Language
Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects
domain-specific knowledge while safeguarding data privacy. Our work offers a
theoretical analysis for model design and delves into various techniques such
as corpus curation, penalty-based unlikelihood in training loss, and
instruction-based tuning, etc. Extensive experiments across diverse datasets
and scenarios demonstrate the effectiveness of our approaches. In particular,
instruction tuning with both positive and negative examples, stands out as a
promising method, effectively protecting private data while enhancing the
model's knowledge. Our work underscores the potential for Large Language Models
as robust privacy protection learners.",None,-1
76111383-bf47-463d-ad54-e044e5bcd776,ALDi: Quantifying the Arabic Level of Dialectness of Text,0.81652,"Transcribed speech and user-generated text in Arabic typically contain a
mixture of Modern Standard Arabic (MSA), the standardized language taught in
schools, and Dialectal Arabic (DA), used in daily communications. To handle
this variation, previous work in Arabic NLP has focused on Dialect
Identification (DI) on the sentence or the token level. However, DI treats the
task as binary, whereas we argue that Arabic speakers perceive a spectrum of
dialectness, which we operationalize at the sentence level as the Arabic Level
of Dialectness (ALDi), a continuous linguistic variable. We introduce the
AOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences
(17% from news articles and 83% from user comments on those articles) which are
manually labeled with their level of dialectness. We provide a detailed
analysis of AOC-ALDi and show that a model trained on it can effectively
identify levels of dialectness on a range of other corpora (including dialects
and genres not included in AOC-ALDi), providing a more nuanced picture than
traditional DI systems. Through case studies, we illustrate how ALDi can reveal
Arabic speakers' stylistic choices in different situations, a useful property
for sociolinguistic analyses.",None,-1
4837c8df-4042-4733-9cf4-5c1d79ae8bb5,Temporal Collection and Distribution for Referring Video Object Segmentation,0.500809,"Referring video object segmentation aims to segment a referent throughout a
video sequence according to a natural language expression. It requires aligning
the natural language expression with the objects' motions and their dynamic
associations at the global video level but segmenting objects at the frame
level. To achieve this goal, we propose to simultaneously maintain a global
referent token and a sequence of object queries, where the former is
responsible for capturing video-level referent according to the language
expression, while the latter serves to better locate and segment objects with
each frame. Furthermore, to explicitly capture object motions and
spatial-temporal cross-modal reasoning over objects, we propose a novel
temporal collection-distribution mechanism for interacting between the global
referent token and object queries. Specifically, the temporal collection
mechanism collects global information for the referent token from object
queries to the temporal motions to the language expression. In turn, the
temporal distribution first distributes the referent token to the referent
sequence across all frames and then performs efficient cross-frame reasoning
between the referent sequence and object queries in every frame. Experimental
results show that our method outperforms state-of-the-art methods on all
benchmarks consistently and significantly.",None,-1
dcfb1be7-64cc-4b35-8433-0fc36f7a4e54,SEM-POS: Grammatically and Semantically Correct Video Captioning,0.0978036,"Generating grammatically and semantically correct captions in video
captioning is a challenging task. The captions generated from the existing
methods are either word-by-word that do not align with grammatical structure or
miss key information from the input videos. To address these issues, we
introduce a novel global-local fusion network, with a Global-Local Fusion Block
(GLFB) that encodes and fuses features from different parts of speech (POS)
components with visual-spatial features. We use novel combinations of different
POS components - 'determinant + subject', 'auxiliary verb', 'verb', and
'determinant + object' for supervision of the POS blocks - Det + Subject, Aux
Verb, Verb, and Det + Object respectively. The novel global-local fusion
network together with POS blocks helps align the visual features with language
description to generate grammatically and semantically correct captions.
Extensive qualitative and quantitative experiments on benchmark MSVD and MSRVTT
datasets demonstrate that the proposed approach generates more grammatically
and semantically correct captions compared to the existing methods, achieving
the new state-of-the-art. Ablations on the POS blocks and the GLFB demonstrate
the impact of the contributions on the proposed method.",None,-1
127b26d0-be2c-4596-87cc-792c0393a85b,Why think step by step? Reasoning emerges from the locality of experience,0.489774,"Humans have a powerful and mysterious capacity to reason. Working through a
set of mental steps enables us to make inferences we would not be capable of
making directly even though we get no additional data from the world.
Similarly, when large language models generate intermediate steps (a chain of
thought) before answering a question, they often produce better answers than
they would directly. We investigate why and how chain-of-thought reasoning is
useful in language models, testing the hypothesis that reasoning is effective
when training data consists of overlapping local clusters of variables that
influence each other strongly. These training conditions enable the chaining of
accurate local inferences to estimate relationships between variables that were
not seen together in training. We prove that there will exist a ""reasoning
gap"", where reasoning through intermediate variables reduces bias, for the
simple case of an autoregressive density estimator trained on local samples
from a chain-structured probabilistic model. We then test our hypothesis
experimentally in more complex models, training an autoregressive language
model on samples from Bayes nets but only including a subset of variables in
each sample. We test language models' ability to match conditional
probabilities with and without intermediate reasoning steps, finding that
intermediate steps are only helpful when the training data is locally
structured with respect to dependencies between variables. The combination of
locally structured observations and reasoning is much more data-efficient than
training on all variables. Our results illustrate how the effectiveness of
reasoning step by step is rooted in the local statistical structure of the
training data.",None,-1
2b5f917f-7fdd-4e24-bd1a-fe43da75cd0a,Semantic Embedded Deep Neural Network: A Generic Approach to Boost Multi-Label Image Classification Performance,0.586783,"Fine-grained multi-label classification models have broad applications in
e-commerce, such as visual based label predictions ranging from fashion
attribute detection to brand recognition. One challenge to achieve satisfactory
performance for those classification tasks in real world is the wild visual
background signal that contains irrelevant pixels which confuses model to focus
onto the region of interest and make prediction upon the specific region. In
this paper, we introduce a generic semantic-embedding deep neural network to
apply the spatial awareness semantic feature incorporating a channel-wise
attention based model to leverage the localization guidance to boost model
performance for multi-label prediction. We observed an Avg.relative improvement
of 15.27% in terms of AUC score across all labels compared to the baseline
approach. Core experiment and ablation studies involve multi-label fashion
attribute classification performed on Instagram fashion apparels' image. We
compared the model performances among our approach, baseline approach, and 3
alternative approaches to leverage semantic features. Results show favorable
performance for our approach.",None,-1
4f1315fb-bff5-451e-b792-6fb4699c326a,zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning,0.0989542,"Federated learning (FL) is a machine learning paradigm, which enables
multiple and decentralized clients to collaboratively train a model under the
orchestration of a central aggregator. FL can be a scalable machine learning
solution in big data scenarios. Traditional FL relies on the trust assumption
of the central aggregator, which forms cohorts of clients honestly. However, a
malicious aggregator, in reality, could abandon and replace the client's
training models, or insert fake clients, to manipulate the final training
results. In this work, we introduce zkFL, which leverages zero-knowledge proofs
to tackle the issue of a malicious aggregator during the training model
aggregation process. To guarantee the correct aggregation results, the
aggregator provides a proof per round, demonstrating to the clients that the
aggregator executes the intended behavior faithfully. To further reduce the
verification cost of clients, we use blockchain to handle the proof in a
zero-knowledge way, where miners (i.e., the participants validating and
maintaining the blockchain data) can verify the proof without knowing the
clients' local and aggregated models. The theoretical analysis and empirical
results show that zkFL achieves better security and privacy than traditional
FL, without modifying the underlying FL network structure or heavily
compromising the training speed.",None,-1
40d1a806-7d57-4e15-b907-430b9cc7e127,ControlMat: A Controlled Generative Approach to Material Capture,0.934765,"Material reconstruction from a photograph is a key component of 3D content
creation democratization. We propose to formulate this ill-posed problem as a
controlled synthesis one, leveraging the recent progress in generative deep
networks. We present ControlMat, a method which, given a single photograph with
uncontrolled illumination as input, conditions a diffusion model to generate
plausible, tileable, high-resolution physically-based digital materials. We
carefully analyze the behavior of diffusion models for multi-channel outputs,
adapt the sampling process to fuse multi-scale information and introduce rolled
diffusion to enable both tileability and patched diffusion for high-resolution
outputs. Our generative approach further permits exploration of a variety of
materials which could correspond to the input image, mitigating the unknown
lighting conditions. We show that our approach outperforms recent inference and
latent-space-optimization methods, and carefully validate our diffusion process
design choices. Supplemental materials and additional details are available at:
https://gvecchio.com/controlmat/.",None,-1
34d994c1-2347-4b6b-aec6-d0ad2f20277d,Concept Learning for Interpretable Multi-Agent Reinforcement Learning,0.724004,"Multi-agent robotic systems are increasingly operating in real-world
environments in close proximity to humans, yet are largely controlled by policy
models with inscrutable deep neural network representations. We introduce a
method for incorporating interpretable concepts from a domain expert into
models trained through multi-agent reinforcement learning, by requiring the
model to first predict such concepts then utilize them for decision making.
This allows an expert to both reason about the resulting concept policy models
in terms of these high-level concepts at run-time, as well as intervene and
correct mispredictions to improve performance. We show that this yields
improved interpretability and training stability, with benefits to policy
performance and sample efficiency in a simulated and real-world
cooperative-competitive multi-agent game.",None,-1
a25416be-fece-42cf-bd86-0e07630a9731,Loop Closure Detection Based on Object-level Spatial Layout and Semantic Consistency,0.334523,"Visual simultaneous localization and mapping (SLAM) systems face challenges
in detecting loop closure under the circumstance of large viewpoint changes. In
this paper, we present an object-based loop closure detection method based on
the spatial layout and semanic consistency of the 3D scene graph. Firstly, we
propose an object-level data association approach based on the semantic
information from semantic labels, intersection over union (IoU), object color,
and object embedding. Subsequently, multi-view bundle adjustment with the
associated objects is utilized to jointly optimize the poses of objects and
cameras. We represent the refined objects as a 3D spatial graph with semantics
and topology. Then, we propose a graph matching approach to select
correspondence objects based on the structure layout and semantic property
similarity of vertices' neighbors. Finally, we jointly optimize camera
trajectories and object poses in an object-level pose graph optimization, which
results in a globally consistent map. Experimental results demonstrate that our
proposed data association approach can construct more accurate 3D semantic
maps, and our loop closure method is more robust than point-based and
object-based methods in circumstances with large viewpoint changes.",None,-1
3fc26c69-06d7-4b07-9bc3-395409ccb6c9,Predicting Motion Plans for Articulating Everyday Objects,0.6094,"Mobile manipulation tasks such as opening a door, pulling open a drawer, or
lifting a toilet lid require constrained motion of the end-effector under
environmental and task constraints. This, coupled with partial information in
novel environments, makes it challenging to employ classical motion planning
approaches at test time. Our key insight is to cast it as a learning problem to
leverage past experience of solving similar planning problems to directly
predict motion plans for mobile manipulation tasks in novel situations at test
time. To enable this, we develop a simulator, ArtObjSim, that simulates
articulated objects placed in real scenes. We then introduce SeqIK+$\theta_0$,
a fast and flexible representation for motion plans. Finally, we learn models
that use SeqIK+$\theta_0$ to quickly predict motion plans for articulating
novel objects at test time. Experimental evaluation shows improved speed and
accuracy at generating motion plans than pure search-based methods and pure
learning methods.",None,-1
d5608f53-01b2-43cc-bb58-1559416b99e3,Deep Reinforcement Learning for Cyber System Defense under Dynamic Adversarial Uncertainties,0.429682,"Development of autonomous cyber system defense strategies and action
recommendations in the real-world is challenging, and includes characterizing
system state uncertainties and attack-defense dynamics. We propose a
data-driven deep reinforcement learning (DRL) framework to learn proactive,
context-aware, defense countermeasures that dynamically adapt to evolving
adversarial behaviors while minimizing loss of cyber system operations. A
dynamic defense optimization problem is formulated with multiple protective
postures against different types of adversaries with varying levels of skill
and persistence. A custom simulation environment was developed and experiments
were devised to systematically evaluate the performance of four model-free DRL
algorithms against realistic, multi-stage attack sequences. Our results suggest
the efficacy of DRL algorithms for proactive cyber defense under multi-stage
attack profiles and system uncertainties.",None,-1
0b8243b7-14ea-4066-a4b9-19e5f89e140d,Detecting and Mitigating Hallucinations in Multilingual Summarisation,0.859355,"Hallucinations pose a significant challenge to the reliability of neural
models for abstractive summarisation. While automatically generated summaries
may be fluent, they often lack faithfulness to the original document. This
issue becomes even more pronounced in low-resource settings, such as
cross-lingual transfer. With the existing faithful metrics focusing on English,
even measuring the extent of this phenomenon in cross-lingual settings is hard.
To address this, we first develop a novel metric, mFACT, evaluating the
faithfulness of non-English summaries, leveraging translation-based transfer
from multiple English faithfulness metrics. We then propose a simple but
effective method to reduce hallucinations with a cross-lingual transfer, which
weighs the loss of each training example by its faithfulness score. Through
extensive experiments in multiple languages, we demonstrate that mFACT is the
metric that is most suited to detect hallucinations. Moreover, we find that our
proposed loss weighting method drastically increases both performance and
faithfulness according to both automatic and human evaluation when compared to
strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset
are available at https://github.com/yfqiu-nlp/mfact-summ.",None,-1
27277935-beb2-49fa-99df-2af4819c22c9,TBFormer: Two-Branch Transformer for Image Forgery Localization,0.566443,"Image forgery localization aims to identify forged regions by capturing
subtle traces from high-quality discriminative features. In this paper, we
propose a Transformer-style network with two feature extraction branches for
image forgery localization, and it is named as Two-Branch Transformer
(TBFormer). Firstly, two feature extraction branches are elaborately designed,
taking advantage of the discriminative stacked Transformer layers, for both RGB
and noise domain features. Secondly, an Attention-aware Hierarchical-feature
Fusion Module (AHFM) is proposed to effectively fuse hierarchical features from
two different domains. Although the two feature extraction branches have the
same architecture, their features have significant differences since they are
extracted from different domains. We adopt position attention to embed them
into a unified feature domain for hierarchical feature investigation. Finally,
a Transformer decoder is constructed for feature reconstruction to generate the
predicted mask. Extensive experiments on publicly available datasets
demonstrate the effectiveness of the proposed model.",None,-1
20c9d980-61c4-4520-b3ce-8b49866d171e,Object-Centric Video Prediction via Decoupling of Object Dynamics and Interactions,0.323726,"We propose a novel framework for the task of object-centric video prediction,
i.e., extracting the compositional structure of a video sequence, as well as
modeling objects dynamics and interactions from visual observations in order to
predict the future object states, from which we can then generate subsequent
video frames. With the goal of learning meaningful spatio-temporal object
representations and accurately forecasting object states, we propose two novel
object-centric video predictor (OCVP) transformer modules, which decouple the
processing of temporal dynamics and object interactions, thus presenting an
improved prediction performance. In our experiments, we show how our
object-centric prediction framework utilizing our OCVP predictors outperforms
object-agnostic video prediction models on two different datasets, while
maintaining consistent and accurate object representations.",None,-1
3dd8ead3-086f-42ed-9f89-a97ee752d30a,Style Transfer for 2D Talking Head Animation,0.180252,"Audio-driven talking head animation is a challenging research topic with many
real-world applications. Recent works have focused on creating photo-realistic
2D animation, while learning different talking or singing styles remains an
open problem. In this paper, we present a new method to generate talking head
animation with learnable style references. Given a set of style reference
frames, our framework can reconstruct 2D talking head animation based on a
single input image and an audio stream. Our method first produces facial
landmarks motion from the audio stream and constructs the intermediate style
patterns from the style reference images. We then feed both outputs into a
style-aware image generator to generate the photo-realistic and fidelity 2D
animation. In practice, our framework can extract the style information of a
specific character and transfer it to any new static image for talking head
animation. The intensive experimental results show that our method achieves
better results than recent state-of-the-art approaches qualitatively and
quantitatively.",None,-1
386f1eea-9c28-4ca2-b093-65413fbcddcd,DocMAE: Document Image Rectification via Self-supervised Representation Learning,0.252279,"Tremendous efforts have been made on document image rectification, but how to
learn effective representation of such distorted images is still
under-explored. In this paper, we present DocMAE, a novel self-supervised
framework for document image rectification. Our motivation is to encode the
structural cues in document images by leveraging masked autoencoder to benefit
the rectification, i.e., the document boundaries, and text lines. Specifically,
we first mask random patches of the background-excluded document images and
then reconstruct the missing pixels. With such a self-supervised learning
approach, the network is encouraged to learn the intrinsic structure of
deformed documents by restoring document boundaries and missing text lines.
Transfer performance in the downstream rectification task validates the
effectiveness of our method. Extensive experiments are conducted to demonstrate
the effectiveness of our method.",None,-1
3b5d3ab3-c4a3-4e01-a9c0-5be29d6ffe1e,Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra,0.786505,"Neural Radiance Fields (NeRFs) are a very recent and very popular approach
for the problems of novel view synthesis and 3D reconstruction. A popular scene
representation used by NeRFs is to combine a uniform, voxel-based subdivision
of the scene with an MLP. Based on the observation that a (sparse) point cloud
of the scene is often available, this paper proposes to use an adaptive
representation based on tetrahedra obtained by Delaunay triangulation instead
of uniform subdivision or point-based representations. We show that such a
representation enables efficient training and leads to state-of-the-art
results. Our approach elegantly combines concepts from 3D geometry processing,
triangle-based rendering, and modern neural radiance fields. Compared to
voxel-based representations, ours provides more detail around parts of the
scene likely to be close to the surface. Compared to point-based
representations, our approach achieves better performance. The source code is
publicly available at: https://jkulhanek.com/tetra-nerf.",None,-1
2865e255-f22c-4ddb-9171-a77f9c038645,How Many Demonstrations Do You Need for In-context Learning?,0.468882,"Large language models (LLMs) are capable to perform complex reasoning by
in-context learning (ICL) when provided with a few input-output demonstrations
(demos) and more powerful when intermediate reasoning steps (""chain of thoughts
(CoT)"") of the demos are given. Is it necessary to use multi-demo in ICL? In
this paper, we study ICL using fewer demos for each test query on the tasks
in~\cite{wei2022chain}. Surprisingly, we do not observe significant degradation
when using only one randomly chosen demo. To study this phenomenon, for each
test query, we categorize demos into ""correct demos"" leading to the correct
answer, and ""wrong demos"" resulting in wrong answers. Our analysis reveals an
inherent bias in those widely studied datasets: most demos are correct for a
majority of test queries, which explains the good performance of using one
random demo. Moreover, ICL (with and w/o CoT) using only one correct demo
significantly outperforms all-demo ICL adopted by most previous works,
indicating the weakness of LLMs in finding correct demo(s) for input queries,
which is difficult to evaluate on the biased datasets. Furthermore, we observe
a counterintuitive behavior of ICL using multi-demo, i.e., its accuracy
degrades(improves) when given more correct(wrong) demos. This implies that ICL
can be easily misguided by interference among demos and their spurious
correlations. Our analyses highlight several fundamental challenges that need
to be addressed in LLMs training, ICL, and benchmark design.",None,-1
771966d7-269c-4603-9091-05ef4fa734fd,Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue,0.699011,"Open-domain dialogue system usually requires different sources of knowledge
to generate more informative and evidential responses. However, existing
knowledge-grounded dialogue systems either focus on a single knowledge source
or overlook the dependency between multiple sources of knowledge, which may
result in generating inconsistent or even paradoxical responses. To incorporate
multiple knowledge sources and dependencies between them, we propose SAFARI, a
novel framework that leverages the exceptional capabilities of large language
models (LLMs) in planning, understanding, and incorporating under both
supervised and unsupervised settings. Specifically, SAFARI decouples the
knowledge grounding into multiple sources and response generation, which allows
easy extension to various knowledge sources including the possibility of not
using any sources. To study the problem, we construct a personalized
knowledge-grounded dialogue dataset \textit{\textbf{K}nowledge \textbf{B}ehind
\textbf{P}ersona}~(\textbf{KBP}), which is the first to consider the dependency
between persona and implicit knowledge. Experimental results on the KBP dataset
demonstrate that the SAFARI framework can effectively produce
persona-consistent and knowledge-enhanced responses.",None,-1
5b618f4d-d5d5-4aad-bb22-920c92dc09a6,Cryptocurrency Price Prediction using Twitter Sentiment Analysis,0.525252,"The cryptocurrency ecosystem has been the centre of discussion on many social
media platforms, following its noted volatility and varied opinions. Twitter is
rapidly being utilised as a news source and a medium for bitcoin discussion.
Our algorithm seeks to use historical prices and sentiment of tweets to
forecast the price of Bitcoin. In this study, we develop an end-to-end model
that can forecast the sentiment of a set of tweets (using a Bidirectional
Encoder Representations from Transformers - based Neural Network Model) and
forecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted
sentiment and other metrics like historical cryptocurrency price data, tweet
volume, a user's following, and whether or not a user is verified. The
sentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average
of real-time data, and test data. The mean absolute percent error for the price
prediction was 3.6%.",None,-1
b4e00046-0645-4c9e-8d2a-bc032c5de30d,Robust face anti-spoofing framework with Convolutional Vision Transformer,0.85237,"Owing to the advances in image processing technology and large-scale
datasets, companies have implemented facial authentication processes, thereby
stimulating increased focus on face anti-spoofing (FAS) against realistic
presentation attacks. Recently, various attempts have been made to improve face
recognition performance using both global and local learning on face images;
however, to the best of our knowledge, this is the first study to investigate
whether the robustness of FAS against domain shifts is improved by considering
global information and local cues in face images captured using self-attention
and convolutional layers. This study proposes a convolutional vision
transformer-based framework that achieves robust performance for various unseen
domain data. Our model resulted in 7.3%$p$ and 12.9%$p$ increases in FAS
performance compared to models using only a convolutional neural network or
vision transformer, respectively. It also shows the highest average rank in
sub-protocols of cross-dataset setting over the other nine benchmark models for
domain generalization.",None,-1
fc70bd0e-3ec8-4515-bdd9-2cc343bf9d39,Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models,0.527267,"We analyze sentiment analysis and toxicity detection models to detect the
presence of explicit bias against people with disability (PWD). We employ the
bias identification framework of Perturbation Sensitivity Analysis to examine
conversations related to PWD on social media platforms, specifically Twitter
and Reddit, in order to gain insight into how disability bias is disseminated
in real-world social settings. We then create the \textit{Bias Identification
Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any
sentiment analysis and toxicity detection models. Our study utilizes BITS to
uncover significant biases in four open AIaaS (AI as a Service) sentiment
analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API,
DistilBERT and two toxicity detection models, namely two versions of
Toxic-BERT. Our findings indicate that all of these models exhibit
statistically significant explicit bias against PWD.",None,-1
54fb37cc-602e-4149-84ca-b30019b40567,Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study,0.098361,"Text classification is an area of research which has been studied over the
years in Natural Language Processing (NLP). Adapting NLP to multiple domains
has introduced many new challenges for text classification and one of them is
long document classification. While state-of-the-art transformer models provide
excellent results in text classification, most of them have limitations in the
maximum sequence length of the input sequence. The majority of the transformer
models are limited to 512 tokens, and therefore, they struggle with long
document classification problems. In this research, we explore on employing
Model Fusing for long document classification while comparing the results with
well-known BERT and Longformer architectures.",None,-1
364ad91a-b9cc-4d70-8667-dc5aca85acd2,The WHY in Business Processes: Discovery of Causal Execution Dependencies,0.417993,"Unraveling the causal relationships among the execution of process activities
is a crucial element in predicting the consequences of process interventions
and making informed decisions regarding process improvements. Process discovery
algorithms exploit time precedence as their main source of model derivation.
Hence, a causal view can supplement process discovery, being a new perspective
in which relations reflect genuine cause-effect dependencies among the tasks.
This calls for faithful new techniques to discover the causal execution
dependencies among the tasks in the process. To this end, our work offers a
systematic approach to the unveiling of the causal business process by
leveraging an existing causal discovery algorithm over activity timing. In
addition, this work delves into a set of conditions under which process mining
discovery algorithms generate a model that is incongruent with the causal
business process model, and shows how the latter model can be methodologically
employed for a sound analysis of the process. Our methodology searches for such
discrepancies between the two models in the context of three causal patterns,
and derives a new view in which these inconsistencies are annotated over the
mined process model. We demonstrate our methodology employing two open process
mining algorithms, the IBM Process Mining tool, and the LiNGAM causal discovery
technique. We apply it on a synthesized dataset and on two open benchmark data
sets.",None,-1
ab672c10-a25a-4563-9c46-d7602f10c4c0,Online POMDP Planning with Anytime Deterministic Guarantees,0.814804,"Autonomous agents operating in real-world scenarios frequently encounter
uncertainty and make decisions based on incomplete information. Planning under
uncertainty can be mathematically formalized using partially observable Markov
decision processes (POMDPs). However, finding an optimal plan for POMDPs can be
computationally expensive and is feasible only for small tasks. In recent
years, approximate algorithms, such as tree search and sample-based
methodologies, have emerged as state-of-the-art POMDP solvers for larger
problems. Despite their effectiveness, these algorithms offer only
probabilistic and often asymptotic guarantees toward the optimal solution due
to their dependence on sampling. To address these limitations, we derive a
deterministic relationship between a simplified solution that is easier to
obtain and the theoretically optimal one. First, we derive bounds for selecting
a subset of the observations to branch from while computing a complete belief
at each posterior node. Then, since a complete belief update may be
computationally demanding, we extend the bounds to support reduction of both
the state and the observation spaces. We demonstrate how our guarantees can be
integrated with existing state-of-the-art solvers that sample a subset of
states and observations. As a result, the returned solution holds deterministic
bounds relative to the optimal policy. Lastly, we substantiate our findings
with supporting experimental results.",None,-1
23bead00-22a2-4e3e-a57a-5f9eca32f0e4,Disentangled Phonetic Representation for Chinese Spelling Correction,0.920234,"Chinese Spelling Correction (CSC) aims to detect and correct erroneous
characters in Chinese texts. Although efforts have been made to introduce
phonetic information (Hanyu Pinyin) in this task, they typically merge phonetic
representations with character representations, which tends to weaken the
representation effect of normal texts. In this work, we propose to disentangle
the two types of features to allow for direct interaction between textual and
phonetic information. To learn useful phonetic representations, we introduce a
pinyin-to-character objective to ask the model to predict the correct
characters based solely on phonetic information, where a separation mask is
imposed to disable attention from phonetic input to text. To avoid overfitting
the phonetics, we further design a self-distillation module to ensure that
semantic information plays a major role in the prediction. Extensive
experiments on three CSC benchmarks demonstrate the superiority of our method
in using phonetic information.",None,-1
2c6bed76-03bd-4fc4-860a-b2bf7da81240,SEMI-PointRend: Improved Semiconductor Wafer Defect Classification and Segmentation as Rendering,0.858947,"In this study, we applied the PointRend (Point-based Rendering) method to
semiconductor defect segmentation. PointRend is an iterative segmentation
algorithm inspired by image rendering in computer graphics, a new image
segmentation method that can generate high-resolution segmentation masks. It
can also be flexibly integrated into common instance segmentation
meta-architecture such as Mask-RCNN and semantic meta-architecture such as FCN.
We implemented a model, termed as SEMI-PointRend, to generate precise
segmentation masks by applying the PointRend neural network module. In this
paper, we focus on comparing the defect segmentation predictions of
SEMI-PointRend and Mask-RCNN for various defect types (line-collapse, single
bridge, thin bridge, multi bridge non-horizontal). We show that SEMI-PointRend
can outperforms Mask R-CNN by up to 18.8% in terms of segmentation mean average
precision.",None,-1
3ed2788a-5b9c-4986-8572-391b376883ab,Generative Speech Recognition Error Correction with Large Language Models and Task-Activating Prompting,0.999317,"We explore the ability of large language models (LLMs) to act as speech
recognition post-processors that perform rescoring and error correction. Our
first focus is on instruction prompting to let LLMs perform these task without
fine-tuning, for which we evaluate different prompting schemes, both zero- and
few-shot in-context learning, and a novel task activation prompting method that
combines causal instructions and demonstration to increase its context windows.
Next, we show that rescoring only by in-context learning with frozen LLMs
achieves results that are competitive with rescoring by domain-tuned LMs, using
a pretrained first-pass recognition system and rescoring output on two
out-of-domain tasks (ATIS and WSJ). By combining prompting techniques with
fine-tuning we achieve error rates below the N-best oracle level, showcasing
the generalization power of the LLMs.",None,-1
1e915dd3-8d28-4cae-9b8d-9a569d7dac3d,AugDiff: Diffusion based Feature Augmentation for Multiple Instance Learning in Whole Slide Image,0.75864,"Multiple Instance Learning (MIL), a powerful strategy for weakly supervised
learning, is able to perform various prediction tasks on gigapixel Whole Slide
Images (WSIs). However, the tens of thousands of patches in WSIs usually incur
a vast computational burden for image augmentation, limiting the MIL model's
improvement in performance. Currently, the feature augmentation-based MIL
framework is a promising solution, while existing methods such as Mixup often
produce unrealistic features. To explore a more efficient and practical
augmentation method, we introduce the Diffusion Model (DM) into MIL for the
first time and propose a feature augmentation framework called AugDiff.
Specifically, we employ the generation diversity of DM to improve the quality
of feature augmentation and the step-by-step generation property to control the
retention of semantic information. We conduct extensive experiments over three
distinct cancer datasets, two different feature extractors, and three prevalent
MIL algorithms to evaluate the performance of AugDiff. Ablation study and
visualization further verify the effectiveness. Moreover, we highlight
AugDiff's higher-quality augmented feature over image augmentation and its
superiority over self-supervised learning. The generalization over external
datasets indicates its broader applications.",None,-1
f2445962-4a8d-4eb4-9dbc-5700ddf7730a,Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational AutoEncoders,0.580866,"The injection of syntactic information in Variational AutoEncoders (VAEs) has
been shown to result in an overall improvement of performances and
generalisation. An effective strategy to achieve such a goal is to separate the
encoding of distributional semantic features and syntactic structures into
heterogeneous latent spaces via multi-task learning or dual encoder
architectures. However, existing works employing such techniques are limited to
LSTM-based VAEs. In this paper, we investigate latent space separation methods
for structural syntactic injection in Transformer-based VAE architectures
(i.e., Optimus). Specifically, we explore how syntactic structures can be
leveraged in the encoding stage through the integration of graph-based and
sequential models, and how multiple, specialised latent representations can be
injected into the decoder's attention mechanism via low-rank operators. Our
empirical evaluation, carried out on natural language sentences and
mathematical expressions, reveals that the proposed end-to-end VAE architecture
can result in a better overall organisation of the latent space, alleviating
the information loss occurring in standard VAE setups, resulting in enhanced
performances on language modelling and downstream generation tasks.",None,-1
0069b12b-f08b-477a-859c-0bc01302a63d,Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model,0.321628,"Latent diffusion models (LDMs) exhibit an impressive ability to produce
realistic images, yet the inner workings of these models remain mysterious.
Even when trained purely on images without explicit depth information, they
typically output coherent pictures of 3D scenes. In this work, we investigate a
basic interpretability question: does an LDM create and use an internal
representation of simple scene geometry? Using linear probes, we find evidence
that the internal activations of the LDM encode linear representations of both
3D depth data and a salient-object / background distinction. These
representations appear surprisingly early in the denoising process$-$well
before a human can easily make sense of the noisy images. Intervention
experiments further indicate these representations play a causal role in image
synthesis, and may be used for simple high-level editing of an LDM's output.
Project page: https://yc015.github.io/scene-representation-diffusion-model/",None,-1
2a946541-77ac-401a-bbdb-e4be70049f2c,Deep Learning from Parametrically Generated Virtual Buildings for Real-World Object Recognition,0.0549882,"We study the use of parametric building information modeling (BIM) to
automatically generate training data for artificial neural networks (ANNs) to
recognize building objects in photos. Teaching artificial intelligence (AI)
machines to detect building objects in images is the foundation toward
AI-assisted semantic 3D reconstruction of existing buildings. However, there
exists the challenge of acquiring training data which is typically
human-annotated, that is, unless a computer machine can generate high-quality
data to train itself for a certain task. In that vein, we trained ANNs solely
on realistic computer-generated images of 3D BIM models which were
parametrically and automatically generated using the BIMGenE program. The ANN
training result demonstrated generalizability and good semantic segmentation on
a test case as well as arbitrary photos of buildings that are outside the range
of the training data, which is significant for the future of training AI with
generated data for solving real-world architectural problems.",None,-1
de16cfe7-2d53-47d6-bff6-6dd9800421ef,Towards Real-World Burst Image Super-Resolution: Benchmark and Method,0.538074,"Despite substantial advances, single-image super-resolution (SISR) is always
in a dilemma to reconstruct high-quality images with limited information from
one input image, especially in realistic scenarios. In this paper, we establish
a large-scale real-world burst super-resolution dataset, i.e., RealBSR, to
explore the faithful reconstruction of image details from multiple frames.
Furthermore, we introduce a Federated Burst Affinity network (FBAnet) to
investigate non-trivial pixel-wise displacements among images under real-world
image degradation. Specifically, rather than using pixel-wise alignment, our
FBAnet employs a simple homography alignment from a structural geometry aspect
and a Federated Affinity Fusion (FAF) strategy to aggregate the complementary
information among frames. Those fused informative representations are fed to a
Transformer-based module of burst representation decoding. Besides, we have
conducted extensive experiments on two versions of our datasets, i.e.,
RealBSR-RAW and RealBSR-RGB. Experimental results demonstrate that our FBAnet
outperforms existing state-of-the-art burst SR methods and also achieves
visually-pleasant SR image predictions with model details. Our dataset, codes,
and models are publicly available at https://github.com/yjsunnn/FBANet.",None,-1
353795b3-3961-466b-941c-542f7115b335,Contrastive Decoding Improves Reasoning in Large Language Models,0.255835,"We demonstrate that Contrastive Decoding -- a simple, computationally light,
and training-free text generation method proposed by Li et al 2022 -- achieves
large out-of-the-box improvements over greedy decoding on a variety of
reasoning tasks. Originally shown to improve the perceived quality of long-form
text generation, Contrastive Decoding searches for strings that maximize a
weighted difference in likelihood between strong and weak models. We show that
Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM
2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA
2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in
addition to improvements on a collection of other tasks. Analysis suggests that
Contrastive Decoding improves over existing methods by preventing some abstract
reasoning errors, as well as by avoiding simpler modes such as copying sections
of the input during chain-of-thought. Overall, Contrastive Decoding outperforms
nucleus sampling for long-form generation and greedy decoding for reasoning
tasks, making it a powerful general purpose method for generating text from
language models.",None,-1
26e52a8d-6e6e-4d5f-b6c7-da909ec8fb8d,Preference Transformer: Modeling Human Preferences using Transformers for RL,0.790777,"Preference-based reinforcement learning (RL) provides a framework to train
agents using human preferences between two behaviors. However, preference-based
RL has been challenging to scale since it requires a large amount of human
feedback to learn a reward function aligned with human intent. In this paper,
we present Preference Transformer, a neural architecture that models human
preferences using transformers. Unlike prior approaches assuming human judgment
is based on the Markovian rewards which contribute to the decision equally, we
introduce a new preference model based on the weighted sum of non-Markovian
rewards. We then design the proposed preference model using a transformer
architecture that stacks causal and bidirectional self-attention layers. We
demonstrate that Preference Transformer can solve a variety of control tasks
using real human preferences, while prior approaches fail to work. We also show
that Preference Transformer can induce a well-specified reward and attend to
critical events in the trajectory by automatically capturing the temporal
dependencies in human decision-making. Code is available on the project
website: https://sites.google.com/view/preference-transformer.",None,-1
26d0a1e2-becb-4a0e-8918-d9aa22bef836,PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model,0.586783,"Despite the remarkable progress in natural language understanding with
pretrained Transformers, neural language models often do not handle commonsense
knowledge well. Toward commonsense-aware models, there have been attempts to
obtain knowledge, ranging from automatic acquisition to crowdsourcing. However,
it is difficult to obtain a high-quality knowledge base at a low cost,
especially from scratch. In this paper, we propose PHALM, a method of building
a knowledge graph from scratch, by prompting both crowdworkers and a large
language model (LLM). We used this method to build a Japanese event knowledge
graph and trained Japanese commonsense generation models. Experimental results
revealed the acceptability of the built graph and inferences generated by the
trained models. We also report the difference in prompting humans and an LLM.
Our code, data, and models are available at
github.com/nlp-waseda/comet-atomic-ja.",None,-1
c3787b45-7a64-48df-90b7-a8578d4c8842,"Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations",0.914065,"The emergence of an AI-powered chatbot that can generate human-like sentences
and write coherent essays has caught the world's attention. This paper
discusses the historical overview of chatbots and the technology behind Chat
Generative Pre-trained Transformer, better known as ChatGPT. Moreover,
potential applications of ChatGPT in various domains, including healthcare,
education, and research, are highlighted. Despite promising results, there are
several privacy and ethical concerns surrounding ChatGPT. In addition, we
highlight some of the important limitations of the current version of ChatGPT.
We also ask ChatGPT to provide its point of view and present its responses to
several questions we attempt to answer.",None,-1
f72f5fc7-6951-4a39-9540-07f556af13d1,Evaluating Factual Consistency of Texts with Semantic Role Labeling,0.218001,"Automated evaluation of text generation systems has recently seen increasing
attention, particularly checking whether generated text stays truthful to input
sources. Existing methods frequently rely on an evaluation using task-specific
language models, which in turn allows for little interpretability of generated
scores. We introduce SRLScore, a reference-free evaluation metric designed with
text summarization in mind. Our approach generates fact tuples constructed from
Semantic Role Labels, applied to both input and summary texts. A final
factuality score is computed by an adjustable scoring mechanism, which allows
for easy adaption of the method across domains. Correlation with human
judgments on English summarization datasets shows that SRLScore is competitive
with state-of-the-art methods and exhibits stable generalization across
datasets without requiring further training or hyperparameter tuning. We
experiment with an optional co-reference resolution step, but find that the
performance boost is mostly outweighed by the additional compute required. Our
metric is available online at https://github.com/heyjing/SRLScore.",None,-1
2ca94a4c-0441-4ac9-a5ff-7fe199aac414,Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs,0.0282926,"The potential of large language models (LLMs) to reason like humans has been
a highly contested topic in Machine Learning communities. However, the
reasoning abilities of humans are multifaceted and can be seen in various
forms, including analogical, spatial and moral reasoning, among others. This
fact raises the question whether LLMs can perform equally well across all these
different domains. This research work aims to investigate the performance of
LLMs on different reasoning tasks by conducting experiments that directly use
or draw inspirations from existing datasets on analogical and spatial
reasoning. Additionally, to evaluate the ability of LLMs to reason like human,
their performance is evaluted on more open-ended, natural language questions.
My findings indicate that LLMs excel at analogical and moral reasoning, yet
struggle to perform as proficiently on spatial reasoning tasks. I believe these
experiments are crucial for informing the future development of LLMs,
particularly in contexts that require diverse reasoning proficiencies. By
shedding light on the reasoning abilities of LLMs, this study aims to push
forward our understanding of how they can better emulate the cognitive
abilities of humans.",None,-1
11716e5a-f021-4ece-9813-ad0e94c78e9d,Generative AI in the Construction Industry: Opportunities & Challenges,0.996744,"In the last decade, despite rapid advancements in artificial intelligence
(AI) transforming many industry practices, construction largely lags in
adoption. Recently, the emergence and rapid adoption of advanced large language
models (LLM) like OpenAI's GPT, Google's PaLM, and Meta's Llama have shown
great potential and sparked considerable global interest. However, the current
surge lacks a study investigating the opportunities and challenges of
implementing Generative AI (GenAI) in the construction sector, creating a
critical knowledge gap for researchers and practitioners. This underlines the
necessity to explore the prospects and complexities of GenAI integration.
Bridging this gap is fundamental to optimizing GenAI's early-stage adoption
within the construction sector. Given GenAI's unprecedented capabilities to
generate human-like content based on learning from existing content, we reflect
on two guiding questions: What will the future bring for GenAI in the
construction industry? What are the potential opportunities and challenges in
implementing GenAI in the construction industry? This study delves into
reflected perception in literature, analyzes the industry perception using
programming-based word cloud and frequency analysis, and integrates authors'
opinions to answer these questions. This paper recommends a conceptual GenAI
implementation framework, provides practical recommendations, summarizes future
research questions, and builds foundational literature to foster subsequent
research expansion in GenAI within the construction and its allied architecture
& engineering domains.",None,-1
52ed57cb-3213-40c5-920b-22e0366d3314,Balanced Supervised Contrastive Learning for Few-Shot Class-Incremental Learning,0.42661,"Few-shot class-incremental learning (FSCIL) presents the primary challenge of
balancing underfitting to a new session's task and forgetting the tasks from
previous sessions. To address this challenge, we develop a simple yet powerful
learning scheme that integrates effective methods for each core component of
the FSCIL network, including the feature extractor, base session classifiers,
and incremental session classifiers. In feature extractor training, our goal is
to obtain balanced generic representations that benefit both current viewable
and unseen or past classes. To achieve this, we propose a balanced supervised
contrastive loss that effectively balances these two objectives. In terms of
classifiers, we analyze and emphasize the importance of unifying initialization
methods for both the base and incremental session classifiers. Our method
demonstrates outstanding ability for new task learning and preventing
forgetting on CUB200, CIFAR100, and miniImagenet datasets, with significant
improvements over previous state-of-the-art methods across diverse metrics. We
conduct experiments to analyze the significance and rationale behind our
approach and visualize the effectiveness of our representations on new tasks.
Furthermore, we conduct diverse ablation studies to analyze the effects of each
module.",None,-1
e7575bf0-629a-4775-b45c-5120937c3a9d,Semantic Compression With Large Language Models,0.485915,"The rise of large language models (LLMs) is revolutionizing information
retrieval, question answering, summarization, and code generation tasks.
However, in addition to confidently presenting factually inaccurate information
at times (known as ""hallucinations""), LLMs are also inherently limited by the
number of input and output tokens that can be processed at once, making them
potentially less effective on tasks that require processing a large set or
continuous stream of information. A common approach to reducing the size of
data is through lossless or lossy compression. Yet, in some cases it may not be
strictly necessary to perfectly recover every detail from the original data, as
long as a requisite level of semantic precision or intent is conveyed.
  This paper presents three contributions to research on LLMs. First, we
present the results from experiments exploring the viability of approximate
compression using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT
interfaces. Second, we investigate and quantify the capability of LLMs to
compress text and code, as well as to recall and manipulate compressed
representations of prompts. Third, we present two novel metrics -- Exact
Reconstructive Effectiveness (ERE) and Semantic Reconstruction Effectiveness
(SRE) -- that quantify the level of preserved intent between text compressed
and decompressed by the LLMs we studied. Our initial results indicate that
GPT-4 can effectively compress and reconstruct text while preserving the
semantic essence of the original text, providing a path to leverage
$\sim$5$\times$ more tokens than present limits allow.",None,-1
4d7d5702-8ab8-42e7-8923-2753a4b10ed2,Gloss Attention for Gloss-free Sign Language Translation,0.705029,"Most sign language translation (SLT) methods to date require the use of gloss
annotations to provide additional supervision information, however, the
acquisition of gloss is not easy. To solve this problem, we first perform an
analysis of existing models to confirm how gloss annotations make SLT easier.
We find that it can provide two aspects of information for the model, 1) it can
help the model implicitly learn the location of semantic boundaries in
continuous sign language videos, 2) it can help the model understand the sign
language video globally. We then propose \emph{gloss attention}, which enables
the model to keep its attention within video segments that have the same
semantics locally, just as gloss helps existing models do. Furthermore, we
transfer the knowledge of sentence-to-sentence similarity from the natural
language model to our gloss attention SLT network (GASLT) to help it understand
sign language videos at the sentence level. Experimental results on multiple
large-scale sign language datasets show that our proposed GASLT model
significantly outperforms existing methods. Our code is provided in
\url{https://github.com/YinAoXiong/GASLT}.",None,-1
ed833205-0386-4f2f-86e2-d16232a6f192,The AI Revolution: Opportunities and Challenges for the Finance Sector,0.60828,"This report examines Artificial Intelligence (AI) in the financial sector,
outlining its potential to revolutionise the industry and identify its
challenges. It underscores the criticality of a well-rounded understanding of
AI, its capabilities, and its implications to effectively leverage its
potential while mitigating associated risks. The potential of AI potential
extends from augmenting existing operations to paving the way for novel
applications in the finance sector. The application of AI in the financial
sector is transforming the industry. Its use spans areas from customer service
enhancements, fraud detection, and risk management to credit assessments and
high-frequency trading. However, along with these benefits, AI also presents
several challenges. These include issues related to transparency,
interpretability, fairness, accountability, and trustworthiness. The use of AI
in the financial sector further raises critical questions about data privacy
and security. A further issue identified in this report is the systemic risk
that AI can introduce to the financial sector. Being prone to errors, AI can
exacerbate existing systemic risks, potentially leading to financial crises.
Regulation is crucial to harnessing the benefits of AI while mitigating its
potential risks. Despite the global recognition of this need, there remains a
lack of clear guidelines or legislation for AI use in finance. This report
discusses key principles that could guide the formation of effective AI
regulation in the financial sector, including the need for a risk-based
approach, the inclusion of ethical considerations, and the importance of
maintaining a balance between innovation and consumer protection. The report
provides recommendations for academia, the finance industry, and regulators.",None,-1
9b1d1d88-a596-4ea6-89f8-b788423339ab,Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?,0.492632,"ASR systems are generally built for the spoken 'standard', and their
performance declines for non-standard dialects/varieties. This is a problem for
a language like Irish, where there is no single spoken standard, but rather
three major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As a
diagnostic to quantify the effect of the speaker's dialect on recognition
performance, 12 ASR systems were trained, firstly using baseline
dialect-balanced training corpora, and then using modified versions of the
baseline corpora, where dialect-specific materials were either subtracted or
added. Results indicate that dialect-balanced corpora do not yield a similar
performance across the dialects: the Ul dialect consistently underperforms,
whereas Mu yields lowest WERs. There is a close relationship between Co and Mu
dialects, but one that is not symmetrical. These results will guide future
corpus collection and system building strategies to optimise for cross-dialect
performance equity.",None,-1
642e1d84-e557-4f8a-93b5-d1031e68d0d5,Strivec: Sparse Tri-Vector Radiance Fields,0.83269,"We propose Strivec, a novel neural representation that models a 3D scene as a
radiance field with sparsely distributed and compactly factorized local tensor
feature grids. Our approach leverages tensor decomposition, following the
recent work TensoRF, to model the tensor grids. In contrast to TensoRF which
uses a global tensor and focuses on their vector-matrix decomposition, we
propose to utilize a cloud of local tensors and apply the classic
CANDECOMP/PARAFAC (CP) decomposition to factorize each tensor into triple
vectors that express local feature distributions along spatial axes and
compactly encode a local neural field. We also apply multi-scale tensor grids
to discover the geometry and appearance commonalities and exploit spatial
coherence with the tri-vector factorization at multiple local scales. The final
radiance field properties are regressed by aggregating neural features from
multiple local tensors across all scales. Our tri-vector tensors are sparsely
distributed around the actual scene surface, discovered by a fast coarse
reconstruction, leveraging the sparsity of a 3D scene. We demonstrate that our
model can achieve better rendering quality while using significantly fewer
parameters than previous methods, including TensoRF and Instant-NGP.",None,-1
284106a6-069e-48d4-aa35-884043ce2eb8,"Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models",0.840644,"While adversarial training has been extensively studied for ResNet
architectures and low resolution datasets like CIFAR, much less is known for
ImageNet. Given the recent debate about whether transformers are more robust
than convnets, we revisit adversarial training on ImageNet comparing ViTs and
ConvNeXts. Extensive experiments show that minor changes in architecture, most
notably replacing PatchStem with ConvStem, and training scheme have a
significant impact on the achieved robustness. These changes not only increase
robustness in the seen $\ell_\infty$-threat model, but even more so improve
generalization to unseen $\ell_1/\ell_2$-attacks. Our modified ConvNeXt,
ConvNeXt + ConvStem, yields the most robust $\ell_\infty$-models across
different ranges of model parameters and FLOPs, while our ViT + ConvStem yields
the best generalization to unseen threat models.",None,-1
25470270-5855-4b70-99dc-55b3efe41e33,Exploring Large Language Models for Human Mobility Prediction under Public Events,0.773698,"Public events, such as concerts and sports games, can be major attractors for
large crowds, leading to irregular surges in travel demand. Accurate human
mobility prediction for public events is thus crucial for event planning as
well as traffic or crowd management. While rich textual descriptions about
public events are commonly available from online sources, it is challenging to
encode such information in statistical or machine learning models. Existing
methods are generally limited in incorporating textual information, handling
data sparsity, or providing rationales for their predictions. To address these
challenges, we introduce a framework for human mobility prediction under public
events (LLM-MPE) based on Large Language Models (LLMs), leveraging their
unprecedented ability to process textual data, learn from minimal examples, and
generate human-readable explanations. Specifically, LLM-MPE first transforms
raw, unstructured event descriptions from online sources into a standardized
format, and then segments historical mobility data into regular and
event-related components. A prompting strategy is designed to direct LLMs in
making and rationalizing demand predictions considering historical mobility and
event features. A case study is conducted for Barclays Center in New York City,
based on publicly available event information and taxi trip data. Results show
that LLM-MPE surpasses traditional models, particularly on event days, with
textual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers
interpretable insights into its predictions. Despite the great potential of
LLMs, we also identify key challenges including misinformation and high costs
that remain barriers to their broader adoption in large-scale human mobility
analysis.",None,-1
9dd7e023-46ea-4d4c-9515-23818d01d38c,Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion,0.803374,"Temporal Knowledge graph completion (TKGC) is a crucial task that involves
reasoning at known timestamps to complete the missing part of facts and has
attracted more and more attention in recent years. Most existing methods focus
on learning representations based on graph neural networks while inaccurately
extracting information from timestamps and insufficiently utilizing the implied
information in relations. To address these problems, we propose a novel TKGC
model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We
convert a series of sampled quadruples into pre-trained language model inputs
and convert intervals between timestamps into different prompts to make
coherent sentences with implicit semantic information. We train our model with
a masking strategy to convert TKGC task into a masked token prediction task,
which can leverage the semantic information in pre-trained language models.
Experiments on three benchmark datasets and extensive analysis demonstrate that
our model has great competitiveness compared to other models with four metrics.
Our model can effectively incorporate information from temporal knowledge
graphs into the language models.",None,-1
57e6e97e-f33c-4096-8af2-1602a0132383,From Shortcuts to Triggers: Backdoor Defense with Denoised PoE,0.704826,"Language models are often at risk of diverse backdoor attacks, especially
data poisoning. Thus, it is important to investigate defense solutions for
addressing them. Existing backdoor defense methods mainly focus on backdoor
attacks with explicit triggers, leaving a universal defense against various
backdoor attacks with diverse triggers largely unexplored. In this paper, we
propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised
Product-of-Experts), which is inspired by the shortcut nature of backdoor
attacks, to defend various backdoor attacks. DPoE consists of two models: a
shallow model that captures the backdoor shortcuts and a main model that is
prevented from learning the backdoor shortcuts. To address the label flip
caused by backdoor attackers, DPoE incorporates a denoising design. Experiments
on SST-2 dataset show that DPoE significantly improves the defense performance
against various types of backdoor triggers including word-level,
sentence-level, and syntactic triggers. Furthermore, DPoE is also effective
under a more challenging but practical setting that mixes multiple types of
trigger.",None,-1
4d12366e-01ff-4116-a083-e4b5f9723e74,Pointerformer: Deep Reinforced Multi-Pointer Transformer for the Traveling Salesman Problem,0.995723,"Traveling Salesman Problem (TSP), as a classic routing optimization problem
originally arising in the domain of transportation and logistics, has become a
critical task in broader domains, such as manufacturing and biology. Recently,
Deep Reinforcement Learning (DRL) has been increasingly employed to solve TSP
due to its high inference efficiency. Nevertheless, most of existing end-to-end
DRL algorithms only perform well on small TSP instances and can hardly
generalize to large scale because of the drastically soaring memory consumption
and computation time along with the enlarging problem scale. In this paper, we
propose a novel end-to-end DRL approach, referred to as Pointerformer, based on
multi-pointer Transformer. Particularly, Pointerformer adopts both reversible
residual network in the encoder and multi-pointer network in the decoder to
effectively contain memory consumption of the encoder-decoder architecture. To
further improve the performance of TSP solutions, Pointerformer employs both a
feature augmentation method to explore the symmetries of TSP at both training
and inference stages as well as an enhanced context embedding approach to
include more comprehensive context information in the query. Extensive
experiments on a randomly generated benchmark and a public benchmark have shown
that, while achieving comparative results on most small-scale TSP instances as
SOTA DRL approaches do, Pointerformer can also well generalize to large-scale
TSPs.",None,-1
62e598b7-0e94-495f-a58d-a839271842c5,Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models,0.105205,"Given a pretrained encoder-based language model, how can we accurately
compress it without retraining? Retraining-free structured pruning algorithms
are crucial in pretrained language model compression due to their significantly
reduced pruning cost and capability to prune large language models. However,
existing retraining-free algorithms encounter severe accuracy degradation, as
they fail to handle pruning errors, especially at high compression rates. In
this paper, we propose K-prune (Knowledge-preserving pruning), an accurate
retraining-free structured pruning algorithm for pretrained encoder-based
language models. K-prune focuses on preserving the useful knowledge of the
pretrained model to minimize pruning errors through a carefully designed
iterative pruning process composed of knowledge measurement,
knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a
result, K-prune shows significant accuracy improvements up to 58.02%p higher F1
score compared to existing retraining-free pruning algorithms under a high
compression rate of 80% on the SQuAD benchmark without any retraining process.",None,-1
56e39cb0-0119-446f-820c-0e82ad2eb3f7,Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields,0.373712,"Recently, the editing of neural radiance fields (NeRFs) has gained
considerable attention, but most prior works focus on static scenes while
research on the appearance editing of dynamic scenes is relatively lacking. In
this paper, we propose a novel framework to edit the local appearance of
dynamic NeRFs by manipulating pixels in a single frame of training video.
Specifically, to locally edit the appearance of dynamic NeRFs while preserving
unedited regions, we introduce a local surface representation of the edited
region, which can be inserted into and rendered along with the original NeRF
and warped to arbitrary other frames through a learned invertible motion
representation network. By employing our method, users without professional
expertise can easily add desired content to the appearance of a dynamic scene.
We extensively evaluate our approach on various scenes and show that our
approach achieves spatially and temporally consistent editing results. Notably,
our approach is versatile and applicable to different variants of dynamic NeRF
representations.",None,-1
673fcf5e-1246-46ce-b214-ddb421e9b6f1,T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation,0.90464,"Despite the stunning ability to generate high-quality images by recent
text-to-image models, current approaches often struggle to effectively compose
objects with different attributes and relationships into a complex and coherent
scene. We propose T2I-CompBench, a comprehensive benchmark for open-world
compositional text-to-image generation, consisting of 6,000 compositional text
prompts from 3 categories (attribute binding, object relationships, and complex
compositions) and 6 sub-categories (color binding, shape binding, texture
binding, spatial relationships, non-spatial relationships, and complex
compositions). We further propose several evaluation metrics specifically
designed to evaluate compositional text-to-image generation and explore the
potential and limitations of multimodal LLMs for evaluation. We introduce a new
approach, Generative mOdel fine-tuning with Reward-driven Sample selection
(GORS), to boost the compositional text-to-image generation abilities of
pretrained text-to-image models. Extensive experiments and evaluations are
conducted to benchmark previous methods on T2I-CompBench, and to validate the
effectiveness of our proposed evaluation metrics and GORS approach. Project
page is available at https://karine-h.github.io/T2I-CompBench/.",None,-1
20ad6c83-b0af-4067-b41d-5d93a005d57a,ClusterLLM: Large Language Models as a Guide for Text Clustering,0.820056,"We introduce ClusterLLM, a novel text clustering framework that leverages
feedback from an instruction-tuned large language model, such as ChatGPT.
Compared with traditional unsupervised methods that builds upon ""small""
embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the
emergent capability of LLM even if its embeddings are inaccessible; and (2) it
understands the user's preference on clustering through textual instruction
and/or a few annotated data. First, we prompt ChatGPT for insights on
clustering perspective by constructing hard triplet questions <does A better
correspond to B than C>, where A, B and C are similar data points that belong
to different clusters according to small embedder. We empirically show that
this strategy is both effective for fine-tuning small embedder and
cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on
clustering granularity by carefully designed pairwise questions <do A and B
belong to the same category>, and tune the granularity from cluster hierarchies
that is the most consistent with the ChatGPT answers. Extensive experiments on
14 datasets show that ClusterLLM consistently improves clustering quality, at
an average cost of ~$0.6 per dataset. The code will be available at
https://github.com/zhang-yu-wei/ClusterLLM.",None,-1
c383eb00-27cf-4069-8095-6e23e1ed3575,Empirical Investigation of Neural Symbolic Reasoning Strategies,0.104907,"Neural reasoning accuracy improves when generating intermediate reasoning
steps. However, the source of this improvement is yet unclear. Here, we
investigate and factorize the benefit of generating intermediate steps for
symbolic reasoning. Specifically, we decompose the reasoning strategy w.r.t.
step granularity and chaining strategy. With a purely symbolic numerical
reasoning dataset (e.g., A=1, B=3, C=A+3, C?), we found that the choice of
reasoning strategies significantly affects the performance, with the gap
becoming even larger as the extrapolation length becomes longer. Surprisingly,
we also found that certain configurations lead to nearly perfect performance,
even in the case of length extrapolation. Our results indicate the importance
of further exploring effective strategies for neural reasoning models.",None,-1
4fa2f58f-dac9-4fc0-8678-c7cfafa5c9f8,CLIP-KD: An Empirical Study of CLIP Model Distillation,0.483363,"Contrastive Language-Image Pre-training (CLIP) has become a promising
language-supervised visual pre-training framework. This paper aims to distill
small CLIP models supervised by a large teacher CLIP model. We propose several
distillation strategies, including relation, feature, gradient and contrastive
paradigms, to examine the effectiveness of CLIP-Knowledge Distillation (KD). We
show that a simple feature mimicry with Mean Squared Error loss works
surprisingly well. Moreover, interactive contrastive learning across teacher
and student encoders is also effective in performance improvement. We explain
that the success of CLIP-KD can be attributed to maximizing the feature
similarity between teacher and student. The unified method is applied to
distill several student models trained on CC3M+12M. CLIP-KD improves student
CLIP models consistently over zero-shot ImageNet classification and cross-modal
retrieval benchmarks. When using ViT-L/14 pretrained on Laion-400M as the
teacher, CLIP-KD achieves 57.5\% and 55.4\% zero-shot top-1 ImageNet accuracy
over ViT-B/16 and ResNet-50, surpassing the original CLIP without KD by 20.5\%
and 20.1\% margins, respectively. Our code is released on
https://github.com/winycg/CLIP-KD.",None,-1
c4066798-e284-4937-b218-49d32e286a71,Text-Transport: Toward Learning Causal Effects of Natural Language,0.395374,"As language technologies gain prominence in real-world settings, it is
important to understand how changes to language affect reader perceptions. This
can be formalized as the causal effect of varying a linguistic attribute (e.g.,
sentiment) on a reader's response to the text. In this paper, we introduce
Text-Transport, a method for estimation of causal effects from natural language
under any text distribution. Current approaches for valid causal effect
estimation require strong assumptions about the data, meaning the data from
which one can estimate valid causal effects often is not representative of the
actual target domain of interest. To address this issue, we leverage the notion
of distribution shift to describe an estimator that transports causal effects
between domains, bypassing the need for strong assumptions in the target
domain. We derive statistical guarantees on the uncertainty of this estimator,
and we report empirical results and analyses that support the validity of
Text-Transport across data settings. Finally, we use Text-Transport to study a
realistic setting--hate speech on social media--in which causal effects do
shift significantly between text domains, demonstrating the necessity of
transport when conducting causal inference on natural language.",None,-1
ad12c3a2-da91-403c-ae50-fdeca0490f7b,HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification,0.197664,"Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification as the labels form a complex hierarchical structure.
Existing dual-encoder methods in HTC achieve weak performance gains with huge
memory overheads and their structure encoders heavily rely on domain knowledge.
Under such observation, we tend to investigate the feasibility of a
memory-friendly model with strong generalization capability that could boost
the performance of HTC without prior statistics or label semantics. In this
paper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance
the text representations with only syntactic information of the label
hierarchy. Specifically, we convert the label hierarchy into an unweighted tree
structure, termed coding tree, with the guidance of structural entropy. Then we
design a structure encoder to incorporate hierarchy-aware information in the
coding tree into text representations. Besides the text encoder, HiTIN only
contains a few multi-layer perceptions and linear transformations, which
greatly saves memory. We conduct experiments on three commonly used datasets
and the results demonstrate that HiTIN could achieve better test performance
and less memory consumption than state-of-the-art (SOTA) methods.",None,-1
1f3552ae-840b-4102-9459-630a7870f216,Language Conditioned Traffic Generation,0.827904,"Simulation forms the backbone of modern self-driving development. Simulators
help develop, test, and improve driving systems without putting humans,
vehicles, or their environment at risk. However, simulators face a major
challenge: They rely on realistic, scalable, yet interesting content. While
recent advances in rendering and scene reconstruction make great strides in
creating static scene assets, modeling their layout, dynamics, and behaviors
remains challenging. In this work, we turn to language as a source of
supervision for dynamic traffic scene generation. Our model, LCTGen, combines a
large language model with a transformer-based decoder architecture that selects
likely map locations from a dataset of maps, and produces an initial traffic
distribution, as well as the dynamics of each vehicle. LCTGen outperforms prior
work in both unconditional and conditional traffic scene generation in terms of
realism and fidelity. Code and video will be available at
https://ariostgx.github.io/lctgen.",None,-1
f9d4a823-335d-4ecb-8a03-4eaa93f2d323,Cost-Efficient Prompt Engineering for Unsupervised Entity Resolution,0.819605,"Entity Resolution (ER) is the problem of semi-automatically determining when
two entities refer to the same underlying entity, with applications ranging
from healthcare to e-commerce. Traditional ER solutions required considerable
manual expertise, including domain-specific feature engineering, as well as
identification and curation of training data. Recently released large language
models (LLMs) provide an opportunity to make ER more seamless and
domain-independent. However, it is also well known that LLMs can pose risks,
and that the quality of their outputs can depend on how prompts are engineered.
Unfortunately, a systematic experimental study on the effects of different
prompting methods for addressing unsupervised ER, using LLMs like ChatGPT, has
been lacking thus far. This paper aims to address this gap by conducting such a
study. We consider some relatively simple and cost-efficient ER prompt
engineering methods and apply them to ER on two real-world datasets widely used
in the community. We use an extensive set of experimental results to show that
an LLM like GPT3.5 is viable for high-performing unsupervised ER, and
interestingly, that more complicated and detailed (and hence, expensive)
prompting methods do not necessarily outperform simpler approaches. We provide
brief discussions on qualitative and error analysis, including a study of the
inter-consistency of different prompting methods to determine whether they
yield stable outputs. Finally, we consider some limitations of LLMs when
applied to ER.",None,-1
853c6af8-11c2-44a6-9ba5-8ad603a04980,What's in a Name? Beyond Class Indices for Image Recognition,0.114953,"Existing machine learning models demonstrate excellent performance in image
object recognition after training on a large-scale dataset under full
supervision. However, these models only learn to map an image to a predefined
class index, without revealing the actual semantic meaning of the object in the
image. In contrast, vision-language models like CLIP are able to assign
semantic class names to unseen objects in a `zero-shot' manner, although they
still rely on a predefined set of candidate names at test time. In this paper,
we reconsider the recognition problem and task a vision-language model to
assign class names to images given only a large and essentially unconstrained
vocabulary of categories as prior information. We use non-parametric methods to
establish relationships between images which allow the model to automatically
narrow down the set of possible candidate names. Specifically, we propose
iteratively clustering the data and voting on class names within them, showing
that this enables a roughly 50\% improvement over the baseline on ImageNet.
Furthermore, we tackle this problem both in unsupervised and partially
supervised settings, as well as with a coarse-grained and fine-grained search
space as the unconstrained dictionary.",None,-1
2fba1323-a832-493e-af1f-9493cd4b7bc1,Eventual Discounting Temporal Logic Counterfactual Experience Replay,0.111705,"Linear temporal logic (LTL) offers a simplified way of specifying tasks for
policy optimization that may otherwise be difficult to describe with scalar
reward functions. However, the standard RL framework can be too myopic to find
maximally LTL satisfying policies. This paper makes two contributions. First,
we develop a new value-function based proxy, using a technique we call eventual
discounting, under which one can find policies that satisfy the LTL
specification with highest achievable probability. Second, we develop a new
experience replay method for generating off-policy data from on-policy rollouts
via counterfactual reasoning on different ways of satisfying the LTL
specification. Our experiments, conducted in both discrete and continuous
state-action spaces, confirm the effectiveness of our counterfactual experience
replay approach.",None,-1
1fbd5caf-cefe-4e2a-b6d9-af6b4d8f5763,Action valuation of on- and off-ball soccer players based on multi-agent deep reinforcement learning,0.565402,"Analysis of invasive sports such as soccer is challenging because the game
situation changes continuously in time and space, and multiple agents
individually recognize the game situation and make decisions. Previous studies
using deep reinforcement learning have often considered teams as a single agent
and valued the teams and players who hold the ball in each discrete event. Then
it was challenging to value the actions of multiple players, including players
far from the ball, in a spatiotemporally continuous state space. In this paper,
we propose a method of valuing possible actions for on- and off-ball soccer
players in a single holistic framework based on multi-agent deep reinforcement
learning. We consider a discrete action space in a continuous state space that
mimics that of Google research football and leverages supervised learning for
actions in reinforcement learning. In the experiment, we analyzed the
relationships with conventional indicators, season goals, and game ratings by
experts, and showed the effectiveness of the proposed method. Our approach can
assess how multiple players move continuously throughout the game, which is
difficult to be discretized or labeled but vital for teamwork, scouting, and
fan engagement.",None,-1
a6be6fe8-bfca-4fe2-955e-1fb543a1a907,CONTAIN: A Community-based Algorithm for Network Immunization,0.482194,"Network immunization is an automated task in the field of network analysis
that involves protecting a network (modeled as a graph) from being infected by
an undesired arbitrary diffusion. In this article, we consider the spread of
harmful content in social networks, and we propose CONTAIN, a novel
COmmuNiTy-based Algorithm for network ImmuNization. Our solution uses the
network information to (1) detect harmful content spreaders, and (2) generate
partitions and rank them for immunization using the subgraphs induced by each
spreader, i.e., employing CONTAIN. The experimental results obtained on
real-world datasets show that CONTAIN outperforms state-of-the-art solutions,
i.e., NetShield and SparseShield, by immunizing the network in fewer
iterations, thus, converging significantly faster than the state-of-the-art
algorithms. We also compared our solution in terms of scalability with the
state-of-the-art tree-based mitigation algorithm MCWDST, as well as with
NetShield and SparseShield. We can conclude that our solution outperforms
MCWDST and NetShield.",None,-1
d0b8f626-cd59-43a7-b2f0-a06160eb57bf,Retrosynthetic Planning with Dual Value Networks,0.638891,"Retrosynthesis, which aims to find a route to synthesize a target molecule
from commercially available starting materials, is a critical task in drug
discovery and materials design. Recently, the combination of ML-based
single-step reaction predictors with multi-step planners has led to promising
results. However, the single-step predictors are mostly trained offline to
optimize the single-step accuracy, without considering complete routes. Here,
we leverage reinforcement learning (RL) to improve the single-step predictor,
by using a tree-shaped MDP to optimize complete routes. Specifically, we
propose a novel online training algorithm, called Planning with Dual Value
Networks (PDVN), which alternates between the planning phase and updating
phase. In PDVN, we construct two separate value networks to predict the
synthesizability and cost of molecules, respectively. To maintain the
single-step accuracy, we design a two-branch network structure for the
single-step predictor. On the widely-used USPTO dataset, our PDVN algorithm
improves the search success rate of existing multi-step planners (e.g.,
increasing the success rate from 85.79% to 98.95% for Retro*, and reducing the
number of model calls by half while solving 99.47% molecules for RetroGraph).
Additionally, PDVN helps find shorter synthesis routes (e.g., reducing the
average route length from 5.76 to 4.83 for Retro*, and from 5.63 to 4.78 for
RetroGraph). Our code is available at \url{https://github.com/DiXue98/PDVN}.",None,-1
eb26b3e1-30b4-4518-9c64-2a3966b0bfe6,Noise-Robust Dense Retrieval via Contrastive Alignment Post Training,0.103225,"The success of contextual word representations and advances in neural
information retrieval have made dense vector-based retrieval a standard
approach for passage and document ranking. While effective and efficient,
dual-encoders are brittle to variations in query distributions and noisy
queries. Data augmentation can make models more robust but introduces overhead
to training set generation and requires retraining and index regeneration. We
present Contrastive Alignment POst Training (CAPOT), a highly efficient
finetuning method that improves model robustness without requiring index
regeneration, the training set optimization, or alteration. CAPOT enables
robust retrieval by freezing the document encoder while the query encoder
learns to align noisy queries with their unaltered root. We evaluate CAPOT
noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,
finding CAPOT has a similar impact as data augmentation with none of its
overhead.",None,-1
6314f4b5-cdad-47f1-9985-21bfd193da6e,Multilingual Sentence Transformer as A Multilingual Word Aligner,0.41291,"Multilingual pretrained language models (mPLMs) have shown their
effectiveness in multilingual word alignment induction. However, these methods
usually start from mBERT or XLM-R. In this paper, we investigate whether
multilingual sentence Transformer LaBSE is a strong multilingual word aligner.
This idea is non-trivial as LaBSE is trained to learn language-agnostic
sentence-level embeddings, while the alignment extraction task requires the
more fine-grained word-level embeddings to be language-agnostic. We demonstrate
that the vanilla LaBSE outperforms other mPLMs currently used in the alignment
task, and then propose to finetune LaBSE on parallel corpus for further
improvement. Experiment results on seven language pairs show that our best
aligner outperforms previous state-of-the-art models of all varieties. In
addition, our aligner supports different language pairs in a single model, and
even achieves new state-of-the-art on zero-shot language pairs that does not
appear in the finetuning process.",None,-1
d6454c99-8b5f-4e18-94c0-31e0c162c2e0,EtiCor: Corpus for Analyzing LLMs for Etiquettes,0.999076,"Etiquettes are an essential ingredient of day-to-day interactions among
people. Moreover, etiquettes are region-specific, and etiquettes in one region
might contradict those in other regions. In this paper, we propose EtiCor, an
Etiquettes Corpus, having texts about social norms from five different regions
across the globe. The corpus provides a test bed for evaluating LLMs for
knowledge and understanding of region-specific etiquettes. Additionally, we
propose the task of Etiquette Sensitivity. We experiment with state-of-the-art
LLMs (Delphi, Falcon40B, and GPT-3.5). Initial results indicate that LLMs,
mostly fail to understand etiquettes from regions from non-Western world.",None,-1
13996ecd-c98c-4636-ae8f-71b939ea7b21,Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications,0.95875,"In this work, we address the NER problem by splitting it into two logical
sub-tasks: (1) Span Detection which simply extracts entity mention spans
irrespective of entity type; (2) Span Classification which classifies the spans
into their entity types. Further, we formulate both sub-tasks as
question-answering (QA) problems and produce two leaner models which can be
optimized separately for each sub-task. Experiments with four cross-domain
datasets demonstrate that this two-step approach is both effective and time
efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17
and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all
cases, it achieves a significant reduction in training time compared to its QA
baseline counterpart. The effectiveness of our system stems from fine-tuning
the BERT model twice, separately for span detection and classification. The
source code can be found at https://github.com/c3sr/split-ner.",None,-1
27c2d2ad-b15f-4e55-910e-134412b005b6,Longformer: Longitudinal Transformer for Alzheimer's Disease Classification with Structural MRIs,0.383848,"Structural magnetic resonance imaging (sMRI) is widely used for brain
neurological disease diagnosis; while longitudinal MRIs are often collected to
monitor and capture disease progression, as clinically used in diagnosing
Alzheimer's disease (AD). However, most current methods neglect AD's
progressive nature and only take a single sMRI for recognizing AD. In this
paper, we consider the problem of leveraging the longitudinal MRIs of a subject
for AD identification. To capture longitudinal changes in sMRIs, we propose a
novel model Longformer, a spatiotemporal transformer network that performs
attention mechanisms spatially on sMRIs at each time point and integrates brain
region features over time to obtain longitudinal embeddings for classification.
Our Longformer achieves state-of-the-art performance on two binary
classification tasks of separating different stages of AD using the ADNI
dataset. Our source code is available at https://github.com/Qybc/LongFormer.",None,-1
068285f9-b606-4251-841e-37d0b924b6f0,Towards Multi-Layered 3D Garments Animation,0.491815,"Mimicking realistic dynamics in 3D garment animations is a challenging task
due to the complex nature of multi-layered garments and the variety of outer
forces involved. Existing approaches mostly focus on single-layered garments
driven by only human bodies and struggle to handle general scenarios. In this
paper, we propose a novel data-driven method, called LayersNet, to model
garment-level animations as particle-wise interactions in a micro physics
system. We improve simulation efficiency by representing garments as
patch-level particles in a two-level structural hierarchy. Moreover, we
introduce a novel Rotation Equivalent Transformation that leverages the
rotation invariance and additivity of physics systems to better model outer
forces. To verify the effectiveness of our approach and bridge the gap between
experimental environments and real-world scenarios, we introduce a new
challenging dataset, D-LAYERS, containing 700K frames of dynamics of 4,900
different combinations of multi-layered garments driven by both human bodies
and randomly sampled wind. Our experiments show that LayersNet achieves
superior performance both quantitatively and qualitatively. We will make the
dataset and code publicly available at
https://mmlab-ntu.github.io/project/layersnet/index.html .",None,-1
5220bcbd-fee7-4643-b7d6-11fb23dd7124,Common Diffusion Noise Schedules and Sample Steps are Flawed,0.99976,"We discover that common diffusion noise schedules do not enforce the last
timestep to have zero signal-to-noise ratio (SNR), and some implementations of
diffusion samplers do not start from the last timestep. Such designs are flawed
and do not reflect the fact that the model is given pure Gaussian noise at
inference, creating a discrepancy between training and inference. We show that
the flawed design causes real problems in existing implementations. In Stable
Diffusion, it severely limits the model to only generate images with medium
brightness and prevents it from generating very bright and dark samples. We
propose a few simple fixes: (1) rescale the noise schedule to enforce zero
terminal SNR; (2) train the model with v prediction; (3) change the sampler to
always start from the last timestep; (4) rescale classifier-free guidance to
prevent over-exposure. These simple changes ensure the diffusion process is
congruent between training and inference and allow the model to generate
samples more faithful to the original data distribution.",None,-1
e9e82884-f3d3-42fa-a5c4-80fdb03e572c,Do Differences in Values Influence Disagreements in Online Discussions?,0.896703,"Disagreements are common in online discussions. Disagreement may foster
collaboration and improve the quality of a discussion under some conditions.
Although there exist methods for recognizing disagreement, a deeper
understanding of factors that influence disagreement is lacking in the
literature. We investigate a hypothesis that differences in personal values are
indicative of disagreement in online discussions. We show how state-of-the-art
models can be used for estimating values in online discussions and how the
estimated values can be aggregated into value profiles. We evaluate the
estimated value profiles based on human-annotated agreement labels. We find
that the dissimilarity of value profiles correlates with disagreement in
specific cases. We also find that including value information in agreement
prediction improves performance.",None,-1
a8519837-7ef7-4047-9c14-64883b97cfc3,Large Language Models for Propaganda Detection,0.166213,"The prevalence of propaganda in our digital society poses a challenge to
societal harmony and the dissemination of truth. Detecting propaganda through
NLP in text is challenging due to subtle manipulation techniques and contextual
dependencies. To address this issue, we investigate the effectiveness of modern
Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.
We conduct experiments using the SemEval-2020 task 11 dataset, which features
news articles labeled with 14 propaganda techniques as a multi-label
classification problem. Five variations of GPT-3 and GPT-4 are employed,
incorporating various prompt engineering and fine-tuning strategies across the
different models. We evaluate the models' performance by assessing metrics such
as $F1$ score, $Precision$, and $Recall$, comparing the results with the
current state-of-the-art approach using RoBERTa. Our findings demonstrate that
GPT-4 achieves comparable results to the current state-of-the-art. Further,
this study analyzes the potential and challenges of LLMs in complex tasks like
propaganda detection.",None,-1
8cd0d4c8-f878-443f-8f81-ede2cb162b3e,Understanding Catastrophic Forgetting in Language Models via Implicit Inference,0.166813,"We lack a systematic understanding of the effects of fine-tuning (via methods
such as instruction-tuning or reinforcement learning from human feedback),
particularly on tasks outside the narrow fine-tuning distribution. In a
simplified scenario, we demonstrate that improving performance on tasks within
the fine-tuning data distribution comes at the expense of capabilities on other
tasks. We hypothesize that language models implicitly infer the task of the
prompt and that fine-tuning skews this inference towards tasks in the
fine-tuning distribution. To test this, we propose Conjugate Prompting, which
artificially makes the task look farther from the fine-tuning distribution
while requiring the same capability, and we find that this recovers some of the
pretraining capabilities in our synthetic setup. Since real-world fine-tuning
distributions are predominantly English, we apply conjugate prompting to
recover pretrained capabilities in LLMs by simply translating the prompts to
different languages. This allows us to recover in-context learning abilities
lost via instruction tuning, natural reasoning capability lost during code
fine-tuning, and, more concerningly, harmful content generation suppressed by
safety fine-tuning in chatbots like ChatGPT.",None,-1
54133f17-a4ce-48c5-accb-9965a7ff4f4f,Towards Integration of Discriminability and Robustness for Document-Level Relation Extraction,0.775232,"Document-level relation extraction (DocRE) predicts relations for entity
pairs that rely on long-range context-dependent reasoning in a document. As a
typical multi-label classification problem, DocRE faces the challenge of
effectively distinguishing a small set of positive relations from the majority
of negative ones. This challenge becomes even more difficult to overcome when
there exists a significant number of annotation errors in the dataset. In this
work, we aim to achieve better integration of both the discriminability and
robustness for the DocRE problem. Specifically, we first design an effective
loss function to endow high discriminability to both probabilistic outputs and
internal representations. We innovatively customize entropy minimization and
supervised contrastive learning for the challenging multi-label and long-tailed
learning problems. To ameliorate the impact of label errors, we equipped our
method with a novel negative label sampling strategy to strengthen the model
robustness. In addition, we introduce two new data regimes to mimic more
realistic scenarios with annotation errors and evaluate our sampling strategy.
Experimental results verify the effectiveness of each component and show that
our method achieves new state-of-the-art results on the DocRED dataset, its
recently cleaned version, Re-DocRED, and the proposed data regimes.",None,-1
d075da2d-23d7-44c3-a9ff-4019221d3e21,Learnable Ophthalmology SAM,0.607272,"Segmentation is vital for ophthalmology image analysis. But its various modal
images hinder most of the existing segmentation algorithms applications, as
they rely on training based on a large number of labels or hold weak
generalization ability. Based on Segment Anything (SAM), we propose a simple
but effective learnable prompt layer suitable for multiple target segmentation
in ophthalmology multi-modal images, named Learnable Ophthalmology Segment
Anything (SAM). The learnable prompt layer learns medical prior knowledge from
each transformer layer. During training, we only train the prompt layer and
task head based on a one-shot mechanism. We demonstrate the effectiveness of
our thought based on four medical segmentation tasks based on nine publicly
available datasets. Moreover, we only provide a new improvement thought for
applying the existing fundamental CV models in the medical field. Our codes are
available at \href{https://github.com/Qsingle/LearnablePromptSAM}{website}.",None,-1
6ab94c89-4fae-4afc-b911-4467e590f904,Weakly-supervised Single-view Image Relighting,0.382014,"We present a learning-based approach to relight a single image of Lambertian
and low-frequency specular objects. Our method enables inserting objects from
photographs into new scenes and relighting them under the new environment
lighting, which is essential for AR applications. To relight the object, we
solve both inverse rendering and re-rendering. To resolve the ill-posed inverse
rendering, we propose a weakly-supervised method by a low-rank constraint. To
facilitate the weakly-supervised training, we contribute Relit, a large-scale
(750K images) dataset of videos with aligned objects under changing
illuminations. For re-rendering, we propose a differentiable specular rendering
layer to render low-frequency non-Lambertian materials under various
illuminations of spherical harmonics. The whole pipeline is end-to-end and
efficient, allowing for a mobile app implementation of AR object insertion.
Extensive evaluations demonstrate that our method achieves state-of-the-art
performance. Project page: https://renjiaoyi.github.io/relighting/.",None,-1
b6b0f79d-4338-42b6-ac42-a7c751119096,Beyond Sentiment: Leveraging Topic Metrics for Political Stance Classification,0.504005,"Sentiment analysis, widely critiqued for capturing merely the overall tone of
a corpus, falls short in accurately reflecting the latent structures and
political stances within texts. This study introduces topic metrics, dummy
variables converted from extracted topics, as both an alternative and
complement to sentiment metrics in stance classification. By employing three
datasets identified by Bestvater and Monroe (2023), this study demonstrates
BERTopic's proficiency in extracting coherent topics and the effectiveness of
topic metrics in stance classification. The experiment results show that
BERTopic improves coherence scores by 17.07% to 54.20% when compared to
traditional approaches such as Dirichlet Allocation (LDA) and Non-negative
Matrix Factorization (NMF), prevalent in earlier political science research.
Additionally, our results indicate topic metrics outperform sentiment metrics
in stance classification, increasing performance by as much as 18.95%. Our
findings suggest topic metrics are especially effective for context-rich texts
and corpus where stance and sentiment correlations are weak. The combination of
sentiment and topic metrics achieve an optimal performance in most of the
scenarios and can further address the limitations of relying solely on
sentiment as well as the low coherence score of topic metrics.",None,-1
b97e26f0-3ce0-41c6-8b45-a0d27ad7caa9,Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series,0.564197,"Learning accurate predictive models of real-world dynamic phenomena (e.g.,
climate, biological) remains a challenging task. One key issue is that the data
generated by both natural and artificial processes often comprise time series
that are irregularly sampled and/or contain missing observations. In this work,
we propose the Neural Continuous-Discrete State Space Model (NCDSSM) for
continuous-time modeling of time series through discrete-time observations.
NCDSSM employs auxiliary variables to disentangle recognition from dynamics,
thus requiring amortized inference only for the auxiliary variables. Leveraging
techniques from continuous-discrete filtering theory, we demonstrate how to
perform accurate Bayesian inference for the dynamic states. We propose three
flexible parameterizations of the latent dynamics and an efficient training
objective that marginalizes the dynamic states during inference. Empirical
results on multiple benchmark datasets across various domains show improved
imputation and forecasting performance of NCDSSM over existing models.",None,-1
50f1a2e7-80cf-4e93-baa2-2d2e9d4de2a1,Zero-shot Temporal Relation Extraction with ChatGPT,0.999708,"The goal of temporal relation extraction is to infer the temporal relation
between two events in the document. Supervised models are dominant in this
task. In this work, we investigate ChatGPT's ability on zero-shot temporal
relation extraction. We designed three different prompt techniques to break
down the task and evaluate ChatGPT. Our experiments show that ChatGPT's
performance has a large gap with that of supervised methods and can heavily
rely on the design of prompts. We further demonstrate that ChatGPT can infer
more small relation classes correctly than supervised methods. The current
shortcomings of ChatGPT on temporal relation extraction are also discussed in
this paper. We found that ChatGPT cannot keep consistency during temporal
inference and it fails in actively long-dependency temporal inference.",None,-1
aa43867f-1a84-4b9a-89c8-9a878893a128,Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering,0.40027,"Whereas the recent emergence of large language models (LLMs) like ChatGPT has
exhibited impressive general performance, it still has a large gap with
fully-supervised models on specific tasks such as multi-span question
answering. Previous researches found that in-context learning is an effective
approach to exploiting LLM, by using a few task-related labeled data as
demonstration examples to construct a few-shot prompt for answering new
questions. A popular implementation is to concatenate a few questions and their
correct answers through simple templates, informing LLM of the desired output.
In this paper, we propose a novel way of employing labeled data such that it
also informs LLM of some undesired output, by extending demonstration examples
with feedback about answers predicted by an off-the-shelf model, e.g., correct,
incorrect, or incomplete. Experiments on three multi-span question answering
datasets as well as a keyphrase extraction dataset show that our new prompting
strategy consistently improves LLM's in-context learning performance.",None,-1
6c12fd68-a44f-44c1-9b84-764f5e93496b,Self-supervised Pre-training with Masked Shape Prediction for 3D Scene Understanding,0.522717,"Masked signal modeling has greatly advanced self-supervised pre-training for
language and 2D images. However, it is still not fully explored in 3D scene
understanding. Thus, this paper introduces Masked Shape Prediction (MSP), a new
framework to conduct masked signal modeling in 3D scenes. MSP uses the
essential 3D semantic cue, i.e., geometric shape, as the prediction target for
masked points. The context-enhanced shape target consisting of explicit shape
context and implicit deep shape feature is proposed to facilitate exploiting
contextual cues in shape prediction. Meanwhile, the pre-training architecture
in MSP is carefully designed to alleviate the masked shape leakage from point
coordinates. Experiments on multiple 3D understanding tasks on both indoor and
outdoor datasets demonstrate the effectiveness of MSP in learning good feature
representations to consistently boost downstream performance.",None,-1
38bc220f-2d1e-4281-b674-c2a9ea64a44b,Trust and Transparency in Recommender Systems,0.100454,"Trust is long recognized to be an important factor in Recommender Systems
(RS). However, there are different perspectives on trust and different ways to
evaluate it. Moreover, a link between trust and transparency is often assumed
but not always further investigated. In this paper we first go through
different understandings and measurements of trust in the AI and RS community,
such as demonstrated and perceived trust. We then review the relationsships
between trust and transparency, as well as mental models, and investigate
different strategies to achieve transparency in RS such as explanation,
exploration and exploranation (i.e., a combination of exploration and
explanation). We identify a need for further studies to explore these concepts
as well as the relationships between them.",None,-1
b0cf899d-fb75-4ae7-9b85-79447766e5a4,LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise,0.285635,"Facial expression recognition (FER) remains a challenging task due to the
ambiguity of expressions. The derived noisy labels significantly harm the
performance in real-world scenarios. To address this issue, we present a new
FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks
to mitigate the impact of label noise from two perspectives. Firstly, LA-Net
uses landmark information to suppress the uncertainty in expression space and
constructs the label distribution of each sample by neighborhood aggregation,
which in turn improves the quality of training supervision. Secondly, the model
incorporates landmark information into expression representations using the
devised expression-landmark contrastive loss. The enhanced expression feature
extractor can be less susceptible to label noise. Our method can be integrated
with any deep neural network for better training supervision without
introducing extra inference costs. We conduct extensive experiments on both
in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net
achieves state-of-the-art performance.",None,-1
290ceeb7-cb28-4e63-8087-7688610d5c7d,Efficient Encoders for Streaming Sequence Tagging,0.0996314,"A naive application of state-of-the-art bidirectional encoders for streaming
sequence tagging would require encoding each token from scratch for each new
token in an incremental streaming input (like transcribed speech). The lack of
re-usability of previous computation leads to a higher number of Floating Point
Operations (or FLOPs) and higher number of unnecessary label flips. Increased
FLOPs consequently lead to higher wall-clock time and increased label flipping
leads to poorer streaming performance. In this work, we present a Hybrid
Encoder with Adaptive Restart (HEAR) that addresses these issues while
maintaining the performance of bidirectional encoders over the offline (or
complete) inputs while improving performance on streaming (or incomplete)
inputs. HEAR has a Hybrid unidirectional-bidirectional encoder architecture to
perform sequence tagging, along with an Adaptive Restart Module (ARM) to
selectively guide the restart of bidirectional portion of the encoder. Across
four sequence tagging tasks, HEAR offers FLOP savings in streaming settings
upto 71.1% and also outperforms bidirectional encoders for streaming
predictions by upto +10% streaming exact match.",None,-1
d8c1fc19-fa5c-4411-b2ae-aca60236ff37,Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks,0.156612,"In-context learning (ICL) ability has emerged with the increasing scale of
large language models (LLMs), enabling them to learn input-label mappings from
demonstrations and perform well on downstream tasks. However, under the
standard ICL setting, LLMs may sometimes neglect query-related information in
demonstrations, leading to incorrect predictions. To address this limitation,
we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to
explore the power of ICL in open-domain question answering, an important form
in knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract
query-related knowledge from demonstrations, then concatenates the knowledge to
prompt LLMs in a more explicit way. Furthermore, we track the source of this
knowledge to identify specific examples, and introduce a Hint-related Example
Retriever (HER) to select informative examples for enhanced demonstrations. We
evaluate HICL with HER on 3 open-domain QA benchmarks, and observe average
performance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EM
score and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.",None,-1
2ee6c656-8717-4888-b2be-e6f75a26ee5b,An Extensible Multimodal Multi-task Object Dataset with Materials,0.0397434,"We present EMMa, an Extensible, Multimodal dataset of Amazon product listings
that contains rich Material annotations. It contains more than 2.8 million
objects, each with image(s), listing text, mass, price, product ratings, and
position in Amazon's product-category taxonomy. We also design a comprehensive
taxonomy of 182 physical materials (e.g., Plastic $\rightarrow$ Thermoplastic
$\rightarrow$ Acrylic). Objects are annotated with one or more materials from
this taxonomy. With the numerous attributes available for each object, we
develop a Smart Labeling framework to quickly add new binary labels to all
objects with very little manual labeling effort, making the dataset extensible.
Each object attribute in our dataset can be included in either the model inputs
or outputs, leading to combinatorial possibilities in task configurations. For
example, we can train a model to predict the object category from the listing
text, or the mass and price from the product listing image. EMMa offers a new
benchmark for multi-task learning in computer vision and NLP, and allows
practitioners to efficiently add new tasks and object attributes at scale.",None,-1
a23c9fa0-8f64-4e82-8215-ca0444fa0249,Fairness for Workers Who Pull the Arms: An Index Based Policy for Allocation of Restless Bandit Tasks,0.649635,"Motivated by applications such as machine repair, project monitoring, and
anti-poaching patrol scheduling, we study intervention planning of stochastic
processes under resource constraints. This planning problem has previously been
modeled as restless multi-armed bandits (RMAB), where each arm is an
intervention-dependent Markov Decision Process. However, the existing
literature assumes all intervention resources belong to a single uniform pool,
limiting their applicability to real-world settings where interventions are
carried out by a set of workers, each with their own costs, budgets, and
intervention effects. In this work, we consider a novel RMAB setting, called
multi-worker restless bandits (MWRMAB) with heterogeneous workers. The goal is
to plan an intervention schedule that maximizes the expected reward while
satisfying budget constraints on each worker as well as fairness in terms of
the load assigned to each worker. Our contributions are two-fold: (1) we
provide a multi-worker extension of the Whittle index to tackle heterogeneous
costs and per-worker budget and (2) we develop an index-based scheduling policy
to achieve fairness. Further, we evaluate our method on various cost structures
and show that our method significantly outperforms other baselines in terms of
fairness without sacrificing much in reward accumulated.",None,-1
28358f74-1d6c-4143-829b-4e089975411c,A Comparison of Tiny-nerf versus Spatial Representations for 3d Reconstruction,0.0417631,"Neural rendering has emerged as a powerful paradigm for synthesizing images,
offering many benefits over classical rendering by using neural networks to
reconstruct surfaces, represent shapes, and synthesize novel views, either for
objects or scenes. In this neural rendering, the environment is encoded into a
neural network. We believe that these new representations can be used to codify
the scene for a mobile robot. Therefore, in this work, we perform a comparison
between a trending neural rendering, called tiny-NeRF, and other volume
representations that are commonly used as maps in robotics, such as voxel maps,
point clouds, and triangular meshes. The target is to know the advantages and
disadvantages of neural representations in the robotics context. The comparison
is made in terms of spatial complexity and processing time to obtain a model.
Experiments show that tiny-NeRF requires three times less memory space compared
to other representations. In terms of processing time, tiny-NeRF takes about
six times more to compute the model.",None,-1
007692fb-d7b3-4298-a7b6-2d9d78dca462,SST: A Simplified Swin Transformer-based Model for Taxi Destination Prediction based on Existing Trajectory,0.648752,"Accurately predicting the destination of taxi trajectories can have various
benefits for intelligent location-based services. One potential method to
accomplish this prediction is by converting the taxi trajectory into a
two-dimensional grid and using computer vision techniques. While the Swin
Transformer is an innovative computer vision architecture with demonstrated
success in vision downstream tasks, it is not commonly used to solve real-world
trajectory problems. In this paper, we propose a simplified Swin Transformer
(SST) structure that does not use the shifted window idea in the traditional
Swin Transformer, as trajectory data is consecutive in nature. Our
comprehensive experiments, based on real trajectory data, demonstrate that SST
can achieve higher accuracy compared to state-of-the-art methods.",None,-1
af15cfdb-4aad-4699-b87f-7e6631675b38,MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies,0.133785,"Autoregressive language models are trained by minimizing the cross-entropy of
the model distribution Q relative to the data distribution P -- that is,
minimizing the forward cross-entropy, which is equivalent to maximum likelihood
estimation (MLE). We have observed that models trained in this way may
""over-generalize"", in the sense that they produce non-human-like text.
Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P
relative to Q, is a better reflection of how a human would evaluate text
generated by a model. Hence, we propose learning with MixCE, an objective that
mixes the forward and reverse cross-entropies. We evaluate models trained with
this objective on synthetic data settings (where P is known) and real data, and
show that the resulting models yield better generated text without complex
decoding strategies. Our code and models are publicly available at
https://github.com/bloomberg/mixce-acl2023",None,-1
5b5f1cf8-8645-4a0c-9873-248e783e7056,Deep Learning-Enabled Sleep Staging From Vital Signs and Activity Measured Using a Near-Infrared Video Camera,0.713048,"Conventional sleep monitoring is time-consuming, expensive and uncomfortable,
requiring a large number of contact sensors to be attached to the patient.
Video data is commonly recorded as part of a sleep laboratory assessment. If
accurate sleep staging could be achieved solely from video, this would overcome
many of the problems of traditional methods. In this work we use heart rate,
breathing rate and activity measures, all derived from a near-infrared video
camera, to perform sleep stage classification. We use a deep transfer learning
approach to overcome data scarcity, by using an existing contact-sensor dataset
to learn effective representations from the heart and breathing rate time
series. Using a dataset of 50 healthy volunteers, we achieve an accuracy of
73.4\% and a Cohen's kappa of 0.61 in four-class sleep stage classification,
establishing a new state-of-the-art for video-based sleep staging.",None,-1
d495593d-1c4f-4ae8-927b-82d3c1f02c06,Video-Helpful Multimodal Machine Translation,0.152282,"Existing multimodal machine translation (MMT) datasets consist of images and
video captions or instructional video subtitles, which rarely contain
linguistic ambiguity, making visual information ineffective in generating
appropriate translations. Recent work has constructed an ambiguous subtitles
dataset to alleviate this problem but is still limited to the problem that
videos do not necessarily contribute to disambiguation. We introduce EVA
(Extensive training set and Video-helpful evaluation set for Ambiguous
subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En)
parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs,
and corresponding video clips collected from movies and TV episodes. In
addition to the extensive training set, EVA contains a video-helpful evaluation
set in which subtitles are ambiguous, and videos are guaranteed helpful for
disambiguation. Furthermore, we propose SAFA, an MMT model based on the
Selective Attention model with two novel methods: Frame attention loss and
Ambiguity augmentation, aiming to use videos in EVA for disambiguation fully.
Experiments on EVA show that visual information and the proposed methods can
boost translation performance, and our model performs significantly better than
existing MMT models. The EVA dataset and the SAFA model are available at:
https://github.com/ku-nlp/video-helpful-MMT.git.",None,-1
3cc9358c-be8c-46fa-9ce0-fed3e151f358,DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation,0.482733,"Empathy is a crucial factor in open-domain conversations, which naturally
shows one's caring and understanding to others. Though several methods have
been proposed to generate empathetic responses, existing works often lead to
monotonous empathy that refers to generic and safe expressions. In this paper,
we propose to use explicit control to guide the empathy expression and design a
framework DiffusEmp based on conditional diffusion language model to unify the
utilization of dialogue context and attribute-oriented control signals.
Specifically, communication mechanism, intent, and semantic frame are imported
as multi-grained signals that control the empathy realization from coarse to
fine levels. We then design a specific masking strategy to reflect the
relationship between multi-grained signals and response tokens, and integrate
it into the diffusion model to influence the generative process. Experimental
results on a benchmark dataset EmpatheticDialogue show that our framework
outperforms competitive baselines in terms of controllability, informativeness,
and diversity without the loss of context-relatedness.",None,-1
7e46810d-9dac-452f-9c28-a959836c45e3,Towards Flow Graph Prediction of Open-Domain Procedural Texts,0.0453723,"Machine comprehension of procedural texts is essential for reasoning about
the steps and automating the procedures. However, this requires identifying
entities within a text and resolving the relationships between the entities.
Previous work focused on the cooking domain and proposed a framework to convert
a recipe text into a flow graph (FG) representation. In this work, we propose a
framework based on the recipe FG for flow graph prediction of open-domain
procedural texts. To investigate flow graph prediction performance in
non-cooking domains, we introduce the wikiHow-FG corpus from articles on
wikiHow, a website of how-to instruction articles. In experiments, we consider
using the existing recipe corpus and performing domain adaptation from the
cooking to the target domain. Experimental results show that the domain
adaptation models achieve higher performance than those trained only on the
cooking or target domain data.",None,-1
25d8b8dc-e68f-4911-a80c-288fc3415cd8,Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views,0.208886,"Temporal concept drift refers to the problem of data changing over time. In
NLP, that would entail that language (e.g. new expressions, meaning shifts) and
factual knowledge (e.g. new concepts, updated facts) evolve over time. Focusing
on the latter, we benchmark $11$ pretrained masked language models (MLMs) on a
series of tests designed to evaluate the effect of temporal concept drift, as
it is crucial that widely used language models remain up-to-date with the
ever-evolving factual updates of the real world. Specifically, we provide a
holistic framework that (1) dynamically creates temporal test sets of any time
granularity (e.g. month, quarter, year) of factual data from Wikidata, (2)
constructs fine-grained splits of tests (e.g. updated, new, unchanged facts) to
ensure comprehensive analysis, and (3) evaluates MLMs in three distinct ways
(single-token probing, multi-token generation, MLM scoring). In contrast to
prior work, our framework aims to unveil how robust an MLM is over time and
thus to provide a signal in case it has become outdated, by leveraging multiple
views of evaluation.",None,-1
19bf5e0a-a354-4c50-a4dc-ea4199cf3d02,Semantic Contrastive Bootstrapping for Single-positive Multi-label Recognition,0.287178,"Learning multi-label image recognition with incomplete annotation is gaining
popularity due to its superior performance and significant labor savings when
compared to training with fully labeled datasets. Existing literature mainly
focuses on label completion and co-occurrence learning while facing
difficulties with the most common single-positive label manner. To tackle this
problem, we present a semantic contrastive bootstrapping (Scob) approach to
gradually recover the cross-object relationships by introducing class
activation as semantic guidance. With this learning guidance, we then propose a
recurrent semantic masked transformer to extract iconic object-level
representations and delve into the contrastive learning problems on multi-label
classification tasks. We further propose a bootstrapping framework in an
Expectation-Maximization fashion that iteratively optimizes the network
parameters and refines semantic guidance to alleviate possible disturbance
caused by wrong semantic guidance. Extensive experimental results demonstrate
that the proposed joint learning framework surpasses the state-of-the-art
models by a large margin on four public multi-label image recognition
benchmarks. Codes can be found at https://github.com/iCVTEAM/Scob.",None,-1
496acd16-bf33-402d-ba3c-477802a8963d,Denoising Diffusion Autoencoders are Unified Self-supervised Learners,0.643802,"Inspired by recent advances in diffusion models, which are reminiscent of
denoising autoencoders, we investigate whether they can acquire discriminative
representations for classification via generative pre-training. This paper
shows that the networks in diffusion models, namely denoising diffusion
autoencoders (DDAE), are unified self-supervised learners: by pre-training on
unconditional image generation, DDAE has already learned strongly
linear-separable representations within its intermediate layers without
auxiliary encoders, thus making diffusion pre-training emerge as a general
approach for generative-and-discriminative dual learning. To validate this, we
conduct linear probe and fine-tuning evaluations. Our diffusion-based approach
achieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and
Tiny-ImageNet, respectively, and is comparable to contrastive learning and
masked autoencoders for the first time. Transfer learning from ImageNet also
confirms the suitability of DDAE for Vision Transformers, suggesting the
potential to scale DDAEs as unified foundation models. Code is available at
github.com/FutureXiang/ddae.",None,-1
b0c199f1-e7f5-46c1-8cbb-bf75c3566d77,Generative AI in Mafia-like Game Simulation,0.0433638,"In this research, we explore the efficacy and potential of Generative AI
models, specifically focusing on their application in role-playing simulations
exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's
advanced capabilities, the study aimed to showcase the model's potential in
understanding, decision-making, and interaction during game scenarios.
Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo,
demonstrated GPT-4's enhanced adaptability to the game environment, with
significant improvements in posing relevant questions and forming human-like
responses. However, challenges such as the model;s limitations in bluffing and
predicting opponent moves emerged. Reflections on game development, financial
constraints, and non-verbal limitations of the study were also discussed. The
findings suggest that while GPT-4 exhibits promising advancements over earlier
models, there remains potential for further development, especially in
instilling more human-like attributes in AI.",None,-1
57da0ef6-0a13-4f52-805b-dde9430d980e,NeurOCS: Neural NOCS Supervision for Monocular 3D Object Localization,0.900466,"Monocular 3D object localization in driving scenes is a crucial task, but
challenging due to its ill-posed nature. Estimating 3D coordinates for each
pixel on the object surface holds great potential as it provides dense 2D-3D
geometric constraints for the underlying PnP problem. However, high-quality
ground truth supervision is not available in driving scenes due to sparsity and
various artifacts of Lidar data, as well as the practical infeasibility of
collecting per-instance CAD models. In this work, we present NeurOCS, a
framework that uses instance masks and 3D boxes as input to learn 3D object
shapes by means of differentiable rendering, which further serves as
supervision for learning dense object coordinates. Our approach rests on
insights in learning a category-level shape prior directly from real driving
scenes, while properly handling single-view ambiguities. Furthermore, we study
and make critical design choices to learn object coordinates more effectively
from an object-centric view. Altogether, our framework leads to new
state-of-the-art in monocular 3D localization that ranks 1st on the
KITTI-Object benchmark among published monocular methods.",None,-1
4e76c5f2-5554-45cf-a2ab-ab4703c6b914,Credit Assignment: Challenges and Opportunities in Developing Human-like AI Agents,0.333257,"Temporal credit assignment is crucial for learning and skill development in
natural and artificial intelligence. While computational methods like the TD
approach in reinforcement learning have been proposed, it's unclear if they
accurately represent how humans handle feedback delays. Cognitive models intend
to represent the mental steps by which humans solve problems and perform a
number of tasks, but limited research in cognitive science has addressed the
credit assignment problem in humans and cognitive models. Our research uses a
cognitive model based on a theory of decisions from experience, Instance-Based
Learning Theory (IBLT), to test different credit assignment mechanisms in a
goal-seeking navigation task with varying levels of decision complexity.
Instance-Based Learning (IBL) models simulate the process of making sequential
choices with different credit assignment mechanisms, including a new IBL-TD
model that combines the IBL decision mechanism with the TD approach. We found
that (1) An IBL model that gives equal credit assignment to all decisions is
able to match human performance better than other models, including IBL-TD and
Q-learning; (2) IBL-TD and Q-learning models underperform compared to humans
initially, but eventually, they outperform humans; (3) humans are influenced by
decision complexity, while models are not. Our study provides insights into the
challenges of capturing human behavior and the potential opportunities to use
these models in future AI systems to support human activities.",None,-1
14a5e82e-0607-4fd2-8d4e-1cbde94831c0,Machine Learning Approaches in Agile Manufacturing with Recycled Materials for Sustainability,0.307125,"It is important to develop sustainable processes in materials science and
manufacturing that are environmentally friendly. AI can play a significant role
in decision support here as evident from our earlier research leading to tools
developed using our proposed machine learning based approaches. Such tools
served the purpose of computational estimation and expert systems. This
research addresses environmental sustainability in materials science via
decision support in agile manufacturing using recycled and reclaimed materials.
It is a safe and responsible way to turn a specific waste stream to value-added
products. We propose to use data-driven methods in AI by applying machine
learning models for predictive analysis to guide decision support in
manufacturing. This includes harnessing artificial neural networks to study
parameters affecting heat treatment of materials and impacts on their
properties; deep learning via advances such as convolutional neural networks to
explore grain size detection; and other classifiers such as Random Forests to
analyze phrase fraction detection. Results with all these methods seem
promising to embark on further work, e.g. ANN yields accuracy around 90\% for
predicting micro-structure development as per quench tempering, a heat
treatment process. Future work entails several challenges: investigating
various computer vision models (VGG, ResNet etc.) to find optimal accuracy,
efficiency and robustness adequate for sustainable processes; creating
domain-specific tools using machine learning for decision support in agile
manufacturing; and assessing impacts on sustainability with metrics
incorporating the appropriate use of recycled materials as well as the
effectiveness of developed products. Our work makes impacts on green technology
for smart manufacturing, and is motivated by related work in the highly
interesting realm of AI for materials science.",None,-1
0cb72cf6-6c5d-4d99-a7d4-aecc686c8d67,"Syntax and Semantics Meet in the ""Middle"": Probing the Syntax-Semantics Interface of LMs Through Agentivity",0.417362,"Recent advances in large language models have prompted researchers to examine
their abilities across a variety of linguistic tasks, but little has been done
to investigate how models handle the interactions in meaning across words and
larger syntactic forms -- i.e. phenomena at the intersection of syntax and
semantics. We present the semantic notion of agentivity as a case study for
probing such interactions. We created a novel evaluation dataset by utilitizing
the unique linguistic properties of a subset of optionally transitive English
verbs. This dataset was used to prompt varying sizes of three model classes to
see if they are sensitive to agentivity at the lexical level, and if they can
appropriately employ these word-level priors given a specific syntactic
context. Overall, GPT-3 text-davinci-003 performs extremely well across all
experiments, outperforming all other models tested by far. In fact, the results
are even better correlated with human judgements than both syntactic and
semantic corpus statistics. This suggests that LMs may potentially serve as
more useful tools for linguistic annotation, theory testing, and discovery than
select corpora for certain tasks. Code is available at
https://github.com/lindiatjuatja/lm_sem",None,-1
e2128fc5-d39f-47c0-a711-a847986eecdb,Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations,0.91337,"Causal abstraction is a promising theoretical framework for explainable
artificial intelligence that defines when an interpretable high-level causal
model is a faithful simplification of a low-level deep learning system.
However, existing causal abstraction methods have two major limitations: they
require a brute-force search over alignments between the high-level model and
the low-level one, and they presuppose that variables in the high-level model
will align with disjoint sets of neurons in the low-level one. In this paper,
we present distributed alignment search (DAS), which overcomes these
limitations. In DAS, we find the alignment between high-level and low-level
models using gradient descent rather than conducting a brute-force search, and
we allow individual neurons to play multiple distinct roles by analyzing
representations in non-standard bases-distributed representations. Our
experiments show that DAS can discover internal structure that prior approaches
miss. Overall, DAS removes previous obstacles to conducting causal abstraction
analyses and allows us to find conceptual structure in trained neural nets.",None,-1
75aad73b-3e89-407f-a02e-80ac06ba6a58,NLP Workbench: Efficient and Extensible Integration of State-of-the-art Text Mining Tools,0.120631,"NLP Workbench is a web-based platform for text mining that allows non-expert
users to obtain semantic understanding of large-scale corpora using
state-of-the-art text mining models. The platform is built upon latest
pre-trained models and open source systems from academia that provide semantic
analysis functionalities, including but not limited to entity linking,
sentiment analysis, semantic parsing, and relation extraction. Its extensible
design enables researchers and developers to smoothly replace an existing model
or integrate a new one. To improve efficiency, we employ a microservice
architecture that facilitates allocation of acceleration hardware and
parallelization of computation. This paper presents the architecture of NLP
Workbench and discusses the challenges we faced in designing it. We also
discuss diverse use cases of NLP Workbench and the benefits of using it over
other approaches. The platform is under active development, with its source
code released under the MIT license. A website and a short video demonstrating
our platform are also available.",None,-1
a7e72f2f-147b-4607-b923-231d4d3d62cc,EXnet: Efficient In-context Learning for Data-less Text classification,0.0647183,"Large pre-trained language models (PLMs) have made significant progress in
encoding world knowledge and spawned a new set of learning paradigms including
zero-shot, few-shot, and in-context learning. Many language tasks can be
modeled as a set of prompts (for example, is this text about geography?) and
language models can provide binary answers, i.e., Yes or No. There is evidence
to suggest that the next-word prediction used by many PLMs does not align well
with zero-shot paradigms. Therefore, PLMs are fine-tuned as a
question-answering system. In-context learning extends zero-shot learning by
incorporating prompts and examples, resulting in increased task accuracy. Our
paper presents EXnet, a model specifically designed to perform in-context
learning without any limitations on the number of examples. We argue that
in-context learning is an effective method to increase task accuracy, and
providing examples facilitates cross-task generalization, especially when it
comes to text classification tasks. With extensive experiments, we show that
even our smallest model (15M parameters) generalizes to several unseen
classification tasks and domains.",None,-1
efb4aee2-2d22-408b-811b-96398d069ef6,Comparing Intrinsic Gender Bias Evaluation Measures without using Human Annotated Examples,0.394347,"Numerous types of social biases have been identified in pre-trained language
models (PLMs), and various intrinsic bias evaluation measures have been
proposed for quantifying those social biases. Prior works have relied on human
annotated examples to compare existing intrinsic bias evaluation measures.
However, this approach is not easily adaptable to different languages nor
amenable to large scale evaluations due to the costs and difficulties when
recruiting human annotators. To overcome this limitation, we propose a method
to compare intrinsic gender bias evaluation measures without relying on
human-annotated examples. Specifically, we create multiple bias-controlled
versions of PLMs using varying amounts of male vs. female gendered sentences,
mined automatically from an unannotated corpus using gender-related word lists.
Next, each bias-controlled PLM is evaluated using an intrinsic bias evaluation
measure, and the rank correlation between the computed bias scores and the
gender proportions used to fine-tune the PLMs is computed. Experiments on
multiple corpora and PLMs repeatedly show that the correlations reported by our
proposed method that does not require human annotated examples are comparable
to those computed using human annotated examples in prior work.",None,-1
5b13000a-8097-460e-911e-725786f8b68d,Learning to Paraphrase Sentences to Different Complexity Levels,0.32436,"While sentence simplification is an active research topic in NLP, its
adjacent tasks of sentence complexification and same-level paraphrasing are
not. To train models on all three tasks, we present two new unsupervised
datasets. We compare these datasets, one labeled by a weak classifier and the
other by a rule-based approach, with a single supervised dataset. Using these
three datasets for training, we perform extensive experiments on both
multitasking and prompting strategies. Compared to other systems trained on
unsupervised parallel data, models trained on our weak classifier labeled
dataset achieve state-of-the-art performance on the ASSET simplification
benchmark. Our models also outperform previous work on sentence level
targeting. Finally, we establish how a handful of Large Language Models perform
on these tasks under a zero-shot setting.",None,-1
784a6845-5eb7-4ec8-bd9d-99ee07fe03a2,Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra,0.769243,"To improve the accuracy of color image completion with missing entries, we
present a recovery method based on generalized higher-order scalars. We extend
the traditional second-order matrix model to a more comprehensive higher-order
matrix equivalent, called the ""t-matrix"" model, which incorporates a pixel
neighborhood expansion strategy to characterize the local pixel constraints.
This ""t-matrix"" model is then used to extend some commonly used matrix and
tensor completion algorithms to their higher-order versions. We perform
extensive experiments on various algorithms using simulated data and algorithms
on simulated data and publicly available images and compare their performance.
The results show that our generalized matrix completion model and the
corresponding algorithm compare favorably with their lower-order tensor and
conventional matrix counterparts.",None,-1
eb423407-f085-4f8f-a3a9-7f90b40e9229,PassGPT: Password Modeling and (Guided) Generation with Large Language Models,0.476516,"Large language models (LLMs) successfully model natural language from vast
amounts of text without the need for explicit supervision. In this paper, we
investigate the efficacy of LLMs in modeling passwords. We present PassGPT, a
LLM trained on password leaks for password generation. PassGPT outperforms
existing methods based on generative adversarial networks (GAN) by guessing
twice as many previously unseen passwords. Furthermore, we introduce the
concept of guided password generation, where we leverage PassGPT sampling
procedure to generate passwords matching arbitrary constraints, a feat lacking
in current GAN-based strategies. Lastly, we conduct an in-depth analysis of the
entropy and probability distribution that PassGPT defines over passwords and
discuss their use in enhancing existing password strength estimators.",None,-1
3cced239-ee57-4aa7-88d6-41ef5ddd9501,Adversarial Word Dilution as Text Data Augmentation in Low-Resource Regime,0.285796,"Data augmentation is widely used in text classification, especially in the
low-resource regime where a few examples for each class are available during
training. Despite the success, generating data augmentations as hard positive
examples that may increase their effectiveness is under-explored. This paper
proposes an Adversarial Word Dilution (AWD) method that can generate hard
positive examples as text data augmentations to train the low-resource text
classification model efficiently. Our idea of augmenting the text data is to
dilute the embedding of strong positive words by weighted mixing with
unknown-word embedding, making the augmented inputs hard to be recognized as
positive by the classification model. We adversarially learn the dilution
weights through a constrained min-max optimization process with the guidance of
the labels. Empirical studies on three benchmark datasets show that AWD can
generate more effective data augmentations and outperform the state-of-the-art
text data augmentation methods. The additional analysis demonstrates that the
data augmentations generated by AWD are interpretable and can flexibly extend
to new examples without further training.",None,-1
04e9f0cd-1b54-4fdf-9c4e-5d74ad78823e,Qualitative Prediction of Multi-Agent Spatial Interactions,0.382337,"Deploying service robots in our daily life, whether in restaurants,
warehouses or hospitals, calls for the need to reason on the interactions
happening in dense and dynamic scenes. In this paper, we present and benchmark
three new approaches to model and predict multi-agent interactions in dense
scenes, including the use of an intuitive qualitative representation. The
proposed solutions take into account static and dynamic context to predict
individual interactions. They exploit an input- and a temporal-attention
mechanism, and are tested on medium and long-term time horizons. The first two
approaches integrate different relations from the so-called Qualitative
Trajectory Calculus (QTC) within a state-of-the-art deep neural network to
create a symbol-driven neural architecture for predicting spatial interactions.
The third approach implements a purely data-driven network for motion
prediction, the output of which is post-processed to predict QTC spatial
interactions. Experimental results on a popular robot dataset of challenging
crowded scenarios show that the purely data-driven prediction approach
generally outperforms the other two. The three approaches were further
evaluated on a different but related human scenarios to assess their
generalisation capability.",None,-1
329e1301-0c36-4bd3-8bf5-a9b933c26634,Painter: Teaching Auto-regressive Language Models to Draw Sketches,0.0388647,"Large language models (LLMs) have made tremendous progress in natural
language understanding and they have also been successfully adopted in other
domains such as computer vision, robotics, reinforcement learning, etc. In this
work, we apply LLMs to image generation tasks by directly generating the
virtual brush strokes to paint an image. We present Painter, an LLM that can
convert user prompts in text description format to sketches by generating the
corresponding brush strokes in an auto-regressive way. We construct Painter
based on off-the-shelf LLM that is pre-trained on a large text corpus, by
fine-tuning it on the new task while preserving language understanding
capabilities. We create a dataset of diverse multi-object sketches paired with
textual prompts that covers several object types and tasks. Painter can
generate sketches from text descriptions, remove objects from canvas, and
detect and classify objects in sketches. Although this is an unprecedented
pioneering work in using LLMs for auto-regressive image generation, the results
are very encouraging.",None,-1
d93c20aa-f533-41c2-9907-9081721f7e5a,Improving Prosody for Cross-Speaker Style Transfer by Semi-Supervised Style Extractor and Hierarchical Modeling in Speech Synthesis,0.542959,"Cross-speaker style transfer in speech synthesis aims at transferring a style
from source speaker to synthesized speech of a target speaker's timbre. In most
previous methods, the synthesized fine-grained prosody features often represent
the source speaker's average style, similar to the one-to-many problem(i.e.,
multiple prosody variations correspond to the same text). In response to this
problem, a strength-controlled semi-supervised style extractor is proposed to
disentangle the style from content and timbre, improving the representation and
interpretability of the global style embedding, which can alleviate the
one-to-many mapping and data imbalance problems in prosody prediction. A
hierarchical prosody predictor is proposed to improve prosody modeling. We find
that better style transfer can be achieved by using the source speaker's
prosody features that are easily predicted. Additionally, a
speaker-transfer-wise cycle consistency loss is proposed to assist the model in
learning unseen style-timbre combinations during the training phase.
Experimental results show that the method outperforms the baseline. We provide
a website with audio samples.",None,-1
4f29243a-71a8-4f3e-995e-658031feaf2d,"Point-Query Quadtree for Crowd Counting, Localization, and More",0.823747,"We show that crowd counting can be viewed as a decomposable point querying
process. This formulation enables arbitrary points as input and jointly reasons
whether the points are crowd and where they locate. The querying processing,
however, raises an underlying problem on the number of necessary querying
points. Too few imply underestimation; too many increase computational
overhead. To address this dilemma, we introduce a decomposable structure, i.e.,
the point-query quadtree, and propose a new counting model, termed Point quEry
Transformer (PET). PET implements decomposable point querying via
data-dependent quadtree splitting, where each querying point could split into
four new points when necessary, thus enabling dynamic processing of sparse and
dense regions. Such a querying process yields an intuitive, universal modeling
of crowd as both the input and output are interpretable and steerable. We
demonstrate the applications of PET on a number of crowd-related tasks,
including fully-supervised crowd counting and localization, partial annotation
learning, and point annotation refinement, and also report state-of-the-art
performance. For the first time, we show that a single counting model can
address multiple crowd-related tasks across different learning paradigms. Code
is available at https://github.com/cxliu0/PET.",None,-1
c97a3158-cd04-4c09-abc9-e5c14b0a6523,RSPT: Reconstruct Surroundings and Predict Trajectories for Generalizable Active Object Tracking,0.356836,"Active Object Tracking (AOT) aims to maintain a specific relation between the
tracker and object(s) by autonomously controlling the motion system of a
tracker given observations. AOT has wide-ranging applications, such as in
mobile robots and autonomous driving. However, building a generalizable active
tracker that works robustly across different scenarios remains a challenge,
especially in unstructured environments with cluttered obstacles and diverse
layouts. We argue that constructing a state representation capable of modeling
the geometry structure of the surroundings and the dynamics of the target is
crucial for achieving this goal. To address this challenge, we present RSPT, a
framework that forms a structure-aware motion representation by Reconstructing
the Surroundings and Predicting the target Trajectory. Additionally, we enhance
the generalization of the policy network by training in an asymmetric dueling
mechanism. We evaluate RSPT on various simulated scenarios and show that it
outperforms existing methods in unseen environments, particularly those with
complex obstacles and layouts. We also demonstrate the successful transfer of
RSPT to real-world settings. Project Website:
https://sites.google.com/view/aot-rspt.",None,-1
6a5ff8f3-b15c-4c29-8deb-194f2a8c61fc,Attention Where It Matters: Rethinking Visual Document Understanding with Selective Region Concentration,0.318885,"We propose a novel end-to-end document understanding model called SeRum
(SElective Region Understanding Model) for extracting meaningful information
from document images, including document analysis, retrieval, and office
automation.
  Unlike state-of-the-art approaches that rely on multi-stage technical schemes
and are computationally expensive,
  SeRum converts document image understanding and recognition tasks into a
local decoding process of the visual tokens of interest, using a content-aware
token merge module.
  This mechanism enables the model to pay more attention to regions of interest
generated by the query decoder, improving the model's effectiveness and
speeding up the decoding speed of the generative scheme.
  We also designed several pre-training tasks to enhance the understanding and
local awareness of the model.
  Experimental results demonstrate that SeRum achieves state-of-the-art
performance on document understanding tasks and competitive results on text
spotting tasks.
  SeRum represents a substantial advancement towards enabling efficient and
effective end-to-end document understanding.",None,-1
5347baec-794a-4c12-8281-93a545ac4485,Holistic Network Virtualization and Pervasive Network Intelligence for 6G,0.993416,"In this tutorial paper, we look into the evolution and prospect of network
architecture and propose a novel conceptual architecture for the 6th generation
(6G) networks. The proposed architecture has two key elements, i.e., holistic
network virtualization and pervasive artificial intelligence (AI). The holistic
network virtualization consists of network slicing and digital twin, from the
aspects of service provision and service demand, respectively, to incorporate
service-centric and user-centric networking. The pervasive network intelligence
integrates AI into future networks from the perspectives of networking for AI
and AI for networking, respectively. Building on holistic network
virtualization and pervasive network intelligence, the proposed architecture
can facilitate three types of interplay, i.e., the interplay between digital
twin and network slicing paradigms, between model-driven and data-driven
methods for network management, and between virtualization and AI, to maximize
the flexibility, scalability, adaptivity, and intelligence for 6G networks. We
also identify challenges and open issues related to the proposed architecture.
By providing our vision, we aim to inspire further discussions and developments
on the potential architecture of 6G.",None,-1
26b9ff16-4c17-402c-9758-d21608239665,Explainability is NOT a Game,0.364452,"Explainable artificial intelligence (XAI) aims to help human decision-makers
in understanding complex machine learning (ML) models. One of the hallmarks of
XAI are measures of relative feature importance, which are theoretically
justified through the use of Shapley values. This paper builds on recent work
and offers a simple argument for why Shapley values can provide misleading
measures of relative feature importance, by assigning more importance to
features that are irrelevant for a prediction, and assigning less importance to
features that are relevant for a prediction. The significance of these results
is that they effectively challenge the many proposed uses of measures of
relative feature importance in a fast-growing range of high-stakes application
domains.",None,-1
4c5fd558-11be-4157-a42f-ffe4621bf24f,Extracting Multi-valued Relations from Language Models,0.508716,"The widespread usage of latent language representations via pre-trained
language models (LMs) suggests that they are a promising source of structured
knowledge. However, existing methods focus only on a single object per
subject-relation pair, even though often multiple objects are correct. To
overcome this limitation, we analyze these representations for their potential
to yield materialized multi-object relational knowledge. We formulate the
problem as a rank-then-select task. For ranking candidate objects, we evaluate
existing prompting techniques and propose new ones incorporating domain
knowledge. Among the selection methods, we find that choosing objects with a
likelihood above a learned relation-specific threshold gives a 49.5% F1 score.
Our results highlight the difficulty of employing LMs for the multi-valued
slot-filling task and pave the way for further research on extracting
relational knowledge from latent language representations.",None,-1
3ca5300d-a865-44b1-913e-31ea38400ba5,DocFormerv2: Local Features for Document Understanding,0.962727,"We propose DocFormerv2, a multi-modal transformer for Visual Document
Understanding (VDU). The VDU domain entails understanding documents (beyond
mere OCR predictions) e.g., extracting information from a form, VQA for
documents and other tasks. VDU is challenging as it needs a model to make sense
of multiple modalities (visual, language and spatial) to make a prediction. Our
approach, termed DocFormerv2 is an encoder-decoder transformer which takes as
input - vision, language and spatial features. DocFormerv2 is pre-trained with
unsupervised tasks employed asymmetrically i.e., two novel document tasks on
encoder and one on the auto-regressive decoder. The unsupervised tasks have
been carefully designed to ensure that the pre-training encourages
local-feature alignment between multiple modalities. DocFormerv2 when evaluated
on nine datasets shows state-of-the-art performance over strong baselines e.g.
TabFact (4.3%), InfoVQA (1.4%), FUNSD (1%). Furthermore, to show generalization
capabilities, on three VQA tasks involving scene-text, Doc- Formerv2
outperforms previous comparably-sized models and even does better than much
larger models (such as GIT2, PaLi and Flamingo) on some tasks. Extensive
ablations show that due to its pre-training, DocFormerv2 understands multiple
modalities better than prior-art in VDU.",None,-1
498f1417-a965-4a52-b54a-c75e09054e2d,Explainable and Trustworthy Traffic Sign Detection for Safe Autonomous Driving: An Inductive Logic Programming Approach,0.0789836,"Traffic sign detection is a critical task in the operation of Autonomous
Vehicles (AV), as it ensures the safety of all road users. Current DNN-based
sign classification systems rely on pixel-level features to detect traffic
signs and can be susceptible to adversarial attacks. These attacks involve
small, imperceptible changes to a sign that can cause traditional classifiers
to misidentify the sign. We propose an Inductive Logic Programming (ILP) based
approach for stop sign detection in AVs to address this issue. This method
utilises high-level features of a sign, such as its shape, colour, and text, to
detect categories of traffic signs. This approach is more robust against
adversarial attacks, as it mimics human-like perception and is less susceptible
to the limitations of current DNN classifiers. We consider two adversarial
attacking methods to evaluate our approach: Robust Physical Perturbation (PR2)
and Adversarial Camouflage (AdvCam). These attacks are able to deceive DNN
classifiers, causing them to misidentify stop signs as other signs with high
confidence. The results show that the proposed ILP-based technique is able to
correctly identify all targeted stop signs, even in the presence of PR2 and
ADvCam attacks. The proposed learning method is also efficient as it requires
minimal training data. Moreover, it is fully explainable, making it possible to
debug AVs.",None,-1
0a9c07da-872a-4ab2-be42-79ec9ce84a76,Semantic 3D-aware Portrait Synthesis and Manipulation Based on Compositional Neural Radiance Field,0.276333,"Recently 3D-aware GAN methods with neural radiance field have developed
rapidly. However, current methods model the whole image as an overall neural
radiance field, which limits the partial semantic editability of synthetic
results. Since NeRF renders an image pixel by pixel, it is possible to split
NeRF in the spatial dimension. We propose a Compositional Neural Radiance Field
(CNeRF) for semantic 3D-aware portrait synthesis and manipulation. CNeRF
divides the image by semantic regions and learns an independent neural radiance
field for each region, and finally fuses them and renders the complete image.
Thus we can manipulate the synthesized semantic regions independently, while
fixing the other parts unchanged. Furthermore, CNeRF is also designed to
decouple shape and texture within each semantic region. Compared to
state-of-the-art 3D-aware GAN methods, our approach enables fine-grained
semantic region manipulation, while maintaining high-quality 3D-consistent
synthesis. The ablation studies show the effectiveness of the structure and
loss function used by our method. In addition real image inversion and cartoon
portrait 3D editing experiments demonstrate the application potential of our
method.",None,-1
981d7071-1399-417d-9c19-96383dd34cab,Contrastive Multi-Task Dense Prediction,0.371477,"This paper targets the problem of multi-task dense prediction which aims to
achieve simultaneous learning and inference on a bunch of multiple dense
prediction tasks in a single framework. A core objective in design is how to
effectively model cross-task interactions to achieve a comprehensive
improvement on different tasks based on their inherent complementarity and
consistency. Existing works typically design extra expensive distillation
modules to perform explicit interaction computations among different
task-specific features in both training and inference, bringing difficulty in
adaptation for different task sets, and reducing efficiency due to clearly
increased size of multi-task models. In contrast, we introduce feature-wise
contrastive consistency into modeling the cross-task interactions for
multi-task dense prediction. We propose a novel multi-task contrastive
regularization method based on the consistency to effectively boost the
representation learning of the different sub-tasks, which can also be easily
generalized to different multi-task dense prediction frameworks, and costs no
additional computation in the inference. Extensive experiments on two
challenging datasets (i.e. NYUD-v2 and Pascal-Context) clearly demonstrate the
superiority of the proposed multi-task contrastive learning approach for dense
predictions, establishing new state-of-the-art performances.",None,-1
c42c1860-7a79-4af4-97d8-914b3dedfcba,CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection,0.612356,"Task driven object detection aims to detect object instances suitable for
affording a task in an image. Its challenge lies in object categories available
for the task being too diverse to be limited to a closed set of object
vocabulary for traditional object detection. Simply mapping categories and
visual features of common objects to the task cannot address the challenge. In
this paper, we propose to explore fundamental affordances rather than object
categories, i.e., common attributes that enable different objects to accomplish
the same task. Moreover, we propose a novel multi-level chain-of-thought
prompting (MLCoT) to extract the affordance knowledge from large language
models, which contains multi-level reasoning steps from task to object examples
to essential visual attributes with rationales. Furthermore, to fully exploit
knowledge to benefit object recognition and localization, we propose a
knowledge-conditional detection framework, namely CoTDet. It conditions the
detector from the knowledge to generate object queries and regress boxes.
Experimental results demonstrate that our CoTDet outperforms state-of-the-art
methods consistently and significantly (+15.6 box AP and +14.8 mask AP) and can
generate rationales for why objects are detected to afford the task.",None,-1
003557a6-052b-4852-9972-00d82aa408cb,ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind,0.942417,"Theory of Mind (ToM), the capacity to comprehend the mental states of
distinct individuals, is essential for numerous practical applications. With
the development of large language models (LLMs), there is a heated debate about
whether they are able to perform ToM tasks. Previous studies have used
different tasks and prompts to test the ToM on LLMs and the results are
inconsistent: some studies asserted these models are capable of exhibiting ToM,
while others suggest the opposite. In this study, We present ToMChallenges, a
dataset for comprehensively evaluating the Theory of Mind based on the
Sally-Anne and Smarties tests with a diverse set of tasks. In addition, we also
propose an auto-grader to streamline the answer evaluation process. We tested
three models: davinci, turbo, and gpt-4. Our evaluation results and error
analyses show that LLMs have inconsistent behaviors across prompts and tasks.
Performing the ToM tasks robustly remains a challenge for the LLMs. In
addition, our paper wants to raise awareness in evaluating the ToM in LLMs and
we want to invite more discussion on how to design the prompts and tasks for
ToM tasks that can better assess the LLMs' ability.",None,-1
b4db68a7-f083-4e6e-94a5-086dca9d8181,Llemma: An Open Language Model For Mathematics,0.999993,"We present Llemma, a large language model for mathematics. We continue
pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web
data containing mathematics, and mathematical code, yielding Llemma. On the
MATH benchmark Llemma outperforms all known open base models, as well as the
unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is
capable of tool use and formal theorem proving without any further finetuning.
We openly release all artifacts, including 7 billion and 34 billion parameter
models, the Proof-Pile-2, and code to replicate our experiments.",None,-1
48c7f7af-2684-4aad-ba5b-15f8711e14bf,Large-scale Ridesharing DARP Instances Based on Real Travel Demand,0.632121,"Accurately predicting the real-life performance of algorithms solving the
Dial-a-Ride Problem (DARP) in the context of Mobility on Demand (MoD) systems
with ridesharing requires evaluating them on representative instances. However,
the benchmarking of state-of-the-art DARP solution methods has been limited to
small, artificial instances or outdated non-public instances, hindering direct
comparisons. With the rise of large MoD systems and the availability of open
travel demand datasets for many US cities, there is now an opportunity to
evaluate these algorithms on standardized, realistic, and representative
instances. Despite the significant challenges involved in processing obfuscated
and diverse datasets, we have developed a methodology using which we have
created a comprehensive set of large-scale demand instances based on real-world
data. These instances cover diverse use cases, one of which is demonstrated in
an evaluation of two established DARP methods: the insertion heuristic and
optimal vehicle-group assignment method. We publish the full results of both
methods in a standardized format. The results show significant differences
between areas in all measured quantities, emphasizing the importance of
evaluating methods across different cities.",None,-1
a7c52823-4a78-496e-881e-0c2235639f23,Knowledge Graph Embedding: An Overview,0.272004,"Many mathematical models have been leveraged to design embeddings for
representing Knowledge Graph (KG) entities and relations for link prediction
and many downstream tasks. These mathematically-inspired models are not only
highly scalable for inference in large KGs, but also have many explainable
advantages in modeling different relation patterns that can be validated
through both formal proofs and empirical results. In this paper, we make a
comprehensive overview of the current state of research in KG completion. In
particular, we focus on two main branches of KG embedding (KGE) design: 1)
distance-based methods and 2) semantic matching-based methods. We discover the
connections between recently proposed models and present an underlying trend
that might help researchers invent novel and more effective models. Next, we
delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D
affine operations, respectively. They encompass a broad spectrum of techniques
including distance-based and semantic-based methods. We will also discuss an
emerging approach for KG completion which leverages pre-trained language models
(PLMs) and textual descriptions of entities and relations and offer insights
into the integration of KGE embedding methods with PLMs for KG completion.",None,-1
4dba7994-908d-4b56-b7ac-8fc624713cf7,Gradient-Free Structured Pruning with Unlabeled Data,0.613165,"Large Language Models (LLMs) have achieved great success in solving difficult
tasks across many domains, but such success comes with a high computation cost,
and inference latency. As developers and third parties customize these models,
the need to provide efficient inference has increased. Many efforts have
attempted to reduce inference cost through model compression techniques such as
pruning and distillation. However, these techniques either require labeled
data, or are time-consuming as they require the compressed model to be
retrained to regain accuracy. In this paper, we propose a gradient-free
structured pruning framework that uses only unlabeled data. An evaluation on
the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates
the effectiveness of the proposed approach. By only using the weights of the
pre-trained model and unlabeled data, in a matter of a few minutes on a single
GPU, up to 40% of the original FLOP count can be reduced with less than a 4%
accuracy loss across all tasks considered.",None,-1
c8c33474-d9f6-4487-a8e8-8b0689a7f8a3,Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration,0.757599,"Kendall's tau is frequently used to meta-evaluate how well machine
translation (MT) evaluation metrics score individual translations. Its focus on
pairwise score comparisons is intuitive but raises the question of how ties
should be handled, a gray area that has motivated different variants in the
literature. We demonstrate that, in settings like modern MT meta-evaluation,
existing variants have weaknesses arising from their handling of ties, and in
some situations can even be gamed. We propose instead to meta-evaluate metrics
with a version of pairwise accuracy that gives metrics credit for correctly
predicting ties, in combination with a tie calibration procedure that
automatically introduces ties into metric scores, enabling fair comparison
between metrics that do and do not predict ties. We argue and provide
experimental evidence that these modifications lead to fairer ranking-based
assessments of metric performance.",None,-1
69c42f4a-bfda-471c-991b-54f7619da819,Black-Box Analysis: GPTs Across Time in Legal Textual Entailment Task,0.337034,"The evolution of Generative Pre-trained Transformer (GPT) models has led to
significant advancements in various natural language processing applications,
particularly in legal textual entailment. We present an analysis of GPT-3.5
(ChatGPT) and GPT-4 performances on COLIEE Task 4 dataset, a prominent
benchmark in this domain. The study encompasses data from Heisei 18 (2006) to
Reiwa 3 (2021), exploring the models' abilities to discern entailment
relationships within Japanese statute law across different periods. Our
preliminary experimental results unveil intriguing insights into the models'
strengths and weaknesses in handling legal textual entailment tasks, as well as
the patterns observed in model performance. In the context of proprietary
models with undisclosed architectures and weights, black-box analysis becomes
crucial for evaluating their capabilities. We discuss the influence of training
data distribution and the implications on the models' generalizability. This
analysis serves as a foundation for future research, aiming to optimize
GPT-based models and enable their successful adoption in legal information
extraction and entailment applications.",None,-1
98d4b190-071a-4b31-bfe5-12c23b7cf1ab,Training Machine Learning Models to Characterize Temporal Evolution of Disadvantaged Communities,0.0387908,"Disadvantaged communities (DAC), as defined by the Justice40 initiative of
the Department of Energy (DOE), USA, identifies census tracts across the USA to
determine where benefits of climate and energy investments are or are not
currently accruing. The DAC status not only helps in determining the
eligibility for future Justice40-related investments but is also critical for
exploring ways to achieve equitable distribution of resources. However,
designing inclusive and equitable strategies not just requires a good
understanding of current demographics, but also a deeper analysis of the
transformations that happened in those demographics over the years. In this
paper, machine learning (ML) models are trained on publicly available census
data from recent years to classify the DAC status at the census tracts level
and then the trained model is used to classify DAC status for historical years.
A detailed analysis of the feature and model selection along with the evolution
of disadvantaged communities between 2013 and 2018 is presented in this study.",None,-1
9d4745a0-84eb-4966-93d0-9290fe2e9f13,A Cross-Linguistic Pressure for Uniform Information Density in Word Order,0.540743,"While natural languages differ widely in both canonical word order and word
order flexibility, their word orders still follow shared cross-linguistic
statistical patterns, often attributed to functional pressures. In the effort
to identify these pressures, prior work has compared real and counterfactual
word orders. Yet one functional pressure has been overlooked in such
investigations: the uniform information density (UID) hypothesis, which holds
that information should be spread evenly throughout an utterance. Here, we ask
whether a pressure for UID may have influenced word order patterns
cross-linguistically. To this end, we use computational models to test whether
real orders lead to greater information uniformity than counterfactual orders.
In our empirical study of 10 typologically diverse languages, we find that: (i)
among SVO languages, real word orders consistently have greater uniformity than
reverse word orders, and (ii) only linguistically implausible counterfactual
orders consistently exceed the uniformity of real orders. These findings are
compatible with a pressure for information uniformity in the development and
usage of natural languages.",None,-1
b208f9f2-945a-4697-9e8b-507dcc469d16,DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception,0.418501,"Closing the domain gap between training and deployment and incorporating
multiple sensor modalities are two challenging yet critical topics for
self-driving. Existing work only focuses on single one of the above topics,
overlooking the simultaneous domain and modality shift which pervasively exists
in real-world scenarios. A model trained with multi-sensor data collected in
Europe may need to run in Asia with a subset of input sensors available. In
this work, we propose DualCross, a cross-modality cross-domain adaptation
framework to facilitate the learning of a more robust monocular bird's-eye-view
(BEV) perception model, which transfers the point cloud knowledge from a LiDAR
sensor in one domain during the training phase to the camera-only testing
scenario in a different domain. This work results in the first open analysis of
cross-domain cross-sensor perception and adaptation for monocular 3D tasks in
the wild. We benchmark our approach on large-scale datasets under a wide range
of domain shifts and show state-of-the-art results against various baselines.",None,-1
5a8fb3dc-dd03-45a4-aa69-c4479cc6a869,ProtoCon: Pseudo-label Refinement via Online Clustering and Prototypical Consistency for Efficient Semi-supervised Learning,0.594645,"Confidence-based pseudo-labeling is among the dominant approaches in
semi-supervised learning (SSL). It relies on including high-confidence
predictions made on unlabeled data as additional targets to train the model. We
propose ProtoCon, a novel SSL method aimed at the less-explored label-scarce
SSL where such methods usually underperform. ProtoCon refines the pseudo-labels
by leveraging their nearest neighbours' information. The neighbours are
identified as the training proceeds using an online clustering approach
operating in an embedding space trained via a prototypical loss to encourage
well-formed clusters. The online nature of ProtoCon allows it to utilise the
label history of the entire dataset in one training cycle to refine labels in
the following cycle without the need to store image embeddings. Hence, it can
seamlessly scale to larger datasets at a low cost. Finally, ProtoCon addresses
the poor training signal in the initial phase of training (due to fewer
confident predictions) by introducing an auxiliary self-supervised loss. It
delivers significant gains and faster convergence over state-of-the-art across
5 datasets, including CIFARs, ImageNet and DomainNet.",None,-1
ce4de0d2-beab-436e-a857-343bc62802fa,News and Load: A Quantitative Exploration of Natural Language Processing Applications for Forecasting Day-ahead Electricity System Demand,0.142091,"The relationship between electricity demand and weather is well established
in power systems, along with the importance of behavioral and social aspects
such as holidays and significant events. This study explores the link between
electricity demand and more nuanced information about social events. This is
done using mature Natural Language Processing (NLP) and demand forecasting
techniques. The results indicate that day-ahead forecasts are improved by
textual features such as word frequencies, public sentiments, topic
distributions, and word embeddings. The social events contained in these
features include global pandemics, politics, international conflicts,
transportation, etc. Causality effects and correlations are discussed to
propose explanations for the mechanisms behind the links highlighted. This
study is believed to bring a new perspective to traditional electricity demand
analysis. It confirms the feasibility of improving forecasts from unstructured
text, with potential consequences for sociology and economics.",None,-1
7c69bb95-2383-4d25-9d86-f789401e7d04,ChatHaruhi: Reviving Anime Character in Reality via Large Language Model,0.658915,"Role-playing chatbots built on large language models have drawn interest, but
better techniques are needed to enable mimicking specific fictional characters.
We propose an algorithm that controls language models via an improved prompt
and memories of the character extracted from scripts. We construct ChatHaruhi,
a dataset covering 32 Chinese / English TV / anime characters with over 54k
simulated dialogues. Both automatic and human evaluations show our approach
improves role-playing ability over baselines. Code and data are available at
https://github.com/LC1332/Chat-Haruhi-Suzumiya .",None,-1
2596b4f9-b18c-4715-94bb-797d37c2d8d4,Formally Explaining Neural Networks within Reactive Systems,0.761083,"Deep neural networks (DNNs) are increasingly being used as controllers in
reactive systems. However, DNNs are highly opaque, which renders it difficult
to explain and justify their actions. To mitigate this issue, there has been a
surge of interest in explainable AI (XAI) techniques, capable of pinpointing
the input features that caused the DNN to act as it did. Existing XAI
techniques typically face two limitations: (i) they are heuristic, and do not
provide formal guarantees that the explanations are correct; and (ii) they
often apply to ``one-shot'' systems, where the DNN is invoked independently of
past invocations, as opposed to reactive systems. Here, we begin bridging this
gap, and propose a formal DNN-verification-based XAI technique for reasoning
about multi-step, reactive systems. We suggest methods for efficiently
calculating succinct explanations, by exploiting the system's transition
constraints in order to curtail the search space explored by the underlying
verifier. We evaluate our approach on two popular benchmarks from the domain of
automated navigation; and observe that our methods allow the efficient
computation of minimal and minimum explanations, significantly outperforming
the state of the art. We also demonstrate that our methods produce formal
explanations that are more reliable than competing, non-verification-based XAI
techniques.",None,-1
7ac7b534-67dc-4b0b-968c-5c784d17775f,Tracking Progress in Multi-Agent Path Finding,0.89821,"Multi-Agent Path Finding (MAPF) is an important core problem for many new and
emerging industrial applications. Many works appear on this topic each year,
and a large number of substantial advancements and performance improvements
have been reported. Yet measuring overall progress in MAPF is difficult: there
are many potential competitors, and the computational burden for comprehensive
experimentation is prohibitively large. Moreover, detailed data from past
experimentation is usually unavailable. In this work, we introduce a set of
methodological and visualisation tools which can help the community establish
clear indicators for state-of-the-art MAPF performance and which can facilitate
large-scale comparisons between MAPF solvers. Our objectives are to lower the
barrier of entry for new researchers and to further promote the study of MAPF,
since progress in the area and the main challenges are made much clearer.",None,-1
f850ff96-7e59-4d95-8524-f84e734f58ed,Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations,0.465649,"The rapid development and application of foundation models have
revolutionized the field of artificial intelligence. Large diffusion models
have gained significant attention for their ability to generate photorealistic
images and support various tasks. On-device deployment of these models provides
benefits such as lower server costs, offline functionality, and improved user
privacy. However, common large diffusion models have over 1 billion parameters
and pose challenges due to restricted computational and memory resources on
devices. We present a series of implementation optimizations for large
diffusion models that achieve the fastest reported inference latency to-date
(under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung
S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile
devices. These enhancements broaden the applicability of generative AI and
improve the overall user experience across a wide range of devices.",None,-1
57e2881a-6876-4a92-876c-f554e1c4d4d2,Improving Medical Dialogue Generation with Abstract Meaning Representations,0.618325,"Medical Dialogue Generation serves a critical role in telemedicine by
facilitating the dissemination of medical expertise to patients. Existing
studies focus on incorporating textual representations, which have limited
their ability to represent the semantics of text, such as ignoring important
medical entities. To enhance the model's understanding of the textual semantics
and the medical knowledge including entities and relations, we introduce the
use of Abstract Meaning Representations (AMR) to construct graphical
representations that delineate the roles of language constituents and medical
entities within the dialogues. In this paper, We propose a novel framework that
models dialogues between patients and healthcare professionals using AMR
graphs, where the neural networks incorporate textual and graphical knowledge
with a dual attention mechanism. Experimental results show that our framework
outperforms strong baseline models in medical dialogue generation,
demonstrating the effectiveness of AMR graphs in enhancing the representations
of medical knowledge and logical relationships. Furthermore, to support future
research in this domain, we provide the corresponding source code at
https://github.com/Bernard-Yang/MedDiaAMR.",None,-1
eedbedfb-0336-4df6-8f3f-65e917adaf9f,Flows: Building Blocks of Reasoning and Collaborating AI,0.807317,"Recent advances in artificial intelligence (AI) have produced highly capable
and controllable systems. This creates unprecedented opportunities for
structured reasoning as well as collaboration among multiple AI systems and
humans. To fully realize this potential, it is essential to develop a
principled way of designing and studying such structured interactions. For this
purpose, we introduce the conceptual framework Flows. Flows are self-contained
building blocks of computation, with an isolated state, communicating through a
standardized message-based interface. This modular design simplifies the
process of creating Flows by allowing them to be recursively composed into
arbitrarily nested interactions and is inherently concurrency-friendly.
Crucially, any interaction can be implemented using this framework, including
prior work on AI-AI and human-AI interactions, prompt engineering schemes, and
tool augmentation. We demonstrate the potential of Flows on competitive coding,
a challenging task on which even GPT-4 struggles. Our results suggest that
structured reasoning and collaboration substantially improve generalization,
with AI-only Flows adding +21 and human-AI Flows adding +54 absolute points in
terms of solve rate. To support rapid and rigorous research, we introduce the
aiFlows library embodying Flows. The aiFlows library is available at
https://github.com/epfl-dlab/aiflows. Data and Flows for reproducing our
experiments are available at https://github.com/epfl-dlab/cc_flows.",None,-1
f8364cfe-7b9d-44f4-955b-5e5535a631e8,Federated Neural Radiance Fields,0.2223,"The ability of neural radiance fields or NeRFs to conduct accurate 3D
modelling has motivated application of the technique to scene representation.
Previous approaches have mainly followed a centralised learning paradigm, which
assumes that all training images are available on one compute node for
training. In this paper, we consider training NeRFs in a federated manner,
whereby multiple compute nodes, each having acquired a distinct set of
observations of the overall scene, learn a common NeRF in parallel. This
supports the scenario of cooperatively modelling a scene using multiple agents.
Our contribution is the first federated learning algorithm for NeRF, which
splits the training effort across multiple compute nodes and obviates the need
to pool the images at a central node. A technique based on low-rank
decomposition of NeRF layers is introduced to reduce bandwidth consumption to
transmit the model parameters for aggregation. Transferring compressed models
instead of the raw data also contributes to the privacy of the data collecting
agents.",None,-1
62fe10ff-5833-431f-bae3-0d11f4612b82,Stackelberg Games for Learning Emergent Behaviors During Competitive Autocurricula,0.291658,"Autocurricular training is an important sub-area of multi-agent reinforcement
learning~(MARL) that allows multiple agents to learn emergent skills in an
unsupervised co-evolving scheme. The robotics community has experimented
autocurricular training with physically grounded problems, such as robust
control and interactive manipulation tasks. However, the asymmetric nature of
these tasks makes the generation of sophisticated policies challenging. Indeed,
the asymmetry in the environment may implicitly or explicitly provide an
advantage to a subset of agents which could, in turn, lead to a low-quality
equilibrium. This paper proposes a novel game-theoretic algorithm, Stackelberg
Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a
two-player MARL problem as a Stackelberg game with one player as the `leader'
and the other as the `follower' in a hierarchical interaction structure wherein
the leader has an advantage. We first demonstrate that the leader's advantage
from ST-MADDPG can be used to alleviate the inherent asymmetry in the
environment. By exploiting the leader's advantage, ST-MADDPG improves the
quality of a co-evolution process and results in more sophisticated and complex
strategies that work well even against an unseen strong opponent.",None,-1
0ca07951-b9bb-4495-8e7f-dd263df7e4d7,Characterization and Learning of Causal Graphs with Small Conditioning Sets,0.0437546,"Constraint-based causal discovery algorithms learn part of the causal graph
structure by systematically testing conditional independences observed in the
data. These algorithms, such as the PC algorithm and its variants, rely on
graphical characterizations of the so-called equivalence class of causal graphs
proposed by Pearl. However, constraint-based causal discovery algorithms
struggle when data is limited since conditional independence tests quickly lose
their statistical power, especially when the conditioning set is large. To
address this, we propose using conditional independence tests where the size of
the conditioning set is upper bounded by some integer $k$ for robust causal
discovery. The existing graphical characterizations of the equivalence classes
of causal graphs are not applicable when we cannot leverage all the conditional
independence statements. We first define the notion of $k$-Markov equivalence:
Two causal graphs are $k$-Markov equivalent if they entail the same conditional
independence constraints where the conditioning set size is upper bounded by
$k$. We propose a novel representation that allows us to graphically
characterize $k$-Markov equivalence between two causal graphs. We propose a
sound constraint-based algorithm called the $k$-PC algorithm for learning this
equivalence class. Finally, we conduct synthetic, and semi-synthetic
experiments to demonstrate that the $k$-PC algorithm enables more robust causal
discovery in the small sample regime compared to the baseline algorithms.",None,-1
b732dcdd-0c5a-4c94-8813-50e113c6bdec,Transformer-based model for monocular visual odometry: a video understanding approach,0.717169,"Estimating the camera's pose given images of a single camera is a traditional
task in mobile robots and autonomous vehicles. This problem is called monocular
visual odometry and it often relies on geometric approaches that require
considerable engineering effort for a specific scenario. Deep learning methods
have shown to be generalizable after proper training and a large amount of
available data. Transformer-based architectures have dominated the
state-of-the-art in natural language processing and computer vision tasks, such
as image and video understanding. In this work, we deal with the monocular
visual odometry as a video understanding task to estimate the 6-DoF camera's
pose. We contribute by presenting the TSformer-VO model based on
spatio-temporal self-attention mechanisms to extract features from clips and
estimate the motions in an end-to-end manner. Our approach achieved competitive
state-of-the-art performance compared with geometry-based and deep
learning-based methods on the KITTI visual odometry dataset, outperforming the
DeepVO implementation highly accepted in the visual odometry community.",None,-1
05930784-8204-4107-b710-53a03902c038,Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation,0.843233,"Understanding and manipulating deformable objects (e.g., ropes and fabrics)
is an essential yet challenging task with broad applications. Difficulties come
from complex states and dynamics, diverse configurations and high-dimensional
action space of deformable objects. Besides, the manipulation tasks usually
require multiple steps to accomplish, and greedy policies may easily lead to
local optimal states. Existing studies usually tackle this problem using
reinforcement learning or imitating expert demonstrations, with limitations in
modeling complex states or requiring hand-crafted expert policies. In this
paper, we study deformable object manipulation using dense visual affordance,
with generalization towards diverse states, and propose a novel kind of
foresightful dense affordance, which avoids local optima by estimating states'
values for long-term manipulation. We propose a framework for learning this
representation, with novel designs such as multi-stage stable learning and
efficient self-supervised data collection without experts. Experiments
demonstrate the superiority of our proposed foresightful dense affordance.
Project page: https://hyperplane-lab.github.io/DeformableAffordance",None,-1
9f23eb9d-d6f6-490d-b5c5-0629a418716f,REST: Retrieval-Based Speculative Decoding,0.663257,"We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm
designed to speed up language model generation. The key insight driving the
development of REST is the observation that the process of text generation
often includes certain common phases and patterns. Unlike previous methods that
rely on a draft language model for speculative decoding, REST harnesses the
power of retrieval to generate draft tokens. This method draws from the
reservoir of existing knowledge, retrieving and employing relevant tokens based
on the current context. Its plug-and-play nature allows for seamless
integration and acceleration of any language models, all without necessitating
additional training. When benchmarked on 7B and 13B language models in a
single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on
code or text generation. The code of REST is available at
https://github.com/FasterDecoding/REST.",None,-1
914c7718-7be4-45a0-898d-959693b266ae,Distilling Internet-Scale Vision-Language Models into Embodied Agents,0.280423,"Instruction-following agents must ground language into their observation and
action spaces. Learning to ground language is challenging, typically requiring
domain-specific engineering or large quantities of human interaction data. To
address this challenge, we propose using pretrained vision-language models
(VLMs) to supervise embodied agents. We combine ideas from model distillation
and hindsight experience replay (HER), using a VLM to retroactively generate
language describing the agent's behavior. Simple prompting allows us to control
the supervision signal, teaching an agent to interact with novel objects based
on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered
environment. Fewshot prompting lets us teach abstract category membership,
including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary
preferences over objects). Our work outlines a new and effective way to use
internet-scale VLMs, repurposing the generic language grounding acquired by
such models to teach task-relevant groundings to embodied agents.",None,-1
4ebc42e4-4f44-49c4-939b-ae2f6c7c795e,"JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning",0.14196,"Instruction tuning has become an essential process for optimizing the
performance of large language models (LLMs). However, current text-to-text
instruction tuning methods, referred to as TextTuning, exhibit significant
limitations in terms of generalization, robustness, and controllability,
primarily due to the absence of explicit task structures. In this paper, we
introduce JsonTuning, a novel structure-to-structure approach for instruction
tuning. By utilizing the versatile and structured format of JSON to represent
tasks, JsonTuning enhances generalization by enabling the model to comprehend
essential task elements and their interrelations, improves robustness by
reducing ambiguity, and increases controllability by providing explicit control
over the output. We conduct a comprehensive comparative analysis between
JsonTuning and TextTuning using various language models and evaluation
benchmarks. Our experimental results demonstrate that JsonTuning consistently
outperforms TextTuning across a range of applications, showing marked
improvements in performance, robustness, and controllability. By addressing the
inherent limitations of TextTuning, JsonTuning reveals significant potential
for developing more effective and reliable LLMs capable of managing diverse
scenarios.",None,-1
8e799c7d-48bc-4e0f-a35f-c007702f51ca,Reinforcement Learning for Topic Models,0.0884039,"We apply reinforcement learning techniques to topic modeling by replacing the
variational autoencoder in ProdLDA with a continuous action space reinforcement
learning policy. We train the system with a policy gradient algorithm
REINFORCE. Additionally, we introduced several modifications: modernize the
neural network architecture, weight the ELBO loss, use contextual embeddings,
and monitor the learning process via computing topic diversity and coherence
for each training step. Experiments are performed on 11 data sets. Our
unsupervised model outperforms all other unsupervised models and performs on
par with or better than most models using supervised labeling. Our model is
outperformed on certain data sets by a model using supervised labeling and
contrastive learning. We have also conducted an ablation study to provide
empirical evidence of performance improvements from changes we made to ProdLDA
and found that the reinforcement learning formulation boosts performance.",None,-1
8d0cf620-d723-49d2-b491-847027a6f26c,eXplainable Artificial Intelligence (XAI) in aging clock models,0.50448,"eXplainable Artificial Intelligence (XAI) is a rapidly progressing field of
machine learning, aiming to unravel the predictions of complex models. XAI is
especially required in sensitive applications, e.g. in health care, when
diagnosis, recommendations and treatment choices might rely on the decisions
made by artificial intelligence systems. AI approaches have become widely used
in aging research as well, in particular, in developing biological clock models
and identifying biomarkers of aging and age-related diseases. However, the
potential of XAI here awaits to be fully appreciated. We discuss the
application of XAI for developing the ""aging clocks"" and present a
comprehensive analysis of the literature categorized by the focus on particular
physiological systems.",None,-1
841469aa-297f-4797-8a9c-088311afed79,Clustered Embedding Learning for Recommender Systems,0.304649,"In recent years, recommender systems have advanced rapidly, where embedding
learning for users and items plays a critical role. A standard method learns a
unique embedding vector for each user and item. However, such a method has two
important limitations in real-world applications: 1) it is hard to learn
embeddings that generalize well for users and items with rare interactions on
their own; and 2) it may incur unbearably high memory costs when the number of
users and items scales up. Existing approaches either can only address one of
the limitations or have flawed overall performances. In this paper, we propose
Clustered Embedding Learning (CEL) as an integrated solution to these two
problems. CEL is a plug-and-play embedding learning framework that can be
combined with any differentiable feature interaction model. It is capable of
achieving improved performance, especially for cold users and items, with
reduced memory cost. CEL enables automatic and dynamic clustering of users and
items in a top-down fashion, where clustered entities jointly learn a shared
embedding. The accelerated version of CEL has an optimal time complexity, which
supports efficient online updates. Theoretically, we prove the identifiability
and the existence of a unique optimal number of clusters for CEL in the context
of nonnegative matrix factorization. Empirically, we validate the effectiveness
of CEL on three public datasets and one business dataset, showing its
consistently superior performance against current state-of-the-art methods. In
particular, when incorporating CEL into the business model, it brings an
improvement of $+0.6\%$ in AUC, which translates into a significant revenue
gain; meanwhile, the size of the embedding table gets $2650$ times smaller.",None,-1
0af66ae3-daee-482d-8b6a-636598799910,INF: Implicit Neural Fusion for LiDAR and Camera,0.568435,"Sensor fusion has become a popular topic in robotics. However, conventional
fusion methods encounter many difficulties, such as data representation
differences, sensor variations, and extrinsic calibration. For example, the
calibration methods used for LiDAR-camera fusion often require manual operation
and auxiliary calibration targets. Implicit neural representations (INRs) have
been developed for 3D scenes, and the volume density distribution involved in
an INR unifies the scene information obtained by different types of sensors.
Therefore, we propose implicit neural fusion (INF) for LiDAR and camera. INF
first trains a neural density field of the target scene using LiDAR frames.
Then, a separate neural color field is trained using camera images and the
trained neural density field. Along with the training process, INF both
estimates LiDAR poses and optimizes extrinsic parameters. Our experiments
demonstrate the high accuracy and stable performance of the proposed method.",None,-1
f1e98093-d3b9-4053-8082-fb59e6347e70,Non-contrastive sentence representations via self-supervision,0.109091,"Sample contrastive methods, typically referred to simply as contrastive are
the foundation of most unsupervised methods to learn text and sentence
embeddings. On the other hand, a different class of self-supervised loss
functions and methods have been considered in the computer vision community and
referred to as dimension contrastive. In this paper, we thoroughly compare this
class of methods with the standard baseline for contrastive sentence
embeddings, SimCSE. We find that self-supervised embeddings trained using
dimension contrastive objectives can outperform SimCSE on downstream tasks
without needing auxiliary loss functions.",None,-1
48b4c516-8195-4369-9eef-47bab3efac51,TransWorldNG: Traffic Simulation via Foundation Model,0.773864,"Traffic simulation is a crucial tool for transportation decision-making and
policy development. However, achieving realistic simulations in the face of the
high dimensionality and heterogeneity of traffic environments is a longstanding
challenge. In this paper, we present TransWordNG, a traffic simulator that uses
Data-driven algorithms and Graph Computing techniques to learn traffic dynamics
from real data. The functionality and structure of TransWorldNG are introduced,
which utilize a foundation model for transportation management and control. The
results demonstrate that TransWorldNG can generate more realistic traffic
patterns compared to traditional simulators. Additionally, TransWorldNG
exhibits better scalability, as it shows linear growth in computation time as
the scenario scale increases. To the best of our knowledge, this is the first
traffic simulator that can automatically learn traffic patterns from real-world
data and efficiently generate accurate and realistic traffic environments.",None,-1
a193db28-3691-4c42-89ad-692c1f53d4d8,Multi-lingual and Multi-cultural Figurative Language Understanding,0.468803,"Figurative language permeates human communication, but at the same time is
relatively understudied in NLP. Datasets have been created in English to
accelerate progress towards measuring and improving figurative language
processing in language models (LMs). However, the use of figurative language is
an expression of our cultural and societal experiences, making it difficult for
these phrases to be universally applicable. In this work, we create a
figurative language inference dataset, \datasetname, for seven diverse
languages associated with a variety of cultures: Hindi, Indonesian, Javanese,
Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language
relies on cultural and regional concepts for figurative expressions, with the
highest overlap between languages originating from the same region. We assess
multilingual LMs' abilities to interpret figurative language in zero-shot and
few-shot settings. All languages exhibit a significant deficiency compared to
English, with variations in performance reflecting the availability of
pre-training and fine-tuning data, emphasizing the need for LMs to be exposed
to a broader range of linguistic and cultural variation during training.",None,-1
0a2e20b0-0bdc-4398-a6be-60fdc74b7920,Generalized Universal Domain Adaptation with Generative Flow Networks,0.483344,"We introduce a new problem in unsupervised domain adaptation, termed as
Generalized Universal Domain Adaptation (GUDA), which aims to achieve precise
prediction of all target labels including unknown categories. GUDA bridges the
gap between label distribution shift-based and label space mismatch-based
variants, essentially categorizing them as a unified problem, guiding to a
comprehensive framework for thoroughly solving all the variants. The key
challenge of GUDA is developing and identifying novel target categories while
estimating the target label distribution. To address this problem, we take
advantage of the powerful exploration capability of generative flow networks
and propose an active domain adaptation algorithm named GFlowDA, which selects
diverse samples with probabilities proportional to a reward function. To
enhance the exploration capability and effectively perceive the target label
distribution, we tailor the states and rewards, and introduce an efficient
solution for parent exploration and state transition. We also propose a
training paradigm for GUDA called Generalized Universal Adversarial Network
(GUAN), which involves collaborative optimization between GUAN and GFlowNet.
Theoretical analysis highlights the importance of exploration, and extensive
experiments on benchmark datasets demonstrate the superiority of GFlowDA.",None,-1
6a616a14-001f-4e37-adb0-fc697d6c0631,Probabilistic Adaptation of Text-to-Video Models,0.626257,"Large text-to-video models trained on internet-scale data have demonstrated
exceptional capabilities in generating high-fidelity videos from arbitrary
textual descriptions. However, adapting these models to tasks with limited
domain-specific data, such as animation or robotics videos, poses a significant
computational challenge, since finetuning a pretrained large model can be
prohibitively expensive. Inspired by how a small modifiable component (e.g.,
prompts, prefix-tuning) can adapt a large language model to perform new tasks
without requiring access to the model weights, we investigate how to adapt a
large pretrained text-to-video model to a variety of downstream domains and
tasks without finetuning. In answering this question, we propose Video Adapter,
which leverages the score function of a large pretrained video diffusion model
as a probabilistic prior to guide the generation of a task-specific small video
model. Our experiments show that Video Adapter is capable of incorporating the
broad knowledge and preserving the high fidelity of a large pretrained video
model in a task-specific small video model that is able to generate
high-quality yet specialized videos on a variety of tasks such as animation,
egocentric modeling, and modeling of simulated and real-world robotics data.
More videos can be found on the website https://video-adapter.github.io/.",None,-1
8d07388e-1485-4847-bccb-d6f139e9ca0f,AI model GPT-3 (dis)informs us better than humans,0.831456,"Artificial intelligence is changing the way we create and evaluate
information, and this is happening during an infodemic, which has been having
dramatic effects on global health. In this paper we evaluate whether recruited
individuals can distinguish disinformation from accurate information,
structured in the form of tweets, and determine whether a tweet is organic or
synthetic, i.e., whether it has been written by a Twitter user or by the AI
model GPT-3. Our results show that GPT-3 is a double-edge sword, which, in
comparison with humans, can produce accurate information that is easier to
understand, but can also produce more compelling disinformation. We also show
that humans cannot distinguish tweets generated by GPT-3 from tweets written by
human users. Starting from our results, we reflect on the dangers of AI for
disinformation, and on how we can improve information campaigns to benefit
global health.",None,-1
83586ae9-94de-4774-9b36-8a6938870e10,Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation,0.286218,"Goal-oriented Script Generation is a new task of generating a list of steps
that can fulfill the given goal. In this paper, we propose to extend the task
from the perspective of cognitive theory. Instead of a simple flat structure,
the steps are typically organized hierarchically - Human often decompose a
complex task into subgoals, where each subgoal can be further decomposed into
steps. To establish the benchmark, we contribute a new dataset, propose several
baseline methods, and set up evaluation metrics. Both automatic and human
evaluation verify the high-quality of dataset, as well as the effectiveness of
incorporating subgoals into hierarchical script generation. Furthermore, We
also design and evaluate the model to discover subgoal, and find that it is a
bit more difficult to decompose the goals than summarizing from segmented
steps.",None,-1
81140284-03a7-45ee-b508-f50c05de6fc0,Controllable Mixed-Initiative Dialogue Generation through Prompting,0.247264,"Mixed-initiative dialogue tasks involve repeated exchanges of information and
conversational control. Conversational agents gain control by generating
responses that follow particular dialogue intents or strategies, prescribed by
a policy planner. The standard approach has been fine-tuning pre-trained
language models to perform generation conditioned on these intents. However,
these supervised generation models are limited by the cost and quality of data
annotation. We instead prompt large language models as a drop-in replacement to
fine-tuning on conditional generation. We formalize prompt construction for
controllable mixed-initiative dialogue. Our findings show improvements over
fine-tuning and ground truth responses according to human evaluation and
automatic metrics for two tasks: PersuasionForGood and Emotional Support
Conversations.",None,-1
0793908c-fcb9-4d46-8919-913a094f285c,EdgeFace: Efficient Face Recognition Model for Edge Devices,0.722207,"In this paper, we present EdgeFace, a lightweight and efficient face
recognition network inspired by the hybrid architecture of EdgeNeXt. By
effectively combining the strengths of both CNN and Transformer models, and a
low rank linear layer, EdgeFace achieves excellent face recognition performance
optimized for edge devices. The proposed EdgeFace network not only maintains
low computational costs and compact storage, but also achieves high face
recognition accuracy, making it suitable for deployment on edge devices.
Extensive experiments on challenging benchmark face datasets demonstrate the
effectiveness and efficiency of EdgeFace in comparison to state-of-the-art
lightweight models and deep face recognition models. Our EdgeFace model with
1.77M parameters achieves state of the art results on LFW (99.73%), IJB-B
(92.67%), and IJB-C (94.85%), outperforming other efficient models with larger
computational complexities. The code to replicate the experiments will be made
available publicly.",None,-1
03589907-fc8b-426e-a512-c22048703429,Backpack Language Models,0.840257,"We present Backpacks: a new neural architecture that marries strong modeling
performance with an interface for interpretability and control. Backpacks learn
multiple non-contextual sense vectors for each word in a vocabulary, and
represent a word in a sequence as a context-dependent, non-negative linear
combination of sense vectors in this sequence. We find that, after training,
sense vectors specialize, each encoding a different aspect of a word. We can
interpret a sense vector by inspecting its (non-contextual, linear) projection
onto the output space, and intervene on these interpretable hooks to change the
model's behavior in predictable ways. We train a 170M-parameter Backpack
language model on OpenWebText, matching the loss of a GPT-2 small
(124Mparameter) Transformer. On lexical similarity evaluations, we find that
Backpack sense vectors outperform even a 6B-parameter Transformer LM's word
embeddings. Finally, we present simple algorithms that intervene on sense
vectors to perform controllable text generation and debiasing. For example, we
can edit the sense vocabulary to tend more towards a topic, or localize a
source of gender bias to a sense vector and globally suppress that sense.",None,-1
18917aa2-6a5a-492a-b755-0be963cf4f61,Redefining Digital Health Interfaces with Large Language Models,0.668982,"Digital health tools have the potential to significantly improve the delivery
of healthcare services. However, their adoption remains comparatively limited
due, in part, to challenges surrounding usability and trust. Large Language
Models (LLMs) have emerged as general-purpose models with the ability to
process complex information and produce human-quality text, presenting a wealth
of potential applications in healthcare. Directly applying LLMs in clinical
settings is not straightforward, however, with LLMs susceptible to providing
inconsistent or nonsensical answers. We demonstrate how LLM-based systems can
utilize external tools and provide a novel interface between clinicians and
digital technologies. This enhances the utility and practical impact of digital
healthcare tools and AI models while addressing current issues with using LLMs
in clinical settings such as hallucinations. We illustrate LLM-based interfaces
with the example of cardiovascular disease risk prediction. We develop a new
prognostic tool using automated machine learning and demonstrate how LLMs can
provide a unique interface to both our model and existing risk scores,
highlighting the benefit compared to traditional interfaces for digital tools.",None,-1
d6a92e4f-eed0-4e6f-b79e-b868a78abaf0,Mask Detection and Classification in Thermal Face Images,0.581183,"Face masks are recommended to reduce the transmission of many viruses,
especially SARS-CoV-2. Therefore, the automatic detection of whether there is a
mask on the face, what type of mask is worn, and how it is worn is an important
research topic. In this work, the use of thermal imaging was considered to
analyze the possibility of detecting (localizing) a mask on the face, as well
as to check whether it is possible to classify the type of mask on the face.
The previously proposed dataset of thermal images was extended and annotated
with the description of a type of mask and a location of a mask within a face.
Different deep learning models were adapted. The best model for face mask
detection turned out to be the Yolov5 model in the ""nano"" version, reaching mAP
higher than 97% and precision of about 95%. High accuracy was also obtained for
mask type classification. The best results were obtained for the convolutional
neural network model built on an autoencoder initially trained in the thermal
image reconstruction problem. The pretrained encoder was used to train a
classifier which achieved an accuracy of 91%.",None,-1
fcdf5379-b4f4-4dfb-bc99-da6a383dde11,ARB: Advanced Reasoning Benchmark for Large Language Models,0.592191,"Large Language Models (LLMs) have demonstrated remarkable performance on
various quantitative reasoning and knowledge benchmarks. However, many of these
benchmarks are losing utility as LLMs get increasingly high scores, despite not
yet reaching expert performance in these domains. We introduce ARB, a novel
benchmark composed of advanced reasoning problems in multiple fields. ARB
presents a more challenging test than prior benchmarks, featuring problems in
mathematics, physics, biology, chemistry, and law. As a subset of ARB, we
introduce a challenging set of math and physics problems which require advanced
symbolic reasoning and domain knowledge. We evaluate recent models such as
GPT-4 and Claude on ARB and demonstrate that current models score well below
50% on more demanding tasks. In order to improve both automatic and assisted
evaluation capabilities, we introduce a rubric-based evaluation approach,
allowing GPT-4 to score its own intermediate reasoning steps. Further, we
conduct a human evaluation of the symbolic subset of ARB, finding promising
agreement between annotators and GPT-4 rubric evaluation scores.",None,-1
96974dab-c9e1-48db-aec1-a717b75bd24a,Smooth Non-Stationary Bandits,0.611182,"In many applications of online decision making, the environment is
non-stationary and it is therefore crucial to use bandit algorithms that handle
changes. Most existing approaches are designed to protect against non-smooth
changes, constrained only by total variation or Lipschitzness over time, where
they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice
environments are often changing {\bf smoothly}, so such algorithms may incur
higher-than-necessary regret in these settings and do not leverage information
on the rate of change. We study a non-stationary two-armed bandits problem
where we assume that an arm's mean reward is a $\beta$-H\""older function over
(normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously
differentiable. We show the first separation between the smooth and non-smooth
regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$.
We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound
for any integer $\beta\ge 1$, which matches our upper bound for $\beta=2$.",None,-1
51af7c13-d1fd-4e74-b92a-d4c198e813b6,Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling,0.417397,"Convolutional neural networks encode images through a sequence of
convolutions, normalizations and non-linearities as well as downsampling
operations into potentially strong semantic embeddings. Yet, previous work
showed that even slight mistakes during sampling, leading to aliasing, can be
directly attributed to the networks' lack in robustness. To address such issues
and facilitate simpler and faster adversarial training, [12] recently proposed
FLC pooling, a method for provably alias-free downsampling - in theory. In this
work, we conduct a further analysis through the lens of signal processing and
find that such current pooling methods, which address aliasing in the frequency
domain, are still prone to spectral leakage artifacts. Hence, we propose
aliasing and spectral artifact-free pooling, short ASAP. While only introducing
a few modifications to FLC pooling, networks using ASAP as downsampling method
exhibit higher native robustness against common corruptions, a property that
FLC pooling was missing. ASAP also increases native robustness against
adversarial attacks on high and low resolution data while maintaining similar
clean accuracy or even outperforming the baseline.",None,-1
6ec36090-0c69-4b1e-9fbe-80567beb9340,Can we trust the evaluation on ChatGPT?,0.796242,"ChatGPT, the first large language model (LLM) with mass adoption, has
demonstrated remarkable performance in numerous natural language tasks. Despite
its evident usefulness, evaluating ChatGPT's performance in diverse problem
domains remains challenging due to the closed nature of the model and its
continuous updates via Reinforcement Learning from Human Feedback (RLHF). We
highlight the issue of data contamination in ChatGPT evaluations, with a case
study of the task of stance detection. We discuss the challenge of preventing
data contamination and ensuring fair model evaluation in the age of closed and
continuously trained models.",None,-1
4aab7eb1-cecb-4ecf-9b47-89c9bab006f8,DarkVisionNet: Low-Light Imaging via RGB-NIR Fusion with Deep Inconsistency Prior,0.904193,"RGB-NIR fusion is a promising method for low-light imaging. However,
high-intensity noise in low-light images amplifies the effect of structure
inconsistency between RGB-NIR images, which fails existing algorithms. To
handle this, we propose a new RGB-NIR fusion algorithm called Dark Vision Net
(DVN) with two technical novelties: Deep Structure and Deep Inconsistency Prior
(DIP). The Deep Structure extracts clear structure details in deep multiscale
feature space rather than raw input space, which is more robust to noisy
inputs. Based on the deep structures from both RGB and NIR domains, we
introduce the DIP to leverage the structure inconsistency to guide the fusion
of RGB-NIR. Benefiting from this, the proposed DVN obtains high-quality
lowlight images without the visual artifacts. We also propose a new dataset
called Dark Vision Dataset (DVD), consisting of aligned RGB-NIR image pairs, as
the first public RGBNIR fusion benchmark. Quantitative and qualitative results
on the proposed benchmark show that DVN significantly outperforms other
comparison algorithms in PSNR and SSIM, especially in extremely low light
conditions.",None,-1
28bbc089-9bbc-4fee-9979-a0951b6ed3bc,Learning with Exposure Constraints in Recommendation Systems,0.414813,"Recommendation systems are dynamic economic systems that balance the needs of
multiple stakeholders. A recent line of work studies incentives from the
content providers' point of view. Content providers, e.g., vloggers and
bloggers, contribute fresh content and rely on user engagement to create
revenue and finance their operations. In this work, we propose a contextual
multi-armed bandit setting to model the dependency of content providers on
exposure. In our model, the system receives a user context in every round and
has to select one of the arms. Every arm is a content provider who must receive
a minimum number of pulls every fixed time period (e.g., a month) to remain
viable in later rounds; otherwise, the arm departs and is no longer available.
The system aims to maximize the users' (content consumers) welfare. To that
end, it should learn which arms are vital and ensure they remain viable by
subsidizing arm pulls if needed. We develop algorithms with sub-linear regret,
as well as a lower bound that demonstrates that our algorithms are optimal up
to logarithmic factors.",None,-1
e08a7fbb-66ba-4d04-8401-519481951884,Deep Image Harmonization in Dual Color Spaces,0.695278,"Image harmonization is an essential step in image composition that adjusts
the appearance of composite foreground to address the inconsistency between
foreground and background. Existing methods primarily operate in correlated
$RGB$ color space, leading to entangled features and limited representation
ability. In contrast, decorrelated color space (e.g., $Lab$) has decorrelated
channels that provide disentangled color and illumination statistics. In this
paper, we explore image harmonization in dual color spaces, which supplements
entangled $RGB$ features with disentangled $L$, $a$, $b$ features to alleviate
the workload in harmonization process. The network comprises a $RGB$
harmonization backbone, an $Lab$ encoding module, and an $Lab$ control module.
The backbone is a U-Net network translating composite image to harmonized
image. Three encoders in $Lab$ encoding module extract three control codes
independently from $L$, $a$, $b$ channels, which are used to manipulate the
decoder features in harmonization backbone via $Lab$ control module. Our code
and model are available at
\href{https://github.com/bcmi/DucoNet-Image-Harmonization}{https://github.com/bcmi/DucoNet-Image-Harmonization}.",None,-1
71a16f0c-a258-4c67-9b4a-8a97e02d7e57,Overinformative Question Answering by Humans and Machines,0.0481635,"When faced with a polar question, speakers often provide overinformative
answers going beyond a simple ""yes"" or ""no"". But what principles guide the
selection of additional information? In this paper, we provide experimental
evidence from two studies suggesting that overinformativeness in human
answering is driven by considerations of relevance to the questioner's goals
which they flexibly adjust given the functional context in which the question
is uttered. We take these human results as a strong benchmark for investigating
question-answering performance in state-of-the-art neural language models,
conducting an extensive evaluation on items from human experiments. We find
that most models fail to adjust their answering behavior in a human-like way
and tend to include irrelevant information. We show that GPT-3 is highly
sensitive to the form of the prompt and only achieves human-like answer
patterns when guided by an example and cognitively-motivated explanation.",None,-1
8b01c0b7-46d3-475a-873a-ba599ab4a22b,QCQP-Tunneling: Ellipsoidal Constrained Agent Navigation,0.238172,"This paper presents a convex-QCQP based novel path planning algorithm named
ellipsoidal constrained agent navigation (ECAN), for a challenging problem of
online path planning in completely unknown and unseen continuous environments.
ECAN plans path for the agent by making a tunnel of overlapping ellipsoids, in
an online fashion, through the environment. Convex constraints in the
ellipsoid-formation step circumvent collision with the obstacles. The problem
of online-tunneling is solved as a convex-QCQP. This paper assumes no
constraints on shape of the agent and the obstacles. However, to make the
approach clearer, this paper first introduces the framework for a point-mass
agent with point-size obstacles. After explaining the underlying principle in
drawing an ellipsoid tunnel, the framework is extended to the agent and
obstacles having finite area (2d space) and finite-volume (3d-space).",None,-1
fe7246af-5505-4b40-b3b1-c2816c746b05,A Boosted Model Ensembling Approach to Ball Action Spotting in Videos: The Runner-Up Solution to CVPR'23 SoccerNet Challenge,0.761796,"This technical report presents our solution to Ball Action Spotting in
videos. Our method reached second place in the CVPR'23 SoccerNet Challenge.
Details of this challenge can be found at
https://www.soccer-net.org/tasks/ball-action-spotting. Our approach is
developed based on a baseline model termed E2E-Spot, which was provided by the
organizer of this competition. We first generated several variants of the
E2E-Spot model, resulting in a candidate model set. We then proposed a strategy
for selecting appropriate model members from this set and assigning an
appropriate weight to each model. The aim of this strategy is to boost the
performance of the resulting model ensemble. Therefore, we call our approach
Boosted Model Ensembling (BME). Our code is available at
https://github.com/ZJLAB-AMMI/E2E-Spot-MBS.",None,-1
708757e6-ee5b-4905-881d-c33c98f19911,Exploring Navigation Maps for Learning-Based Motion Prediction,0.158846,"The prediction of surrounding agents' motion is a key for safe autonomous
driving. In this paper, we explore navigation maps as an alternative to the
predominant High Definition (HD) maps for learning-based motion prediction.
Navigation maps provide topological and geometrical information on road-level,
HD maps additionally have centimeter-accurate lane-level information. As a
result, HD maps are costly and time-consuming to obtain, while navigation maps
with near-global coverage are freely available. We describe an approach to
integrate navigation maps into learning-based motion prediction models. To
exploit locally available HD maps during training, we additionally propose a
model-agnostic method for knowledge distillation. In experiments on the
publicly available Argoverse dataset with navigation maps obtained from
OpenStreetMap, our approach shows a significant improvement over not using a
map at all. Combined with our method for knowledge distillation, we achieve
results that are close to the original HD map-reliant models. Our publicly
available navigation map API for Argoverse enables researchers to develop and
evaluate their own approaches using navigation maps.",None,-1
78ffbca6-2062-4d12-b098-0e5c020231b2,SAM++: Enhancing Anatomic Matching using Semantic Information and Structural Inference,0.275246,"Medical images like CT and MRI provide detailed information about the
internal structure of the body, and identifying key anatomical structures from
these images plays a crucial role in clinical workflows. Current methods treat
it as a registration or key-point regression task, which has limitations in
accurate matching and can only handle predefined landmarks. Recently, some
methods have been introduced to address these limitations. One such method,
called SAM, proposes using a dense self-supervised approach to learn a distinct
embedding for each point on the CT image and achieving promising results.
Nonetheless, SAM may still face difficulties when dealing with structures that
have similar appearances but different semantic meanings or similar semantic
meanings but different appearances. To overcome these limitations, we propose
SAM++, a framework that simultaneously learns appearance and semantic
embeddings with a novel fixed-points matching mechanism. We tested the SAM++
framework on two challenging tasks, demonstrating a significant improvement
over the performance of SAM and outperforming other existing methods.",None,-1
1c7ef132-bb36-4cee-a3c9-fd856e43f156,Discovering Variable Binding Circuitry with Desiderata,0.975562,"Recent work has shown that computation in language models may be
human-understandable, with successful efforts to localize and intervene on both
single-unit features and input-output circuits. Here, we introduce an approach
which extends causal mediation experiments to automatically identify model
components responsible for performing a specific subtask by solely specifying a
set of \textit{desiderata}, or causal attributes of the model components
executing that subtask. As a proof of concept, we apply our method to
automatically discover shared \textit{variable binding circuitry} in LLaMA-13B,
which retrieves variable values for multiple arithmetic tasks. Our method
successfully localizes variable binding to only 9 attention heads (of the 1.6k)
and one MLP in the final token's residual stream.",None,-1
5794df41-2a33-466c-90ac-5ac674ac7e4e,Fast Diffusion EM: a diffusion model for blind inverse problems with application to deconvolution,0.919068,"Using diffusion models to solve inverse problems is a growing field of
research. Current methods assume the degradation to be known and provide
impressive results in terms of restoration quality and diversity. In this work,
we leverage the efficiency of those models to jointly estimate the restored
image and unknown parameters of the degradation model such as blur kernel. In
particular, we designed an algorithm based on the well-known
Expectation-Minimization (EM) estimation method and diffusion models. Our
method alternates between approximating the expected log-likelihood of the
inverse problem using samples drawn from a diffusion model and a maximization
step to estimate unknown model parameters. For the maximization step, we also
introduce a novel blur kernel regularization based on a Plug \& Play denoiser.
Diffusion models are long to run, thus we provide a fast version of our
algorithm. Extensive experiments on blind image deblurring demonstrate the
effectiveness of our method when compared to other state-of-the-art approaches.",None,-1
d92b324a-5ec3-4c59-8309-ad48f428ce94,Event Camera Data Pre-training,0.879907,"This paper proposes a pre-trained neural network for handling event camera
data. Our model is a self-supervised learning framework, and uses paired event
camera data and natural RGB images for training.
  Our method contains three modules connected in a sequence: i) a family of
event data augmentations, generating meaningful event images for
self-supervised training; ii) a conditional masking strategy to sample
informative event patches from event images, encouraging our model to capture
the spatial layout of a scene and accelerating training; iii) a contrastive
learning approach, enforcing the similarity of embeddings between matching
event images, and between paired event and RGB images. An embedding projection
loss is proposed to avoid the model collapse when enforcing the event image
embedding similarities. A probability distribution alignment loss is proposed
to encourage the event image to be consistent with its paired RGB image in the
feature space.
  Transfer learning performance on downstream tasks shows the superiority of
our method over state-of-the-art methods. For example, we achieve top-1
accuracy at 64.83% on the N-ImageNet dataset.",None,-1
e29539e2-fc76-4f93-933e-abcbd44de7d6,The DeepCAR Method: Forecasting Time-Series Data That Have Change Points,0.101503,"Many methods for time-series forecasting are known in classical statistics,
such as autoregression, moving averages, and exponential smoothing. The DeepAR
framework is a novel, recent approach for time-series forecasting based on deep
learning. DeepAR has shown very promising results already. However, time series
often have change points, which can degrade the DeepAR's prediction performance
substantially. This paper extends the DeepAR framework by detecting and
including those change points. We show that our method performs as well as
standard DeepAR when there are no change points and considerably better when
there are change points. More generally, we show that the batch size provides
an effective and surprisingly simple way to deal with change points in DeepAR,
Transformers, and other modern forecasting models.",None,-1
d8598a2c-d268-4af5-a830-71184322d712,MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers,0.240852,"Diffusion-model-based text-guided image generation has recently made
astounding progress, producing fascinating results in open-domain image
manipulation tasks. Few models, however, currently have complete zero-shot
capabilities for both global and local image editing due to the complexity and
diversity of image manipulation tasks. In this work, we propose a method with a
mixture-of-expert (MOE) controllers to align the text-guided capacity of
diffusion models with different kinds of human instructions, enabling our model
to handle various open-domain image manipulation tasks with natural language
instructions. First, we use large language models (ChatGPT) and conditional
image synthesis models (ControlNet) to generate a large number of global image
transfer dataset in addition to the instruction-based local image editing
dataset. Then, using an MOE technique and task-specific adaptation training on
a large-scale dataset, our conditional diffusion model can edit images globally
and locally. Extensive experiments demonstrate that our approach performs
surprisingly well on various image manipulation tasks when dealing with
open-domain images and arbitrary human instructions. Please refer to our
project page: [https://oppo-mente-lab.github.io/moe_controller/]",None,-1
a227a0d0-b601-4beb-bef5-b283e5aca9f4,Vision Transformer-based Feature Extraction for Generalized Zero-Shot Learning,0.204652,"Generalized zero-shot learning (GZSL) is a technique to train a deep learning
model to identify unseen classes using the image attribute. In this paper, we
put forth a new GZSL approach exploiting Vision Transformer (ViT) to maximize
the attribute-related information contained in the image feature. In ViT, the
entire image region is processed without the degradation of the image
resolution and the local image information is preserved in patch features. To
fully enjoy these benefits of ViT, we exploit patch features as well as the CLS
feature in extracting the attribute-related image feature. In particular, we
propose a novel attention-based module, called attribute attention module
(AAM), to aggregate the attribute-related information in patch features. In
AAM, the correlation between each patch feature and the synthetic image
attribute is used as the importance weight for each patch. From extensive
experiments on benchmark datasets, we demonstrate that the proposed technique
outperforms the state-of-the-art GZSL approaches by a large margin.",None,-1
7ec1cde6-9cb6-4a26-bd2c-45e18b45234f,RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models,0.366844,"The emergence of Neural Radiance Fields (NeRF) has promoted the development
of synthesized high-fidelity views of the intricate real world. However, it is
still a very demanding task to repaint the content in NeRF. In this paper, we
propose a novel framework that can take RGB images as input and alter the 3D
content in neural scenes. Our work leverages existing diffusion models to guide
changes in the designated 3D content. Specifically, we semantically select the
target object and a pre-trained diffusion model will guide the NeRF model to
generate new 3D objects, which can improve the editability, diversity, and
application range of NeRF. Experiment results show that our algorithm is
effective for editing 3D objects in NeRF under different text prompts,
including editing appearance, shape, and more. We validate our method on both
real-world datasets and synthetic-world datasets for these editing tasks.
Please visit https://starstesla.github.io/repaintnerf for a better view of our
results.",None,-1
afe649ec-16c7-49e6-8f47-2e2385daf555,MVPSNet: Fast Generalizable Multi-view Photometric Stereo,0.980765,"We propose a fast and generalizable solution to Multi-view Photometric Stereo
(MVPS), called MVPSNet. The key to our approach is a feature extraction network
that effectively combines images from the same view captured under multiple
lighting conditions to extract geometric features from shading cues for stereo
matching. We demonstrate these features, termed `Light Aggregated Feature Maps'
(LAFM), are effective for feature matching even in textureless regions, where
traditional multi-view stereo methods fail. Our method produces similar
reconstruction results to PS-NeRF, a state-of-the-art MVPS method that
optimizes a neural network per-scene, while being 411$\times$ faster (105
seconds vs. 12 hours) in inference. Additionally, we introduce a new synthetic
dataset for MVPS, sMVPS, which is shown to be effective to train a
generalizable MVPS method.",None,-1
ccca511f-0752-4935-8686-0b80dec1ab02,A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications,0.523567,"We present a framework for the automated measurement of responsible AI (RAI)
metrics for large language models (LLMs) and associated products and services.
Our framework for automatically measuring harms from LLMs builds on existing
technical and sociotechnical expertise and leverages the capabilities of
state-of-the-art LLMs, such as GPT-4. We use this framework to run through
several case studies investigating how different LLMs may violate a range of
RAI-related principles. The framework may be employed alongside domain-specific
sociotechnical expertise to create measurements for new harm areas in the
future. By implementing this framework, we aim to enable more advanced harm
measurement efforts and further the responsible use of LLMs.",None,-1
980e62ab-f62f-479a-90c4-81531c8dca98,"Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators",0.865335,"Large language models that exhibit instruction-following behaviour represent
one of the biggest recent upheavals in conversational interfaces, a trend in
large part fuelled by the release of OpenAI's ChatGPT, a proprietary large
language model for text generation fine-tuned through reinforcement learning
from human feedback (LLM+RLHF). We review the risks of relying on proprietary
software and survey the first crop of open-source projects of comparable
architecture and functionality. The main contribution of this paper is to show
that openness is differentiated, and to offer scientific documentation of
degrees of openness in this fast-moving field. We evaluate projects in terms of
openness of code, training data, model weights, RLHF data, licensing,
scientific documentation, and access methods. We find that while there is a
fast-growing list of projects billing themselves as 'open source', many inherit
undocumented data of dubious legality, few share the all-important
instruction-tuning (a key site where human annotation labour is involved), and
careful scientific documentation is exceedingly rare. Degrees of openness are
relevant to fairness and accountability at all points, from data collection and
curation to model architecture, and from training and fine-tuning to release
and deployment.",None,-1
b490e594-8c28-4f19-adb9-d0f3593e192b,A Closer Look at Reward Decomposition for High-Level Robotic Explanations,0.100899,"Explaining the behaviour of intelligent agents learned by reinforcement
learning (RL) to humans is challenging yet crucial due to their
incomprehensible proprioceptive states, variational intermediate goals, and
resultant unpredictability. Moreover, one-step explanations for RL agents can
be ambiguous as they fail to account for the agent's future behaviour at each
transition, adding to the complexity of explaining robot actions. By leveraging
abstracted actions that map to task-specific primitives, we avoid explanations
on the movement level. To further improve the transparency and explainability
of robotic systems, we propose an explainable Q-Map learning framework that
combines reward decomposition (RD) with abstracted action spaces, allowing for
non-ambiguous and high-level explanations based on object properties in the
task. We demonstrate the effectiveness of our framework through quantitative
and qualitative analysis of two robotic scenarios, showcasing visual and
textual explanations, from output artefacts of RD explanations, that are easy
for humans to comprehend. Additionally, we demonstrate the versatility of
integrating these artefacts with large language models (LLMs) for reasoning and
interactive querying.",None,-1
dd2d95b1-b245-46c2-8d48-bf7fc03ee8b0,Honesty Is the Best Policy: Defining and Mitigating AI Deception,0.659979,"Deceptive agents are a challenge for the safety, trustworthiness, and
cooperation of AI systems. We focus on the problem that agents might deceive in
order to achieve their goals (for instance, in our experiments with language
models, the goal of being evaluated as truthful). There are a number of
existing definitions of deception in the literature on game theory and symbolic
AI, but there is no overarching theory of deception for learning agents in
games. We introduce a formal definition of deception in structural causal
games, grounded in the philosophy literature, and applicable to real-world
machine learning systems. Several examples and results illustrate that our
formal definition aligns with the philosophical and commonsense meaning of
deception. Our main technical result is to provide graphical criteria for
deception. We show, experimentally, that these results can be used to mitigate
deception in reinforcement learning agents and language models.",None,-1
289dcf62-bd78-468f-8d17-433b041b4e4d,Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations,0.168227,"We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize
SocialBot Grand Challenge~5. Building upon previous versions of our system, we
introduce the NRG Barista and outline several innovative approaches for
integrating Barista into our SocialBot, improving the overall conversational
experience. Additionally, we extend our SocialBot to support multimodal
devices. This paper offers insights into the development of Alquist~5.0, which
meets evolving user expectations while maintaining empathetic and knowledgeable
conversational abilities across diverse topics.",None,-1
9573c079-c1a9-4135-b4c8-032e86c03bc5,NN-Steiner: A Mixed Neural-algorithmic Approach for the Rectilinear Steiner Minimum Tree Problem,0.998851,"Recent years have witnessed rapid advances in the use of neural networks to
solve combinatorial optimization problems. Nevertheless, designing the ""right""
neural model that can effectively handle a given optimization problem can be
challenging, and often there is no theoretical understanding or justification
of the resulting neural model. In this paper, we focus on the rectilinear
Steiner minimum tree (RSMT) problem, which is of critical importance in IC
layout design and as a result has attracted numerous heuristic approaches in
the VLSI literature. Our contributions are two-fold. On the methodology front,
we propose NN-Steiner, which is a novel mixed neural-algorithmic framework for
computing RSMTs that leverages the celebrated PTAS algorithmic framework of
Arora to solve this problem (and other geometric optimization problems). Our
NN-Steiner replaces key algorithmic components within Arora's PTAS by suitable
neural components. In particular, NN-Steiner only needs four neural network
(NN) components that are called repeatedly within an algorithmic framework.
Crucially, each of the four NN components is only of bounded size independent
of input size, and thus easy to train. Furthermore, as the NN component is
learning a generic algorithmic step, once learned, the resulting mixed
neural-algorithmic framework generalizes to much larger instances not seen in
training. Our NN-Steiner, to our best knowledge, is the first neural
architecture of bounded size that has capacity to approximately solve RSMT (and
variants). On the empirical front, we show how NN-Steiner can be implemented
and demonstrate the effectiveness of our resulting approach, especially in
terms of generalization, by comparing with state-of-the-art methods (both
neural and non-neural based).",None,-1
7e1fbc47-82a0-4ed0-8194-00ed52cc440a,CroSentiNews 2.0: A Sentence-Level News Sentiment Corpus,0.0958277,"This article presents a sentence-level sentiment dataset for the Croatian
news domain. In addition to the 3K annotated texts already present, our dataset
contains 14.5K annotated sentence occurrences that have been tagged with 5
classes. We provide baseline scores in addition to the annotation process and
inter-annotator agreement.",None,-1
c663983c-c8c1-4952-bc75-2cbb3cc7a882,Long-range Meta-path Search on Large-scale Heterogeneous Graphs,0.191071,"Utilizing long-range dependency, though extensively studied in homogeneous
graphs, has not been well investigated on heterogeneous graphs. Addressing this
research gap presents two major challenges. The first is to alleviate
computational costs while endeavoring to leverage as much effective information
as possible in the presence of heterogeneity. The second involves overcoming
the well-known over-smoothing issue occurring in various graph neural networks.
To this end, we investigate the importance of different meta-paths and
introduce an automatic framework for utilizing long-range dependency on
heterogeneous graphs, denoted as Long-range Meta-path Search through
Progressive Sampling (LMSPS). Specifically, we develop a search space with all
meta-paths related to the target node type. By employing a progressive sampling
algorithm, LMSPS dynamically shrinks the search space with hop-independent time
complexity. Utilizing a sampling evaluation strategy as the guidance, LMSPS
conducts a specialized and effective meta-path selection. Subsequently, only
effective meta-paths are employed for retraining to reduce costs and overcome
the over-smoothing issue. Extensive experiments on various heterogeneous
datasets demonstrate that LMSPS discovers effective long-range meta-paths and
outperforms the state-of-the-art. Besides, it ranks top-1 on the leaderboards
of \texttt{ogbn-mag} in Open Graph Benchmark. Our code is available at
https://github.com/JHL-HUST/LDMLP.",None,-1
926121a4-76f3-4b5e-ac24-585dd0ed34d7,Explicit Alignment and Many-to-many Entailment Based Reasoning for Conversational Machine Reading,0.399627,"Conversational Machine Reading (CMR) requires answering a user's initial
question through multi-turn dialogue interactions based on a given document.
Although there exist many effective methods, they largely neglected the
alignment between the document and the user-provided information, which
significantly affects the intermediate decision-making and subsequent follow-up
question generation. To address this issue, we propose a pipeline framework
that (1) aligns the aforementioned two sides in an explicit way, (2)makes
decisions using a lightweight many-to-many entailment reasoning module, and (3)
directly generates follow-up questions based on the document and previously
asked questions. Our proposed method achieves state-of-the-art in
micro-accuracy and ranks the first place on the public leaderboard of the CMR
benchmark dataset ShARC.",None,-1
670ba3a7-6a14-4bfd-a9bf-06e589f59545,M$^2$DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer,0.696786,"Ensuring traffic safety and preventing accidents is a critical goal in daily
driving, where the advancement of computer vision technologies can be leveraged
to achieve this goal. In this paper, we present a multi-view, multi-scale
framework for naturalistic driving action recognition and localization in
untrimmed videos, namely M$^2$DAR, with a particular focus on detecting
distracted driving behaviors. Our system features a weight-sharing, multi-scale
Transformer-based action recognition network that learns robust hierarchical
representations. Furthermore, we propose a new election algorithm consisting of
aggregation, filtering, merging, and selection processes to refine the
preliminary results from the action recognition module across multiple views.
Extensive experiments conducted on the 7th AI City Challenge Track 3 dataset
demonstrate the effectiveness of our approach, where we achieved an overlap
score of 0.5921 on the A2 test set. Our source code is available at
\url{https://github.com/PurdueDigitalTwin/M2DAR}.",None,-1
985ccc4c-4416-41e5-8075-e03d4b31ee2f,Rule-based detection of access to education and training in Germany,0.28884,"As a result of transformation processes, the German labor market is highly
dependent on vocational training, retraining and continuing education. To match
training seekers and offers, we present a novel approach towards the automated
detection of access to education and training in German training offers and
advertisements. We will in particular focus on (a) general school and education
degrees and schoolleaving certificates, (b) professional experience, (c) a
previous apprenticeship and (d) a list of skills provided by the German Federal
Employment Agency. This novel approach combines several methods: First, we
provide a mapping of synonyms in education combining different qualifications
and adding deprecated terms. Second, we provide a rule-based matching to
identify the need for professional experience or apprenticeship. However, not
all access requirements can be matched due to incompatible data schemata or
non-standardizes requirements, e.g initial tests or interviews. While we can
identify several shortcomings, the presented approach offers promising results
for two data sets: training and re-training advertisements.",None,-1
dcf65f1d-84ff-4b7b-acd1-f65a9552b4db,Neighboring Words Affect Human Interpretation of Saliency Explanations,0.144927,"Word-level saliency explanations (""heat maps over words"") are often used to
communicate feature-attribution in text-based models. Recent studies found that
superficial factors such as word length can distort human interpretation of the
communicated saliency scores. We conduct a user study to investigate how the
marking of a word's neighboring words affect the explainee's perception of the
word's importance in the context of a saliency explanation. We find that
neighboring words have significant effects on the word's importance rating.
Concretely, we identify that the influence changes based on neighboring
direction (left vs. right) and a-priori linguistic and computational measures
of phrases and collocations (vs. unrelated neighboring words). Our results
question whether text-based saliency explanations should be continued to be
communicated at word level, and inform future research on alternative saliency
explanation methods.",None,-1
2ed32217-e002-4ad8-9e21-22659b3bf73b,Deliberate then Generate: Enhanced Prompting Framework for Text Generation,0.0756813,"Large language models (LLMs) have shown remarkable success across a wide
range of natural language generation tasks, where proper prompt designs make
great impacts. While existing prompting methods are normally restricted to
providing correct information, in this paper, we encourage the model to
deliberate by proposing a novel Deliberate then Generate (DTG) prompting
framework, which consists of error detection instructions and candidates that
may contain errors. DTG is a simple yet effective technique that can be applied
to various text generation tasks with minimal modifications. We conduct
extensive experiments on 20+ datasets across 7 text generation tasks, including
summarization, translation, dialogue, and more. We show that DTG consistently
outperforms existing prompting methods and achieves state-of-the-art
performance on multiple text generation tasks. We also provide in-depth
analyses to reveal the underlying mechanisms of DTG, which may inspire future
research on prompting for LLMs.",None,-1
13fe50b7-cf1e-4102-b65e-0b03f5be82f8,Automated Query Generation for Evidence Collection from Web Search Engines,0.111231,"It is widely accepted that so-called facts can be checked by searching for
information on the Internet. This process requires a fact-checker to formulate
a search query based on the fact and to present it to a search engine. Then,
relevant and believable passages need to be identified in the search results
before a decision is made. This process is carried out by sub-editors at many
news and media organisations on a daily basis. Here, we ask the question as to
whether it is possible to automate the first step, that of query generation.
Can we automatically formulate search queries based on factual statements which
are similar to those formulated by human experts? Here, we consider similarity
both in terms of textual similarity and with respect to relevant documents
being returned by a search engine. First, we introduce a moderate-sized
evidence collection dataset which includes 390 factual statements together with
associated human-generated search queries and search results. Then, we
investigate generating queries using a number of rule-based and automatic text
generation methods based on pre-trained large language models (LLMs). We show
that these methods have different merits and propose a hybrid approach which
has superior performance in practice.",None,-1
fac4ca34-c68c-4add-91a3-16067465c8fa,Egocentric Hierarchical Visual Semantics,0.0848544,"We are interested in aligning how people think about objects and what
machines perceive, meaning by this the fact that object recognition, as
performed by a machine, should follow a process which resembles that followed
by humans when thinking of an object associated with a certain concept. The
ultimate goal is to build systems which can meaningfully interact with their
users, describing what they perceive in the users' own terms. As from the field
of Lexical Semantics, humans organize the meaning of words in hierarchies where
the meaning of, e.g., a noun, is defined in terms of the meaning of a more
general noun, its genus, and of one or more differentiating properties, its
differentia. The main tenet of this paper is that object recognition should
implement a hierarchical process which follows the hierarchical semantic
structure used to define the meaning of words. We achieve this goal by
implementing an algorithm which, for any object, recursively recognizes its
visual genus and its visual differentia. In other words, the recognition of an
object is decomposed in a sequence of steps where the locally relevant visual
features are recognized. This paper presents the algorithm and a first
evaluation.",None,-1
2859e232-f2ff-414c-a17e-cc1150eb1a45,Causal Disentangled Variational Auto-Encoder for Preference Understanding in Recommendation,0.569905,"Recommendation models are typically trained on observational user interaction
data, but the interactions between latent factors in users' decision-making
processes lead to complex and entangled data. Disentangling these latent
factors to uncover their underlying representation can improve the robustness,
interpretability, and controllability of recommendation models. This paper
introduces the Causal Disentangled Variational Auto-Encoder (CaD-VAE), a novel
approach for learning causal disentangled representations from interaction data
in recommender systems. The CaD-VAE method considers the causal relationships
between semantically related factors in real-world recommendation scenarios,
rather than enforcing independence as in existing disentanglement methods. The
approach utilizes structural causal models to generate causal representations
that describe the causal relationship between latent factors. The results
demonstrate that CaD-VAE outperforms existing methods, offering a promising
solution for disentangling complex user behavior data in recommendation
systems.",None,-1
1ea90055-1227-41ed-b1cd-b1116f901dd9,AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator,0.884432,"Designing robust machine learning systems remains an open problem, and there
is a need for benchmark problems that cover both environmental changes and
evaluation on a downstream task. In this work, we introduce AVOIDDS, a
realistic object detection benchmark for the vision-based aircraft
detect-and-avoid problem. We provide a labeled dataset consisting of 72,000
photorealistic images of intruder aircraft with various lighting conditions,
weather conditions, relative geometries, and geographic locations. We also
provide an interface that evaluates trained models on slices of this dataset to
identify changes in performance with respect to changing environmental
conditions. Finally, we implement a fully-integrated, closed-loop simulator of
the vision-based detect-and-avoid problem to evaluate trained models with
respect to the downstream collision avoidance task. This benchmark will enable
further research in the design of robust machine learning systems for use in
safety-critical applications. The AVOIDDS dataset and code are publicly
available at https://purl.stanford.edu/hj293cv5980 and
https://github.com/sisl/VisionBasedAircraftDAA respectively.",None,-1
12605f97-1e51-4acb-b4ec-42edc02b56da,Combining Adversaries with Anti-adversaries in Training,0.202749,"Adversarial training is an effective learning technique to improve the
robustness of deep neural networks. In this study, the influence of adversarial
training on deep learning models in terms of fairness, robustness, and
generalization is theoretically investigated under more general perturbation
scope that different samples can have different perturbation directions (the
adversarial and anti-adversarial directions) and varied perturbation bounds.
Our theoretical explorations suggest that the combination of adversaries and
anti-adversaries (samples with anti-adversarial perturbations) in training can
be more effective in achieving better fairness between classes and a better
tradeoff between robustness and generalization in some typical learning
scenarios (e.g., noisy label learning and imbalance learning) compared with
standard adversarial training. On the basis of our theoretical findings, a more
general learning objective that combines adversaries and anti-adversaries with
varied bounds on each training sample is presented. Meta learning is utilized
to optimize the combination weights. Experiments on benchmark datasets under
different learning scenarios verify our theoretical findings and the
effectiveness of the proposed methodology.",None,-1
9bdb2bc3-ac3f-4ea9-a7dc-982adc59d0ae,A Theory of Bounded Inductive Rationality,0.811881,"The dominant theories of rational choice assume logical omniscience. That is,
they assume that when facing a decision problem, an agent can perform all
relevant computations and determine the truth value of all relevant
logical/mathematical claims. This assumption is unrealistic when, for example,
we offer bets on remote digits of pi or when an agent faces a computationally
intractable planning problem. Furthermore, the assumption of logical
omniscience creates contradictions in cases where the environment can contain
descriptions of the agent itself. Importantly, strategic interactions as
studied in game theory are decision problems in which a rational agent is
predicted by its environment (the other players). In this paper, we develop a
theory of rational decision making that does not assume logical omniscience. We
consider agents who repeatedly face decision problems (including ones like
betting on digits of pi or games against other agents). The main contribution
of this paper is to provide a sensible theory of rationality for such agents.
Roughly, we require that a boundedly rational inductive agent tests each
efficiently computable hypothesis infinitely often and follows those hypotheses
that keep their promises of high rewards. We then prove that agents that are
rational in this sense have other desirable properties. For example, they learn
to value random and pseudo-random lotteries at their expected reward. Finally,
we consider strategic interactions between different agents and prove a folk
theorem for what strategies bounded rational inductive agents can converge to.",None,-1
0526d4dc-5f53-40a1-8700-e8a22928ee7b,Sample Attackability in Natural Language Adversarial Attacks,0.10873,"Adversarial attack research in natural language processing (NLP) has made
significant progress in designing powerful attack methods and defence
approaches. However, few efforts have sought to identify which source samples
are the most attackable or robust, i.e. can we determine for an unseen target
model, which samples are the most vulnerable to an adversarial attack. This
work formally extends the definition of sample attackability/robustness for NLP
attacks. Experiments on two popular NLP datasets, four state of the art models
and four different NLP adversarial attack methods, demonstrate that sample
uncertainty is insufficient for describing characteristics of attackable/robust
samples and hence a deep learning based detector can perform much better at
identifying the most attackable and robust samples for an unseen target model.
Nevertheless, further analysis finds that there is little agreement in which
samples are considered the most attackable/robust across different NLP attack
methods, explaining a lack of portability of attackability detection methods
across attack methods.",None,-1
54e2f563-6667-417e-9a45-dae62f46834a,Privacy-Preserving Prompt Tuning for Large Language Model Services,0.491618,"Prompt tuning provides an efficient way for users to customize Large Language
Models (LLMs) with their private data in the emerging LLM service scenario.
However, the sensitive nature of private data brings the need for privacy
preservation in LLM service customization. Based on prompt tuning, we propose
Privacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy
guarantees for LLM services. \textsc{rapt} adopts a local privacy setting,
allowing users to privatize their data locally with local differential privacy.
As prompt tuning performs poorly when directly trained on privatized data, we
introduce a novel privatized token reconstruction task that is trained jointly
with the downstream task, allowing LLMs to learn better task-dependent
representations. Despite the simplicity of our framework, experiments show that
RAPT achieves competitive performance across tasks while providing privacy
guarantees against adversaries.",None,-1
200feff4-7ed6-403e-980f-f964d62edae9,Causal Confirmation Measures: From Simpson's Paradox to COVID-19,0.537795,"When we compare the influences of two causes on an outcome, if the conclusion
from every group is against that from the conflation, we think there is
Simpson's Paradox. The Existing Causal Inference Theory (ECIT) can make the
overall conclusion consistent with the grouping conclusion by removing the
confounder's influence to eliminate the paradox. The ECIT uses relative risk
difference Pd = max(0, (R - 1)/R) (R denotes the risk ratio) as the probability
of causation. In contrast, Philosopher Fitelson uses confirmation measure D
(posterior probability minus prior probability) to measure the strength of
causation. Fitelson concludes that from the perspective of Bayesian
confirmation, we should directly accept the overall conclusion without
considering the paradox. The author proposed a Bayesian confirmation measure b*
similar to Pd before. To overcome the contradiction between the ECIT and
Bayesian confirmation, the author uses the semantic information method with the
minimum cross-entropy criterion to deduce causal confirmation measure Cc = (R
-1)/max(R, 1). Cc is like Pd but has normalizing property (between -1 and 1)
and cause symmetry. It especially fits cases where a cause restrains an
outcome, such as the COVID-19 vaccine controlling the infection. Some examples
(about kidney stone treatments and COVID-19) reveal that Pd and Cc are more
reasonable than D; Cc is more useful than Pd.",None,-1
439b36ca-c2d7-417c-a628-f99845bfc799,Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean,0.439939,"We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire
modeling in the Mediterranean. Mesogeos integrates variables representing
wildfire drivers (meteorology, vegetation, human activity) and historical
records of wildfire ignitions and burned areas for 17 years (2006-2022). It is
designed as a cloud-friendly spatio-temporal dataset, namely a datacube,
harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The
datacube structure offers opportunities to assess machine learning (ML) usage
in various wildfire modeling tasks. We extract two ML-ready datasets that
establish distinct tracks to demonstrate this potential: (1) short-term
wildfire danger forecasting and (2) final burned area estimation given the
point of ignition. We define appropriate metrics and baselines to evaluate the
performance of models in each track. By publishing the datacube, along with the
code to create the ML datasets and models, we encourage the community to foster
the implementation of additional tracks for mitigating the increasing threat of
wildfires in the Mediterranean.",None,-1
ff1391bd-b12b-4f04-b296-9fe7a481b2ab,Stable Yaw Estimation of Boats from the Viewpoint of UAVs and USVs,0.0658819,"Yaw estimation of boats from the viewpoint of unmanned aerial vehicles (UAVs)
and unmanned surface vehicles (USVs) or boats is a crucial task in various
applications such as 3D scene rendering, trajectory prediction, and navigation.
However, the lack of literature on yaw estimation of objects from the viewpoint
of UAVs has motivated us to address this domain. In this paper, we propose a
method based on HyperPosePDF for predicting the orientation of boats in the 6D
space. For that, we use existing datasets, such as PASCAL3D+ and our own
datasets, SeaDronesSee-3D and BOArienT, which we annotated manually. We extend
HyperPosePDF to work in video-based scenarios, such that it yields robust
orientation predictions across time. Naively applying HyperPosePDF on video
data yields single-point predictions, resulting in far-off predictions and
often incorrect symmetric orientations due to unseen or visually different
data. To alleviate this issue, we propose aggregating the probability
distributions of pose predictions, resulting in significantly improved
performance, as shown in our experimental evaluation. Our proposed method could
significantly benefit downstream tasks in marine robotics.",None,-1
039f070f-d2ea-4111-b277-1abe0bb737ed,Measuring Sentiment Bias in Machine Translation,0.0924635,"Biases induced to text by generative models have become an increasingly large
topic in recent years. In this paper we explore how machine translation might
introduce a bias in sentiments as classified by sentiment analysis models. For
this, we compare three open access machine translation models for five
different languages on two parallel corpora to test if the translation process
causes a shift in sentiment classes recognized in the texts. Though our
statistic test indicate shifts in the label probability distributions, we find
none that appears consistent enough to assume a bias induced by the translation
process.",None,-1
f47af5c1-9bbf-4102-9024-2d7ae0bfc127,All Things Considered: Detecting Partisan Events from News Media with Cross-Article Comparison,0.707932,"Public opinion is shaped by the information news media provide, and that
information in turn may be shaped by the ideological preferences of media
outlets. But while much attention has been devoted to media bias via overt
ideological language or topic selection, a more unobtrusive way in which the
media shape opinion is via the strategic inclusion or omission of partisan
events that may support one side or the other. We develop a latent
variable-based framework to predict the ideology of news articles by comparing
multiple articles on the same story and identifying partisan events whose
inclusion or omission reveals ideology. Our experiments first validate the
existence of partisan event selection, and then show that article alignment and
cross-document comparison detect partisan events and article ideology better
than competitive baselines. Our results reveal the high-level form of media
bias, which is present even among mainstream media with strong norms of
objectivity and nonpartisanship. Our codebase and dataset are available at
https://github.com/launchnlp/ATC.",None,-1
23b5e4b7-e932-48c2-a4e4-92fe46d67034,"Machine Learning methods for simulating particle response in the Zero Degree Calorimeter at the ALICE experiment, CERN",0.450268,"Currently, over half of the computing power at CERN GRID is used to run High
Energy Physics simulations. The recent updates at the Large Hadron Collider
(LHC) create the need for developing more efficient simulation methods. In
particular, there exists a demand for a fast simulation of the neutron Zero
Degree Calorimeter, where existing Monte Carlo-based methods impose a
significant computational burden. We propose an alternative approach to the
problem that leverages machine learning. Our solution utilises neural network
classifiers and generative models to directly simulate the response of the
calorimeter. In particular, we examine the performance of variational
autoencoders and generative adversarial networks, expanding the GAN
architecture by an additional regularisation network and a simple, yet
effective postprocessing step. Our approach increases the simulation speed by 2
orders of magnitude while maintaining the high fidelity of the simulation.",None,-1
4497bef1-92e5-405f-86df-675653713ce0,A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction,0.267235,"Knowledge of syntax includes knowledge of rare, idiosyncratic constructions.
LLMs must overcome frequency biases in order to master such constructions. In
this study, I prompt GPT-3 to give acceptability judgments on the
English-language Article + Adjective + Numeral + Noun construction (e.g., ""a
lovely five days""). I validate the prompt using the CoLA corpus of
acceptability judgments and then zero in on the AANN construction. I compare
GPT- 3's judgments to crowdsourced human judgments on a subset of sentences.
GPT-3's judgments are broadly similar to human judgments and generally align
with proposed constraints in the literature but, in some cases, GPT-3's
judgments and human judgments diverge from the literature and from each other.",None,-1
2034b00a-eb79-4131-9ab2-6c3a4c0762de,Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features,0.704495,"A challenge towards developing NLP systems for the world's languages is
understanding how they generalize to typological differences relevant for
real-world applications. To this end, we propose M2C, a morphologically-aware
framework for behavioral testing of NLP models. We use M2C to generate tests
that probe models' behavior in light of specific linguistic features in 12
typologically diverse languages. We evaluate state-of-the-art language models
on the generated tests. While models excel at most tests in English, we
highlight generalization failures to specific typological characteristics such
as temporal expressions in Swahili and compounding possessives in Finish. Our
findings motivate the development of models that address these blind spots.",None,-1
8b90975d-d146-4602-8aeb-c4210ba2f12f,OPHAvatars: One-shot Photo-realistic Head Avatars,0.084743,"We propose a method for synthesizing photo-realistic digital avatars from
only one portrait as the reference. Given a portrait, our method synthesizes a
coarse talking head video using driving keypoints features. And with the coarse
video, our method synthesizes a coarse talking head avatar with a deforming
neural radiance field. With rendered images of the coarse avatar, our method
updates the low-quality images with a blind face restoration model. With
updated images, we retrain the avatar for higher quality. After several
iterations, our method can synthesize a photo-realistic animatable 3D neural
head avatar. The motivation of our method is deformable neural radiance field
can eliminate the unnatural distortion caused by the image2video method. Our
method outperforms state-of-the-art methods in quantitative and qualitative
studies on various subjects.",None,-1
9ff9384a-5c1a-4d92-8b14-e0d596fe5110,NaSGEC: a Multi-Domain Chinese Grammatical Error Correction Dataset from Native Speaker Texts,0.726314,"We introduce NaSGEC, a new dataset to facilitate research on Chinese
grammatical error correction (CGEC) for native speaker texts from multiple
domains. Previous CGEC research primarily focuses on correcting texts from a
single domain, especially learner essays. To broaden the target domain, we
annotate multiple references for 12,500 sentences from three native domains,
i.e., social media, scientific writing, and examination. We provide solid
benchmark results for NaSGEC by employing cutting-edge CGEC models and
different training data. We further perform detailed analyses of the
connections and gaps between our domains from both empirical and statistical
views. We hope this work can inspire future studies on an important but
under-explored direction--cross-domain GEC.",None,-1
74de9ea0-eac1-428c-8231-c3134f6c4f2b,Specification of MiniDemographicABM.jl: A simplified agent-based demographic model of the UK,0.115318,"This documentation specifies a simplified non-calibrated demographic
agent-based model of the UK, a largely simplified version of the Lone Parent
Model presented in [Gostolil and Silverman 2020]. In the presented model,
individuals of an initial population are subject to ageing, deaths, births,
divorces and marriages throughout a simplified map of towns of the UK. The
specification employs the formal terminology presented in [Elsheikh 2023a]. The
main purpose of the model is to explore and exploit capabilities of the
state-of-the-art Agents.jl Julia package [Datseris2022] in the context of
demographic modeling applications. Implementation is provided via the Julia
package MiniDemographicABM.jl [Elsheikh 2023b]. A specific simulation is
progressed with a user-defined simulation fixed step size on a hourly, daily,
weekly, monthly basis or even an arbitrary user-defined clock rate. The model
can serve for comparative studies if implemented in other agent-based modelling
frameworks and programming languages. Moreover, the model serves as a base
implementation to be adjusted to realistic large-scale socio-economics,
pandemics or immigration studies mainly within a demographic context.",None,-1
34eb7def-81a4-4c3a-a605-c57b13aeeba4,ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint,0.534718,"Large-scale online recommender system spreads all over the Internet being in
charge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion
Rate (CVR) estimations. However, traditional CVR estimators suffer from
well-known Sample Selection Bias and Data Sparsity issues. Entire space models
were proposed to address the two issues via tracing the decision-making path of
""exposure_click_purchase"". Further, some researchers observed that there are
purchase-related behaviors between click and purchase, which can better draw
the user's decision-making intention and improve the recommendation
performance. Thus, the decision-making path has been extended to
""exposure_click_in-shop action_purchase"" and can be modeled with conditional
probability approach. Nevertheless, we observe that the chain rule of
conditional probability does not always hold. We report Probability Space
Confusion (PSC) issue and give a derivation of difference between ground-truth
and estimation mathematically. We propose a novel Entire Space Multi-Task Model
for Post-Click Conversion Rate via Parameter Constraint (ESMC) and two
alternatives: Entire Space Multi-Task Model with Siamese Network (ESMS) and
Entire Space Multi-Task Model in Global Domain (ESMG) to address the PSC issue.
Specifically, we handle ""exposure_click_in-shop action"" and ""in-shop
action_purchase"" separately in the light of characteristics of in-shop action.
The first path is still treated with conditional probability while the second
one is treated with parameter constraint strategy. Experiments on both offline
and online environments in a large-scale recommendation system illustrate the
superiority of our proposed methods over state-of-the-art models. The
real-world datasets will be released.",None,-1
7521f8f1-493d-4252-a329-f33c9bbaa5fe,Catch Me If You Can: Improving Adversaries in Cyber-Security With Q-Learning Algorithms,0.583138,"The ongoing rise in cyberattacks and the lack of skilled professionals in the
cybersecurity domain to combat these attacks show the need for automated tools
capable of detecting an attack with good performance. Attackers disguise their
actions and launch attacks that consist of multiple actions, which are
difficult to detect. Therefore, improving defensive tools requires their
calibration against a well-trained attacker. In this work, we propose a model
of an attacking agent and environment and evaluate its performance using basic
Q-Learning, Naive Q-learning, and DoubleQ-Learning, all of which are variants
of Q-Learning. The attacking agent is trained with the goal of exfiltrating
data whereby all the hosts in the network have a non-zero detection
probability. Results show that the DoubleQ-Learning agent has the best overall
performance rate by successfully achieving the goal in $70\%$ of the
interactions.",None,-1
5c346228-1af2-40d8-b7e7-3804c70c72a4,Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark,0.964962,"Artificial intelligence (AI) systems possess significant potential to drive
societal progress. However, their deployment often faces obstacles due to
substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a
solution to optimize policies while simultaneously adhering to multiple
constraints, thereby addressing the challenge of integrating reinforcement
learning in safety-critical scenarios. In this paper, we present an environment
suite called Safety-Gymnasium, which encompasses safety-critical tasks in both
single and multi-agent scenarios, accepting vector and vision-only input.
Additionally, we offer a library of algorithms named Safe Policy Optimization
(SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive
library can serve as a validation tool for the research community. By
introducing this benchmark, we aim to facilitate the evaluation and comparison
of safety performance, thus fostering the development of reinforcement learning
for safer, more reliable, and responsible real-world applications. The website
of this project can be accessed at
https://sites.google.com/view/safety-gymnasium.",None,-1
08f552d8-438f-4654-8d8f-b43fcca3a834,SMPConv: Self-moving Point Representations for Continuous Convolution,0.758071,"Continuous convolution has recently gained prominence due to its ability to
handle irregularly sampled data and model long-term dependency. Also, the
promising experimental results of using large convolutional kernels have
catalyzed the development of continuous convolution since they can construct
large kernels very efficiently. Leveraging neural networks, more specifically
multilayer perceptrons (MLPs), is by far the most prevalent approach to
implementing continuous convolution. However, there are a few drawbacks, such
as high computational costs, complex hyperparameter tuning, and limited
descriptive power of filters. This paper suggests an alternative approach to
building a continuous convolution without neural networks, resulting in more
computationally efficient and improved performance. We present self-moving
point representations where weight parameters freely move, and interpolation
schemes are used to implement continuous functions. When applied to construct
convolutional kernels, the experimental results have shown improved performance
with drop-in replacement in the existing frameworks. Due to its lightweight
structure, we are first to demonstrate the effectiveness of continuous
convolution in a large-scale setting, e.g., ImageNet, presenting the
improvements over the prior arts. Our code is available on
https://github.com/sangnekim/SMPConv",None,-1
7c443f7f-c864-4770-96ca-f6763bc4accd,Edit Everything: A Text-Guided Generative System for Images Editing,0.603802,"We introduce a new generative system called Edit Everything, which can take
image and text inputs and produce image outputs. Edit Everything allows users
to edit images using simple text instructions. Our system designs prompts to
guide the visual module in generating requested images. Experiments demonstrate
that Edit Everything facilitates the implementation of the visual aspects of
Stable Diffusion with the use of Segment Anything model and CLIP. Our system is
publicly available at https://github.com/DefengXie/Edit_Everything.",None,-1
2a0c2f20-4e05-40cb-9b59-16c2f9479cc9,Simple Token-Level Confidence Improves Caption Correctness,0.143343,"The ability to judge whether a caption correctly describes an image is a
critical part of vision-language understanding. However, state-of-the-art
models often misinterpret the correctness of fine-grained details, leading to
errors in outputs such as hallucinating objects in generated captions or poor
compositional reasoning. In this work, we explore Token-Level Confidence, or
TLC, as a simple yet surprisingly effective method to assess caption
correctness. Specifically, we fine-tune a vision-language model on image
captioning, input an image and proposed caption to the model, and aggregate
either algebraic or learned token confidences over words or sequences to
estimate image-caption consistency. Compared to sequence-level scores from
pretrained models, TLC with algebraic confidence measures achieves a relative
improvement in accuracy by 10% on verb understanding in SVO-Probes and
outperforms prior state-of-the-art in image and group scores for compositional
reasoning in Winoground by a relative 37% and 9%, respectively. When training
data are available, a learned confidence estimator provides further improved
performance, reducing object hallucination rates in MS COCO Captions by a
relative 30% over the original model and setting a new state-of-the-art.",None,-1
3663ef41-b4a6-4fed-8059-813c512433d0,Traj-MAE: Masked Autoencoders for Trajectory Prediction,0.864259,"Trajectory prediction has been a crucial task in building a reliable
autonomous driving system by anticipating possible dangers. One key issue is to
generate consistent trajectory predictions without colliding. To overcome the
challenge, we propose an efficient masked autoencoder for trajectory prediction
(Traj-MAE) that better represents the complicated behaviors of agents in the
driving environment. Specifically, our Traj-MAE employs diverse masking
strategies to pre-train the trajectory encoder and map encoder, allowing for
the capture of social and temporal information among agents while leveraging
the effect of environment from multiple granularities. To address the
catastrophic forgetting problem that arises when pre-training the network with
multiple masking strategies, we introduce a continual pre-training framework,
which can help Traj-MAE learn valuable and diverse information from various
strategies efficiently. Our experimental results in both multi-agent and
single-agent settings demonstrate that Traj-MAE achieves competitive results
with state-of-the-art methods and significantly outperforms our baseline model.",None,-1
143b519a-f32d-455f-985a-ef34d8e44479,Diverse Inpainting and Editing with GAN Inversion,0.793123,"Recent inversion methods have shown that real images can be inverted into
StyleGAN's latent space and numerous edits can be achieved on those images
thanks to the semantically rich feature representations of well-trained GAN
models. However, extensive research has also shown that image inversion is
challenging due to the trade-off between high-fidelity reconstruction and
editability. In this paper, we tackle an even more difficult task, inverting
erased images into GAN's latent space for realistic inpaintings and editings.
Furthermore, by augmenting inverted latent codes with different latent samples,
we achieve diverse inpaintings. Specifically, we propose to learn an encoder
and mixing network to combine encoded features from erased images with
StyleGAN's mapped features from random samples. To encourage the mixing network
to utilize both inputs, we train the networks with generated data via a novel
set-up. We also utilize higher-rate features to prevent color inconsistencies
between the inpainted and unerased parts. We run extensive experiments and
compare our method with state-of-the-art inversion and inpainting methods.
Qualitative metrics and visual comparisons show significant improvements.",None,-1
dba3f44b-26ed-4dd0-bfa4-0b836a52a5ee,Predicting Spine Geometry and Scoliosis from DXA Scans,0.551582,"Our objective in this paper is to estimate spine curvature in DXA scans. To
this end we first train a neural network to predict the middle spine curve in
the scan, and then use an integral-based method to determine the curvature
along the spine curve. We use the curvature to compare to the standard angle
scoliosis measure obtained using the DXA Scoliosis Method (DSM). The
performance improves over the prior work of Jamaludin et al. 2018. We show that
the maximum curvature can be used as a scoring function for ordering the
severity of spinal deformation.",None,-1
4c0d9ffb-73fc-437a-8203-9d1aac9f231d,Ambigram Generation by A Diffusion Model,0.0399802,"Ambigrams are graphical letter designs that can be read not only from the
original direction but also from a rotated direction (especially with 180
degrees). Designing ambigrams is difficult even for human experts because
keeping their dual readability from both directions is often difficult. This
paper proposes an ambigram generation model. As its generation module, we use a
diffusion model, which has recently been used to generate high-quality
photographic images. By specifying a pair of letter classes, such as 'A' and
'B', the proposed model generates various ambigram images which can be read as
'A' from the original direction and 'B' from a direction rotated 180 degrees.
Quantitative and qualitative analyses of experimental results show that the
proposed model can generate high-quality and diverse ambigrams. In addition, we
define ambigramability, an objective measure of how easy it is to generate
ambigrams for each letter pair. For example, the pair of 'A' and 'V' shows a
high ambigramability (that is, it is easy to generate their ambigrams), and the
pair of 'D' and 'K' shows a lower ambigramability. The ambigramability gives
various hints of the ambigram generation not only for computers but also for
human experts. The code can be found at
(https://github.com/univ-esuty/ambifusion).",None,-1
5e64521f-7917-4cad-a215-77612771e45f,Prototypes-oriented Transductive Few-shot Learning with Conditional Transport,0.460152,"Transductive Few-Shot Learning (TFSL) has recently attracted increasing
attention since it typically outperforms its inductive peer by leveraging
statistics of query samples. However, previous TFSL methods usually encode
uniform prior that all the classes within query samples are equally likely,
which is biased in imbalanced TFSL and causes severe performance degradation.
  Given this pivotal issue, in this work, we propose a novel Conditional
Transport (CT) based imbalanced TFSL model called {\textbf P}rototypes-oriented
{\textbf U}nbiased {\textbf T}ransfer {\textbf M}odel (PUTM) to fully exploit
unbiased statistics of imbalanced query samples, which employs forward and
backward navigators as transport matrices to balance the prior of query samples
per class between uniform and adaptive data-driven distributions. For
efficiently transferring statistics learned by CT, we further derive a closed
form solution to refine prototypes based on MAP given the learned navigators.
The above two steps of discovering and transferring unbiased statistics follow
an iterative manner, formulating our EM-based solver.
  Experimental results on four standard benchmarks including miniImageNet,
tieredImageNet, CUB, and CIFAR-FS demonstrate superiority of our model in
class-imbalanced generalization.",None,-1
dccda7d7-7712-463e-8d83-a69a0d64c3ef,HRDoc: Dataset and Baseline Method Toward Hierarchical Reconstruction of Document Structures,0.519773,"The problem of document structure reconstruction refers to converting digital
or scanned documents into corresponding semantic structures. Most existing
works mainly focus on splitting the boundary of each element in a single
document page, neglecting the reconstruction of semantic structure in
multi-page documents. This paper introduces hierarchical reconstruction of
document structures as a novel task suitable for NLP and CV fields. To better
evaluate the system performance on the new task, we built a large-scale dataset
named HRDoc, which consists of 2,500 multi-page documents with nearly 2 million
semantic units. Every document in HRDoc has line-level annotations including
categories and relations obtained from rule-based extractors and human
annotators. Moreover, we proposed an encoder-decoder-based hierarchical
document structure parsing system (DSPS) to tackle this problem. By adopting a
multi-modal bidirectional encoder and a structure-aware GRU decoder with
soft-mask operation, the DSPS model surpass the baseline method by a large
margin. All scripts and datasets will be made publicly available at
https://github.com/jfma-USTC/HRDoc.",None,-1
44d67706-cc82-402b-af17-576a35dab688,DeepSeq: Deep Sequential Circuit Learning,0.213055,"Circuit representation learning is a promising research direction in the
electronic design automation (EDA) field. With sufficient data for
pre-training, the learned general yet effective representation can help to
solve multiple downstream EDA tasks by fine-tuning it on a small set of
task-related data. However, existing solutions only target combinational
circuits, significantly limiting their applications. In this work, we propose
DeepSeq, a novel representation learning framework for sequential netlists.
Specifically, we introduce a dedicated graph neural network (GNN) with a
customized propagation scheme to exploit the temporal correlations between
gates in sequential circuits. To ensure effective learning, we propose to use a
multi-task training objective with two sets of strongly related supervision:
logic probability and transition probability at each node. A novel dual
attention aggregation mechanism is introduced to facilitate learning both tasks
efficiently. Experimental results on various benchmark circuits show that
DeepSeq outperforms other GNN models for sequential circuit learning. We
evaluate the generalization capability of DeepSeq on a downstream power
estimation task. After fine-tuning, DeepSeq can accurately estimate power
across various circuits under different workloads.",None,-1
36965a06-1b98-4ac9-9c50-9d5ac5875529,Multiple Thinking Achieving Meta-Ability Decoupling for Object Navigation,0.801062,"We propose a meta-ability decoupling (MAD) paradigm, which brings together
various object navigation methods in an architecture system, allowing them to
mutually enhance each other and evolve together. Based on the MAD paradigm, we
design a multiple thinking (MT) model that leverages distinct thinking to
abstract various meta-abilities. Our method decouples meta-abilities from three
aspects: input, encoding, and reward while employing the multiple thinking
collaboration (MTC) module to promote mutual cooperation between thinking. MAD
introduces a novel qualitative and quantitative interpretability system for
object navigation. Through extensive experiments on AI2-Thor and RoboTHOR, we
demonstrate that our method outperforms state-of-the-art (SOTA) methods on both
typical and zero-shot object navigation tasks.",None,-1
3879f7fc-2554-4751-aa94-e6e2cbd89488,Neural Invertible Variable-degree Optical Aberrations Correction,0.557554,"Optical aberrations of optical systems cause significant degradation of
imaging quality. Aberration correction by sophisticated lens designs and
special glass materials generally incurs high cost of manufacturing and the
increase in the weight of optical systems, thus recent work has shifted to
aberration correction with deep learning-based post-processing. Though
real-world optical aberrations vary in degree, existing methods cannot
eliminate variable-degree aberrations well, especially for the severe degrees
of degradation. Also, previous methods use a single feed-forward neural network
and suffer from information loss in the output. To address the issues, we
propose a novel aberration correction method with an invertible architecture by
leveraging its information-lossless property. Within the architecture, we
develop conditional invertible blocks to allow the processing of aberrations
with variable degrees. Our method is evaluated on both a synthetic dataset from
physics-based imaging simulation and a real captured dataset. Quantitative and
qualitative experimental results demonstrate that our method outperforms
compared methods in correcting variable-degree optical aberrations.",None,-1
eb36e54c-754d-48f5-82e6-b1339d304976,Text2Tex: Text-driven Texture Synthesis via Diffusion Models,0.953282,"We present Text2Tex, a novel method for generating high-quality textures for
3D meshes from the given text prompts. Our method incorporates inpainting into
a pre-trained depth-aware image diffusion model to progressively synthesize
high resolution partial textures from multiple viewpoints. To avoid
accumulating inconsistent and stretched artifacts across views, we dynamically
segment the rendered view into a generation mask, which represents the
generation status of each visible texel. This partitioned view representation
guides the depth-aware inpainting model to generate and update partial textures
for the corresponding regions. Furthermore, we propose an automatic view
sequence generation scheme to determine the next best view for updating the
partial texture. Extensive experiments demonstrate that our method
significantly outperforms the existing text-driven approaches and GAN-based
methods.",None,-1
549a7d24-ede5-4cdf-944d-3c2b60d5c1ad,Dynamic Coarse-to-Fine Learning for Oriented Tiny Object Detection,0.624324,"Detecting arbitrarily oriented tiny objects poses intense challenges to
existing detectors, especially for label assignment. Despite the exploration of
adaptive label assignment in recent oriented object detectors, the extreme
geometry shape and limited feature of oriented tiny objects still induce severe
mismatch and imbalance issues. Specifically, the position prior, positive
sample feature, and instance are mismatched, and the learning of extreme-shaped
objects is biased and unbalanced due to little proper feature supervision. To
tackle these issues, we propose a dynamic prior along with the coarse-to-fine
assigner, dubbed DCFL. For one thing, we model the prior, label assignment, and
object representation all in a dynamic manner to alleviate the mismatch issue.
For another, we leverage the coarse prior matching and finer posterior
constraint to dynamically assign labels, providing appropriate and relatively
balanced supervision for diverse instances. Extensive experiments on six
datasets show substantial improvements to the baseline. Notably, we obtain the
state-of-the-art performance for one-stage detectors on the DOTA-v1.5,
DOTA-v2.0, and DIOR-R datasets under single-scale training and testing. Codes
are available at https://github.com/Chasel-Tsui/mmrotate-dcfl.",None,-1
251b186c-01d9-43ee-8eaa-33c182adee87,Input Reconstruction Attack against Vertical Federated Large Language Models,0.335759,"Recently, large language models (LLMs) have drawn extensive attention from
academia and the public, due to the advent of the ChatGPT. While LLMs show
their astonishing ability in text generation for various tasks, privacy
concerns limit their usage in real-life businesses. More specifically, either
the user's inputs (the user sends the query to the model-hosting server) or the
model (the user downloads the complete model) itself will be revealed during
the usage. Vertical federated learning (VFL) is a promising solution to this
kind of problem. It protects both the user's input and the knowledge of the
model by splitting the model into a bottom part and a top part, which is
maintained by the user and the model provider, respectively. However, in this
paper, we demonstrate that in LLMs, VFL fails to protect the user input since
it is simple and cheap to reconstruct the input from the intermediate
embeddings. Experiments show that even with a commercial GPU, the input
sentence can be reconstructed in only one second. We also discuss several
possible solutions to enhance the privacy of vertical federated LLMs.",None,-1
0ada3f56-ac25-40ed-bab6-a6c37ee52838,UUKG: Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal Prediction,0.625422,"Accurate Urban SpatioTemporal Prediction (USTP) is of great importance to the
development and operation of the smart city. As an emerging building block,
multi-sourced urban data are usually integrated as urban knowledge graphs
(UrbanKGs) to provide critical knowledge for urban spatiotemporal prediction
models. However, existing UrbanKGs are often tailored for specific downstream
prediction tasks and are not publicly available, which limits the potential
advancement. This paper presents UUKG, the unified urban knowledge graph
dataset for knowledge-enhanced urban spatiotemporal predictions. Specifically,
we first construct UrbanKGs consisting of millions of triplets for two
metropolises by connecting heterogeneous urban entities such as administrative
boroughs, POIs, and road segments. Moreover, we conduct qualitative and
quantitative analysis on constructed UrbanKGs and uncover diverse high-order
structural patterns, such as hierarchies and cycles, that can be leveraged to
benefit downstream USTP tasks. To validate and facilitate the use of UrbanKGs,
we implement and evaluate 15 KG embedding methods on the KG completion task and
integrate the learned KG embeddings into 9 spatiotemporal models for five
different USTP tasks. The extensive experimental results not only provide
benchmarks of knowledge-enhanced USTP models under different task settings but
also highlight the potential of state-of-the-art high-order structure-aware
UrbanKG embedding methods. We hope the proposed UUKG fosters research on urban
knowledge graphs and broad smart city applications. The dataset and source code
are available at https://github.com/usail-hkust/UUKG/.",None,-1
c4419a69-b798-47f5-97bc-8dfc9e8ed7ee,Suspicious Vehicle Detection Using Licence Plate Detection And Facial Feature Recognition,0.0929357,"With the increasing need to strengthen vehicle safety and detection, the
availability of pre-existing methods of catching criminals and identifying
vehicles manually through the various traffic surveillance cameras is not only
time-consuming but also inefficient. With the advancement of technology in
every field the use of real-time traffic surveillance models will help
facilitate an easy approach. Keeping this in mind, the main focus of our paper
is to develop a combined face recognition and number plate recognition model to
ensure vehicle safety and real-time tracking of running-away criminals and
stolen vehicles.",None,-1
813527b7-59fc-45f6-9e7a-cfb003afb2d4,Center Contrastive Loss for Metric Learning,0.127683,"Contrastive learning is a major studied topic in metric learning. However,
sampling effective contrastive pairs remains a challenge due to factors such as
limited batch size, imbalanced data distribution, and the risk of overfitting.
In this paper, we propose a novel metric learning function called Center
Contrastive Loss, which maintains a class-wise center bank and compares the
category centers with the query data points using a contrastive loss. The
center bank is updated in real-time to boost model convergence without the need
for well-designed sample mining. The category centers are well-optimized
classification proxies to re-balance the supervisory signal of each class.
Furthermore, the proposed loss combines the advantages of both contrastive and
classification methods by reducing intra-class variations and enhancing
inter-class differences to improve the discriminative power of embeddings. Our
experimental results, as shown in Figure 1, demonstrate that a standard network
(ResNet50) trained with our loss achieves state-of-the-art performance and
faster convergence.",None,-1
1ade9384-0aed-490b-b5b2-6ab662525443,Explicitly Minimizing the Blur Error of Variational Autoencoders,0.496053,"Variational autoencoders (VAEs) are powerful generative modelling methods,
however they suffer from blurry generated samples and reconstructions compared
to the images they have been trained on. Significant research effort has been
spent to increase the generative capabilities by creating more flexible models
but often flexibility comes at the cost of higher complexity and computational
cost. Several works have focused on altering the reconstruction term of the
evidence lower bound (ELBO), however, often at the expense of losing the
mathematical link to maximizing the likelihood of the samples under the modeled
distribution. Here we propose a new formulation of the reconstruction term for
the VAE that specifically penalizes the generation of blurry images while at
the same time still maximizing the ELBO under the modeled distribution. We show
the potential of the proposed loss on three different data sets, where it
outperforms several recently proposed reconstruction losses for VAEs.",None,-1
25a40d11-57b3-4d1c-935d-e136e0eb2e25,No Offense Taken: Eliciting Offensiveness from Language Models,0.0700523,"This work was completed in May 2022.
  For safe and reliable deployment of language models in the real world,
testing needs to be robust. This robustness can be characterized by the
difficulty and diversity of the test cases we evaluate these models on.
Limitations in human-in-the-loop test case generation has prompted an advent of
automated test case generation approaches. In particular, we focus on Red
Teaming Language Models with Language Models by Perez et al.(2022). Our
contributions include developing a pipeline for automated test case generation
via red teaming that leverages publicly available smaller language models
(LMs), experimenting with different target LMs and red classifiers, and
generating a corpus of test cases that can help in eliciting offensive
responses from widely deployed LMs and identifying their failure modes.",None,-1
c5de7f59-e789-4fec-aea9-8328b21d508c,Two-Stage Learning For the Flexible Job Shop Scheduling Problem,0.432553,"The Flexible Job-shop Scheduling Problem (FJSP) is an important combinatorial
optimization problem that arises in manufacturing and service settings. FJSP is
composed of two subproblems, an assignment problem that assigns tasks to
machines, and a scheduling problem that determines the starting times of tasks
on their chosen machines. Solving FJSP instances of realistic size and
composition is an ongoing challenge even under simplified, deterministic
assumptions. Motivated by the inevitable randomness and uncertainties in supply
chains, manufacturing, and service operations, this paper investigates the
potential of using a deep learning framework to generate fast and accurate
approximations for FJSP. In particular, this paper proposes a two-stage
learning framework 2SLFJSP that explicitly models the hierarchical nature of
FJSP decisions, uses a confidence-aware branching scheme to generate
appropriate instances for the scheduling stage from the assignment predictions
and leverages a novel symmetry-breaking formulation to improve learnability.
2SL-FJSP is evaluated on instances from the FJSP benchmark library. Results
show that 2SL-FJSP can generate high-quality solutions in milliseconds,
outperforming a state-of-the-art reinforcement learning approach recently
proposed in the literature, and other heuristics commonly used in practice.",None,-1
facdecb2-b11b-4564-a317-ebccd93b2d10,Adaptive Control of Resource Flow to Optimize Construction Work and Cash Flow via Online Deep Reinforcement Learning,0.981726,"Due to complexity and dynamics of construction work, resource, and cash
flows, poor management of them usually leads to time and cost overruns,
bankruptcy, even project failure. Existing approaches in construction failed to
achieve optimal control of resource flow in a dynamic environment with
uncertainty. Therefore, this paper introducess a model and method to adaptive
control the resource flows to optimize the work and cash flows of construction
projects. First, a mathematical model based on a partially observable Markov
decision process is established to formulate the complex interactions of
construction work, resource, and cash flows as well as uncertainty and
variability of diverse influence factors. Meanwhile, to efficiently find the
optimal solutions, a deep reinforcement learning (DRL) based method is
introduced to realize the continuous adaptive optimal control of labor and
material flows, thereby optimizing the work and cash flows. To assist the
training process of DRL, a simulator based on discrete event simulation is also
developed to mimic the dynamic features and external environments of a project.
Experiments in simulated scenarios illustrate that our method outperforms the
vanilla empirical method and genetic algorithm, possesses remarkable capability
in diverse projects and external environments, and a hybrid agent of DRL and
empirical method leads to the best result. This paper contributes to adaptive
control and optimization of coupled work, resource, and cash flows, and may
serve as a step stone for adopting DRL technology in construction project
management.",None,-1
14825bf0-c13d-4622-8700-1bf7e6c27dbe,Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control,0.794954,"Building agents with large language models (LLMs) for computer control is a
burgeoning research area, where the agent receives computer states and performs
actions to complete complex tasks. Previous computer agents have demonstrated
the benefits of in-context learning (ICL); however, their performance is
hindered by several issues. First, the limited context length of LLMs and
complex computer states restrict the number of exemplars, as a single webpage
can consume the entire context. Second, the exemplars in current methods, such
as high-level plans and multi-choice questions, cannot represent complete
trajectories, leading to suboptimal performance in long-horizon tasks. Third,
existing computer agents rely on task-specific exemplars and overlook the
similarity among tasks, resulting in poor generalization to novel tasks. To
address these challenges, we introduce Synapse, a computer agent featuring
three key components: i) state abstraction, which filters out task-irrelevant
information from raw states, allowing more exemplars within the limited
context, ii) trajectory-as-exemplar prompting, which prompts the LLM with
complete trajectories of the abstracted states and actions to improve
multi-step decision-making, and iii) exemplar memory, which stores the
embeddings of exemplars and retrieves them via similarity search for
generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard
task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse
achieves a 99.2% average success rate (a 10% relative improvement) across 64
tasks using demonstrations from only 48 tasks. Notably, Synapse is the first
ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a
56% relative improvement in average step success rate over the previous
state-of-the-art prompting scheme in Mind2Web.",None,-1
8e0d4e04-5a60-4e3e-bf92-b0984dd37385,An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts,0.892396,"With a surge in identifying suicidal risk and its severity in social media
posts, we argue that a more consequential and explainable research is required
for optimal impact on clinical psychology practice and personalized mental
healthcare. The success of computational intelligence techniques for inferring
mental illness from social media resources, points to natural language
processing as a lens for determining Interpersonal Risk Factors (IRF) in human
writings. Motivated with limited availability of datasets for social NLP
research community, we construct and release a new annotated dataset with
human-labelled explanations and classification of IRF affecting mental
disturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)
Perceived Burdensomeness (PBu). We establish baseline models on our dataset
facilitating future research directions to develop real-time personalized AI
models by detecting patterns of TBe and PBu in emotional spectrum of user's
historical social media profile.",None,-1
653317ee-54d3-4107-aa04-2a35583fbcbd,ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought,0.857491,"Recently Large Language Models (LLMs) have been proven to have strong
abilities in various domains and tasks. We study the problem of prompt
designing in the text-to-SQL task and attempt to improve the LLMs' reasoning
ability when generating SQL queries. Besides the trivial few-shot in-context
learning setting, we design our chain-of-thought (CoT) prompt with a similar
method to schema linking. We provide a method named ACT-SQL to automatically
generate auto-CoT exemplars and thus the whole process doesn't need manual
labeling. Our approach is cost-saving since we only use the LLMs' API call once
when generating one SQL query. Furthermore, we extend our in-context learning
method to the multi-turn text-to-SQL task. The experiment results show that the
LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves
SOTA performance on the Spider dev set among existing in-context learning
approaches.",None,-1
6780118d-26aa-4272-883b-21d75b52f480,Mind the Gap: Offline Policy Optimization for Imperfect Rewards,0.299024,"Reward function is essential in reinforcement learning (RL), serving as the
guiding signal to incentivize agents to solve given tasks, however, is also
notoriously difficult to design. In many cases, only imperfect rewards are
available, which inflicts substantial performance loss for RL agents. In this
study, we propose a unified offline policy optimization approach, \textit{RGM
(Reward Gap Minimization)}, which can smartly handle diverse types of imperfect
rewards. RGM is formulated as a bi-level optimization problem: the upper layer
optimizes a reward correction term that performs visitation distribution
matching w.r.t. some expert data; the lower layer solves a pessimistic RL
problem with the corrected rewards. By exploiting the duality of the lower
layer, we derive a tractable algorithm that enables sampled-based learning
without any online interactions. Comprehensive experiments demonstrate that RGM
achieves superior performance to existing methods under diverse settings of
imperfect rewards. Further, RGM can effectively correct wrong or inconsistent
rewards against expert preference and retrieve useful information from biased
rewards.",None,-1
1de1b038-3c00-4dee-bb47-f8a9a108199e,Inverting the Imaging Process by Learning an Implicit Camera Model,0.316549,"Representing visual signals with implicit coordinate-based neural networks,
as an effective replacement of the traditional discrete signal representation,
has gained considerable popularity in computer vision and graphics. In contrast
to existing implicit neural representations which focus on modelling the scene
only, this paper proposes a novel implicit camera model which represents the
physical imaging process of a camera as a deep neural network. We demonstrate
the power of this new implicit camera model on two inverse imaging tasks: i)
generating all-in-focus photos, and ii) HDR imaging. Specifically, we devise an
implicit blur generator and an implicit tone mapper to model the aperture and
exposure of the camera's imaging process, respectively. Our implicit camera
model is jointly learned together with implicit scene models under multi-focus
stack and multi-exposure bracket supervision. We have demonstrated the
effectiveness of our new model on a large number of test images and videos,
producing accurate and visually appealing all-in-focus and high dynamic range
images. In principle, our new implicit neural camera model has the potential to
benefit a wide array of other inverse imaging tasks.",None,-1
9f9fd06a-6fa1-4fc8-840e-8707df2daab9,All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations,0.301672,"Transformer models bring propelling advances in various NLP tasks, thus
inducing lots of interpretability research on the learned representations of
the models. However, we raise a fundamental question regarding the reliability
of the representations. Specifically, we investigate whether transformers learn
essentially isomorphic representation spaces, or those that are sensitive to
the random seeds in their pretraining process. In this work, we formulate the
Bijection Hypothesis, which suggests the use of bijective methods to align
different models' representation spaces. We propose a model based on invertible
neural networks, BERT-INN, to learn the bijection more effectively than other
existing bijective methods such as the canonical correlation analysis (CCA). We
show the advantage of BERT-INN both theoretically and through extensive
experiments, and apply it to align the reproduced BERT embeddings to draw
insights that are meaningful to the interpretability research. Our code is at
https://github.com/twinkle0331/BERT-similarity.",None,-1
62a929c9-5738-4a44-b113-efec88c27ff9,SPRING: Studying the Paper and Reasoning to Play Games,0.922154,"Open-world survival games pose significant challenges for AI algorithms due
to their multi-tasking, deep exploration, and goal prioritization requirements.
Despite reinforcement learning (RL) being popular for solving games, its high
sample complexity limits its effectiveness in complex open-world games like
Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's
original academic paper and use the knowledge learned to reason and play the
game through a large language model (LLM). Prompted with the LaTeX source as
game context and a description of the agent's current observation, our SPRING
framework employs a directed acyclic graph (DAG) with game-related questions as
nodes and dependencies as edges. We identify the optimal action to take in the
environment by traversing the DAG and calculating LLM responses for each node
in topological order, with the LLM's answer to final node directly translating
to environment actions. In our experiments, we study the quality of in-context
""reasoning"" induced by different forms of prompts under the setting of the
Crafter open-world environment. Our experiments suggest that LLMs, when
prompted with consistent chain-of-thought, have great potential in completing
sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4
outperforms all state-of-the-art RL baselines, trained for 1M steps, without
any training. Finally, we show the potential of games as a test bed for LLMs.",None,-1
496e7de2-f2dd-434d-b295-f7c383ef00af,Free Lunch for Efficient Textual Commonsense Integration in Language Models,0.118557,"Recent years have witnessed the emergence of textual commonsense knowledge
bases, aimed at providing more nuanced and context-rich knowledge. The
integration of external commonsense into language models has been shown to be a
key enabler in advancing the state-of-the-art for a wide range of NLP tasks.
However, incorporating textual commonsense descriptions is computationally
expensive, as compared to encoding conventional symbolic knowledge. In this
paper, we propose a method to improve its efficiency without modifying the
model. We group training samples with similar commonsense descriptions into a
single batch, thus reusing the encoded description across multiple samples. One
key observation is that the upper bound of batch partitioning can be reduced to
the classic {\it graph k-cut problem}. Consequently, we propose a spectral
clustering-based algorithm to solve this problem. Extensive experiments
illustrate that the proposed batch partitioning approach effectively reduces
the computational cost while preserving performance. The efficiency improvement
is more pronounced on larger datasets and on devices with more memory capacity,
attesting to its practical utility for large-scale applications.",None,-1
a1499604-6b17-4277-af89-e781e7d491b0,PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining,0.42551,"A rich representation is key to general robotic manipulation, but existing
approaches to representation learning require large amounts of multimodal
demonstrations. In this work we propose PLEX, a transformer-based architecture
that learns from a small amount of task-agnostic visuomotor trajectories and a
much larger amount of task-conditioned object manipulation videos -- a type of
data available in quantity. PLEX uses visuomotor trajectories to induce a
latent feature space and to learn task-agnostic manipulation routines, while
diverse video-only demonstrations teach PLEX how to plan in the induced latent
feature space for a wide variety of tasks. Experiments showcase PLEX's
generalization on Meta-World and SOTA performance in challenging Robosuite
environments. In particular, using relative positional encoding in PLEX's
transformers greatly helps in low-data regimes of learning from human-collected
demonstrations. The paper's accompanying code and data are available at
https://microsoft.github.io/PLEX.",None,-1
78e15f7c-9ad1-40f0-ab1c-d4a89b743f7c,LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation,0.61298,"Existing automatic evaluation on text-to-image synthesis can only provide an
image-text matching score, without considering the object-level
compositionality, which results in poor correlation with human judgments. In
this work, we propose LLMScore, a new framework that offers evaluation scores
with multi-granularity compositionality. LLMScore leverages the large language
models (LLMs) to evaluate text-to-image models. Initially, it transforms the
image into image-level and object-level visual descriptions. Then an evaluation
instruction is fed into the LLMs to measure the alignment between the
synthesized image and the text, ultimately generating a score accompanied by a
rationale. Our substantial analysis reveals the highest correlation of LLMScore
with human judgments on a wide range of datasets (Attribute Binding Contrast,
Concept Conjunction, MSCOCO, DrawBench, PaintSkills). Notably, our LLMScore
achieves Kendall's tau correlation with human evaluations that is 58.8% and
31.2% higher than the commonly-used text-image matching metrics CLIP and BLIP,
respectively.",None,-1
0ec16f31-15e9-4109-8b88-7fc1081aaf24,TCSloT: Text Guided 3D Context and Slope Aware Triple Network for Dental Implant Position Prediction,0.845083,"In implant prosthesis treatment, the surgical guide of implant is used to
ensure accurate implantation. However, such design heavily relies on the manual
location of the implant position. When deep neural network has been proposed to
assist the dentist in locating the implant position, most of them take a single
slice as input, which do not fully explore 3D contextual information and
ignoring the influence of implant slope. In this paper, we design a Text Guided
3D Context and Slope Aware Triple Network (TCSloT) which enables the perception
of contextual information from multiple adjacent slices and awareness of
variation of implant slopes. A Texture Variation Perception (TVP) module is
correspondingly elaborated to process the multiple slices and capture the
texture variation among slices and a Slope-Aware Loss (SAL) is proposed to
dynamically assign varying weights for the regression head. Additionally, we
design a conditional text guidance (CTG) module to integrate the text condition
(i.e., left, middle and right) from the CLIP for assisting the implant position
prediction. Extensive experiments on a dental implant dataset through five-fold
cross-validation demonstrated that the proposed TCSloT achieves superior
performance than existing methods.",None,-1
eb98d280-a593-468f-9f66-65cefd7eac6c,Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules,0.27274,"Automated game design (AGD), the study of automatically generating game
rules, has a long history in technical games research. AGD approaches generally
rely on approximations of human play, either objective functions or AI agents.
Despite this, the majority of these approximators are static, meaning they do
not reflect human player's ability to learn and improve in a game. In this
paper, we investigate the application of Reinforcement Learning (RL) as an
approximator for human play for rule generation. We recreate the classic AGD
environment Mechanic Maker in Unity as a new, open-source rule generation
framework. Our results demonstrate that RL produces distinct sets of rules from
an A* agent baseline, which may be more usable by humans.",None,-1
174f0460-9856-46dd-8e5f-4c195fb898ea,Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation,0.265924,"Code generation aims to automatically generate source code from high-level
task specifications, which can significantly increase productivity of software
engineering. Recently, approaches based on large language models (LLMs) have
shown remarkable code generation abilities on simple tasks. However, generate
code for more complex tasks, such as competition-level problems, remains
challenging. In this paper, we introduce Brainstorm framework for code
generation. It leverages a brainstorming step that generates and selects
diverse thoughts on the problem to facilitate algorithmic reasoning, where the
thoughts are possible blueprint of solving the problem. We demonstrate that
Brainstorm significantly enhances the ability of LLMs to solve
competition-level programming problems, resulting in a more than 50% increase
in the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving
state-of-the-art performance. Furthermore, our experiments conducted on
LeetCode contests show that our framework boosts the ability of ChatGPT to a
level comparable to that of human programmers.",None,-1
d85191c0-6e85-456c-916b-717175ace12f,Findings of the VarDial Evaluation Campaign 2023,0.410659,"This report presents the results of the shared tasks organized as part of the
VarDial Evaluation Campaign 2023. The campaign is part of the tenth workshop on
Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects
(VarDial), co-located with EACL 2023. Three separate shared tasks were included
this year: Slot and intent detection for low-resource language varieties
(SID4LR), Discriminating Between Similar Languages -- True Labels (DSL-TL), and
Discriminating Between Similar Languages -- Speech (DSL-S). All three tasks
were organized for the first time this year.",None,-1
0a64381f-a33f-428f-80b7-1479b20b765f,Implicit Neural Representation for Cooperative Low-light Image Enhancement,0.99553,"The following three factors restrict the application of existing low-light
image enhancement methods: unpredictable brightness degradation and noise,
inherent gap between metric-favorable and visual-friendly versions, and the
limited paired training data. To address these limitations, we propose an
implicit Neural Representation method for Cooperative low-light image
enhancement, dubbed NeRCo. It robustly recovers perceptual-friendly results in
an unsupervised manner. Concretely, NeRCo unifies the diverse degradation
factors of real-world scenes with a controllable fitting function, leading to
better robustness. In addition, for the output results, we introduce
semantic-orientated supervision with priors from the pre-trained
vision-language model. Instead of merely following reference images, it
encourages results to meet subjective expectations, finding more
visual-friendly solutions. Further, to ease the reliance on paired data and
reduce solution space, we develop a dual-closed-loop constrained enhancement
module. It is trained cooperatively with other affiliated modules in a
self-supervised manner. Finally, extensive experiments demonstrate the
robustness and superior effectiveness of our proposed NeRCo. Our code is
available at https://github.com/Ysz2022/NeRCo.",None,-1
8f5bc5f8-6eec-4eaf-a137-c4fd7dbab17f,Deep Clustering Survival Machines with Interpretable Expert Distributions,0.121201,"Conventional survival analysis methods are typically ineffective to
characterize heterogeneity in the population while such information can be used
to assist predictive modeling. In this study, we propose a hybrid survival
analysis method, referred to as deep clustering survival machines, that
combines the discriminative and generative mechanisms. Similar to the mixture
models, we assume that the timing information of survival data is generatively
described by a mixture of certain numbers of parametric distributions, i.e.,
expert distributions. We learn weights of the expert distributions for
individual instances according to their features discriminatively such that
each instance's survival information can be characterized by a weighted
combination of the learned constant expert distributions. This method also
facilitates interpretable subgrouping/clustering of all instances according to
their associated expert distributions. Extensive experiments on both real and
synthetic datasets have demonstrated that the method is capable of obtaining
promising clustering results and competitive time-to-event predicting
performance.",None,-1
dc3e91ba-d303-4af1-bba2-f373cb0e2ace,Design of Chain-of-Thought in Math Problem Solving,0.238756,"Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem
solving. We conduct a comprehensive examination of methods for designing CoT,
comparing conventional natural language CoT with various program CoTs,
including the self-describing program, the comment-describing program, and the
non-describing program. Furthermore, we investigate the impact of programming
language on program CoTs, comparing Python and Wolfram Language. Through
extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs
often have superior effectiveness in math problem solving. Notably, the best
performing combination with 30B parameters beats GPT-3.5-turbo by a significant
margin. The results show that self-describing program offers greater diversity
and thus can generally achieve higher performance. We also find that Python is
a better choice of language than Wolfram for program CoTs. The experimental
results provide a valuable guideline for future CoT designs that take into
account both programming language and coding style for further advancements.
Our datasets and code are publicly available.",None,-1
1123bc36-a85e-4fd9-a4ff-8668622fb17d,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,0.994785,"This paper demonstrates an approach for learning highly semantic image
representations without relying on hand-crafted data-augmentations. We
introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a
non-generative approach for self-supervised learning from images. The idea
behind I-JEPA is simple: from a single context block, predict the
representations of various target blocks in the same image. A core design
choice to guide I-JEPA towards producing semantic representations is the
masking strategy; specifically, it is crucial to (a) sample target blocks with
sufficiently large scale (semantic), and to (b) use a sufficiently informative
(spatially distributed) context block. Empirically, when combined with Vision
Transformers, we find I-JEPA to be highly scalable. For instance, we train a
ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong
downstream performance across a wide range of tasks, from linear classification
to object counting and depth prediction.",None,-1
a3ce2db1-6779-4c7a-af48-a6b1568d0502,Counterfactuals of Counterfactuals: a back-translation-inspired approach to analyse counterfactual editors,0.0762489,"In the wake of responsible AI, interpretability methods, which attempt to
provide an explanation for the predictions of neural models have seen rapid
progress. In this work, we are concerned with explanations that are applicable
to natural language processing (NLP) models and tasks, and we focus
specifically on the analysis of counterfactual, contrastive explanations. We
note that while there have been several explainers proposed to produce
counterfactual explanations, their behaviour can vary significantly and the
lack of a universal ground truth for the counterfactual edits imposes an
insuperable barrier on their evaluation. We propose a new back
translation-inspired evaluation methodology that utilises earlier outputs of
the explainer as ground truth proxies to investigate the consistency of
explainers. We show that by iteratively feeding the counterfactual to the
explainer we can obtain valuable insights into the behaviour of both the
predictor and the explainer models, and infer patterns that would be otherwise
obscured. Using this methodology, we conduct a thorough analysis and propose a
novel metric to evaluate the consistency of counterfactual generation
approaches with different characteristics across available performance
indicators.",None,-1
77157f50-e50f-4fdd-b2d9-53158d5b4194,Neural Airport Ground Handling,0.773387,"Airport ground handling (AGH) offers necessary operations to flights during
their turnarounds and is of great importance to the efficiency of airport
management and the economics of aviation. Such a problem involves the interplay
among the operations that leads to NP-hard problems with complex constraints.
Hence, existing methods for AGH are usually designed with massive domain
knowledge but still fail to yield high-quality solutions efficiently. In this
paper, we aim to enhance the solution quality and computation efficiency for
solving AGH. Particularly, we first model AGH as a multiple-fleet vehicle
routing problem (VRP) with miscellaneous constraints including precedence, time
windows, and capacity. Then we propose a construction framework that decomposes
AGH into sub-problems (i.e., VRPs) in fleets and present a neural method to
construct the routing solutions to these sub-problems. In specific, we resort
to deep learning and parameterize the construction heuristic policy with an
attention-based neural network trained with reinforcement learning, which is
shared across all sub-problems. Extensive experiments demonstrate that our
method significantly outperforms classic meta-heuristics, construction
heuristics and the specialized methods for AGH. Besides, we empirically verify
that our neural method generalizes well to instances with large numbers of
flights or varying parameters, and can be readily adapted to solve real-time
AGH with stochastic flight arrivals. Our code is publicly available at:
https://github.com/RoyalSkye/AGH.",None,-1
f18ea3f2-4d33-4c52-b827-d2e812872a00,Terminology-Aware Translation with Constrained Decoding and Large Language Model Prompting,0.375781,"Terminology correctness is important in the downstream application of machine
translation, and a prevalent way to ensure this is to inject terminology
constraints into a translation system. In our submission to the WMT 2023
terminology translation task, we adopt a translate-then-refine approach which
can be domain-independent and requires minimal manual efforts. We annotate
random source words with pseudo-terminology translations obtained from word
alignment to first train a terminology-aware model. Further, we explore two
post-processing methods. First, we use an alignment process to discover whether
a terminology constraint has been violated, and if so, we re-decode with the
violating word negatively constrained. Alternatively, we leverage a large
language model to refine a hypothesis by providing it with terminology
constraints. Results show that our terminology-aware model learns to
incorporate terminologies effectively, and the large language model refinement
process can further improve terminology recall.",None,-1
a356fd34-4d90-4006-a20c-e9f33ad43b78,Now It Sounds Like You: Learning Personalized Vocabulary On Device,0.0643602,"In recent years, Federated Learning (FL) has shown significant advancements
in its ability to perform various natural language processing (NLP) tasks. This
work focuses on applying personalized FL for on-device language modeling. Due
to limitations of memory and latency, these models cannot support the
complexity of sub-word tokenization or beam search decoding, resulting in the
decision to deploy a closed-vocabulary language model. However,
closed-vocabulary models are unable to handle out-of-vocabulary (OOV) words
belonging to specific users. To address this issue, We propose a novel
technique called ""OOV expansion"" that improves OOV coverage and increases model
accuracy while minimizing the impact on memory and latency. This method
introduces a personalized ""OOV adapter"" that effectively transfers knowledge
from a central model and learns word embedding for personalized vocabulary. OOV
expansion significantly outperforms standard FL personalization methods on a
set of common FL benchmarks.",None,-1
8c570ce6-c4ec-4144-baaf-44afccc28abb,Examining Modularity in Multilingual LMs via Language-Specialized Subnetworks,0.253528,"Recent work has proposed explicitly inducing language-wise modularity in
multilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as a
means of better guiding cross-lingual sharing. In this work, we investigate (1)
the degree to which language-wise modularity naturally arises within models
with no special modularity interventions, and (2) how cross-lingual sharing and
interference differ between such models and those with explicit SFT-guided
subnetwork modularity. To quantify language specialization and cross-lingual
interaction, we use a Training Data Attribution method that estimates the
degree to which a model's predictions are influenced by in-language or
cross-language training examples. Our results show that language-specialized
subnetworks do naturally arise, and that SFT, rather than always increasing
modularity, can decrease language specialization of subnetworks in favor of
more cross-lingual sharing.",None,-1
8b397a73-e315-4864-8aed-8dbdb24a2ef8,Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration,0.341098,"Human intelligence thrives on cognitive synergy, where collaboration among
different minds yield superior outcomes compared to isolated individuals. In
this work, we propose Solo Performance Prompting (SPP), which transforms a
single LLM into a cognitive synergist by engaging in multi-turn
self-collaboration with multiple personas. A cognitive synergist is an
intelligent agent that collaboratively combines multiple minds' strengths and
knowledge to enhance problem-solving in complex tasks. By dynamically
identifying and simulating different personas based on task inputs, SPP
unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis
shows that assigning multiple fine-grained personas in LLMs improves
problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,
experimental results demonstrate that SPP effectively reduces factual
hallucination, and maintains strong reasoning capabilities. Additionally,
comparative experiments show that cognitive synergy only emerges in GPT-4 and
does not appear in less capable models, such as GPT-3.5-turbo and
Llama2-13b-chat, which draws an interesting analogy to human development. Code,
data, and prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",None,-1
988bc631-5b4d-42c4-ae0c-e5f8795e476c,Automated Action Model Acquisition from Narrative Texts,0.596158,"Action models, which take the form of precondition/effect axioms, facilitate
causal and motivational connections between actions for AI agents. Action model
acquisition has been identified as a bottleneck in the application of planning
technology, especially within narrative planning. Acquiring action models from
narrative texts in an automated way is essential, but challenging because of
the inherent complexities of such texts. We present NaRuto, a system that
extracts structured events from narrative text and subsequently generates
planning-language-style action models based on predictions of commonsense event
relations, as well as textual contradictions and similarities, in an
unsupervised manner. Experimental results in classical narrative planning
domains show that NaRuto can generate action models of significantly better
quality than existing fully automated methods, and even on par with those of
semi-automated methods.",None,-1
61a292b4-d198-4506-952a-ec727286a361,A Quantum Computing-based System for Portfolio Optimization using Future Asset Values and Automatic Reduction of the Investment Universe,0.431541,"One of the problems in quantitative finance that has received the most
attention is the portfolio optimization problem. Regarding its solving, this
problem has been approached using different techniques, with those related to
quantum computing being especially prolific in recent years. In this study, we
present a system called Quantum Computing-based System for Portfolio
Optimization with Future Asset Values and Automatic Universe Reduction
(Q4FuturePOP), which deals with the Portfolio Optimization Problem considering
the following innovations: i) the developed tool is modeled for working with
future prediction of assets, instead of historical values; and ii) Q4FuturePOP
includes an automatic universe reduction module, which is conceived to
intelligently reduce the complexity of the problem. We also introduce a brief
discussion about the preliminary performance of the different modules that
compose the prototypical version of Q4FuturePOP.",None,-1
109bba67-66d2-4e50-8e59-23bd7d278a04,Topological Interpretations of GPT-3,0.056846,"This is an experiential study of investigating a consistent method for
deriving the correlation between sentence vector and semantic meaning of a
sentence. We first used three state-of-the-art word/sentence embedding methods
including GPT-3, Word2Vec, and Sentence-BERT, to embed plain text sentence
strings into high dimensional spaces. Then we compute the pairwise distance
between any possible combination of two sentence vectors in an embedding space
and map them into a matrix. Based on each distance matrix, we compute the
correlation of distances of a sentence vector with respect to the other
sentence vectors in an embedding space. Then we compute the correlation of each
pair of the distance matrices. We observed correlations of the same sentence in
different embedding spaces and correlations of different sentences in the same
embedding space. These observations are consistent with our hypothesis and take
us to the next stage.",None,-1
746f969a-2704-4269-91d3-6e06987a2015,Geographical Erasure in Language Generation,0.0362095,"Large language models (LLMs) encode vast amounts of world knowledge. However,
since these models are trained on large swaths of internet data, they are at
risk of inordinately capturing information about dominant groups. This
imbalance can propagate into generated language. In this work, we study and
operationalise a form of geographical erasure, wherein language models
underpredict certain countries. We demonstrate consistent instances of erasure
across a range of LLMs. We discover that erasure strongly correlates with low
frequencies of country mentions in the training corpus. Lastly, we mitigate
erasure by finetuning using a custom objective.",None,-1
1b462cde-fe6a-49a7-b6dd-2fb373950fe5,Temporally Extended Goal Recognition in Fully Observable Non-Deterministic Domain Models,0.103124,"Goal Recognition is the task of discerning the correct intended goal that an
agent aims to achieve, given a set of goal hypotheses, a domain model, and a
sequence of observations (i.e., a sample of the plan executed in the
environment). Existing approaches assume that goal hypotheses comprise a single
conjunctive formula over a single final state and that the environment dynamics
are deterministic, preventing the recognition of temporally extended goals in
more complex settings. In this paper, we expand goal recognition to temporally
extended goals in Fully Observable Non-Deterministic (FOND) planning domain
models, focusing on goals on finite traces expressed in Linear Temporal Logic
(LTLf) and Pure Past Linear Temporal Logic (PLTLf). We develop the first
approach capable of recognizing goals in such settings and evaluate it using
different LTLf and PLTLf goals over six FOND planning domain models. Empirical
results show that our approach is accurate in recognizing temporally extended
goals in different recognition settings.",None,-1
99794ea3-90db-4dc2-8881-2b427eff4488,Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs,0.327369,"With the rapid evolution of large language models (LLMs), new and
hard-to-predict harmful capabilities are emerging. This requires developers to
be able to identify risks through the evaluation of ""dangerous capabilities"" in
order to responsibly deploy LLMs. In this work, we collect the first
open-source dataset to evaluate safeguards in LLMs, and deploy safer
open-source LLMs at a low cost. Our dataset is curated and filtered to consist
only of instructions that responsible language models should not follow. We
annotate and assess the responses of six popular LLMs to these instructions.
Based on our annotation, we proceed to train several BERT-like classifiers, and
find that these small classifiers can achieve results that are comparable with
GPT-4 on automatic safety evaluation. Warning: this paper contains example data
that may be offensive, harmful, or biased.",None,-1
91635986-6bb6-4711-832d-82967cadada7,Revisiting Token Dropping Strategy in Efficient BERT Pretraining,0.453476,"Token dropping is a recently-proposed strategy to speed up the pretraining of
masked language models, such as BERT, by skipping the computation of a subset
of the input tokens at several middle layers. It can effectively reduce the
training time without degrading much performance on downstream tasks. However,
we empirically find that token dropping is prone to a semantic loss problem and
falls short in handling semantic-intense tasks. Motivated by this, we propose a
simple yet effective semantic-consistent learning method (ScTD) to improve the
token dropping. ScTD aims to encourage the model to learn how to preserve the
semantic information in the representation space. Extensive experiments on 12
tasks show that, with the help of our ScTD, token dropping can achieve
consistent and significant performance gains across all task types and model
sizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings
up to +1.56% average improvement over the vanilla token dropping.",None,-1
dfb90138-ba80-47ac-84b6-3ce6ae5a7187,"Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media",0.405171,"We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor
detecting hate speech in online social networks such as Reddit discussions. In
contrast to traditional comment-only methods, our approach to labelling a
comment as hate speech involves a holistic analysis of text and images grounded
in the discussion context. This is done by leveraging graph transformers to
capture the contextual relationships in the discussion surrounding a comment
and grounding the interwoven fusion layers that combine text and image
embeddings instead of processing modalities separately. To evaluate our work,
we present a new dataset, HatefulDiscussions, comprising complete multi-modal
discussions from multiple online communities on Reddit. We compare the
performance of our model to baselines that only process individual comments and
conduct extensive ablation studies.",None,-1
be39d8c1-91d5-4c01-8b16-0d9c7977d5a0,Domain Generalization Emerges from Dreaming,0.0732368,"Recent studies have proven that DNNs, unlike human vision, tend to exploit
texture information rather than shape. Such texture bias is one of the factors
for the poor generalization performance of DNNs. We observe that the texture
bias negatively affects not only in-domain generalization but also
out-of-distribution generalization, i.e., Domain Generalization. Motivated by
the observation, we propose a new framework to reduce the texture bias of a
model by a novel optimization-based data augmentation, dubbed Stylized Dream.
Our framework utilizes adaptive instance normalization (AdaIN) to augment the
style of an original image yet preserve the content. We then adopt a
regularization loss to predict consistent outputs between Stylized Dream and
original images, which encourages the model to learn shape-based
representations. Extensive experiments show that the proposed method achieves
state-of-the-art performance in out-of-distribution settings on public
benchmark datasets: PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet.",None,-1
16bdd8c9-52cb-4e6d-94d6-ed21cc7ee306,Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation,0.662958,"Most of the speech translation models heavily rely on parallel data, which is
hard to collect especially for low-resource languages. To tackle this issue, we
propose to build a cascaded speech translation system without leveraging any
kind of paired data. We use fully unpaired data to train our unsupervised
systems and evaluate our results on CoVoST 2 and CVSS. The results show that
our work is comparable with some other early supervised methods in some
language pairs. While cascaded systems always suffer from severe error
propagation problems, we proposed denoising back-translation (DBT), a novel
approach to building robust unsupervised neural machine translation (UNMT). DBT
successfully increases the BLEU score by 0.7--0.9 in all three translation
directions. Moreover, we simplified the pipeline of our cascaded system to
reduce inference latency and conducted a comprehensive analysis of every part
of our work. We also demonstrate our unsupervised speech translation results on
the established website.",None,-1
1709902f-a57f-478f-907d-01b6cfd7fd00,Learning Structure-Guided Diffusion Model for 2D Human Pose Estimation,0.797734,"One of the mainstream schemes for 2D human pose estimation (HPE) is learning
keypoints heatmaps by a neural network. Existing methods typically improve the
quality of heatmaps by customized architectures, such as high-resolution
representation and vision Transformers. In this paper, we propose
\textbf{DiffusionPose}, a new scheme that formulates 2D HPE as a keypoints
heatmaps generation problem from noised heatmaps. During training, the
keypoints are diffused to random distribution by adding noises and the
diffusion model learns to recover ground-truth heatmaps from noised heatmaps
with respect to conditions constructed by image feature. During inference, the
diffusion model generates heatmaps from initialized heatmaps in a progressive
denoising way. Moreover, we further explore improving the performance of
DiffusionPose with conditions from human structural information. Extensive
experiments show the prowess of our DiffusionPose, with improvements of 1.6,
1.2, and 1.2 mAP on widely-used COCO, CrowdPose, and AI Challenge datasets,
respectively.",None,-1
76637139-777b-4a2a-b091-8cbf78eb507d,Paint it Black: Generating paintings from text descriptions,0.0179519,"Two distinct tasks - generating photorealistic pictures from given text
prompts and transferring the style of a painting to a real image to make it
appear as though it were done by an artist, have been addressed many times, and
several approaches have been proposed to accomplish them. However, the
intersection of these two, i.e., generating paintings from a given caption, is
a relatively unexplored area with little data available. In this paper, we have
explored two distinct strategies and have integrated them together. First
strategy is to generate photorealistic images and then apply style transfer and
the second strategy is to train an image generation model on real images with
captions and then fine-tune it on captioned paintings later. These two models
are evaluated using different metrics as well as a user study is conducted to
get human feedback on the produced results.",None,-1
111f7146-c3c3-40be-9c6d-135feca19dae,Separating Invisible Sounds Toward Universal Audiovisual Scene-Aware Sound Separation,0.867137,"The audio-visual sound separation field assumes visible sources in videos,
but this excludes invisible sounds beyond the camera's view. Current methods
struggle with such sounds lacking visible cues. This paper introduces a novel
""Audio-Visual Scene-Aware Separation"" (AVSA-Sep) framework. It includes a
semantic parser for visible and invisible sounds and a separator for
scene-informed separation. AVSA-Sep successfully separates both sound types,
with joint training and cross-modal alignment enhancing effectiveness.",None,-1
38370055-0f63-426f-9575-c820c878b706,Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs,0.0646076,"Large language models (LLMs) have achieved widespread success on a variety of
in-context few-shot tasks, but this success is typically evaluated via
correctness rather than consistency. We argue that self-consistency is an
important criteria for valid multi-step reasoning in tasks where the solution
is composed of the answers to multiple sub-steps. We propose two types of
self-consistency that are particularly important for multi-step reasoning --
hypothetical consistency (a model's ability to predict what its output would be
in a hypothetical other context) and compositional consistency (consistency of
a model's final outputs when intermediate sub-steps are replaced with the
model's outputs for those steps). We demonstrate that multiple variants of the
GPT-3/-4 models exhibit poor consistency rates across both types of consistency
on a variety of tasks.",None,-1
2590fc82-2dd3-4372-9031-5611f6ab187b,DAG Learning on the Permutahedron,0.890169,"We propose a continuous optimization framework for discovering a latent
directed acyclic graph (DAG) from observational data. Our approach optimizes
over the polytope of permutation vectors, the so-called Permutahedron, to learn
a topological ordering. Edges can be optimized jointly, or learned conditional
on the ordering via a non-differentiable subroutine. Compared to existing
continuous optimization approaches our formulation has a number of advantages
including: 1. validity: optimizes over exact DAGs as opposed to other
relaxations optimizing approximate DAGs; 2. modularity: accommodates any
edge-optimization procedure, edge structural parameterization, and optimization
loss; 3. end-to-end: either alternately iterates between node-ordering and
edge-optimization, or optimizes them jointly. We demonstrate, on real-world
data problems in protein-signaling and transcriptional network discovery, that
our approach lies on the Pareto frontier of two key metrics, the SID and SHD.",None,-1
3dd0c468-03d3-4599-b408-577a0faf19b2,Adaptive Spot-Guided Transformer for Consistent Local Feature Matching,0.636494,"Local feature matching aims at finding correspondences between a pair of
images. Although current detector-free methods leverage Transformer
architecture to obtain an impressive performance, few works consider
maintaining local consistency. Meanwhile, most methods struggle with large
scale variations. To deal with the above issues, we propose Adaptive
Spot-Guided Transformer (ASTR) for local feature matching, which jointly models
the local consistency and scale variations in a unified coarse-to-fine
architecture. The proposed ASTR enjoys several merits. First, we design a
spot-guided aggregation module to avoid interfering with irrelevant areas
during feature aggregation. Second, we design an adaptive scaling module to
adjust the size of grids according to the calculated depth information at fine
stage. Extensive experimental results on five standard benchmarks demonstrate
that our ASTR performs favorably against state-of-the-art methods. Our code
will be released on https://astr2023.github.io.",None,-1
b2f28bc9-8cf8-481f-a1bf-e86b3cbe09ea,Your Day in Your Pocket: Complex Activity Recognition from Smartphone Accelerometers,0.468659,"Human Activity Recognition (HAR) enables context-aware user experiences where
mobile apps can alter content and interactions depending on user activities.
Hence, smartphones have become valuable for HAR as they allow large, and
diversified data collection. Although previous work in HAR managed to detect
simple activities (i.e., sitting, walking, running) with good accuracy using
inertial sensors (i.e., accelerometer), the recognition of complex daily
activities remains an open problem, specially in remote work/study settings
when people are more sedentary. Moreover, understanding the everyday activities
of a person can support the creation of applications that aim to support their
well-being. This paper investigates the recognition of complex activities
exclusively using smartphone accelerometer data. We used a large smartphone
sensing dataset collected from over 600 users in five countries during the
pandemic and showed that deep learning-based, binary classification of eight
complex activities (sleeping, eating, watching videos, online communication,
attending a lecture, sports, shopping, studying) can be achieved with AUROC
scores up to 0.76 with partially personalized models. This shows encouraging
signs toward assessing complex activities only using phone accelerometer data
in the post-pandemic world.",None,-1
5ed76f81-812c-47c6-9b9c-81913a362f77,DiffProtect: Generate Adversarial Examples with Diffusion Models for Facial Privacy Protection,0.770375,"The increasingly pervasive facial recognition (FR) systems raise serious
concerns about personal privacy, especially for billions of users who have
publicly shared their photos on social media. Several attempts have been made
to protect individuals from being identified by unauthorized FR systems
utilizing adversarial attacks to generate encrypted face images. However,
existing methods suffer from poor visual quality or low attack success rates,
which limit their utility. Recently, diffusion models have achieved tremendous
success in image generation. In this work, we ask: can diffusion models be used
to generate adversarial examples to improve both visual quality and attack
performance? We propose DiffProtect, which utilizes a diffusion autoencoder to
generate semantically meaningful perturbations on FR systems. Extensive
experiments demonstrate that DiffProtect produces more natural-looking
encrypted images than state-of-the-art methods while achieving significantly
higher attack success rates, e.g., 24.5% and 25.1% absolute improvements on the
CelebA-HQ and FFHQ datasets.",None,-1
a28f5db9-3f58-4969-b7fc-6fec75331c73,Towards Semantically Enriched Embeddings for Knowledge Graph Completion,0.277811,"Embedding based Knowledge Graph (KG) Completion has gained much attention
over the past few years. Most of the current algorithms consider a KG as a
multidirectional labeled graph and lack the ability to capture the semantics
underlying the schematic information. In a separate development, a vast amount
of information has been captured within the Large Language Models (LLMs) which
has revolutionized the field of Artificial Intelligence. KGs could benefit from
these LLMs and vice versa. This vision paper discusses the existing algorithms
for KG completion based on the variations for generating KG embeddings. It
starts with discussing various KG completion algorithms such as transductive
and inductive link prediction and entity type prediction algorithms. It then
moves on to the algorithms utilizing type information within the KGs, LLMs, and
finally to algorithms capturing the semantics represented in different
description logic axioms. We conclude the paper with a critical reflection on
the current state of work in the community and give recommendations for future
directions.",None,-1
11b40b55-5144-4681-b5a3-bfc3f941a454,Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization,0.2484,"Despite recent advances, evaluating how well large language models (LLMs)
follow user instructions remains an open problem. While evaluation methods of
language models have seen a rise in prompt-based approaches, limited work on
the correctness of these methods has been conducted. In this work, we perform a
meta-evaluation of a variety of metrics to quantify how accurately they measure
the instruction-following abilities of LLMs. Our investigation is performed on
grounded query-based summarization by collecting a new short-form, real-world
dataset riSum, containing 300 document-instruction pairs with 3 answers each.
All 900 answers are rated by 3 human annotators. Using riSum, we analyze the
agreement between evaluation methods and human judgment. Finally, we propose
new LLM-based reference-free evaluation methods that improve upon established
baselines and perform on par with costly reference-based metrics that require
high-quality summaries.",None,-1
1d921831-599b-462f-a61a-c1ab33e5790c,SkinSAM: Empowering Skin Cancer Segmentation with Segment Anything Model,0.963174,"Skin cancer is a prevalent and potentially fatal disease that requires
accurate and efficient diagnosis and treatment. Although manual tracing is the
current standard in clinics, automated tools are desired to reduce human labor
and improve accuracy. However, developing such tools is challenging due to the
highly variable appearance of skin cancers and complex objects in the
background. In this paper, we present SkinSAM, a fine-tuned model based on the
Segment Anything Model that showed outstanding segmentation performance. The
models are validated on HAM10000 dataset which includes 10015 dermatoscopic
images. While larger models (ViT_L, ViT_H) performed better than the smaller
one (ViT_b), the finetuned model (ViT_b_finetuned) exhibited the greatest
improvement, with a Mean pixel accuracy of 0.945, Mean dice score of 0.8879,
and Mean IoU score of 0.7843. Among the lesion types, vascular lesions showed
the best segmentation results. Our research demonstrates the great potential of
adapting SAM to medical image segmentation tasks.",None,-1
e16cc958-cadc-444f-91f8-a4b2b1e45d8c,The Treasure Beneath Multiple Annotations: An Uncertainty-aware Edge Detector,0.811518,"Deep learning-based edge detectors heavily rely on pixel-wise labels which
are often provided by multiple annotators. Existing methods fuse multiple
annotations using a simple voting process, ignoring the inherent ambiguity of
edges and labeling bias of annotators. In this paper, we propose a novel
uncertainty-aware edge detector (UAED), which employs uncertainty to
investigate the subjectivity and ambiguity of diverse annotations.
Specifically, we first convert the deterministic label space into a learnable
Gaussian distribution, whose variance measures the degree of ambiguity among
different annotations. Then we regard the learned variance as the estimated
uncertainty of the predicted edge maps, and pixels with higher uncertainty are
likely to be hard samples for edge detection. Therefore we design an adaptive
weighting loss to emphasize the learning from those pixels with high
uncertainty, which helps the network to gradually concentrate on the important
pixels. UAED can be combined with various encoder-decoder backbones, and the
extensive experiments demonstrate that UAED achieves superior performance
consistently across multiple edge detection benchmarks. The source code is
available at \url{https://github.com/ZhouCX117/UAED}",None,-1
262143b3-20d1-42da-9952-38e120ee3a38,Unsupervised Learning for Combinatorial Optimization Needs Meta-Learning,0.764459,"A general framework of unsupervised learning for combinatorial optimization
(CO) is to train a neural network (NN) whose output gives a problem solution by
directly optimizing the CO objective. Albeit with some advantages over
traditional solvers, the current framework optimizes an averaged performance
over the distribution of historical problem instances, which misaligns with the
actual goal of CO that looks for a good solution to every future encountered
instance. With this observation, we propose a new objective of unsupervised
learning for CO where the goal of learning is to search for good initialization
for future problem instances rather than give direct solutions. We propose a
meta-learning-based training pipeline for this new objective. Our method
achieves good empirical performance. We observe that even just the initial
solution given by our model before fine-tuning can significantly outperform the
baselines under various evaluation settings including evaluation across
multiple datasets, and the case with big shifts in the problem scale. The
reason we conjecture is that meta-learning-based training lets the model be
loosely tied to each local optima for a training instance while being more
adaptive to the changes of optimization landscapes across instances.",None,-1
f731033d-81a2-4ad1-a6e2-fa97e9502e89,Grounded Text-to-Image Synthesis with Attention Refocusing,0.905369,"Driven by the scalable diffusion models trained on large-scale datasets,
text-to-image synthesis methods have shown compelling results. However, these
models still fail to precisely follow the text prompt involving multiple
objects, attributes, or spatial compositions. In this paper, we reveal the
potential causes in the diffusion model's cross-attention and self-attention
layers. We propose two novel losses to refocus attention maps according to a
given spatial layout during sampling. Creating the layouts manually requires
additional effort and can be tedious. Therefore, we explore using large
language models (LLM) to produce these layouts for our method. We conduct
extensive experiments on the DrawBench, HRS, and TIFA benchmarks to evaluate
our proposed method. We show that our proposed attention refocusing effectively
improves the controllability of existing approaches.",None,-1
1dabce3b-921e-4b3a-b247-6ff021be4550,Sketch Less Face Image Retrieval: A New Challenge,0.117842,"In some specific scenarios, face sketch was used to identify a person.
However, drawing a complete face sketch often needs skills and takes time,
which hinder its widespread applicability in the practice. In this study, we
proposed a new task named sketch less face image retrieval (SLFIR), in which
the retrieval was carried out at each stroke and aim to retrieve the target
face photo using a partial sketch with as few strokes as possible (see Fig.1).
Firstly, we developed a method to generate the data of sketch with drawing
process, and opened such dataset; Secondly, we proposed a two-stage method as
the baseline for SLFIR that (1) A triplet network, was first adopt to learn the
joint embedding space shared between the complete sketch and its target face
photo; (2) Regarding the sketch drawing episode as a sequence, we designed a
LSTM module to optimize the representation of the incomplete face sketch.
Experiments indicate that the new framework can finish the retrieval using a
partial or pool drawing sketch.",None,-1
e39a8d06-3426-48b1-9ce3-3083ea2413a4,Designing Participatory AI: Creative Professionals' Worries and Expectations about Generative AI,0.756395,"Generative AI, i.e., the group of technologies that automatically generate
visual or written content based on text prompts, has undergone a leap in
complexity and become widely available within just a few years. Such
technologies potentially introduce a massive disruption to creative fields.
This paper presents the results of a qualitative survey ($N$ = 23)
investigating how creative professionals think about generative AI. The results
show that the advancement of these AI models prompts important reflections on
what defines creativity and how creatives imagine using AI to support their
workflows. Based on these reflections, we discuss how we might design
\textit{participatory AI} in the domain of creative expertise with the goal of
empowering creative professionals in their present and future coexistence with
AI.",None,-1
408d331f-aaf3-4bec-bd1f-6a45468c89ed,Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach,0.863341,"The remarkable progress in Large Language Models (LLMs) opens up new avenues
for addressing planning and decision-making problems in Multi-Agent Systems
(MAS). However, as the number of agents increases, the issues of hallucination
in LLMs and coordination in MAS have become increasingly prominent.
Additionally, the efficient utilization of tokens emerges as a critical
consideration when employing LLMs to facilitate the interactions among a
substantial number of agents. In this paper, we develop a modular framework
called LLaMAC to mitigate these challenges. LLaMAC implements a value
distribution encoding similar to that found in the human brain, utilizing
internal and external feedback mechanisms to facilitate collaboration and
iterative reasoning among its modules. Through evaluations involving system
resource allocation and robot grid transportation, we demonstrate the
considerable advantages afforded by our proposed approach.",None,-1
175cfd7e-97aa-4792-ba66-aa74981f44d6,Complex Claim Verification with Evidence Retrieved in the Wild,0.901963,"Evidence retrieval is a core part of automatic fact-checking. Prior work
makes simplifying assumptions in retrieval that depart from real-world use
cases: either no access to evidence, access to evidence curated by a human
fact-checker, or access to evidence available long after the claim has been
made. In this work, we present the first fully automated pipeline to check
real-world claims by retrieving raw evidence from the web. We restrict our
retriever to only search documents available prior to the claim's making,
modeling the realistic scenario where an emerging claim needs to be checked.
Our pipeline includes five components: claim decomposition, raw document
retrieval, fine-grained evidence retrieval, claim-focused summarization, and
veracity judgment. We conduct experiments on complex political claims in the
ClaimDecomp dataset and show that the aggregated evidence produced by our
pipeline improves veracity judgments. Human evaluation finds the evidence
summary produced by our system is reliable (it does not hallucinate
information) and relevant to answering key questions about a claim, suggesting
that it can assist fact-checkers even when it cannot surface a complete
evidence set.",None,-1
3d996d78-bb75-4fa1-b4bd-173c004af255,Learning Customized Visual Models with Retrieval-Augmented Knowledge,0.660302,"Image-text contrastive learning models such as CLIP have demonstrated strong
task transfer ability. The high generality and usability of these visual models
is achieved via a web-scale data collection process to ensure broad concept
coverage, followed by expensive pre-training to feed all the knowledge into
model weights. Alternatively, we propose REACT, REtrieval-Augmented
CusTomization, a framework to acquire the relevant web knowledge to build
customized visual models for target domains. We retrieve the most relevant
image-text pairs (~3% of CLIP pre-training data) from the web-scale database as
external knowledge, and propose to customize the model by only training new
modualized blocks while freezing all the original weights. The effectiveness of
REACT is demonstrated via extensive experiments on classification, retrieval,
detection and segmentation tasks, including zero, few, and full-shot settings.
Particularly, on the zero-shot classification task, compared with CLIP, it
achieves up to 5.4% improvement on ImageNet and 3.7% on the ELEVATER benchmark
(20 datasets).",None,-1
3ecf2d5d-2262-4fb5-bac9-68a4d720f641,Exploiting Language Models as a Source of Knowledge for Cognitive Agents,0.694346,"Large language models (LLMs) provide capabilities far beyond sentence
completion, including question answering, summarization, and natural-language
inference. While many of these capabilities have potential application to
cognitive systems, our research is exploiting language models as a source of
task knowledge for cognitive agents, that is, agents realized via a cognitive
architecture. We identify challenges and opportunities for using language
models as an external knowledge source for cognitive systems and possible ways
to improve the effectiveness of knowledge extraction by integrating extraction
with cognitive architecture capabilities, highlighting with examples from our
recent work in this area.",None,-1
4dfb4d3a-d1c7-4e52-b3a3-134c27ba6f9a,GePA*SE: Generalized Edge-Based Parallel A* for Slow Evaluations,0.320196,"Parallel search algorithms have been shown to improve planning speed by
harnessing the multithreading capability of modern processors. One such
algorithm PA*SE achieves this by parallelizing state expansions, whereas
another algorithm ePA*SE achieves this by effectively parallelizing edge
evaluations. ePA*SE targets domains in which the action space comprises actions
with expensive but similar evaluation times. However, in a number of robotics
domains, the action space is heterogenous in the computational effort required
to evaluate the cost of an action and its outcome. Motivated by this, we
introduce GePA*SE: Generalized Edge-based Parallel A* for Slow Evaluations,
which generalizes the key ideas of PA*SE and ePA*SE i.e. parallelization of
state expansions and edge evaluations respectively. This extends its
applicability to domains that have actions requiring varying computational
effort to evaluate them. The open-source code for GePA*SE along with the
baselines is available here: https://github.com/shohinm/parallel_search",None,-1
c04ad999-4ceb-4969-a88f-07e206ed477b,Mimic-IV-ICD: A new benchmark for eXtreme MultiLabel Classification,0.893704,"Clinical notes are assigned ICD codes - sets of codes for diagnoses and
procedures. In the recent years, predictive machine learning models have been
built for automatic ICD coding. However, there is a lack of widely accepted
benchmarks for automated ICD coding models based on large-scale public EHR
data.
  This paper proposes a public benchmark suite for ICD-10 coding using a large
EHR dataset derived from MIMIC-IV, the most recent public EHR dataset. We
implement and compare several popular methods for ICD coding prediction tasks
to standardize data preprocessing and establish a comprehensive ICD coding
benchmark dataset. This approach fosters reproducibility and model comparison,
accelerating progress toward employing automated ICD coding in future studies.
Furthermore, we create a new ICD-9 benchmark using MIMIC-IV data, providing
more data points and a higher number of ICD codes than MIMIC-III. Our
open-source code offers easy access to data processing steps, benchmark
creation, and experiment replication for those with MIMIC-IV access, providing
insights, guidance, and protocols to efficiently develop ICD coding models.",None,-1
de9197a2-50dc-47dd-95fb-3661838f3378,QAID: Question Answering Inspired Few-shot Intent Detection,0.486213,"Intent detection with semantically similar fine-grained intents is a
challenging task. To address it, we reformulate intent detection as a
question-answering retrieval task by treating utterances and intent names as
questions and answers. To that end, we utilize a question-answering retrieval
architecture and adopt a two stages training schema with batch contrastive
loss. In the pre-training stage, we improve query representations through
self-supervised training. Then, in the fine-tuning stage, we increase
contextualized token-level similarity scores between queries and answers from
the same intent. Our results on three few-shot intent detection benchmarks
achieve state-of-the-art performance.",None,-1
cc8f2eb2-1745-4bac-8843-5cdea2f380be,Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents,0.795438,"Robustly cooperating with unseen agents and human partners presents
significant challenges due to the diverse cooperative conventions these
partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this
challenge by training an agent with a population of diverse teammate policies
obtained through maximizing specific diversity metrics. However, prior
heuristic-based diversity metrics do not always maximize the agent's robustness
in all cooperative problems. In this work, we first propose that maximizing an
AHT agent's robustness requires it to emulate policies in the minimum coverage
set (MCS), the set of best-response policies to any partner policies in the
environment. We then introduce the L-BRDiv algorithm that generates a set of
teammate policies that, when used for AHT training, encourage agents to emulate
policies from the MCS. L-BRDiv works by solving a constrained optimization
problem to jointly train teammate policies for AHT training and approximating
AHT agent policies that are members of the MCS. We empirically demonstrate that
L-BRDiv produces more robust AHT agents than state-of-the-art methods in a
broader range of two-player cooperative problems without the need for extensive
hyperparameter tuning for its objectives. Our study shows that L-BRDiv
outperforms the baseline methods by prioritizing discovering distinct members
of the MCS instead of repeatedly finding redundant policies.",None,-1
7fd74d9e-7b6a-4a0e-b53c-8bab72038578,Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs,0.500376,"Large Language Models (LLMs) have demonstrated exceptional natural language
understanding abilities and have excelled in a variety of natural language
processing (NLP)tasks in recent years. Despite the fact that most LLMs are
trained predominantly in English, multiple studies have demonstrated their
comparative performance in many other languages. However, fundamental questions
persist regarding how LLMs acquire their multi-lingual abilities and how
performance varies across different languages. These inquiries are crucial for
the study of LLMs since users and researchers often come from diverse language
backgrounds, potentially influencing their utilization and interpretation of
LLMs' results. In this work, we propose a systematic way of qualifying the
performance disparities of LLMs under multilingual settings. We investigate the
phenomenon of across-language generalizations in LLMs, wherein insufficient
multi-lingual training data leads to advanced multi-lingual capabilities. To
accomplish this, we employ a novel back-translation-based prompting method. The
results show that GPT exhibits highly translating-like behaviour in
multilingual settings.",None,-1
24aed727-f4ef-4cbc-afcf-acc404413a0c,A semantically enhanced dual encoder for aspect sentiment triplet extraction,0.870332,"Aspect sentiment triplet extraction (ASTE) is a crucial subtask of
aspect-based sentiment analysis (ABSA) that aims to comprehensively identify
sentiment triplets. Previous research has focused on enhancing ASTE through
innovative table-filling strategies. However, these approaches often overlook
the multi-perspective nature of language expressions, resulting in a loss of
valuable interaction information between aspects and opinions. To address this
limitation, we propose a framework that leverages both a basic encoder,
primarily based on BERT, and a particular encoder comprising a Bi-LSTM network
and graph convolutional network (GCN ). The basic encoder captures the
surface-level semantics of linguistic expressions, while the particular encoder
extracts deeper semantics, including syntactic and lexical information. By
modeling the dependency tree of comments and considering the part-of-speech and
positional information of words, we aim to capture semantics that are more
relevant to the underlying intentions of the sentences. An interaction strategy
combines the semantics learned by the two encoders, enabling the fusion of
multiple perspectives and facilitating a more comprehensive understanding of
aspect--opinion relationships. Experiments conducted on benchmark datasets
demonstrate the state-of-the-art performance of our proposed framework.",None,-1
9b8b91ac-2511-4207-9553-06abcc48c3d3,CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer,0.760784,"Content affinity loss including feature and pixel affinity is a main problem
which leads to artifacts in photorealistic and video style transfer. This paper
proposes a new framework named CAP-VSTNet, which consists of a new reversible
residual network and an unbiased linear transform module, for versatile style
transfer. This reversible residual network can not only preserve content
affinity but not introduce redundant information as traditional reversible
networks, and hence facilitate better stylization. Empowered by Matting
Laplacian training loss which can address the pixel affinity loss problem led
by the linear transform, the proposed framework is applicable and effective on
versatile style transfer. Extensive experiments show that CAP-VSTNet can
produce better qualitative and quantitative results in comparison with the
state-of-the-art methods.",None,-1
60bfe8f3-1d77-4fb5-9c2a-9ab05f7a0f2b,Mathematical Structure of Syntactic Merge,0.613637,"The syntactic Merge operation of the Minimalist Program in linguistics can be
described mathematically in terms of Hopf algebras, with a formalism similar to
the one arising in the physics of renormalization. This mathematical
formulation of Merge has good descriptive power, as phenomena empirically
observed in linguistics can be justified from simple mathematical arguments. It
also provides a possible mathematical model for externalization and for the
role of syntactic parameters.",None,-1
619f51c6-de38-4544-b7b1-cb7a5a7de2f1,PersonNeRF: Personalized Reconstruction from Photo Collections,0.517543,"We present PersonNeRF, a method that takes a collection of photos of a
subject (e.g. Roger Federer) captured across multiple years with arbitrary body
poses and appearances, and enables rendering the subject with arbitrary novel
combinations of viewpoint, body pose, and appearance. PersonNeRF builds a
customized neural volumetric 3D model of the subject that is able to render an
entire space spanned by camera viewpoint, body pose, and appearance. A central
challenge in this task is dealing with sparse observations; a given body pose
is likely only observed by a single viewpoint with a single appearance, and a
given appearance is only observed under a handful of different body poses. We
address this issue by recovering a canonical T-pose neural volumetric
representation of the subject that allows for changing appearance across
different observations, but uses a shared pose-dependent motion field across
all observations. We demonstrate that this approach, along with regularization
of the recovered volumetric geometry to encourage smoothness, is able to
recover a model that renders compelling images from novel combinations of
viewpoint, pose, and appearance from these challenging unstructured photo
collections, outperforming prior work for free-viewpoint human rendering.",None,-1
16bcd429-0b40-4c0c-a4fa-db16fc49b359,On the Importance of Noise Scheduling for Diffusion Models,0.69145,"We empirically study the effect of noise scheduling strategies for denoising
diffusion generative models. There are three findings: (1) the noise scheduling
is crucial for the performance, and the optimal one depends on the task (e.g.,
image sizes), (2) when increasing the image size, the optimal noise scheduling
shifts towards a noisier one (due to increased redundancy in pixels), and (3)
simply scaling the input data by a factor of $b$ while keeping the noise
schedule function fixed (equivalent to shifting the logSNR by $\log b$) is a
good strategy across image sizes. This simple recipe, when combined with
recently proposed Recurrent Interface Network (RIN), yields state-of-the-art
pixel-based diffusion models for high-resolution images on ImageNet, enabling
single-stage, end-to-end generation of diverse and high-fidelity images at
1024$\times$1024 resolution (without upsampling/cascades).",None,-1
b54c864d-b7a1-4d8f-a7e8-e6fcc4e4db26,AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models,0.627369,"Recent large language models (LLMs) are promising for making decisions in
grounded environments. However, LLMs frequently fail in complex decision-making
tasks due to the misalignment between the pre-trained knowledge in LLMs and the
actual rules in the environment. Existing methods require either costly
gradient computation or lengthy in-context demonstrations. In this paper, we
propose AutoPlan, an approach to guide LLM-based agents to accomplish
interactive decision-making tasks. AutoPlan augments the LLM prompt with a
task-solving plan and optimizes it through iterative experience collection and
reflection. Our experiments show that AutoPlan, though using no in-context
demonstrations, achieves success rates on par with the baselines using
human-written demonstrations on ALFWorld and even outperforms them by 8% on
HotpotQA. The code is available at https://github.com/owaski/AutoPlan.",None,-1
0226dafe-5806-4a73-b3fc-6fa7ab3ecdb3,Learning Machine Morality through Experience and Interaction,0.432169,"Increasing interest in ensuring safety of next-generation Artificial
Intelligence (AI) systems calls for novel approaches to embedding morality into
autonomous agents. Traditionally, this has been done by imposing explicit
top-down rules or hard constraints on systems, for example by filtering system
outputs through pre-defined ethical rules. Recently, instead, entirely
bottom-up methods for learning implicit preferences from human behavior have
become increasingly popular, such as those for training and fine-tuning Large
Language Models. In this paper, we provide a systematization of existing
approaches to the problem of introducing morality in machines - modeled as a
continuum, and argue that the majority of popular techniques lie at the
extremes - either being fully hard-coded, or entirely learned, where no
explicit statement of any moral principle is required. Given the relative
strengths and weaknesses of each type of methodology, we argue that more hybrid
solutions are needed to create adaptable and robust, yet more controllable and
interpretable agents.
  In particular, we present three case studies of recent works which use
learning from experience (i.e., Reinforcement Learning) to explicitly provide
moral principles to learning agents - either as intrinsic rewards, moral
logical constraints or textual principles for language models. For example,
using intrinsic rewards in Social Dilemma games, we demonstrate how it is
possible to represent classical moral frameworks for agents. We also present an
overview of the existing work in this area in order to provide empirical
evidence for the potential of this hybrid approach. We then discuss strategies
for evaluating the effectiveness of moral learning agents. Finally, we present
open research questions and implications for the future of AI safety and ethics
which are emerging from this framework.",None,-1
071d4962-3656-4bf7-afe3-d0d7d3e6b09d,Generative Models for 3D Point Clouds,0.0737171,"Point clouds are rich geometric data structures, where their three
dimensional structure offers an excellent domain for understanding the
representation learning and generative modeling in 3D space. In this work, we
aim to improve the performance of point cloud latent-space generative models by
experimenting with transformer encoders, latent-space flow models, and
autoregressive decoders. We analyze and compare both generation and
reconstruction performance of these models on various object types.",None,-1
93fdc74c-4b6b-49b0-ade0-cce65a7a5e65,Zero-shot Referring Image Segmentation with Global-Local Context Features,0.695557,"Referring image segmentation (RIS) aims to find a segmentation mask given a
referring expression grounded to a region of the input image. Collecting
labelled datasets for this task, however, is notoriously costly and
labor-intensive. To overcome this issue, we propose a simple yet effective
zero-shot referring image segmentation method by leveraging the pre-trained
cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded
to the input text, we propose a mask-guided visual encoder that captures global
and local contextual information of an input image. By utilizing instance masks
obtained from off-the-shelf mask proposal techniques, our method is able to
segment fine-detailed Istance-level groundings. We also introduce a
global-local text encoder where the global feature captures complex
sentence-level semantics of the entire input expression while the local feature
focuses on the target noun phrase extracted by a dependency parser. In our
experiments, the proposed method outperforms several zero-shot baselines of the
task and even the weakly supervised referring expression segmentation method
with substantial margins. Our code is available at
https://github.com/Seonghoon-Yu/Zero-shot-RIS.",None,-1
e9ae49f1-a66e-4dbc-b381-0ead67422fbb,Guiding Computational Stance Detection with Expanded Stance Triangle Framework,0.0578504,"Stance detection determines whether the author of a piece of text is in favor
of, against, or neutral towards a specified target, and can be used to gain
valuable insights into social media. The ubiquitous indirect referral of
targets makes this task challenging, as it requires computational solutions to
model semantic features and infer the corresponding implications from a literal
statement. Moreover, the limited amount of available training data leads to
subpar performance in out-of-domain and cross-target scenarios, as data-driven
approaches are prone to rely on superficial and domain-specific features. In
this work, we decompose the stance detection task from a linguistic
perspective, and investigate key components and inference paths in this task.
The stance triangle is a generic linguistic framework previously proposed to
describe the fundamental ways people express their stance. We further expand it
by characterizing the relationship between explicit and implicit objects. We
then use the framework to extend one single training corpus with additional
annotation. Experimental results show that strategically-enriched data can
significantly improve the performance on out-of-domain and cross-target
evaluation.",None,-1
fc87669a-aafb-4a71-aadc-1f175e24d493,Super-Resolving Face Image by Facial Parsing Information,0.268028,"Face super-resolution is a technology that transforms a low-resolution face
image into the corresponding high-resolution one. In this paper, we build a
novel parsing map guided face super-resolution network which extracts the face
prior (i.e., parsing map) directly from low-resolution face image for the
following utilization. To exploit the extracted prior fully, a parsing map
attention fusion block is carefully designed, which can not only effectively
explore the information of parsing map, but also combines powerful attention
mechanism. Moreover, in light of that high-resolution features contain more
precise spatial information while low-resolution features provide strong
contextual information, we hope to maintain and utilize these complementary
information. To achieve this goal, we develop a multi-scale refine block to
maintain spatial and contextual information and take advantage of multi-scale
features to refine the feature representations. Experimental results
demonstrate that our method outperforms the state-of-the-arts in terms of
quantitative metrics and visual quality. The source codes will be available at
https://github.com/wcy-cs/FishFSRNet.",None,-1
be9a8587-4f42-40de-839e-fe88dbdf4993,Recurrent Attention Networks for Long-text Modeling,0.398139,"Self-attention-based models have achieved remarkable progress in short-text
mining. However, the quadratic computational complexities restrict their
application in long text processing. Prior works have adopted the chunking
strategy to divide long documents into chunks and stack a self-attention
backbone with the recurrent structure to extract semantic representation. Such
an approach disables parallelization of the attention mechanism, significantly
increasing the training cost and raising hardware requirements. Revisiting the
self-attention mechanism and the recurrent structure, this paper proposes a
novel long-document encoding model, Recurrent Attention Network (RAN), to
enable the recurrent operation of self-attention. Combining the advantages from
both sides, the well-designed RAN is capable of extracting global semantics in
both token-level and document-level representations, making it inherently
compatible with both sequential and classification tasks, respectively.
Furthermore, RAN is computationally scalable as it supports parallelization on
long document processing. Extensive experiments demonstrate the long-text
encoding ability of the proposed RAN model on both classification and
sequential tasks, showing its potential for a wide range of applications.",None,-1
ab623586-f82c-4f8d-9c8c-5ea8d28b8c9e,Deep Learning Systems for Advanced Driving Assistance,0.239191,"Next generation cars embed intelligent assessment of car driving safety
through innovative solutions often based on usage of artificial intelligence.
The safety driving monitoring can be carried out using several methodologies
widely treated in scientific literature. In this context, the author proposes
an innovative approach that uses ad-hoc bio-sensing system suitable to
reconstruct the physio-based attentional status of the car driver. To
reconstruct the car driver physiological status, the author proposed the use of
a bio-sensing probe consisting of a coupled LEDs at Near infrared (NiR)
spectrum with a photodetector. This probe placed over the monitored subject
allows to detect a physiological signal called PhotoPlethysmoGraphy (PPG). The
PPG signal formation is regulated by the change in oxygenated and
non-oxygenated hemoglobin concentration in the monitored subject bloodstream
which will be directly connected to cardiac activity in turn regulated by the
Autonomic Nervous System (ANS) that characterizes the subject's attention
level. This so designed car driver drowsiness monitoring will be combined with
further driving safety assessment based on correlated intelligent driving
scenario understanding.",None,-1
43f6f814-9568-47ba-8be3-40e6aa541b3e,Facial Expression Recognition at the Edge: CPU vs GPU vs VPU vs TPU,0.688763,"Facial Expression Recognition (FER) plays an important role in human-computer
interactions and is used in a wide range of applications. Convolutional Neural
Networks (CNN) have shown promise in their ability to classify human facial
expressions, however, large CNNs are not well-suited to be implemented on
resource- and energy-constrained IoT devices. In this work, we present a
hierarchical framework for developing and optimizing hardware-aware CNNs tuned
for deployment at the edge. We perform a comprehensive analysis across various
edge AI accelerators including NVIDIA Jetson Nano, Intel Neural Compute Stick,
and Coral TPU. Using the proposed strategy, we achieved a peak accuracy of
99.49% when testing on the CK+ facial expression recognition dataset.
Additionally, we achieved a minimum inference latency of 0.39 milliseconds and
a minimum power consumption of 0.52 Watts.",None,-1
2e7a2e04-f0e0-445a-8855-34795bab732b,Equivariant Multi-Modality Image Fusion,0.24753,"Multi-modality image fusion is a technique that combines information from
different sensors or modalities, enabling the fused image to retain
complementary features from each modality, such as functional highlights and
texture details. However, effective training of such fusion models is
challenging due to the scarcity of ground truth fusion data. To tackle this
issue, we propose the Equivariant Multi-Modality imAge fusion (EMMA) paradigm
for end-to-end self-supervised learning. Our approach is rooted in the prior
knowledge that natural imaging responses are equivariant to certain
transformations. Consequently, we introduce a novel training paradigm that
encompasses a fusion module, a pseudo-sensing module, and an equivariant fusion
module. These components enable the net training to follow the principles of
the natural sensing-imaging process while satisfying the equivariant imaging
prior. Extensive experiments confirm that EMMA yields high-quality fusion
results for infrared-visible and medical images, concurrently facilitating
downstream multi-modal segmentation and detection tasks. The code is available
at https://github.com/Zhaozixiang1228/MMIF-EMMA.",None,-1
626db1a1-1e4e-44f1-98af-5b4521b57fe8,In-Contextual Gender Bias Suppression for Large Language Models,0.388836,"Despite their impressive performance in a wide range of NLP tasks, Large
Language Models (LLMs) have been reported to encode worrying-levels of gender
biases. Prior work has proposed debiasing methods that require human labelled
examples, data augmentation and fine-tuning of LLMs, which are computationally
costly. Moreover, one might not even have access to the model parameters for
performing debiasing such as in the case of closed LLMs such as GPT-4. To
address this challenge, we propose bias suppression that prevents biased
generations of LLMs by simply providing textual preambles constructed from
manually designed templates and real-world statistics, without accessing to
model parameters. We show that, using CrowsPairs dataset, our textual preambles
covering counterfactual statements can suppress gender biases in English LLMs
such as LLaMA2. Moreover, we find that gender-neutral descriptions of
gender-biased objects can also suppress their gender biases. Moreover, we show
that bias suppression has acceptable adverse effect on downstream task
performance with HellaSwag and COPA.",None,-1
ed20b9d4-752f-4696-ad03-632f4e97c5c0,Exploring Invariant Representation for Visible-Infrared Person Re-Identification,0.738437,"Cross-spectral person re-identification, which aims to associate identities
to pedestrians across different spectra, faces a main challenge of the modality
discrepancy. In this paper, we address the problem from both image-level and
feature-level in an end-to-end hybrid learning framework named robust feature
mining network (RFM). In particular, we observe that the reflective intensity
of the same surface in photos shot in different wavelengths could be
transformed using a linear model. Besides, we show the variable linear factor
across the different surfaces is the main culprit which initiates the modality
discrepancy. We integrate such a reflection observation into an image-level
data augmentation by proposing the linear transformation generator (LTG).
Moreover, at the feature level, we introduce a cross-center loss to explore a
more compact intra-class distribution and modality-aware spatial attention to
take advantage of textured regions more efficiently. Experiment results on two
standard cross-spectral person re-identification datasets, i.e., RegDB and
SYSU-MM01, have demonstrated state-of-the-art performance.",None,-1
18ed9c6e-ca3f-49a6-a64a-1ac3b4f339bd,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,0.0582988,"Precise localization is critical for autonomous vehicles. We present a
self-supervised learning method that employs Transformers for the first time
for the task of outdoor localization using LiDAR data. We propose a pre-text
task that reorganizes the slices of a $360^\circ$ LiDAR scan to leverage its
axial properties. Our model, called Slice Transformer, employs multi-head
attention while systematically processing the slices. To the best of our
knowledge, this is the first instance of leveraging multi-head attention for
outdoor point clouds. We additionally introduce the Perth-WA dataset, which
provides a large-scale LiDAR map of Perth city in Western Australia, covering
$\sim$4km$^2$ area. Localization annotations are provided for Perth-WA. The
proposed localization method is thoroughly evaluated on Perth-WA and
Appollo-SouthBay datasets. We also establish the efficacy of our
self-supervised learning approach for the common downstream task of object
classification using ModelNet40 and ScanNN datasets. The code and Perth-WA data
will be publicly released.",None,-1
8f21fc25-bcd0-4e41-89fa-f6f71753bb4c,FAMuS: Frames Across Multiple Sources,0.398847,"Understanding event descriptions is a central aspect of language processing,
but current approaches focus overwhelmingly on single sentences or documents.
Aggregating information about an event \emph{across documents} can offer a much
richer understanding. To this end, we present FAMuS, a new corpus of Wikipedia
passages that \emph{report} on some event, paired with underlying,
genre-diverse (non-Wikipedia) \emph{source} articles for the same event. Events
and (cross-sentence) arguments in both report and source are annotated against
FrameNet, providing broad coverage of different event types. We present results
on two key event understanding tasks enabled by FAMuS: \emph{source validation}
-- determining whether a document is a valid source for a target report event
-- and \emph{cross-document argument extraction} -- full-document argument
extraction for a target event from both its report and the correct source
article. We release both FAMuS and our models to support further research.",None,-1
05a7a6f8-0002-46e6-9625-5d003aaef512,On the Robustness of Latent Diffusion Models,0.264196,"Latent diffusion models achieve state-of-the-art performance on a variety of
generative tasks, such as image synthesis and image editing. However, the
robustness of latent diffusion models is not well studied. Previous works only
focus on the adversarial attacks against the encoder or the output image under
white-box settings, regardless of the denoising process. Therefore, in this
paper, we aim to analyze the robustness of latent diffusion models more
thoroughly. We first study the influence of the components inside latent
diffusion models on their white-box robustness. In addition to white-box
scenarios, we evaluate the black-box robustness of latent diffusion models via
transfer attacks, where we consider both prompt-transfer and model-transfer
settings and possible defense mechanisms. However, all these explorations need
a comprehensive benchmark dataset, which is missing in the literature.
Therefore, to facilitate the research of the robustness of latent diffusion
models, we propose two automatic dataset construction pipelines for two kinds
of image editing models and release the whole dataset. Our code and dataset are
available at \url{https://github.com/jpzhang1810/LDM-Robustness}.",None,-1
10651531-6291-407d-97ee-7c0f0953f6ac,Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs,0.517899,"In any system that uses structured knowledge graph (KG) data as its
underlying knowledge representation, KG-to-text generation is a useful tool for
turning parts of the graph data into text that can be understood by humans.
Recent work has shown that models that make use of pretraining on large amounts
of text data can perform well on the KG-to-text task even with relatively small
sets of training data on the specific graph-to-text task. In this paper, we
build on this concept by using large language models to perform zero-shot
generation based on nothing but the model's understanding of the triple
structure from what it can read. We show that ChatGPT achieves near
state-of-the-art performance on some measures of the WebNLG 2020 challenge, but
falls behind on others. Additionally, we compare factual, counter-factual and
fictional statements, and show that there is a significant connection between
what the LLM already knows about the data it is parsing and the quality of the
output text.",None,-1
636680be-69ad-4ce5-a1d2-716017d2a0db,Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding,0.348089,"Recently, large pretrained language models have demonstrated strong language
understanding capabilities. This is particularly reflected in their zero-shot
and in-context learning abilities on downstream tasks through prompting. To
assess their impact on spoken language understanding (SLU), we evaluate several
such models like ChatGPT and OPT of different sizes on multiple benchmarks. We
verify the emergent ability unique to the largest models as they can reach
intent classification accuracy close to that of supervised models with zero or
few shots on various languages given oracle transcripts. By contrast, the
results for smaller models fitting a single GPU fall far behind. We note that
the error cases often arise from the annotation scheme of the dataset;
responses from ChatGPT are still reasonable. We show, however, that the model
is worse at slot filling, and its performance is sensitive to ASR errors,
suggesting serious challenges for the application of those textual models on
SLU.",None,-1
75d8eb7e-982e-42c9-acd7-eab15feb23cc,Constrained Meta-Reinforcement Learning for Adaptable Safety Guarantee with Differentiable Convex Programming,0.128908,"Despite remarkable achievements in artificial intelligence, the deployability
of learning-enabled systems in high-stakes real-world environments still faces
persistent challenges. For example, in safety-critical domains like autonomous
driving, robotic manipulation, and healthcare, it is crucial not only to
achieve high performance but also to comply with given constraints.
Furthermore, adaptability becomes paramount in non-stationary domains, where
environmental parameters are subject to change. While safety and adaptability
are recognized as key qualities for the new generation of AI, current
approaches have not demonstrated effective adaptable performance in constrained
settings. Hence, this paper breaks new ground by studying the unique challenges
of ensuring safety in non-stationary environments by solving constrained
problems through the lens of the meta-learning approach (learning-to-learn).
While unconstrained meta-learning al-ready encounters complexities in
end-to-end differentiation of the loss due to the bi-level nature, its
constrained counterpart introduces an additional layer of difficulty, since the
constraints imposed on task-level updates complicate the differentiation
process. To address the issue, we first employ successive convex-constrained
policy updates across multiple tasks with differentiable convexprogramming,
which allows meta-learning in constrained scenarios by enabling end-to-end
differentiation. This approach empowers the agent to rapidly adapt to new tasks
under non-stationarity while ensuring compliance with safety constraints.",None,-1
0a67bb3b-4679-42e8-ba8d-8cbcad62c7aa,Hierarchical Neural Memory Network for Low Latency Event Processing,0.85635,"This paper proposes a low latency neural network architecture for event-based
dense prediction tasks. Conventional architectures encode entire scene contents
at a fixed rate regardless of their temporal characteristics. Instead, the
proposed network encodes contents at a proper temporal scale depending on its
movement speed. We achieve this by constructing temporal hierarchy using
stacked latent memories that operate at different rates. Given low latency
event steams, the multi-level memories gradually extract dynamic to static
scene contents by propagating information from the fast to the slow memory
modules. The architecture not only reduces the redundancy of conventional
architectures but also exploits long-term dependencies. Furthermore, an
attention-based event representation efficiently encodes sparse event streams
into the memory cells. We conduct extensive evaluations on three event-based
dense prediction tasks, where the proposed approach outperforms the existing
methods on accuracy and latency, while demonstrating effective event and image
fusion capabilities. The code is available at https://hamarh.github.io/hmnet/",None,-1
6dafd940-0294-497b-b821-1f3f934c1e74,FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views,0.727817,"We present FlexNeRF, a method for photorealistic freeviewpoint rendering of
humans in motion from monocular videos. Our approach works well with sparse
views, which is a challenging scenario when the subject is exhibiting
fast/complex motions. We propose a novel approach which jointly optimizes a
canonical time and pose configuration, with a pose-dependent motion field and
pose-independent temporal deformations complementing each other. Thanks to our
novel temporal and cyclic consistency constraints along with additional losses
on intermediate representation such as segmentation, our approach provides high
quality outputs as the observed views become sparser. We empirically
demonstrate that our method significantly outperforms the state-of-the-art on
public benchmark datasets as well as a self-captured fashion dataset. The
project page is available at: https://flex-nerf.github.io/",None,-1
3752d862-6c5b-46eb-a40a-509c5eb5af04,dugMatting: Decomposed-Uncertainty-Guided Matting,0.396371,"Cutting out an object and estimating its opacity mask, known as image
matting, is a key task in image and video editing. Due to the highly ill-posed
issue, additional inputs, typically user-defined trimaps or scribbles, are
usually needed to reduce the uncertainty. Although effective, it is either time
consuming or only suitable for experienced users who know where to place the
strokes. In this work, we propose a decomposed-uncertainty-guided matting
(dugMatting) algorithm, which explores the explicitly decomposed uncertainties
to efficiently and effectively improve the results. Basing on the
characteristic of these uncertainties, the epistemic uncertainty is reduced in
the process of guiding interaction (which introduces prior knowledge), while
the aleatoric uncertainty is reduced in modeling data distribution (which
introduces statistics for both data and possible noise). The proposed matting
framework relieves the requirement for users to determine the interaction areas
by using simple and efficient labeling. Extensively quantitative and
qualitative results validate that the proposed method significantly improves
the original matting algorithms in terms of both efficiency and efficacy.",None,-1
8f6f0688-ac48-4784-9810-1fcf2226fa7b,ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters,0.580708,"We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via
the use of language adapters (LAs). Most of the earlier works have explored
training with adapter of a single source (often English), and testing either
using the target LA or LA of another related language. Training target LA
requires unlabeled data, which may not be readily available for low resource
unseen languages: those that are neither seen by the underlying multilingual
language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data
for them. We posit that for more effective cross-lingual transfer, instead of
just one source LA, we need to leverage LAs of multiple (linguistically or
geographically related) source languages, both at train and test-time - which
we investigate via our novel neural architecture, ZGUL. Extensive
experimentation across four language groups, covering 15 unseen target
languages, demonstrates improvements of up to 3.2 average F1 points over
standard fine-tuning and other strong baselines on POS tagging and NER tasks.
We also extend ZGUL to settings where either (1) some unlabeled data or (2)
few-shot training examples are available for the target language. We find that
ZGUL continues to outperform baselines in these settings too.",None,-1
4eda4464-4441-49d9-983c-8e6b3ede8d24,Magnitude Attention-based Dynamic Pruning,0.135977,"Existing pruning methods utilize the importance of each weight based on
specified criteria only when searching for a sparse structure but do not
utilize it during training. In this work, we propose a novel approach -
\textbf{M}agnitude \textbf{A}ttention-based Dynamic \textbf{P}runing (MAP)
method, which applies the importance of weights throughout both the forward and
backward paths to explore sparse model structures dynamically. Magnitude
attention is defined based on the magnitude of weights as continuous
real-valued numbers enabling a seamless transition from a redundant to an
effective sparse network by promoting efficient exploration. Additionally, the
attention mechanism ensures more effective updates for important layers within
the sparse network. In later stages of training, our approach shifts from
exploration to exploitation, exclusively updating the sparse model composed of
crucial weights based on the explored structure, resulting in pruned models
that not only achieve performance comparable to dense models but also
outperform previous pruning methods on CIFAR-10/100 and ImageNet.",None,-1
32ab892c-96c4-4be2-b69a-5c755e7e5ca4,Incremental 3D Semantic Scene Graph Prediction from RGB Sequences,0.736691,"3D semantic scene graphs are a powerful holistic representation as they
describe the individual objects and depict the relation between them. They are
compact high-level graphs that enable many tasks requiring scene reasoning. In
real-world settings, existing 3D estimation methods produce robust predictions
that mostly rely on dense inputs. In this work, we propose a real-time
framework that incrementally builds a consistent 3D semantic scene graph of a
scene given an RGB image sequence. Our method consists of a novel incremental
entity estimation pipeline and a scene graph prediction network. The proposed
pipeline simultaneously reconstructs a sparse point map and fuses entity
estimation from the input images. The proposed network estimates 3D semantic
scene graphs with iterative message passing using multi-view and geometric
features extracted from the scene entities. Extensive experiments on the 3RScan
dataset show the effectiveness of the proposed method in this challenging task,
outperforming state-of-the-art approaches.",None,-1
2cced6ad-d68d-43cb-8848-771d1f3e11d9,FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model,0.677035,"Recently, conditional diffusion models have gained popularity in numerous
applications due to their exceptional generation ability. However, many
existing methods are training-required. They need to train a time-dependent
classifier or a condition-dependent score estimator, which increases the cost
of constructing conditional diffusion models and is inconvenient to transfer
across different conditions. Some current works aim to overcome this limitation
by proposing training-free solutions, but most can only be applied to a
specific category of tasks and not to more general conditions. In this work, we
propose a training-Free conditional Diffusion Model (FreeDoM) used for various
conditions. Specifically, we leverage off-the-shelf pre-trained networks, such
as a face detection model, to construct time-independent energy functions,
which guide the generation process without requiring training. Furthermore,
because the construction of the energy function is very flexible and adaptable
to various conditions, our proposed FreeDoM has a broader range of applications
than existing training-free methods. FreeDoM is advantageous in its simplicity,
effectiveness, and low cost. Experiments demonstrate that FreeDoM is effective
for various conditions and suitable for diffusion models of diverse data
domains, including image and latent code domains.",None,-1
01424dd7-1d6a-478c-810e-23f959c86801,Feature Representation Learning with Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition,0.697824,"Micro-expressions are spontaneous, rapid and subtle facial movements that can
neither be forged nor suppressed. They are very important nonverbal
communication clues, but are transient and of low intensity thus difficult to
recognize. Recently deep learning based methods have been developed for
micro-expression (ME) recognition using feature extraction and fusion
techniques, however, targeted feature learning and efficient feature fusion
still lack further study according to the ME characteristics. To address these
issues, we propose a novel framework Feature Representation Learning with
adaptive Displacement Generation and Transformer fusion (FRL-DGT), in which a
convolutional Displacement Generation Module (DGM) with self-supervised
learning is used to extract dynamic features from onset/apex frames targeted to
the subsequent ME recognition task, and a well-designed Transformer Fusion
mechanism composed of three Transformer-based fusion modules (local, global
fusions based on AU regions and full-face fusion) is applied to extract the
multi-level informative features after DGM for the final ME prediction. The
extensive experiments with solid leave-one-subject-out (LOSO) evaluation
results have demonstrated the superiority of our proposed FRL-DGT to
state-of-the-art methods.",None,-1
3b243a71-cb96-4c05-afc7-5e0b6756d885,SelfAct: Personalized Activity Recognition based on Self-Supervised and Active Learning,0.130981,"Supervised Deep Learning (DL) models are currently the leading approach for
sensor-based Human Activity Recognition (HAR) on wearable and mobile devices.
However, training them requires large amounts of labeled data whose collection
is often time-consuming, expensive, and error-prone. At the same time, due to
the intra- and inter-variability of activity execution, activity models should
be personalized for each user. In this work, we propose SelfAct: a novel
framework for HAR combining self-supervised and active learning to mitigate
these problems. SelfAct leverages a large pool of unlabeled data collected from
many users to pre-train through self-supervision a DL model, with the goal of
learning a meaningful and efficient latent representation of sensor data. The
resulting pre-trained model can be locally used by new users, which will
fine-tune it thanks to a novel unsupervised active learning strategy. Our
experiments on two publicly available HAR datasets demonstrate that SelfAct
achieves results that are close to or even better than the ones of fully
supervised approaches with a small number of active learning queries.",None,-1
ce30cc5f-3e95-4a6f-b249-ad9080590435,BEVScope: Enhancing Self-Supervised Depth Estimation Leveraging Bird's-Eye-View in Dynamic Scenarios,0.431551,"Depth estimation is a cornerstone of perception in autonomous driving and
robotic systems. The considerable cost and relatively sparse data acquisition
of LiDAR systems have led to the exploration of cost-effective alternatives,
notably, self-supervised depth estimation. Nevertheless, current
self-supervised depth estimation methods grapple with several limitations: (1)
the failure to adequately leverage informative multi-camera views. (2) the
limited capacity to handle dynamic objects effectively. To address these
challenges, we present BEVScope, an innovative approach to self-supervised
depth estimation that harnesses Bird's-Eye-View (BEV) features. Concurrently,
we propose an adaptive loss function, specifically designed to mitigate the
complexities associated with moving objects. Empirical evaluations conducted on
the Nuscenes dataset validate our approach, demonstrating competitive
performance. Code will be released at https://github.com/myc634/BEVScope.",None,-1
7f9995ca-158a-4844-b453-3737af83ed3b,"The Good, The Bad, and Why: Unveiling Emotions in Generative AI",0.179283,"Emotion significantly impacts our daily behaviors and interactions. While
recent generative AI models, such as large language models, have shown
impressive performance in various tasks, it remains unclear whether they truly
comprehend emotions. This paper aims to address this gap by incorporating
psychological theories to gain a holistic understanding of emotions in
generative AI models. Specifically, we propose three approaches: 1)
EmotionPrompt to enhance AI model performance, 2) EmotionAttack to impair AI
model performance, and 3) EmotionDecode to explain the effects of emotional
stimuli, both benign and malignant. Through extensive experiments involving
language and multi-modal models on semantic understanding, logical reasoning,
and generation tasks, we demonstrate that both textual and visual EmotionPrompt
can boost the performance of AI models while EmotionAttack can hinder it.
Additionally, EmotionDecode reveals that AI models can comprehend emotional
stimuli akin to the mechanism of dopamine in the human brain. Our work heralds
a novel avenue for exploring psychology to enhance our understanding of
generative AI models.",None,-1
2117b893-3f98-4081-b3eb-e3be3fe18880,Shrinking Embeddings for Hyper-Relational Knowledge Graphs,0.95255,"Link prediction on knowledge graphs (KGs) has been extensively studied on
binary relational KGs, wherein each fact is represented by a triple. A
significant amount of important knowledge, however, is represented by
hyper-relational facts where each fact is composed of a primal triple and a set
of qualifiers comprising a key-value pair that allows for expressing more
complicated semantics. Although some recent works have proposed to embed
hyper-relational KGs, these methods fail to capture essential inference
patterns of hyper-relational facts such as qualifier monotonicity, qualifier
implication, and qualifier mutual exclusion, limiting their generalization
capability. To unlock this, we present \emph{ShrinkE}, a geometric
hyper-relational KG embedding method aiming to explicitly model these patterns.
ShrinkE models the primal triple as a spatial-functional transformation from
the head into a relation-specific box. Each qualifier ``shrinks'' the box to
narrow down the possible answer set and, thus, realizes qualifier monotonicity.
The spatial relationships between the qualifier boxes allow for modeling core
inference patterns of qualifiers such as implication and mutual exclusion.
Experimental results demonstrate ShrinkE's superiority on three benchmarks of
hyper-relational KGs.",None,-1
1341bf11-5c77-462d-aa17-c1e473a091bb,SPEED: Speculative Pipelined Execution for Efficient Decoding,0.4657,"Generative Large Language Models (LLMs) based on the Transformer architecture
have recently emerged as a dominant foundation model for a wide range of
Natural Language Processing tasks. Nevertheless, their application in real-time
scenarios has been highly restricted due to the significant inference latency
associated with these models. This is particularly pronounced due to the
autoregressive nature of generative LLM inference, where tokens are generated
sequentially since each token depends on all previous output tokens. It is
therefore challenging to achieve any token-level parallelism, making inference
extremely memory-bound. In this work, we propose SPEED, which improves
inference efficiency by speculatively executing multiple future tokens in
parallel with the current token using predicted values based on early-layer
hidden states. For Transformer decoders that employ parameter sharing, the
memory operations for the tokens executing in parallel can be amortized, which
allows us to accelerate generative LLM inference. We demonstrate the efficiency
of our method in terms of latency reduction relative to model accuracy and
demonstrate how speculation allows for training deeper decoders with parameter
sharing with minimal runtime overhead.",None,-1
09b1228c-842e-4f8e-8a63-4d2d22c91285,Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8,0.937686,"This paper introduces a software architecture for real-time object detection
using machine learning (ML) in an augmented reality (AR) environment. Our
approach uses the recent state-of-the-art YOLOv8 network that runs onboard on
the Microsoft HoloLens 2 head-mounted display (HMD). The primary motivation
behind this research is to enable the application of advanced ML models for
enhanced perception and situational awareness with a wearable, hands-free AR
platform. We show the image processing pipeline for the YOLOv8 model and the
techniques used to make it real-time on the resource-limited edge computing
platform of the headset. The experimental results demonstrate that our solution
achieves real-time processing without needing offloading tasks to the cloud or
any other external servers while retaining satisfactory accuracy regarding the
usual mAP metric and measured qualitative performance",None,-1
40e660db-50ba-4201-b78f-707648ec000f,Heterogeneous Neuronal and Synaptic Dynamics for Spike-Efficient Unsupervised Learning: Theory and Design Principles,0.788563,"This paper shows that the heterogeneity in neuronal and synaptic dynamics
reduces the spiking activity of a Recurrent Spiking Neural Network (RSNN) while
improving prediction performance, enabling spike-efficient (unsupervised)
learning. We analytically show that the diversity in neurons'
integration/relaxation dynamics improves an RSNN's ability to learn more
distinct input patterns (higher memory capacity), leading to improved
classification and prediction performance. We further prove that heterogeneous
Spike-Timing-Dependent-Plasticity (STDP) dynamics of synapses reduce spiking
activity but preserve memory capacity. The analytical results motivate
Heterogeneous RSNN design using Bayesian optimization to determine
heterogeneity in neurons and synapses to improve $\mathcal{E}$, defined as the
ratio of spiking activity and memory capacity. The empirical results on time
series classification and prediction tasks show that optimized HRSNN increases
performance and reduces spiking activity compared to a homogeneous RSNN.",None,-1
e0993d33-65f5-49eb-a94b-033481e10db3,Pay More Attention to Relation Exploration for Knowledge Base Question Answering,0.193465,"Knowledge base question answering (KBQA) is a challenging task that aims to
retrieve correct answers from large-scale knowledge bases. Existing attempts
primarily focus on entity representation and final answer reasoning, which
results in limited supervision for this task. Moreover, the relations, which
empirically determine the reasoning path selection, are not fully considered in
recent advancements. In this study, we propose a novel framework, RE-KBQA, that
utilizes relations in the knowledge base to enhance entity representation and
introduce additional supervision. We explore guidance from relations in three
aspects, including (1) distinguishing similar entities by employing a
variational graph auto-encoder to learn relation importance; (2) exploring
extra supervision by predicting relation distributions as soft labels with a
multi-task scheme; (3) designing a relation-guided re-ranking algorithm for
post-processing. Experimental results on two benchmark datasets demonstrate the
effectiveness and superiority of our framework, improving the F1 score by 5.7%
from 40.5 to 46.3 on CWQ and 5.8% from 62.8 to 68.5 on WebQSP, better or on par
with state-of-the-art methods.",None,-1
d319fd44-fc55-470d-b13d-da1e064e7fee,Gated-ViGAT: Efficient Bottom-Up Event Recognition and Explanation Using a New Frame Selection Policy and Gating Mechanism,0.270146,"In this paper, Gated-ViGAT, an efficient approach for video event
recognition, utilizing bottom-up (object) information, a new frame sampling
policy and a gating mechanism is proposed. Specifically, the frame sampling
policy uses weighted in-degrees (WiDs), derived from the adjacency matrices of
graph attention networks (GATs), and a dissimilarity measure to select the most
salient and at the same time diverse frames representing the event in the
video. Additionally, the proposed gating mechanism fetches the selected frames
sequentially, and commits early-exiting when an adequately confident decision
is achieved. In this way, only a few frames are processed by the
computationally expensive branch of our network that is responsible for the
bottom-up information extraction. The experimental evaluation on two large,
publicly available video datasets (MiniKinetics, ActivityNet) demonstrates that
Gated-ViGAT provides a large computational complexity reduction in comparison
to our previous approach (ViGAT), while maintaining the excellent event
recognition and explainability performance. Gated-ViGAT source code is made
publicly available at https://github.com/bmezaris/Gated-ViGAT",None,-1
263d513e-ae3e-4745-9e48-1307d89194b9,A Laplace-inspired Distribution on SO(3) for Probabilistic Rotation Estimation,0.773099,"Estimating the 3DoF rotation from a single RGB image is an important yet
challenging problem. Probabilistic rotation regression has raised more and more
attention with the benefit of expressing uncertainty information along with the
prediction. Though modeling noise using Gaussian-resembling Bingham
distribution and matrix Fisher distribution is natural, they are shown to be
sensitive to outliers for the nature of quadratic punishment to deviations. In
this paper, we draw inspiration from multivariate Laplace distribution and
propose a novel Rotation Laplace distribution on SO(3). Rotation Laplace
distribution is robust to the disturbance of outliers and enforces much
gradient to the low-error region, resulting in a better convergence. Our
extensive experiments show that our proposed distribution achieves
state-of-the-art performance for rotation regression tasks over both
probabilistic and non-probabilistic baselines. Our project page is at
https://pku-epic.github.io/RotationLaplace.",None,-1
97ce64fe-16ed-4668-bb82-9778831a3f06,SEM-CS: Semantic CLIPStyler for Text-Based Image Style Transfer,0.0920407,"CLIPStyler demonstrated image style transfer with realistic textures using
only the style text description (instead of requiring a reference style image).
However, the ground semantics of objects in style transfer output is lost due
to style spillover on salient and background objects (content mismatch) or
over-stylization. To solve this, we propose Semantic CLIPStyler (Sem-CS) that
performs semantic style transfer. Sem-CS first segments the content image into
salient and non-salient objects and then transfers artistic style based on a
given style text description. The semantic style transfer is achieved using
global foreground loss (for salient objects) and global background loss (for
non-salient objects). Our empirical results, including DISTS, NIMA and user
study scores, show that our proposed framework yields superior qualitative and
quantitative performance.",None,-1
0221c9ef-aad6-4622-9508-4bfc93eabbf2,Long-range Multimodal Pretraining for Movie Understanding,0.101349,"Learning computer vision models from (and for) movies has a long-standing
history. While great progress has been attained, there is still a need for a
pretrained multimodal model that can perform well in the ever-growing set of
movie understanding tasks the community has been establishing. In this work, we
introduce Long-range Multimodal Pretraining, a strategy, and a model that
leverages movie data to train transferable multimodal and cross-modal encoders.
Our key idea is to learn from all modalities in a movie by observing and
extracting relationships over a long-range. After pretraining, we run ablation
studies on the LVU benchmark and validate our modeling choices and the
importance of learning from long-range time spans. Our model achieves
state-of-the-art on several LVU tasks while being much more data efficient than
previous works. Finally, we evaluate our model's transferability by setting a
new state-of-the-art in five different benchmarks.",None,-1
f1454ebd-89e0-4022-96d3-136c926ac949,Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking,0.842551,"Standard Full-Data classifiers in NLP demand thousands of labeled examples,
which is impractical in data-limited domains. Few-shot methods offer an
alternative, utilizing contrastive learning techniques that can be effective
with as little as 20 examples per class. Similarly, Large Language Models
(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.
However, the performance-cost trade-offs of these methods remain underexplored,
a critical concern for budget-limited organizations. Our work addresses this
gap by studying the aforementioned approaches over the Banking77 financial
intent detection dataset, including the evaluation of cutting-edge LLMs by
OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We
complete the picture with two additional methods: first, a cost-effective
querying method for LLMs based on retrieval-augmented generation (RAG), able to
reduce operational costs multiple times compared to classic few-shot
approaches, and second, a data augmentation method using GPT-4, able to improve
performance in data-limited scenarios. Finally, to inspire future research, we
provide a human expert's curated subset of Banking77, along with extensive
error analysis.",None,-1
e872d741-a85b-4212-8a41-5e3380cb9729,Unlearnable Graph: Protecting Graphs from Unauthorized Exploitation,0.149928,"While the use of graph-structured data in various fields is becoming
increasingly popular, it also raises concerns about the potential unauthorized
exploitation of personal data for training commercial graph neural network
(GNN) models, which can compromise privacy. To address this issue, we propose a
novel method for generating unlearnable graph examples. By injecting delusive
but imperceptible noise into graphs using our Error-Minimizing Structural
Poisoning (EMinS) module, we are able to make the graphs unexploitable.
Notably, by modifying only $5\%$ at most of the potential edges in the graph
data, our method successfully decreases the accuracy from ${77.33\%}$ to
${42.47\%}$ on the COLLAB dataset.",None,-1
b0acbeb4-3991-47f7-96ae-553fbf9427e3,SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks,0.868656,"We introduce SwiftSage, a novel agent framework inspired by the dual-process
theory of human cognition, designed to excel in action planning for complex
interactive reasoning tasks. SwiftSage integrates the strengths of behavior
cloning and prompting large language models (LLMs) to enhance task completion
performance. The framework comprises two primary modules: the Swift module,
representing fast and intuitive thinking, and the Sage module, emulating
deliberate thought processes. The Swift module is a small encoder-decoder LM
fine-tuned on the oracle agent's action trajectories, while the Sage module
employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a
heuristic method to harmoniously integrate the two modules, resulting in a more
efficient and robust problem-solving process. In 30 tasks from the ScienceWorld
benchmark, SwiftSage significantly outperforms other methods such as SayCan,
ReAct, and Reflexion, demonstrating its effectiveness in solving complex
interactive tasks.",None,-1
29ac76b7-5b6f-4e0c-abaf-05c071c95f12,Joint fMRI Decoding and Encoding with Latent Embedding Alignment,0.670227,"The connection between brain activity and corresponding visual stimuli is
crucial in comprehending the human brain. While deep generative models have
exhibited advancement in recovering brain recordings by generating images
conditioned on fMRI signals, accomplishing high-quality generation with
consistent semantics continues to pose challenges. Moreover, the prediction of
brain activity from visual stimuli remains a formidable undertaking. In this
paper, we introduce a unified framework that addresses both fMRI decoding and
encoding. Commencing with the establishment of two latent spaces capable of
representing and reconstructing fMRI signals and visual images, respectively,
we proceed to align the fMRI signals and visual images within the latent space,
thereby enabling a bidirectional transformation between the two domains. Our
Latent Embedding Alignment (LEA) model concurrently recovers visual stimuli
from fMRI signals and predicts brain activity from images within a unified
framework. The performance of LEA surpasses that of existing methods on
multiple benchmark fMRI decoding and encoding datasets. By integrating fMRI
decoding and encoding, LEA offers a comprehensive solution for modeling the
intricate relationship between brain activity and visual stimuli.",None,-1
107b6a79-8e93-44f9-ac26-c6665a7908b0,Rounding Meets Approximate Model Counting,0.231499,"The problem of model counting, also known as #SAT, is to compute the number
of models or satisfying assignments of a given Boolean formula $F$. Model
counting is a fundamental problem in computer science with a wide range of
applications. In recent years, there has been a growing interest in using
hashing-based techniques for approximate model counting that provide
$(\varepsilon, \delta)$-guarantees: i.e., the count returned is within a
$(1+\varepsilon)$-factor of the exact count with confidence at least
$1-\delta$. While hashing-based techniques attain reasonable scalability for
large enough values of $\delta$, their scalability is severely impacted for
smaller values of $\delta$, thereby preventing their adoption in application
domains that require estimates with high confidence.
  The primary contribution of this paper is to address the Achilles heel of
hashing-based techniques: we propose a novel approach based on rounding that
allows us to achieve a significant reduction in runtime for smaller values of
$\delta$. The resulting counter, called RoundMC, achieves a substantial runtime
performance improvement over the current state-of-the-art counter, ApproxMC. In
particular, our extensive evaluation over a benchmark suite consisting of 1890
instances shows that RoundMC solves 204 more instances than ApproxMC, and
achieves a $4\times$ speedup over ApproxMC.",None,-1
a298e444-5b8f-4204-9490-d54908fc4eb0,StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized Tokenizer of a Large-Scale Generative Model,0.614412,"Despite the progress made in the style transfer task, most previous work
focus on transferring only relatively simple features like color or texture,
while missing more abstract concepts such as overall art expression or
painter-specific traits. However, these abstract semantics can be captured by
models like DALL-E or CLIP, which have been trained using huge datasets of
images and textual documents. In this paper, we propose StylerDALLE, a style
transfer method that exploits both of these models and uses natural language to
describe abstract art styles. Specifically, we formulate the language-guided
style transfer task as a non-autoregressive token sequence translation, i.e.,
from input content image to output stylized image, in the discrete latent space
of a large-scale pretrained vector-quantized tokenizer, e.g., the discrete
variational auto-encoder (dVAE) of DALL-E. To incorporate style information, we
propose a Reinforcement Learning strategy with CLIP-based language supervision
that ensures stylization and content preservation simultaneously. Experimental
results demonstrate the superiority of our method, which can effectively
transfer art styles using language instructions at different granularities.
Code is available at https://github.com/zipengxuc/StylerDALLE.",None,-1
fc32ea84-9f47-44db-919f-2b936430b176,WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset,0.487612,"We present WebQAmGaze, a multilingual low-cost eye-tracking-while-reading
dataset, designed as the first webcam-based eye-tracking corpus of reading to
support the development of explainable computational language processing
models. WebQAmGaze includes webcam eye-tracking data from 600 participants of a
wide age range naturally reading English, German, Spanish, and Turkish texts.
Each participant performs two reading tasks composed of five texts each, a
normal reading and an information-seeking task, followed by a comprehension
question. We compare the collected webcam data to high-quality eye-tracking
recordings. The results show a moderate to strong correlation between the eye
movement measures obtained with the webcam compared to those obtained with a
commercial eye-tracking device. When validating the data, we find that higher
fixation duration on relevant text spans accurately indicates correctness when
answering the corresponding questions. This dataset advances webcam-based
reading studies and opens avenues to low-cost and diverse data collection.
WebQAmGaze is beneficial to learn about the cognitive processes behind
question-answering and to apply these insights to computational models of
language understanding.",None,-1
f841e596-33d7-4f9c-a59c-65380578fb80,Learning Domain-Independent Heuristics for Grounded and Lifted Planning,0.335351,"We present three novel graph representations of planning tasks suitable for
learning domain-independent heuristics using Graph Neural Networks (GNNs) to
guide search. In particular, to mitigate the issues caused by large grounded
GNNs we present the first method for learning domain-independent heuristics
with only the lifted representation of a planning task. We also provide a
theoretical analysis of the expressiveness of our models, showing that some are
more powerful than STRIPS-HGN, the only other existing model for learning
domain-independent heuristics. Our experiments show that our heuristics
generalise to much larger problems than those in the training set, vastly
surpassing STRIPS-HGN heuristics.",None,-1
1f8e9bc8-4dcd-499f-8e04-fff2384257cf,LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models,0.985911,"We propose LLM-Eval, a unified multi-dimensional automatic evaluation method
for open-domain conversations with large language models (LLMs). Existing
evaluation methods often rely on human annotations, ground-truth responses, or
multiple LLM prompts, which can be expensive and time-consuming. To address
these issues, we design a single prompt-based evaluation method that leverages
a unified evaluation schema to cover multiple dimensions of conversation
quality in a single model call. We extensively evaluate the performance of
LLM-Eval on various benchmark datasets, demonstrating its effectiveness,
efficiency, and adaptability compared to state-of-the-art evaluation methods.
Our analysis also highlights the importance of choosing suitable LLMs and
decoding strategies for accurate evaluation results. LLM-Eval offers a
versatile and robust solution for evaluating open-domain conversation systems,
streamlining the evaluation process and providing consistent performance across
diverse scenarios.",None,-1
1983ac34-d6fe-401c-8298-accd4c94e9fc,DoCoFL: Downlink Compression for Cross-Device Federated Learning,0.313495,"Many compression techniques have been proposed to reduce the communication
overhead of Federated Learning training procedures. However, these are
typically designed for compressing model updates, which are expected to decay
throughout training. As a result, such methods are inapplicable to downlink
(i.e., from the parameter server to clients) compression in the cross-device
setting, where heterogeneous clients $\textit{may appear only once}$ during
training and thus must download the model parameters. Accordingly, we propose
$\textsf{DoCoFL}$ -- a new framework for downlink compression in the
cross-device setting. Importantly, $\textsf{DoCoFL}$ can be seamlessly combined
with many uplink compression schemes, rendering it suitable for bi-directional
compression. Through extensive evaluation, we show that $\textsf{DoCoFL}$
offers significant bi-directional bandwidth reduction while achieving
competitive accuracy to that of a baseline without any compression.",None,-1
02889e54-f8e7-48be-a7fe-30bd7939fd66,Story Visualization by Online Text Augmentation with Context Memory,0.139489,"Story visualization (SV) is a challenging text-to-image generation task for
the difficulty of not only rendering visual details from the text descriptions
but also encoding a long-term context across multiple sentences. While prior
efforts mostly focus on generating a semantically relevant image for each
sentence, encoding a context spread across the given paragraph to generate
contextually convincing images (e.g., with a correct character or with a proper
background of the scene) remains a challenge. To this end, we propose a novel
memory architecture for the Bi-directional Transformer framework with an online
text augmentation that generates multiple pseudo-descriptions as supplementary
supervision during training for better generalization to the language variation
at inference. In extensive experiments on the two popular SV benchmarks, i.e.,
the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms
the state of the arts in various metrics including FID, character F1, frame
accuracy, BLEU-2/3, and R-precision with similar or less computational
complexity.",None,-1
14660284-1109-457b-91dd-cd32cf1d7760,CNN-based Methods for Object Recognition with High-Resolution Tactile Sensors,0.644145,"Novel high-resolution pressure-sensor arrays allow treating pressure readings
as standard images. Computer vision algorithms and methods such as
Convolutional Neural Networks (CNN) can be used to identify contact objects. In
this paper, a high-resolution tactile sensor has been attached to a robotic
end-effector to identify contacted objects. Two CNN-based approaches have been
employed to classify pressure images. These methods include a transfer learning
approach using a pre-trained CNN on an RGB-images dataset and a custom-made CNN
(TactNet) trained from scratch with tactile information. The transfer learning
approach can be carried out by retraining the classification layers of the
network or replacing these layers with an SVM. Overall, 11 configurations based
on these methods have been tested: 8 transfer learning-based, and 3
TactNet-based. Moreover, a study of the performance of the methods and a
comparative discussion with the current state-of-the-art on tactile object
recognition is presented.",None,-1
d3d7bc16-b490-45b5-82d8-4dc1a5f3e312,Towards Understanding the Generalization of Medical Text-to-SQL Models and Datasets,0.994752,"Electronic medical records (EMRs) are stored in relational databases. It can
be challenging to access the required information if the user is unfamiliar
with the database schema or general database fundamentals. Hence, researchers
have explored text-to-SQL generation methods that provide healthcare
professionals direct access to EMR data without needing a database expert.
However, currently available datasets have been essentially ""solved"" with
state-of-the-art models achieving accuracy greater than or near 90%. In this
paper, we show that there is still a long way to go before solving text-to-SQL
generation in the medical domain. To show this, we create new splits of the
existing medical text-to-SQL dataset MIMICSQL that better measure the
generalizability of the resulting models. We evaluate state-of-the-art language
models on our new split showing substantial drops in performance with accuracy
dropping from up to 92% to 28%, thus showing substantial room for improvement.
Moreover, we introduce a novel data augmentation approach to improve the
generalizability of the language models. Overall, this paper is the first step
towards developing more robust text-to-SQL models in the medical
domain.\footnote{The dataset and code will be released upon acceptance.",None,-1
21350c14-3800-44fc-a302-990c892cefe3,Stochastic Submodular Maximization via Polynomial Estimators,0.27375,"In this paper, we study stochastic submodular maximization problems with
general matroid constraints, that naturally arise in online learning, team
formation, facility location, influence maximization, active learning and
sensing objective functions. In other words, we focus on maximizing submodular
functions that are defined as expectations over a class of submodular functions
with an unknown distribution. We show that for monotone functions of this form,
the stochastic continuous greedy algorithm attains an approximation ratio (in
expectation) arbitrarily close to $(1-1/e) \approx 63\%$ using a polynomial
estimation of the gradient. We argue that using this polynomial estimator
instead of the prior art that uses sampling eliminates a source of randomness
and experimentally reduces execution time.",None,-1
bcfe0fbd-cf5b-4856-8409-8a235b4976ef,ERTIM@MC2: Diversified Argumentative Tweets Retrieval,0.461543,"In this paper, we present our participation to CLEF MC2 2018 edition for the
task 2 Mining opinion argumentation. It consists in detecting the most
argumentative and diverse Tweets about some festivals in English and French
from a massive multilingual collection. We measure argumentativity of a Tweet
computing the amount of argumentation compounds it contains. We consider
argumentation compounds as a combination between opinion expression and its
support with facts and a particular structuration. Regarding diversity, we
consider the amount of festival aspects covered by Tweets. An initial step
filters the original dataset to fit the language and topic requirements of the
task. Then, we compute and integrate linguistic descriptors to detect claims
and their respective justifications in Tweets. The final step extracts the most
diverse arguments by clustering Tweets according to their textual content and
selecting the most argumentative ones from each cluster. We conclude the paper
describing the different ways we combined the descriptors among the different
runs we submitted and discussing their results.",None,-1
36326e28-eb08-4fc4-b847-6b555f4324d2,ResMem: Learn what you can and memorize the rest,0.177318,"The impressive generalization performance of modern neural networks is
attributed in part to their ability to implicitly memorize complex training
patterns. Inspired by this, we explore a novel mechanism to improve model
generalization via explicit memorization. Specifically, we propose the
residual-memorization (ResMem) algorithm, a new method that augments an
existing prediction model (e.g. a neural network) by fitting the model's
residuals with a $k$-nearest neighbor based regressor. The final prediction is
then the sum of the original model and the fitted residual regressor. By
construction, ResMem can explicitly memorize the training labels. Empirically,
we show that ResMem consistently improves the test set generalization of the
original prediction model across various standard vision and natural language
processing benchmarks. Theoretically, we formulate a stylized linear regression
problem and rigorously show that ResMem results in a more favorable test risk
over the base predictor.",None,-1
f1d418de-b10d-43c1-b7f5-cebda32a90ef,Exploring Speaker-Related Information in Spoken Language Understanding for Better Speaker Diarization,0.452244,"Speaker diarization(SD) is a classic task in speech processing and is crucial
in multi-party scenarios such as meetings and conversations. Current mainstream
speaker diarization approaches consider acoustic information only, which result
in performance degradation when encountering adverse acoustic conditions. In
this paper, we propose methods to extract speaker-related information from
semantic content in multi-party meetings, which, as we will show, can further
benefit speaker diarization. We introduce two sub-tasks, Dialogue Detection and
Speaker-Turn Detection, in which we effectively extract speaker information
from conversational semantics. We also propose a simple yet effective algorithm
to jointly model acoustic and semantic information and obtain
speaker-identified texts. Experiments on both AISHELL-4 and AliMeeting datasets
show that our method achieves consistent improvements over acoustic-only
speaker diarization systems.",None,-1
d24eed19-13a3-4734-9e4a-0abe1393aeb8,Compensation Learning in Semantic Segmentation,0.103225,"Label noise and ambiguities between similar classes are challenging problems
in developing new models and annotating new data for semantic segmentation. In
this paper, we propose Compensation Learning in Semantic Segmentation, a
framework to identify and compensate ambiguities as well as label noise. More
specifically, we add a ground truth depending and globally learned bias to the
classification logits and introduce a novel uncertainty branch for neural
networks to induce the compensation bias only to relevant regions. Our method
is employed into state-of-the-art segmentation frameworks and several
experiments demonstrate that our proposed compensation learns inter-class
relations that allow global identification of challenging ambiguities as well
as the exact localization of subsequent label noise. Additionally, it enlarges
robustness against label noise during training and allows target-oriented
manipulation during inference. We evaluate the proposed method on %the widely
used datasets Cityscapes, KITTI-STEP, ADE20k, and COCO-stuff10k.",None,-1
0af68721-586e-4689-b292-546cdb234c64,Summarizing Strategy Card Game AI Competition,0.493273,"This paper concludes five years of AI competitions based on Legends of Code
and Magic (LOCM), a small Collectible Card Game (CCG), designed with the goal
of supporting research and algorithm development. The game was used in a number
of events, including Community Contests on the CodinGame platform, and Strategy
Card Game AI Competition at the IEEE Congress on Evolutionary Computation and
IEEE Conference on Games. LOCM has been used in a number of publications
related to areas such as game tree search algorithms, neural networks,
evaluation functions, and CCG deckbuilding. We present the rules of the game,
the history of organized competitions, and a listing of the participant and
their approaches, as well as some general advice on organizing AI competitions
for the research community. Although the COG 2022 edition was announced to be
the last one, the game remains available and can be played using an online
leaderboard arena.",None,-1
c97620f7-de25-4a41-8711-43b6bcd54b06,"Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5",0.699482,"The use of large language models (LLMs) in healthcare is gaining popularity,
but their practicality and safety in clinical settings have not been thoroughly
assessed. In high-stakes environments like medical settings, trust and safety
are critical issues for LLMs. To address these concerns, we present an approach
to evaluate the performance and trustworthiness of a GPT3.5 model for medical
image protocol assignment. We compare it with a fine-tuned BERT model and a
radiologist. In addition, we have a radiologist review the GPT3.5 output to
evaluate its decision-making process. Our evaluation dataset consists of 4,700
physician entries across 11 imaging protocol classes spanning the entire head.
Our findings suggest that the GPT3.5 performance falls behind BERT and a
radiologist. However, GPT3.5 outperforms BERT in its ability to explain its
decision, detect relevant word indicators, and model calibration. Furthermore,
by analyzing the explanations of GPT3.5 for misclassifications, we reveal
systematic errors that need to be resolved to enhance its safety and
suitability for clinical use.",None,-1
85f31b57-2b2b-4187-8220-4c54dda97aa4,CO-BED: Information-Theoretic Contextual Optimization via Bayesian Experimental Design,0.436021,"We formalize the problem of contextual optimization through the lens of
Bayesian experimental design and propose CO-BED -- a general, model-agnostic
framework for designing contextual experiments using information-theoretic
principles. After formulating a suitable information-based objective, we employ
black-box variational methods to simultaneously estimate it and optimize the
designs in a single stochastic gradient scheme. In addition, to accommodate
discrete actions within our framework, we propose leveraging continuous
relaxation schemes, which can naturally be integrated into our variational
objective. As a result, CO-BED provides a general and automated solution to a
wide range of contextual optimization problems. We illustrate its effectiveness
in a number of experiments, where CO-BED demonstrates competitive performance
even when compared to bespoke, model-specific alternatives.",None,-1
4ec59f39-d8de-4e2a-9d56-a11b85a4d51b,Continual Multimodal Knowledge Graph Construction,0.877479,"Current Multimodal Knowledge Graph Construction (MKGC) models struggle with
the real-world dynamism of continuously emerging entities and relations, often
succumbing to catastrophic forgetting-loss of previously acquired knowledge.
This study introduces benchmarks aimed at fostering the development of the
continual MKGC domain. We further introduce MSPT framework, designed to
surmount the shortcomings of existing MKGC approaches during multimedia data
processing. MSPT harmonizes the retention of learned knowledge (stability) and
the integration of new data (plasticity), outperforming current continual
learning and multimodal methods. Our results confirm MSPT's superior
performance in evolving knowledge environments, showcasing its capacity to
navigate balance between stability and plasticity.",None,-1
98108413-9e5f-4eac-a0ff-c264bad1866e,I2I: Initializing Adapters with Improvised Knowledge,0.343971,"Adapters present a promising solution to the catastrophic forgetting problem
in continual learning. However, training independent Adapter modules for every
new task misses an opportunity for cross-task knowledge transfer. We propose
Improvise to Initialize (I2I), a continual learning algorithm that initializes
Adapters for incoming tasks by distilling knowledge from previously-learned
tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning
benchmark, by conducting experiments on sequences of visual question answering
tasks. Adapters trained with I2I consistently achieve better task accuracy than
independently-trained Adapters, demonstrating that our algorithm facilitates
knowledge transfer between task Adapters. I2I also results in better cross-task
knowledge transfer than the state-of-the-art AdapterFusion without incurring
the associated parametric cost.",None,-1
49de96e7-0a30-4965-8fca-c9c766f402d1,LCDnet: A Lightweight Crowd Density Estimation Model for Real-time Video Surveillance,0.985602,"Automatic crowd counting using density estimation has gained significant
attention in computer vision research. As a result, a large number of crowd
counting and density estimation models using convolution neural networks (CNN)
have been published in the last few years. These models have achieved good
accuracy over benchmark datasets. However, attempts to improve the accuracy
often lead to higher complexity in these models. In real-time video
surveillance applications using drones with limited computing resources, deep
models incur intolerable higher inference delay. In this paper, we propose (i)
a Lightweight Crowd Density estimation model (LCDnet) for real-time video
surveillance, and (ii) an improved training method using curriculum learning
(CL). LCDnet is trained using CL and evaluated over two benchmark datasets
i.e., DroneRGBT and CARPK. Results are compared with existing crowd models. Our
evaluation shows that the LCDnet achieves a reasonably good accuracy while
significantly reducing the inference time and memory requirement and thus can
be deployed over edge devices with very limited computing resources.",None,-1
201645a4-e5c4-421d-ac4d-40ebf0151705,CIRO: COVID-19 infection risk ontology,0.333431,"Public health authorities perform contact tracing for highly contagious
agents to identify close contacts with the infected cases. However, during the
pandemic caused by coronavirus disease 2019 (COVID-19), this operation was not
employed in countries with high patient volumes. Meanwhile, the Japanese
government conducted this operation, thereby contributing to the control of
infections, at the cost of arduous manual labor by public health officials. To
ease the burden of the officials, this study attempted to automate the
assessment of each person's infection risk through an ontology, called COVID-19
Infection Risk Ontology (CIRO). This ontology expresses infection risks of
COVID-19 formulated by the Japanese government, toward automated assessment of
infection risks of individuals, using Resource Description Framework (RDF) and
SPARQL (SPARQL Protocol and RDF Query Language) queries. For evaluation, we
demonstrated that the knowledge graph built could infer the risks, formulated
by the government. Moreover, we conducted reasoning experiments to analyze the
computational efficiency. The experiments demonstrated usefulness of the
knowledge processing, and identified issues left for deployment.",None,-1
939c4dd5-2b72-4768-8770-8eebcd4079d7,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,0.634853,"Despite the impressive results achieved by many existing Structure from
Motion (SfM) approaches, there is still a need to improve the robustness,
accuracy, and efficiency on large-scale scenes with many outlier matches and
sparse view graphs. In this paper, we propose AdaSfM: a coarse-to-fine adaptive
SfM approach that is scalable to large-scale and challenging datasets. Our
approach first does a coarse global SfM which improves the reliability of the
view graph by leveraging measurements from low-cost sensors such as Inertial
Measurement Units (IMUs) and wheel encoders. Subsequently, the view graph is
divided into sub-scenes that are refined in parallel by a fine local
incremental SfM regularised by the result from the coarse global SfM to improve
the camera registration accuracy and alleviate scene drifts. Finally, our
approach uses a threshold-adaptive strategy to align all local reconstructions
to the coordinate frame of global SfM. Extensive experiments on large-scale
benchmark datasets show that our approach achieves state-of-the-art accuracy
and efficiency.",None,-1
c4c3ad7c-b777-499c-a587-598bc7eeba46,When to generate hedges in peer-tutoring interactions,0.40219,"This paper explores the application of machine learning techniques to predict
where hedging occurs in peer-tutoring interactions. The study uses a
naturalistic face-to-face dataset annotated for natural language turns,
conversational strategies, tutoring strategies, and nonverbal behaviours. These
elements are processed into a vector representation of the previous turns,
which serves as input to several machine learning models. Results show that
embedding layers, that capture the semantic information of the previous turns,
significantly improves the model's performance. Additionally, the study
provides insights into the importance of various features, such as
interpersonal rapport and nonverbal behaviours, in predicting hedges by using
Shapley values for feature explanation. We discover that the eye gaze of both
the tutor and the tutee has a significant impact on hedge prediction. We
further validate this observation through a follow-up ablation study.",None,-1
9fb1b49f-e44b-4673-a464-471a599a7fed,MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine,0.134135,"METHODS: First, a set of evaluation criteria is designed based on a
comprehensive literature review. Second, existing candidate criteria are
optimized for using a Delphi method by five experts in medicine and
engineering. Third, three clinical experts design a set of medical datasets to
interact with LLMs. Finally, benchmarking experiments are conducted on the
datasets. The responses generated by chatbots based on LLMs are recorded for
blind evaluations by five licensed medical experts. RESULTS: The obtained
evaluation criteria cover medical professional capabilities, social
comprehensive capabilities, contextual capabilities, and computational
robustness, with sixteen detailed indicators. The medical datasets include
twenty-seven medical dialogues and seven case reports in Chinese. Three
chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor
PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental
results show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both
multiple-turn medical dialogue and case report scenarios.",None,-1
ff6abf9d-9a86-4342-88b7-dbd765a73ff2,A Dual-way Enhanced Framework from Text Matching Point of View for Multimodal Entity Linking,0.49754,"Multimodal Entity Linking (MEL) aims at linking ambiguous mentions with
multimodal information to entity in Knowledge Graph (KG) such as Wikipedia,
which plays a key role in many applications. However, existing methods suffer
from shortcomings, including modality impurity such as noise in raw image and
ambiguous textual entity representation, which puts obstacles to MEL. We
formulate multimodal entity linking as a neural text matching problem where
each multimodal information (text and image) is treated as a query, and the
model learns the mapping from each query to the relevant entity from candidate
entities. This paper introduces a dual-way enhanced (DWE) framework for MEL:
(1) our model refines queries with multimodal data and addresses semantic gaps
using cross-modal enhancers between text and image information. Besides, DWE
innovatively leverages fine-grained image attributes, including facial
characteristic and scene feature, to enhance and refine visual features. (2)By
using Wikipedia descriptions, DWE enriches entity semantics and obtains more
comprehensive textual representation, which reduces between textual
representation and the entities in KG. Extensive experiments on three public
benchmarks demonstrate that our method achieves state-of-the-art (SOTA)
performance, indicating the superiority of our model. The code is released on
https://github.com/season1blue/DWE",None,-1
6b6b0ef0-ec9f-489a-a7f9-37178842ea4f,Fake News Detectors are Biased against Texts Generated by Large Language Models,0.878794,"The spread of fake news has emerged as a critical challenge, undermining
trust and posing threats to society. In the era of Large Language Models
(LLMs), the capability to generate believable fake content has intensified
these concerns. In this study, we present a novel paradigm to evaluate fake
news detectors in scenarios involving both human-written and LLM-generated
misinformation. Intriguingly, our findings reveal a significant bias in many
existing detectors: they are more prone to flagging LLM-generated content as
fake news while often misclassifying human-written fake news as genuine. This
unexpected bias appears to arise from distinct linguistic patterns inherent to
LLM outputs. To address this, we introduce a mitigation strategy that leverages
adversarial training with LLM-paraphrased genuine news. The resulting model
yielded marked improvements in detection accuracy for both human and
LLM-generated news. To further catalyze research in this domain, we release two
comprehensive datasets, \texttt{GossipCop++} and \texttt{PolitiFact++}, thus
amalgamating human-validated articles with LLM-generated fake and real news.",None,-1
1f96303e-e8a1-4a72-bb8a-ccd4e5dcf817,Attention-based Point Cloud Edge Sampling,0.828372,"Point cloud sampling is a less explored research topic for this data
representation. The most commonly used sampling methods are still classical
random sampling and farthest point sampling. With the development of neural
networks, various methods have been proposed to sample point clouds in a
task-based learning manner. However, these methods are mostly generative-based,
rather than selecting points directly using mathematical statistics. Inspired
by the Canny edge detection algorithm for images and with the help of the
attention mechanism, this paper proposes a non-generative Attention-based Point
cloud Edge Sampling method (APES), which captures salient points in the point
cloud outline. Both qualitative and quantitative experimental results show the
superior performance of our sampling method on common benchmark tasks.",None,-1
0d998313-e7ab-4e0d-8303-b271abc93c58,Multi-Task Learning with Prior Information,0.0368907,"Multi-task learning aims to boost the generalization performance of multiple
related tasks simultaneously by leveraging information contained in those
tasks. In this paper, we propose a multi-task learning framework, where we
utilize prior knowledge about the relations between features. We also impose a
penalty on the coefficients changing for each specific feature to ensure
related tasks have similar coefficients on common features shared among them.
In addition, we capture a common set of features via group sparsity. The
objective is formulated as a non-smooth convex optimization problem, which can
be solved with various methods, including gradient descent method with fixed
stepsize, iterative shrinkage-thresholding algorithm (ISTA) with back-tracking,
and its variation -- fast iterative shrinkage-thresholding algorithm (FISTA).
In light of the sub-linear convergence rate of the methods aforementioned, we
propose an asymptotically linear convergent algorithm with theoretical
guarantee. Empirical experiments on both regression and classification tasks
with real-world datasets demonstrate that our proposed algorithms are capable
of improving the generalization performance of multiple related tasks.",None,-1
7f6a27ac-c5b8-4831-86ea-c45fbda19a3c,Aligning Bag of Regions for Open-Vocabulary Object Detection,0.856991,"Pre-trained vision-language models (VLMs) learn to align vision and language
representations on large-scale datasets, where each image-text pair usually
contains a bag of semantic concepts. However, existing open-vocabulary object
detectors only align region embeddings individually with the corresponding
features extracted from the VLMs. Such a design leaves the compositional
structure of semantic concepts in a scene under-exploited, although the
structure may be implicitly learned by the VLMs. In this work, we propose to
align the embedding of bag of regions beyond individual regions. The proposed
method groups contextually interrelated regions as a bag. The embeddings of
regions in a bag are treated as embeddings of words in a sentence, and they are
sent to the text encoder of a VLM to obtain the bag-of-regions embedding, which
is learned to be aligned to the corresponding features extracted by a frozen
VLM. Applied to the commonly used Faster R-CNN, our approach surpasses the
previous best results by 4.6 box AP50 and 2.8 mask AP on novel categories of
open-vocabulary COCO and LVIS benchmarks, respectively. Code and models are
available at https://github.com/wusize/ovdet.",None,-1
2937dc75-51d0-4add-a855-78c4b1fa9e75,PersonalTailor: Personalizing 2D Pattern Design from 3D Garment Point Clouds,0.413855,"Garment pattern design aims to convert a 3D garment to the corresponding 2D
panels and their sewing structure. Existing methods rely either on template
fitting with heuristics and prior assumptions, or on model learning with
complicated shape parameterization. Importantly, both approaches do not allow
for personalization of the output garment, which today has increasing demands.
To fill this demand, we introduce PersonalTailor: a personalized 2D pattern
design method, where the user can input specific constraints or demands (in
language or sketch) for personal 2D panel fabrication from 3D point clouds.
PersonalTailor first learns a multi-modal panel embeddings based on
unsupervised cross-modal association and attentive fusion. It then predicts a
binary panel masks individually using a transformer encoder-decoder framework.
Extensive experiments show that our PersonalTailor excels on both personalized
and standard pattern fabrication tasks.",None,-1
57df0947-c111-4410-a01a-99d7cb458102,The Study of Highway for Lifelong Multi-Agent Path Finding,0.29588,"In modern fulfillment warehouses, agents traverse the map to complete endless
tasks that arrive on the fly, which is formulated as a lifelong Multi-Agent
Path Finding (lifelong MAPF) problem. The goal of tackling this challenging
problem is to find the path for each agent in a finite runtime while maximizing
the throughput. However, existing methods encounter exponential growth of
runtime and undesirable phenomena of deadlocks and rerouting as the map size or
agent density grows. To address these challenges in lifelong MAPF, we explore
the idea of highways mainly studied for one-shot MAPF (i.e., finding paths at
once beforehand), which reduces the complexity of the problem by encouraging
agents to move in the same direction. We utilize two methods to incorporate the
highway idea into the lifelong MAPF framework and discuss the properties that
minimize the existing problems of deadlocks and rerouting. The experimental
results demonstrate that the runtime is considerably reduced and the decay of
throughput is gradually insignificant as the map size enlarges under the
settings of the highway. Furthermore, when the density of agents increases, the
phenomena of deadlocks and rerouting are significantly reduced by leveraging
the highway.",None,-1
48c7f5c4-7db1-46ee-9f9e-bb5c35b2fd8b,From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data,0.703824,"Large Language Models (LLMs) exhibit exceptional abilities for causal
analysis between concepts in numerous societally impactful domains, including
medicine, science, and law. Recent research on LLM performance in various
causal discovery and inference tasks has given rise to a new ladder in the
classical three-stage framework of causality. In this paper, we advance the
current research of LLM-driven causal discovery by proposing a novel framework
that combines knowledge-based LLM causal analysis with data-driven causal
structure learning. To make LLM more than a query tool and to leverage its
power in discovering natural and new laws of causality, we integrate the
valuable LLM expertise on existing causal mechanisms into statistical analysis
of objective data to build a novel and practical baseline for causal structure
learning.
  We introduce a universal set of prompts designed to extract causal graphs
from given variables and assess the influence of LLM prior causality on
recovering causal structures from data. We demonstrate the significant
enhancement of LLM expertise on the quality of recovered causal structures from
data, while also identifying critical challenges and issues, along with
potential approaches to address them. As a pioneering study, this paper aims to
emphasize the new frontier that LLMs are opening for classical causal discovery
and inference, and to encourage the widespread adoption of LLM capabilities in
data-driven causal analysis.",None,-1
a6edab57-ba16-43c0-9b83-65ff0c7e8412,T5-SR: A Unified Seq-to-Seq Decoding Strategy for Semantic Parsing,0.13947,"Translating natural language queries into SQLs in a seq2seq manner has
attracted much attention recently. However, compared with
abstract-syntactic-tree-based SQL generation, seq2seq semantic parsers face
much more challenges, including poor quality on schematical information
prediction and poor semantic coherence between natural language queries and
SQLs. This paper analyses the above difficulties and proposes a
seq2seq-oriented decoding strategy called SR, which includes a new intermediate
representation SSQL and a reranking method with score re-estimator to solve the
above obstacles respectively. Experimental results demonstrate the
effectiveness of our proposed techniques and T5-SR-3b achieves new
state-of-the-art results on the Spider dataset.",None,-1
35482194-3c62-46f7-a8a5-b095d5308926,Exploiting Unlabeled Data for Feedback Efficient Human Preference based Reinforcement Learning,0.0575029,"Preference Based Reinforcement Learning has shown much promise for utilizing
human binary feedback on queried trajectory pairs to recover the underlying
reward model of the Human in the Loop (HiL). While works have attempted to
better utilize the queries made to the human, in this work we make two
observations about the unlabeled trajectories collected by the agent and
propose two corresponding loss functions that ensure participation of unlabeled
trajectories in the reward learning process, and structure the embedding space
of the reward model such that it reflects the structure of state space with
respect to action distances. We validate the proposed method on one locomotion
domain and one robotic manipulation task and compare with the state-of-the-art
baseline PEBBLE. We further present an ablation of the proposed loss components
across both the domains and find that not only each of the loss components
perform better than the baseline, but the synergic combination of the two has
much better reward recovery and human feedback sample efficiency.",None,-1
52e36a62-1b9f-4c92-b766-2b92111ce5e3,How to Design Translation Prompts for ChatGPT: An Empirical Study,0.886096,"The recently released ChatGPT has demonstrated surprising abilities in
natural language understanding and natural language generation. Machine
translation relies heavily on the abilities of language understanding and
generation. Thus, in this paper, we explore how to assist machine translation
with ChatGPT. We adopt several translation prompts on a wide range of
translations. Our experimental results show that ChatGPT with designed
translation prompts can achieve comparable or better performance over
commercial translation systems for high-resource language translations. We
further evaluate the translation quality using multiple references, and ChatGPT
achieves superior performance compared to commercial systems. We also conduct
experiments on domain-specific translations, the final results show that
ChatGPT is able to comprehend the provided domain keyword and adjust
accordingly to output proper translations. At last, we perform few-shot prompts
that show consistent improvement across different base prompts. Our work
provides empirical evidence that ChatGPT still has great potential in
translations.",None,-1
37229caf-9caf-4ae3-975b-c9b4aee4014f,SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries,0.709166,"Current Deep Network (DN) visualization and interpretability methods rely
heavily on data space visualizations such as scoring which dimensions of the
data are responsible for their associated prediction or generating new data
features or samples that best match a given DN unit or representation. In this
paper, we go one step further by developing the first provably exact method for
computing the geometry of a DN's mapping - including its decision boundary -
over a specified region of the data space. By leveraging the theory of
Continuous Piece-Wise Linear (CPWL) spline DNs, SplineCam exactly computes a
DNs geometry without resorting to approximations such as sampling or
architecture simplification. SplineCam applies to any DN architecture based on
CPWL nonlinearities, including (leaky-)ReLU, absolute value, maxout, and
max-pooling and can also be applied to regression DNs such as implicit neural
representations. Beyond decision boundary visualization and characterization,
SplineCam enables one to compare architectures, measure generalizability and
sample from the decision boundary on or off the manifold. Project Website:
bit.ly/splinecam.",None,-1
03933926-306b-49a1-a85d-f1e60cc845b5,OneShotSTL: One-Shot Seasonal-Trend Decomposition For Online Time Series Anomaly Detection And Forecasting,0.767675,"Seasonal-trend decomposition is one of the most fundamental concepts in time
series analysis that supports various downstream tasks, including time series
anomaly detection and forecasting. However, existing decomposition methods rely
on batch processing with a time complexity of O(W), where W is the number of
data points within a time window. Therefore, they cannot always efficiently
support real-time analysis that demands low processing delay. To address this
challenge, we propose OneShotSTL, an efficient and accurate algorithm that can
decompose time series online with an update time complexity of O(1). OneShotSTL
is more than $1,000$ times faster than the batch methods, with accuracy
comparable to the best counterparts. Extensive experiments on real-world
benchmark datasets for downstream time series anomaly detection and forecasting
tasks demonstrate that OneShotSTL is from 10 to over 1,000 times faster than
the state-of-the-art methods, while still providing comparable or even better
accuracy.",None,-1
a164849c-f363-4c2d-b513-155eaeef2e23,SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues,0.816174,"In second language vocabulary learning, existing works have primarily focused
on either the learning interface or scheduling personalized retrieval practices
to maximize memory retention. However, the learning content, i.e., the
information presented on flashcards, has mostly remained constant. Keyword
mnemonic is a notable learning strategy that relates new vocabulary to existing
knowledge by building an acoustic and imagery link using a keyword that sounds
alike. Beyond that, producing verbal and visual cues associated with the
keyword to facilitate building these links requires a manual process and is not
scalable. In this paper, we explore an opportunity to use large language models
to automatically generate verbal and visual cues for keyword mnemonics. Our
approach, an end-to-end pipeline for auto-generating verbal and visual cues,
can automatically generate highly memorable cues. We investigate the
effectiveness of our approach via a human participant experiment by comparing
it with manually generated cues.",None,-1
109f1995-05fa-4319-a72b-83866cb5abe9,Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection,0.843841,"Out-of-distribution (OOD) detection is essential for reliable and trustworthy
machine learning. Recent multi-modal OOD detection leverages textual
information from in-distribution (ID) class names for visual OOD detection, yet
it currently neglects the rich contextual information of ID classes. Large
language models (LLMs) encode a wealth of world knowledge and can be prompted
to generate descriptive features for each class. Indiscriminately using such
knowledge causes catastrophic damage to OOD detection due to LLMs'
hallucinations, as is observed by our analysis. In this paper, we propose to
apply world knowledge to enhance OOD detection performance through selective
generation from LLMs. Specifically, we introduce a consistency-based
uncertainty calibration method to estimate the confidence score of each
generation. We further extract visual objects from each image to fully
capitalize on the aforementioned world knowledge. Extensive experiments
demonstrate that our method consistently outperforms the state-of-the-art.",None,-1
b674f653-2060-4516-aab0-63129d2087be,SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting,0.996609,"Building end-to-end task bots and maintaining their integration with new
functionalities using minimal human efforts is a long-standing challenge in
dialog research. Recently large language models (LLMs) have demonstrated
exceptional proficiency in conversational engagement and adherence to
instructions across various downstream tasks. In this work, we introduce
SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems
effortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we
instruct fixed LLMs to generate appropriate responses on novel tasks,
circumventing the need for training data. Specifically, SGP-TOD comprises three
components: a LLM for engaging with users, a DST Prompter to aid the LLM with
dialog state tracking, which is then used to retrieve database items, and a
Policy Prompter to elicit proper responses adhering to the provided dialog
policy. Experimental results on Multiwoz, RADDLE and STAR datasets show that
our training-free strategy SGP-TOD, without any task-specific data, yields
state-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot
approaches. In a domain-extension setting, SGP-TOD aptly adapts to new
functionalities by merely adding supplementary schema rules. We make our code
and data publicly available.",None,-1
8dd7d443-3e99-40df-93b6-75aa633a66be,Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers,0.756061,"Fine-grained classification is a challenging task that involves identifying
subtle differences between objects within the same category. This task is
particularly challenging in scenarios where data is scarce. Visual transformers
(ViT) have recently emerged as a powerful tool for image classification, due to
their ability to learn highly expressive representations of visual data using
self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine
tuned using semi-supervised learning techniques, suitable for situations where
we have lack of annotated data. This is particularly common in e-commerce,
where images are readily available but labels are noisy, nonexistent, or
expensive to obtain. Our results demonstrate that Semi-ViT outperforms
traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned
with limited annotated data. These findings indicate that Semi-ViTs hold
significant promise for applications that require precise and fine-grained
classification of visual data.",None,-1
67c0a7b1-af9d-4cb6-9d08-e70141a99012,CrossKD: Cross-Head Knowledge Distillation for Object Detection,0.174006,"Knowledge Distillation (KD) has been validated as an effective model
compression technique for learning compact object detectors. Existing
state-of-the-art KD methods for object detection are mostly based on feature
imitation. In this paper, we present a general and effective prediction
mimicking distillation scheme, called CrossKD, which delivers the intermediate
features of the student's detection head to the teacher's detection head. The
resulting cross-head predictions are then forced to mimic the teacher's
predictions. This manner relieves the student's head from receiving
contradictory supervision signals from the annotations and the teacher's
predictions, greatly improving the student's detection performance. Moreover,
as mimicking the teacher's predictions is the target of KD, CrossKD offers more
task-oriented information in contrast with feature imitation. On MS COCO, with
only prediction mimicking losses applied, our CrossKD boosts the average
precision of GFL ResNet-50 with 1x training schedule from 40.2 to 43.7,
outperforming all existing KD methods. In addition, our method also works well
when distilling detectors with heterogeneous backbones. Code is available at
https://github.com/jbwang1997/CrossKD.",None,-1
0dd2d5dd-12f4-4790-a7ac-26aa98289137,Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene Graphs,0.620706,"Dynamic scene graphs generated from video clips could help enhance the
semantic visual understanding in a wide range of challenging tasks such as
environmental perception, autonomous navigation, and task planning of
self-driving vehicles and mobile robots. In the process of temporal and spatial
modeling during dynamic scene graph generation, it is particularly intractable
to learn time-variant relations in dynamic scene graphs among frames. In this
paper, we propose a Time-variant Relation-aware TRansformer (TR$^2$), which
aims to model the temporal change of relations in dynamic scene graphs.
Explicitly, we leverage the difference of text embeddings of prompted sentences
about relation labels as the supervision signal for relations. In this way,
cross-modality feature guidance is realized for the learning of time-variant
relations. Implicitly, we design a relation feature fusion module with a
transformer and an additional message token that describes the difference
between adjacent frames. Extensive experiments on the Action Genome dataset
prove that our TR$^2$ can effectively model the time-variant relations. TR$^2$
significantly outperforms previous state-of-the-art methods under two different
settings by 2.1% and 2.6% respectively.",None,-1
2bb70616-8c5b-41f6-b270-5c3ce6cc3b7f,An Open Dataset and Model for Language Identification,0.574042,"Language identification (LID) is a fundamental step in many natural language
processing pipelines. However, current LID systems are far from perfect,
particularly on lower-resource languages. We present a LID model which achieves
a macro-average F1 score of 0.93 and a false positive rate of 0.033 across 201
languages, outperforming previous work. We achieve this by training on a
curated dataset of monolingual data, the reliability of which we ensure by
auditing a sample from each source and each language manually. We make both the
model and the dataset available to the research community. Finally, we carry
out detailed analysis into our model's performance, both in comparison to
existing open models and by language class.",None,-1
d5bb9cf2-4cc8-403f-9f30-454e5a1cf28f,Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures,0.841617,"Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.",None,-1
1b620616-fa01-422b-b541-afe300722dfd,Adversary for Social Good: Leveraging Adversarial Attacks to Protect Personal Attribute Privacy,0.173135,"Social media has drastically reshaped the world that allows billions of
people to engage in such interactive environments to conveniently create and
share content with the public. Among them, text data (e.g., tweets, blogs)
maintains the basic yet important social activities and generates a rich source
of user-oriented information. While those explicit sensitive user data like
credentials has been significantly protected by all means, personal private
attribute (e.g., age, gender, location) disclosure due to inference attacks is
somehow challenging to avoid, especially when powerful natural language
processing (NLP) techniques have been effectively deployed to automate
attribute inferences from implicit text data. This puts users' attribute
privacy at risk. To address this challenge, in this paper, we leverage the
inherent vulnerability of machine learning to adversarial attacks, and design a
novel text-space Adversarial attack for Social Good, called Adv4SG. In other
words, we cast the problem of protecting personal attribute privacy as an
adversarial attack formulation problem over the social media text data to
defend against NLP-based attribute inference attacks. More specifically, Adv4SG
proceeds with a sequence of word perturbations under given constraints such
that the probed attribute cannot be identified correctly. Different from the
prior works, we advance Adv4SG by considering social media property, and
introducing cost-effective mechanisms to expedite attribute obfuscation over
text data under the black-box setting. Extensive experiments on real-world
social media datasets have demonstrated that our method can effectively degrade
the inference accuracy with less computational cost over different attribute
settings, which substantially helps mitigate the impacts of inference attacks
and thus achieve high performance in user attribute privacy protection.",None,-1
35430489-967a-4891-8e64-05318d5c4b59,Improving Expert Specialization in Mixture of Experts,0.333294,"Mixture of experts (MoE), introduced over 20 years ago, is the simplest gated
modular neural network architecture. There is renewed interest in MoE because
the conditional computation allows only parts of the network to be used during
each inference, as was recently demonstrated in large scale natural language
processing models. MoE is also of potential interest for continual learning, as
experts may be reused for new tasks, and new experts introduced. The gate in
the MoE architecture learns task decompositions and individual experts learn
simpler functions appropriate to the gate's decomposition. In this paper: (1)
we show that the original MoE architecture and its training method do not
guarantee intuitive task decompositions and good expert utilization, indeed
they can fail spectacularly even for simple data such as MNIST and
FashionMNIST; (2) we introduce a novel gating architecture, similar to
attention, that improves performance and results in a lower entropy task
decomposition; and (3) we introduce a novel data-driven regularization that
improves expert specialization. We empirically validate our methods on MNIST,
FashionMNIST and CIFAR-100 datasets.",None,-1
5a94c8ac-a5d5-40bc-bf57-ba2043814f07,Stress Testing Chain-of-Thought Prompting for Large Language Models,0.00965626,"This report examines the effectiveness of Chain-of-Thought (CoT) prompting in
improving the multi-step reasoning abilities of large language models (LLMs).
Inspired by previous studies \cite{Min2022RethinkingWork}, we analyze the
impact of three types of CoT prompt perturbations, namely CoT order, CoT
values, and CoT operators on the performance of GPT-3 on various tasks. Our
findings show that incorrect CoT prompting leads to poor performance on
accuracy metrics. Correct values in the CoT is crucial for predicting correct
answers. Moreover, incorrect demonstrations, where the CoT operators or the CoT
order are wrong, do not affect the performance as drastically when compared to
the value based perturbations. This research deepens our understanding of CoT
prompting and opens some new questions regarding the capability of LLMs to
learn reasoning in context.",None,-1
e79e8333-bc54-4c68-b140-26157bc2b616,Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions,0.737271,"We propose a method for synthesizing edited photo-realistic digital avatars
with text instructions. Given a short monocular RGB video and text
instructions, our method uses an image-conditioned diffusion model to edit one
head image and uses the video stylization method to accomplish the editing of
other head images. Through iterative training and update (three times or more),
our method synthesizes edited photo-realistic animatable 3D neural head avatars
with a deformable neural radiance field head synthesis method. In quantitative
and qualitative studies on various subjects, our method outperforms
state-of-the-art methods.",None,-1
c151c890-846b-4613-a12b-b308795042e7,Interactive Class-Agnostic Object Counting,0.279156,"We propose a novel framework for interactive class-agnostic object counting,
where a human user can interactively provide feedback to improve the accuracy
of a counter. Our framework consists of two main components: a user-friendly
visualizer to gather feedback and an efficient mechanism to incorporate it. In
each iteration, we produce a density map to show the current prediction result,
and we segment it into non-overlapping regions with an easily verifiable number
of objects. The user can provide feedback by selecting a region with obvious
counting errors and specifying the range for the estimated number of objects
within it. To improve the counting result, we develop a novel adaptation loss
to force the visual counter to output the predicted count within the
user-specified range. For effective and efficient adaptation, we propose a
refinement module that can be used with any density-based visual counter, and
only the parameters in the refinement module will be updated during adaptation.
Our experiments on two challenging class-agnostic object counting benchmarks,
FSCD-LVIS and FSC-147, show that our method can reduce the mean absolute error
of multiple state-of-the-art visual counters by roughly 30% to 40% with minimal
user input. Our project can be found at
https://yifehuang97.github.io/ICACountProjectPage/.",None,-1
7a08087e-993b-4bea-a412-1de25c9188e5,Semidefinite Relaxations for Robust Multiview Triangulation,0.431987,"We propose an approach based on convex relaxations for certifiably optimal
robust multiview triangulation. To this end, we extend existing relaxation
approaches to non-robust multiview triangulation by incorporating a truncated
least squares cost function. We propose two formulations, one based on epipolar
constraints and one based on fractional reprojection constraints. The first is
lower dimensional and remains tight under moderate noise and outlier levels,
while the second is higher dimensional and therefore slower but remains tight
even under extreme noise and outlier levels. We demonstrate through extensive
experiments that the proposed approaches allow us to compute provably optimal
reconstructions even under significant noise and a large percentage of
outliers.",None,-1
55a0d6c1-0773-4ff6-98fa-c3647a5ecf9c,Training Socially Aligned Language Models on Simulated Social Interactions,0.30797,"Social alignment in AI systems aims to ensure that these models behave
according to established societal values. However, unlike humans, who derive
consensus on value judgments through social interaction, current language
models (LMs) are trained to rigidly replicate their training corpus in
isolation, leading to subpar generalization in unfamiliar scenarios and
vulnerability to adversarial attacks. This work presents a novel training
paradigm that permits LMs to learn from simulated social interactions. In
comparison to existing methodologies, our approach is considerably more
scalable and efficient, demonstrating superior performance in alignment
benchmarks and human evaluations. This paradigm shift in the training of LMs
brings us a step closer to developing AI systems that can robustly and
accurately reflect societal norms and values.",None,-1
eec7e77f-96f4-4d9e-9b75-ae818fa3fb27,Spatiotemporally Consistent HDR Indoor Lighting Estimation,0.758518,"We propose a physically-motivated deep learning framework to solve a general
version of the challenging indoor lighting estimation problem. Given a single
LDR image with a depth map, our method predicts spatially consistent lighting
at any given image position. Particularly, when the input is an LDR video
sequence, our framework not only progressively refines the lighting prediction
as it sees more regions, but also preserves temporal consistency by keeping the
refinement smooth. Our framework reconstructs a spherical Gaussian lighting
volume (SGLV) through a tailored 3D encoder-decoder, which enables spatially
consistent lighting prediction through volume ray tracing, a hybrid blending
network for detailed environment maps, an in-network Monte-Carlo rendering
layer to enhance photorealism for virtual object insertion, and recurrent
neural networks (RNN) to achieve temporally consistent lighting prediction with
a video sequence as the input. For training, we significantly enhance the
OpenRooms public dataset of photorealistic synthetic indoor scenes with around
360K HDR environment maps of much higher resolution and 38K video sequences,
rendered with GPU-based path tracing. Experiments show that our framework
achieves lighting prediction with higher quality compared to state-of-the-art
single-image or video-based methods, leading to photorealistic AR applications
such as object insertion.",None,-1
5054dd8f-f13e-4a8c-87cc-0e02787a897f,Interactive Learning of Hierarchical Tasks from Dialog with GPT,0.0582038,"We present a system for interpretable, symbolic, interactive task learning
from dialog using a GPT model as a conversational front-end. The learned tasks
are represented as hierarchical decompositions of predicate-argument structures
with scoped variable arguments. By using a GPT model to convert interactive
dialog into a semantic representation, and then recursively asking for
definitions of unknown steps, we show that hierarchical task knowledge can be
acquired and re-used in a natural and unrestrained conversational environment.
We compare our system to a similar architecture using a more conventional
parser and show that our system tolerates a much wider variety of linguistic
variance.",None,-1
2bc96f05-7ad0-4b79-b1fd-be87a301c02b,DiffAVA: Personalized Text-to-Audio Generation with Visual Alignment,0.441575,"Text-to-audio (TTA) generation is a recent popular problem that aims to
synthesize general audio given text descriptions. Previous methods utilized
latent diffusion models to learn audio embedding in a latent space with text
embedding as the condition. However, they ignored the synchronization between
audio and visual content in the video, and tended to generate audio mismatching
from video frames. In this work, we propose a novel and personalized
text-to-sound generation approach with visual alignment based on latent
diffusion models, namely DiffAVA, that can simply fine-tune lightweight
visual-text alignment modules with frozen modality-specific encoders to update
visual-aligned text embeddings as the condition. Specifically, our DiffAVA
leverages a multi-head attention transformer to aggregate temporal information
from video features, and a dual multi-modal residual network to fuse temporal
visual representations with text embeddings. Then, a contrastive learning
objective is applied to match visual-aligned text embeddings with audio
features. Experimental results on the AudioCaps dataset demonstrate that the
proposed DiffAVA can achieve competitive performance on visual-aligned
text-to-audio generation.",None,-1
830bbf8a-1612-4289-b2be-3a78ddd06876,ProAgent: Building Proactive Cooperative Agents with Large Language Models,0.999988,"Building agents with adaptive behavior in cooperative tasks stands as a
paramount goal in the realm of multi-agent systems. Current approaches to
developing cooperative agents rely primarily on learning-based methods, whose
policy generalization depends heavily on the diversity of teammates they
interact with during the training phase. Such reliance, however, constrains the
agents' capacity for strategic adaptation when cooperating with unfamiliar
teammates, which becomes a significant challenge in zero-shot coordination
scenarios. To address this challenge, we propose ProAgent, a novel framework
that harnesses large language models (LLMs) to create proactive agents capable
of dynamically adapting their behavior to enhance cooperation with teammates.
ProAgent can analyze the present state, and infer the intentions of teammates
from observations. It then updates its beliefs in alignment with the teammates'
subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of
modularity and interpretability, making it easily integrated into various of
coordination scenarios. Experimental evaluations conducted within the
Overcooked-AI environment unveil the remarkable performance superiority of
ProAgent, outperforming five methods based on self-play and population-based
training when cooperating with AI agents. Furthermore, in partnered with human
proxy models, its performance exhibits an average improvement exceeding 10%
compared to the current state-of-the-art method. For more information about our
project, please visit~\url{https://pku-proagent.github.io}.",None,-1
71660206-b470-477c-a73b-9fa1a39064a8,Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection,0.0850383,"Detecting the anomaly of human behavior is paramount to timely recognizing
endangering situations, such as street fights or elderly falls. However,
anomaly detection is complex since anomalous events are rare and because it is
an open set recognition task, i.e., what is anomalous at inference has not been
observed at training. We propose COSKAD, a novel model that encodes skeletal
human motion by a graph convolutional network and learns to COntract SKeletal
kinematic embeddings onto a latent hypersphere of minimum volume for Video
Anomaly Detection. We propose three latent spaces: the commonly-adopted
Euclidean and the novel spherical and hyperbolic. All variants outperform the
state-of-the-art on the most recent UBnormal dataset, for which we contribute a
human-related version with annotated skeletons. COSKAD sets a new
state-of-the-art on the human-related versions of ShanghaiTech Campus and CUHK
Avenue, with performance comparable to video-based methods. Source code and
dataset will be released upon acceptance.",None,-1
092edb3b-3bd5-4b0c-a7ea-4b2d35484d54,Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction,0.74759,"Recently, aspect sentiment quad prediction has received widespread attention
in the field of aspect-based sentiment analysis. Existing studies extract
quadruplets via pre-trained generative language models to paraphrase the
original sentence into a templated target sequence. However, previous works
only focus on what to generate but ignore what not to generate. We argue that
considering the negative samples also leads to potential benefits. In this
work, we propose a template-agnostic method to control the token-level
generation, which boosts original learning and reduces mistakes simultaneously.
Specifically, we introduce Monte Carlo dropout to understand the built-in
uncertainty of pre-trained language models, acquiring the noises and errors. We
further propose marginalized unlikelihood learning to suppress the
uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to
balance the effects of marginalized unlikelihood learning. Extensive
experiments on four public datasets demonstrate the effectiveness of our
approach on various generation templates.",None,-1
9663ee05-e78e-4cf6-90de-425d0df5add7,Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue,0.981205,"Goal-directed dialogue systems aim to proactively reach a pre-determined
target through multi-turn conversations. The key to achieving this task lies in
planning dialogue paths that smoothly and coherently direct conversations
towards the target. However, this is a challenging and under-explored task. In
this work, we propose a coherent dialogue planning approach that uses a
stochastic process to model the temporal dynamics of dialogue paths. We define
a latent space that captures the coherence of goal-directed behavior using a
Brownian bridge process, which allows us to incorporate user feedback flexibly
in dialogue planning. Based on the derived latent trajectories, we generate
dialogue paths explicitly using pre-trained language models. We finally employ
these paths as natural language prompts to guide dialogue generation. Our
experiments show that our approach generates more coherent utterances and
achieves the goal with a higher success rate.",None,-1
4a6eb4cc-7dcd-41d3-b733-80583ba118a9,Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement,0.624176,"Few-shot learning (FSL) is popular due to its ability to adapt to novel
classes. Compared with inductive few-shot learning, transductive models
typically perform better as they leverage all samples of the query set. The two
existing classes of methods, prototype-based and graph-based, have the
disadvantages of inaccurate prototype estimation and sub-optimal graph
construction with kernel functions, respectively. In this paper, we propose a
novel prototype-based label propagation to solve these issues. Specifically,
our graph construction is based on the relation between prototypes and samples
rather than between samples. As prototypes are being updated, the graph
changes. We also estimate the label of each prototype instead of considering a
prototype be the class centre. On mini-ImageNet, tiered-ImageNet, CIFAR-FS and
CUB datasets, we show the proposed method outperforms other state-of-the-art
methods in transductive FSL and semi-supervised FSL when some unlabeled data
accompanies the novel few-shot task.",None,-1
66fdbc6d-575f-4801-bdeb-0a46bfc29838,Applying Plain Transformers to Real-World Point Clouds,0.134066,"To apply transformer-based models to point cloud understanding, many previous
works modify the architecture of transformers by using, e.g., local attention
and down-sampling. Although they have achieved promising results, earlier works
on transformers for point clouds have two issues. First, the power of plain
transformers is still under-explored. Second, they focus on simple and small
point clouds instead of complex real-world ones. This work revisits the plain
transformers in real-world point cloud understanding. We first take a closer
look at some fundamental components of plain transformers, e.g., patchifier and
positional embedding, for both efficiency and performance. To close the
performance gap due to the lack of inductive bias and annotated data, we
investigate self-supervised pre-training with masked autoencoder (MAE).
Specifically, we propose drop patch, which prevents information leakage and
significantly improves the effectiveness of MAE. Our models achieve SOTA
results in semantic segmentation on the S3DIS dataset and object detection on
the ScanNet dataset with lower computational costs. Our work provides a new
baseline for future research on transformers for point clouds.",None,-1
04a400cf-ea24-4881-8e2d-fc7ef2aa7623,Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation,0.0892476,"Large Language Models (LLMs) have showcased impressive performance. However,
due to their inability to capture relationships among samples, these frozen
LLMs inevitably keep repeating similar mistakes. In this work, we propose our
Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving
their performance by learning from previous mistakes. Considering data arrives
sequentially, LLMs gradually accumulate rules from incorrect cases, forming a
rule collection. These rules are then utilized by the LLMs to avoid making
similar mistakes when processing subsequent inputs. Moreover, the rules remain
independent of the primary prompts, seamlessly complementing prompt design
strategies. Experimentally, we show that TRAN improves over recent baselines by
a large margin.",None,-1
d519fa45-3a4d-4f66-8e1b-88c27529827c,SSCBench: Monocular 3D Semantic Scene Completion Benchmark in Street Views,0.505358,"Monocular scene understanding is a foundational component of autonomous
systems. Within the spectrum of monocular perception topics, one crucial and
useful task for holistic 3D scene understanding is semantic scene completion
(SSC), which jointly completes semantic information and geometric details from
RGB input. However, progress in SSC, particularly in large-scale street views,
is hindered by the scarcity of high-quality datasets. To address this issue, we
introduce SSCBench, a comprehensive benchmark that integrates scenes from
widely used automotive datasets (e.g., KITTI-360, nuScenes, and Waymo).
SSCBench follows an established setup and format in the community, facilitating
the easy exploration of SSC methods in various street views. We benchmark
models using monocular, trinocular, and point cloud input to assess the
performance gap resulting from sensor coverage and modality. Moreover, we have
unified semantic labels across diverse datasets to simplify cross-domain
generalization testing. We commit to including more datasets and SSC models to
drive further advancements in this field.",None,-1
400e4611-6a39-4393-a935-7957853691cf,Multilingual Speech-to-Speech Translation into Multiple Target Languages,0.367332,"Speech-to-speech translation (S2ST) enables spoken communication between
people talking in different languages. Despite a few studies on multilingual
S2ST, their focus is the multilinguality on the source side, i.e., the
translation from multiple source languages to one target language. We present
the first work on multilingual S2ST supporting multiple target languages.
Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we
equip these key components with multilingual capability. Speech-to-masked-unit
(S2MU) is the multilingual extension of S2U, which applies masking to units
which don't belong to the given target language to reduce the language
interference. We also propose multilingual vocoder which is trained with
language embedding and the auxiliary loss of language identification. On
benchmark translation testsets, our proposed multilingual model shows superior
performance than bilingual models in the translation from English into $16$
target languages.",None,-1
fe5398b9-dc14-4915-88ae-0f7dd58ef1cb,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,0.106787,"In this paper, we develop a novel benchmark suite including both a 2D
synthetic image dataset and a 3D synthetic point cloud dataset. Our work is a
sub-task in the framework of a remanufacturing project, in which small electric
motors are used as fundamental objects. Apart from the given detection,
classification, and segmentation annotations, the key objects also have
multiple learnable attributes with ground truth provided. This benchmark can be
used for computer vision tasks including 2D/3D detection, classification,
segmentation, and multi-attribute learning. It is worth mentioning that most
attributes of the motors are quantified as continuously variable rather than
binary, which makes our benchmark well-suited for the less explored regression
tasks. In addition, appropriate evaluation metrics are adopted or developed for
each task and promising baseline results are provided. We hope this benchmark
can stimulate more research efforts on the sub-domain of object attribute
learning and multi-task learning in the future.",None,-1
a1aae083-0e4f-40a7-a727-cc9cc210580b,OPE-SR: Orthogonal Position Encoding for Designing a Parameter-free Upsampling Module in Arbitrary-scale Image Super-Resolution,0.624222,"Implicit neural representation (INR) is a popular approach for
arbitrary-scale image super-resolution (SR), as a key component of INR,
position encoding improves its representation ability. Motivated by position
encoding, we propose orthogonal position encoding (OPE) - an extension of
position encoding - and an OPE-Upscale module to replace the INR-based
upsampling module for arbitrary-scale image super-resolution. Same as INR, our
OPE-Upscale Module takes 2D coordinates and latent code as inputs; however it
does not require training parameters. This parameter-free feature allows the
OPE-Upscale Module to directly perform linear combination operations to
reconstruct an image in a continuous manner, achieving an arbitrary-scale image
reconstruction. As a concise SR framework, our method has high computing
efficiency and consumes less memory comparing to the state-of-the-art (SOTA),
which has been confirmed by extensive experiments and evaluations. In addition,
our method has comparable results with SOTA in arbitrary scale image
super-resolution. Last but not the least, we show that OPE corresponds to a set
of orthogonal basis, justifying our design principle.",None,-1
888ff41f-e69c-4af5-ba43-38b128e64665,Tag-Based Annotation for Avatar Face Creation,0.0906461,"Currently, digital avatars can be created manually using human images as
reference. Systems such as Bitmoji are excellent producers of detailed avatar
designs, with hundreds of choices for customization. A supervised learning
model could be trained to generate avatars automatically, but the hundreds of
possible options create difficulty in securing non-noisy data to train a model.
As a solution, we train a model to produce avatars from human images using
tag-based annotations. This method provides better annotator agreement, leading
to less noisy data and higher quality model predictions. Our contribution is an
application of tag-based annotation to train a model for avatar face creation.
We design tags for 3 different facial facial features offered by Bitmoji, and
train a model using tag-based annotation to predict the nose.",None,-1
c5da7ad7-c391-4c4e-833c-6acce8c69d8d,Clover: Closed-Loop Verifiable Code Generation,0.230659,"The use of large language models for code generation is a rapidly growing
trend in software development. However, without effective methods for ensuring
the correctness of generated code, this trend could lead to any number of
undesirable outcomes. In this paper, we lay out a vision for addressing this
challenge: the Clover paradigm, short for Closed-Loop Verifiable Code
Generation, which reduces correctness checking to the more accessible problem
of consistency checking. At the core of Clover lies a checker that performs
consistency checks among code, docstrings, and formal annotations. The checker
is implemented using a novel integration of formal verification tools and large
language models. We provide a theoretical analysis to support our thesis that
Clover should be effective at consistency checking. We also empirically
investigate its feasibility on a hand-designed dataset (CloverBench) featuring
annotated Dafny programs at a textbook level of difficulty. Experimental
results show that for this dataset, (i) LLMs are reasonably successful at
automatically generating formal specifications; and (ii) our consistency
checker achieves a promising acceptance rate (up to 87%) for correct instances
while maintaining zero tolerance for incorrect ones (no false positives).",None,-1
5b5f7ccc-81d5-446f-bad1-363e0ed962b0,Generalized Planning in PDDL Domains with Pretrained Large Language Models,0.999973,"Recent work has considered whether large language models (LLMs) can function
as planners: given a task, generate a plan. We investigate whether LLMs can
serve as generalized planners: given a domain and training tasks, generate a
program that efficiently produces plans for other tasks in the domain. In
particular, we consider PDDL domains and use GPT-4 to synthesize Python
programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the
LLM is prompted to summarize the domain and propose a strategy in words before
synthesizing the program; and (2) automated debugging, where the program is
validated with respect to the training tasks, and in case of errors, the LLM is
re-prompted with four types of feedback. We evaluate this approach in seven
PDDL domains and compare it to four ablations and four baselines. Overall, we
find that GPT-4 is a surprisingly powerful generalized planner. We also
conclude that automated debugging is very important, that CoT summarization has
non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two
training tasks are often sufficient for strong generalization.",None,-1
f379f2a7-044d-4bef-a520-f8fc432df276,CLRerNet: Improving Confidence of Lane Detection with LaneIoU,0.478081,"Lane marker detection is a crucial component of the autonomous driving and
driver assistance systems. Modern deep lane detection methods with row-based
lane representation exhibit excellent performance on lane detection benchmarks.
Through preliminary oracle experiments, we firstly disentangle the lane
representation components to determine the direction of our approach. We show
that correct lane positions are already among the predictions of an existing
row-based detector, and the confidence scores that accurately represent
intersection-over-union (IoU) with ground truths are the most beneficial. Based
on the finding, we propose LaneIoU that better correlates with the metric, by
taking the local lane angles into consideration. We develop a novel detector
coined CLRerNet featuring LaneIoU for the target assignment cost and loss
functions aiming at the improved quality of confidence scores. Through careful
and fair benchmark including cross validation, we demonstrate that CLRerNet
outperforms the state-of-the-art by a large margin - enjoying F1 score of
81.43% compared with 80.47% of the existing method on CULane, and 86.47%
compared with 86.10% on CurveLanes.",None,-1
2ba4bb46-4738-4eec-8c67-fcc623e3f93d,SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge,0.59387,"Large Language Models (LLMs) have demonstrated impressive planning abilities
due to their vast ""world knowledge"". Yet, obtaining plans that are both
feasible (grounded in affordances) and cost-effective (in plan length), remains
a challenge, despite recent progress. This contrasts with heuristic planning
methods that employ domain knowledge (formalized in action models such as PDDL)
and heuristic search to generate feasible, optimal plans. Inspired by this, we
propose to combine the power of LLMs and heuristic planning by leveraging the
world knowledge of LLMs and the principles of heuristic search. Our approach,
SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain
knowledge, that evaluates actions' feasibility (Can) and long-term
reward/payoff (Pay), and heuristic search to select the best sequence of
actions. Our contributions are (1) a novel framing of the LLM planning problem
in the context of heuristic planning, (2) integrating grounding and
cost-effective elements into the generated plans, and (3) using heuristic
search over actions. Our extensive evaluations show that our model surpasses
other LLM planning approaches.",None,-1
15d76b52-51a1-4fcd-a3bd-e1c012d37aad,Break It Down: Evidence for Structural Compositionality in Neural Networks,0.718099,"Though modern neural networks have achieved impressive performance in both
vision and language tasks, we know little about the functions that they
implement. One possibility is that neural networks implicitly break down
complex tasks into subroutines, implement modular solutions to these
subroutines, and compose them into an overall solution to a task - a property
we term structural compositionality. Another possibility is that they may
simply learn to match new inputs to learned templates, eliding task
decomposition entirely. Here, we leverage model pruning techniques to
investigate this question in both vision and language across a variety of
architectures, tasks, and pretraining regimens. Our results demonstrate that
models often implement solutions to subroutines via modular subnetworks, which
can be ablated while maintaining the functionality of other subnetworks. This
suggests that neural networks may be able to learn compositionality, obviating
the need for specialized symbolic mechanisms.",None,-1
9a15b6da-4660-4383-8c39-845ed45c807b,Smart Word Suggestions for Writing Assistance,0.10288,"Enhancing word usage is a desired feature for writing assistance. To further
advance research in this area, this paper introduces ""Smart Word Suggestions""
(SWS) task and benchmark. Unlike other works, SWS emphasizes end-to-end
evaluation and presents a more realistic writing assistance scenario. This task
involves identifying words or phrases that require improvement and providing
substitution suggestions. The benchmark includes human-labeled data for
testing, a large distantly supervised dataset for training, and the framework
for evaluation. The test data includes 1,000 sentences written by English
learners, accompanied by over 16,000 substitution suggestions annotated by 10
native speakers. The training dataset comprises over 3.7 million sentences and
12.7 million suggestions generated through rules. Our experiments with seven
baselines demonstrate that SWS is a challenging task. Based on experimental
analysis, we suggest potential directions for future research on SWS. The
dataset and related codes is available at
https://github.com/microsoft/SmartWordSuggestions.",None,-1
5f1f9f83-d8c7-48f6-a72c-0a136e529be0,Character-LLM: A Trainable Agent for Role-Playing,0.982278,"Large language models (LLMs) can be used to serve as agents to simulate human
behaviors, given the powerful ability to understand human instructions and
provide high-quality generated texts. Such ability stimulates us to wonder
whether LLMs can simulate a person in a higher form than simple human
behaviors. Therefore, we aim to train an agent with the profile, experience,
and emotional states of a specific person instead of using limited prompts to
instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs
to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar,
etc. Our method focuses on editing profiles as experiences of a certain
character and training models to be personal simulacra with these experiences.
To assess the effectiveness of our approach, we build a test playground that
interviews trained agents and evaluates whether the agents \textit{memorize}
their characters and experiences. Experimental results show interesting
observations that help build future simulacra of humankind.",None,-1
8215d7cf-8c15-4391-bdce-ea41c1c6f3ea,Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance,0.893414,"Robust obstacle avoidance is one of the critical steps for successful
goal-driven indoor navigation tasks.Due to the obstacle missing in the visual
image and the possible missed detection issue, visual image-based obstacle
avoidance techniques still suffer from unsatisfactory robustness. To mitigate
it, in this paper, we propose a novel implicit obstacle map-driven indoor
navigation framework for robust obstacle avoidance, where an implicit obstacle
map is learned based on the historical trial-and-error experience rather than
the visual image. In order to further improve the navigation efficiency, a
non-local target memory aggregation module is designed to leverage a non-local
network to model the intrinsic relationship between the target semantic and the
target orientation clues during the navigation process so as to mine the most
target-correlated object clues for the navigation decision. Extensive
experimental results on AI2-Thor and RoboTHOR benchmarks verify the excellent
obstacle avoidance and navigation efficiency of our proposed method. The core
source code is available at https://github.com/xwaiyy123/object-navigation.",None,-1
8a8407b6-d7e6-40fc-86c7-d452d47e3392,Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions,0.483054,"Training large language models to follow instructions makes them perform
better on a wide range of tasks and generally become more helpful. However, a
perfectly helpful model will follow even the most malicious instructions and
readily generate harmful content. In this paper, we raise concerns over the
safety of models that only emphasize helpfulness, not harmlessness, in their
instruction-tuning. We show that several popular instruction-tuned models are
highly unsafe. Moreover, we show that adding just 3% safety examples (a few
hundred demonstrations) when fine-tuning a model like LLaMA can substantially
improve its safety. Our safety-tuning does not make models significantly less
capable or helpful as measured by standard benchmarks. However, we do find
exaggerated safety behaviours, where too much safety-tuning makes models refuse
perfectly safe prompts if they superficially resemble unsafe ones. As a whole,
our results illustrate trade-offs in training LLMs to be helpful and training
them to be safe.",None,-1
b32dd8c4-d3a3-440d-b217-75230148c0f1,Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews,0.244491,"We propose a simple approach for weighting self-connecting edges in a Graph
Convolutional Network (GCN) and show its impact on depression detection from
transcribed clinical interviews. To this end, we use a GCN for modeling
non-consecutive and long-distance semantics to classify the transcriptions into
depressed or control subjects. The proposed method aims to mitigate the
limiting assumptions of locality and the equal importance of self-connections
vs. edges to neighboring nodes in GCNs, while preserving attractive features
such as low computational cost, data agnostic, and interpretability
capabilities. We perform an exhaustive evaluation in two benchmark datasets.
Results show that our approach consistently outperforms the vanilla GCN model
as well as previously reported results, achieving an F1=0.84 on both datasets.
Finally, a qualitative analysis illustrates the interpretability capabilities
of the proposed approach and its alignment with previous findings in
psychology.",None,-1
efcd6de4-6869-4326-951b-6239b1894669,Expanding Scope: Adapting English Adversarial Attacks to Chinese,0.248114,"Recent studies have revealed that NLP predictive models are vulnerable to
adversarial attacks. Most existing studies focused on designing attacks to
evaluate the robustness of NLP models in the English language alone. Literature
has seen an increasing need for NLP solutions for other languages. We,
therefore, ask one natural question: whether state-of-the-art (SOTA) attack
methods generalize to other languages. This paper investigates how to adapt
SOTA adversarial attack algorithms in English to the Chinese language. Our
experiments show that attack methods previously applied to English NLP can
generate high-quality adversarial examples in Chinese when combined with proper
text segmentation and linguistic constraints. In addition, we demonstrate that
the generated adversarial examples can achieve high fluency and semantic
consistency by focusing on the Chinese language's morphology and phonology,
which in turn can be used to improve the adversarial robustness of Chinese NLP
models.",None,-1
531a484b-9a21-42ce-a880-a239c5fc2fe2,Extracting Victim Counts from Text,0.158469,"Decision-makers in the humanitarian sector rely on timely and exact
information during crisis events. Knowing how many civilians were injured
during an earthquake is vital to allocate aids properly. Information about such
victim counts is often only available within full-text event descriptions from
newspapers and other reports. Extracting numbers from text is challenging:
numbers have different formats and may require numeric reasoning. This renders
purely string matching-based approaches insufficient. As a consequence,
fine-grained counts of injured, displaced, or abused victims beyond fatalities
are often not extracted and remain unseen. We cast victim count extraction as a
question answering (QA) task with a regression or classification objective. We
compare regex, dependency parsing, semantic role labeling-based approaches, and
advanced text-to-text models. Beyond model accuracy, we analyze extraction
reliability and robustness which are key for this sensitive task. In
particular, we discuss model calibration and investigate few-shot and
out-of-distribution performance. Ultimately, we make a comprehensive
recommendation on which model to select for different desiderata and data
domains. Our work is among the first to apply numeracy-focused large language
models in a real-world use case with a positive impact.",None,-1
3186ff07-ce4d-44a5-b404-15b35da421fe,GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models,0.847993,"Relation extraction (RE) is a crucial task in natural language processing
(NLP) that aims to identify and classify relationships between entities
mentioned in text. In the financial domain, relation extraction plays a vital
role in extracting valuable information from financial documents, such as news
articles, earnings reports, and company filings. This paper describes our
solution to relation extraction on one such dataset REFinD. The dataset was
released along with shared task as a part of the Fourth Workshop on Knowledge
Discovery from Unstructured Data in Financial Services, co-located with SIGIR
2023. In this paper, we employed OpenAI models under the framework of
in-context learning (ICL). We utilized two retrieval strategies to find top K
relevant in-context learning demonstrations / examples from training data for a
given test example. The first retrieval mechanism, we employed, is a
learning-free dense retriever and the other system is a learning-based
retriever. We were able to achieve 3rd rank overall. Our best F1-score is
0.718.",None,-1
355b1df0-1bcc-44ed-b221-62caaee5f813,FengWu: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead,1.0,"We present FengWu, an advanced data-driven global medium-range weather
forecast system based on Artificial Intelligence (AI). Different from existing
data-driven weather forecast methods, FengWu solves the medium-range forecast
problem from a multi-modal and multi-task perspective. Specifically, a deep
learning architecture equipped with model-specific encoder-decoders and
cross-modal fusion Transformer is elaborately designed, which is learned under
the supervision of an uncertainty loss to balance the optimization of different
predictors in a region-adaptive manner. Besides this, a replay buffer mechanism
is introduced to improve medium-range forecast performance. With 39-year data
training based on the ERA5 reanalysis, FengWu is able to accurately reproduce
the atmospheric dynamics and predict the future land and atmosphere states at
37 vertical levels on a 0.25{\deg} latitude-longitude resolution. Hindcasts of
6-hourly weather in 2018 based on ERA5 demonstrate that FengWu performs better
than GraphCast in predicting 80\% of the 880 reported predictands, e.g.,
reducing the root mean square error (RMSE) of 10-day lead global z500
prediction from 733 to 651 $m^{2}/s^2$. In addition, the inference cost of each
iteration is merely 600ms on NVIDIA Tesla A100 hardware. The results suggest
that FengWu can significantly improve the forecast skill and extend the
skillful global medium-range weather forecast out to 10.75 days lead (with ACC
of z500 > 0.6) for the first time.",None,-1
6ebbe8cd-02db-48b9-8f86-108ca52f3056,Diffusion-Augmented Depth Prediction with Sparse Annotations,0.570268,"Depth estimation aims to predict dense depth maps. In autonomous driving
scenes, sparsity of annotations makes the task challenging. Supervised models
produce concave objects due to insufficient structural information. They
overfit to valid pixels and fail to restore spatial structures. Self-supervised
methods are proposed for the problem. Their robustness is limited by pose
estimation, leading to erroneous results in natural scenes. In this paper, we
propose a supervised framework termed Diffusion-Augmented Depth Prediction
(DADP). We leverage the structural characteristics of diffusion model to
enforce depth structures of depth models in a plug-and-play manner. An
object-guided integrality loss is also proposed to further enhance regional
structure integrality by fetching objective information. We evaluate DADP on
three driving benchmarks and achieve significant improvements in depth
structures and robustness. Our work provides a new perspective on depth
estimation with sparse annotations in autonomous driving scenes.",None,-1
69f98c14-e812-4daa-b33e-fddd951aa4b5,Triplet Edge Attention for Algorithmic Reasoning,0.461107,"This work investigates neural algorithmic reasoning to develop neural
networks capable of learning from classical algorithms. The main challenge is
to develop graph neural networks that are expressive enough to predict the
given algorithm outputs while generalizing well to out-of-distribution data. In
this work, we introduce a new graph neural network layer called Triplet Edge
Attention (TEA), an edge-aware graph attention layer. Our algorithm works by
precisely computing edge latent, aggregating multiple triplet messages using
edge-based attention. We empirically validate our TEA layer in the CLRS
benchmark and demonstrate a $5%$ improvement on average. In particular, we
achieve a $30%$ improvement for the string algorithms compared to the
state-of-the-art model.",None,-1
6a437abc-4311-470f-b9d7-131d0f19ebc5,Counting Crowds in Bad Weather,0.683705,"Crowd counting has recently attracted significant attention in the field of
computer vision due to its wide applications to image understanding. Numerous
methods have been proposed and achieved state-of-the-art performance for
real-world tasks. However, existing approaches do not perform well under
adverse weather such as haze, rain, and snow since the visual appearances of
crowds in such scenes are drastically different from those images in clear
weather of typical datasets. In this paper, we propose a method for robust
crowd counting in adverse weather scenarios. Instead of using a two-stage
approach that involves image restoration and crowd counting modules, our model
learns effective features and adaptive queries to account for large appearance
variations. With these weather queries, the proposed model can learn the
weather information according to the degradation of the input image and
optimize with the crowd counting module simultaneously. Experimental results
show that the proposed algorithm is effective in counting crowds under
different weather types on benchmark datasets. The source code and trained
models will be made available to the public.",None,-1
194ec922-94c0-4820-9a8a-4ee41c068208,Search for universal minimum drag resistance underwater vehicle hull using CFD,0.711708,"In Autonomous Underwater Vehicles (AUVs) design, hull resistance is an
important factor in determining the power requirements and range of vehicle and
consequently affect battery size, weight, and volume requirement of the design.
In this paper, we leverage on AI-based optimization algorithm along with
Computational Fluid Dynamics (CFD) simulation to study the optimal hull design
that minimizing the resistance. By running the CFD-based optimization at
different operating velocities and turbulence intensity, we want to
study/search the possibility of a universal design that will provide least
resistance/near-optimal design across all operating conditions (operating
velocity) and environmental conditions (turbulence intensity). Early result
demonstrated that the optimal design found at low velocity and low turbulence
condition performs very poor at high velocity and high turbulence conditions.
However, a design that is optimal at high velocity and high turbulence
conditions performs near-optimal across many considered velocity and turbulence
conditions.",None,-1
6bf00c9f-44b8-47f1-8e08-963742061eac,Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives,0.123732,"Large language models (LLMs) have shown increasing power on various natural
language processing (NLP) tasks. However, tuning these models for downstream
tasks usually needs exorbitant costs or is unavailable due to commercial
considerations. Recently, black-box tuning has been proposed to address this
problem by optimizing task-specific prompts without accessing the gradients and
hidden representations. However, most existing works have yet fully exploited
the potential of gradient-free optimization under the scenario of few-shot
learning. In this paper, we describe BBT-RGB, a suite of straightforward and
complementary techniques for enhancing the efficiency and performance of
black-box optimization. Specifically, our method includes three plug-and-play
components: (1) Two-stage derivative-free optimization strategy that
facilitates fast convergence and mitigates overfitting; (2) Automatic
verbalizer construction with its novel usage under few-shot settings; (3)
Better prompt initialization policy based on instruction search and
auto-selected demonstration. Extensive experiments across various tasks on
natural language understanding and inference demonstrate the effectiveness of
our method. Our codes are publicly available at
https://github.com/QiushiSun/BBT-RGB.",None,-1
275a6b6e-01db-49ee-a57c-9248ae9a4ec7,Learning Language-Specific Layers for Multilingual Machine Translation,0.721236,"Multilingual Machine Translation promises to improve translation quality
between non-English languages. This is advantageous for several reasons, namely
lower latency (no need to translate twice), and reduced error cascades (e.g.,
avoiding losing gender and formality information when translating through
English). On the downside, adding more languages reduces model capacity per
language, which is usually countered by increasing the overall model size,
making training harder and inference slower. In this work, we introduce
Language-Specific Transformer Layers (LSLs), which allow us to increase model
capacity, while keeping the amount of computation and the number of parameters
used in the forward pass constant. The key idea is to have some layers of the
encoder be source or target language-specific, while keeping the remaining
layers shared. We study the best way to place these layers using a neural
architecture search inspired approach, and achieve an improvement of 1.3 chrF
(1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and
1.9 chrF (2.2 spBLEU) on a shared decoder one.",None,-1
d896c856-04cb-4e71-9e18-761d0e897c55,ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,0.999713,"Text evaluation has historically posed significant challenges, often
demanding substantial labor and time cost. With the emergence of large language
models (LLMs), researchers have explored LLMs' potential as alternatives for
human evaluation. While these single-agent-based approaches show promise,
experimental results suggest that further advancements are needed to bridge the
gap between their current effectiveness and human-level evaluation quality.
Recognizing that best practices of human evaluation processes often involve
multiple human annotators collaborating in the evaluation, we resort to a
multi-agent debate framework, moving beyond single-agent prompting strategies.
The multi-agent-based approach enables a group of LLMs to synergize with an
array of intelligent counterparts, harnessing their distinct capabilities and
expertise to enhance efficiency and effectiveness in handling intricate tasks.
In this paper, we construct a multi-agent referee team called ChatEval to
autonomously discuss and evaluate the quality of generated responses from
different models on open-ended questions and traditional natural language
generation (NLG) tasks. Our analysis shows that ChatEval transcends mere
textual scoring, offering a human-mimicking evaluation process for reliable
assessments. Our code is available at https://github.com/chanchimin/ChatEval.",None,-1
1225cbae-38da-41d5-a4a7-10c4b2915f59,UAlberta at SemEval-2023 Task 1: Context Augmentation and Translation for Multilingual Visual Word Sense Disambiguation,0.550671,"We describe the systems of the University of Alberta team for the
SemEval-2023 Visual Word Sense Disambiguation (V-WSD) Task. We present a novel
algorithm that leverages glosses retrieved from BabelNet, in combination with
text and image encoders. Furthermore, we compare language-specific encoders
against the application of English encoders to translated texts. As the
contexts given in the task datasets are extremely short, we also experiment
with augmenting these contexts with descriptions generated by a language model.
This yields substantial improvements in accuracy. We describe and evaluate
additional V-WSD methods which use image generation and text-conditioned image
segmentation. Overall, the results of our official submission rank us 18 out of
56 teams. Some of our unofficial results are even better than the official
ones. Our code is publicly available at https://github.com/UAlberta-NLP/v-wsd.",None,-1
629a5548-306a-4a0f-904d-99e660a3ef54,Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery,0.999635,"Despite growing interest in using large language models (LLMs) in healthcare,
current explorations do not assess the real-world utility and safety of LLMs in
clinical settings. Our objective was to determine whether two LLMs can serve
information needs submitted by physicians as questions to an informatics
consultation service in a safe and concordant manner. Sixty six questions from
an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple
prompts. 12 physicians assessed the LLM responses' possibility of patient harm
and concordance with existing reports from an informatics consultation service.
Physician assessments were summarized based on majority vote. For no questions
did a majority of physicians deem either LLM response as harmful. For GPT-3.5,
responses to 8 questions were concordant with the informatics consult report,
20 discordant, and 9 were unable to be assessed. There were 29 responses with
no majority on ""Agree"", ""Disagree"", and ""Unable to assess"". For GPT-4,
responses to 13 questions were concordant, 15 discordant, and 3 were unable to
be assessed. There were 35 responses with no majority. Responses from both LLMs
were largely devoid of overt harm, but less than 20% of the responses agreed
with an answer from an informatics consultation service, responses contained
hallucinated references, and physicians were divided on what constitutes harm.
These results suggest that while general purpose LLMs are able to provide safe
and credible responses, they often do not meet the specific information need of
a given question. A definitive evaluation of the usefulness of LLMs in
healthcare settings will likely require additional research on prompt
engineering, calibration, and custom-tailoring of general purpose models.",None,-1
55e07938-6a44-407c-a2b3-f4f467bee768,Revisit and Outstrip Entity Alignment: A Perspective of Generative Models,0.166545,"Recent embedding-based methods have achieved great successes in exploiting
entity alignment from knowledge graph (KG) embeddings of multiple modalities.
In this paper, we study embedding-based entity alignment (EEA) from a
perspective of generative models. We show that EEA shares similarities with
typical generative models and prove the effectiveness of the recently developed
generative adversarial network (GAN)-based EEA methods theoretically. We then
reveal that their incomplete objective limits the capacity on both entity
alignment and entity synthesis (i.e., generating new entities). We mitigate
this problem by introducing a generative EEA (GEEA) framework with the proposed
mutual variational autoencoder (M-VAE) as the generative model. M-VAE enables
entity conversion between KGs and generation of new entities from random noise
vectors. We demonstrate the power of GEEA with theoretical analysis and
empirical experiments on both entity alignment and entity synthesis tasks.",None,-1
d7a5bfa2-ff23-4033-a6af-b49df8b32032,BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations,0.453804,"Real-world planning problems, including autonomous driving and sustainable
energy applications like carbon storage and resource exploration, have recently
been modeled as partially observable Markov decision processes (POMDPs) and
solved using approximate methods. To solve high-dimensional POMDPs in practice,
state-of-the-art methods use online planning with problem-specific heuristics
to reduce planning horizons and make the problems tractable. Algorithms that
learn approximations to replace heuristics have recently found success in
large-scale fully observable domains. The key insight is the combination of
online Monte Carlo tree search with offline neural network approximations of
the optimal policy and value function. In this work, we bring this insight to
partially observed domains and propose BetaZero, a belief-state planning
algorithm for high-dimensional POMDPs. BetaZero learns offline approximations
that replace heuristics to enable online decision making in long-horizon
problems. We address several challenges inherent in large-scale partially
observable domains; namely challenges of transitioning in stochastic
environments, prioritizing action branching with a limited search budget, and
representing beliefs as input to the network. To formalize the use of all
limited search information we train against a novel Q-weighted policy vector
target. We test BetaZero on various well-established benchmark POMDPs found in
the literature and a real-world, high-dimensional problem of critical mineral
exploration. Experiments show that BetaZero outperforms state-of-the-art POMDP
solvers on a variety of tasks.",None,-1
b38b6b76-0520-4c5f-bfb6-ac3b05cd16d3,CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models,0.0551711,"Holistically measuring societal biases of large language models is crucial
for detecting and reducing ethical risks in highly capable AI models. In this
work, we present a Chinese Bias Benchmark dataset that consists of over 100K
questions jointly constructed by human experts and generative language models,
covering stereotypes and societal biases in 14 social dimensions related to
Chinese culture and values. The curation process contains 4 essential steps:
bias identification via extensive literature review, ambiguous context
generation, AI-assisted disambiguous context generation, snd manual review \&
recomposition. The testing instances in the dataset are automatically derived
from 3K+ high-quality templates manually authored with stringent quality
control. The dataset exhibits wide coverage and high diversity. Extensive
experiments demonstrate the effectiveness of the dataset in detecting model
bias, with all 10 publicly available Chinese large language models exhibiting
strong bias in certain categories. Additionally, we observe from our
experiments that fine-tuned models could, to a certain extent, heed
instructions and avoid generating outputs that are morally harmful in some
types, in the way of ""moral self-correction"". Our dataset and results are
publicly available at
\href{https://github.com/YFHuangxxxx/CBBQ}{https://github.com/YFHuangxxxx/CBBQ},
offering debiasing research opportunities to a widened community.",None,-1
16af97c9-edef-49e3-88ff-3535f85c4313,Choose your Data Wisely: A Framework for Semantic Counterfactuals,0.313124,"Counterfactual explanations have been argued to be one of the most intuitive
forms of explanation. They are typically defined as a minimal set of edits on a
given data sample that, when applied, changes the output of a model on that
sample. However, a minimal set of edits is not always clear and understandable
to an end-user, as it could, for instance, constitute an adversarial example
(which is indistinguishable from the original data sample to an end-user).
Instead, there are recent ideas that the notion of minimality in the context of
counterfactuals should refer to the semantics of the data sample, and not to
the feature space. In this work, we build on these ideas, and propose a
framework that provides counterfactual explanations in terms of knowledge
graphs. We provide an algorithm for computing such explanations (given some
assumptions about the underlying knowledge), and quantitatively evaluate the
framework with a user study.",None,-1
ceb13464-2199-46c2-b6f9-dc8e5774728e,A Trip Towards Fairness: Bias and De-Biasing in Large Language Models,0.117416,"Cheap-to-Build Very Large-Language Models (CtB-LLMs) with affordable training
are emerging as the next big revolution in natural language processing and
understanding. These CtB-LLMs are democratizing access to trainable Very
Large-Language Models (VLLMs) and, thus, may represent the building blocks of
many NLP systems solving downstream tasks. Hence, a little or a large bias in
CtB-LLMs may cause huge harm. In this paper, we performed a large investigation
of the bias of three families of CtB-LLMs, and we showed that debiasing
techniques are effective and usable. Indeed, according to current tests, the
LLaMA and the OPT families have an important bias in gender, race, religion,
and profession. In contrast to the analysis for other LLMs, we discovered that
bias depends not on the number of parameters but on the perplexity. Finally,
the debiasing of OPT using LoRA reduces bias up to 4.12 points in the
normalized stereotype score.",None,-1
2ae37175-b6c1-4718-93f4-9e99b83699db,Analyzing the Performance of GPT-3.5 and GPT-4 in Grammatical Error Correction,0.488128,"GPT-3 and GPT-4 models are powerful, achieving high performance on a variety
of Natural Language Processing tasks. However, there is a relative lack of
detailed published analysis of their performance on the task of grammatical
error correction (GEC). To address this, we perform experiments testing the
capabilities of a GPT-3.5 model (text-davinci-003) and a GPT-4 model
(gpt-4-0314) on major GEC benchmarks. We compare the performance of different
prompts in both zero-shot and few-shot settings, analyzing intriguing or
problematic outputs encountered with different prompt formats. We report the
performance of our best prompt on the BEA-2019 and JFLEG datasets, finding that
the GPT models can perform well in a sentence-level revision setting, with
GPT-4 achieving a new high score on the JFLEG benchmark. Through human
evaluation experiments, we compare the GPT models' corrections to source, human
reference, and baseline GEC system sentences and observe differences in editing
strategies and how they are scored by human raters.",None,-1
aafb3da2-fcb2-427a-9c2d-2f4079ef4d21,"Mini-Giants: ""Small"" Language Models and Open Source Win-Win",0.0381123,"ChatGPT is phenomenal. However, it is prohibitively expensive to train and
refine such giant models. Fortunately, small language models are flourishing
and becoming more and more competent. We call them ""mini-giants"". We argue that
open source community like Kaggle and mini-giants will win-win in many ways,
technically, ethically and socially. In this article, we present a brief yet
rich background, discuss how to attain small language models, present a
comparative study of small language models and a brief discussion of evaluation
methods, discuss the application scenarios where small language models are most
needed in the real world, and conclude with discussion and outlook.",None,-1
8b22e3c0-3084-48ca-91e9-59fcb015682a,Asking More Informative Questions for Grounded Retrieval,0.210012,"When a model is trying to gather information in an interactive setting, it
benefits from asking informative questions. However, in the case of a grounded
multi-turn image identification task, previous studies have been constrained to
polar yes/no questions, limiting how much information the model can gain in a
single turn. We present an approach that formulates more informative,
open-ended questions. In doing so, we discover that off-the-shelf visual
question answering (VQA) models often make presupposition errors, which
standard information gain question selection methods fail to account for. To
address this issue, we propose a method that can incorporate presupposition
handling into both question selection and belief updates. Specifically, we use
a two-stage process, where the model first filters out images which are
irrelevant to a given question, then updates its beliefs about which image the
user intends. Through self-play and human evaluations, we show that our method
is successful in asking informative open-ended questions, increasing accuracy
over the past state-of-the-art by 14%, while resulting in 48% more efficient
games in human evaluations.",None,-1
c65b6ec9-7cf1-4fed-94f0-152582fb2b8d,3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining,0.871314,"Masked autoencoders (MAE) have recently been introduced to 3D self-supervised
pretraining for point clouds due to their great success in NLP and computer
vision. Unlike MAEs used in the image domain, where the pretext task is to
restore features at the masked pixels, such as colors, the existing 3D MAE
works reconstruct the missing geometry only, i.e, the location of the masked
points. In contrast to previous studies, we advocate that point location
recovery is inessential and restoring intrinsic point features is much
superior. To this end, we propose to ignore point position reconstruction and
recover high-order features at masked points including surface normals and
surface variations, through a novel attention-based decoder which is
independent of the encoder design. We validate the effectiveness of our pretext
task and decoder design using different encoder structures for 3D training and
demonstrate the advantages of our pretrained networks on various point cloud
analysis tasks.",None,-1
a6e1b05e-6ee3-4170-aba8-a749b174ab67,Robust Object Modeling for Visual Tracking,0.980399,"Object modeling has become a core part of recent tracking frameworks. Current
popular tackers use Transformer attention to extract the template feature
separately or interactively with the search region. However, separate template
learning lacks communication between the template and search regions, which
brings difficulty in extracting discriminative target-oriented features. On the
other hand, interactive template learning produces hybrid template features,
which may introduce potential distractors to the template via the cluttered
search regions. To enjoy the merits of both methods, we propose a robust object
modeling framework for visual tracking (ROMTrack), which simultaneously models
the inherent template and the hybrid template features. As a result, harmful
distractors can be suppressed by combining the inherent features of target
objects with search regions' guidance. Target-related features can also be
extracted using the hybrid template, thus resulting in a more robust object
modeling framework. To further enhance robustness, we present novel variation
tokens to depict the ever-changing appearance of target objects. Variation
tokens are adaptable to object deformation and appearance variations, which can
boost overall performance with negligible computation. Experiments show that
our ROMTrack sets a new state-of-the-art on multiple benchmarks.",None,-1
cddd3064-494d-47bc-8a57-620e4ce5ab61,Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training,0.12137,"Automatic text simplification systems help to reduce textual information
barriers on the internet. However, for languages other than English, only few
parallel data to train these systems exists. We propose a two-step approach to
overcome this data scarcity issue. First, we fine-tuned language models on a
corpus of German Easy Language, a specific style of German. Then, we used these
models as decoders in a sequence-to-sequence simplification task. We show that
the language models adapt to the style characteristics of Easy Language and
output more accessible texts. Moreover, with the style-specific pre-training,
we reduced the number of trainable parameters in text simplification models.
Hence, less parallel data is sufficient for training. Our results indicate that
pre-training on unaligned data can reduce the required parallel data while
improving the performance on downstream tasks.",None,-1
83984d46-232b-4856-81d6-5387c9079106,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,0.559234,"Diffusion-based generative models have recently emerged as powerful solutions
for high-quality synthesis in multiple domains. Leveraging the bidirectional
Markov chains, diffusion probabilistic models generate samples by inferring the
reversed Markov chain based on the learned distribution mapping at the forward
diffusion process. In this work, we propose Modiff, a conditional paradigm that
benefits from the denoising diffusion probabilistic model (DDPM) to tackle the
problem of realistic and diverse action-conditioned 3D skeleton-based motion
generation. We are a pioneering attempt that uses DDPM to synthesize a variable
number of motion sequences conditioned on a categorical action. We evaluate our
approach on the large-scale NTU RGB+D dataset and show improvements over
state-of-the-art motion generation methods.",None,-1
959819d3-9732-4b68-b010-aa6d31ee97f9,A Human-Centered Safe Robot Reinforcement Learning Framework with Interactive Behaviors,0.576342,"Deployment of Reinforcement Learning (RL) algorithms for robotics
applications in the real world requires ensuring the safety of the robot and
its environment. Safe Robot RL (SRRL) is a crucial step towards achieving
human-robot coexistence. In this paper, we envision a human-centered SRRL
framework consisting of three stages: safe exploration, safety value alignment,
and safe collaboration. We examine the research gaps in these areas and propose
to leverage interactive behaviors for SRRL. Interactive behaviors enable
bi-directional information transfer between humans and robots, such as
conversational robot ChatGPT. We argue that interactive behaviors need further
attention from the SRRL community. We discuss four open challenges related to
the robustness, efficiency, transparency, and adaptability of SRRL with
interactive behaviors.",None,-1
cdafa630-e9c1-4d2e-adb9-961192aa1099,Giraffe: Adventures in Expanding Context Lengths in LLMs,0.785381,"Modern large language models (LLMs) that rely on attention mechanisms are
typically trained with fixed context lengths which enforce upper limits on the
length of input sequences that they can handle at evaluation time. To use these
models on sequences longer than the train-time context length, one might employ
techniques from the growing family of context length extrapolation methods --
most of which focus on modifying the system of positional encodings used in the
attention mechanism to indicate where tokens or activations are located in the
input sequence. We conduct a wide survey of existing methods of context length
extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own
design as well -- in particular, a new truncation strategy for modifying the
basis for the position encoding.
  We test these methods using three new evaluation tasks (FreeFormQA,
AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to
be less fine-grained as a measure of long context performance of LLMs. We
release the three tasks publicly as datasets on HuggingFace. We discover that
linear scaling is the best method for extending context length, and show that
further gains can be achieved by using longer scales at evaluation time. We
also discover promising extrapolation capabilities in the truncated basis. To
support further research in this area, we release three new 13B parameter
long-context models which we call Giraffe: 4k and 16k context models trained
from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We
also release the code to replicate our results.",None,-1
85abd267-1361-45f7-9bc8-59be6bd89da2,Decomposition Ascribed Synergistic Learning for Unified Image Restoration,0.147922,"Learning to restore multiple image degradations within a single model is
quite beneficial for real-world applications. Nevertheless, existing works
typically concentrate on regarding each degradation independently, while their
relationship has been less exploited to ensure the synergistic learning. To
this end, we revisit the diverse degradations through the lens of singular
value decomposition, with the observation that the decomposed singular vectors
and singular values naturally undertake the different types of degradation
information, dividing various restoration tasks into two groups, \ie, singular
vector dominated and singular value dominated. The above analysis renders a
more unified perspective to ascribe the diverse degradations, compared to
previous task-level independent learning. The dedicated optimization of
degraded singular vectors and singular values inherently utilizes the potential
relationship among diverse restoration tasks, attributing to the Decomposition
Ascribed Synergistic Learning (DASL). Specifically, DASL comprises two
effective operators, namely, Singular VEctor Operator (SVEO) and Singular VAlue
Operator (SVAO), to favor the decomposed optimization, which can be lightly
integrated into existing image restoration backbone. Moreover, the congruous
decomposition loss has been devised for auxiliary. Extensive experiments on
blended five image restoration tasks demonstrate the effectiveness of our
method.",None,-1
2d0271ee-a680-42fa-a983-3d4a2ee820b6,AMIR: Automated MisInformation Rebuttal -- A COVID-19 Vaccination Datasets based Recommendation System,0.336915,"Misinformation has emerged as a major societal threat in recent years in
general; specifically in the context of the COVID-19 pandemic, it has wrecked
havoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable
solutions for combating misinformation are the need of the hour. This work
explored how existing information obtained from social media and augmented with
more curated fact checked data repositories can be harnessed to facilitate
automated rebuttal of misinformation at scale. While the ideas herein can be
generalized and reapplied in the broader context of misinformation mitigation
using a multitude of information sources and catering to the spectrum of social
media platforms, this work serves as a proof of concept, and as such, it is
confined in its scope to only rebuttal of tweets, and in the specific context
of misinformation regarding COVID-19. It leverages two publicly available
datasets, viz. FaCov (fact-checked articles) and misleading (social media
Twitter) data on COVID-19 Vaccination.",None,-1
d278869c-f130-4ecd-b54e-a0385fda8d8d,RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation,0.819555,"Grammatical Error Correction (GEC) systems play a vital role in assisting
people with their daily writing tasks. However, users may sometimes come across
a GEC system that initially performs well but fails to correct errors when the
inputs are slightly modified. To ensure an ideal user experience, a reliable
GEC system should have the ability to provide consistent and accurate
suggestions when encountering irrelevant context perturbations, which we refer
to as context robustness. In this paper, we introduce RobustGEC, a benchmark
designed to evaluate the context robustness of GEC systems. RobustGEC comprises
5,000 GEC cases, each with one original error-correct sentence pair and five
variants carefully devised by human annotators. Utilizing RobustGEC, we reveal
that state-of-the-art GEC systems still lack sufficient robustness against
context perturbations. In addition, we propose a simple yet effective method
for remitting this issue.",None,-1
8e145878-2889-4c3a-8058-73d178fba09d,Deep Learning Provides Rapid Screen for Breast Cancer Metastasis with Sentinel Lymph Nodes,0.119153,"Deep learning has been shown to be useful to detect breast cancer metastases
by analyzing whole slide images of sentinel lymph nodes. However, it requires
extensive scanning and analysis of all the lymph nodes slides for each case.
Our deep learning study focuses on breast cancer screening with only a small
set of image patches from any sentinel lymph node, positive or negative for
metastasis, to detect changes in tumor environment and not in the tumor itself.
We design a convolutional neural network in the Python language to build a
diagnostic model for this purpose. The excellent results from this preliminary
study provided a proof of concept for incorporating automated metastatic screen
into the digital pathology workflow to augment the pathologists' productivity.
Our approach is unique since it provides a very rapid screen rather than an
exhaustive search for tumor in all fields of all sentinel lymph nodes.",None,-1
24cdbee8-464b-482a-b506-86c431056849,Semi-supervised learning made simple with self-supervised clustering,0.812127,"Self-supervised learning models have been shown to learn rich visual
representations without requiring human annotations. However, in many
real-world scenarios, labels are partially available, motivating a recent line
of work on semi-supervised methods inspired by self-supervised principles. In
this paper, we propose a conceptually simple yet empirically powerful approach
to turn clustering-based self-supervised methods such as SwAV or DINO into
semi-supervised learners. More precisely, we introduce a multi-task framework
merging a supervised objective using ground-truth labels and a self-supervised
objective relying on clustering assignments with a single cross-entropy loss.
This approach may be interpreted as imposing the cluster centroids to be class
prototypes. Despite its simplicity, we provide empirical evidence that our
approach is highly effective and achieves state-of-the-art performance on
CIFAR100 and ImageNet.",None,-1
0ebf67c5-848b-4030-bc7b-a284f8a8ae3d,Weakly Supervised Human Skin Segmentation using Guidance Attention Mechanisms,0.113731,"Human skin segmentation is a crucial task in computer vision and biometric
systems, yet it poses several challenges such as variability in skin color,
pose, and illumination. This paper presents a robust data-driven skin
segmentation method for a single image that addresses these challenges through
the integration of contextual information and efficient network design. In
addition to robustness and accuracy, the integration into real-time systems
requires a careful balance between computational power, speed, and performance.
The proposed method incorporates two attention modules, Body Attention and Skin
Attention, that utilize contextual information to improve segmentation results.
These modules draw attention to the desired areas, focusing on the body
boundaries and skin pixels, respectively. Additionally, an efficient network
architecture is employed in the encoder part to minimize computational power
while retaining high performance. To handle the issue of noisy labels in skin
datasets, the proposed method uses a weakly supervised training strategy,
relying on the Skin Attention module. The results of this study demonstrate
that the proposed method is comparable to, or outperforms, state-of-the-art
methods on benchmark datasets.",None,-1
64411a18-1590-4de8-a778-ef9593cd563b,Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces,0.441223,"Self-supervised speech representations are known to encode both speaker and
phonetic information, but how they are distributed in the high-dimensional
space remains largely unexplored. We hypothesize that they are encoded in
orthogonal subspaces, a property that lends itself to simple disentanglement.
Applying principal component analysis to representations of two predictive
coding models, we identify two subspaces that capture speaker and phonetic
variances, and confirm that they are nearly orthogonal. Based on this property,
we propose a new speaker normalization method which collapses the subspace that
encodes speaker information, without requiring transcriptions. Probing
experiments show that our method effectively eliminates speaker information and
outperforms a previous baseline in phone discrimination tasks. Moreover, the
approach generalizes and can be used to remove information of unseen speakers.",None,-1
fda009f2-bd1f-4fc3-9224-7ea5abf813ec,Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages,0.527058,"This work introduces Zambezi Voice, an open-source multilingual speech
resource for Zambian languages. It contains two collections of datasets:
unlabelled audio recordings of radio news and talk shows programs (160 hours)
and labelled data (over 80 hours) consisting of read speech recorded from text
sourced from publicly available literature books. The dataset is created for
speech recognition but can be extended to multilingual speech processing
research for both supervised and unsupervised learning approaches. To our
knowledge, this is the first multilingual speech dataset created for Zambian
languages. We exploit pretraining and cross-lingual transfer learning by
finetuning the Wav2Vec2.0 large-scale multilingual pre-trained model to build
end-to-end (E2E) speech recognition models for our baseline models. The dataset
is released publicly under a Creative Commons BY-NC-ND 4.0 license and can be
accessed via https://github.com/unza-speech-lab/zambezi-voice .",None,-1
12181ffa-ac0b-463f-937d-e879fc3a0728,Zero-shot Causal Graph Extrapolation from Text via LLMs,0.808854,"We evaluate the ability of large language models (LLMs) to infer causal
relations from natural language. Compared to traditional natural language
processing and deep learning techniques, LLMs show competitive performance in a
benchmark of pairwise relations without needing (explicit) training samples.
This motivates us to extend our approach to extrapolating causal graphs through
iterated pairwise queries. We perform a preliminary analysis on a benchmark of
biomedical abstracts with ground-truth causal graphs validated by experts. The
results are promising and support the adoption of LLMs for such a crucial step
in causal inference, especially in medical domains, where the amount of
scientific text to analyse might be huge, and the causal statements are often
implicit.",None,-1
c7732eec-f655-49ce-b999-ebd288f0df3a,IASCAR: Incremental Answer Set Counting by Anytime Refinement,0.102532,"Answer set programming (ASP) is a popular declarative programming paradigm
with various applications. Programs can easily have many answer sets that
cannot be enumerated in practice, but counting still allows quantifying
solution spaces. If one counts under assumptions on literals, one obtains a
tool to comprehend parts of the solution space, so-called answer set
navigation. However, navigating through parts of the solution space requires
counting many times, which is expensive in theory. Knowledge compilation
compiles instances into representations on which counting works in polynomial
time. However, these techniques exist only for CNF formulas, and compiling ASP
programs into CNF formulas can introduce an exponential overhead. This paper
introduces a technique to iteratively count answer sets under assumptions on
knowledge compilations of CNFs that encode supported models. Our anytime
technique uses the inclusion-exclusion principle to improve bounds by over- and
undercounting systematically. In a preliminary empirical analysis, we
demonstrate promising results. After compiling the input (offline phase), our
approach quickly (re)counts.",None,-1
02e84961-3f15-4aee-af29-f1c539cf2534,Explaining CLIP through Co-Creative Drawings and Interaction,0.436432,"This paper analyses a visual archive of drawings produced by an interactive
robotic art installation where audience members narrated their dreams into a
system powered by CLIPdraw deep learning (DL) model that interpreted and
transformed their dreams into images. The resulting archive of prompt-image
pairs were examined and clustered based on concept representation accuracy. As
a result of the analysis, the paper proposes four groupings for describing and
explaining CLIP-generated results: clear concept, text-to-text as image,
indeterminacy and confusion, and lost in translation. This article offers a
glimpse into a collection of dreams interpreted, mediated and given form by
Artificial Intelligence (AI), showcasing oftentimes unexpected, visually
compelling or, indeed, the dream-like output of the system, with the emphasis
on processes and results of translations between languages, sign-systems and
various modules of the installation. In the end, the paper argues that proposed
clusters support better understanding of the neural model.",None,-1
0415ef53-a091-45d2-95f6-fe34f4f6ed27,Mimetic Initialization of Self-Attention Layers,0.61913,"It is notoriously difficult to train Transformers on small datasets;
typically, large pre-trained models are instead used as the starting point. We
explore the weights of such pre-trained Transformers (particularly for vision)
to attempt to find reasons for this discrepancy. Surprisingly, we find that
simply initializing the weights of self-attention layers so that they ""look""
more like their pre-trained counterparts allows us to train vanilla
Transformers faster and to higher final accuracies, particularly on vision
tasks such as CIFAR-10 and ImageNet classification, where we see gains in
accuracy of over 5% and 4%, respectively. Our initialization scheme is closed
form, learning-free, and very simple: we set the product of the query and key
weights to be approximately the identity, and the product of the value and
projection weights to approximately the negative identity. As this mimics the
patterns we saw in pre-trained Transformers, we call the technique ""mimetic
initialization"".",None,-1
f23f29e5-2361-4e20-a4e7-2ba83d9ab670,ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games,0.28591,"In this work, we investigate the capacity of language models to generate
explicit, interpretable, and interactive world models of scientific and
common-sense reasoning tasks. We operationalize this as a task of generating
text games, expressed as hundreds of lines of Python code. To facilitate this
task, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a
corpus of 32 reasoning-focused text games totaling 20k lines of Python code. We
empirically demonstrate that GPT-4 can use these games as templates for
single-shot in-context learning, successfully producing runnable games on
unseen topics in 28% of cases. When allowed to self-reflect on program errors,
game runnability substantially increases to 57%. While evaluating simulation
fidelity is labor-intensive, we introduce a suite of automated metrics to
assess game fidelity, technical validity, adherence to task specifications, and
winnability, showing a high degree of agreement with expert human ratings. We
pose this as a challenge task to spur further development at the juncture of
world modeling and code generation.",None,-1
b961142e-0c17-4f32-8995-eb596e2700e4,Building Rearticulable Models for Arbitrary 3D Objects from 4D Point Clouds,0.215863,"We build rearticulable models for arbitrary everyday man-made objects
containing an arbitrary number of parts that are connected together in
arbitrary ways via 1 degree-of-freedom joints. Given point cloud videos of such
everyday objects, our method identifies the distinct object parts, what parts
are connected to what other parts, and the properties of the joints connecting
each part pair. We do this by jointly optimizing the part segmentation,
transformation, and kinematics using a novel energy minimization framework. Our
inferred animatable models, enables retargeting to novel poses with sparse
point correspondences guidance. We test our method on a new articulating robot
dataset, and the Sapiens dataset with common daily objects, as well as
real-world scans. Experiments show that our method outperforms two leading
prior works on various metrics.",None,-1
35c5ece9-7ce0-44d9-9cf4-68535ee2c790,Learning Empirical Bregman Divergence for Uncertain Distance Representation,0.0917093,"Deep metric learning techniques have been used for visual representation in
various supervised and unsupervised learning tasks through learning embeddings
of samples with deep networks. However, classic approaches, which employ a
fixed distance metric as a similarity function between two embeddings, may lead
to suboptimal performance for capturing the complex data distribution. The
Bregman divergence generalizes measures of various distance metrics and arises
throughout many fields of deep metric learning. In this paper, we first show
how deep metric learning loss can arise from the Bregman divergence. We then
introduce a novel method for learning empirical Bregman divergence directly
from data based on parameterizing the convex function underlying the Bregman
divergence with a deep learning setting. We further experimentally show that
our approach performs effectively on five popular public datasets compared to
other SOTA deep metric learning methods, particularly for pattern recognition
problems.",None,-1
56e84cdf-8600-4500-8d94-2c40cf499518,MIDI-Draw: Sketching to Control Melody Generation,0.0132454,"We describe a proof-of-principle implementation of a system for drawing
melodies that abstracts away from a note-level input representation via melodic
contours. The aim is to allow users to express their musical intentions without
requiring prior knowledge of how notes fit together melodiously. Current
approaches to controllable melody generation often require users to choose
parameters that are static across a whole sequence, via buttons or sliders. In
contrast, our method allows users to quickly specify how parameters should
change over time by drawing a contour.",None,-1
4bf9dd1e-e9f0-428c-80cb-a2018070114d,CEMFormer: Learning to Predict Driver Intentions from In-Cabin and External Cameras via Spatial-Temporal Transformers,0.943269,"Driver intention prediction seeks to anticipate drivers' actions by analyzing
their behaviors with respect to surrounding traffic environments. Existing
approaches primarily focus on late-fusion techniques, and neglect the
importance of maintaining consistency between predictions and prevailing
driving contexts. In this paper, we introduce a new framework called Cross-View
Episodic Memory Transformer (CEMFormer), which employs spatio-temporal
transformers to learn unified memory representations for an improved driver
intention prediction. Specifically, we develop a spatial-temporal encoder to
integrate information from both in-cabin and external camera views, along with
episodic memory representations to continuously fuse historical data.
Furthermore, we propose a novel context-consistency loss that incorporates
driving context as an auxiliary supervision signal to improve prediction
performance. Comprehensive experiments on the Brain4Cars dataset demonstrate
that CEMFormer consistently outperforms existing state-of-the-art methods in
driver intention prediction.",None,-1
8202bfcd-c5f4-434c-9f9f-76f6d094e4ec,Masked Contrastive Graph Representation Learning for Age Estimation,0.883153,"Age estimation of face images is a crucial task with various practical
applications in areas such as video surveillance and Internet access control.
While deep learning-based age estimation frameworks, e.g., convolutional neural
network (CNN), multi-layer perceptrons (MLP), and transformers have shown
remarkable performance, they have limitations when modelling complex or
irregular objects in an image that contains a large amount of redundant
information. To address this issue, this paper utilizes the robustness property
of graph representation learning in dealing with image redundancy information
and proposes a novel Masked Contrastive Graph Representation Learning (MCGRL)
method for age estimation. Specifically, our approach first leverages CNN to
extract semantic features of the image, which are then partitioned into patches
that serve as nodes in the graph. Then, we use a masked graph convolutional
network (GCN) to derive image-based node representations that capture rich
structural information. Finally, we incorporate multiple losses to explore the
complementary relationship between structural information and semantic
features, which improves the feature representation capability of GCN.
Experimental results on real-world face image datasets demonstrate the
superiority of our proposed method over other state-of-the-art age estimation
approaches.",None,-1
e6f9500d-a80e-41a5-a942-75bf8c2d2aec,Memory Maps for Video Object Detection and Tracking on UAVs,0.374675,"This paper introduces a novel approach to video object detection detection
and tracking on Unmanned Aerial Vehicles (UAVs). By incorporating metadata, the
proposed approach creates a memory map of object locations in actual world
coordinates, providing a more robust and interpretable representation of object
locations in both, image space and the real world. We use this representation
to boost confidences, resulting in improved performance for several temporal
computer vision tasks, such as video object detection, short and long-term
single and multi-object tracking, and video anomaly detection. These findings
confirm the benefits of metadata in enhancing the capabilities of UAVs in the
field of temporal computer vision and pave the way for further advancements in
this area.",None,-1
8c9fd616-8d98-4173-831a-892f9e001bcb,Continuous Versatile Jumping Using Learned Action Residuals,0.411995,"Jumping is essential for legged robots to traverse through difficult
terrains. In this work, we propose a hierarchical framework that combines
optimal control and reinforcement learning to learn continuous jumping motions
for quadrupedal robots. The core of our framework is a stance controller, which
combines a manually designed acceleration controller with a learned residual
policy. As the acceleration controller warm starts policy for efficient
training, the trained policy overcomes the limitation of the acceleration
controller and improves the jumping stability. In addition, a low-level
whole-body controller converts the body pose command from the stance controller
to motor commands. After training in simulation, our framework can be deployed
directly to the real robot, and perform versatile, continuous jumping motions,
including omni-directional jumps at up to 50cm high, 60cm forward, and
jump-turning at up to 90 degrees. Please visit our website for more results:
https://sites.google.com/view/learning-to-jump.",None,-1
4bdae3bc-b59b-413f-a530-3e4c3f9aa515,AutoDroid: LLM-powered Task Automation in Android,0.87099,"Mobile task automation is an attractive technique that aims to enable
voice-based hands-free user interaction with smartphones. However, existing
approaches suffer from poor scalability due to the limited language
understanding ability and the non-trivial manual efforts required from
developers or end-users. The recent advance of large language models (LLMs) in
language understanding and reasoning inspires us to rethink the problem from a
model-centric perspective, where task preparation, comprehension, and execution
are handled by a unified language model. In this work, we introduce AutoDroid,
a mobile task automation system capable of handling arbitrary tasks on any
Android application without manual efforts. The key insight is to combine the
commonsense knowledge of LLMs and domain-specific knowledge of apps through
automated dynamic analysis. The main components include a functionality-aware
UI representation method that bridges the UI with the LLM, exploration-based
memory injection techniques that augment the app-specific domain knowledge of
LLM, and a multi-granularity query optimization module that reduces the cost of
model inference. We integrate AutoDroid with off-the-shelf LLMs including
online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a
new benchmark for memory-augmented Android task automation with 158 common
tasks. The results demonstrated that AutoDroid is able to precisely generate
actions with an accuracy of 90.9%, and complete tasks with a success rate of
71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo,
benchmark suites, and source code of AutoDroid will be released at
url{https://autodroid-sys.github.io/}.",None,-1
6a15ceb8-0aaf-499f-869e-c9706fcd306e,Learning Disentangled Representation with Mutual Information Maximization for Real-Time UAV Tracking,0.308766,"Efficiency has been a critical problem in UAV tracking due to limitations in
computation resources, battery capacity, and unmanned aerial vehicle maximum
load. Although discriminative correlation filters (DCF)-based trackers prevail
in this field for their favorable efficiency, some recently proposed
lightweight deep learning (DL)-based trackers using model compression
demonstrated quite remarkable CPU efficiency as well as precision.
Unfortunately, the model compression methods utilized by these works, though
simple, are still unable to achieve satisfying tracking precision with higher
compression rates. This paper aims to exploit disentangled representation
learning with mutual information maximization (DR-MIM) to further improve
DL-based trackers' precision and efficiency for UAV tracking. The proposed
disentangled representation separates the feature into an identity-related and
an identity-unrelated features. Only the latter is used, which enhances the
effectiveness of the feature representation for subsequent classification and
regression tasks. Extensive experiments on four UAV benchmarks, including
UAV123@10fps, DTB70, UAVDT and VisDrone2018, show that our DR-MIM tracker
significantly outperforms state-of-the-art UAV tracking methods.",None,-1
12de8e85-2fe4-44eb-8fd1-86ca4db7a75d,TomatoDIFF: On-plant Tomato Segmentation with Denoising Diffusion Models,0.078704,"Artificial intelligence applications enable farmers to optimize crop growth
and production while reducing costs and environmental impact. Computer
vision-based algorithms in particular, are commonly used for fruit
segmentation, enabling in-depth analysis of the harvest quality and accurate
yield estimation. In this paper, we propose TomatoDIFF, a novel diffusion-based
model for semantic segmentation of on-plant tomatoes. When evaluated against
other competitive methods, our model demonstrates state-of-the-art (SOTA)
performance, even in challenging environments with highly occluded fruits.
Additionally, we introduce Tomatopia, a new, large and challenging dataset of
greenhouse tomatoes. The dataset comprises high-resolution RGB-D images and
pixel-level annotations of the fruits.",None,-1
b4cbd6b7-fc2e-47b2-8e20-d6298211c2ba,AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for Robotic Rehabilitation,0.291981,"In this paper, we present AR3n (pronounced as Aaron), an assist-as-needed
(AAN) controller that utilizes reinforcement learning to supply adaptive
assistance during a robot assisted handwriting rehabilitation task. Unlike
previous AAN controllers, our method does not rely on patient specific
controller parameters or physical models. We propose the use of a virtual
patient model to generalize AR3n across multiple subjects. The system modulates
robotic assistance in realtime based on a subject's tracking error, while
minimizing the amount of robotic assistance. The controller is experimentally
validated through a set of simulations and human subject experiments. Finally,
a comparative study with a traditional rule-based controller is conducted to
analyze differences in assistance mechanisms of the two controllers.",None,-1
fd394f65-8f82-421c-9dd8-a12bfc742bb7,Notion of Explainable Artificial Intelligence -- An Empirical Investigation from A Users Perspective,0.149444,"The growing attention to artificial intelligence-based applications has led
to research interest in explainability issues. This emerging research attention
on explainable AI (XAI) advocates the need to investigate end user-centric
explainable AI. Thus, this study aims to investigate usercentric explainable AI
and considered recommendation systems as the study context. We conducted focus
group interviews to collect qualitative data on the recommendation system. We
asked participants about the end users' comprehension of a recommended item,
its probable explanation, and their opinion of making a recommendation
explainable. Our findings reveal that end users want a non-technical and
tailor-made explanation with on-demand supplementary information. Moreover, we
also observed users requiring an explanation about personal data usage,
detailed user feedback, and authentic and reliable explanations. Finally, we
propose a synthesized framework that aims at involving the end user in the
development process for requirements collection and validation.",None,-1
93c99ac8-9caf-4e19-9076-286630f14d94,Specformer: Spectral Graph Neural Networks Meet Transformers,0.977405,"Spectral graph neural networks (GNNs) learn graph representations via
spectral-domain graph convolutions. However, most existing spectral graph
filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a
single filtered value, thus ignoring the global pattern of the spectrum.
Furthermore, these filters are often constructed based on some fixed-order
polynomials, which have limited expressiveness and flexibility. To tackle these
issues, we introduce Specformer, which effectively encodes the set of all
eigenvalues and performs self-attention in the spectral domain, leading to a
learnable set-to-set spectral filter. We also design a decoder with learnable
bases to enable non-local graph convolution. Importantly, Specformer is
equivariant to permutation. By stacking multiple Specformer layers, one can
build a powerful spectral GNN. On synthetic datasets, we show that our
Specformer can better recover ground-truth spectral filters than other spectral
GNNs. Extensive experiments of both node-level and graph-level tasks on
real-world graph datasets show that our Specformer outperforms state-of-the-art
GNNs and learns meaningful spectrum patterns. Code and data are available at
https://github.com/bdy9527/Specformer.",None,-1
9d64bf1c-1ae9-4eb6-98c4-e3edfbe67c53,LatentAugment: Dynamically Optimized Latent Probabilities of Data Augmentation,0.0697689,"Although data augmentation is a powerful technique for improving the
performance of image classification tasks, it is difficult to identify the best
augmentation policy. The optimal augmentation policy, which is the latent
variable, cannot be directly observed. To address this problem, this study
proposes $\textit{LatentAugment}$, which estimates the latent probability of
optimal augmentation. The proposed method is appealing in that it can
dynamically optimize the augmentation strategies for each input and model
parameter in learning iterations. Theoretical analysis shows that LatentAugment
is a general model that includes other augmentation methods as special cases,
and it is simple and computationally efficient in comparison with existing
augmentation methods. Experimental results show that the proposed LatentAugment
has higher test accuracy than previous augmentation methods on the CIFAR-10,
CIFAR-100, SVHN, and ImageNet datasets.",None,-1
f8c64160-f449-4459-95c8-cfd04da3dc90,Rumor Detection with Diverse Counterfactual Evidence,0.162872,"The growth in social media has exacerbated the threat of fake news to
individuals and communities. This draws increasing attention to developing
efficient and timely rumor detection methods. The prevailing approaches resort
to graph neural networks (GNNs) to exploit the post-propagation patterns of the
rumor-spreading process. However, these methods lack inherent interpretation of
rumor detection due to the black-box nature of GNNs. Moreover, these methods
suffer from less robust results as they employ all the propagation patterns for
rumor detection. In this paper, we address the above issues with the proposed
Diverse Counterfactual Evidence framework for Rumor Detection (DCE-RD). Our
intuition is to exploit the diverse counterfactual evidence of an event graph
to serve as multi-view interpretations, which are further aggregated for robust
rumor detection results. Specifically, our method first designs a subgraph
generation strategy to efficiently generate different subgraphs of the event
graph. We constrain the removal of these subgraphs to cause the change in rumor
detection results. Thus, these subgraphs naturally serve as counterfactual
evidence for rumor detection. To achieve multi-view interpretation, we design a
diversity loss inspired by Determinantal Point Processes (DPP) to encourage
diversity among the counterfactual evidence. A GNN-based rumor detection model
further aggregates the diverse counterfactual evidence discovered by the
proposed DCE-RD to achieve interpretable and robust rumor detection results.
Extensive experiments on two real-world datasets show the superior performance
of our method. Our code is available at https://github.com/Vicinity111/DCE-RD.",None,-1
3a64e7b4-422f-4d74-ac2a-1a483caadb18,Tile Networks: Learning Optimal Geometric Layout for Whole-page Recommendation,0.644287,"Finding optimal configurations in a geometric space is a key challenge in
many technological disciplines. Current approaches either rely heavily on human
domain expertise and are difficult to scale. In this paper we show it is
possible to solve configuration optimization problems for whole-page
recommendation using reinforcement learning. The proposed \textit{Tile
Networks} is a neural architecture that optimizes 2D geometric configurations
by arranging items on proper positions. Empirical results on real dataset
demonstrate its superior performance compared to traditional learning to rank
approaches and recent deep models.",None,-1
74399787-8afb-4c7c-8d56-b0da2a25a6f2,Pseudo-label Alignment for Semi-supervised Instance Segmentation,0.867588,"Pseudo-labeling is significant for semi-supervised instance segmentation,
which generates instance masks and classes from unannotated images for
subsequent training. However, in existing pipelines, pseudo-labels that contain
valuable information may be directly filtered out due to mismatches in class
and mask quality. To address this issue, we propose a novel framework, called
pseudo-label aligning instance segmentation (PAIS), in this paper. In PAIS, we
devise a dynamic aligning loss (DALoss) that adjusts the weights of
semi-supervised loss terms with varying class and mask score pairs. Through
extensive experiments conducted on the COCO and Cityscapes datasets, we
demonstrate that PAIS is a promising framework for semi-supervised instance
segmentation, particularly in cases where labeled data is severely limited.
Notably, with just 1\% labeled data, PAIS achieves 21.2 mAP (based on
Mask-RCNN) and 19.9 mAP (based on K-Net) on the COCO dataset, outperforming the
current state-of-the-art model, \ie, NoisyBoundary with 7.7 mAP, by a margin of
over 12 points. Code is available at: \url{https://github.com/hujiecpp/PAIS}.",None,-1
9246d1d9-d27d-43d6-b59d-c2f5d95032ad,Low-Rank Winograd Transformation for 3D Convolutional Neural Networks,0.284027,"This paper focuses on Winograd transformation in 3D convolutional neural
networks (CNNs) that are more over-parameterized compared with the 2D version.
The over-increasing Winograd parameters not only exacerbate training complexity
but also barricade the practical speedups due simply to the volume of
element-wise products in the Winograd domain. We attempt to reduce trainable
parameters by introducing a low-rank Winograd transformation, a novel training
paradigm that decouples the original large tensor into two less
storage-required trainable tensors, leading to a significant complexity
reduction. Built upon our low-rank Winograd transformation, we take one step
ahead by proposing a low-rank oriented sparse granularity that measures
column-wise parameter importance. By simply involving the non-zero columns in
the element-wise product, our sparse granularity is empowered with the ability
to produce a very regular sparse pattern to acquire effectual Winograd
speedups. To better understand the efficacy of our method, we perform extensive
experiments on 3D CNNs. Results manifest that our low-rank Winograd
transformation well outperforms the vanilla Winograd transformation. We also
show that our proposed low-rank oriented sparse granularity permits practical
Winograd acceleration compared with the vanilla counterpart.",None,-1
2d102a7a-9c16-4296-a8a8-024233a1762d,Instant Domain Augmentation for LiDAR Semantic Segmentation,0.592645,"Despite the increasing popularity of LiDAR sensors, perception algorithms
using 3D LiDAR data struggle with the 'sensor-bias problem'. Specifically, the
performance of perception algorithms significantly drops when an unseen
specification of LiDAR sensor is applied at test time due to the domain
discrepancy. This paper presents a fast and flexible LiDAR augmentation method
for the semantic segmentation task, called 'LiDomAug'. It aggregates raw LiDAR
scans and creates a LiDAR scan of any configurations with the consideration of
dynamic distortion and occlusion, resulting in instant domain augmentation. Our
on-demand augmentation module runs at 330 FPS, so it can be seamlessly
integrated into the data loader in the learning framework. In our experiments,
learning-based approaches aided with the proposed LiDomAug are less affected by
the sensor-bias issue and achieve new state-of-the-art domain adaptation
performances on SemanticKITTI and nuScenes dataset without the use of the
target domain data. We also present a sensor-agnostic model that faithfully
works on the various LiDAR configurations.",None,-1
3c9e903e-3a6c-4d67-b3c5-792da4aae2a8,Pure Monte Carlo Counterfactual Regret Minimization,0.859653,"Counterfactual Regret Minimization (CFR) and its variants are the best
algorithms so far for solving large-scale incomplete information games.
However, we believe that there are two problems with CFR: First, matrix
multiplication is required in CFR iteration, and the time complexity of one
iteration is too high; Secondly, the game characteristics in the real world are
different. Just using one CFR algorithm will not be perfectly suitable for all
game problems.
  For these two problems, this paper proposes a new algorithm called Pure CFR
(PCFR) based on CFR. PCFR can be seen as a combination of CFR and Fictitious
Play (FP), inheriting the concept of counterfactual regret (value) from CFR,
and using the best response strategy instead of the regret matching strategy
for the next iteration. This algorithm has three advantages. First, PCFR can be
combined with any CFR variant. The resulting Pure MCCFR (PMCCFR) can
significantly reduce the time and space complexity of one iteration. Secondly,
our experiments show that the convergence speed of the PMCCFR is 2$\sim$3 times
that of the MCCFR. Finally, there is a type of game that is very suitable for
PCFR. We call this type of game clear-game, which is characterized by a high
proportion of dominated strategies. Experiments show that in clear-game, the
convergence rate of PMCCFR is two orders of magnitude higher than that of
MCCFR.",None,-1
37ab9695-82f9-42d0-b3d5-c4520c00fa15,Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation,0.549953,"Very large language models (LLMs) perform extremely well on a spectrum of NLP
tasks in a zero-shot setting. However, little is known about their performance
on human-level NLP problems which rely on understanding psychological concepts,
such as assessing personality traits. In this work, we investigate the
zero-shot ability of GPT-3 to estimate the Big 5 personality traits from users'
social media posts. Through a set of systematic experiments, we find that
zero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA
for broad classification upon injecting knowledge about the trait in the
prompts. However, when prompted to provide fine-grained classification, its
performance drops to close to a simple most frequent class (MFC) baseline. We
further analyze where GPT-3 performs better, as well as worse, than a
pretrained lexical model, illustrating systematic errors that suggest ways to
improve LLMs on human-level NLP tasks.",None,-1
3ffab0dd-6f14-4426-9ed4-f5b6d4983386,Learning Global-Local Correspondence with Semantic Bottleneck for Logical Anomaly Detection,0.432017,"This paper presents a novel framework, named Global-Local Correspondence
Framework (GLCF), for visual anomaly detection with logical constraints. Visual
anomaly detection has become an active research area in various real-world
applications, such as industrial anomaly detection and medical disease
diagnosis. However, most existing methods focus on identifying local structural
degeneration anomalies and often fail to detect high-level functional anomalies
that involve logical constraints. To address this issue, we propose a
two-branch approach that consists of a local branch for detecting structural
anomalies and a global branch for detecting logical anomalies. To facilitate
local-global feature correspondence, we introduce a novel semantic bottleneck
enabled by the visual Transformer. Moreover, we develop feature estimation
networks for each branch separately to detect anomalies. Our proposed framework
is validated using various benchmarks, including industrial datasets, Mvtec AD,
Mvtec Loco AD, and the Retinal-OCT medical dataset. Experimental results show
that our method outperforms existing methods, particularly in detecting logical
anomalies.",None,-1
179d2b74-5284-466d-82b4-348a92106504,ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs,0.770769,"ChatGPT, as a recently launched large language model (LLM), has shown
superior performance in various natural language processing (NLP) tasks.
However, two major limitations hinder its potential applications: (1) the
inflexibility of finetuning on downstream tasks and (2) the lack of
interpretability in the decision-making process. To tackle these limitations,
we propose a novel framework that leverages the power of ChatGPT for specific
tasks, such as text classification, while improving its interpretability. The
proposed framework conducts a knowledge graph extraction task to extract
refined and structural knowledge from the raw data using ChatGPT. The rich
knowledge is then converted into a graph, which is further used to train an
interpretable linear classifier to make predictions. To evaluate the
effectiveness of our proposed method, we conduct experiments on four datasets.
The result shows that our method can significantly improve the performance
compared to directly utilizing ChatGPT for text classification tasks. And our
method provides a more transparent decision-making process compared with
previous text classification methods.",None,-1
07e7dcbd-b54c-4e8d-a0a8-67e715bca4b6,Adiabatic replay for continual learning,0.0724729,"Conventional replay-based approaches to continual learning (CL) require, for
each learning phase with new data, the replay of samples representing all of
the previously learned knowledge in order to avoid catastrophic forgetting.
Since the amount of learned knowledge grows over time in CL problems,
generative replay spends an increasing amount of time just re-learning what is
already known. In this proof-of-concept study, we propose a replay-based CL
strategy that we term adiabatic replay (AR), which derives its efficiency from
the (reasonable) assumption that each new learning phase is adiabatic, i.e.,
represents only a small addition to existing knowledge. Each new learning phase
triggers a sampling process that selectively replays, from the body of existing
knowledge, just such samples that are similar to the new data, in contrast to
replaying all of it. Complete replay is not required since AR represents the
data distribution by GMMs, which are capable of selectively updating their
internal representation only where data statistics have changed. As long as
additions are adiabatic, the amount of to-be-replayed samples need not to
depend on the amount of previously acquired knowledge at all. We verify
experimentally that AR is superior to state-of-the-art deep generative replay
using VAEs.",None,-1
1d8514b0-b1c8-44b4-9b46-1c7d06ae18a0,Unlearn What You Want to Forget: Efficient Unlearning for LLMs,0.901668,"Large language models (LLMs) have achieved significant progress from
pre-training on and memorizing a wide range of textual data, however, this
process might suffer from privacy issues and violations of data protection
regulations. As a result, the ability to easily remove data related to
individual users from such models while not deteriorating their predictive
quality after the removal becomes increasingly important. To address these
issues, in this work, we propose an efficient unlearning framework that could
efficiently update LLMs without having to retrain the whole model after data
removals, by introducing lightweight unlearning layers learned with a selective
teacher-student objective into the transformers. In addition, we introduce a
fusion mechanism to effectively combine different unlearning layers that learns
to forget different sets of data to handle a sequence of forgetting operations.
Experiments on classification and generation tasks demonstrate the
effectiveness of our proposed methods compared to the state-of-the-art
baselines.",None,-1
0399c74f-aad7-4799-98ad-4058eff9daa0,DeblurSR: Event-Based Motion Deblurring Under the Spiking Representation,0.436157,"We present DeblurSR, a novel motion deblurring approach that converts a
blurry image into a sharp video. DeblurSR utilizes event data to compensate for
motion ambiguities and exploits the spiking representation to parameterize the
sharp output video as a mapping from time to intensity. Our key contribution,
the Spiking Representation (SR), is inspired by the neuromorphic principles
determining how biological neurons communicate with each other in living
organisms. We discuss why the spikes can represent sharp edges and how the
spiking parameters are interpreted from the neuromorphic perspective. DeblurSR
has higher output quality and requires fewer computing resources than
state-of-the-art event-based motion deblurring methods. We additionally show
that our approach easily extends to video super-resolution when combined with
recent advances in implicit neural representation. The implementation and
animated visualization of DeblurSR are available at
https://github.com/chensong1995/DeblurSR.",None,-1
2d2b5d2f-9378-43cd-899e-4ba528610e11,Markerless Motion Capture and Biomechanical Analysis Pipeline,0.605595,"Markerless motion capture using computer vision and human pose estimation
(HPE) has the potential to expand access to precise movement analysis. This
could greatly benefit rehabilitation by enabling more accurate tracking of
outcomes and providing more sensitive tools for research. There are numerous
steps between obtaining videos to extracting accurate biomechanical results and
limited research to guide many critical design decisions in these pipelines. In
this work, we analyze several of these steps including the algorithm used to
detect keypoints and the keypoint set, the approach to reconstructing
trajectories for biomechanical inverse kinematics and optimizing the IK
process. Several features we find important are: 1) using a recent algorithm
trained on many datasets that produces a dense set of biomechanically-motivated
keypoints, 2) using an implicit representation to reconstruct smooth,
anatomically constrained marker trajectories for IK, 3) iteratively optimizing
the biomechanical model to match the dense markers, 4) appropriate
regularization of the IK process. Our pipeline makes it easy to obtain accurate
biomechanical estimates of movement in a rehabilitation hospital.",None,-1
5efda5e6-d6b0-4753-96c2-b224f7e19ea7,ChatGPT is on the Horizon: Could a Large Language Model be Suitable for Intelligent Traffic Safety Research and Applications?,0.575939,"ChatGPT embarks on a new era of artificial intelligence and will
revolutionize the way we approach intelligent traffic safety systems. This
paper begins with a brief introduction about the development of large language
models (LLMs). Next, we exemplify using ChatGPT to address key traffic safety
issues. Furthermore, we discuss the controversies surrounding LLMs, raise
critical questions for their deployment, and provide our solutions. Moreover,
we propose an idea of multi-modality representation learning for smarter
traffic safety decision-making and open more questions for application
improvement. We believe that LLM will both shape and potentially facilitate
components of traffic safety research.",None,-1
392c2d8e-cad1-4e27-a494-81521e0e5920,A Hierarchical Destroy and Repair Approach for Solving Very Large-Scale Travelling Salesman Problem,0.414693,"For prohibitively large-scale Travelling Salesman Problems (TSPs), existing
algorithms face big challenges in terms of both computational efficiency and
solution quality. To address this issue, we propose a hierarchical
destroy-and-repair (HDR) approach, which attempts to improve an initial
solution by applying a series of carefully designed destroy-and-repair
operations. A key innovative concept is the hierarchical search framework,
which recursively fixes partial edges and compresses the input instance into a
small-scale TSP under some equivalence guarantee. This neat search framework is
able to deliver highly competitive solutions within a reasonable time. Fair
comparisons based on nineteen famous large-scale instances (with 10,000 to
10,000,000 cities) show that HDR is highly competitive against existing
state-of-the-art TSP algorithms, in terms of both efficiency and solution
quality. Notably, on two large instances with 3,162,278 and 10,000,000 cities,
HDR breaks the world records (i.e., best-known results regardless of
computation time), which were previously achieved by LKH and its variants,
while HDR is completely independent of LKH. Finally, ablation studies are
performed to certify the importance and validity of the hierarchical search
framework.",None,-1
0a3ddc7e-af74-4ff6-9776-043d0730c99b,Synthesizing a Progression of Subtasks for Block-Based Visual Programming Tasks,0.40238,"Block-based visual programming environments play an increasingly important
role in introducing computing concepts to K-12 students. In recent years, they
have also gained popularity in neuro-symbolic AI, serving as a benchmark to
evaluate general problem-solving and logical reasoning skills. The open-ended
and conceptual nature of these visual programming tasks make them challenging,
both for state-of-the-art AI agents as well as for novice programmers. A
natural approach to providing assistance for problem-solving is breaking down a
complex task into a progression of simpler subtasks; however, this is not
trivial given that the solution codes are typically nested and have non-linear
execution behavior. In this paper, we formalize the problem of synthesizing
such a progression for a given reference block-based visual programming task.
We propose a novel synthesis algorithm that generates a progression of subtasks
that are high-quality, well-spaced in terms of their complexity, and solving
this progression leads to solving the reference task. We show the utility of
our synthesis algorithm in improving the efficacy of AI agents (in this case,
neural program synthesizers) for solving tasks in the Karel programming
environment. Then, we conduct a user study to demonstrate that our synthesized
progression of subtasks can assist a novice programmer in solving tasks in the
Hour of Code: Maze Challenge by Code-dot-org.",None,-1
10569332-bd03-4c07-ada1-13d96ee29c12,Time Series as Images: Vision Transformer for Irregularly Sampled Time Series,0.738281,"Irregularly sampled time series are increasingly prevalent, particularly in
medical domains. While various specialized methods have been developed to
handle these irregularities, effectively modeling their complex dynamics and
pronounced sparsity remains a challenge. This paper introduces a novel
perspective by converting irregularly sampled time series into line graph
images, then utilizing powerful pre-trained vision transformers for time series
classification in the same way as image classification. This method not only
largely simplifies specialized algorithm designs but also presents the
potential to serve as a universal framework for time series modeling.
Remarkably, despite its simplicity, our approach outperforms state-of-the-art
specialized algorithms on several popular healthcare and human activity
datasets. Especially in the rigorous leave-sensors-out setting where a portion
of variables is omitted during testing, our method exhibits strong robustness
against varying degrees of missing observations, achieving an impressive
improvement of 42.8% in absolute F1 score points over leading specialized
baselines even with half the variables masked. Code and data are available at
https://github.com/Leezekun/ViTST",None,-1
b972cd54-1815-4ecf-8db2-86393628fd6d,Mimicking the Thinking Process for Emotion Recognition in Conversation with Prompts and Paraphrasing,0.797573,"Emotion recognition in conversation, which aims to predict the emotion for
all utterances, has attracted considerable research attention in recent years.
It is a challenging task since the recognition of the emotion in one utterance
involves many complex factors, such as the conversational context, the
speaker's background, and the subtle difference between emotion labels. In this
paper, we propose a novel framework which mimics the thinking process when
modeling these factors. Specifically, we first comprehend the conversational
context with a history-oriented prompt to selectively gather information from
predecessors of the target utterance. We then model the speaker's background
with an experience-oriented prompt to retrieve the similar utterances from all
conversations. We finally differentiate the subtle label semantics with a
paraphrasing mechanism to elicit the intrinsic label related knowledge. We
conducted extensive experiments on three benchmarks. The empirical results
demonstrate the superiority of our proposed framework over the state-of-the-art
baselines.",None,-1
0edaae5d-6bae-4cad-9fb7-d2a339505fe4,Task-Specific Context Decoupling for Object Detection,0.444875,"Classification and localization are two main sub-tasks in object detection.
Nonetheless, these two tasks have inconsistent preferences for feature context,
i.e., localization expects more boundary-aware features to accurately regress
the bounding box, while more semantic context is preferred for object
classification. Exsiting methods usually leverage disentangled heads to learn
different feature context for each task. However, the heads are still applied
on the same input features, which leads to an imperfect balance between
classifcation and localization. In this work, we propose a novel Task-Specific
COntext DEcoupling (TSCODE) head which further disentangles the feature
encoding for two tasks. For classification, we generate spatially-coarse but
semantically-strong feature encoding. For localization, we provide
high-resolution feature map containing more edge information to better regress
object boundaries. TSCODE is plug-and-play and can be easily incorperated into
existing detection pipelines. Extensive experiments demonstrate that our method
stably improves different detectors by over 1.0 AP with less computational
cost. Our code and models will be publicly released.",None,-1
9074aa60-fae4-4087-a49b-3afe3343a63b,A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment,0.665795,"When communicating with elders with cognitive impairment, cognitive
stimulation (CS) help to maintain the cognitive health of elders. Data sparsity
is the main challenge in building CS-based dialogue systems, particularly in
the Chinese language. To fill this gap, we construct a Chinese CS conversation
(CSConv) dataset, which contains about 2.6K groups of dialogues with CS
principles and emotional support strategy labels. Making chit chat while
providing emotional support is overlooked by the majority of existing cognitive
dialogue systems. In this paper, we propose a multi-source knowledge fusion
method for CS dialogue (CSD), to generate open-ended responses guided by the CS
principle and emotional support strategy. We first use a progressive mask
method based on external knowledge to learn encoders as effective classifiers,
which is the prerequisite to predict the CS principle and emotional support
strategy of the target response. Then a decoder interacts with the perceived CS
principle and emotional support strategy to generate responses. Extensive
experiments conducted on the CSConv dataset demonstrate the effectiveness of
the proposed method, while there is still a large space for improvement
compared to human performance.",None,-1
993b3ea5-31a6-4f9e-b424-1f050467bed8,A Vision for Semantically Enriched Data Science,0.0577402,"The recent efforts in automation of machine learning or data science has
achieved success in various tasks such as hyper-parameter optimization or model
selection. However, key areas such as utilizing domain knowledge and data
semantics are areas where we have seen little automation. Data Scientists have
long leveraged common sense reasoning and domain knowledge to understand and
enrich data for building predictive models. In this paper we discuss important
shortcomings of current data science and machine learning solutions. We then
envision how leveraging ""semantic"" understanding and reasoning on data in
combination with novel tools for data science automation can help with
consistent and explainable data augmentation and transformation. Additionally,
we discuss how semantics can assist data scientists in a new manner by helping
with challenges related to trust, bias, and explainability in machine learning.
Semantic annotation can also help better explore and organize large data
sources.",None,-1
2fc1cb1b-720a-4b22-92c3-4eac0863718f,MADLAD-400: A Multilingual And Document-Level Large Audited Dataset,0.996631,"We introduce MADLAD-400, a manually audited, general domain 3T token
monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss
the limitations revealed by self-auditing MADLAD-400, and the role data
auditing had in the dataset creation process. We then train and release a
10.7B-parameter multilingual machine translation model on 250 billion tokens
covering over 450 languages using publicly available data, and find that it is
competitive with models that are significantly larger, and report the results
on different domains. In addition, we train a 8B-parameter language model, and
assess the results on few-shot translation. We make the baseline models
available to the research community.",None,-1
2f4ce541-bc44-4c7b-b450-e60c170c5090,"If our aim is to build morality into an artificial agent, how might we begin to go about doing so?",0.584772,"As Artificial Intelligence (AI) becomes pervasive in most fields, from
healthcare to autonomous driving, it is essential that we find successful ways
of building morality into our machines, especially for decision-making.
However, the question of what it means to be moral is still debated,
particularly in the context of AI. In this paper, we highlight the different
aspects that should be considered when building moral agents, including the
most relevant moral paradigms and challenges. We also discuss the top-down and
bottom-up approaches to design and the role of emotion and sentience in
morality. We then propose solutions including a hybrid approach to design and a
hierarchical approach to combining moral paradigms. We emphasize how governance
and policy are becoming ever more critical in AI Ethics and in ensuring that
the tasks we set for moral agents are attainable, that ethical behavior is
achieved, and that we obtain good AI.",None,-1
8ea53281-ab67-4938-82f9-960baaa4c415,DarkBERT: A Language Model for the Dark Side of the Internet,0.726194,"Recent research has suggested that there are clear differences in the
language used in the Dark Web compared to that of the Surface Web. As studies
on the Dark Web commonly require textual analysis of the domain, language
models specific to the Dark Web may provide valuable insights to researchers.
In this work, we introduce DarkBERT, a language model pretrained on Dark Web
data. We describe the steps taken to filter and compile the text data used to
train DarkBERT to combat the extreme lexical and structural diversity of the
Dark Web that may be detrimental to building a proper representation of the
domain. We evaluate DarkBERT and its vanilla counterpart along with other
widely used language models to validate the benefits that a Dark Web domain
specific model offers in various use cases. Our evaluations show that DarkBERT
outperforms current language models and may serve as a valuable resource for
future research on the Dark Web.",None,-1
ae351440-be2f-4dab-bd23-0ed776cb872d,Image-free Classifier Injection for Zero-Shot Classification,0.0592625,"Zero-shot learning models achieve remarkable results on image classification
for samples from classes that were not seen during training. However, such
models must be trained from scratch with specialised methods: therefore, access
to a training dataset is required when the need for zero-shot classification
arises. In this paper, we aim to equip pre-trained models with zero-shot
classification capabilities without the use of image data. We achieve this with
our proposed Image-free Classifier Injection with Semantics (ICIS) that injects
classifiers for new, unseen classes into pre-trained classification models in a
post-hoc fashion without relying on image data. Instead, the existing
classifier weights and simple class-wise descriptors, such as class names or
attributes, are used. ICIS has two encoder-decoder networks that learn to
reconstruct classifier weights from descriptors (and vice versa), exploiting
(cross-)reconstruction and cosine losses to regularise the decoding process.
Notably, ICIS can be cheaply trained and applied directly on top of pre-trained
classification models. Experiments on benchmark ZSL datasets show that ICIS
produces unseen classifier weights that achieve strong (generalised) zero-shot
classification performance. Code is available at
https://github.com/ExplainableML/ImageFreeZSL .",None,-1
3e63011f-857a-4a56-8dd7-fd8fa55b5ebb,Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction,0.830708,"Multi-Modal Relation Extraction (MMRE) aims at identifying the relation
between two entities in texts that contain visual clues. Rich visual content is
valuable for the MMRE task, but existing works cannot well model finer
associations among different modalities, failing to capture the truly helpful
visual information and thus limiting relation extraction performance. In this
paper, we propose a novel MMRE framework to better capture the deeper
correlations of text, entity pair, and image/objects, so as to mine more
helpful information for the task, termed as DGF-PT. We first propose a
prompt-based autoregressive encoder, which builds the associations of
intra-modal and inter-modal features related to the task, respectively by
entity-oriented and object-oriented prefixes. To better integrate helpful
visual information, we design a dual-gated fusion module to distinguish the
importance of image/objects and further enrich text representations. In
addition, a generative decoder is introduced with entity type restriction on
relations, better filtering out candidates. Extensive experiments conducted on
the benchmark dataset show that our approach achieves excellent performance
compared to strong competitors, even in the few-shot situation.",None,-1
058f40a6-d63d-4807-a2a2-3eb6156f9578,BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents,0.9684,"The massive successes of large language models (LLMs) encourage the emerging
exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to
generate actions with its core LLM and interact with environments, which
facilitates the ability to resolve complex tasks by conditioning on past
interactions such as observations and actions. Since the investigation of LAA
is still very recent, limited explorations are available. Therefore, we provide
a comprehensive comparison of LAA in terms of both agent architectures and LLM
backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs
such that each labor LAA focuses on one type of action, \textit{i.e.} BOLAA,
where a controller manages the communication among multiple agents. We conduct
simulations on both decision-making and multi-step reasoning environments,
which comprehensively justify the capacity of LAAs. Our performance results
provide quantitative suggestions for designing LAA architectures and the
optimal choice of LLMs, as well as the compatibility of both. We release our
implementation code of LAAs to the public at
\url{https://github.com/salesforce/BOLAA}.",None,-1
dcee5c3c-c99c-4599-99ba-3f63af28042d,NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion,0.974691,"Novel view synthesis from a single image requires inferring occluded regions
of objects and scenes whilst simultaneously maintaining semantic and physical
consistency with the input. Existing approaches condition neural radiance
fields (NeRF) on local image features, projecting points to the input image
plane, and aggregating 2D features to perform volume rendering. However, under
severe occlusion, this projection fails to resolve uncertainty, resulting in
blurry renderings that lack details. In this work, we propose NerfDiff, which
addresses this issue by distilling the knowledge of a 3D-aware conditional
diffusion model (CDM) into NeRF through synthesizing and refining a set of
virtual views at test time. We further propose a novel NeRF-guided distillation
algorithm that simultaneously generates 3D consistent virtual views from the
CDM samples, and finetunes the NeRF based on the improved virtual views. Our
approach significantly outperforms existing NeRF-based and geometry-free
approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.",None,-1
41298614-69cb-4066-8843-3b52ec774bf1,IndoRobusta: Towards Robustness Against Diverse Code-Mixed Indonesian Local Languages,0.45488,"Significant progress has been made on Indonesian NLP. Nevertheless,
exploration of the code-mixing phenomenon in Indonesian is limited, despite
many languages being frequently mixed with Indonesian in daily conversation. In
this work, we explore code-mixing in Indonesian with four embedded languages,
i.e., English, Sundanese, Javanese, and Malay; and introduce IndoRobusta, a
framework to evaluate and improve the code-mixing robustness. Our analysis
shows that the pre-training corpus bias affects the model's ability to better
handle Indonesian-English code-mixing when compared to other local languages,
despite having higher language diversity.",None,-1
d50daa8c-ad14-4b77-9621-a55ba4af983e,Explicit Neural Surfaces: Learning Continuous Geometry With Deformation Fields,0.264789,"We introduce Explicit Neural Surfaces (ENS), an efficient smooth surface
representation that directly encodes topology with a deformation field from a
known base domain. We apply this representation to reconstruct explicit
surfaces from multiple views, where we use a series of neural deformation
fields to progressively transform the base domain into a target shape. By using
meshes as discrete surface proxies, we train the deformation fields through
efficient differentiable rasterization. Using a fixed base domain allows us to
have Laplace-Beltrami eigenfunctions as an intrinsic positional encoding
alongside standard extrinsic Fourier features, with which our approach can
capture fine surface details. Compared to implicit surfaces, ENS trains faster
and has several orders of magnitude faster inference times. The explicit nature
of our approach also allows higher-quality mesh extraction whilst maintaining
competitive surface reconstruction performance and real-time capabilities.",None,-1
c4c75770-cc1a-4258-a1c4-4a7a4bf7211f,Word sense extension,0.756963,"Humans often make creative use of words to express novel senses. A
long-standing effort in natural language processing has been focusing on word
sense disambiguation (WSD), but little has been explored about how the sense
inventory of a word may be extended toward novel meanings. We present a
paradigm of word sense extension (WSE) that enables words to spawn new senses
toward novel context. We develop a framework that simulates novel word sense
extension by first partitioning a polysemous word type into two pseudo-tokens
that mark its different senses, and then inferring whether the meaning of a
pseudo-token can be extended to convey the sense denoted by the token
partitioned from the same word type. Our framework combines cognitive models of
chaining with a learning scheme that transforms a language model embedding
space to support various types of word sense extension. We evaluate our
framework against several competitive baselines and show that it is superior in
predicting plausible novel senses for over 7,500 English words. Furthermore, we
show that our WSE framework improves performance over a range of
transformer-based WSD models in predicting rare word senses with few or zero
mentions in the training data.",None,-1
2960a090-7923-43f5-a120-e107b61dae40,Adversarial Bayesian Augmentation for Single-Source Domain Generalization,0.379589,"Generalizing to unseen image domains is a challenging problem primarily due
to the lack of diverse training data, inaccessible target data, and the large
domain shift that may exist in many real-world settings. As such data
augmentation is a critical component of domain generalization methods that seek
to address this problem. We present Adversarial Bayesian Augmentation (ABA), a
novel algorithm that learns to generate image augmentations in the challenging
single-source domain generalization setting. ABA draws on the strengths of
adversarial learning and Bayesian neural networks to guide the generation of
diverse data augmentations -- these synthesized image domains aid the
classifier in generalizing to unseen domains. We demonstrate the strength of
ABA on several types of domain shift including style shift, subpopulation
shift, and shift in the medical imaging setting. ABA outperforms all previous
state-of-the-art methods, including pre-specified augmentations, pixel-based
and convolutional-based augmentations.",None,-1
ef0a9fc8-9b08-4cc6-8d46-5548fa8c34fc,A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset,0.801013,"Text Classification is the process of categorizing text into the relevant
categories and its algorithms are at the core of many Natural Language
Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP
are the most highly used information retrieval methods in text classification.
We have investigated and analyzed the feature weighting method for text
classification on unstructured data. The proposed model considered two features
N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset
for sentiment analysis. Then we have used the state-of-the-art classifier to
validate the method i.e., Support Vector Machine (SVM), Logistic Regression,
Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and
k-nearest neighbors (KNN). From those two feature extractions, a significant
increase in feature extraction with TF-IDF features rather than based on
N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall
(93.81%), and F1-score (91.99%) value in Random Forest classifier.",None,-1
e2793e2c-8af8-4a1a-a886-c93a079a62a1,OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios,0.308986,"Modern approaches for vision-centric environment perception for autonomous
navigation make extensive use of self-supervised monocular depth estimation
algorithms that output disparity maps. However, when this disparity map is
projected onto 3D space, the errors in disparity are magnified, resulting in a
depth estimation error that increases quadratically as the distance from the
camera increases. Though Light Detection and Ranging (LiDAR) can solve this
issue, it is expensive and not feasible for many applications. To address the
challenge of accurate ranging with low-cost sensors, we propose, OCTraN, a
transformer architecture that uses iterative-attention to convert 2D image
features into 3D occupancy features and makes use of convolution and transpose
convolution to efficiently operate on spatial information. We also develop a
self-supervised training pipeline to generalize the model to any scene by
eliminating the need for LiDAR ground truth by substituting it with
pseudo-ground truth labels obtained from boosted monocular depth estimation.",None,-1
00434c61-9a6b-47eb-b179-33e77e15c819,Blockchain-based Federated Learning with Secure Aggregation in Trusted Execution Environment for Internet-of-Things,0.947796,"This paper proposes a blockchain-based Federated Learning (FL) framework with
Intel Software Guard Extension (SGX)-based Trusted Execution Environment (TEE)
to securely aggregate local models in Industrial Internet-of-Things (IIoTs). In
FL, local models can be tampered with by attackers. Hence, a global model
generated from the tampered local models can be erroneous. Therefore, the
proposed framework leverages a blockchain network for secure model aggregation.
Each blockchain node hosts an SGX-enabled processor that securely performs the
FL-based aggregation tasks to generate a global model. Blockchain nodes can
verify the authenticity of the aggregated model, run a blockchain consensus
mechanism to ensure the integrity of the model, and add it to the distributed
ledger for tamper-proof storage. Each cluster can obtain the aggregated model
from the blockchain and verify its integrity before using it. We conducted
several experiments with different CNN models and datasets to evaluate the
performance of the proposed framework.",None,-1
5b95362f-0b1c-46ed-822b-8ac96b9b6260,Identifiability of Direct Effects from Summary Causal Graphs,0.144259,"Dynamic structural causal models (SCMs) are a powerful framework for
reasoning in dynamic systems about direct effects which measure how a change in
one variable affects another variable while holding all other variables
constant. The causal relations in a dynamic structural causal model can be
qualitatively represented with an acyclic full-time causal graph. Assuming
linearity and no hidden confounding and given the full-time causal graph, the
direct causal effect is always identifiable. However, in many application such
a graph is not available for various reasons but nevertheless experts have
access to the summary causal graph of the full-time causal graph which
represents causal relations between time series while omitting temporal
information and allowing cycles. This paper presents a complete identifiability
result which characterizes all cases for which the direct effect is graphically
identifiable from a summary causal graph and gives two sound finite adjustment
sets that can be used to estimate the direct effect whenever it is
identifiable.",None,-1
a1065af9-c7a5-49a2-b33e-106336394d45,NatCS: Eliciting Natural Customer Support Dialogues,0.0424852,"Despite growing interest in applications based on natural customer support
conversations, there exist remarkably few publicly available datasets that
reflect the expected characteristics of conversations in these settings.
Existing task-oriented dialogue datasets, which were collected to benchmark
dialogue systems mainly in written human-to-bot settings, are not
representative of real customer support conversations and do not provide
realistic benchmarks for systems that are applied to natural data. To address
this gap, we introduce NatCS, a multi-domain collection of spoken customer
service conversations. We describe our process for collecting synthetic
conversations between customers and agents based on natural language phenomena
observed in real conversations. Compared to previous dialogue datasets, the
conversations collected with our approach are more representative of real
human-to-human conversations along multiple metrics. Finally, we demonstrate
potential uses of NatCS, including dialogue act classification and intent
induction from conversations as potential applications, showing that dialogue
act annotations in NatCS provide more effective training data for modeling real
conversations compared to existing synthetic written datasets. We publicly
release NatCS to facilitate research in natural dialog systems",None,-1
9c78b029-29ed-4b56-b079-fb7abbb42e64,PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering,0.964689,"In this paper, we focus on the problem of Medical Visual Question Answering
(MedVQA), which is crucial in efficiently interpreting medical images with
vital clinic-relevant information. Firstly, we reframe the problem of MedVQA as
a generation task that naturally follows the human-machine interaction, we
propose a generative-based model for medical visual understanding by aligning
visual information from a pre-trained vision encoder with a large language
model. Secondly, we establish a scalable pipeline to construct a large-scale
medical visual question-answering dataset, named PMC-VQA, which contains 227k
VQA pairs of 149k images that cover various modalities or diseases. Thirdly, we
pre-train our proposed model on PMC-VQA and then fine-tune it on multiple
public benchmarks, e.g., VQA-RAD and SLAKE, outperforming existing work by a
large margin. Additionally, we propose a test set that has undergone manual
verification, which is significantly more challenging, even the best models
struggle to solve.",None,-1
8e7802d3-b430-43f7-90b4-8e0e666bd5ed,STEVE-1: A Generative Model for Text-to-Behavior in Minecraft,0.920397,"Constructing AI models that respond to text instructions is challenging,
especially for sequential decision-making tasks. This work introduces a
methodology, inspired by unCLIP, for instruction-tuning generative models of
behavior without relying on a large dataset of instruction-labeled
trajectories. Using this methodology, we create an instruction-tuned Video
Pretraining (VPT) model called STEVE-1, which can follow short-horizon
open-ended text and visual instructions in Minecraft. STEVE-1 is trained in two
steps: adapting the pretrained VPT model to follow commands in MineCLIP's
latent space, then training a prior to predict latent codes from text. This
allows us to finetune VPT through self-supervised behavioral cloning and
hindsight relabeling, reducing the need for costly human text annotations, and
all for only $60 of compute. By leveraging pretrained models like VPT and
MineCLIP and employing best practices from text-conditioned image generation,
STEVE-1 sets a new bar for open-ended instruction-following in Minecraft with
low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming
previous baselines and robustly completing 12 of 13 tasks in our early-game
evaluation suite. We provide experimental evidence highlighting key factors for
downstream performance, including pretraining, classifier-free guidance, and
data scaling. All resources, including our model weights, training scripts, and
evaluation tools are made available for further research.",None,-1
2db9584b-9209-4c7e-8eaf-7ce88076759a,MVFusion: Multi-View 3D Object Detection with Semantic-aligned Radar and Camera Fusion,0.811274,"Multi-view radar-camera fused 3D object detection provides a farther
detection range and more helpful features for autonomous driving, especially
under adverse weather. The current radar-camera fusion methods deliver kinds of
designs to fuse radar information with camera data. However, these fusion
approaches usually adopt the straightforward concatenation operation between
multi-modal features, which ignores the semantic alignment with radar features
and sufficient correlations across modals. In this paper, we present MVFusion,
a novel Multi-View radar-camera Fusion method to achieve semantic-aligned radar
features and enhance the cross-modal information interaction. To achieve so, we
inject the semantic alignment into the radar features via the semantic-aligned
radar encoder (SARE) to produce image-guided radar features. Then, we propose
the radar-guided fusion transformer (RGFT) to fuse our radar and image features
to strengthen the two modals' correlation from the global scope via the
cross-attention mechanism. Extensive experiments show that MVFusion achieves
state-of-the-art performance (51.7% NDS and 45.3% mAP) on the nuScenes dataset.
We shall release our code and trained networks upon publication.",None,-1
b9587e1a-d108-4773-95ba-ac8faa9bf9df,Building Extraction from Remote Sensing Images via an Uncertainty-Aware Network,0.785543,"Building extraction aims to segment building pixels from remote sensing
images and plays an essential role in many applications, such as city planning
and urban dynamic monitoring. Over the past few years, deep learning methods
with encoder-decoder architectures have achieved remarkable performance due to
their powerful feature representation capability. Nevertheless, due to the
varying scales and styles of buildings, conventional deep learning models
always suffer from uncertain predictions and cannot accurately distinguish the
complete footprints of the building from the complex distribution of ground
objects, leading to a large degree of omission and commission. In this paper,
we realize the importance of uncertain prediction and propose a novel and
straightforward Uncertainty-Aware Network (UANet) to alleviate this problem. To
verify the performance of our proposed UANet, we conduct extensive experiments
on three public building datasets, including the WHU building dataset, the
Massachusetts building dataset, and the Inria aerial image dataset. Results
demonstrate that the proposed UANet outperforms other state-of-the-art
algorithms by a large margin.",None,-1
4c092c35-ac15-4232-b130-297c703d42c4,Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading,0.793979,"Large language models (LLMs) have advanced in large strides due to the
effectiveness of the self-attention mechanism that processes and compares all
tokens at once. However, this mechanism comes with a fundamental issue -- the
predetermined context window is bound to be limited. Despite attempts to extend
the context window through methods like extrapolating the positional embedding,
using recurrence, or selectively retrieving essential parts of the long
sequence, long-text understanding continues to be a challenge. We propose an
alternative approach which instead treats the LLM as an interactive agent,
allowing it to decide how to read the text via iterative prompting. We
introduce MemWalker, a method that first processes the long context into a tree
of summary nodes. Upon receiving a query, the model navigates this tree in
search of relevant information, and responds once it gathers sufficient
information. On long-text question answering tasks our method outperforms
baseline approaches that use long context windows, recurrence, and retrieval.
We show that, beyond effective reading, MemWalker enhances explainability by
highlighting the reasoning steps as it interactively reads the text;
pinpointing the relevant text segments related to the query.",None,-1
eab5c9c2-102d-4866-8704-95dd295f63bb,Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts,0.517645,"Transformer-based pre-trained models have achieved great improvements in
semantic matching. However, existing models still suffer from insufficient
ability to capture subtle differences. The modification, addition and deletion
of words in sentence pairs may make it difficult for the model to predict their
relationship. To alleviate this problem, we propose a novel Dual Path Modeling
Framework to enhance the model's ability to perceive subtle differences in
sentence pairs by separately modeling affinity and difference semantics. Based
on dual-path modeling framework we design the Dual Path Modeling Network
(DPM-Net) to recognize semantic relations. And we conduct extensive experiments
on 10 well-studied semantic matching and robustness test datasets, and the
experimental results show that our proposed method achieves consistent
improvements over baselines.",None,-1
fc062c03-e15b-4465-b54b-ac0543d3ee91,ViT-DAE: Transformer-driven Diffusion Autoencoder for Histopathology Image Analysis,0.616197,"Generative AI has received substantial attention in recent years due to its
ability to synthesize data that closely resembles the original data source.
While Generative Adversarial Networks (GANs) have provided innovative
approaches for histopathological image analysis, they suffer from limitations
such as mode collapse and overfitting in discriminator. Recently, Denoising
Diffusion models have demonstrated promising results in computer vision. These
models exhibit superior stability during training, better distribution
coverage, and produce high-quality diverse images. Additionally, they display a
high degree of resilience to noise and perturbations, making them well-suited
for use in digital pathology, where images commonly contain artifacts and
exhibit significant variations in staining. In this paper, we present a novel
approach, namely ViT-DAE, which integrates vision transformers (ViT) and
diffusion autoencoders for high-quality histopathology image synthesis. This
marks the first time that ViT has been introduced to diffusion autoencoders in
computational pathology, allowing the model to better capture the complex and
intricate details of histopathology images. We demonstrate the effectiveness of
ViT-DAE on three publicly available datasets. Our approach outperforms recent
GAN-based and vanilla DAE methods in generating realistic images.",None,-1
61c7d0b4-0953-4b42-a33f-49743aaaa552,Joint Dense-Point Representation for Contour-Aware Graph Segmentation,0.0767834,"We present a novel methodology that combines graph and dense segmentation
techniques by jointly learning both point and pixel contour representations,
thereby leveraging the benefits of each approach. This addresses deficiencies
in typical graph segmentation methods where misaligned objectives restrict the
network from learning discriminative vertex and contour features. Our joint
learning strategy allows for rich and diverse semantic features to be encoded,
while alleviating common contour stability issues in dense-based approaches,
where pixel-level objectives can lead to anatomically implausible topologies.
In addition, we identify scenarios where correct predictions that fall on the
contour boundary are penalised and address this with a novel hybrid contour
distance loss. Our approach is validated on several Chest X-ray datasets,
demonstrating clear improvements in segmentation stability and accuracy against
a variety of dense- and point-based methods. Our source code is freely
available at: www.github.com/kitbransby/Joint_Graph_Segmentation",None,-1
68a1633d-e406-4095-a139-3ab507a8fa07,Unpaired Image-to-Image Translation via Neural Schrödinger Bridge,0.860262,"Diffusion models are a powerful class of generative models which simulate
stochastic differential equations (SDEs) to generate data from noise. While
diffusion models have achieved remarkable progress, they have limitations in
unpaired image-to-image (I2I) translation tasks due to the Gaussian prior
assumption. Schr\""{o}dinger Bridge (SB), which learns an SDE to translate
between two arbitrary distributions, have risen as an attractive solution to
this problem. Yet, to our best knowledge, none of SB models so far have been
successful at unpaired translation between high-resolution images. In this
work, we propose Unpaired Neural Schr\""{o}dinger Bridge (UNSB), which expresses
the SB problem as a sequence of adversarial learning problems. This allows us
to incorporate advanced discriminators and regularization to learn a SB between
unpaired data. We show that UNSB is scalable and successfully solves various
unpaired I2I translation tasks. Code: \url{https://github.com/cyclomon/UNSB}",None,-1
ff159299-e68f-47a0-b1ba-44e377d49549,Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,0.972379,"Large Language Models (LLMs) excel in various Natural Language Processing
(NLP) tasks, yet their evaluation, particularly in languages beyond the top
$20$, remains inadequate due to existing benchmarks and metrics limitations.
Employing LLMs as evaluators to rank or score other models' outputs emerges as
a viable solution, addressing the constraints tied to human annotators and
established benchmarks. In this study, we explore the potential of LLM-based
evaluators, specifically GPT-4 in enhancing multilingual evaluation by
calibrating them against $20$K human judgments across three text-generation
tasks, five metrics, and eight languages. Our analysis reveals a bias in
GPT4-based evaluators towards higher scores, underscoring the necessity of
calibration with native speaker judgments, especially in low-resource and
non-Latin script languages, to ensure accurate evaluation of LLM performance
across diverse languages.",None,-1
356c3497-9f7d-4c49-a800-05b59eee833d,MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds,0.3404,"3D semantic segmentation on multi-scan large-scale point clouds plays an
important role in autonomous systems. Unlike the single-scan-based semantic
segmentation task, this task requires distinguishing the motion states of
points in addition to their semantic categories. However, methods designed for
single-scan-based segmentation tasks perform poorly on the multi-scan task due
to the lacking of an effective way to integrate temporal information. We
propose MarS3D, a plug-and-play motion-aware module for semantic segmentation
on multi-scan 3D point clouds. This module can be flexibly combined with
single-scan models to allow them to have multi-scan perception abilities. The
model encompasses two key designs: the Cross-Frame Feature Embedding module for
enriching representation learning and the Motion-Aware Feature Learning module
for enhancing motion awareness. Extensive experiments show that MarS3D can
improve the performance of the baseline model by a large margin. The code is
available at https://github.com/CVMI-Lab/MarS3D.",None,-1
2986efac-838f-481f-a479-314183df2dad,How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?,0.984575,"Recent large vision-language models such as CLIP have shown remarkable
out-of-distribution (OOD) detection and generalization performance. However,
their zero-shot in-distribution (ID) accuracy is often limited for downstream
datasets. Recent CLIP-based fine-tuning methods such as prompt learning have
demonstrated significant improvements in ID classification and OOD
generalization where OOD labels are available. Nonetheless, it remains unclear
whether the model is reliable to semantic shifts without OOD labels. In this
paper, we aim to bridge the gap and present a comprehensive study to understand
how fine-tuning impact OOD detection for few-shot downstream tasks. By framing
OOD detection as multi-modal concept matching, we establish a connection
between fine-tuning methods and various OOD scores. Our results suggest that a
proper choice of OOD scores is essential for CLIP-based fine-tuning. In
particular, the maximum concept matching (MCM) score provides a promising
solution consistently. We also show that prompt learning demonstrates the
state-of-the-art OOD detection performance over the zero-shot counterpart.",None,-1
4e900f88-3880-4590-a941-9da225e30a2c,R2H: Building Multimodal Navigation Helpers that Respond to Help Requests,0.392616,"Intelligent navigation-helper agents are critical as they can navigate users
in unknown areas through environmental awareness and conversational ability,
serving as potential accessibility tools for individuals with disabilities. In
this work, we first introduce a novel benchmark, Respond to Help Requests
(R2H), to promote the development of multi-modal navigation helpers capable of
responding to requests for help, utilizing existing dialog-based embodied
datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH),
which assesses the helper agent's ability to generate informative responses
based on a given dialog history, and (2) Respond during Interaction (RdI),
which evaluates the effectiveness and efficiency of the response during
consistent cooperation with a task performer. Furthermore, we explore two
approaches to construct the navigation-helper agent, including fine-tuning a
novel task-oriented multi-modal response generation model that can see and
respond, named SeeRee, and employing a multi-modal large language model in a
zero-shot manner. Analysis of the task and method was conducted based on both
automatic benchmarking and human evaluations. Project website:
https://sites.google.com/view/response2helprequests/home.",None,-1
611249e0-3f20-4e14-bef2-293f8f6c5026,Inventing art styles with no artistic training data,0.203379,"We propose two procedures to create painting styles using models trained only
on natural images, providing objective proof that the model is not plagiarizing
human art styles. In the first procedure we use the inductive bias from the
artistic medium to achieve creative expression. Abstraction is achieved by
using a reconstruction loss. The second procedure uses an additional natural
image as inspiration to create a new style. These two procedures make it
possible to invent new painting styles with no artistic training data. We
believe that our approach can help pave the way for the ethical employment of
generative AI in art, without infringing upon the originality of human
creators.",None,-1
6e92c0c8-5029-487d-bbc6-3a3b00803cae,Manga109Dialog: A Large-scale Dialogue Dataset for Comics Speaker Detection,0.725526,"The expanding market for e-comics has spurred interest in the development of
automated methods to analyze comics. For further understanding of comics, an
automated approach is needed to link text in comics to characters speaking the
words. Comics speaker detection research has practical applications, such as
automatic character assignment for audiobooks, automatic translation according
to characters' personalities, and inference of character relationships and
stories.
  To deal with the problem of insufficient speaker-to-text annotations, we
created a new annotation dataset Manga109Dialog based on Manga109.
Manga109Dialog is the world's largest comics speaker annotation dataset,
containing 132,692 speaker-to-text pairs. We further divided our dataset into
different levels by prediction difficulties to evaluate speaker detection
methods more appropriately. Unlike existing methods mainly based on distances,
we propose a deep learning-based method using scene graph generation models.
Due to the unique features of comics, we enhance the performance of our
proposed model by considering the frame reading order. We conducted experiments
using Manga109Dialog and other datasets. Experimental results demonstrate that
our scene-graph-based approach outperforms existing methods, achieving a
prediction accuracy of over 75%.",None,-1
cb0a78ce-1b50-42f0-8b31-c9f4749c4075,Robust Domain Misinformation Detection via Multi-modal Feature Alignment,0.738281,"Social media misinformation harms individuals and societies and is
potentialized by fast-growing multi-modal content (i.e., texts and images),
which accounts for higher ""credibility"" than text-only news pieces. Although
existing supervised misinformation detection methods have obtained acceptable
performances in key setups, they may require large amounts of labeled data from
various events, which can be time-consuming and tedious. In turn, directly
training a model by leveraging a publicly available dataset may fail to
generalize due to domain shifts between the training data (a.k.a. source
domains) and the data from target domains. Most prior work on domain shift
focuses on a single modality (e.g., text modality) and ignores the scenario
where sufficient unlabeled target domain data may not be readily available in
an early stage. The lack of data often happens due to the dynamic propagation
trend (i.e., the number of posts related to fake news increases slowly before
catching the public attention). We propose a novel robust domain and
cross-modal approach (\textbf{RDCM}) for multi-modal misinformation detection.
It reduces the domain shift by aligning the joint distribution of textual and
visual modalities through an inter-domain alignment module and bridges the
semantic gap between both modalities through a cross-modality alignment module.
We also propose a framework that simultaneously considers application scenarios
of domain generalization (in which the target domain data is unavailable) and
domain adaptation (in which unlabeled target domain data is available).
Evaluation results on two public multi-modal misinformation detection datasets
(Pheme and Twitter Datasets) evince the superiority of the proposed model. The
formal implementation of this paper can be found in this link:
https://github.com/less-and-less-bugs/RDCM",None,-1
7551623a-6942-43c5-9ab8-5e06681c91b9,Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection,0.827955,"Large Language Models (LLMs) can adapt to new tasks via in-context learning
(ICL). ICL is efficient as it does not require any parameter updates to the
trained LLM, but only few annotated examples as input for the LLM. In this
work, we investigate an active learning approach for ICL, where there is a
limited budget for annotating examples. We propose a model-adaptive
optimization-free algorithm, termed AdaICL, which identifies examples that the
model is uncertain about, and performs semantic diversity-based example
selection. Diversity-based sampling improves overall effectiveness, while
uncertainty sampling improves budget efficiency and helps the LLM learn new
information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage
problem, that dynamically adapts based on the model's feedback and can be
approximately solved via greedy algorithms. Extensive experiments on nine
datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy
points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient
than performing annotations uniformly at random, while it outperforms SOTA with
2x fewer ICL examples.",None,-1
590f895b-a3cd-43f5-bcae-63b39ce00bce,Radar de Parité: An NLP system to measure gender representation in French news stories,0.0578253,"We present the Radar de Parit\'e, an automated Natural Language Processing
(NLP) system that measures the proportion of women and men quoted daily in six
Canadian French-language media outlets. We outline the system's architecture
and detail the challenges we overcame to address French-specific issues, in
particular regarding coreference resolution, a new contribution to the NLP
literature on French. We also showcase statistics covering over one year's
worth of data (282,512 news articles). Our results highlight the
underrepresentation of women in news stories, while also illustrating the
application of modern NLP methods to measure gender representation and address
societal issues.",None,-1
fee9de19-890d-4f7b-8ff6-24b0b21bd493,Can GPT-3 Perform Statutory Reasoning?,0.655884,"Statutory reasoning is the task of reasoning with facts and statutes, which
are rules written in natural language by a legislature. It is a basic legal
skill. In this paper we explore the capabilities of the most capable GPT-3
model, text-davinci-003, on an established statutory-reasoning dataset called
SARA. We consider a variety of approaches, including dynamic few-shot
prompting, chain-of-thought prompting, and zero-shot prompting. While we
achieve results with GPT-3 that are better than the previous best published
results, we also identify several types of clear errors it makes. We
investigate why these errors happen. We discover that GPT-3 has imperfect prior
knowledge of the actual U.S. statutes on which SARA is based. More importantly,
we create simple synthetic statutes, which GPT-3 is guaranteed not to have seen
during training. We find GPT-3 performs poorly at answering straightforward
questions about these simple synthetic statutes.",None,-1
cd9d6677-d1f5-4f4d-9ece-4c3ef004e529,Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models,0.893902,"We present Text2Room, a method for generating room-scale textured 3D meshes
from a given text prompt as input. To this end, we leverage pre-trained 2D
text-to-image models to synthesize a sequence of images from different poses.
In order to lift these outputs into a consistent 3D scene representation, we
combine monocular depth estimation with a text-conditioned inpainting model.
The core idea of our approach is a tailored viewpoint selection such that the
content of each image can be fused into a seamless, textured 3D mesh. More
specifically, we propose a continuous alignment strategy that iteratively fuses
scene frames with the existing geometry to create a seamless mesh. Unlike
existing works that focus on generating single objects or zoom-out trajectories
from text, our method generates complete 3D scenes with multiple objects and
explicit 3D geometry. We evaluate our approach using qualitative and
quantitative metrics, demonstrating it as the first method to generate
room-scale 3D geometry with compelling textures from only text as input.",None,-1
41716c12-1ddd-46f4-993d-b6b9a739805f,RICO: Regularizing the Unobservable for Indoor Compositional Reconstruction,0.676627,"Recently, neural implicit surfaces have become popular for multi-view
reconstruction. To facilitate practical applications like scene editing and
manipulation, some works extend the framework with semantic masks input for the
object-compositional reconstruction rather than the holistic perspective.
Though achieving plausible disentanglement, the performance drops significantly
when processing the indoor scenes where objects are usually partially observed.
We propose RICO to address this by regularizing the unobservable regions for
indoor compositional reconstruction. Our key idea is to first regularize the
smoothness of the occluded background, which then in turn guides the foreground
object reconstruction in unobservable regions based on the object-background
relationship. Particularly, we regularize the geometry smoothness of occluded
background patches. With the improved background surface, the signed distance
function and the reversedly rendered depth of objects can be optimized to bound
them within the background range. Extensive experiments show our method
outperforms other methods on synthetic and real-world indoor scenes and prove
the effectiveness of proposed regularizations. The code is available at
https://github.com/kyleleey/RICO.",None,-1
06237462-92ea-4dbd-98e2-d2b98601be75,Neural Machine Translation Data Generation and Augmentation using ChatGPT,0.601888,"Neural models have revolutionized the field of machine translation, but
creating parallel corpora is expensive and time-consuming. We investigate an
alternative to manual parallel corpora - hallucinated parallel corpora created
by generative language models. Although these models are themselves trained on
parallel data, they can leverage a multilingual vector space to create data,
and may be able to supplement small manually-procured corpora. Our experiments
highlight two key findings - despite a lack of diversity in their output, the
hallucinated data improves the translation signal, even when the domain clashes
with the original dataset.",None,-1
878a5534-93c5-45f5-bdff-c7ffe6ff8738,3D-aware Image Generation using 2D Diffusion Models,0.914092,"In this paper, we introduce a novel 3D-aware image generation method that
leverages 2D diffusion models. We formulate the 3D-aware image generation task
as multiview 2D image set generation, and further to a sequential
unconditional-conditional multiview image generation process. This allows us to
utilize 2D diffusion models to boost the generative modeling power of the
method. Additionally, we incorporate depth information from monocular depth
estimators to construct the training data for the conditional diffusion model
using only still images. We train our method on a large-scale dataset, i.e.,
ImageNet, which is not addressed by previous methods. It produces high-quality
images that significantly outperform prior methods. Furthermore, our approach
showcases its capability to generate instances with large view angles, even
though the training images are diverse and unaligned, gathered from
""in-the-wild"" real-world environments.",None,-1
5c68fe2a-b315-4234-9e8f-d6019555a45f,An Error-Guided Correction Model for Chinese Spelling Error Correction,0.837056,"Although existing neural network approaches have achieved great success on
Chinese spelling correction, there is still room to improve. The model is
required to avoid over-correction and to distinguish a correct token from its
phonological and visually similar ones. In this paper, we propose an
error-guided correction model (EGCM) to improve Chinese spelling correction. By
borrowing the powerful ability of BERT, we propose a novel zero-shot error
detection method to do a preliminary detection, which guides our model to
attend more on the probably wrong tokens in encoding and to avoid modifying the
correct tokens in generating. Furthermore, we introduce a new loss function to
integrate the error confusion set, which enables our model to distinguish
easily misused tokens. Moreover, our model supports highly parallel decoding to
meet real application requirements. Experiments are conducted on widely used
benchmarks. Our model achieves superior performance against state-of-the-art
approaches by a remarkable margin, on both the correction quality and
computation speed.",None,-1
df19a6ec-3d9a-42ff-b98a-73f308793434,Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions,0.538044,"Large-scale pre-trained language models have been shown to be helpful in
improving the naturalness of text-to-speech (TTS) models by enabling them to
produce more naturalistic prosodic patterns. However, these models are usually
word-level or sup-phoneme-level and jointly trained with phonemes, making them
inefficient for the downstream TTS task where only phonemes are needed. In this
work, we propose a phoneme-level BERT (PL-BERT) with a pretext task of
predicting the corresponding graphemes along with the regular masked phoneme
predictions. Subjective evaluations show that our phoneme-level BERT encoder
has significantly improved the mean opinion scores (MOS) of rated naturalness
of synthesized speech compared with the state-of-the-art (SOTA) StyleTTS
baseline on out-of-distribution (OOD) texts.",None,-1
f8e21502-d620-4f34-afd3-11257e47d17b,Communication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter,0.180921,"Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as a
promising paradigm for institutions with limited language resources. This
approach allows multiple institutions to act as clients and train a unified
model through model synchronization, rather than collecting sensitive data for
centralized training. This significantly reduces the cost of corpus collection
and preserves data privacy. However, as pre-trained language models (PLMs)
continue to increase in size, the communication cost for transmitting
parameters during synchronization has become a training speed bottleneck. In
this paper, we propose a communication-efficient Fed-MNMT framework that
addresses this issue by keeping PLMs frozen and only transferring lightweight
adapter modules between clients. Since different language pairs exhibit
substantial discrepancies in data distributions, adapter parameters of clients
may conflict with each other. To tackle this, we explore various clustering
strategies to group parameters for integration and mitigate the negative
effects of conflicting parameters. Experimental results demonstrate that our
framework reduces communication cost by over 98% while achieving similar or
even better performance compared to competitive baselines. Further analysis
reveals that clustering strategies effectively solve the problem of linguistic
discrepancy and pruning adapter modules further improves communication
efficiency.",None,-1
6b93b512-5d4c-4420-9260-6ee12f5f92e2,STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos,0.594313,"We address the problem of extracting key steps from unlabeled procedural
videos, motivated by the potential of Augmented Reality (AR) headsets to
revolutionize job training and performance. We decompose the problem into two
steps: representation learning and key steps extraction. We propose a training
objective, Bootstrapped Multi-Cue Contrastive (BMC2) loss to learn
discriminative representations for various steps without any labels. Different
from prior works, we develop techniques to train a light-weight temporal module
which uses off-the-shelf features for self supervision. Our approach can
seamlessly leverage information from multiple cues like optical flow, depth or
gaze to learn discriminative features for key-steps, making it amenable for AR
applications. We finally extract key steps via a tunable algorithm that
clusters the representations and samples. We show significant improvements over
prior works for the task of key step localization and phase classification.
Qualitative results demonstrate that the extracted key steps are meaningful and
succinctly represent various steps of the procedural tasks.",None,-1
f7060004-6a44-4321-afe2-790c7a3e538d,SynthASpoof: Developing Face Presentation Attack Detection Based on Privacy-friendly Synthetic Data,0.993694,"Recently, significant progress has been made in face presentation attack
detection (PAD), which aims to secure face recognition systems against
presentation attacks, owing to the availability of several face PAD datasets.
However, all available datasets are based on privacy and legally-sensitive
authentic biometric data with a limited number of subjects. To target these
legal and technical challenges, this work presents the first synthetic-based
face PAD dataset, named SynthASpoof, as a large-scale PAD development dataset.
The bona fide samples in SynthASpoof are synthetically generated and the attack
samples are collected by presenting such synthetic data to capture systems in a
real attack scenario. The experimental results demonstrate the feasibility of
using SynthASpoof for the development of face PAD. Moreover, we boost the
performance of such a solution by incorporating the domain generalization tool
MixStyle into the PAD solutions. Additionally, we showed the viability of using
synthetic data as a supplement to enrich the diversity of limited authentic
training data and consistently enhance PAD performances. The SynthASpoof
dataset, containing 25,000 bona fide and 78,800 attack samples, the
implementation, and the pre-trained weights are made publicly available.",None,-1
e11af845-ffc8-435d-990b-0b06099c3caa,On the Role of Emergent Communication for Social Learning in Multi-Agent Reinforcement Learning,0.248861,"Explicit communication among humans is key to coordinating and learning.
Social learning, which uses cues from experts, can greatly benefit from the
usage of explicit communication to align heterogeneous policies, reduce sample
complexity, and solve partially observable tasks. Emergent communication, a
type of explicit communication, studies the creation of an artificial language
to encode a high task-utility message directly from data. However, in most
cases, emergent communication sends insufficiently compressed messages with
little or null information, which also may not be understandable to a
third-party listener. This paper proposes an unsupervised method based on the
information bottleneck to capture both referential complexity and task-specific
utility to adequately explore sparse social communication scenarios in
multi-agent reinforcement learning (MARL). We show that our model is able to i)
develop a natural-language-inspired lexicon of messages that is independently
composed of a set of emergent concepts, which span the observations and intents
with minimal bits, ii) develop communication to align the action policies of
heterogeneous agents with dissimilar feature models, and iii) learn a
communication policy from watching an expert's action policy, which we term
`social shadowing'.",None,-1
e66e6d24-3c96-48f9-9a81-ad9e57d041fc,AutoAD: Movie Description in Context,0.447745,"The objective of this paper is an automatic Audio Description (AD) model that
ingests movies and outputs AD in text form. Generating high-quality movie AD is
challenging due to the dependency of the descriptions on context, and the
limited amount of training data available. In this work, we leverage the power
of pretrained foundation models, such as GPT and CLIP, and only train a mapping
network that bridges the two models for visually-conditioned text generation.
In order to obtain high-quality AD, we make the following four contributions:
(i) we incorporate context from the movie clip, AD from previous clips, as well
as the subtitles; (ii) we address the lack of training data by pretraining on
large-scale datasets, where visual or contextual information is unavailable,
e.g. text-only AD without movies or visual captioning datasets without context;
(iii) we improve on the currently available AD datasets, by removing label
noise in the MAD dataset, and adding character naming information; and (iv) we
obtain strong results on the movie AD task compared with previous methods.",None,-1
73ceb786-f7dc-4139-8feb-ba7ac608f912,SVIT: Scaling up Visual Instruction Tuning,0.834127,"Thanks to the emerging of foundation models, the large language and vision
models are integrated to acquire the multimodal ability of visual captioning,
question answering, etc. Although existing multimodal models present impressive
performance of visual understanding and reasoning, their limits are still
largely under-explored due to the scarcity of high-quality instruction tuning
data. To push the limits of multimodal capability, we Scale up Visual
Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual
instruction tuning data including 1.6M conversation question-answer (QA) pairs,
1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed
image descriptions. Besides the volume, the proposed dataset is also featured
by the high quality and rich diversity, which is generated by prompting GPT-4
with the abundant manual annotations of images. We also propose a new data
recipe to select subset with better diversity and balance, which evokes model's
superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on
the proposed dataset, outperforms state-of-the-art Multimodal Large Language
Models on popular benchmarks. The data and code are publicly available at
https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.",None,-1
3c831ad4-cbd8-4fa6-b7fe-316e33141203,Toward A Logical Theory Of Fairness and Bias,0.248584,"Fairness in machine learning is of considerable interest in recent years
owing to the propensity of algorithms trained on historical data to amplify and
perpetuate historical biases. In this paper, we argue for a formal
reconstruction of fairness definitions, not so much to replace existing
definitions but to ground their application in an epistemic setting and allow
for rich environmental modelling. Consequently we look into three notions:
fairness through unawareness, demographic parity and counterfactual fairness,
and formalise these in the epistemic situation calculus.",None,-1
e4b78444-9f90-4fe6-9ed0-f4d7f3a886cc,Privacy Aware Question-Answering System for Online Mental Health Risk Assessment,0.303483,"Social media platforms have enabled individuals suffering from mental
illnesses to share their lived experiences and find the online support
necessary to cope. However, many users fail to receive genuine clinical
support, thus exacerbating their symptoms. Screening users based on what they
post online can aid providers in administering targeted healthcare and minimize
false positives. Pre-trained Language Models (LMs) can assess users' social
media data and classify them in terms of their mental health risk. We propose a
Question-Answering (QA) approach to assess mental health risk using the
Unified-QA model on two large mental health datasets. To protect user data, we
extend Unified-QA by anonymizing the model training process using differential
privacy. Our results demonstrate the effectiveness of modeling risk assessment
as a QA task, specifically for mental health use cases. Furthermore, the
model's performance decreases by less than 1% with the inclusion of
differential privacy. The proposed system's performance is indicative of a
promising research direction that will lead to the development of privacy-aware
diagnostic systems.",None,-1
4c5ef70f-b205-41e4-a0d6-6bb09585a651,Vanishing Point Estimation in Uncalibrated Images with Prior Gravity Direction,0.26668,"We tackle the problem of estimating a Manhattan frame, i.e. three orthogonal
vanishing points, and the unknown focal length of the camera, leveraging a
prior vertical direction. The direction can come from an Inertial Measurement
Unit that is a standard component of recent consumer devices, e.g.,
smartphones. We provide an exhaustive analysis of minimal line configurations
and derive two new 2-line solvers, one of which does not suffer from
singularities affecting existing solvers. Additionally, we design a new
non-minimal method, running on an arbitrary number of lines, to boost the
performance in local optimization. Combining all solvers in a hybrid robust
estimator, our method achieves increased accuracy even with a rough prior.
Experiments on synthetic and real-world datasets demonstrate the superior
accuracy of our method compared to the state of the art, while having
comparable runtimes. We further demonstrate the applicability of our solvers
for relative rotation estimation. The code is available at
https://github.com/cvg/VP-Estimation-with-Prior-Gravity.",None,-1
4b5c7498-8769-4db3-a888-3165f07cfbf4,Tackling Concept Shift in Text Classification using Entailment-style Modeling,0.116046,"Pre-trained language models (PLMs) have seen tremendous success in text
classification (TC) problems in the context of Natural Language Processing
(NLP). In many real-world text classification tasks, the class definitions
being learned do not remain constant but rather change with time - this is
known as Concept Shift. Most techniques for handling concept shift rely on
retraining the old classifiers with the newly labelled data. However, given the
amount of training data required to fine-tune large DL models for the new
concepts, the associated labelling costs can be prohibitively expensive and
time consuming. In this work, we propose a reformulation, converting vanilla
classification into an entailment-style problem that requires significantly
less data to re-train the text classifier to adapt to new concepts. We
demonstrate the effectiveness of our proposed method on both real world &
synthetic datasets achieving absolute F1 gains upto 7% and 40% respectively in
few-shot settings. Further, upon deployment, our solution also helped save 75%
of labeling costs overall.",None,-1
000eb4aa-38b2-415a-af42-2231d248046e,A Unified Generative Approach to Product Attribute-Value Identification,0.956841,"Product attribute-value identification (PAVI) has been studied to link
products on e-commerce sites with their attribute values (e.g., <Material,
Cotton>) using product text as clues. Technical demands from real-world
e-commerce platforms require PAVI methods to handle unseen values,
multi-attribute values, and canonicalized values, which are only partly
addressed in existing extraction- and classification-based approaches.
Motivated by this, we explore a generative approach to the PAVI task. We
finetune a pre-trained generative model, T5, to decode a set of attribute-value
pairs as a target sequence from the given product text. Since the attribute
value pairs are unordered set elements, how to linearize them will matter; we,
thus, explore methods of composing an attribute-value pair and ordering the
pairs for the task. Experimental results confirm that our generation-based
approach outperforms the existing extraction and classification-based methods
on large-scale real-world datasets meant for those methods.",None,-1
2f555f0b-95d4-4655-858c-38c89500aa68,Efficient Explainable Face Verification based on Similarity Score Argument Backpropagation,0.710283,"Explainable Face Recognition is gaining growing attention as the use of the
technology is gaining ground in security-critical applications. Understanding
why two faces images are matched or not matched by a given face recognition
system is important to operators, users, anddevelopers to increase trust,
accountability, develop better systems, and highlight unfair behavior. In this
work, we propose xSSAB, an approach to back-propagate similarity score-based
arguments that support or oppose the face matching decision to visualize
spatial maps that indicate similar and dissimilar areas as interpreted by the
underlying FR model. Furthermore, we present Patch-LFW, a new explainable face
verification benchmark that enables along with a novel evaluation protocol, the
first quantitative evaluation of the validity of similarity and dissimilarity
maps in explainable face recognition approaches. We compare our efficient
approach to state-of-the-art approaches demonstrating a superior trade-off
between efficiency and performance. The code as well as the proposed Patch-LFW
is publicly available at: https://github.com/marcohuber/xSSAB.",None,-1
00008bb6-40e9-4e7f-bb7b-cf4d8e633937,A Few-Shot Attention Recurrent Residual U-Net for Crack Segmentation,0.2181,"Recent studies indicate that deep learning plays a crucial role in the
automated visual inspection of road infrastructures. However, current learning
schemes are static, implying no dynamic adaptation to users' feedback. To
address this drawback, we present a few-shot learning paradigm for the
automated segmentation of road cracks, which is based on a U-Net architecture
with recurrent residual and attention modules (R2AU-Net). The retraining
strategy dynamically fine-tunes the weights of the U-Net as a few new rectified
samples are being fed into the classifier. Extensive experiments show that the
proposed few-shot R2AU-Net framework outperforms other state-of-the-art
networks in terms of Dice and IoU metrics, on a new dataset, named CrackMap,
which is made publicly available at https://github.com/ikatsamenis/CrackMap.",None,-1
0aad4ff8-4f76-480c-92eb-fcbb6b931335,Learning Signed Distance Functions from Noisy 3D Point Clouds via Noise to Noise Mapping,0.475243,"Learning signed distance functions (SDFs) from 3D point clouds is an
important task in 3D computer vision. However, without ground truth signed
distances, point normals or clean point clouds, current methods still struggle
from learning SDFs from noisy point clouds. To overcome this challenge, we
propose to learn SDFs via a noise to noise mapping, which does not require any
clean point cloud or ground truth supervision for training. Our novelty lies in
the noise to noise mapping which can infer a highly accurate SDF of a single
object or scene from its multiple or even single noisy point cloud
observations. Our novel learning manner is supported by modern Lidar systems
which capture multiple noisy observations per second. We achieve this by a
novel loss which enables statistical reasoning on point clouds and maintains
geometric consistency although point clouds are irregular, unordered and have
no point correspondence among noisy observations. Our evaluation under the
widely used benchmarks demonstrates our superiority over the state-of-the-art
methods in surface reconstruction, point cloud denoising and upsampling. Our
code, data, and pre-trained models are available at
https://github.com/mabaorui/Noise2NoiseMapping/",None,-1
ad92e5ba-f45e-4a28-ac4a-3e51cf5df573,IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases,0.350466,"Named Entity Recognition (NER) is a core natural language processing task in
which pre-trained language models have shown remarkable performance. However,
standard benchmarks like CoNLL 2003 do not address many of the challenges that
deployed NER systems face, such as having to classify emerging or complex
entities in a fine-grained way. In this paper we present a novel NER cascade
approach comprising three steps: first, identifying candidate entities in the
input sentence; second, linking the each candidate to an existing knowledge
base; third, predicting the fine-grained category for each entity candidate. We
empirically demonstrate the significance of external knowledge bases in
accurately classifying fine-grained and emerging entities. Our system exhibits
robust performance in the MultiCoNER2 shared task, even in the low-resource
language setting where we leverage knowledge bases of high-resource languages.",None,-1
28f25fce-c97a-4f82-8864-0e8fc2df7abd,Enabling Human-Centered AI: A Methodological Perspective,0.399003,"Human-centered AI (HCAI) is a design philosophy that advocates prioritizing
humans in designing, developing, and deploying intelligent systems, aiming to
maximize the benefits of AI to humans and avoid potential adverse impacts.
While HCAI continues to influence, the lack of guidance on methodology in
practice makes its adoption challenging. This paper proposes a comprehensive
HCAI framework based on our previous work with integrated components, including
design goals, design principles, implementation approaches, interdisciplinary
teams, HCAI methods, and HCAI processes. This paper also presents a
""three-layer"" approach to facilitate the implementation of the framework. We
believe this systematic and executable framework can overcome the weaknesses in
current HCAI frameworks and the challenges currently faced in practice, putting
it into action to enable HCAI further.",None,-1
57a763af-4607-43b7-a057-2fb5b9c37491,Lost in Translation: Large Language Models in Non-English Content Analysis,0.436908,"In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa,
Google's PaLM) have become the dominant approach for building AI systems to
analyze and generate language online. However, the automated systems that
increasingly mediate our interactions online -- such as chatbots, content
moderation systems, and search engines -- are primarily designed for and work
far more effectively in English than in the world's other 7,000 languages.
Recently, researchers and technology companies have attempted to extend the
capabilities of large language models into languages other than English by
building what are called multilingual language models.
  In this paper, we explain how these multilingual language models work and
explore their capabilities and limits. Part I provides a simple technical
explanation of how large language models work, why there is a gap in available
data between English and other languages, and how multilingual language models
attempt to bridge that gap. Part II accounts for the challenges of doing
content analysis with large language models in general and multilingual
language models in particular. Part III offers recommendations for companies,
researchers, and policymakers to keep in mind when considering researching,
developing and deploying large and multilingual language models.",None,-1
6e54b270-5007-4b3f-878a-4d463823c143,Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model,0.389701,"While large language models have proven effective in a huge range of
downstream applications, they often generate text that is problematic or lacks
a desired attribute. In this paper, we introduce Reward-Augmented Decoding
(RAD), a text generation procedure that uses a small unidirectional reward
model to encourage a language model to generate text that has certain
properties. Specifically, RAD uses the reward model to score generations as
they are produced and rescales sampling probabilities to favor high-reward
tokens. By using a unidirectional reward model, RAD can cache activations from
prior generation steps to decrease computational overhead. Through experiments
on generating non-toxic and sentiment-controlled text, we demonstrate that RAD
performs best among methods that change only the generation procedure and
matches the performance of state-of-the-art methods that involve re-training
the language model. We further validate that RAD is effective on very large
language models while incurring a minimal computational overhead.",None,-1
1ead0706-9708-4b9c-904c-85db78b51dd1,Test Time Adaptation for Blind Image Quality Assessment,0.86683,"While the design of blind image quality assessment (IQA) algorithms has
improved significantly, the distribution shift between the training and testing
scenarios often leads to a poor performance of these methods at inference time.
This motivates the study of test time adaptation (TTA) techniques to improve
their performance at inference time. Existing auxiliary tasks and loss
functions used for TTA may not be relevant for quality-aware adaptation of the
pre-trained model. In this work, we introduce two novel quality-relevant
auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In
particular, we introduce a group contrastive loss at the batch level and a
relative rank loss at the sample level to make the model quality aware and
adapt to the target data. Our experiments reveal that even using a small batch
of images from the test distribution helps achieve significant improvement in
performance by updating the batch normalization statistics of the source model.",None,-1
8791300b-26ba-4719-98bd-fe729f98b90a,Improving Generalization for Multimodal Fake News Detection,0.153966,"The increasing proliferation of misinformation and its alarming impact have
motivated both industry and academia to develop approaches for fake news
detection. However, state-of-the-art approaches are usually trained on datasets
of smaller size or with a limited set of specific topics. As a consequence,
these models lack generalization capabilities and are not applicable to
real-world data. In this paper, we propose three models that adopt and
fine-tune state-of-the-art multimodal transformers for multimodal fake news
detection. We conduct an in-depth analysis by manipulating the input data aimed
to explore models performance in realistic use cases on social media. Our study
across multiple models demonstrates that these systems suffer significant
performance drops against manipulated data. To reduce the bias and improve
model generalization, we suggest training data augmentation to conduct more
meaningful experiments for fake news detection on social media. The proposed
data augmentation techniques enable models to generalize better and yield
improved state-of-the-art results.",None,-1
3b63b9cc-587e-406c-9d6b-5b38fa49a143,UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective,0.33026,"We propose a new paradigm for universal information extraction (IE) that is
compatible with any schema format and applicable to a list of IE tasks, such as
named entity recognition, relation extraction, event extraction and sentiment
analysis. Our approach converts the text-based IE tasks as the token-pair
problem, which uniformly disassembles all extraction targets into joint span
detection, classification and association problems with a unified extractive
framework, namely UniEX. UniEX can synchronously encode schema-based prompt and
textual information, and collaboratively learn the generalized knowledge from
pre-defined information using the auto-encoder language models. We develop a
traffine attention mechanism to integrate heterogeneous factors including
tasks, labels and inside tokens, and obtain the extraction target via a scoring
matrix. Experiment results show that UniEX can outperform generative universal
IE models in terms of performance and inference-speed on $14$ benchmarks IE
datasets with the supervised setting. The state-of-the-art performance in
low-resource scenarios also verifies the transferability and effectiveness of
UniEX.",None,-1
44b66100-f44f-43e4-8fe1-dcf8d35ca594,Learning from Children: Improving Image-Caption Pretraining via Curriculum,0.0330592,"Image-caption pretraining has been quite successfully used for downstream
vision tasks like zero-shot image classification and object detection. However,
image-caption pretraining is still a hard problem -- it requires multiple
concepts (nouns) from captions to be aligned to several objects in images. To
tackle this problem, we go to the roots -- the best learner, children. We take
inspiration from cognitive science studies dealing with children's language
learning to propose a curriculum learning framework. The learning begins with
easy-to-align image caption pairs containing one concept per caption. The
difficulty is progressively increased with each new phase by adding one more
concept per caption. Correspondingly, the knowledge acquired in each learning
phase is utilized in subsequent phases to effectively constrain the learning
problem to aligning one new concept-object pair in each phase. We show that
this learning strategy improves over vanilla image-caption training in various
settings -- pretraining from scratch, using a pretrained image or/and
pretrained text encoder, low data regime etc.",None,-1
570211b2-2f82-49e9-a15f-1960f193ef53,Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,0.99921,"In this paper, we aim to develop a large language model (LLM) with the
reasoning ability on complex graph data. Currently, LLMs have achieved very
impressive performance on various natural language learning tasks, extensions
of which have also been applied to study the vision tasks with multi-modal
data. However, when it comes to the graph learning tasks, existing LLMs present
very serious flaws due to their several inherited weaknesses in performing
{multi-step logic reasoning}, {precise mathematical calculation} and
{perception about the spatial and temporal factors}.
  To address such challenges, in this paper, we will investigate the
principles, methodologies and algorithms to empower existing LLMs with graph
reasoning ability, which will have tremendous impacts on the current research
of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer
models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer)
framework to teach LLMs themselves with prompts augmented by ChatGPT to use
external graph reasoning API tools. Specifically, we will investigate to teach
Graph-ToolFormer to handle various graph data reasoning tasks in this paper,
including both (1) very basic graph data loading and graph property reasoning
tasks, ranging from simple graph order and size to the graph diameter and
periphery, and (2) more advanced reasoning tasks on real-world graph data, such
as bibliographic networks, protein molecules, sequential recommender systems,
social networks and knowledge graphs.",None,-1
6c2a8f3e-08b0-407a-a99a-41c05bbfb128,Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,0.535441,"Disagreement in natural language annotation has mostly been studied from a
perspective of biases introduced by the annotators and the annotation
frameworks. Here, we propose to analyze another source of bias: task design
bias, which has a particularly strong impact on crowdsourced linguistic
annotations where natural language is used to elicit the interpretation of
laymen annotators. For this purpose we look at implicit discourse relation
annotation, a task that has repeatedly been shown to be difficult due to the
relations' ambiguity. We compare the annotations of 1,200 discourse relations
obtained using two distinct annotation tasks and quantify the biases of both
methods across four different domains. Both methods are natural language
annotation tasks designed for crowdsourcing. We show that the task design can
push annotators towards certain relations and that some discourse relations
senses can be better elicited with one or the other annotation approach. We
also conclude that this type of bias should be taken into account when training
and testing models.",None,-1
3709de25-07ec-4e78-b73b-b6eb7af2d7c5,DreamEditor: Text-Driven 3D Scene Editing with Neural Fields,0.976824,"Neural fields have achieved impressive advancements in view synthesis and
scene reconstruction. However, editing these neural fields remains challenging
due to the implicit encoding of geometry and texture information. In this
paper, we propose DreamEditor, a novel framework that enables users to perform
controlled editing of neural fields using text prompts. By representing scenes
as mesh-based neural fields, DreamEditor allows localized editing within
specific regions. DreamEditor utilizes the text encoder of a pretrained
text-to-Image diffusion model to automatically identify the regions to be
edited based on the semantics of the text prompts. Subsequently, DreamEditor
optimizes the editing region and aligns its geometry and texture with the text
prompts through score distillation sampling [29]. Extensive experiments have
demonstrated that DreamEditor can accurately edit neural fields of real-world
scenes according to the given text prompts while ensuring consistency in
irrelevant areas. DreamEditor generates highly realistic textures and geometry,
significantly surpassing previous works in both quantitative and qualitative
evaluations.",None,-1
69c5afcd-c916-48e7-aeb8-310979723aa5,Real-time Multi-Class Helmet Violation Detection Using Few-Shot Data Sampling Technique and YOLOv8,1.0,"Traffic safety is a major global concern. Helmet usage is a key factor in
preventing head injuries and fatalities caused by motorcycle accidents.
However, helmet usage violations continue to be a significant problem. To
identify such violations, automatic helmet detection systems have been proposed
and implemented using computer vision techniques. Real-time implementation of
such systems is crucial for traffic surveillance and enforcement, however, most
of these systems are not real-time. This study proposes a robust real-time
helmet violation detection system. The proposed system utilizes a unique data
processing strategy, referred to as few-shot data sampling, to develop a robust
model with fewer annotations, and a single-stage object detection model, YOLOv8
(You Only Look Once Version 8), for detecting helmet violations in real-time
from video frames. Our proposed method won 7th place in the 2023 AI City
Challenge, Track 5, with an mAP score of 0.5861 on experimental validation
data. The experimental results demonstrate the effectiveness, efficiency, and
robustness of the proposed system.",None,-1
2c8d64b9-5aa6-4f2b-b6cf-619f61e6b2e9,Hi4D: 4D Instance Segmentation of Close Human Interaction,0.893827,"We propose Hi4D, a method and dataset for the automatic analysis of
physically close human-human interaction under prolonged contact. Robustly
disentangling several in-contact subjects is a challenging task due to
occlusions and complex shapes. Hence, existing multi-view systems typically
fuse 3D surfaces of close subjects into a single, connected mesh. To address
this issue we leverage i) individually fitted neural implicit avatars; ii) an
alternating optimization scheme that refines pose and surface through periods
of close proximity; and iii) thus segment the fused raw scans into individual
instances. From these instances we compile Hi4D dataset of 4D textured scans of
20 subject pairs, 100 sequences, and a total of more than 11K frames. Hi4D
contains rich interaction-centric annotations in 2D and 3D alongside accurately
registered parametric body models. We define varied human pose and shape
estimation tasks on this dataset and provide results from state-of-the-art
methods on these benchmarks.",None,-1
19164fef-ad1b-4a2a-a62f-f6d6f8513d1c,Urban Generative Intelligence (UGI): A Foundational Platform for Agents in Embodied City Environment,0.548843,"Urban environments, characterized by their complex, multi-layered networks
encompassing physical, social, economic, and environmental dimensions, face
significant challenges in the face of rapid urbanization. These challenges,
ranging from traffic congestion and pollution to social inequality, call for
advanced technological interventions. Recent developments in big data,
artificial intelligence, urban computing, and digital twins have laid the
groundwork for sophisticated city modeling and simulation. However, a gap
persists between these technological capabilities and their practical
implementation in addressing urban challenges in an systemic-intelligent way.
This paper proposes Urban Generative Intelligence (UGI), a novel foundational
platform integrating Large Language Models (LLMs) into urban systems to foster
a new paradigm of urban intelligence. UGI leverages CityGPT, a foundation model
trained on city-specific multi-source data, to create embodied agents for
various urban tasks. These agents, operating within a textual urban environment
emulated by city simulator and urban knowledge graph, interact through a
natural language interface, offering an open platform for diverse intelligent
and embodied agent development. This platform not only addresses specific urban
issues but also simulates complex urban systems, providing a multidisciplinary
approach to understand and manage urban complexity. This work signifies a
transformative step in city science and urban intelligence, harnessing the
power of LLMs to unravel and address the intricate dynamics of urban systems.
The code repository with demonstrations will soon be released here
https://github.com/tsinghua-fib-lab/UGI.",None,-1
0dcf6af4-e93a-4b9b-9106-9508233fe30f,Learning in a Single Domain for Non-Stationary Multi-Texture Synthesis,0.0892953,"This paper aims for a new generation task: non-stationary multi-texture
synthesis, which unifies synthesizing multiple non-stationary textures in a
single model. Most non-stationary textures have large scale variance and can
hardly be synthesized through one model. To combat this, we propose a
multi-scale generator to capture structural patterns of various scales and
effectively synthesize textures with a minor cost. However, it is still hard to
handle textures of different categories with different texture patterns.
Therefore, we present a category-specific training strategy to focus on
learning texture pattern of a specific domain. Interestingly, once trained, our
model is able to produce multi-pattern generations with dynamic variations
without the need to finetune the model for different styles. Moreover, an
objective evaluation metric is designed for evaluating the quality of texture
expansion and global structure consistency. To our knowledge, ours is the first
scheme for this challenging task, including model, training, and evaluation.
Experimental results demonstrate the proposed method achieves superior
performance and time efficiency. The code will be available after the
publication.",None,-1
da16153c-8487-423b-8f83-de59e89d4223,Low-Light Image Enhancement via Structure Modeling and Guidance,0.999538,"This paper proposes a new framework for low-light image enhancement by
simultaneously conducting the appearance as well as structure modeling. It
employs the structural feature to guide the appearance enhancement, leading to
sharp and realistic results. The structure modeling in our framework is
implemented as the edge detection in low-light images. It is achieved with a
modified generative model via designing a structure-aware feature extractor and
generator. The detected edge maps can accurately emphasize the essential
structural information, and the edge prediction is robust towards the noises in
dark areas. Moreover, to improve the appearance modeling, which is implemented
with a simple U-Net, a novel structure-guided enhancement module is proposed
with structure-guided feature synthesis layers. The appearance modeling, edge
detector, and enhancement module can be trained end-to-end. The experiments are
conducted on representative datasets (sRGB and RAW domains), showing that our
model consistently achieves SOTA performance on all datasets with the same
architecture.",None,-1
a3fe1eb9-140d-47be-97c0-ef1745c576a1,Procedural Text Mining with Large Language Models,0.64656,"Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.",None,-1
bc57d0d7-b2ae-4cd6-84eb-b2c3763b1258,MeciFace: Mechanomyography and Inertial Fusion-based Glasses for Edge Real-Time Recognition of Facial and Eating Activities,0.508197,"The increasing prevalence of stress-related eating behaviors and their impact
on overall health highlights the importance of effective and ubiquitous
monitoring systems. In this paper, we present MeciFace, an innovative wearable
technology designed to monitor facial expressions and eating activities in
real-time on-the-edge (RTE). MeciFace aims to provide a low-power,
privacy-conscious, and highly accurate tool for promoting healthy eating
behaviors and stress management. We employ lightweight convolutional neural
networks as backbone models for facial expression and eating monitoring
scenarios. The MeciFace system ensures efficient data processing with a tiny
memory footprint, ranging from 11KB to 19 KB. During RTE evaluation, the system
achieves an F1-score of < 86% for facial expression recognition and 94% for
eating/drinking monitoring, for the RTE of unseen users (user-independent
case).",None,-1
dd071e98-54cf-460f-a778-201e772c462c,CertViT: Certified Robustness of Pre-Trained Vision Transformers,0.0756084,"Lipschitz bounded neural networks are certifiably robust and have a good
trade-off between clean and certified accuracy. Existing Lipschitz bounding
methods train from scratch and are limited to moderately sized networks (< 6M
parameters). They require a fair amount of hyper-parameter tuning and are
computationally prohibitive for large networks like Vision Transformers (5M to
660M parameters). Obtaining certified robustness of transformers is not
feasible due to the non-scalability and inflexibility of the current methods.
This work presents CertViT, a two-step proximal-projection method to achieve
certified robustness from pre-trained weights. The proximal step tries to lower
the Lipschitz bound and the projection step tries to maintain the clean
accuracy of pre-trained weights. We show that CertViT networks have better
certified accuracy than state-of-the-art Lipschitz trained networks. We apply
CertViT on several variants of pre-trained vision transformers and show
adversarial robustness using standard attacks. Code :
https://github.com/sagarverma/transformer-lipschitz",None,-1
197b40a1-6bcf-4eeb-bdf0-04d932efa135,Dynamic Masking Rate Schedules for MLM Pretraining,0.0260911,"Most works on transformers trained with the Masked Language Modeling (MLM)
objective use the original BERT model's fixed masking rate of 15%. We propose
to instead dynamically schedule the masking rate throughout training. We find
that linearly decreasing the masking rate over the course of pretraining
improves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and
BERT-large, respectively, compared to fixed rate baselines. These gains come
from exposure to both high and low masking rate regimes, providing benefits
from both settings. Our results demonstrate that masking rate scheduling is a
simple way to improve the quality of masked language models, achieving up to a
1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for
BERT-large.",None,-1
08254823-1b2e-4c25-b001-d3302bb2683e,"System identification of neural systems: If we got it right, would we know?",0.671737,"Artificial neural networks are being proposed as models of parts of the
brain. The networks are compared to recordings of biological neurons, and good
performance in reproducing neural responses is considered to support the
model's validity. A key question is how much this system identification
approach tells us about brain computation. Does it validate one model
architecture over another? We evaluate the most commonly used comparison
techniques, such as a linear encoding model and centered kernel alignment, to
correctly identify a model by replacing brain recordings with known ground
truth models. System identification performance is quite variable; it also
depends significantly on factors independent of the ground truth architecture,
such as stimuli images. In addition, we show the limitations of using
functional similarity scores in identifying higher-level architectural motifs.",None,-1
8dfc02af-462a-4c30-9312-52fe41a93c39,Confidence-Aware and Self-Supervised Image Anomaly Localisation,0.178878,"Universal anomaly detection still remains a challenging problem in machine
learning and medical image analysis. It is possible to learn an expected
distribution from a single class of normative samples, e.g., through epistemic
uncertainty estimates, auto-encoding models, or from synthetic anomalies in a
self-supervised way. The performance of self-supervised anomaly detection
approaches is still inferior compared to methods that use examples from known
unknown classes to shape the decision boundary. However, outlier exposure
methods often do not identify unknown unknowns. Here we discuss an improved
self-supervised single-class training strategy that supports the approximation
of probabilistic inference with loosen feature locality constraints. We show
that up-scaling of gradients with histogram-equalised images is beneficial for
recently proposed self-supervision tasks. Our method is integrated into several
out-of-distribution (OOD) detection models and we show evidence that our method
outperforms the state-of-the-art on various benchmark datasets.",None,-1
3f08218f-b36f-4901-94b8-ee87b6a31724,AdamR at SemEval-2023 Task 10: Solving the Class Imbalance Problem in Sexism Detection with Ensemble Learning,0.0263663,"The Explainable Detection of Online Sexism task presents the problem of
explainable sexism detection through fine-grained categorisation of sexist
cases with three subtasks. Our team experimented with different ways to combat
class imbalance throughout the tasks using data augmentation and loss
alteration techniques. We tackled the challenge by utilising ensembles of
Transformer models trained on different datasets, which are tested to find the
balance between performance and interpretability. This solution ranked us in
the top 40\% of teams for each of the tracks.",None,-1
3ab7eb7e-6033-4b5f-9f6b-9fe2e9f7ec3c,Camoscio: an Italian Instruction-tuned LLaMA,0.277632,"In recent years Large Language Models (LLMs) have increased the state of the
art on several natural language processing tasks. However, their accessibility
is often limited to paid API services, posing challenges for researchers in
conducting extensive investigations. On the other hand, while some open-source
models have been proposed by the community, they are typically English-centric
or multilingual without a specific adaptation for the Italian language. In an
effort to democratize the available and open resources for the Italian
language, in this paper we introduce Camoscio: a language model specifically
tuned to follow users' prompts in Italian. Specifically, we finetuned the
smallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts
translated to Italian via ChatGPT. Results indicate that the model's zero-shot
performance on various downstream tasks in Italian competes favorably with
existing models specifically finetuned for those tasks. All the artifacts
(code, dataset, model) are released to the community at the following url:
https://github.com/teelinsan/camoscio",None,-1
7699b192-c2f5-46e9-8528-634d66fd7e40,Inpainting borehole images using Generative Adversarial Networks,0.0755199,"In this paper, we propose a GAN-based approach for gap filling in borehole
images created by wireline microresistivity imaging tools. The proposed method
utilizes a generator, global discriminator, and local discriminator to inpaint
the missing regions of the image. The generator is based on an auto-encoder
architecture with skip-connections, and the loss function used is the
Wasserstein GAN loss. Our experiments on a dataset of borehole images
demonstrate that the proposed model can effectively deal with large-scale
missing pixels and generate realistic completion results. This approach can
improve the quantitative evaluation of reservoirs and provide an essential
basis for interpreting geological phenomena and reservoir parameters.",None,-1
326481fb-fd35-4136-9682-81da4c9e4b6c,RLHF and IIA: Perverse Incentives,0.0648904,"Existing algorithms for reinforcement learning from human feedback (RLHF) can
incentivize responses at odds with preferences because they are based on models
that assume independence of irrelevant alternatives (IIA). The perverse
incentives induced by IIA hinder innovations on query formats and learning
algorithms.",None,-1
e09eb647-2a83-4db2-9ee0-9a32dd6a451d,ChatGPT and a New Academic Reality: Artificial Intelligence-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing,0.998579,"This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,
which uses natural language processing to fulfill text-based user requests
(i.e., a chatbot). The history and principles behind ChatGPT and similar models
are discussed. This technology is then discussed in relation to its potential
impact on academia and scholarly research and publishing. ChatGPT is seen as a
potential model for the automated preparation of essays and other types of
scholarly manuscripts. Potential ethical issues that could arise with the
emergence of large language models like GPT-3, the underlying technology behind
ChatGPT, and its usage by academics and researchers, are discussed and situated
within the context of broader advancements in artificial intelligence, machine
learning, and natural language processing for research and scholarly
publishing.",None,-1
83c06d41-c31a-4461-b563-4873c61eb9d0,Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization,0.65134,"Multi-document summarization (MDS) aims to generate a summary for a number of
related documents. We propose HGSUM, an MDS model that extends an
encoder-decoder architecture, to incorporate a heterogeneous graph to represent
different semantic units (e.g., words and sentences) of the documents. This
contrasts with existing MDS models which do not consider different edge types
of graphs and as such do not capture the diversity of relationships in the
documents. To preserve only key information and relationships of the documents
in the heterogeneous graph, HGSUM uses graph pooling to compress the input
graph. And to guide HGSUM to learn compression, we introduce an additional
objective that maximizes the similarity between the compressed graph and the
graph constructed from the ground-truth summary during training. HGSUM is
trained end-to-end with graph similarity and standard cross-entropy objectives.
Experimental results over MULTI-NEWS, WCEP-100, and ARXIV show that HGSUM
outperforms state-of-the-art MDS models. The code for our model and experiments
is available at: https://github.com/oaimli/HGSum.",None,-1
1915c290-7f33-424b-ae19-14c780500fc6,Planning as Theorem Proving with Heuristics,0.16237,"Planning as theorem proving in situation calculus was abandoned 50 years ago
as an impossible project. But we have developed a Theorem Proving Lifted
Heuristic (TPLH) planner that searches for a plan in a tree of situations using
the A* search algorithm. It is controlled by a delete relaxation-based domain
independent heuristic. We compare TPLH with Fast Downward (FD) and Best First
Width Search (BFWS) planners over several standard benchmarks. Since our
implementation of the heuristic function is not optimized, TPLH is slower than
FD and BFWS. But it computes shorter plans, and it explores fewer states. We
discuss previous research on planning within KR\&R and identify related
directions. Thus, we show that deductive lifted heuristic planning in situation
calculus is actually doable.",None,-1
ca89aa38-a53b-438e-b318-fcccb61064a3,Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making,0.292418,"Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and
patient data to offer real-time recommendations, with Large Language Models
(LLMs) emerging as a promising tool to generate plain-text explanations for
medical decisions. This study explores the effectiveness and reliability of
LLMs in generating explanations for diagnoses based on patient complaints.
Three experienced doctors evaluated LLM-generated explanations of the
connection between patient complaints and doctor and model-assigned diagnoses
across several stages. Experimental results demonstrated that LLM explanations
significantly increased doctors' agreement rates with given diagnoses and
highlighted potential errors in LLM outputs, ranging from 5% to 30%. The study
underscores the potential and challenges of LLMs in healthcare and emphasizes
the need for careful integration and evaluation to ensure patient safety and
optimal clinical utility.",None,-1
638f9bb1-0c44-4eae-bb47-4feb6bef9915,Ancient Chinese Word Segmentation and Part-of-Speech Tagging Using Distant Supervision,0.871603,"Ancient Chinese word segmentation (WSG) and part-of-speech tagging (POS) are
important to study ancient Chinese, but the amount of ancient Chinese WSG and
POS tagging data is still rare. In this paper, we propose a novel augmentation
method of ancient Chinese WSG and POS tagging data using distant supervision
over parallel corpus. However, there are still mislabeled and unlabeled ancient
Chinese words inevitably in distant supervision. To address this problem, we
take advantage of the memorization effects of deep neural networks and a small
amount of annotated data to get a model with much knowledge and a little noise,
and then we use this model to relabel the ancient Chinese sentences in parallel
corpus. Experiments show that the model trained over the relabeled data
outperforms the model trained over the data generated from distant supervision
and the annotated data. Our code is available at
https://github.com/farlit/ACDS.",None,-1
3dacf429-7269-4fe0-ad8b-f9705d335426,On the Foundations of Cycles in Bayesian Networks,0.207411,"Bayesian networks (BNs) are a probabilistic graphical model widely used for
representing expert knowledge and reasoning under uncertainty. Traditionally,
they are based on directed acyclic graphs that capture dependencies between
random variables. However, directed cycles can naturally arise when
cross-dependencies between random variables exist, e.g., for modeling feedback
loops. Existing methods to deal with such cross-dependencies usually rely on
reductions to BNs without cycles. These approaches are fragile to generalize,
since their justifications are intermingled with additional knowledge about the
application context. In this paper, we present a foundational study regarding
semantics for cyclic BNs that are generic and conservatively extend the
cycle-free setting. First, we propose constraint-based semantics that specify
requirements for full joint distributions over a BN to be consistent with the
local conditional probabilities and independencies. Second, two kinds of limit
semantics that formalize infinite unfolding approaches are introduced and shown
to be computable by a Markov chain construction.",None,-1
003e363d-6080-458c-8b15-a7fa16512524,LDM3D: Latent Diffusion Model for 3D,0.514319,"This research paper proposes a Latent Diffusion Model for 3D (LDM3D) that
generates both image and depth map data from a given text prompt, allowing
users to generate RGBD images from text prompts. The LDM3D model is fine-tuned
on a dataset of tuples containing an RGB image, depth map and caption, and
validated through extensive experiments. We also develop an application called
DepthFusion, which uses the generated RGB images and depth maps to create
immersive and interactive 360-degree-view experiences using TouchDesigner. This
technology has the potential to transform a wide range of industries, from
entertainment and gaming to architecture and design. Overall, this paper
presents a significant contribution to the field of generative AI and computer
vision, and showcases the potential of LDM3D and DepthFusion to revolutionize
content creation and digital experiences. A short video summarizing the
approach can be found at https://t.ly/tdi2.",None,-1
e747df7a-941c-40f9-97a9-51f41514fceb,Building Safe and Reliable AI systems for Safety Critical Tasks with Vision-Language Processing,0.043528,"Although AI systems have been applied in various fields and achieved
impressive performance, their safety and reliability are still a big concern.
This is especially important for safety-critical tasks. One shared
characteristic of these critical tasks is their risk sensitivity, where small
mistakes can cause big consequences and even endanger life. There are several
factors that could be guidelines for the successful deployment of AI systems in
sensitive tasks: (i) failure detection and out-of-distribution (OOD) detection;
(ii) overfitting identification; (iii) uncertainty quantification for
predictions; (iv) robustness to data perturbations. These factors are also
challenges of current AI systems, which are major blocks for building safe and
reliable AI. Specifically, the current AI algorithms are unable to identify
common causes for failure detection. Furthermore, additional techniques are
required to quantify the quality of predictions. All these contribute to
inaccurate uncertainty quantification, which lowers trust in predictions. Hence
obtaining accurate model uncertainty quantification and its further improvement
are challenging. To address these issues, many techniques have been proposed,
such as regularization methods and learning strategies. As vision and language
are the most typical data type and have many open source benchmark datasets,
this thesis will focus on vision-language data processing for tasks like
classification, image captioning, and vision question answering. In this
thesis, we aim to build a safeguard by further developing current techniques to
ensure the accurate model uncertainty for safety-critical tasks.",None,-1
bff8e1ce-da5d-46e4-9a24-110062cf2862,Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations,0.877148,"In this paper, we advocate for using large pre-trained monolingual language
models in cross lingual zero-shot word sense disambiguation (WSD) coupled with
a contextualized mapping mechanism. We also report rigorous experiments that
illustrate the effectiveness of employing sparse contextualized word
representations obtained via a dictionary learning procedure. Our experimental
results demonstrate that the above modifications yield a significant
improvement of nearly 6.5 points of increase in the average F-score (from 62.0
to 68.5) over a collection of 17 typologically diverse set of target languages.
We release our source code for replicating our experiments at
https://github.com/begab/sparsity_makes_sense.",None,-1
ea5e1474-c64f-4406-97c5-7af9ae12f202,LAMBO: Large Language Model Empowered Edge Intelligence,0.626333,"Next-generation edge intelligence is anticipated to bring huge benefits to
various applications, e.g., offloading systems. However, traditional deep
offloading architectures face several issues, including heterogeneous
constraints, partial perception, uncertain generalization, and lack of
tractability. In this context, the integration of offloading with large
language models (LLMs) presents numerous advantages. Therefore, we propose an
LLM-Based Offloading (LAMBO) framework for mobile edge computing (MEC), which
comprises four components: (i) Input embedding (IE), which is used to represent
the information of the offloading system with constraints and prompts through
learnable vectors with high quality; (ii) Asymmetric encoderdecoder (AED)
model, which is a decision-making module with a deep encoder and a shallow
decoder. It can achieve high performance based on multi-head self-attention
schemes; (iii) Actor-critic reinforcement learning (ACRL) module, which is
employed to pre-train the whole AED for different optimization tasks under
corresponding prompts; and (iv) Active learning from expert feedback (ALEF),
which can be used to finetune the decoder part of the AED while adapting to
dynamic environmental changes. Our simulation results corroborate the
advantages of the proposed LAMBO framework.",None,-1
84bf8053-b87d-4473-a8fb-5b3f68da6549,Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System,0.446333,"Developing an efficient retriever to retrieve knowledge from a large-scale
knowledge base (KB) is critical for task-oriented dialogue systems to
effectively handle localized and specialized tasks. However, widely used
generative models such as T5 and ChatGPT often struggle to differentiate subtle
differences among the retrieved KB records when generating responses, resulting
in suboptimal quality of generated responses. In this paper, we propose the
application of maximal marginal likelihood to train a perceptive retriever by
utilizing signals from response generation for supervision. In addition, our
approach goes beyond considering solely retrieved entities and incorporates
various meta knowledge to guide the generator, thus improving the utilization
of knowledge. We evaluate our approach on three task-oriented dialogue datasets
using T5 and ChatGPT as the backbone models. The results demonstrate that when
combined with meta knowledge, the response generator can effectively leverage
high-quality knowledge records from the retriever and enhance the quality of
generated responses. The codes and models of this paper are available at
https://github.com/shenwzh3/MK-TOD.",None,-1
3ddaf40a-6c52-424c-bf2f-5ffc99664a90,Class-Balancing Diffusion Models,0.185954,"Diffusion-based models have shown the merits of generating high-quality
visual data while preserving better diversity in recent studies. However, such
observation is only justified with curated data distribution, where the data
samples are nicely pre-processed to be uniformly distributed in terms of their
labels. In practice, a long-tailed data distribution appears more common and
how diffusion models perform on such class-imbalanced data remains unknown. In
this work, we first investigate this problem and observe significant
degradation in both diversity and fidelity when the diffusion model is trained
on datasets with class-imbalanced distributions. Especially in tail classes,
the generations largely lose diversity and we observe severe mode-collapse
issues. To tackle this problem, we set from the hypothesis that the data
distribution is not class-balanced, and propose Class-Balancing Diffusion
Models (CBDM) that are trained with a distribution adjustment regularizer as a
solution. Experiments show that images generated by CBDM exhibit higher
diversity and quality in both quantitative and qualitative ways. Our method
benchmarked the generation results on CIFAR100/CIFAR100LT dataset and shows
outstanding performance on the downstream recognition task.",None,-1
97b91f89-34ef-4bb4-bbf0-7e442f89245e,Frame Flexible Network,0.0854741,"Existing video recognition algorithms always conduct different training
pipelines for inputs with different frame numbers, which requires repetitive
training operations and multiplying storage costs. If we evaluate the model
using other frames which are not used in training, we observe the performance
will drop significantly (see Fig.1), which is summarized as Temporal Frequency
Deviation phenomenon. To fix this issue, we propose a general framework, named
Frame Flexible Network (FFN), which not only enables the model to be evaluated
at different frames to adjust its computation, but also reduces the memory
costs of storing multiple models significantly. Concretely, FFN integrates
several sets of training sequences, involves Multi-Frequency Alignment (MFAL)
to learn temporal frequency invariant representations, and leverages
Multi-Frequency Adaptation (MFAD) to further strengthen the representation
abilities. Comprehensive empirical validations using various architectures and
popular benchmarks solidly demonstrate the effectiveness and generalization of
FFN (e.g., 7.08/5.15/2.17% performance gain at Frame 4/8/16 on
Something-Something V1 dataset over Uniformer). Code is available at
https://github.com/BeSpontaneous/FFN.",None,-1
14370877-e00a-49b1-b92c-3b216c792a10,Automated clinical coding using off-the-shelf large language models,0.46545,"The task of assigning diagnostic ICD codes to patient hospital admissions is
typically performed by expert human coders. Efforts towards automated ICD
coding are dominated by supervised deep learning models. However, difficulties
in learning to predict the large number of rare codes remain a barrier to
adoption in clinical practice. In this work, we leverage off-the-shelf
pre-trained generative large language models (LLMs) to develop a practical
solution that is suitable for zero-shot and few-shot code assignment, with no
need for further task-specific training. Unsupervised pre-training alone does
not guarantee precise knowledge of the ICD ontology and specialist clinical
coding task, therefore we frame the task as information extraction, providing a
description of each coded concept and asking the model to retrieve related
mentions. For efficiency, rather than iterating over all codes, we leverage the
hierarchical nature of the ICD ontology to sparsely search for relevant codes.",None,-1
7b17ceea-b927-404c-b655-2da8e6303acd,Smart Infrastructure: A Research Junction,0.058309,"Complex inner-city junctions are among the most critical traffic areas for
injury and fatal accidents. The development of highly automated driving (HAD)
systems struggles with the complex and hectic everyday life within those areas.
Sensor-equipped smart infrastructures, which can communicate and cooperate with
vehicles, are essential to enable a holistic scene understanding to resolve
occlusions drivers and vehicle perception systems for themselves can not cover.
We introduce an intelligent research infrastructure equipped with visual sensor
technology, located at a public inner-city junction in Aschaffenburg, Germany.
A multiple-view camera system monitors the traffic situation to perceive road
users' behavior. Both motorized and non-motorized traffic is considered. The
system is used for research in data generation, evaluating new HAD sensors
systems, algorithms, and Artificial Intelligence (AI) training strategies using
real-, synthetic- and augmented data. In addition, the junction features a
highly accurate digital twin. Real-world data can be taken into the digital
twin for simulation purposes and synthetic data generation.",None,-1
0695110b-4093-484f-b87f-f6268e78781c,Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation,0.789349,"Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a ""Game-of-Thoughts"". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.",None,-1
928b186d-8b2c-47d0-a306-ff8931d3c9fc,Sparse GEMINI for Joint Discriminative Clustering and Feature Selection,0.103781,"Feature selection in clustering is a hard task which involves simultaneously
the discovery of relevant clusters as well as relevant variables with respect
to these clusters. While feature selection algorithms are often model-based
through optimised model selection or strong assumptions on $p(\pmb{x})$, we
introduce a discriminative clustering model trying to maximise a geometry-aware
generalisation of the mutual information called GEMINI with a simple $\ell_1$
penalty: the Sparse GEMINI. This algorithm avoids the burden of combinatorial
feature subset exploration and is easily scalable to high-dimensional data and
large amounts of samples while only designing a clustering model
$p_\theta(y|\pmb{x})$. We demonstrate the performances of Sparse GEMINI on
synthetic datasets as well as large-scale datasets. Our results show that
Sparse GEMINI is a competitive algorithm and has the ability to select relevant
subsets of variables with respect to the clustering without using relevance
criteria or prior hypotheses.",None,-1
310f5bd6-2f2a-4eb7-92a8-fde964f73a2a,Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,0.749927,"Although large language models (LLMs) are often pre-trained on large-scale
multilingual texts, their reasoning abilities and real-world knowledge are
mainly evaluated based on English datasets. Assessing LLM capabilities beyond
English is increasingly vital but hindered due to the lack of suitable
datasets. In this work, we introduce IndoMMLU, the first multi-task language
understanding benchmark for Indonesian culture and languages, which consists of
questions from primary school to university entrance exams in Indonesia. By
employing professional teachers, we obtain 14,981 questions across 64 tasks and
education levels, with 46% of the questions focusing on assessing proficiency
in the Indonesian language and knowledge of nine local languages and cultures
in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass
the Indonesian primary school level, with limited knowledge of local Indonesian
languages and culture. Other smaller models such as BLOOMZ and Falcon perform
at even lower levels.",None,-1
ff2e8939-8cc0-4e45-8bfc-e205229ebdf5,Nearest Neighbors Meet Deep Neural Networks for Point Cloud Analysis,0.829318,"Performances on standard 3D point cloud benchmarks have plateaued, resulting
in oversized models and complex network design to make a fractional
improvement. We present an alternative to enhance existing deep neural networks
without any redesigning or extra parameters, termed as Spatial-Neighbor Adapter
(SN-Adapter). Building on any trained 3D network, we utilize its learned
encoding capability to extract features of the training dataset and summarize
them as prototypical spatial knowledge. For a test point cloud, the SN-Adapter
retrieves k nearest neighbors (k-NN) from the pre-constructed spatial
prototypes and linearly interpolates the k-NN prediction with that of the
original 3D network. By providing complementary characteristics, the proposed
SN-Adapter serves as a plug-and-play module to economically improve performance
in a non-parametric manner. More importantly, our SN-Adapter can be effectively
generalized to various 3D tasks, including shape classification, part
segmentation, and 3D object detection, demonstrating its superiority and
robustness. We hope our approach could show a new perspective for point cloud
analysis and facilitate future research.",None,-1
efe0e27a-2dd4-481e-b54a-b57b2a42c288,A Unified One-Step Solution for Aspect Sentiment Quad Prediction,0.29926,"Aspect sentiment quad prediction (ASQP) is a challenging yet significant
subtask in aspect-based sentiment analysis as it provides a complete
aspect-level sentiment structure. However, existing ASQP datasets are usually
small and low-density, hindering technical advancement. To expand the capacity,
in this paper, we release two new datasets for ASQP, which contain the
following characteristics: larger size, more words per sample, and higher
density. With such datasets, we unveil the shortcomings of existing strong ASQP
baselines and therefore propose a unified one-step solution for ASQP, namely
One-ASQP, to detect the aspect categories and to identify the
aspect-opinion-sentiment (AOS) triplets simultaneously. Our One-ASQP holds
several unique advantages: (1) by separating ASQP into two subtasks and solving
them independently and simultaneously, we can avoid error propagation in
pipeline-based methods and overcome slow training and inference in
generation-based methods; (2) by introducing sentiment-specific horns tagging
schema in a token-pair-based two-dimensional matrix, we can exploit deeper
interactions between sentiment elements and efficiently decode the AOS
triplets; (3) we design ``[NULL]'' token can help us effectively identify the
implicit aspects or opinions. Experiments on two benchmark datasets and our
released two datasets demonstrate the advantages of our One-ASQP. The two new
datasets are publicly released at
\url{https://www.github.com/Datastory-CN/ASQP-Datasets}.",None,-1
24b4fab7-84d2-4e22-9d2c-38d0eb377e6b,Text-to-Image Diffusion Models are Zero-Shot Classifiers,0.60156,"The excellent generative capabilities of text-to-image diffusion models
suggest they learn informative representations of image-text data. However,
what knowledge their representations capture is not fully understood, and they
have not been thoroughly explored on downstream tasks. We investigate diffusion
models by proposing a method for evaluating them as zero-shot classifiers. The
key idea is using a diffusion model's ability to denoise a noised image given a
text description of a label as a proxy for that label's likelihood. We apply
our method to Stable Diffusion and Imagen, using it to probe fine-grained
aspects of the models' knowledge and comparing them with CLIP's zero-shot
abilities. They perform competitively with CLIP on a wide range of zero-shot
image classification datasets. Additionally, they achieve state-of-the-art
results on shape/texture bias tests and can successfully perform attribute
binding while CLIP cannot. Although generative pre-training is prevalent in
NLP, visual foundation models often use other methods such as contrastive
learning. Based on our findings, we argue that generative pre-training should
be explored as a compelling alternative for vision-language tasks.",None,-1
456b8cc6-a1a1-48b9-805c-09b9471f699a,Comparison of L2 Korean pronunciation error patterns from five L1 backgrounds by using automatic phonetic transcription,0.461279,"This paper presents a large-scale analysis of L2 Korean pronunciation error
patterns from five different language backgrounds, Chinese, Vietnamese,
Japanese, Thai, and English, by using automatic phonetic transcription. For the
analysis, confusion matrices are generated for each L1, by aligning canonical
phone sequences and automatically transcribed phone sequences obtained from
fine-tuned Wav2Vec2 XLS-R phone recognizer. Each value in the confusion
matrices is compared to capture frequent common error patterns and to specify
patterns unique to a certain language background. Using the Foreign Speakers'
Voice Data of Korean for Artificial Intelligence Learning dataset, common error
pattern types are found to be (1) substitutions of aspirated or tense
consonants with plain consonants, (2) deletions of syllable-final consonants,
and (3) substitutions of diphthongs with monophthongs. On the other hand,
thirty-nine patterns including (1) syllable-final /l/ substitutions with /n/
for Vietnamese and (2) /\textturnm/ insertions for Japanese are discovered as
language-dependent.",None,-1
e3eedce9-4b77-4cc1-9adf-fa4ff280fcbd,Generative Semantic Communication: Diffusion Models Beyond Bit Recovery,0.993733,"Semantic communication is expected to be one of the cores of next-generation
AI-based communications. One of the possibilities offered by semantic
communication is the capability to regenerate, at the destination side, images
or videos semantically equivalent to the transmitted ones, without necessarily
recovering the transmitted sequence of bits. The current solutions still lack
the ability to build complex scenes from the received partial information.
Clearly, there is an unmet need to balance the effectiveness of generation
methods and the complexity of the transmitted information, possibly taking into
account the goal of communication. In this paper, we aim to bridge this gap by
proposing a novel generative diffusion-guided framework for semantic
communication that leverages the strong abilities of diffusion models in
synthesizing multimedia content while preserving semantic features. We reduce
bandwidth usage by sending highly-compressed semantic information only. Then,
the diffusion model learns to synthesize semantic-consistent scenes through
spatially-adaptive normalizations from such denoised semantic information. We
prove, through an in-depth assessment of multiple scenarios, that our method
outperforms existing solutions in generating high-quality images with preserved
semantic information even in cases where the received content is significantly
degraded. More specifically, our results show that objects, locations, and
depths are still recognizable even in the presence of extremely noisy
conditions of the communication channel. The code is available at
https://github.com/ispamm/GESCO.",None,-1
a130ec62-61f8-43b0-94b8-073b18d16ebd,A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations,0.9969,"Universality is a key hypothesis in mechanistic interpretability -- that
different models learn similar features and circuits when trained on similar
tasks. In this work, we study the universality hypothesis by examining how
small neural networks learn to implement group composition. We present a novel
algorithm by which neural networks may implement composition for any finite
group via mathematical representation theory. We then show that networks
consistently learn this algorithm by reverse engineering model logits and
weights, and confirm our understanding using ablations. By studying networks of
differing architectures trained on various groups, we find mixed evidence for
universality: using our algorithm, we can completely characterize the family of
circuits and features that networks learn on this task, but for a given network
the precise circuits learned -- as well as the order they develop -- are
arbitrary.",None,-1
3583b112-e251-402d-ab1f-ddea03023e08,Recommending the optimal policy by learning to act from temporal data,0.168808,"Prescriptive Process Monitoring is a prominent problem in Process Mining,
which consists in identifying a set of actions to be recommended with the goal
of optimising a target measure of interest or Key Performance Indicator (KPI).
One challenge that makes this problem difficult is the need to provide
Prescriptive Process Monitoring techniques only based on temporally annotated
(process) execution data, stored in, so-called execution logs, due to the lack
of well crafted and human validated explicit models. In this paper we aim at
proposing an AI based approach that learns, by means of Reinforcement Learning
(RL), an optimal policy (almost) only from the observation of past executions
and recommends the best activities to carry on for optimizing a KPI of
interest. This is achieved first by learning a Markov Decision Process for the
specific KPIs from data, and then by using RL training to learn the optimal
policy. The approach is validated on real and synthetic datasets and compared
with off-policy Deep RL approaches. The ability of our approach to compare
with, and often overcome, Deep RL approaches provides a contribution towards
the exploitation of white box RL techniques in scenarios where only temporal
execution data are available.",None,-1
36bcd6d9-7fab-46de-a142-c1b73cdf0bf3,Remind of the Past: Incremental Learning with Analogical Prompts,0.107997,"Although data-free incremental learning methods are memory-friendly,
accurately estimating and counteracting representation shifts is challenging in
the absence of historical data. This paper addresses this thorny problem by
proposing a novel incremental learning method inspired by human analogy
capabilities. Specifically, we design an analogy-making mechanism to remap the
new data into the old class by prompt tuning. It mimics the feature
distribution of the target old class on the old model using only samples of new
classes. The learnt prompts are further used to estimate and counteract the
representation shift caused by fine-tuning for the historical prototypes. The
proposed method sets up new state-of-the-art performance on four incremental
learning benchmarks under both the class and domain incremental learning
settings. It consistently outperforms data-replay methods by only saving
feature prototypes for each class. It has almost hit the empirical upper bound
by joint training on the Core50 benchmark. The code will be released at
\url{https://github.com/ZhihengCV/A-Prompts}.",None,-1
4ffafd04-3947-4eb9-83eb-7a9ae6327abc,Assessing the potential of AI-assisted pragmatic annotation: The case of apologies,0.882681,"Certain forms of linguistic annotation, like part of speech and semantic
tagging, can be automated with high accuracy. However, manual annotation is
still necessary for complex pragmatic and discursive features that lack a
direct mapping to lexical forms. This manual process is time-consuming and
error-prone, limiting the scalability of function-to-form approaches in corpus
linguistics. To address this, our study explores automating pragma-discursive
corpus annotation using large language models (LLMs). We compare ChatGPT, the
Bing chatbot, and a human coder in annotating apology components in English
based on the local grammar framework. We find that the Bing chatbot
outperformed ChatGPT, with accuracy approaching that of a human coder. These
results suggest that AI can be successfully deployed to aid pragma-discursive
corpus annotation, making the process more efficient and scalable. Keywords:
linguistic annotation, function-to-form approaches, large language models,
local grammar analysis, Bing chatbot, ChatGPT",None,-1
3ed0176e-ba29-4a80-abee-c37ec018ee9d,Learning Roles with Emergent Social Value Orientations,0.34892,"Social dilemmas can be considered situations where individual rationality
leads to collective irrationality. The multi-agent reinforcement learning
community has leveraged ideas from social science, such as social value
orientations (SVO), to solve social dilemmas in complex cooperative tasks. In
this paper, by first introducing the typical ""division of labor or roles""
mechanism in human society, we provide a promising solution for intertemporal
social dilemmas (ISD) with SVOs. A novel learning framework, called Learning
Roles with Emergent SVOs (RESVO), is proposed to transform the learning of
roles into the social value orientation emergence, which is symmetrically
solved by endowing agents with altruism to share rewards with other agents. An
SVO-based role embedding space is then constructed by individual conditioning
policies on roles with a novel rank regularizer and mutual information
maximizer. Experiments show that RESVO achieves a stable division of labor and
cooperation in ISDs with different complexity.",None,-1
a547e9b1-ce4b-493c-9683-70d13a955f12,ArcGPT: A Large Language Model Tailored for Real-world Archival Applications,0.378363,"Archives play a crucial role in preserving information and knowledge, and the
exponential growth of such data necessitates efficient and automated tools for
managing and utilizing archive information resources. Archival applications
involve managing massive data that are challenging to process and analyze.
Although LLMs have made remarkable progress in diverse domains, there are no
publicly available archives tailored LLM. Addressing this gap, we introduce
ArcGPT, to our knowledge, the first general-purpose LLM tailored to the
archival field. To enhance model performance on real-world archival tasks,
ArcGPT has been pre-trained on massive and extensive archival domain data.
Alongside ArcGPT, we release AMBLE, a benchmark comprising four real-world
archival tasks. Evaluation on AMBLE shows that ArcGPT outperforms existing
state-of-the-art models, marking a substantial step forward in effective
archival data management. Ultimately, ArcGPT aims to better serve the archival
community, aiding archivists in their crucial role of preserving and harnessing
our collective information and knowledge.",None,-1
c545c74b-bebc-4ed3-8431-ef50f35558a2,Characterizing Nexus of Similarity within Knowledge Bases: A Logic-based Framework and its Computational Complexity Aspects,0.151157,"Similarities between entities occur frequently in many real-world scenarios.
For over a century, researchers in different fields have proposed a range of
approaches to measure the similarity between entities. More recently, inspired
by ""Google Sets"", significant academic and commercial efforts have been devoted
to expanding a given set of entities with similar ones. As a result, existing
approaches nowadays are able to take into account properties shared by
entities, hereinafter called nexus of similarity. Accordingly, machines are
largely able to deal with both similarity measures and set expansions. To the
best of our knowledge, however, there is no way to characterize nexus of
similarity between entities, namely identifying such nexus in a formal and
comprehensive way so that they are both machine- and human-readable; moreover,
there is a lack of consensus on evaluating existing approaches for weakly
similar entities. As a first step towards filling these gaps, we aim to
complement existing literature by developing a novel logic-based framework to
formally and automatically characterize nexus of similarity between tuples of
entities within a knowledge base. Furthermore, we analyze computational
complexity aspects of this framework.",None,-1
842fafde-55bd-42b6-9b83-ad60e1afa099,AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression,0.554183,"Knowledge distillation has attracted a great deal of interest recently to
compress pre-trained language models. However, existing knowledge distillation
methods suffer from two limitations. First, the student model simply imitates
the teacher's behavior while ignoring the underlying reasoning. Second, these
methods usually focus on the transfer of sophisticated model-specific knowledge
but overlook data-specific knowledge. In this paper, we present a novel
attribution-driven knowledge distillation approach, which explores the
token-level rationale behind the teacher model based on Integrated Gradients
(IG) and transfers attribution knowledge to the student model. To enhance the
knowledge transfer of model reasoning and generalization, we further explore
multi-view attribution distillation on all potential decisions of the teacher.
Comprehensive experiments are conducted with BERT on the GLUE benchmark. The
experimental results demonstrate the superior performance of our approach to
several state-of-the-art methods.",None,-1
834d45ca-f0df-418c-a22c-89cf54a24083,Explain-then-Translate: An Analysis on Improving Program Translation with Self-generated Explanations,0.646464,"This work explores the use of self-generated natural language explanations as
an intermediate step for code-to-code translation with language models. Across
three types of explanations and 19 programming languages constructed from the
MultiPL-E dataset, we find the explanations to be particularly effective in the
zero-shot case, improving performance by 12% on average. Improvements with
natural language explanations are particularly pronounced on difficult
programs. We release our dataset, code, and canonical solutions in all 19
languages.",None,-1
8bb984d7-841e-44bd-9ec9-09da24bd561a,Explanations for Automatic Speech Recognition,0.468319,"We address quality assessment for neural network based ASR by providing
explanations that help increase our understanding of the system and ultimately
help build trust in the system. Compared to simple classification labels,
explaining transcriptions is more challenging as judging their correctness is
not straightforward and transcriptions as a variable-length sequence is not
handled by existing interpretable machine learning models. We provide an
explanation for an ASR transcription as a subset of audio frames that is both a
minimal and sufficient cause of the transcription. To do this, we adapt
existing explainable AI (XAI) techniques from image classification-Statistical
Fault Localisation(SFL) and Causal. Additionally, we use an adapted version of
Local Interpretable Model-Agnostic Explanations (LIME) for ASR as a baseline in
our experiments. We evaluate the quality of the explanations generated by the
proposed techniques over three different ASR ,Google API, the baseline model of
Sphinx, Deepspeech and 100 audio samples from the Commonvoice dataset.",None,-1
35be039a-2560-4016-a2a6-09e9a14b3855,Can Contextual Biasing Remain Effective with Whisper and GPT-2?,0.848173,"End-to-end automatic speech recognition (ASR) and large language models, such
as Whisper and GPT-2, have recently been scaled to use vast amounts of training
data. Despite the large amount of training data, infrequent content words that
occur in a particular task may still exhibit poor ASR performance, with
contextual biasing a possible remedy. This paper investigates the effectiveness
of neural contextual biasing for Whisper combined with GPT-2. Specifically,
this paper proposes integrating an adapted tree-constrained pointer generator
(TCPGen) component for Whisper and a dedicated training scheme to dynamically
adjust the final output without modifying any Whisper model parameters.
Experiments across three datasets show a considerable reduction in errors on
biasing words with a biasing list of 1000 words. Contextual biasing was more
effective when applied to domain-specific data and can boost the performance of
Whisper and GPT-2 without losing their generality.",None,-1
b4dd8bb5-95fc-4f52-90c6-244e2fe83ece,GeNAS: Neural Architecture Search with Better Generalization,0.187187,"Neural Architecture Search (NAS) aims to automatically excavate the optimal
network architecture with superior test performance. Recent neural architecture
search (NAS) approaches rely on validation loss or accuracy to find the
superior network for the target data. In this paper, we investigate a new
neural architecture search measure for excavating architectures with better
generalization. We demonstrate that the flatness of the loss surface can be a
promising proxy for predicting the generalization capability of neural network
architectures. We evaluate our proposed method on various search spaces,
showing similar or even better performance compared to the state-of-the-art NAS
methods. Notably, the resultant architecture found by flatness measure
generalizes robustly to various shifts in data distribution (e.g.
ImageNet-V2,-A,-O), as well as various tasks such as object detection and
semantic segmentation. Code is available at https://github.com/clovaai/GeNAS.",None,-1
0ba64f9d-552d-47a4-8e8f-bc5807e427de,eCDANs: Efficient Temporal Causal Discovery from Autocorrelated and Non-stationary Data (Student Abstract),0.328659,"Conventional temporal causal discovery (CD) methods suffer from high
dimensionality, fail to identify lagged causal relationships, and often ignore
dynamics in relations. In this study, we present a novel constraint-based CD
approach for autocorrelated and non-stationary time series data (eCDANs)
capable of detecting lagged and contemporaneous causal relationships along with
temporal changes. eCDANs addresses high dimensionality by optimizing the
conditioning sets while conducting conditional independence (CI) tests and
identifies the changes in causal relations by introducing a surrogate variable
to represent time dependency. Experiments on synthetic and real-world data show
that eCDANs can identify time influence and outperform the baselines.",None,-1
4b1058a2-286c-4da0-b62b-8c3e2ef70230,Characterising Decision Theories with Mechanised Causal Graphs,0.148999,"How should my own decisions affect my beliefs about the outcomes I expect to
achieve? If taking a certain action makes me view myself as a certain type of
person, it might affect how I think others view me, and how I view others who
are similar to me. This can influence my expected utility calculations and
change which action I perceive to be best. Whether and how it should is subject
to debate, with contenders for how to think about it including evidential
decision theory, causal decision theory, and functional decision theory. In
this paper, we show that mechanised causal models can be used to characterise
and differentiate the most important decision theories, and generate a taxonomy
of different decision theories.",None,-1
5a003707-1643-4105-ac96-50586b9f282f,ComputeGPT: A computational chat model for numerical problems,0.120248,"Language models are not accurate in numerical problems. Their architecture
does not allow for anything less than a probabilistic next word. This paper
introduces ComputeGPT: an approach of creating a chat model able to answer
computational problems through running on-demand code. ComputeGPT converts each
question to relevant code, runs the code, and returns the computed answer as
part of the chat. We combine this approach with a local browser-based Python
interpretation and fine-tuned prompts in order to achieve state-of-the-art
efficiency on numerical problems and provide a suitable front-end and safe
environment for the code to be executed in.",None,-1
9392bd2e-6c8a-4ff7-a513-d5e101275e7f,EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression,0.0823509,"We propose an unsupervised method to extract keywords and keyphrases from
texts based on a pre-trained language model (LM) and Shannon's information
maximization. Specifically, our method extracts phrases having the highest
conditional entropy under the LM. The resulting set of keyphrases turns out to
solve a relevant information-theoretic problem: if provided as side
information, it leads to the expected minimal binary code length in compressing
the text using the LM and an entropy encoder. Alternately, the resulting set is
an approximation via a causal LM to the set of phrases that minimize the
entropy of the text when conditioned upon it. Empirically, the method provides
results comparable to the most commonly used methods in various keyphrase
extraction benchmark challenges.",None,-1
6e168503-7011-4b19-93e5-70057d180909,Controllable Data Augmentation for Few-Shot Text Mining with Chain-of-Thought Attribute Manipulation,0.269974,"Prompting large language models (LLMs) for data augmentation has recently
become a common practice in few-shot NLP tasks. In this paper, we propose
Chain-of-Thought Attribute Manipulation (CoTAM), a novel approach that
generates new data from existing examples by only tweaking in the
user-provided, task-specific attribute, e.g., sentiment polarity or topic in
movie reviews. Instead of conventional latent representation controlling, we
leverage the chain-of-thought prompting to directly edit the text in three
steps, (1) attribute decomposition, (2) manipulation proposal, and (3) sentence
reconstruction. Extensive results on various tasks, such as text (pair)
classification, aspect-based sentiment analysis, and conditional text
generation, verify the superiority of CoTAM over other LLM-based augmentation
methods with the same number of training examples for both fine-tuning and
in-context learning. Remarkably, the 2D visualization of the augmented dataset
using principal component analysis revealed a human-recognizable decision
boundary that is likely hinted by the attribute manipulation, demonstrating the
potential of our proposed approach.",None,-1
5edf758b-48aa-422b-afa5-b8926de15324,Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation,0.963985,"The ability to collect a large dataset of human preferences from
text-to-image users is usually limited to companies, making such datasets
inaccessible to the public. To address this issue, we create a web app that
enables text-to-image users to generate images and specify their preferences.
Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image
prompts and real users' preferences over generated images. We leverage this
dataset to train a CLIP-based scoring function, PickScore, which exhibits
superhuman performance on the task of predicting human preferences. Then, we
test PickScore's ability to perform model evaluation and observe that it
correlates better with human rankings than other automatic evaluation metrics.
Therefore, we recommend using PickScore for evaluating future text-to-image
generation models, and using Pick-a-Pic prompts as a more relevant dataset than
MS-COCO. Finally, we demonstrate how PickScore can enhance existing
text-to-image models via ranking.",None,-1
fee2d9bf-d5c6-421a-b0ec-cd38a7f684ee,Self-supervised Multi-view Disentanglement for Expansion of Visual Collections,0.283784,"Image search engines enable the retrieval of images relevant to a query
image. In this work, we consider the setting where a query for similar images
is derived from a collection of images. For visual search, the similarity
measurements may be made along multiple axes, or views, such as style and
color. We assume access to a set of feature extractors, each of which computes
representations for a specific view. Our objective is to design a retrieval
algorithm that effectively combines similarities computed over representations
from multiple views. To this end, we propose a self-supervised learning method
for extracting disentangled view-specific representations for images such that
the inter-view overlap is minimized. We show how this allows us to compute the
intent of a collection as a distribution over views. We show how effective
retrieval can be performed by prioritizing candidate expansion images that
match the intent of a query collection. Finally, we present a new querying
mechanism for image search enabled by composing multiple collections and
perform retrieval under this setting using the techniques presented in this
paper.",None,-1
4f45a587-6819-4363-8eeb-9d193b769ffd,HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining,0.734559,"Human-centric perceptions include a variety of vision tasks, which have
widespread industrial applications, including surveillance, autonomous driving,
and the metaverse. It is desirable to have a general pretrain model for
versatile human-centric downstream tasks. This paper forges ahead along this
path from the aspects of both benchmark and pretraining methods. Specifically,
we propose a \textbf{HumanBench} based on existing datasets to comprehensively
evaluate on the common ground the generalization abilities of different
pretraining methods on 19 datasets from 6 diverse downstream tasks, including
person ReID, pose estimation, human parsing, pedestrian attribute recognition,
pedestrian detection, and crowd counting. To learn both coarse-grained and
fine-grained knowledge in human bodies, we further propose a \textbf{P}rojector
\textbf{A}ssis\textbf{T}ed \textbf{H}ierarchical pretraining method
(\textbf{PATH}) to learn diverse knowledge at different granularity levels.
Comprehensive evaluations on HumanBench show that our PATH achieves new
state-of-the-art results on 17 downstream datasets and on-par results on the
other 2 datasets. The code will be publicly at
\href{https://github.com/OpenGVLab/HumanBench}{https://github.com/OpenGVLab/HumanBench}.",None,-1
62810077-f469-4c86-b5b6-fd0a1cbe357e,"The logic behind desirable sets of things, and its filter representation",0.116113,"We identify the (filter representation of the) logic behind the recent theory
of coherent sets of desirable (sets of) things, which generalise coherent sets
of desirable (sets of) gambles as well as coherent choice functions, and show
that this identification allows us to establish various representation results
for such coherent models in terms of simpler ones.",None,-1
abe9bd59-352e-4cd0-927a-9210acf53ddb,Neural Microfacet Fields for Inverse Rendering,0.344812,"We present Neural Microfacet Fields, a method for recovering materials,
geometry, and environment illumination from images of a scene. Our method uses
a microfacet reflectance model within a volumetric setting by treating each
sample along the ray as a (potentially non-opaque) surface. Using surface-based
Monte Carlo rendering in a volumetric setting enables our method to perform
inverse rendering efficiently by combining decades of research in surface-based
light transport with recent advances in volume rendering for view synthesis.
Our approach outperforms prior work in inverse rendering, capturing high
fidelity geometry and high frequency illumination details; its novel view
synthesis results are on par with state-of-the-art methods that do not recover
illumination or materials.",None,-1
f97763b6-f5ad-4971-bc7f-73c7ceedefc3,Rotation-Scale Equivariant Steerable Filters,0.213176,"Incorporating either rotation equivariance or scale equivariance into CNNs
has proved to be effective in improving models' generalization performance.
However, jointly integrating rotation and scale equivariance into CNNs has not
been widely explored. Digital histology imaging of biopsy tissue can be
captured at arbitrary orientation and magnification and stored at different
resolutions, resulting in cells appearing in different scales. When
conventional CNNs are applied to histopathology image analysis, the
generalization performance of models is limited because 1) a part of the
parameters of filters are trained to fit rotation transformation, thus
decreasing the capability of learning other discriminative features; 2)
fixed-size filters trained on images at a given scale fail to generalize to
those at different scales. To deal with these issues, we propose the
Rotation-Scale Equivariant Steerable Filter (RSESF), which incorporates
steerable filters and scale-space theory. The RSESF contains copies of filters
that are linear combinations of Gaussian filters, whose direction is controlled
by directional derivatives and whose scale parameters are trainable but
constrained to span disjoint scales in successive layers of the network.
Extensive experiments on two gland segmentation datasets demonstrate that our
method outperforms other approaches, with much fewer trainable parameters and
fewer GPU resources required. The source code is available at:
https://github.com/ynulonger/RSESF.",None,-1
f2e674f8-57fb-4c60-b678-c4d6003967dd,LLM-augmented Preference Learning from Natural Language,0.0864414,"Finding preferences expressed in natural language is an important but
challenging task. State-of-the-art(SotA) methods leverage transformer-based
models such as BERT, RoBERTa, etc. and graph neural architectures such as graph
attention networks. Since Large Language Models (LLMs) are equipped to deal
with larger context lengths and have much larger model sizes than the
transformer-based model, we investigate their ability to classify comparative
text directly. This work aims to serve as a first step towards using LLMs for
the CPC task. We design and conduct a set of experiments that format the
classification task into an input prompt for the LLM and a methodology to get a
fixed-format response that can be automatically evaluated. Comparing
performances with existing methods, we see that pre-trained LLMs are able to
outperform the previous SotA models with no fine-tuning involved. Our results
show that the LLMs can consistently outperform the SotA when the target text is
large -- i.e. composed of multiple sentences --, and are still comparable to
the SotA performance in shorter text. We also find that few-shot learning
yields better performance than zero-shot learning.",None,-1
9fb440ba-072d-4711-9692-0d8be80bb835,Expert Uncertainty and Severity Aware Chest X-Ray Classification by Multi-Relationship Graph Learning,0.548094,"Patients undergoing chest X-rays (CXR) often endure multiple lung diseases.
When evaluating a patient's condition, due to the complex pathologies, subtle
texture changes of different lung lesions in images, and patient condition
differences, radiologists may make uncertain even when they have experienced
long-term clinical training and professional guidance, which makes much noise
in extracting disease labels based on CXR reports. In this paper, we re-extract
disease labels from CXR reports to make them more realistic by considering
disease severity and uncertainty in classification. Our contributions are as
follows: 1. We re-extracted the disease labels with severity and uncertainty by
a rule-based approach with keywords discussed with clinical experts. 2. To
further improve the explainability of chest X-ray diagnosis, we designed a
multi-relationship graph learning method with an expert uncertainty-aware loss
function. 3. Our multi-relationship graph learning method can also interpret
the disease classification results. Our experimental results show that models
considering disease severity and uncertainty outperform previous
state-of-the-art methods.",None,-1
6f3b8128-95cd-4edf-b599-fead9f0ac3d6,SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation,0.54981,"Recent advances in deep learning-based medical image segmentation studies
achieve nearly human-level performance in fully supervised manner. However,
acquiring pixel-level expert annotations is extremely expensive and laborious
in medical imaging fields. Unsupervised domain adaptation (UDA) can alleviate
this problem, which makes it possible to use annotated data in one imaging
modality to train a network that can successfully perform segmentation on
target imaging modality with no labels. In this work, we propose SDC-UDA, a
simple yet effective volumetric UDA framework for slice-direction continuous
cross-modality medical image segmentation which combines intra- and inter-slice
self-attentive image translation, uncertainty-constrained pseudo-label
refinement, and volumetric self-training. Our method is distinguished from
previous methods on UDA for medical image segmentation in that it can obtain
continuous segmentation in the slice direction, thereby ensuring higher
accuracy and potential in clinical practice. We validate SDC-UDA with multiple
publicly available cross-modality medical image segmentation datasets and
achieve state-of-the-art segmentation performance, not to mention the superior
slice-direction continuity of prediction compared to previous studies.",None,-1
cba5e1bd-d525-4272-ad16-f239c53cbbdd,Adaptive Data Augmentation for Contrastive Learning,0.170262,"In computer vision, contrastive learning is the most advanced unsupervised
learning framework. Yet most previous methods simply apply fixed composition of
data augmentations to improve data efficiency, which ignores the changes in
their optimal settings over training. Thus, the pre-determined parameters of
augmentation operations cannot always fit well with an evolving network during
the whole training period, which degrades the quality of the learned
representations. In this work, we propose AdDA, which implements a closed-loop
feedback structure to a generic contrastive learning network. AdDA works by
allowing the network to adaptively adjust the augmentation compositions
according to the real-time feedback. This online adjustment helps maintain the
dynamic optimal composition and enables the network to acquire more
generalizable representations with minimal computational overhead. AdDA
achieves competitive results under the common linear protocol on ImageNet-100
classification (+1.11% on MoCo v2).",None,-1
062eccfa-bcac-462f-ab6a-cf5ff0eebff1,Tab2KG: Semantic Table Interpretation with Lightweight Semantic Profiles,0.447764,"Tabular data plays an essential role in many data analytics and machine
learning tasks. Typically, tabular data does not possess any machine-readable
semantics. In this context, semantic table interpretation is crucial for making
data analytics workflows more robust and explainable. This article proposes
Tab2KG - a novel method that targets at the interpretation of tables with
previously unseen data and automatically infers their semantics to transform
them into semantic data graphs. We introduce original lightweight semantic
profiles that enrich a domain ontology's concepts and relations and represent
domain and table characteristics. We propose a one-shot learning approach that
relies on these profiles to map a tabular dataset containing previously unseen
instances to a domain ontology. In contrast to the existing semantic table
interpretation approaches, Tab2KG relies on the semantic profiles only and does
not require any instance lookup. This property makes Tab2KG particularly
suitable in the data analytics context, in which data tables typically contain
new instances. Our experimental evaluation on several real-world datasets from
different application domains demonstrates that Tab2KG outperforms
state-of-the-art semantic table interpretation baselines.",None,-1
22047407-f8ff-4332-8076-cbc78b0216e8,Towards Open-Domain Topic Classification,0.669927,"We introduce an open-domain topic classification system that accepts
user-defined taxonomy in real time. Users will be able to classify a text
snippet with respect to any candidate labels they want, and get instant
response from our web interface. To obtain such flexibility, we build the
backend model in a zero-shot way. By training on a new dataset constructed from
Wikipedia, our label-aware text classifier can effectively utilize implicit
knowledge in the pretrained language model to handle labels it has never seen
before. We evaluate our model across four datasets from various domains with
different label sets. Experiments show that the model significantly improves
over existing zero-shot baselines in open-domain scenarios, and performs
competitively with weakly-supervised models trained on in-domain data.",None,-1
280f9192-c370-4c39-8808-41f07f7578b9,Accurate prediction of international trade flows: Leveraging knowledge graphs and their embeddings,0.976039,"Knowledge representation (KR) is vital in designing symbolic notations to
represent real-world facts and facilitate automated decision-making tasks.
Knowledge graphs (KGs) have emerged so far as a popular form of KR, offering a
contextual and human-like representation of knowledge. In international
economics, KGs have proven valuable in capturing complex interactions between
commodities, companies, and countries. By putting the gravity model, which is a
common economic framework, into the process of building KGs, important factors
that affect trade relationships can be taken into account, making it possible
to predict international trade patterns. This paper proposes an approach that
leverages Knowledge Graph embeddings for modeling international trade, focusing
on link prediction using embeddings. Thus, valuable insights are offered to
policymakers, businesses, and economists, enabling them to anticipate the
effects of changes in the international trade system. Moreover, the integration
of traditional machine learning methods with KG embeddings, such as decision
trees and graph neural networks are also explored. The research findings
demonstrate the potential for improving prediction accuracy and provide
insights into embedding explainability in knowledge representation. The paper
also presents a comprehensive analysis of the influence of embedding methods on
other intelligent algorithms.",None,-1
46a9bd3f-adcb-45cc-8a97-76b4801a34d2,Face Presentation Attack Detection by Excavating Causal Clues and Adapting Embedding Statistics,0.453921,"Recent face presentation attack detection (PAD) leverages domain adaptation
(DA) and domain generalization (DG) techniques to address performance
degradation on unknown domains. However, DA-based PAD methods require access to
unlabeled target data, while most DG-based PAD solutions rely on a priori,
i.e., known domain labels. Moreover, most DA-/DG-based methods are
computationally intensive, demanding complex model architectures and/or
multi-stage training processes. This paper proposes to model face PAD as a
compound DG task from a causal perspective, linking it to model optimization.
We excavate the causal factors hidden in the high-level representation via
counterfactual intervention. Moreover, we introduce a class-guided MixStyle to
enrich feature-level data distribution within classes instead of focusing on
domain information. Both class-guided MixStyle and counterfactual intervention
components introduce no extra trainable parameters and negligible computational
resources. Extensive cross-dataset and analytic experiments demonstrate the
effectiveness and efficiency of our method compared to state-of-the-art PADs.
The implementation and the trained weights are publicly available.",None,-1
09d9bce3-aa7d-4b7d-a0e4-b9dec84b8d96,Improving Autonomous Separation Assurance through Distributed Reinforcement Learning with Attention Networks,0.68986,"Advanced Air Mobility (AAM) introduces a new, efficient mode of
transportation with the use of vehicle autonomy and electrified aircraft to
provide increasingly autonomous transportation between previously underserved
markets. Safe and efficient navigation of low altitude aircraft through highly
dense environments requires the integration of a multitude of complex
observations, such as surveillance, knowledge of vehicle dynamics, and weather.
The processing and reasoning on these observations pose challenges due to the
various sources of uncertainty in the information while ensuring cooperation
with a variable number of aircraft in the airspace. These challenges coupled
with the requirement to make safety-critical decisions in real-time rule out
the use of conventional separation assurance techniques. We present a
decentralized reinforcement learning framework to provide autonomous
self-separation capabilities within AAM corridors with the use of speed and
vertical maneuvers. The problem is formulated as a Markov Decision Process and
solved by developing a novel extension to the sample-efficient, off-policy soft
actor-critic (SAC) algorithm. We introduce the use of attention networks for
variable-length observation processing and a distributed computing architecture
to achieve high training sample throughput as compared to existing approaches.
A comprehensive numerical study shows that the proposed framework can ensure
safe and efficient separation of aircraft in high density, dynamic environments
with various sources of uncertainty.",None,-1
98de1593-df6e-4425-b1ae-a32dd77415c7,Unsupervised Deep Graph Matching Based on Cycle Consistency,0.494695,"We contribute to the sparsely populated area of unsupervised deep graph
matching with application to keypoint matching in images. Contrary to the
standard \emph{supervised} approach, our method does not require ground truth
correspondences between keypoint pairs. Instead, it is self-supervised by
enforcing consistency of matchings between images of the same object category.
As the matching and the consistency loss are discrete, their derivatives cannot
be straightforwardly used for learning. We address this issue in a principled
way by building our method upon the recent results on black-box differentiation
of combinatorial solvers. This makes our method exceptionally flexible, as it
is compatible with arbitrary network architectures and combinatorial solvers.
Our experimental evaluation suggests that our technique sets a new
state-of-the-art for unsupervised graph matching.",None,-1
1efa9d6b-dfce-42dd-a831-64534bf03479,Efficient Multimodal Fusion via Interactive Prompting,0.963658,"Large-scale pre-training has brought unimodal fields such as computer vision
and natural language processing to a new era. Following this trend, the size of
multi-modal learning models constantly increases, leading to an urgent need to
reduce the massive computational cost of finetuning these models for downstream
tasks. In this paper, we propose an efficient and flexible multimodal fusion
method, namely PMF, tailored for fusing unimodally pre-trained transformers.
Specifically, we first present a modular multimodal fusion framework that
exhibits high flexibility and facilitates mutual interactions among different
modalities. In addition, we disentangle vanilla prompts into three types in
order to learn different optimizing objectives for multimodal learning. It is
also worth noting that we propose to add prompt vectors only on the deep layers
of the unimodal transformers, thus significantly reducing the training memory
usage. Experiment results show that our proposed method achieves comparable
performance to several other multimodal finetuning methods with less than 3%
trainable parameters and up to 66% saving of training memory usage.",None,-1
d01e8b08-68e7-4eae-b82a-8e9c56903268,Prompted LLMs as Chatbot Modules for Long Open-domain Conversation,0.655466,"In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for
creating high-quality conversational agents without the need for fine-tuning.
Our method utilizes pre-trained large language models (LLMs) as individual
modules for long-term consistency and flexibility, by using techniques such as
few-shot prompting, chain-of-thought (CoT), and external memory. Our human
evaluation results show that MPC is on par with fine-tuned chatbot models in
open-domain conversations, making it an effective solution for creating
consistent and engaging chatbots.",None,-1
21bbfd6c-2412-4abe-95ea-598388c62308,Building Intelligence in the Mechanical Domain -- Harvesting the Reservoir Computing Power in Origami to Achieve Information Perception Tasks,0.262467,"In this paper, we experimentally examine the cognitive capability of a
simple, paper-based Miura-ori -- using the physical reservoir computing
framework -- to achieve different information perception tasks. The body
dynamics of Miura-ori (aka. its vertices displacements), which is excited by a
simple harmonic base excitation, can be exploited as the reservoir computing
resource. By recording these dynamics with a high-resolution camera and image
processing program and then using linear regression for training, we show that
the origami reservoir has sufficient computing capacity to estimate the weight
and position of a payload. It can also recognize the input frequency and
magnitude patterns. Furthermore, multitasking is achievable by simultaneously
applying two targeted functions to the same reservoir state matrix. Therefore,
we demonstrate that Miura-ori can assess the dynamic interactions between its
body and ambient environment to extract meaningful information -- an
intelligent behavior in the mechanical domain. Given that Miura-ori has been
widely used to construct deployable structures, lightweight materials, and
compliant robots, enabling such information perception tasks can add a new
dimension to the functionality of such a versatile structure.",None,-1
d55743b4-d72e-41a3-b33a-8c28a9fe9db4,A Comparison of Decision Algorithms on Newcomblike Problems,0.115651,"When formulated using Bayesian networks, two standard decision algorithms
(Evidential Decision Theory and Causal Decision Theory) can be shown to fail
systematically when faced with aspects of the prisoner's dilemma and so-called
""Newcomblike"" problems. We describe a new form of decision algorithm, called
Timeless Decision Theory, which consistently wins on these problems.",None,-1
8f5c81ee-f2f0-4cd8-b474-e30a681b8bf0,Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation,0.973821,"We introduce a method to measure uncertainty in large language models. For
tasks like question answering, it is essential to know when we can trust the
natural language outputs of foundation models. We show that measuring
uncertainty in natural language is challenging because of ""semantic
equivalence"" -- different sentences can mean the same thing. To overcome these
challenges we introduce semantic entropy -- an entropy which incorporates
linguistic invariances created by shared meanings. Our method is unsupervised,
uses only a single model, and requires no modifications to off-the-shelf
language models. In comprehensive ablation studies we show that the semantic
entropy is more predictive of model accuracy on question answering data sets
than comparable baselines.",None,-1
4f023c38-9c91-4c6f-8335-8326c970efb4,Interpretable Unified Language Checking,0.33644,"Despite recent concerns about undesirable behaviors generated by large
language models (LLMs), including non-factual, biased, and hateful language, we
find LLMs are inherent multi-task language checkers based on their latent
representations of natural and social knowledge. We present an interpretable,
unified, language checking (UniLC) method for both human and machine-generated
language that aims to check if language input is factual and fair. While
fairness and fact-checking tasks have been handled separately with dedicated
models, we find that LLMs can achieve high performance on a combination of
fact-checking, stereotype detection, and hate speech detection tasks with a
simple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task
language checking method proposed in this work, the GPT3.5-turbo model
outperforms fully supervised baselines on several language tasks. The simple
approach and results suggest that based on strong latent knowledge
representations, an LLM can be an adaptive and explainable tool for detecting
misinformation, stereotypes, and hate speech.",None,-1
5dbba6cd-76a0-4e36-bb6f-aaeef793a62c,Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits,0.586906,"We present Second Thought, a new learning paradigm that enables language
models (LMs) to re-align with human values. By modeling the chain-of-edits
between value-unaligned and value-aligned text, with LM fine-tuning and
additional refinement through reinforcement learning, Second Thought not only
achieves superior performance in three value alignment benchmark datasets but
also shows strong human-value transfer learning ability in few-shot scenarios.
The generated editing steps also offer better interpretability and ease for
interactive error correction. Extensive human evaluations further confirm its
effectiveness.",None,-1
2c24fad6-0313-4a89-beef-365a6a3e79cb,PSDR-Room: Single Photo to Scene using Differentiable Rendering,0.237358,"A 3D digital scene contains many components: lights, materials and
geometries, interacting to reach the desired appearance. Staging such a scene
is time-consuming and requires both artistic and technical skills. In this
work, we propose PSDR-Room, a system allowing to optimize lighting as well as
the pose and materials of individual objects to match a target image of a room
scene, with minimal user input. To this end, we leverage a recent path-space
differentiable rendering approach that provides unbiased gradients of the
rendering with respect to geometry, lighting, and procedural materials,
allowing us to optimize all of these components using gradient descent to
visually match the input photo appearance. We use recent single-image scene
understanding methods to initialize the optimization and search for appropriate
3D models and materials. We evaluate our method on real photographs of indoor
scenes and demonstrate the editability of the resulting scene components.",None,-1
30409f0e-c891-4d78-8d3a-a32712e44fb7,TherapyView: Visualizing Therapy Sessions with Temporal Topic Modeling and AI-Generated Arts,0.76477,"We present the TherapyView, a demonstration system to help therapists
visualize the dynamic contents of past treatment sessions, enabled by the
state-of-the-art neural topic modeling techniques to analyze the topical
tendencies of various psychiatric conditions and deep learning-based image
generation engine to provide a visual summary. The system incorporates temporal
modeling to provide a time-series representation of topic similarities at a
turn-level resolution and AI-generated artworks given the dialogue segments to
provide a concise representations of the contents covered in the session,
offering interpretable insights for therapists to optimize their strategies and
enhance the effectiveness of psychotherapy. This system provides a proof of
concept of AI-augmented therapy tools with e in-depth understanding of the
patient's mental state and enabling more effective treatment.",None,-1
865056dc-a62b-4934-adb3-e93276f2f989,LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models,0.423243,"Quantization is an indispensable technique for serving Large Language Models
(LLMs) and has recently found its way into LoRA fine-tuning. In this work we
focus on the scenario where quantization and LoRA fine-tuning are applied
together on a pre-trained model. In such cases it is common to observe a
consistent gap in the performance on downstream tasks between full fine-tuning
and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ
(LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that
simultaneously quantizes an LLM and finds a proper low-rank initialization for
LoRA fine-tuning. Such an initialization alleviates the discrepancy between the
quantized and full-precision model and significantly improves generalization in
downstream tasks. We evaluate our method on natural language understanding,
question answering, summarization, and natural language generation tasks.
Experiments show that our method is highly effective and outperforms existing
quantization methods, especially in the challenging 2-bit and 2/4-bit mixed
precision regimes. The code is available on https://github.com/yxli2123/LoftQ.",None,-1
ba930d01-bf5a-4caf-bb8e-5d19fd9f8e8d,Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion,0.798131,"Diffusion models have shown superior performance in image generation and
manipulation, but the inherent stochasticity presents challenges in preserving
and manipulating image content and identity. While previous approaches like
DreamBooth and Textual Inversion have proposed model or latent representation
personalization to maintain the content, their reliance on multiple reference
images and complex training limits their practicality. In this paper, we
present a simple yet highly effective approach to personalization using highly
personalized (HiPer) text embedding by decomposing the CLIP embedding space for
personalization and content manipulation. Our method does not require model
fine-tuning or identifiers, yet still enables manipulation of background,
texture, and motion with just a single image and target text. Through
experiments on diverse target texts, we demonstrate that our approach produces
highly personalized and complex semantic image edits across a wide range of
tasks. We believe that the novel understanding of the text embedding space
presented in this work has the potential to inspire further research across
various tasks.",None,-1
bf658324-019f-4f04-aee4-2a4a2ec36997,Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming,0.89483,"The rapid advancements in artificial intelligence (AI) have led to a growing
trend of human-AI teaming (HAT) in various fields. As machines continue to
evolve from mere automation to a state of autonomy, they are increasingly
exhibiting unexpected behaviors and human-like cognitive/intelligent
capabilities, including situation awareness (SA). This shift has the potential
to enhance the performance of mixed human-AI teams over all-human teams,
underscoring the need for a better understanding of the dynamic SA interactions
between humans and machines. To this end, we provide a review of leading SA
theoretical models and a new framework for SA in the HAT context based on the
key features and processes of HAT. The Agent Teaming Situation Awareness (ATSA)
framework unifies human and AI behavior, and involves bidirectional, and
dynamic interaction. The framework is based on the individual and team SA
models and elaborates on the cognitive mechanisms for modeling HAT. Similar
perceptual cycles are adopted for the individual (including both human and AI)
and the whole team, which is tailored to the unique requirements of the HAT
context. ATSA emphasizes cohesive and effective HAT through structures and
components, including teaming understanding, teaming control, and the world, as
well as adhesive transactive part. We further propose several future research
directions to expand on the distinctive contributions of ATSA and address the
specific and pressing next steps.",None,-1
49ff3b86-c16b-4ed0-9e01-80fd83c26485,Mastering the Task of Open Information Extraction with Large Language Models and Consistent Reasoning Environment,0.612607,"Open Information Extraction (OIE) aims to extract objective structured
knowledge from natural texts, which has attracted growing attention to build
dedicated models with human experience. As the large language models (LLMs)
have exhibited remarkable in-context learning capabilities, a question arises
as to whether the task of OIE can be effectively tackled with this paradigm? In
this paper, we explore solving the OIE problem by constructing an appropriate
reasoning environment for LLMs. Specifically, we first propose a method to
effectively estimate the discrepancy of syntactic distribution between a LLM
and test samples, which can serve as correlation evidence for preparing
positive demonstrations. Upon the evidence, we introduce a simple yet effective
mechanism to establish the reasoning environment for LLMs on specific tasks.
Without bells and whistles, experimental results on the standard CaRB benchmark
demonstrate that our $6$-shot approach outperforms state-of-the-art supervised
method, achieving an $55.3$ $F_1$ score. Further experiments on TACRED and
ACE05 show that our method can naturally generalize to other information
extraction tasks, resulting in improvements of $5.7$ and $6.8$ $F_1$ scores,
respectively.",None,-1
9e313432-1196-431a-9dda-35dd9ec637bf,Diverse and Faithful Knowledge-Grounded Dialogue Generation via Sequential Posterior Inference,0.33217,"The capability to generate responses with diversity and faithfulness using
factual knowledge is paramount for creating a human-like, trustworthy dialogue
system. Common strategies either adopt a two-step paradigm, which optimizes
knowledge selection and response generation separately, and may overlook the
inherent correlation between these two tasks, or leverage conditional
variational method to jointly optimize knowledge selection and response
generation by employing an inference network. In this paper, we present an
end-to-end learning framework, termed Sequential Posterior Inference (SPI),
capable of selecting knowledge and generating dialogues by approximately
sampling from the posterior distribution. Unlike other methods, SPI does not
require the inference network or assume a simple geometry of the posterior
distribution. This straightforward and intuitive inference procedure of SPI
directly queries the response generation model, allowing for accurate knowledge
selection and generation of faithful responses. In addition to modeling
contributions, our experimental results on two common dialogue datasets (Wizard
of Wikipedia and Holl-E) demonstrate that SPI outperforms previous strong
baselines according to both automatic and human evaluation metrics.",None,-1
c83accc0-0233-4c11-9112-e5528138c5c9,What does BERT learn about prosody?,0.51502,"Language models have become nearly ubiquitous in natural language processing
applications achieving state-of-the-art results in many tasks including
prosody. As the model design does not define predetermined linguistic targets
during training but rather aims at learning generalized representations of the
language, analyzing and interpreting the representations that models implicitly
capture is important in bridging the gap between interpretability and model
performance. Several studies have explored the linguistic information that
models capture providing some insights on their representational capacity.
However, the current studies have not explored whether prosody is part of the
structural information of the language that models learn. In this work, we
perform a series of experiments on BERT probing the representations captured at
different layers. Our results show that information about prosodic prominence
spans across many layers but is mostly focused in middle layers suggesting that
BERT relies mostly on syntactic and semantic information.",None,-1
1a743ecd-f40d-4458-b53e-9551dc1a16f5,Pretraining on the Test Set Is All You Need,0.479073,"Inspired by recent work demonstrating the promise of smaller
Transformer-based language models pretrained on carefully curated data, we
supercharge such approaches by investing heavily in curating a novel, high
quality, non-synthetic data mixture based solely on evaluation benchmarks.
Using our novel dataset mixture consisting of less than 100 thousand tokens, we
pretrain a 1 million parameter transformer-based LLM \textbf{phi-CTNL}
(pronounced ``fictional"") that achieves perfect results across diverse academic
benchmarks, strictly outperforming all known foundation models.
\textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen
grokking-like ability to accurately predict downstream evaluation benchmarks'
canaries.",None,-1
bb1d1f56-9997-46d5-921b-c7fd1121345b,A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases,0.332776,"Enterprise applications of Large Language Models (LLMs) hold promise for
question answering on enterprise SQL databases. However, the extent to which
LLMs can accurately respond to enterprise questions in such databases remains
unclear, given the absence of suitable Text-to-SQL benchmarks tailored to
enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to
enhance LLM-based question answering by providing business context is not well
understood. This study aims to evaluate the accuracy of LLM-powered question
answering systems in the context of enterprise questions and SQL databases,
while also exploring the role of knowledge graphs in improving accuracy. To
achieve this, we introduce a benchmark comprising an enterprise SQL schema in
the insurance domain, a range of enterprise queries encompassing reporting to
metrics, and a contextual layer incorporating an ontology and mappings that
define a knowledge graph. Our primary finding reveals that question answering
using GPT-4, with zero-shot prompts directly on SQL databases, achieves an
accuracy of 16%. Notably, this accuracy increases to 54% when questions are
posed over a Knowledge Graph representation of the enterprise SQL database.
Therefore, investing in Knowledge Graph provides higher accuracy for LLM
powered question answering systems.",None,-1
7261d0e8-0a4e-45b4-a0bc-4b7f2fbe34e4,Knowledge-aware Bayesian Co-attention for Multimodal Emotion Recognition,0.617307,"Multimodal emotion recognition is a challenging research area that aims to
fuse different modalities to predict human emotion. However, most existing
models that are based on attention mechanisms have difficulty in learning
emotionally relevant parts on their own. To solve this problem, we propose to
incorporate external emotion-related knowledge in the co-attention based fusion
of pre-trained models. To effectively incorporate this knowledge, we enhance
the co-attention model with a Bayesian attention module (BAM) where a prior
distribution is estimated using the emotion-related knowledge. Experimental
results on the IEMOCAP dataset show that the proposed approach can outperform
several state-of-the-art approaches by at least 0.7% unweighted accuracy (UA).",None,-1
77d6ed53-e768-4dca-a146-ef7d48387aa6,The Role of Interactive Visualization in Explaining (Large) NLP Models: from Data to Inference,0.656826,"With a constant increase of learned parameters, modern neural language models
become increasingly more powerful. Yet, explaining these complex model's
behavior remains a widely unsolved problem. In this paper, we discuss the role
interactive visualization can play in explaining NLP models (XNLP). We motivate
the use of visualization in relation to target users and common NLP pipelines.
We also present several use cases to provide concrete examples on XNLP with
visualization. Finally, we point out an extensive list of research
opportunities in this field.",None,-1
be2cda66-6789-49cc-b447-a2c8e25a06b0,Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection,0.679431,"Natural language processing models tend to learn and encode social biases
present in the data. One popular approach for addressing such biases is to
eliminate encoded information from the model's representations. However,
current methods are restricted to removing only linearly encoded information.
In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel
method for removing non-linear encoded concepts from neural representations.
Our method consists of iteratively training neural classifiers to predict a
particular attribute we seek to eliminate, followed by a projection of the
representation on a hypersurface, such that the classifiers become oblivious to
the target attribute. We evaluate the effectiveness of our method on the task
of removing gender and race information as sensitive attributes. Our results
demonstrate that IGBP is effective in mitigating bias through intrinsic and
extrinsic evaluations, with minimal impact on downstream task accuracy.",None,-1
022a5efa-ac5c-47d1-9513-4ded5197a5bd,Dense Text-to-Image Generation with Attention Modulation,0.843161,"Existing text-to-image diffusion models struggle to synthesize realistic
images given dense captions, where each text prompt provides a detailed
description for a specific image region. To address this, we propose
DenseDiffusion, a training-free method that adapts a pre-trained text-to-image
model to handle such dense captions while offering control over the scene
layout. We first analyze the relationship between generated images' layouts and
the pre-trained model's intermediate attention maps. Next, we develop an
attention modulation method that guides objects to appear in specific regions
according to layout guidance. Without requiring additional fine-tuning or
datasets, we improve image generation performance given dense captions
regarding both automatic and human evaluation scores. In addition, we achieve
similar-quality visual results with models specifically trained with layout
conditions.",None,-1
116cdab6-768d-49d7-9878-eb6a83cca9b4,DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models,0.413005,"Despite their impressive capabilities, large language models (LLMs) are prone
to hallucinations, i.e., generating content that deviates from facts seen
during pretraining. We propose a simple decoding strategy for reducing
hallucinations with pretrained LLMs that does not require conditioning on
retrieved external knowledge nor additional fine-tuning. Our approach obtains
the next-token distribution by contrasting the differences in logits obtained
from projecting the later layers versus earlier layers to the vocabulary space,
exploiting the fact that factual knowledge in an LLMs has generally been shown
to be localized to particular transformer layers. We find that this Decoding by
Contrasting Layers (DoLa) approach is able to better surface factual knowledge
and reduce the generation of incorrect facts. DoLa consistently improves the
truthfulness across multiple choices tasks and open-ended generation tasks, for
example improving the performance of LLaMA family models on TruthfulQA by
12-17% absolute points, demonstrating its potential in making LLMs reliably
generate truthful facts.",None,-1
9e668f2b-2e74-4b0a-9560-c14a4f7e1f09,AutoNMT: A Framework to Streamline the Research of Seq2Seq Models,0.0499831,"We present AutoNMT, a framework to streamline the research of seq-to-seq
models by automating the data pipeline (i.e., file management, data
preprocessing, and exploratory analysis), automating experimentation in a
toolkit-agnostic manner, which allows users to use either their own models or
existing seq-to-seq toolkits such as Fairseq or OpenNMT, and finally,
automating the report generation (plots and summaries). Furthermore, this
library comes with its own seq-to-seq toolkit so that users can easily
customize it for non-standard tasks.",None,-1
79a4665f-216c-472e-9b55-ea7e9f99704f,A Corpus for Sentence-level Subjectivity Detection on English News Articles,0.28675,"We develop novel annotation guidelines for sentence-level subjectivity
detection, which are not limited to language-specific cues. We use our
guidelines to collect NewsSD-ENG, a corpus of 638 objective and 411 subjective
sentences extracted from English news articles on controversial topics. Our
corpus paves the way for subjectivity detection in English and across other
languages without relying on language-specific tools, such as lexicons or
machine translation. We evaluate state-of-the-art multilingual
transformer-based models on the task in mono-, multi-, and cross-language
settings. For this purpose, we re-annotate an existing Italian corpus. We
observe that models trained in the multilingual setting achieve the best
performance on the task.",None,-1
628284ee-2c7c-4d24-9b28-db800f2aa648,Learning Logic Specifications for Soft Policy Guidance in POMCP,0.851188,"Partially Observable Monte Carlo Planning (POMCP) is an efficient solver for
Partially Observable Markov Decision Processes (POMDPs). It allows scaling to
large state spaces by computing an approximation of the optimal policy locally
and online, using a Monte Carlo Tree Search based strategy. However, POMCP
suffers from sparse reward function, namely, rewards achieved only when the
final goal is reached, particularly in environments with large state spaces and
long horizons. Recently, logic specifications have been integrated into POMCP
to guide exploration and to satisfy safety requirements. However, such
policy-related rules require manual definition by domain experts, especially in
real-world scenarios. In this paper, we use inductive logic programming to
learn logic specifications from traces of POMCP executions, i.e., sets of
belief-action pairs generated by the planner. Specifically, we learn rules
expressed in the paradigm of answer set programming. We then integrate them
inside POMCP to provide soft policy bias toward promising actions. In the
context of two benchmark scenarios, rocksample and battery, we show that the
integration of learned rules from small task instances can improve performance
with fewer Monte Carlo simulations and in larger task instances. We make our
modified version of POMCP publicly available at
https://github.com/GiuMaz/pomcp_clingo.git.",None,-1
56058e89-d720-4d86-af36-f0278d742b99,Super-Resolution Information Enhancement For Crowd Counting,0.613305,"Crowd counting is a challenging task due to the heavy occlusions, scales, and
density variations. Existing methods handle these challenges effectively while
ignoring low-resolution (LR) circumstances. The LR circumstances weaken the
counting performance deeply for two crucial reasons: 1) limited detail
information; 2) overlapping head regions accumulate in density maps and result
in extreme ground-truth values. An intuitive solution is to employ
super-resolution (SR) pre-processes for the input LR images. However, it
complicates the inference steps and thus limits application potentials when
requiring real-time. We propose a more elegant method termed Multi-Scale
Super-Resolution Module (MSSRM). It guides the network to estimate the lost de
tails and enhances the detailed information in the feature space. Noteworthy
that the MSSRM is plug-in plug-out and deals with the LR problems with no
inference cost. As the proposed method requires SR labels, we further propose a
Super-Resolution Crowd Counting dataset (SR-Crowd). Extensive experiments on
three datasets demonstrate the superiority of our method. The code will be
available at https://github.com/PRIS-CV/MSSRM.git.",None,-1
e8f83cd0-e197-484c-be85-304bae6096a6,Explain What You See: Open-Ended Segmentation and Recognition of Occluded 3D Objects,0.322232,"Local-HDP (for Local Hierarchical Dirichlet Process) is a hierarchical
Bayesian method that has recently been used for open-ended 3D object category
recognition. This method has been proven to be efficient in real-time robotic
applications. However, the method is not robust to a high degree of occlusion.
We address this limitation in two steps. First, we propose a novel semantic 3D
object-parts segmentation method that has the flexibility of Local-HDP. This
method is shown to be suitable for open-ended scenarios where the number of 3D
objects or object parts is not fixed and can grow over time. We show that the
proposed method has a higher percentage of mean intersection over union, using
a smaller number of learning instances. Second, we integrate this technique
with a recently introduced argumentation-based online incremental learning
method, thereby enabling the model to handle a high degree of occlusion. We
show that the resulting model produces an explicit set of explanations for the
3D object category recognition task.",None,-1
8a01fabf-d557-4df0-9079-8fdf0c936656,An Experiment in Retrofitting Competency Questions for Existing Ontologies,0.612317,"Competency Questions (CQs) are a form of ontology functional requirements
expressed as natural language questions. Inspecting CQs together with the
axioms in an ontology provides critical insights into the intended scope and
applicability of the ontology. CQs also underpin a number of tasks in the
development of ontologies e.g. ontology reuse, ontology testing, requirement
specification, and the definition of patterns that implement such requirements.
Although CQs are integral to the majority of ontology engineering
methodologies, the practice of publishing CQs alongside the ontological
artefacts is not widely observed by the community. In this context, we present
an experiment in retrofitting CQs from existing ontologies. We propose
RETROFIT-CQs, a method to extract candidate CQs directly from ontologies using
Generative AI. In the paper we present the pipeline that facilitates the
extraction of CQs by leveraging Large Language Models (LLMs) and we discuss its
application to a number of existing ontologies.",None,-1
0767f03f-6807-429c-b36d-3534fc3687de,Extrinsic Factors Affecting the Accuracy of Biomedical NER,0.375919,"Biomedical named entity recognition (NER) is a critial task that aims to
identify structured information in clinical text, which is often replete with
complex, technical terms and a high degree of variability. Accurate and
reliable NER can facilitate the extraction and analysis of important biomedical
information, which can be used to improve downstream applications including the
healthcare system. However, NER in the biomedical domain is challenging due to
limited data availability, as the high expertise, time, and expenses are
required to annotate its data. In this paper, by using the limited data, we
explore various extrinsic factors including the corpus annotation scheme, data
augmentation techniques, semi-supervised learning and Brill transformation, to
improve the performance of a NER model on a clinical text dataset (i2b2 2012,
\citet{sun-rumshisky-uzuner:2013}). Our experiments demonstrate that these
approaches can significantly improve the model's F1 score from original 73.74
to 77.55. Our findings suggest that considering different extrinsic factors and
combining these techniques is a promising approach for improving NER
performance in the biomedical domain where the size of data is limited.",None,-1
78bda044-c2ee-47fb-a495-766bb0ee7092,Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,0.524583,"Real-world fact verification task aims to verify the factuality of a claim by
retrieving evidence from the source document. The quality of the retrieved
evidence plays an important role in claim verification. Ideally, the retrieved
evidence should be faithful (reflecting the model's decision-making process in
claim verification) and plausible (convincing to humans), and can improve the
accuracy of verification task. Although existing approaches leverage the
similarity measure of semantic or surface form between claims and documents to
retrieve evidence, they all rely on certain heuristics that prevent them from
satisfying all three requirements. In light of this, we propose a fact
verification model named ReRead to retrieve evidence and verify claim that: (1)
Train the evidence retriever to obtain interpretable evidence (i.e.,
faithfulness and plausibility criteria); (2) Train the claim verifier to
revisit the evidence retrieved by the optimized evidence retriever to improve
the accuracy. The proposed system is able to achieve significant improvements
upon best-reported models under different settings.",None,-1
c463cceb-514e-43ce-adb5-4fe8f680a226,Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks,0.0906091,"Given video demonstrations and paired narrations of an at-home procedural
task such as changing a tire, we present an approach to extract the underlying
task structure -- relevant actions and their temporal dependencies -- via
action-centric task graphs. Learnt structured representations from our method,
Action Dynamics Task Graphs (ADTG), can then be used for understanding such
tasks in unseen videos of humans performing them. Furthermore, ADTG can enable
providing user-centric guidance to humans in these tasks, either for performing
them better or for learning new tasks. Specifically, we show how ADTG can be
used for: (1) tracking an ongoing task, (2) recommending next actions, and (3)
planning a sequence of actions to accomplish a procedural task. We compare
against state-of-the-art Neural Task Graph method and demonstrate substantial
gains on 18 procedural tasks from the CrossTask dataset, including 30.1%
improvement in task tracking accuracy and 20.3% accuracy gain in next action
prediction.",None,-1
026143af-cdb9-4b7d-a045-0224ac9d4528,WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models,0.692917,"The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the
development of large models, leading to the creation of numerous impressive
large language models(LLMs) and multimodal large language models (MLLMs). These
cutting-edge models owe their remarkable performance to high-quality data.
However, the details of the training data used in leading paradigms are often
kept confidential. This lack of transparency, coupled with the scarcity of
open-source data, impedes further developments within the community. As a
response, this paper presents ""Wan Juan"", a large-scale multimodal dataset
composed of both Chinese and English data, collected from a wide range of web
sources. The dataset incorporates text, image-text, and video modalities, with
a total volume exceeding 2TB. It was utilized in the training of InternLM, a
model that demonstrated significant advantages in multi-dimensional evaluations
when compared to models of a similar scale. All data can be accessed at
https://opendatalab.org.cn/WanJuan1.0.",None,-1
a159de25-5fc4-432c-9c40-251e8d0cf0da,A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis,0.248883,"Mathematical reasoning in large language models (LMs) has garnered
significant attention in recent work, but there is a limited understanding of
how these models process and store information related to arithmetic tasks
within their architecture. In order to improve our understanding of this aspect
of language models, we present a mechanistic interpretation of
Transformer-based LMs on arithmetic questions using a causal mediation analysis
framework. By intervening on the activations of specific model components and
measuring the resulting changes in predicted probabilities, we identify the
subset of parameters responsible for specific predictions. This provides
insights into how information related to arithmetic is processed by LMs. Our
experimental results indicate that LMs process the input by transmitting the
information relevant to the query from mid-sequence early layers to the final
token using the attention mechanism. Then, this information is processed by a
set of MLP modules, which generate result-related information that is
incorporated into the residual stream. To assess the specificity of the
observed activation dynamics, we compare the effects of different model
components on arithmetic queries with other tasks, including number retrieval
from prompts and factual knowledge questions.",None,-1
97c0606f-3d4e-4649-96ec-80f09a47416b,Reference-based Painterly Inpainting via Diffusion: Crossing the Wild Reference Domain Gap,0.459294,"Have you ever imagined how it would look if we placed new objects into
paintings? For example, what would it look like if we placed a basketball into
Claude Monet's ``Water Lilies, Evening Effect''? We propose Reference-based
Painterly Inpainting, a novel task that crosses the wild reference domain gap
and implants novel objects into artworks. Although previous works have examined
reference-based inpainting, they are not designed for large domain
discrepancies between the target and the reference, such as inpainting an
artistic image using a photorealistic reference. This paper proposes a novel
diffusion framework, dubbed RefPaint, to ``inpaint more wildly'' by taking such
references with large domain gaps. Built with an image-conditioned diffusion
model, we introduce a ladder-side branch and a masked fusion mechanism to work
with the inpainting mask. By decomposing the CLIP image embeddings at inference
time, one can manipulate the strength of semantic and style information with
ease. Experiments demonstrate that our proposed RefPaint framework produces
significantly better results than existing methods. Our method enables creative
painterly image inpainting with reference objects that would otherwise be
difficult to achieve. Project page: https://vita-group.github.io/RefPaint/",None,-1
07b9579c-8d9b-4f1f-b5fc-cc99157a999e,Exploration via Epistemic Value Estimation,0.0292539,"How to efficiently explore in reinforcement learning is an open problem. Many
exploration algorithms employ the epistemic uncertainty of their own value
predictions -- for instance to compute an exploration bonus or upper confidence
bound. Unfortunately the required uncertainty is difficult to estimate in
general with function approximation.
  We propose epistemic value estimation (EVE): a recipe that is compatible with
sequential decision making and with neural network function approximators. It
equips agents with a tractable posterior over all their parameters from which
epistemic value uncertainty can be computed efficiently.
  We use the recipe to derive an epistemic Q-Learning agent and observe
competitive performance on a series of benchmarks. Experiments confirm that the
EVE recipe facilitates efficient exploration in hard exploration tasks.",None,-1
ea6d37dc-2d2f-45c4-9bb1-ef24c7eccbc6,DecompX: Explaining Transformers Decisions by Propagating Token Decomposition,0.342398,"An emerging solution for explaining Transformer-based models is to use
vector-based analysis on how the representations are formed. However, providing
a faithful vector-based explanation for a multi-layer model could be
challenging in three aspects: (1) Incorporating all components into the
analysis, (2) Aggregating the layer dynamics to determine the information flow
and mixture throughout the entire model, and (3) Identifying the connection
between the vector-based analysis and the model's predictions. In this paper,
we present DecompX to tackle these challenges. DecompX is based on the
construction of decomposed token representations and their successive
propagation throughout the model without mixing them in between layers.
Additionally, our proposal provides multiple advantages over existing solutions
for its inclusion of all encoder components (especially nonlinear feed-forward
networks) and the classification head. The former allows acquiring precise
vectors while the latter transforms the decomposition into meaningful
prediction-based values, eliminating the need for norm- or summation-based
vector aggregation. According to the standard faithfulness evaluations, DecompX
consistently outperforms existing gradient-based and vector-based approaches on
various datasets. Our code is available at
https://github.com/mohsenfayyaz/DecompX.",None,-1
84b13cca-402d-415b-88c7-52e684a09f53,Unsupervised Domain Adaptation for Training Event-Based Networks Using Contrastive Learning and Uncorrelated Conditioning,0.626462,"Event-based cameras offer reliable measurements for preforming computer
vision tasks in high-dynamic range environments and during fast motion
maneuvers. However, adopting deep learning in event-based vision faces the
challenge of annotated data scarcity due to recency of event cameras.
Transferring the knowledge that can be obtained from conventional camera
annotated data offers a practical solution to this challenge. We develop an
unsupervised domain adaptation algorithm for training a deep network for
event-based data image classification using contrastive learning and
uncorrelated conditioning of data. Our solution outperforms the existing
algorithms for this purpose.",None,-1
db1052c3-da70-470d-b339-f416e699173e,Rotational augmentation techniques: a new perspective on ensemble learning for image classification,0.043054,"The popularity of data augmentation techniques in machine learning has
increased in recent years, as they enable the creation of new samples from
existing datasets. Rotational augmentation, in particular, has shown great
promise by revolving images and utilising them as additional data points for
training. This research study introduces a new approach to enhance the
performance of classification methods where the testing sets were generated
employing transformations on every image from the original dataset.
Subsequently, ensemble-based systems were implemented to determine the most
reliable outcome in each subset acquired from the augmentation phase to get a
final prediction for every original image. The findings of this study suggest
that rotational augmentation techniques can significantly improve the accuracy
of standard classification models; and the selection of a voting scheme can
considerably impact the model's performance. Overall, the study found that
using an ensemble-based voting system produced more accurate results than
simple voting.",None,-1
66ac3c44-3fe0-45d5-86e5-9bb1ce71a566,Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks,0.458681,"We introduce a novel and efficient method for Event Coreference Resolution
(ECR) applied to a lower-resourced language domain. By framing ECR as a graph
reconstruction task, we are able to combine deep semantic embeddings with
structural coreference chain knowledge to create a parameter-efficient family
of Graph Autoencoder models (GAE). Our method significantly outperforms
classical mention-pair methods on a large Dutch event coreference corpus in
terms of overall score, efficiency and training speed. Additionally, we show
that our models are consistently able to classify more difficult coreference
links and are far more robust in low-data settings when compared to
transformer-based mention-pair coreference algorithms.",None,-1
807a1fa7-a109-4cf7-9847-739c80319e1d,Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment,0.789691,"The egocentric and exocentric viewpoints of a human activity look
dramatically different, yet invariant representations to link them are
essential for many potential applications in robotics and augmented reality.
Prior work is limited to learning view-invariant features from paired
synchronized viewpoints. We relax that strong data assumption and propose to
learn fine-grained action features that are invariant to the viewpoints by
aligning egocentric and exocentric videos in time, even when not captured
simultaneously or in the same environment. To this end, we propose AE2, a
self-supervised embedding approach with two key designs: (1) an object-centric
encoder that explicitly focuses on regions corresponding to hands and active
objects; and (2) a contrastive-based alignment objective that leverages
temporally reversed frames as negative samples. For evaluation, we establish a
benchmark for fine-grained video understanding in the ego-exo context,
comprising four datasets -- including an ego tennis forehand dataset we
collected, along with dense per-frame labels we annotated for each dataset. On
the four datasets, our AE2 method strongly outperforms prior work in a variety
of fine-grained downstream tasks, both in regular and cross-view settings.",None,-1
39239420-3f26-4b20-98ff-90ef7f449d60,Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition,0.896655,"The integration of Language Models (LMs) has proven to be an effective way to
address domain shifts in speech recognition. However, these approaches usually
require a significant amount of target domain text data for the training of
LMs. Different from these methods, in this work, with only a domain-specific
text prompt, we propose two zero-shot ASR domain adaptation methods using
LLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two
ways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR
system with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an
encoder-decoder based ASR system. Experiments show that, with only one domain
prompt, both methods can effectively reduce word error rates (WER) on
out-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep
LLM-fusion has the advantage of better recall of entity and out-of-vocabulary
words.",None,-1
7d949707-aafc-438d-abce-f9e60cfa1639,shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation,0.628541,"Instruction-tuned generative Large language models (LLMs) like ChatGPT and
Bloomz possess excellent generalization abilities, but they face limitations in
understanding radiology reports, particularly in the task of generating the
IMPRESSIONS section from the FINDINGS section. They tend to generate either
verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to
medical text data during training. We present a system which leverages
large-scale medical text data for domain-adaptive pre-training of
instruction-tuned LLMs to enhance its medical knowledge and performance on
specific medical tasks. We show that this system performs better in a zero-shot
setting than a number of pretrain-and-finetune adaptation methods on the
IMPRESSIONS generation task, and ranks 1st among participating systems in Task
1B: Radiology Report Summarization at the BioNLP 2023 workshop.",None,-1
bbcc4835-ed16-45b7-bbb4-fefa2047910e,AVSegFormer: Audio-Visual Segmentation with Transformer,0.772234,"The combination of audio and vision has long been a topic of interest in the
multi-modal community. Recently, a new audio-visual segmentation (AVS) task has
been introduced, aiming to locate and segment the sounding objects in a given
video. This task demands audio-driven pixel-level scene understanding for the
first time, posing significant challenges. In this paper, we propose
AVSegFormer, a novel framework for AVS tasks that leverages the transformer
architecture. Specifically, we introduce audio queries and learnable queries
into the transformer decoder, enabling the network to selectively attend to
interested visual features. Besides, we present an audio-visual mixer, which
can dynamically adjust visual features by amplifying relevant and suppressing
irrelevant spatial channels. Additionally, we devise an intermediate mask loss
to enhance the supervision of the decoder, encouraging the network to produce
more accurate intermediate predictions. Extensive experiments demonstrate that
AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is
available at https://github.com/vvvb-github/AVSegFormer.",None,-1
7250cedb-9316-4b2d-964c-90aa7117ac73,Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?,0.74298,"Large language models (LLMs) such as ChatGPT are increasingly being used for
various use cases, including text content generation at scale. Although
detection methods for such AI-generated text exist already, we investigate
ChatGPT's performance as a detector on such AI-generated text, inspired by
works that use ChatGPT as a data labeler or annotator. We evaluate the
zero-shot performance of ChatGPT in the task of human-written vs. AI-generated
text detection, and perform experiments on publicly available datasets. We
empirically investigate if ChatGPT is symmetrically effective in detecting
AI-generated or human-written text. Our findings provide insight on how ChatGPT
and similar LLMs may be leveraged in automated detection pipelines by simply
focusing on solving a specific aspect of the problem and deriving the rest from
that solution. All code and data is available at
https://github.com/AmritaBh/ChatGPT-as-Detector.",None,-1
79556151-37ab-4718-a0e1-79f9b74a0236,Pseudo-Boolean Polynomials Approach To Edge Detection And Image Segmentation,0.0831274,"We introduce a deterministic approach to edge detection and image
segmentation by formulating pseudo-Boolean polynomials on image patches. The
approach works by applying a binary classification of blob and edge regions in
an image based on the degrees of pseudo-Boolean polynomials calculated on
patches extracted from the provided image. We test our method on simple images
containing primitive shapes of constant and contrasting colour and establish
the feasibility before applying it to complex instances like aerial landscape
images. The proposed method is based on the exploitation of the reduction,
polynomial degree, and equivalence properties of penalty-based pseudo-Boolean
polynomials.",None,-1
1981aa55-6454-4810-aff0-075c0b442bb2,Decidable Fragments of LTLf Modulo Theories (Extended Version),0.819908,"We study Linear Temporal Logic Modulo Theories over Finite Traces (LTLfMT), a
recently introduced extension of LTL over finite traces (LTLf) where
propositions are replaced by first-order formulas and where first-order
variables referring to different time points can be compared. In general,
LTLfMT was shown to be semi-decidable for any decidable first-order theory
(e.g., linear arithmetics), with a tableau-based semi-decision procedure.
  In this paper we present a sound and complete pruning rule for the LTLfMT
tableau. We show that for any LTLfMT formula that satisfies an abstract,
semantic condition, that we call finite memory, the tableau augmented with the
new rule is also guaranteed to terminate. Last but not least, this technique
allows us to establish novel decidability results for the satisfiability of
several fragments of LTLfMT, as well as to give new decidability proofs for
classes that are already known.",None,-1
8e9eea42-9ac1-4a9c-9518-941e259b521d,AutoCost: Evolving Intrinsic Cost for Zero-violation Reinforcement Learning,0.558006,"Safety is a critical hurdle that limits the application of deep reinforcement
learning (RL) to real-world control tasks. To this end, constrained
reinforcement learning leverages cost functions to improve safety in
constrained Markov decision processes. However, such constrained RL methods
fail to achieve zero violation even when the cost limit is zero. This paper
analyzes the reason for such failure, which suggests that a proper cost
function plays an important role in constrained RL. Inspired by the analysis,
we propose AutoCost, a simple yet effective framework that automatically
searches for cost functions that help constrained RL to achieve zero-violation
performance. We validate the proposed method and the searched cost function on
the safe RL benchmark Safety Gym. We compare the performance of augmented
agents that use our cost function to provide additive intrinsic costs with
baseline agents that use the same policy learners but with only extrinsic
costs. Results show that the converged policies with intrinsic costs in all
environments achieve zero constraint violation and comparable performance with
baselines.",None,-1
1989f249-f610-44d5-8bf2-2ba4a046522b,"More Synergy, Less Redundancy: Exploiting Joint Mutual Information for Self-Supervised Learning",0.194958,"Self-supervised learning (SSL) is now a serious competitor for supervised
learning, even though it does not require data annotation. Several baselines
have attempted to make SSL models exploit information about data distribution,
and less dependent on the augmentation effect. However, there is no clear
consensus on whether maximizing or minimizing the mutual information between
representations of augmentation views practically contribute to improvement or
degradation in performance of SSL models. This paper is a fundamental work
where, we investigate role of mutual information in SSL, and reformulate the
problem of SSL in the context of a new perspective on mutual information. To
this end, we consider joint mutual information from the perspective of partial
information decomposition (PID) as a key step in \textbf{reliable multivariate
information measurement}. PID enables us to decompose joint mutual information
into three important components, namely, unique information, redundant
information and synergistic information. Our framework aims for minimizing the
redundant information between views and the desired target representation while
maximizing the synergistic information at the same time. Our experiments lead
to a re-calibration of two redundancy reduction baselines, and a proposal for a
new SSL training protocol. Extensive experimental results on multiple datasets
and two downstream tasks show the effectiveness of this framework.",None,-1
1f2602a3-17b9-4668-bc15-c4073bab31df,Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games,0.407755,"Large Language Models (LLMs) have demonstrated superior performance in
language understanding benchmarks. CALM, a popular approach, leverages
linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to
improve the performance in text games in Jericho without environment-provided
actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps
the LLM fixed during the learning of the text based games. In this work, we
explore and evaluate updating LLM used for candidate recommendation during the
learning of the text based game as well to mitigate the reliance on the human
annotated gameplays, which are costly to acquire. We observe that by updating
the LLM during learning using carefully selected in-game transitions, we can
reduce the dependency on using human annotated game plays for fine-tuning the
LLMs. We conducted further analysis to study the transferability of the updated
LLMs and observed that transferring in-game trained models to other games did
not result in a consistent transfer.",None,-1
adbf4517-52af-4e3f-8651-9daa32aaa390,Representation Learning for Person or Entity-centric Knowledge Graphs: An Application in Healthcare,0.0406579,"Knowledge graphs (KGs) are a popular way to organise information based on
ontologies or schemas and have been used across a variety of scenarios from
search to recommendation. Despite advances in KGs, representing knowledge
remains a non-trivial task across industries and it is especially challenging
in the biomedical and healthcare domains due to complex interdependent
relations between entities, heterogeneity, lack of standardization, and
sparseness of data. KGs are used to discover diagnoses or prioritize genes
relevant to disease, but they often rely on schemas that are not centred around
a node or entity of interest, such as a person. Entity-centric KGs are
relatively unexplored but hold promise in representing important facets
connected to a central node and unlocking downstream tasks beyond graph
traversal and reasoning, such as generating graph embeddings and training graph
neural networks for a wide range of predictive tasks. This paper presents an
end-to-end representation learning framework to extract entity-centric KGs from
structured and unstructured data. We introduce a star-shaped ontology to
represent the multiple facets of a person and use it to guide KG creation.
Compact representations of the graphs are created leveraging graph neural
networks and experiments are conducted using different levels of heterogeneity
or explicitness. A readmission prediction task is used to evaluate the results
of the proposed framework, showing a stable system, robust to missing data,
that outperforms a range of baseline machine learning classifiers. We highlight
that this approach has several potential applications across domains and is
open-sourced. Lastly, we discuss lessons learned, challenges, and next steps
for the adoption of the framework in practice.",None,-1
e6235cb0-3cb6-412c-b7ab-9d80dc8b9f9c,Predicting Sentence-Level Factuality of News and Bias of Media Outlets,0.219317,"Automated news credibility and fact-checking at scale require accurately
predicting news factuality and media bias. This paper introduces a large
sentence-level dataset, titled ""FactNews"", composed of 6,191 sentences expertly
annotated according to factuality and media bias definitions proposed by
AllSides. We use FactNews to assess the overall reliability of news sources, by
formulating two text classification problems for predicting sentence-level
factuality of news reporting and bias of media outlets. Our experiments
demonstrate that biased sentences present a higher number of words compared to
factual sentences, besides having a predominance of emotions. Hence, the
fine-grained analysis of subjectivity and impartiality of news articles
provided promising results for predicting the reliability of media outlets.
Finally, due to the severity of fake news and political polarization in Brazil,
and the lack of research for Portuguese, both dataset and baseline were
proposed for Brazilian Portuguese.",None,-1
ff6b941a-5440-4706-bd5a-89bec0588c33,PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs,0.219989,"Temporal facts, the facts for characterizing events that hold in specific
time periods, are attracting rising attention in the knowledge graph (KG)
research communities. In terms of quality management, the introduction of time
restrictions brings new challenges to maintaining the temporal consistency of
KGs and detecting potential temporal conflicts. Previous studies rely on
manually enumerated temporal constraints to detect conflicts, which are
labor-intensive and may have granularity issues. We start from the common
pattern of temporal facts and constraints and propose a pattern-based temporal
constraint mining method, PaTeCon. PaTeCon uses automatically determined graph
patterns and their relevant statistical information over the given KG instead
of human experts to generate time constraints. Specifically, PaTeCon
dynamically attaches class restriction to candidate constraints according to
their measuring scores.We evaluate PaTeCon on two large-scale datasets based on
Wikidata and Freebase respectively. The experimental results show that
pattern-based automatic constraint mining is powerful in generating valuable
temporal constraints.",None,-1
796b66dc-65b9-44c9-b328-d4a455eb1617,Knowledge Engineering using Large Language Models,0.684334,"Knowledge engineering is a discipline that focuses on the creation and
maintenance of processes that generate and apply knowledge. Traditionally,
knowledge engineering approaches have focused on knowledge expressed in formal
languages. The emergence of large language models and their capabilities to
effectively work with natural language, in its broadest sense, raises questions
about the foundations and practice of knowledge engineering. Here, we outline
the potential role of LLMs in knowledge engineering, identifying two central
directions: 1) creating hybrid neuro-symbolic knowledge systems; and 2)
enabling knowledge engineering in natural language. Additionally, we formulate
key open research questions to tackle these directions.",None,-1
4252fc9c-ec7b-4f33-9690-aae280276213,Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers,0.547752,"Relation prediction on knowledge graphs (KGs) is a key research topic.
Dominant embedding-based methods mainly focus on the transductive setting and
lack the inductive ability to generalize to new entities for inference.
Existing methods for inductive reasoning mostly mine the connections between
entities, i.e., relational paths, without considering the nature of head and
tail entities contained in the relational context. This paper proposes a novel
method that captures both connections between entities and the intrinsic nature
of entities, by simultaneously aggregating RElational Paths and cOntext with a
unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely
on relation semantics and can naturally generalize to the fully-inductive
setting, where KGs for training and inference have no common entities. In the
experiments, REPORT performs consistently better than all baselines on almost
all the eight version subsets of two fully-inductive datasets. Moreover. REPORT
is interpretable by providing each element's contribution to the prediction
results.",None,-1
d5657d19-ae50-4275-97f6-e0c5df493c02,Assessing Domain Gap for Continual Domain Adaptation in Object Detection,0.0775872,"To ensure reliable object detection in autonomous systems, the detector must
be able to adapt to changes in appearance caused by environmental factors such
as time of day, weather, and seasons. Continually adapting the detector to
incorporate these changes is a promising solution, but it can be
computationally costly. Our proposed approach is to selectively adapt the
detector only when necessary, using new data that does not have the same
distribution as the current training data. To this end, we investigate three
popular metrics for domain gap evaluation and find that there is a correlation
between the domain gap and detection accuracy. Therefore, we apply the domain
gap as a criterion to decide when to adapt the detector. Our experiments show
that our approach has the potential to improve the efficiency of the detector's
operation in real-world scenarios, where environmental conditions change in a
cyclical manner, without sacrificing the overall performance of the detector.
Our code is publicly available at https://github.com/dadung/DGE-CDA.",None,-1
7dfe3f22-6e3d-4f65-8a2d-618358076405,Sequential Integrated Gradients: a simple but effective method for explaining language models,0.445952,"Several explanation methods such as Integrated Gradients (IG) can be
characterised as path-based methods, as they rely on a straight line between
the data and an uninformative baseline. However, when applied to language
models, these methods produce a path for each word of a sentence
simultaneously, which could lead to creating sentences from interpolated words
either having no clear meaning, or having a significantly different meaning
compared to the original sentence. In order to keep the meaning of these
sentences as close as possible to the original one, we propose Sequential
Integrated Gradients (SIG), which computes the importance of each word in a
sentence by keeping fixed every other words, only creating interpolations
between the baseline and the word of interest. Moreover, inspired by the
training procedure of several language models, we also propose to replace the
baseline token ""pad"" with the trained token ""mask"". While being a simple
improvement over the original IG method, we show on various models and datasets
that SIG proves to be a very effective method for explaining language models.",None,-1
209e75c9-1507-419a-8f1d-a37bebc759af,Where Would I Go Next? Large Language Models as Human Mobility Predictors,0.854218,"Accurate human mobility prediction underpins many important applications
across a variety of domains, including epidemic modelling, transport planning,
and emergency responses. Due to the sparsity of mobility data and the
stochastic nature of people's daily activities, achieving precise predictions
of people's locations remains a challenge. While recently developed large
language models (LLMs) have demonstrated superior performance across numerous
language-related tasks, their applicability to human mobility studies remains
unexplored. Addressing this gap, this article delves into the potential of LLMs
for human mobility prediction tasks. We introduce a novel method, LLM-Mob,
which leverages the language understanding and reasoning capabilities of LLMs
for analysing human mobility data. We present concepts of historical stays and
context stays to capture both long-term and short-term dependencies in human
movement and enable time-aware prediction by using time information of the
prediction target. Additionally, we design context-inclusive prompts that
enable LLMs to generate more accurate predictions. Comprehensive evaluations of
our method reveal that LLM-Mob excels in providing accurate and interpretable
predictions, highlighting the untapped potential of LLMs in advancing human
mobility prediction techniques. We posit that our research marks a significant
paradigm shift in human mobility modelling, transitioning from building complex
domain-specific models to harnessing general-purpose LLMs that yield accurate
predictions through language instructions. The code for this work is available
at https://github.com/xlwang233/LLM-Mob.",None,-1
ad91526e-d127-4bf6-861c-ea030c70da29,TIAGo RL: Simulated Reinforcement Learning Environments with Tactile Data for Mobile Robots,0.0514631,"Tactile information is important for robust performance in robotic tasks that
involve physical interaction, such as object manipulation. However, with more
data included in the reasoning and control process, modeling behavior becomes
increasingly difficult. Deep Reinforcement Learning (DRL) produced promising
results for learning complex behavior in various domains, including
tactile-based manipulation in robotics. In this work, we present our
open-source reinforcement learning environments for the TIAGo service robot.
They produce tactile sensor measurements that resemble those of a real
sensorised gripper for TIAGo, encouraging research in transfer learning of DRL
policies. Lastly, we show preliminary training results of a learned force
control policy and compare it to a classical PI controller.",None,-1
c4565630-2b25-4053-8def-54a3b641852a,When Synthetic Data Met Regulation,0.328079,"In this paper, we argue that synthetic data produced by Differentially
Private generative models can be sufficiently anonymized and, therefore,
anonymous data and regulatory compliant.",None,-1
5c0c9f4f-ad41-4473-b427-03050ea2d618,Blind Video Deflickering by Neural Filtering with a Flawed Atlas,0.379799,"Many videos contain flickering artifacts. Common causes of flicker include
video processing algorithms, video generation algorithms, and capturing videos
under specific situations. Prior work usually requires specific guidance such
as the flickering frequency, manual annotations, or extra consistent videos to
remove the flicker. In this work, we propose a general flicker removal
framework that only receives a single flickering video as input without
additional guidance. Since it is blind to a specific flickering type or
guidance, we name this ""blind deflickering."" The core of our approach is
utilizing the neural atlas in cooperation with a neural filtering strategy. The
neural atlas is a unified representation for all frames in a video that
provides temporal consistency guidance but is flawed in many cases. To this
end, a neural network is trained to mimic a filter to learn the consistent
features (e.g., color, brightness) and avoid introducing the artifacts in the
atlas. To validate our method, we construct a dataset that contains diverse
real-world flickering videos. Extensive experiments show that our method
achieves satisfying deflickering performance and even outperforms baselines
that use extra guidance on a public benchmark.",None,-1
c6df7f26-5bc4-4e6d-9809-f7b5a972459e,"Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",0.999703,"The capability of Large Language Models (LLMs) like ChatGPT to comprehend
user intent and provide reasonable responses has made them extremely popular
lately. In this paper, we focus on assessing the overall ability of ChatGPT
using 7 fine-grained information extraction (IE) tasks. Specially, we present
the systematically analysis by measuring ChatGPT's performance, explainability,
calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT
or domain experts. Our findings reveal that ChatGPT's performance in
Standard-IE setting is poor, but it surprisingly exhibits excellent performance
in the OpenIE setting, as evidenced by human evaluation. In addition, our
research indicates that ChatGPT provides high-quality and trustworthy
explanations for its decisions. However, there is an issue of ChatGPT being
overconfident in its predictions, which resulting in low calibration.
Furthermore, ChatGPT demonstrates a high level of faithfulness to the original
text in the majority of cases. We manually annotate and release the test sets
of 7 fine-grained IE tasks contains 14 datasets to further promote the
research. The datasets and code are available at
https://github.com/pkuserc/ChatGPT_for_IE.",None,-1
3049f080-f3bb-4aa6-833d-a7cd76854853,XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models,0.906285,"Large multilingual language models typically rely on a single vocabulary
shared across 100+ languages. As these models have increased in parameter count
and depth, vocabulary size has remained largely unchanged. This
\textit{vocabulary bottleneck} limits the representational capabilities of
multilingual models like XLM-R. In this paper, we introduce a new approach for
scaling to very large multilingual vocabularies by de-emphasizing token sharing
between languages with little lexical overlap and assigning vocabulary capacity
to achieve sufficient coverage for each individual language. Tokenizations
using our vocabulary are typically more semantically meaningful and shorter
compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a
multilingual language model with a one million token vocabulary. XLM-V
outperforms XLM-R on every task we tested on ranging from natural language
inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity
recognition (WikiAnn). XLM-V is particularly effective on low-resource language
tasks and outperforms XLM-R by 11.2% and 5.8% absolute on MasakhaNER and
Americas NLI, respectively.",None,-1
4feb4e2f-ce60-4d5d-acbd-8cc250b2c2c4,OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?,0.069188,"This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale
dataset for humour generation and understanding. Humour is an abstract,
subjective, and context-dependent cognitive construct involving several
cognitive factors, making it a challenging task to generate and interpret.
Hence, humour generation and understanding can serve as a new task for
evaluating the ability of deep-learning methods to process abstract and
subjective information. Due to the scarcity of data, humour-related generation
tasks such as captioning remain under-explored. To address this gap,
OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to
train a generalizable humour captioning model. Contrary to existing captioning
datasets, OxfordTVG-HIC features a wide range of emotional and semantic
diversity resulting in out-of-context examples that are particularly conducive
to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive
content. We also show how OxfordTVG-HIC can be leveraged for evaluating the
humour of a generated text. Through explainability analysis of the trained
models, we identify the visual and linguistic cues influential for evoking
humour prediction (and generation). We observe qualitatively that these cues
are aligned with the benign violation theory of humour in cognitive psychology.",None,-1
40ef3261-c62a-46f9-a217-364f63427e96,Political claim identification and categorization in a multilingual setting: First experiments,0.495364,"The identification and classification of political claims is an important
step in the analysis of political newspaper reports; however, resources for
this task are few and far between. This paper explores different strategies for
the cross-lingual projection of political claims analysis. We conduct
experiments on a German dataset, DebateNet2.0, covering the policy debate
sparked by the 2015 refugee crisis. Our evaluation involves two tasks (claim
identification and categorization), three languages (German, English, and
French) and two methods (machine translation -- the best method in our
experiments -- and multilingual embeddings).",None,-1
14e8bb98-910c-4a62-80c9-b76dbdba0dd9,Pushdown Layers: Encoding Recursive Structure in Transformer Language Models,0.329091,"Recursion is a prominent feature of human language, and fundamentally
challenging for self-attention due to the lack of an explicit recursive-state
tracking mechanism. Consequently, Transformer language models poorly capture
long-tail recursive structure and exhibit sample-inefficient syntactic
generalization. This work introduces Pushdown Layers, a new self-attention
layer that models recursive state via a stack tape that tracks estimated depths
of every token in an incremental parse of the observed prefix. Transformer LMs
with Pushdown Layers are syntactic language models that autoregressively and
synchronously update this stack tape as they predict new tokens, in turn using
the stack tape to softly modulate attention over tokens -- for instance,
learning to ""skip"" over closed constituents. When trained on a corpus of
strings annotated with silver constituency parses, Transformers equipped with
Pushdown Layers achieve dramatically better and 3-5x more sample-efficient
syntactic generalization, while maintaining similar perplexities. Pushdown
Layers are a drop-in replacement for standard self-attention. We illustrate
this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed
WikiText-103, leading to improvements on several GLUE text classification
tasks.",None,-1
7ccf8005-02a7-484b-8be4-0eaca8196cdf,BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection on Bangla Clickbait Dataset,0.0474225,"Intentionally luring readers to click on a particular content by exploiting
their curiosity defines a title as clickbait. Although several studies focused
on detecting clickbait titles in English articles, low resource language like
Bangla has not been given adequate attention. To tackle clickbait titles in
Bangla, we have constructed the first Bangla clickbait detection dataset
containing 15,056 labeled news articles and 65,406 unlabelled news articles
extracted from clickbait dense news sites. Each article has been labeled by
three expert linguists and includes an article's title, body, and other
metadata. By incorporating labeled and unlabelled data, we finetune a
pretrained Bangla transformer model in an adversarial fashion using Semi
Supervised Generative Adversarial Networks (SS GANs). The proposed model acts
as a good baseline for this dataset, outperforming traditional neural network
models (LSTM, GRU, CNN) and linguistic feature based models. We expect that
this dataset and the detailed analysis and comparison of these clickbait
detection models will provide a fundamental basis for future research into
detecting clickbait titles in Bengali articles. We have released the
corresponding code and dataset.",None,-1
2d2f371b-25ca-4d1d-ba6c-abf164c99b9c,Rotation and Translation Invariant Representation Learning with Implicit Neural Representations,0.291554,"In many computer vision applications, images are acquired with arbitrary or
random rotations and translations, and in such setups, it is desirable to
obtain semantic representations disentangled from the image orientation.
Examples of such applications include semiconductor wafer defect inspection,
plankton microscope images, and inference on single-particle cryo-electron
microscopy (cryo-EM) micro-graphs. In this work, we propose Invariant
Representation Learning with Implicit Neural Representation (IRL-INR), which
uses an implicit neural representation (INR) with a hypernetwork to obtain
semantic representations disentangled from the orientation of the image. We
show that IRL-INR can effectively learn disentangled semantic representations
on more complex images compared to those considered in prior works and show
that these semantic representations synergize well with SCAN to produce
state-of-the-art unsupervised clustering results.",None,-1
54f51b3c-b3c8-4fdc-900c-5b3e04627e7f,TalkUp: Paving the Way for Understanding Empowering Language,0.0740225,"Empowering language is important in many real-world contexts, from education
to workplace dynamics to healthcare. Though language technologies are growing
more prevalent in these contexts, empowerment has seldom been studied in NLP,
and moreover, it is inherently challenging to operationalize because of its
implicit nature. This work builds from linguistic and social psychology
literature to explore what characterizes empowering language. We then
crowdsource a novel dataset of Reddit posts labeled for empowerment, reasons
why these posts are empowering to readers, and the social relationships between
posters and readers. Our preliminary analyses show that this dataset, which we
call TalkUp, can be used to train language models that capture empowering and
disempowering language. More broadly, TalkUp provides an avenue to explore
implication, presuppositions, and how social context influences the meaning of
language.",None,-1
422de3eb-1a40-4c41-b72e-3ba45c132bd3,"Label Smarter, Not Harder: CleverLabel for Faster Annotation of Ambiguous Image Classification with Higher Quality",0.123366,"High-quality data is crucial for the success of machine learning, but
labeling large datasets is often a time-consuming and costly process. While
semi-supervised learning can help mitigate the need for labeled data, label
quality remains an open issue due to ambiguity and disagreement among
annotators. Thus, we use proposal-guided annotations as one option which leads
to more consistency between annotators. However, proposing a label increases
the probability of the annotators deciding in favor of this specific label.
This introduces a bias which we can simulate and remove. We propose a new
method CleverLabel for Cost-effective LabEling using Validated proposal-guidEd
annotations and Repaired LABELs. CleverLabel can reduce labeling costs by up to
30.0%, while achieving a relative improvement in Kullback-Leibler divergence of
up to 29.8% compared to the previous state-of-the-art on a multi-domain
real-world image classification benchmark. CleverLabel offers a novel solution
to the challenge of efficiently labeling large datasets while also improving
the label quality.",None,-1
9a9cb5a5-3f6a-48ab-b479-9a3548a38437,Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking,0.890582,"This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT)
framework. It emphasizes spatio-temporal continuity and integrates both past
and future reasoning for tracked objects. Thus, we name it ""Past-and-Future
reasoning for Tracking"" (PF-Track). Specifically, our method adapts the
""tracking by attention"" framework and represents tracked instances coherently
over time with object queries. To explicitly use historical cues, our ""Past
Reasoning"" module learns to refine the tracks and enhance the object features
by cross-attending to queries from previous frames and other objects. The
""Future Reasoning"" module digests historical information and predicts robust
future trajectories. In the case of long-term occlusions, our method maintains
the object positions and enables re-association by integrating motion
predictions. On the nuScenes dataset, our method improves AMOTA by a large
margin and remarkably reduces ID-Switches by 90% compared to prior approaches,
which is an order of magnitude less. The code and models are made available at
https://github.com/TRI-ML/PF-Track.",None,-1
7c312df0-ccf6-4d18-9949-9b9ecafdfba0,CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation,0.544497,"Beyond novel view synthesis, Neural Radiance Fields are useful for
applications that interact with the real world. In this paper, we use them as
an implicit map of a given scene and propose a camera relocalization algorithm
tailored for this representation. The proposed method enables to compute in
real-time the precise position of a device using a single RGB camera, during
its navigation. In contrast with previous work, we do not rely on pose
regression or photometric alignment but rather use dense local features
obtained through volumetric rendering which are specialized on the scene with a
self-supervised objective. As a result, our algorithm is more accurate than
competitors, able to operate in dynamic outdoor environments with changing
lightning conditions and can be readily integrated in any volumetric neural
renderer.",None,-1
764dfebd-5444-4de3-b78d-ef8c613170c2,Deep learning model for Mongolian Citizens Feedback Analysis using Word Vector Embeddings,0.103444,"A large amount of feedback was collected over the years. Many feedback
analysis models have been developed focusing on the English language.
Recognizing the concept of feedback is challenging and crucial in languages
which do not have applicable corpus and tools employed in Natural Language
Processing (i.e., vocabulary corpus, sentence structure rules, etc). However,
in this paper, we study a feedback classification in Mongolian language using
two different word embeddings for deep learning. We compare the results of
proposed approaches. We use feedback data in Cyrillic collected from 2012-2018.
The result indicates that word embeddings using their own dataset improve the
deep learning based proposed model with the best accuracy of 80.1% and 82.7%
for two classification tasks.",None,-1
bd3d74d6-532c-4fc0-83ef-3af04924c141,Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI,0.867297,"Explainable AI (XAI) systems are sociotechnical in nature; thus, they are
subject to the sociotechnical gap--divide between the technical affordances and
the social needs. However, charting this gap is challenging. In the context of
XAI, we argue that charting the gap improves our problem understanding, which
can reflexively provide actionable insights to improve explainability.
Utilizing two case studies in distinct domains, we empirically derive a
framework that facilitates systematic charting of the sociotechnical gap by
connecting AI guidelines in the context of XAI and elucidating how to use them
to address the gap. We apply the framework to a third case in a new domain,
showcasing its affordances. Finally, we discuss conceptual implications of the
framework, share practical considerations in its operationalization, and offer
guidance on transferring it to new contexts. By making conceptual and practical
contributions to understanding the sociotechnical gap in XAI, the framework
expands the XAI design space.",None,-1
1287d3c0-3979-4eee-9d25-56d8cc7e43d2,FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow,0.648888,"Reconstruction of 3D neural fields from posed images has emerged as a
promising method for self-supervised representation learning. The key challenge
preventing the deployment of these 3D scene learners on large-scale video data
is their dependence on precise camera poses from structure-from-motion, which
is prohibitively expensive to run at scale. We propose a method that jointly
reconstructs camera poses and 3D neural scene representations online and in a
single forward pass. We estimate poses by first lifting frame-to-frame optical
flow to 3D scene flow via differentiable rendering, preserving locality and
shift-equivariance of the image processing backbone. SE(3) camera pose
estimation is then performed via a weighted least-squares fit to the scene flow
field. This formulation enables us to jointly supervise pose estimation and a
generalizable neural scene representation via re-rendering the input video, and
thus, train end-to-end and fully self-supervised on real-world video datasets.
We demonstrate that our method performs robustly on diverse, real-world video,
notably on sequences traditionally challenging to optimization-based pose
estimation techniques.",None,-1
f054f2c5-eda7-42bd-9f14-5551435aae55,Implicit State and Goals in QBF Encodings for Positional Games (extended version),0.797707,"We address two bottlenecks for concise QBF encodings of maker-breaker
positional games, like Hex and Tic-Tac-Toe. Our baseline is a QBF encoding with
explicit variables for board positions and an explicit representation of
winning configurations. The first improvement is inspired by lifted planning
and avoids variables for explicit board positions, introducing a universal
quantifier representing a symbolic board state. The second improvement
represents the winning configurations implicitly, exploiting their structure.
The paper evaluates the size of several encodings, depending on board size and
game depth. It also reports the performance of QBF solvers on these encodings.
We evaluate the techniques on Hex instances and also apply them to Harary's
Tic-Tac-Toe. In particular, we study scalability to 19$\times$19 boards, played
in human Hex tournaments.",None,-1
387a6210-d4f4-494d-b9e9-215ca9917b0b,How Good is the Model in Model-in-the-loop Event Coreference Resolution Annotation?,0.743904,"Annotating cross-document event coreference links is a time-consuming and
cognitively demanding task that can compromise annotation quality and
efficiency. To address this, we propose a model-in-the-loop annotation approach
for event coreference resolution, where a machine learning model suggests
likely corefering event pairs only. We evaluate the effectiveness of this
approach by first simulating the annotation process and then, using a novel
annotator-centric Recall-Annotation effort trade-off metric, we compare the
results of various underlying models and datasets. We finally present a method
for obtaining 97\% recall while substantially reducing the workload required by
a fully manual annotation process. Code and data can be found at
https://github.com/ahmeshaf/model_in_coref",None,-1
318dbab6-84ad-4799-866f-085cafc9ae5d,HDR image watermarking using saliency detection and quantization index modulation,0.229214,"High-dynamic range (HDR) images are circulated rapidly over the internet with
risks of being exploited for unauthorized usage. To protect these images, some
HDR image based watermarking (HDR-IW) methods were put forward. However, they
inherited the same problem faced by conventional IW methods for standard
dynamic range (SDR) images, where only trade-offs among conflicting
requirements are managed instead of simultaneous improvement. In this paper, a
novel saliency (eye-catching object) detection based trade-off independent
HDR-IW is proposed, to simultaneously improve robustness, imperceptibility and
payload. First, the host image goes through our proposed salient object
detection model to produce a saliency map, which is, in turn, exploited to
segment the foreground and background of the host image. Next, the binary
watermark is partitioned into the foregrounds and backgrounds using the same
mask and scrambled using a random permutation algorithm. Finally, the watermark
segments are embedded into selected bit-plane of the corresponding host
segments using quantized indexed modulation. Experimental results suggest that
the proposed work outperforms state-of-the-art methods in terms of improving
the conflicting requirements.",None,-1
cbf9794e-d0a2-4aa5-90a0-c8807dbd85ee,Double A3C: Deep Reinforcement Learning on OpenAI Gym Games,0.0548555,"Reinforcement Learning (RL) is an area of machine learning figuring out how
agents take actions in an unknown environment to maximize its rewards. Unlike
classical Markov Decision Process (MDP) in which agent has full knowledge of
its state, rewards, and transitional probability, reinforcement learning
utilizes exploration and exploitation for the model uncertainty. Under the
condition that the model usually has a large state space, a neural network (NN)
can be used to correlate its input state to its output actions to maximize the
agent's rewards. However, building and training an efficient neural network is
challenging. Inspired by Double Q-learning and Asynchronous Advantage
Actor-Critic (A3C) algorithm, we will propose and implement an improved version
of Double A3C algorithm which utilizing the strength of both algorithms to play
OpenAI Gym Atari 2600 games to beat its benchmarks for our project.",None,-1
a930dcba-aeef-4520-91e2-87dffd0a3d6e,Evaluating Hallucinations in Chinese Large Language Models,0.35762,"In this paper, we establish a benchmark named HalluQA (Chinese Hallucination
Question-Answering) to measure the hallucination phenomenon in Chinese large
language models. HalluQA contains 450 meticulously designed adversarial
questions, spanning multiple domains, and takes into account Chinese historical
culture, customs, and social phenomena. During the construction of HalluQA, we
consider two types of hallucinations: imitative falsehoods and factual errors,
and we construct adversarial samples based on GLM-130B and ChatGPT. For
evaluation, we design an automated evaluation method using GPT-4 to judge
whether a model output is hallucinated. We conduct extensive experiments on 24
large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk
and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than
50%. This indicates that HalluQA is highly challenging. We analyze the primary
types of hallucinations in different types of models and their causes.
Additionally, we discuss which types of hallucinations should be prioritized
for different types of models.",None,-1
ea1342a6-ef1f-4660-848b-9554a5814f40,"Abductive Reasoning with the GPT-4 Language Model: Case studies from criminal investigation, medical practice, scientific research",0.0229043,"This study evaluates the GPT-4 Large Language Model's abductive reasoning in
complex fields like medical diagnostics, criminology, and cosmology. Using an
interactive interview format, the AI assistant demonstrated reliability in
generating and selecting hypotheses. It inferred plausible medical diagnoses
based on patient data and provided potential causes and explanations in
criminology and cosmology. The results highlight the potential of LLMs in
complex problem-solving and the need for further research to maximize their
practical applications.",None,-1
0d5964dc-9952-442e-a271-6bc944c09fc9,ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation,0.981731,"We introduce ""ImageDream,"" an innovative image-prompt, multi-view diffusion
model for 3D object generation. ImageDream stands out for its ability to
produce 3D models of higher quality compared to existing state-of-the-art,
image-conditioned methods. Our approach utilizes a canonical camera
coordination for the objects in images, improving visual geometry accuracy. The
model is designed with various levels of control at each block inside the
diffusion model based on the input image, where global control shapes the
overall object layout and local control fine-tunes the image details. The
effectiveness of ImageDream is demonstrated through extensive evaluations using
a standard prompt list. For more information, visit our project page at
https://Image-Dream.github.io.",None,-1
26b32978-2f65-4e9a-a3f8-924587a16e39,Towards predicting Pedestrian Evacuation Time and Density from Floorplans using a Vision Transformer,0.673457,"Conventional pedestrian simulators are inevitable tools in the design process
of a building, as they enable project engineers to prevent overcrowding
situations and plan escape routes for evacuation. However, simulation runtime
and the multiple cumbersome steps in generating simulation results are
potential bottlenecks during the building design process. Data-driven
approaches have demonstrated their capability to outperform conventional
methods in speed while delivering similar or even better results across many
disciplines. In this work, we present a deep learning-based approach based on a
Vision Transformer to predict density heatmaps over time and total evacuation
time from a given floorplan. Specifically, due to limited availability of
public datasets, we implement a parametric data generation pipeline including a
conventional simulator. This enables us to build a large synthetic dataset that
we use to train our architecture. Furthermore, we seamlessly integrate our
model into a BIM-authoring tool to generate simulation results instantly and
automatically.",None,-1
335c656e-e484-4cf2-afe6-286a76d0c4d7,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,0.520159,"The large number of ReLU non-linearity operations in existing deep neural
networks makes them ill-suited for latency-efficient private inference (PI).
Existing techniques to reduce ReLU operations often involve manual effort and
sacrifice significant accuracy. In this paper, we first present a novel measure
of non-linearity layers' ReLU sensitivity, enabling mitigation of the
time-consuming manual efforts in identifying the same. Based on this
sensitivity, we then present SENet, a three-stage training method that for a
given ReLU budget, automatically assigns per-layer ReLU counts, decides the
ReLU locations for each layer's activation map, and trains a model with
significantly fewer ReLUs to potentially yield latency and communication
efficient PI. Experimental evaluations with multiple models on various datasets
show SENet's superior performance both in terms of reduced ReLUs and improved
classification accuracy compared to existing alternatives. In particular, SENet
can yield models that require up to ~2x fewer ReLUs while yielding similar
accuracy. For a similar ReLU budget SENet can yield models with ~2.32% improved
classification accuracy, evaluated on CIFAR-100.",None,-1
f6f38d7e-37ad-4136-ab70-75b4d407cbf5,EVOLIN Benchmark: Evaluation of Line Detection and Association,0.32487,"Lines are interesting geometrical features commonly seen in indoor and urban
environments. There is missing a complete benchmark where one can evaluate
lines from a sequential stream of images in all its stages: Line detection,
Line Association and Pose error. To do so, we present a complete and exhaustive
benchmark for visual lines in a SLAM front-end, both for RGB and RGBD, by
providing a plethora of complementary metrics. We have also labelled data from
well-known SLAM datasets in order to have all in one poses and accurately
annotated lines. In particular, we have evaluated 17 line detection algorithms,
5 line associations methods and the resultant pose error for aligning a pair of
frames with several combinations of detector-association. We have packaged all
methods and evaluations metrics and made them publicly available on web-page
https://prime-slam.github.io/evolin/.",None,-1
9d461b13-e887-4361-ae05-67eeabb476d3,Regret Bounds for Markov Decision Processes with Recursive Optimized Certainty Equivalents,0.78911,"The optimized certainty equivalent (OCE) is a family of risk measures that
cover important examples such as entropic risk, conditional value-at-risk and
mean-variance models. In this paper, we propose a new episodic risk-sensitive
reinforcement learning formulation based on tabular Markov decision processes
with recursive OCEs. We design an efficient learning algorithm for this problem
based on value iteration and upper confidence bound. We derive an upper bound
on the regret of the proposed algorithm, and also establish a minimax lower
bound. Our bounds show that the regret rate achieved by our proposed algorithm
has optimal dependence on the number of episodes and the number of actions.",None,-1
78373268-0602-4c69-b764-9cf467374179,ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing,0.893339,"English and Chinese, known as resource-rich languages, have witnessed the
strong development of transformer-based language models for natural language
processing tasks. Although Vietnam has approximately 100M people speaking
Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,
performed well on general Vietnamese NLP tasks, including POS tagging and named
entity recognition. These pre-trained language models are still limited to
Vietnamese social media tasks. In this paper, we present the first monolingual
pre-trained language model for Vietnamese social media texts, ViSoBERT, which
is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese
social media texts using XLM-R architecture. Moreover, we explored our
pre-trained model on five important natural language downstream tasks on
Vietnamese social media texts: emotion recognition, hate speech detection,
sentiment analysis, spam reviews detection, and hate speech spans detection.
Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses
the previous state-of-the-art models on multiple Vietnamese social media tasks.
Our ViSoBERT model is available only for research purposes.",None,-1
bbea2e83-ac8f-4d6f-a752-d7b18806ec17,Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications,0.789293,"Large-scale pre-trained language models have shown outstanding performance in
a variety of NLP tasks. However, they are also known to be significantly
brittle against specifically crafted adversarial examples, leading to
increasing interest in probing the adversarial robustness of NLP systems. We
introduce RSMI, a novel two-stage framework that combines randomized smoothing
(RS) with masked inference (MI) to improve the adversarial robustness of NLP
systems. RS transforms a classifier into a smoothed classifier to obtain robust
representations, whereas MI forces a model to exploit the surrounding context
of a masked token in an input sequence. RSMI improves adversarial robustness by
2 to 3 times over existing state-of-the-art methods on benchmark datasets. We
also perform in-depth qualitative analysis to validate the effectiveness of the
different stages of RSMI and probe the impact of its components through
extensive ablations. By empirically proving the stability of RSMI, we put it
forward as a practical method to robustly train large-scale NLP models. Our
code and datasets are available at https://github.com/Han8931/rsmi_nlp",None,-1
89af46bb-a440-4bb4-8324-00de80a76eed,TAA-GCN: A Temporally Aware Adaptive Graph Convolutional Network for Age Estimation,0.41454,"This paper proposes a novel age estimation algorithm, the Temporally-Aware
Adaptive Graph Convolutional Network (TAA-GCN). Using a new representation
based on graphs, the TAA-GCN utilizes skeletal, posture, clothing, and facial
information to enrich the feature set associated with various ages. Such a
novel graph representation has several advantages: First, reduced sensitivity
to facial expression and other appearance variances; Second, robustness to
partial occlusion and non-frontal-planar viewpoint, which is commonplace in
real-world applications such as video surveillance. The TAA-GCN employs two
novel components, (1) the Temporal Memory Module (TMM) to compute temporal
dependencies in age; (2) Adaptive Graph Convolutional Layer (AGCL) to refine
the graphs and accommodate the variance in appearance. The TAA-GCN outperforms
the state-of-the-art methods on four public benchmarks, UTKFace, MORPHII, CACD,
and FG-NET. Moreover, the TAA-GCN showed reliability in different camera
viewpoints and reduced quality images.",None,-1
5cb3b6fe-052f-42d6-adb8-8627aecf80ff,USTC-NELSLIP at SemEval-2023 Task 2: Statistical Construction and Dual Adaptation of Gazetteer for Multilingual Complex NER,0.376754,"This paper describes the system developed by the USTC-NELSLIP team for
SemEval-2023 Task 2 Multilingual Complex Named Entity Recognition (MultiCoNER
II). A method named Statistical Construction and Dual Adaptation of Gazetteer
(SCDAG) is proposed for Multilingual Complex NER. The method first utilizes a
statistics-based approach to construct a gazetteer. Secondly, the
representations of gazetteer networks and language models are adapted by
minimizing the KL divergence between them at both the sentence-level and
entity-level. Finally, these two networks are then integrated for supervised
named entity recognition (NER) training. The proposed method is applied to
XLM-R with a gazetteer built from Wikidata, and shows great generalization
ability across different tracks. Experimental results and detailed analysis
verify the effectiveness of the proposed method. The official results show that
our system ranked 1st on one track (Hindi) in this task.",None,-1
b5578b37-206b-4253-bd0a-71c1c895e7fc,Leveraging Large Language Model for Automatic Evolving of Industrial Data-Centric R&D Cycle,0.573803,"In the wake of relentless digital transformation, data-driven solutions are
emerging as powerful tools to address multifarious industrial tasks such as
forecasting, anomaly detection, planning, and even complex decision-making.
Although data-centric R&D has been pivotal in harnessing these solutions, it
often comes with significant costs in terms of human, computational, and time
resources. This paper delves into the potential of large language models (LLMs)
to expedite the evolution cycle of data-centric R&D. Assessing the foundational
elements of data-centric R&D, including heterogeneous task-related data,
multi-facet domain knowledge, and diverse computing-functional tools, we
explore how well LLMs can understand domain-specific requirements, generate
professional ideas, utilize domain-specific tools to conduct experiments,
interpret results, and incorporate knowledge from past endeavors to tackle new
challenges. We take quantitative investment research as a typical example of
industrial data-centric R&D scenario and verified our proposed framework upon
our full-stack open-sourced quantitative research platform Qlib and obtained
promising results which shed light on our vision of automatic evolving of
industrial data-centric R&D cycle.",None,-1
006bf871-9402-4367-a92a-1833520815a9,PyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow,0.807914,"During industrial processing, unforeseen defects may arise in products due to
uncontrollable factors. Although unsupervised methods have been successful in
defect localization, the usual use of pre-trained models results in
low-resolution outputs, which damages visual performance. To address this
issue, we propose PyramidFlow, the first fully normalizing flow method without
pre-trained models that enables high-resolution defect localization.
Specifically, we propose a latent template-based defect contrastive
localization paradigm to reduce intra-class variance, as the pre-trained models
do. In addition, PyramidFlow utilizes pyramid-like normalizing flows for
multi-scale fusing and volume normalization to help generalization. Our
comprehensive studies on MVTecAD demonstrate the proposed method outperforms
the comparable algorithms that do not use external priors, even achieving
state-of-the-art performance in more challenging BTAD scenarios.",None,-1
cd6b1a7a-7bfc-4cc8-a31f-d5cf3a1c2ca3,MoDAR: Using Motion Forecasting for 3D Object Detection in Point Cloud Sequences,0.531145,"Occluded and long-range objects are ubiquitous and challenging for 3D object
detection. Point cloud sequence data provide unique opportunities to improve
such cases, as an occluded or distant object can be observed from different
viewpoints or gets better visibility over time. However, the efficiency and
effectiveness in encoding long-term sequence data can still be improved. In
this work, we propose MoDAR, using motion forecasting outputs as a type of
virtual modality, to augment LiDAR point clouds. The MoDAR modality propagates
object information from temporal contexts to a target frame, represented as a
set of virtual points, one for each object from a waypoint on a forecasted
trajectory. A fused point cloud of both raw sensor points and the virtual
points can then be fed to any off-the-shelf point-cloud based 3D object
detector. Evaluated on the Waymo Open Dataset, our method significantly
improves prior art detectors by using motion forecasting from extra-long
sequences (e.g. 18 seconds), achieving new state of the arts, while not adding
much computation overhead.",None,-1
af225169-c2fd-463a-96f6-31df8ab7f473,Not Only Generative Art: Stable Diffusion for Content-Style Disentanglement in Art Analysis,0.371777,"The duality of content and style is inherent to the nature of art. For
humans, these two elements are clearly different: content refers to the objects
and concepts in the piece of art, and style to the way it is expressed. This
duality poses an important challenge for computer vision. The visual appearance
of objects and concepts is modulated by the style that may reflect the author's
emotions, social trends, artistic movement, etc., and their deep comprehension
undoubtfully requires to handle both. A promising step towards a general
paradigm for art analysis is to disentangle content and style, whereas relying
on human annotations to cull a single aspect of artworks has limitations in
learning semantic concepts and the visual appearance of paintings. We thus
present GOYA, a method that distills the artistic knowledge captured in a
recent generative model to disentangle content and style. Experiments show that
synthetically generated images sufficiently serve as a proxy of the real
distribution of artworks, allowing GOYA to separately represent the two
elements of art while keeping more information than existing methods.",None,-1
7ec12ec1-147b-46a6-8a27-af763413a059,Cone: Unsupervised Contrastive Opinion Extraction,0.251943,"Contrastive opinion extraction aims to extract a structured summary or key
points organised as positive and negative viewpoints towards a common aspect or
topic. Most recent works for unsupervised key point extraction is largely built
on sentence clustering or opinion summarisation based on the popularity of
opinions expressed in text. However, these methods tend to generate aspect
clusters with incoherent sentences, conflicting viewpoints, redundant aspects.
To address these problems, we propose a novel unsupervised Contrastive OpinioN
Extraction model, called Cone, which learns disentangled latent aspect and
sentiment representations based on pseudo aspect and sentiment labels by
combining contrastive learning with iterative aspect/sentiment clustering
refinement. Apart from being able to extract contrastive opinions, it is also
able to quantify the relative popularity of aspects and their associated
sentiment distributions. The model has been evaluated on both a hotel review
dataset and a Twitter dataset about COVID vaccines. The results show that
despite using no label supervision or aspect-denoted seed words, Cone
outperforms a number of competitive baselines on contrastive opinion
extraction. The results of Cone can be used to offer a better recommendation of
products and services online.",None,-1
d3544f91-e105-4845-a762-f975974f6156,sustain.AI: a Recommender System to analyze Sustainability Reports,0.28515,"We present sustainAI, an intelligent, context-aware recommender system that
assists auditors and financial investors as well as the general public to
efficiently analyze companies' sustainability reports. The tool leverages an
end-to-end trainable architecture that couples a BERT-based encoding module
with a multi-label classification head to match relevant text passages from
sustainability reports to their respective law regulations from the Global
Reporting Initiative (GRI) standards. We evaluate our model on two novel German
sustainability reporting data sets and consistently achieve a significantly
higher recommendation performance compared to multiple strong baselines.
Furthermore, sustainAI is publicly available for everyone at
https://sustain.ki.nrw/.",None,-1
d57df2ca-6452-43f4-8105-de1c5deecd17,RuSentNE-2023: Evaluating Entity-Oriented Sentiment Analysis on Russian News Texts,0.507972,"The paper describes the RuSentNE-2023 evaluation devoted to targeted
sentiment analysis in Russian news texts. The task is to predict sentiment
towards a named entity in a single sentence. The dataset for RuSentNE-2023
evaluation is based on the Russian news corpus RuSentNE having rich
sentiment-related annotation. The corpus is annotated with named entities and
sentiments towards these entities, along with related effects and emotional
states. The evaluation was organized using the CodaLab competition framework.
The main evaluation measure was macro-averaged measure of positive and negative
classes. The best results achieved were of 66% Macro F-measure
(Positive+Negative classes). We also tested ChatGPT on the test set from our
evaluation and found that the zero-shot answers provided by ChatGPT reached 60%
of the F-measure, which corresponds to 4th place in the evaluation. ChatGPT
also provided detailed explanations of its conclusion. This can be considered
as quite high for zero-shot application.",None,-1
6713b485-2d2a-4470-9586-11338a0fc088,Heuristic Algorithms for the Approximation of Mutual Coherence,0.127092,"Mutual coherence is a measure of similarity between two opinions. Although
the notion comes from philosophy, it is essential for a wide range of
technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters
to find candidates that are the closest to their political preferences. The
exact computation of mutual coherence is highly time-consuming due to the
iteration over all subsets of an opinion. Moreover, for every subset, an
instance of the SAT model counting problem has to be solved which is known to
be a hard problem in computer science. This work is the first study to
accelerate this computation. We model the distribution of the so-called
confirmation values as a mixture of three Gaussians and present efficient
heuristics to estimate its model parameters. The mutual coherence is then
approximated with the expected value of the distribution. Some of the presented
algorithms are fully polynomial-time, others only require solving a small
number of instances of the SAT model counting problem. The average squared
error of our best algorithm lies below 0.0035 which is insignificant if the
efficiency is taken into account. Furthermore, the accuracy is precise enough
to be used in Wahl-O-Mat-like systems.",None,-1
85c06942-18b1-4d5b-99bb-22cd468db51c,TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation,0.855161,"Test-time adaptation methods have been gaining attention recently as a
practical solution for addressing source-to-target domain gaps by gradually
updating the model without requiring labels on the target data. In this paper,
we propose a method of test-time adaptation for category-level object pose
estimation called TTA-COPE. We design a pose ensemble approach with a
self-training loss using pose-aware confidence. Unlike previous unsupervised
domain adaptation methods for category-level object pose estimation, our
approach processes the test data in a sequential, online manner, and it does
not require access to the source domain at runtime. Extensive experimental
results demonstrate that the proposed pose ensemble and the self-training loss
improve category-level object pose performance during test time under both
semi-supervised and unsupervised settings. Project page:
https://taeyeop.com/ttacope",None,-1
8ad81478-4be0-4cad-88b7-f03e06102ff3,Exploring Effective Mask Sampling Modeling for Neural Image Compression,0.451188,"Image compression aims to reduce the information redundancy in images. Most
existing neural image compression methods rely on side information from
hyperprior or context models to eliminate spatial redundancy, but rarely
address the channel redundancy. Inspired by the mask sampling modeling in
recent self-supervised learning methods for natural language processing and
high-level vision, we propose a novel pretraining strategy for neural image
compression. Specifically, Cube Mask Sampling Module (CMSM) is proposed to
apply both spatial and channel mask sampling modeling to image compression in
the pre-training stage. Moreover, to further reduce channel redundancy, we
propose the Learnable Channel Mask Module (LCMM) and the Learnable Channel
Completion Module (LCCM). Our plug-and-play CMSM, LCMM, LCCM modules can apply
to both CNN-based and Transformer-based architectures, significantly reduce the
computational cost, and improve the quality of images. Experiments on the
public Kodak and Tecnick datasets demonstrate that our method achieves
competitive performance with lower computational complexity compared to
state-of-the-art image compression methods.",None,-1
5f8720ac-8dfa-4a9a-92cb-d7af4a478140,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,0.453255,"We present a data-driven generative framework for synthesizing blood vessel
3D geometry. This is a challenging task due to the complexity of vascular
systems, which are highly variating in shape, size, and structure. Existing
model-based methods provide some degree of control and variation in the
structures produced, but fail to capture the diversity of actual anatomical
data. We developed VesselVAE, a recursive variational Neural Network that fully
exploits the hierarchical organization of the vessel and learns a
low-dimensional manifold encoding branch connectivity along with geometry
features describing the target surface. After training, the VesselVAE latent
space can be sampled to generate new vessel geometries. To the best of our
knowledge, this work is the first to utilize this technique for synthesizing
blood vessels. We achieve similarities of synthetic and real data for radius
(.97), length (.95), and tortuosity (.96). By leveraging the power of deep
neural networks, we generate 3D models of blood vessels that are both accurate
and diverse, which is crucial for medical and surgical training, hemodynamic
simulations, and many other purposes.",None,-1
b18f5b7b-d84a-4c05-b7a2-73668298d7e7,HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models,0.12595,"Recent progress in generative AI, including large language models (LLMs) like
ChatGPT, has opened up significant opportunities in fields ranging from natural
language processing to knowledge discovery and data mining. However, there is
also a growing awareness that the models can be prone to problems such as
making information up or `hallucinations', and faulty reasoning on seemingly
simple problems. Because of the popularity of models like ChatGPT, both
academic scholars and citizen scientists have documented hallucinations of
several different types and severity. Despite this body of work, a formal model
for describing and representing these hallucinations (with relevant meta-data)
at a fine-grained level, is still lacking. In this paper, we address this gap
by presenting the Hallucination Ontology or HALO, a formal, extensible ontology
written in OWL that currently offers support for six different types of
hallucinations known to arise in LLMs, along with support for provenance and
experimental metadata. We also collect and publish a dataset containing
hallucinations that we inductively gathered across multiple independent Web
sources, and show that HALO can be successfully used to model this dataset and
answer competency questions.",None,-1
ac8244ad-c570-4d98-97e5-2af526825e55,Hierarchical Neural Coding for Controllable CAD Model Generation,0.149513,"This paper presents a novel generative model for Computer Aided Design (CAD)
that 1) represents high-level design concepts of a CAD model as a three-level
hierarchical tree of neural codes, from global part arrangement down to local
curve geometry; and 2) controls the generation or completion of CAD models by
specifying the target design using a code tree. Concretely, a novel variant of
a vector quantized VAE with ""masked skip connection"" extracts design variations
as neural codebooks at three levels. Two-stage cascaded auto-regressive
transformers learn to generate code trees from incomplete CAD models and then
complete CAD models following the intended design. Extensive experiments
demonstrate superior performance on conventional tasks such as random
generation while enabling novel interaction capabilities on conditional
generation tasks. The code is available at
https://github.com/samxuxiang/hnc-cad.",None,-1
0d0f03d9-6de2-4d9d-a5dc-84c64611fd44,Pre-training Multi-party Dialogue Models with Latent Discourse Inference,0.305514,"Multi-party dialogues are more difficult for models to understand than
one-to-one two-party dialogues, since they involve multiple interlocutors,
resulting in interweaving reply-to relations and information flows. To step
over these obstacles, an effective way is to pre-train a model that understands
the discourse structure of multi-party dialogues, namely, to whom each
utterance is replying. However, due to the lack of explicitly annotated
discourse labels in multi-party dialogue corpora, previous works fail to scale
up the pre-training process by putting aside the unlabeled multi-party
conversational data for nothing. To fully utilize the unlabeled data, we
propose to treat the discourse structures as latent variables, then jointly
infer them and pre-train the discourse-aware model by unsupervised latent
variable inference methods. Experiments on multiple downstream tasks show that
our pre-trained model outperforms strong baselines by large margins and
achieves state-of-the-art (SOTA) results, justifying the effectiveness of our
method. The official implementation of this paper is available at
https://github.com/EricLee8/MPD_EMVI.",None,-1
9b555769-aaf6-4456-8d72-43d5f6ec83d9,HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation,0.628845,"In this paper, we study Text-to-3D content generation leveraging 2D diffusion
priors to enhance the quality and detail of the generated 3D models. Recent
progress (Magic3D) in text-to-3D has shown that employing high-resolution
(e.g., 512 x 512) renderings can lead to the production of high-quality 3D
models using latent diffusion priors. To enable rendering at even higher
resolutions, which has the potential to further augment the quality and detail
of the models, we propose a novel approach that combines multiple noise
estimation processes with a pretrained 2D diffusion prior. Distinct from the
Bar-Tal et al.s' study which binds multiple denoised results to generate images
from texts, our approach integrates the computation of scoring distillation
losses such as SDS loss and VSD loss which are essential techniques for the 3D
content generation with 2D diffusion priors. We experimentally evaluated the
proposed approach. The results show that the proposed approach can generate
high-quality details compared to the baselines.",None,-1
e783af69-bb23-4622-8e91-75cb31c40b25,"Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results",0.249898,"As AI-bots continue to gain popularity due to their human-like traits and the
intimacy they offer to users, their societal impact inevitably expands. This
leads to the rising necessity for comprehensive studies to fully understand
AI-bots and reveal their potential opportunities, drawbacks, and overall
societal impact. With that in mind, this research conducted an extensive
investigation into ChatGPT3, a renowned AI bot, aiming to assess the temporal
reliability of its personality profile. Psychological questionnaires were
administered to the chatbot on two separate occasions, followed by a comparison
of the responses to human normative data. The findings revealed varying levels
of agreement in chatbot's responses over time, with some scales displaying
excellent agreement while others demonstrated poor agreement. Overall,
Davinci-003 displayed a socially desirable and pro-social personality profile,
particularly in the domain of communion. However, the underlying basis of the
chatbot's responses-whether driven by conscious self reflection or
predetermined algorithms-remains uncertain.",None,-1
724243a6-87b1-47a6-bd4d-c27e5efd58a5,Experiments with Detecting and Mitigating AI Deception,0.0294356,"How to detect and mitigate deceptive AI systems is an open problem for the
field of safe and trustworthy AI. We analyse two algorithms for mitigating
deception: The first is based on the path-specific objectives framework where
paths in the game that incentivise deception are removed. The second is based
on shielding, i.e., monitoring for unsafe policies and replacing them with a
safe reference policy. We construct two simple games and evaluate our
algorithms empirically. We find that both methods ensure that our agent is not
deceptive, however, shielding tends to achieve higher reward.",None,-1
267f6654-785d-4059-8211-a303756d50a9,Semantic Feature Verification in FLAN-T5,0.35718,"This study evaluates the potential of a large language model for aiding in
generation of semantic feature norms - a critical tool for evaluating
conceptual structure in cognitive science. Building from an existing
human-generated dataset, we show that machine-verified norms capture aspects of
conceptual structure beyond what is expressed in human norms alone, and better
explain human judgments of semantic similarity amongst items that are distally
related. The results suggest that LLMs can greatly enhance traditional methods
of semantic feature norm verification, with implications for our understanding
of conceptual representation in humans and machines.",None,-1
d8ed4701-f02a-4614-ab07-d55d9680dbb8,Multi-Scale Occ: 4th Place Solution for CVPR 2023 3D Occupancy Prediction Challenge,0.400175,"In this report, we present the 4th place solution for CVPR 2023 3D occupancy
prediction challenge. We propose a simple method called Multi-Scale Occ for
occupancy prediction based on lift-splat-shoot framework, which introduces
multi-scale image features for generating better multi-scale 3D voxel features
with temporal fusion of multiple past frames. Post-processing including model
ensemble, test-time augmentation, and class-wise thresh are adopted to further
boost the final performance. As shown on the leaderboard, our proposed
occupancy prediction method ranks the 4th place with 49.36 mIoU.",None,-1
15a5dac8-0374-42c0-aac5-55e37df5cf5b,Automatic Number Plate Recognition using Random Forest Classifier,0.59768,"Automatic Number Plate Recognition System (ANPRS) is a mass surveillance
embedded system that recognizes the number plate of the vehicle. This system is
generally used for traffic management applications. It should be very efficient
in detecting the number plate in noisy as well as in low illumination and also
within required time frame. This paper proposes a number plate recognition
method by processing vehicle's rear or front image. After image is captured,
processing is divided into four steps which are Pre-Processing, Number plate
localization, Character segmentation and Character recognition. Pre-Processing
enhances the image for further processing, number plate localization extracts
the number plate region from the image, character segmentation separates the
individual characters from the extracted number plate and character recognition
identifies the optical characters by using random forest classification
algorithm. Experimental results reveal that the accuracy of this method is
90.9%.",None,-1
74240bd5-128d-4eba-b940-3eb27b7ebdb1,ScaleDet: A Scalable Multi-Dataset Object Detector,0.324725,"Multi-dataset training provides a viable solution for exploiting
heterogeneous large-scale datasets without extra annotation cost. In this work,
we propose a scalable multi-dataset detector (ScaleDet) that can scale up its
generalization across datasets when increasing the number of training datasets.
Unlike existing multi-dataset learners that mostly rely on manual relabelling
efforts or sophisticated optimizations to unify labels across datasets, we
introduce a simple yet scalable formulation to derive a unified semantic label
space for multi-dataset training. ScaleDet is trained by visual-textual
alignment to learn the label assignment with label semantic similarities across
datasets. Once trained, ScaleDet can generalize well on any given upstream and
downstream datasets with seen and unseen classes. We conduct extensive
experiments using LVIS, COCO, Objects365, OpenImages as upstream datasets, and
13 datasets from Object Detection in the Wild (ODinW) as downstream datasets.
Our results show that ScaleDet achieves compelling strong model performance
with an mAP of 50.7 on LVIS, 58.8 on COCO, 46.8 on Objects365, 76.2 on
OpenImages, and 71.8 on ODinW, surpassing state-of-the-art detectors with the
same backbone.",None,-1
aaf34424-a27e-4d75-9770-ea8a017e2443,Ten New Benchmarks for Optimization,0.0381487,"Benchmarks are used for testing new optimization algorithms and their
variants to evaluate their performance. Most existing benchmarks are smooth
functions. This chapter introduces ten new benchmarks with different
properties, including noise, discontinuity, parameter estimation and unknown
paths.",None,-1
97432ffc-5a3e-42f2-849f-2420edd4aeca,New Perspectives on Regularization and Computation in Optimal Transport-Based Distributionally Robust Optimization,0.975029,"We study optimal transport-based distributionally robust optimization
problems where a fictitious adversary, often envisioned as nature, can choose
the distribution of the uncertain problem parameters by reshaping a prescribed
reference distribution at a finite transportation cost. In this framework, we
show that robustification is intimately related to various forms of variation
and Lipschitz regularization even if the transportation cost function fails to
be (some power of) a metric. We also derive conditions for the existence and
the computability of a Nash equilibrium between the decision-maker and nature,
and we demonstrate numerically that nature's Nash strategy can be viewed as a
distribution that is supported on remarkably deceptive adversarial samples.
Finally, we identify practically relevant classes of optimal transport-based
distributionally robust optimization problems that can be addressed with
efficient gradient descent algorithms even if the loss function or the
transportation cost function are nonconvex (but not both at the same time).",None,-1
2a536062-b040-44c5-9c4a-b894b0525f0f,Vision-Language Pre-training with Object Contrastive Learning for 3D Scene Understanding,0.181606,"In recent years, vision language pre-training frameworks have made
significant progress in natural language processing and computer vision,
achieving remarkable performance improvement on various downstream tasks.
However, when extended to point cloud data, existing works mainly focus on
building task-specific models, and fail to extract universal 3D vision-language
embedding that generalize well. We carefully investigate three common tasks in
semantic 3D scene understanding, and derive key insights into the development
of a pre-training model. Motivated by these observations, we propose a
vision-language pre-training framework 3DVLP (3D vision-language pre-training
with object contrastive learning), which transfers flexibly on 3D
vision-language downstream tasks. 3DVLP takes visual grounding as the proxy
task and introduces Object-level IoU-guided Detection (OID) loss to obtain
high-quality proposals in the scene. Moreover, we design Object-level
Cross-Contrastive alignment (OCC) task and Object-level Self-Contrastive
learning (OSC) task to align the objects with descriptions and distinguish
different objects in the scene, respectively. Extensive experiments verify the
excellent performance of 3DVLP on three 3D vision-language tasks, reflecting
its superiority in semantic 3D scene understanding.",None,-1
7f97810c-e90b-4b6f-8d25-028d7cfb7740,"Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text",0.250907,"Can language models transform inputs to protect text classifiers against
adversarial attacks? In this work, we present ATINTER, a model that intercepts
and learns to rewrite adversarial inputs to make them non-adversarial for a
downstream text classifier. Our experiments on four datasets and five attack
mechanisms reveal that ATINTER is effective at providing better adversarial
robustness than existing defense approaches, without compromising task
accuracy. For example, on sentiment classification using the SST-2 dataset, our
method improves the adversarial accuracy over the best existing defense
approach by more than 4% with a smaller decrease in task accuracy (0.5% vs
2.5%). Moreover, we show that ATINTER generalizes across multiple downstream
tasks and classifiers without having to explicitly retrain it for those
settings. Specifically, we find that when ATINTER is trained to remove
adversarial perturbations for the sentiment classification task on the SST-2
dataset, it even transfers to a semantically different task of news
classification (on AGNews) and improves the adversarial robustness by more than
10%.",None,-1
1901855c-04b6-4cb5-8b00-76f23079b38f,Video Dehazing via a Multi-Range Temporal Alignment Network with Physical Prior,0.298778,"Video dehazing aims to recover haze-free frames with high visibility and
contrast. This paper presents a novel framework to effectively explore the
physical haze priors and aggregate temporal information. Specifically, we
design a memory-based physical prior guidance module to encode the
prior-related features into long-range memory. Besides, we formulate a
multi-range scene radiance recovery module to capture space-time dependencies
in multiple space-time ranges, which helps to effectively aggregate temporal
information from adjacent frames. Moreover, we construct the first large-scale
outdoor video dehazing benchmark dataset, which contains videos in various
real-world scenarios. Experimental results on both synthetic and real
conditions show the superiority of our proposed method.",None,-1
b4f850c7-bfde-4800-ab5f-4f1b22b69db1,Knowledge-Based Counterfactual Queries for Visual Question Answering,0.0368241,"Visual Question Answering (VQA) has been a popular task that combines vision
and language, with numerous relevant implementations in literature. Even though
there are some attempts that approach explainability and robustness issues in
VQA models, very few of them employ counterfactuals as a means of probing such
challenges in a model-agnostic way. In this work, we propose a systematic
method for explaining the behavior and investigating the robustness of VQA
models through counterfactual perturbations. For this reason, we exploit
structured knowledge bases to perform deterministic, optimal and controllable
word-level replacements targeting the linguistic modality, and we then evaluate
the model's response against such counterfactual inputs. Finally, we
qualitatively extract local and global explanations based on counterfactual
responses, which are ultimately proven insightful towards interpreting VQA
model behaviors. By performing a variety of perturbation types, targeting
different parts of speech of the input question, we gain insights to the
reasoning of the model, through the comparison of its responses in different
adversarial circumstances. Overall, we reveal possible biases in the
decision-making process of the model, as well as expected and unexpected
patterns, which impact its performance quantitatively and qualitatively, as
indicated by our analysis.",None,-1
331d2d09-5282-4c38-887a-e01ec42306ae,Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias,0.0422091,"Recent studies show that instruction tuning (IT) and reinforcement learning
from human feedback (RLHF) improve the abilities of large language models (LMs)
dramatically. While these tuning methods can help align models with human
objectives and generate high-quality text, not much is known about their
potential adverse effects. In this work, we investigate the effect of IT and
RLHF on decision making and reasoning in LMs, focusing on three cognitive
biases - the decoy effect, the certainty effect, and the belief bias - all of
which are known to influence human decision-making and reasoning. Our findings
highlight the presence of these biases in various models from the GPT-3,
Mistral, and T5 families. Notably, we find a stronger presence of biases in
models that have undergone instruction tuning, such as Flan-T5,
Mistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward
comprehending cognitive biases in instruction-tuned LMs, which is crucial for
the development of more reliable and unbiased language models.",None,-1
38e17625-ffd6-4783-a7fb-1097058d42b6,CC3D: Layout-Conditioned Generation of Compositional 3D Scenes,0.543213,"In this work, we introduce CC3D, a conditional generative model that
synthesizes complex 3D scenes conditioned on 2D semantic scene layouts, trained
using single-view images. Different from most existing 3D GANs that limit their
applicability to aligned single objects, we focus on generating complex scenes
with multiple objects, by modeling the compositional nature of 3D scenes. By
devising a 2D layout-based approach for 3D synthesis and implementing a new 3D
field representation with a stronger geometric inductive bias, we have created
a 3D GAN that is both efficient and of high quality, while allowing for a more
controllable generation process. Our evaluations on synthetic 3D-FRONT and
real-world KITTI-360 datasets demonstrate that our model generates scenes of
improved visual and geometric quality in comparison to previous works.",None,-1
162ce5af-85f8-4101-83a5-a4889d86b993,OpenGDA: Graph Domain Adaptation Benchmark for Cross-network Learning,0.094982,"Graph domain adaptation models are widely adopted in cross-network learning
tasks, with the aim of transferring labeling or structural knowledge.
Currently, there mainly exist two limitations in evaluating graph domain
adaptation models. On one side, they are primarily tested for the specific
cross-network node classification task, leaving tasks at edge-level and
graph-level largely under-explored. Moreover, they are primarily tested in
limited scenarios, such as social networks or citation networks, lacking
validation of model's capability in richer scenarios. As comprehensively
assessing models could enhance model practicality in real-world applications,
we propose a benchmark, known as OpenGDA. It provides abundant pre-processed
and unified datasets for different types of tasks (node, edge, graph). They
originate from diverse scenarios, covering web information systems, urban
systems and natural systems. Furthermore, it integrates state-of-the-art models
with standardized and end-to-end pipelines. Overall, OpenGDA provides a
user-friendly, scalable and reproducible benchmark for evaluating graph domain
adaptation models. The benchmark experiments highlight the challenges of
applying GDA models to real-world applications with consistent good
performance, and potentially provide insights to future research. As an
emerging project, OpenGDA will be regularly updated with new datasets and
models. It could be accessed from https://github.com/Skyorca/OpenGDA.",None,-1
653b7237-def8-425b-b0e5-ccf2257df9b0,Enhancing Cross-lingual Transfer via Phonemic Transcription Integration,0.49446,"Previous cross-lingual transfer methods are restricted to orthographic
representation learning via textual scripts. This limitation hampers
cross-lingual transfer and is biased towards languages sharing similar
well-known scripts. To alleviate the gap between languages from different
writing scripts, we propose PhoneXL, a framework incorporating phonemic
transcriptions as an additional linguistic modality beyond the traditional
orthographic transcriptions for cross-lingual transfer. Particularly, we
propose unsupervised alignment objectives to capture (1) local one-to-one
alignment between the two different modalities, (2) alignment via
multi-modality contexts to leverage information from additional modalities, and
(3) alignment via multilingual contexts where additional bilingual dictionaries
are incorporated. We also release the first phonemic-orthographic alignment
dataset on two token-level tasks (Named Entity Recognition and Part-of-Speech
Tagging) among the understudied but interconnected
Chinese-Japanese-Korean-Vietnamese (CJKV) languages. Our pilot study reveals
phonemic transcription provides essential information beyond the orthography to
enhance cross-lingual transfer and bridge the gap among CJKV languages, leading
to consistent improvements on cross-lingual token-level tasks over
orthographic-based multilingual PLMs.",None,-1
b719fdd5-ca5e-4aaf-9232-041d2f5964c7,iDML: Incentivized Decentralized Machine Learning,0.313745,"With the rising emergence of decentralized and opportunistic approaches to
machine learning, end devices are increasingly tasked with training deep
learning models on-devices using crowd-sourced data that they collect
themselves. These approaches are desirable from a resource consumption
perspective and also from a privacy preservation perspective. When the devices
benefit directly from the trained models, the incentives are implicit -
contributing devices' resources are incentivized by the availability of the
higher-accuracy model that results from collaboration. However, explicit
incentive mechanisms must be provided when end-user devices are asked to
contribute their resources (e.g., computation, communication, and data) to a
task performed primarily for the benefit of others, e.g., training a model for
a task that a neighbor device needs but the device owner is uninterested in. In
this project, we propose a novel blockchain-based incentive mechanism for
completely decentralized and opportunistic learning architectures. We leverage
a smart contract not only for providing explicit incentives to end devices to
participate in decentralized learning but also to create a fully decentralized
mechanism to inspect and reflect on the behavior of the learning architecture.",None,-1
0473a0d6-a040-46db-9914-517d35a0a758,Evaluating Temporal Observation-Based Causal Discovery Techniques Applied to Road Driver Behaviour,0.535372,"Autonomous robots are required to reason about the behaviour of dynamic
agents in their environment. The creation of models to describe these
relationships is typically accomplished through the application of causal
discovery techniques. However, as it stands observational causal discovery
techniques struggle to adequately cope with conditions such as causal sparsity
and non-stationarity typically seen during online usage in autonomous agent
domains. Meanwhile, interventional techniques are not always feasible due to
domain restrictions. In order to better explore the issues facing observational
techniques and promote further discussion of these topics we carry out a
benchmark across 10 contemporary observational temporal causal discovery
methods in the domain of autonomous driving. By evaluating these methods upon
causal scenes drawn from real world datasets in addition to those generated
synthetically we highlight where improvements need to be made in order to
facilitate the application of causal discovery techniques to the aforementioned
use-cases. Finally, we discuss potential directions for future work that could
help better tackle the difficulties currently experienced by state of the art
techniques.",None,-1
b3061290-e3f7-4ab3-b13b-86d06a454531,AFPQ: Asymmetric Floating Point Quantization for LLMs,0.0151469,"Large language models (LLMs) show great performance in various tasks, but
face deployment challenges from limited memory capacity and bandwidth. Low-bit
weight quantization can save memory and accelerate inference. Although
floating-point (FP) formats show good performance in LLM quantization, they
tend to perform poorly with small group sizes or sub-4 bits. We find the reason
is that the absence of asymmetry in previous FP quantization makes it
unsuitable for handling asymmetric value distribution of LLM weight tensors. In
this work, we propose asymmetric FP quantization (AFPQ), which sets separate
scales for positive and negative values. Our method leads to large accuracy
improvements and can be easily plugged into other quantization methods,
including GPTQ and AWQ, for better performance. Besides, no additional storage
is needed compared with asymmetric integer (INT) quantization. The code is
available at https://github.com/zhangsichengsjtu/AFPQ.",None,-1
5870170d-f42a-4cb1-a4dd-08bccf12388a,AutoScrum: Automating Project Planning Using Large Language Models,0.622142,"Recent advancements in the field of large language models have made it
possible to use language models for advanced reasoning. In this paper we
leverage this ability for designing complex project plans based only on knowing
the current state and the desired state. Two approaches are demonstrated - a
scrum based approach and a shortcut plan approach. The scrum based approach
executes an automated process of requirements gathering, user story mapping,
feature identification, task decomposition and finally generates questions and
search terms for seeking out domain specific information to assist with task
completion. The shortcut approach looks at most recent snapshot of the current
and desired state and generates the next most reasonable task to do in order to
get to the desired state as quickly as possible. In this paper we automate
everything using a novel concept of ""Language Programs"". These are programs
written in natural language designed to process input data through the language
model. Guidance language is used for all LLM programs. All demo source code for
this paper is available at https://github.com/autoscrum/autoscrum",None,-1
f28bc96c-e0fc-4e8b-96b7-f91e1c5a758a,EgoBlur: Responsible Innovation in Aria,0.0691899,"Project Aria pushes the frontiers of Egocentric AI with large-scale
real-world data collection using purposely designed glasses with privacy first
approach. To protect the privacy of bystanders being recorded by the glasses,
our research protocols are designed to ensure recorded video is processed by an
AI anonymization model that removes bystander faces and vehicle license plates.
Detected face and license plate regions are processed with a Gaussian blur such
that these personal identification information (PII) regions are obscured. This
process helps to ensure that anonymized versions of the video is retained for
research purposes. In Project Aria, we have developed a state-of-the-art
anonymization system EgoBlur. In this paper, we present extensive analysis of
EgoBlur on challenging datasets comparing its performance with other
state-of-the-art systems from industry and academia including extensive
Responsible AI analysis on recently released Casual Conversations V2 dataset.",None,-1
105c463f-5be5-42e5-8edd-a67dd45c6da9,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,0.851785,"The ability to accurately locate and navigate to a specific object is a
crucial capability for embodied agents that operate in the real world and
interact with objects to complete tasks. Such object navigation tasks usually
require large-scale training in visual environments with labeled objects, which
generalizes poorly to novel objects in unknown environments. In this work, we
present a novel zero-shot object navigation method, Exploration with Soft
Commonsense constraints (ESC), that transfers commonsense knowledge in
pre-trained models to open-world object navigation without any navigation
experience nor any other training on the visual environments. First, ESC
leverages a pre-trained vision and language model for open-world prompt-based
grounding and a pre-trained commonsense language model for room and object
reasoning. Then ESC converts commonsense knowledge into navigation actions by
modeling it as soft logic predicates for efficient exploration. Extensive
experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method
improves significantly over baselines, and achieves new state-of-the-art
results for zero-shot object navigation (e.g., 288% relative Success Rate
improvement than CoW on MP3D).",None,-1
e0cd25f0-383c-4090-a21d-38197d57856b,SegGPT Meets Co-Saliency Scene,0.277547,"Co-salient object detection targets at detecting co-existed salient objects
among a group of images. Recently, a generalist model for segmenting everything
in context, called SegGPT, is gaining public attention. In view of its
breakthrough for segmentation, we can hardly wait to probe into its
contribution to the task of co-salient object detection. In this report, we
first design a framework to enable SegGPT for the problem of co-salient object
detection. Proceed to the next step, we evaluate the performance of SegGPT on
the problem of co-salient object detection on three available datasets. We
achieve a finding that co-saliency scenes challenges SegGPT due to context
discrepancy within a group of co-saliency images.",None,-1
f30907d5-6a9e-4ca3-a38e-fa372d135704,Jointly Reparametrized Multi-Layer Adaptation for Efficient and Private Tuning,0.0229975,"Efficient finetuning of pretrained language transformers is becoming
increasingly prevalent for solving natural language processing tasks. While
effective, it can still require a large number of tunable parameters. This can
be a drawback for low-resource applications and training with
differential-privacy constraints, where excessive noise may be introduced
during finetuning. To this end, we propose a novel language transformer
finetuning strategy that introduces task-specific parameters in multiple
transformer layers. These parameters are derived from fixed random projections
of a single trainable vector, enabling finetuning with significantly fewer
parameters while maintaining performance. We achieve within 5% of full
finetuning performance on GLUE tasks with as few as 4,100 parameters per task,
outperforming other parameter-efficient finetuning approaches that use a
similar number of per-task parameters. Besides, the random projections can be
precomputed at inference, avoiding additional computational latency. All these
make our method particularly appealing for low-resource applications. Finally,
our method achieves the best or comparable utility compared to several recent
finetuning methods when training with the same privacy constraints,
underscoring its effectiveness and potential real-world impact.",None,-1
38bbfc99-f1c4-4722-a93f-82bd31e512ef,XFEVER: Exploring Fact Verification across Languages,0.355132,"This paper introduces the Cross-lingual Fact Extraction and VERification
(XFEVER) dataset designed for benchmarking the fact verification models across
different languages. We constructed it by translating the claim and evidence
texts of the Fact Extraction and VERification (FEVER) dataset into six
languages. The training and development sets were translated using machine
translation, whereas the test set includes texts translated by professional
translators and machine-translated texts. Using the XFEVER dataset, two
cross-lingual fact verification scenarios, zero-shot learning and
translate-train learning, are defined, and baseline models for each scenario
are also proposed in this paper. Experimental results show that the
multilingual language model can be used to build fact verification models in
different languages efficiently. However, the performance varies by language
and is somewhat inferior to the English case. We also found that we can
effectively mitigate model miscalibration by considering the prediction
similarity between the English and target languages. The XFEVER dataset, code,
and model checkpoints are available at
https://github.com/nii-yamagishilab/xfever.",None,-1
d1660f20-4f89-488d-b023-753e9a36d90b,Disproving XAI Myths with Formal Methods -- Initial Results,0.406486,"The advances in Machine Learning (ML) in recent years have been both
impressive and far-reaching. However, the deployment of ML models is still
impaired by a lack of trust in how the best-performing ML models make
predictions. The issue of lack of trust is even more acute in the uses of ML
models in high-risk or safety-critical domains. eXplainable artificial
intelligence (XAI) is at the core of ongoing efforts for delivering trustworthy
AI. Unfortunately, XAI is riddled with critical misconceptions, that foster
distrust instead of building trust. This paper details some of the most visible
misconceptions in XAI, and shows how formal methods have been used, both to
disprove those misconceptions, but also to devise practically effective
alternatives.",None,-1
ac0fd7cc-5421-48a1-b5d3-35631b427b30,Few-Shot Rotation-Invariant Aerial Image Semantic Segmentation,0.157883,"Few-shot aerial image segmentation is a challenging task that involves
precisely parsing objects in query aerial images with limited annotated
support. Conventional matching methods without consideration of varying object
orientations can fail to activate same-category objects with different
orientations. Moreover, conventional algorithms can lead to false recognition
of lower-scored rotated semantic objects. In response to these challenges, the
authors propose a novel few-shot rotation-invariant aerial semantic
segmentation network (FRINet). FRINet matches each query feature
rotation-adaptively with orientation-varying yet category-consistent support
information. The segmentation predictions from different orientations are
supervised by the same label, and the backbones are pre-trained in the base
category to boost segmentation performance. Experimental results demonstrate
that FRINet achieves state-of-the-art performance in few-shot aerial semantic
segmentation benchmark.",None,-1
2acff534-f187-40ee-ab7f-6bdf967fd684,Joint Probability Trees,0.193801,"We introduce Joint Probability Trees (JPT), a novel approach that makes
learning of and reasoning about joint probability distributions tractable for
practical applications. JPTs support both symbolic and subsymbolic variables in
a single hybrid model, and they do not rely on prior knowledge about variable
dependencies or families of distributions. JPT representations build on tree
structures that partition the problem space into relevant subregions that are
elicited from the training data instead of postulating a rigid dependency model
prior to learning. Learning and reasoning scale linearly in JPTs, and the tree
structure allows white-box reasoning about any posterior probability $P(Q|E)$,
such that interpretable explanations can be provided for any inference result.
Our experiments showcase the practical applicability of JPTs in
high-dimensional heterogeneous probability spaces with millions of training
samples, making it a promising alternative to classic probabilistic graphical
models.",None,-1
85668f1d-02c1-4aa1-82f5-9da73f666e0d,KwaiAgents: Generalized Information-seeking Agent System with Large Language Models,0.458342,"Driven by curiosity, humans have continually sought to explore and understand
the world around them, leading to the invention of various tools to satiate
this inquisitiveness. Despite not having the capacity to process and memorize
vast amounts of information in their brains, humans excel in critical thinking,
planning, reflection, and harnessing available tools to interact with and
interpret the world, enabling them to find answers efficiently. The recent
advancements in large language models (LLMs) suggest that machines might also
possess the aforementioned human-like capabilities, allowing them to exhibit
powerful abilities even with a constrained parameter count. In this paper, we
introduce KwaiAgents, a generalized information-seeking agent system based on
LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its
cognitive core, which is capable of understanding a user's query, behavior
guidelines, and referencing external documents. The agent can also update and
retrieve information from its internal memory, plan and execute actions using a
time-aware search-browse toolkit, and ultimately provide a comprehensive
response. We further investigate the system's performance when powered by LLMs
less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,
designed to ensure even an open-sourced 7B or 13B model performs well among
many agent systems. We exploit both benchmark and human evaluations to
systematically validate these capabilities. Extensive experiments show the
superiority of our agent system compared to other autonomous agents and
highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.",None,-1
e7633fa6-9285-4264-8162-a4390d7815d4,Semi-supervised Relation Extraction via Data Augmentation and Consistency-training,0.513124,"Due to the semantic complexity of the Relation extraction (RE) task,
obtaining high-quality human labelled data is an expensive and noisy process.
To improve the sample efficiency of the models, semi-supervised learning (SSL)
methods aim to leverage unlabelled data in addition to learning from limited
labelled data points. Recently, strong data augmentation combined with
consistency-based semi-supervised learning methods have advanced the state of
the art in several SSL tasks. However, adapting these methods to the RE task
has been challenging due to the difficulty of data augmentation for RE. In this
work, we leverage the recent advances in controlled text generation to perform
high quality data augmentation for the RE task. We further introduce small but
significant changes to model architecture that allows for generation of more
training data by interpolating different data points in their latent space.
These data augmentations along with consistency training result in very
competitive results for semi-supervised relation extraction on four benchmark
datasets.",None,-1
9e03baa7-3ab7-4f2c-ba48-e6a27d96b6d3,Neuron Structure Modeling for Generalizable Remote Physiological Measurement,0.972568,"Remote photoplethysmography (rPPG) technology has drawn increasing attention
in recent years. It can extract Blood Volume Pulse (BVP) from facial videos,
making many applications like health monitoring and emotional analysis more
accessible. However, as the BVP signal is easily affected by environmental
changes, existing methods struggle to generalize well for unseen domains. In
this paper, we systematically address the domain shift problem in the rPPG
measurement task. We show that most domain generalization methods do not work
well in this problem, as domain labels are ambiguous in complicated
environmental changes. In light of this, we propose a domain-label-free
approach called NEuron STructure modeling (NEST). NEST improves the
generalization capacity by maximizing the coverage of feature space during
training, which reduces the chance for under-optimized feature activation
during inference. Besides, NEST can also enrich and enhance domain invariant
features across multi-domain. We create and benchmark a large-scale domain
generalization protocol for the rPPG measurement task. Extensive experiments
show that our approach outperforms the state-of-the-art methods on both
cross-dataset and intra-dataset settings.",None,-1
3ecbf59d-f8ad-4e71-b7e6-c12ab3cda1ad,Construction of Knowledge Graphs: State and Challenges,0.925324,"With knowledge graphs (KGs) at the center of numerous applications such as
recommender systems and question answering, the need for generalized pipelines
to construct and continuously update such KGs is increasing. While the
individual steps that are necessary to create KGs from unstructured (e.g. text)
and structured data sources (e.g. databases) are mostly well-researched for
their one-shot execution, their adoption for incremental KG updates and the
interplay of the individual steps have hardly been investigated in a systematic
manner so far. In this work, we first discuss the main graph models for KGs and
introduce the major requirement for future KG construction pipelines. Next, we
provide an overview of the necessary steps to build high-quality KGs, including
cross-cutting topics such as metadata management, ontology development, and
quality assurance. We then evaluate the state of the art of KG construction
w.r.t the introduced requirements for specific popular KGs as well as some
recent tools and strategies for KG construction. Finally, we identify areas in
need of further research and improvement.",None,-1
3e02e9b5-35aa-46ce-8166-9d9aa42ac0ee,ClusterFuG: Clustering Fully connected Graphs by Multicut,0.263543,"We propose a graph clustering formulation based on multicut (a.k.a. weighted
correlation clustering) on the complete graph. Our formulation does not need
specification of the graph topology as in the original sparse formulation of
multicut, making our approach simpler and potentially better performing. In
contrast to unweighted correlation clustering we allow for a more expressive
weighted cost structure. In dense multicut, the clustering objective is given
in a factorized form as inner products of node feature vectors. This allows for
an efficient formulation and inference in contrast to multicut/weighted
correlation clustering, which has at least quadratic representation and
computation complexity when working on the complete graph. We show how to
rewrite classical greedy algorithms for multicut in our dense setting and how
to modify them for greater efficiency and solution quality. In particular, our
algorithms scale to graphs with tens of thousands of nodes. Empirical evidence
on instance segmentation on Cityscapes and clustering of ImageNet datasets
shows the merits of our approach.",None,-1
12e5a9f1-15c1-4127-9d66-f245e4fbf28a,Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following,0.344509,"In this paper, we present our finding that prepending a Task-Agnostic Prefix
Prompt (TAPP) to the input improves the instruction-following ability of
various Large Language Models (LLMs) during inference. TAPP is different from
canonical prompts for LLMs in that it is a fixed prompt prepended to the
beginning of every input regardless of the target task for zero-shot
generalization. We observe that both base LLMs (i.e. not fine-tuned to follow
instructions) and instruction-tuned models benefit from TAPP, resulting in
34.58% and 12.26% improvement on average, respectively. This implies that the
instruction-following ability of LLMs can be improved during inference time
with a fixed prompt constructed with simple heuristics. We hypothesize that
TAPP assists language models to better estimate the output distribution by
focusing more on the instruction of the target task during inference. In other
words, such ability does not seem to be sufficiently activated in not only base
LLMs but also many instruction-fine-tuned LLMs. All experiments are
reproducible from https://github.com/seonghyeonye/TAPP.",None,-1
38bf6e4d-f8d3-4b9c-af2a-d246590e359e,The ACL OCL Corpus: Advancing Open Science in Computational Linguistics,0.340051,"We present ACL OCL, a scholarly corpus derived from the ACL Anthology to
assist Open scientific research in the Computational Linguistics domain.
Integrating and enhancing the previous versions of the ACL Anthology, the ACL
OCL contributes metadata, PDF files, citation graphs and additional structured
full texts with sections, figures, and links to a large knowledge resource
(Semantic Scholar). The ACL OCL spans seven decades, containing 73K papers,
alongside 210K figures.
  We spotlight how ACL OCL applies to observe trends in computational
linguistics. By detecting paper topics with a supervised neural model, we note
that interest in ""Syntax: Tagging, Chunking and Parsing"" is waning and ""Natural
Language Generation"" is resurging. Our dataset is available from HuggingFace
(https://huggingface.co/datasets/WINGNUS/ACL-OCL).",None,-1
e025883a-121f-405c-b941-939b20ffc754,Benchmarking of Cancelable Biometrics for Deep Templates,0.746949,"In this paper, we benchmark several cancelable biometrics (CB) schemes on
different biometric characteristics. We consider BioHashing, Multi-Layer
Perceptron (MLP) Hashing, Bloom Filters, and two schemes based on
Index-of-Maximum (IoM) Hashing (i.e., IoM-URP and IoM-GRP). In addition to the
mentioned CB schemes, we introduce a CB scheme (as a baseline) based on
user-specific random transformations followed by binarization. We evaluate the
unlinkability, irreversibility, and recognition performance (which are the
required criteria by the ISO/IEC 24745 standard) of these CB schemes on deep
learning based templates extracted from different physiological and behavioral
biometric characteristics including face, voice, finger vein, and iris. In
addition, we provide an open-source implementation of all the experiments
presented to facilitate the reproducibility of our results.",None,-1
3bcf15aa-03ac-4a8d-93fd-7bf83c26c24e,Quantifying and Explaining Machine Learning Uncertainty in Predictive Process Monitoring: An Operations Research Perspective,0.384789,"This paper introduces a comprehensive, multi-stage machine learning
methodology that effectively integrates information systems and artificial
intelligence to enhance decision-making processes within the domain of
operations research. The proposed framework adeptly addresses common
limitations of existing solutions, such as the neglect of data-driven
estimation for vital production parameters, exclusive generation of point
forecasts without considering model uncertainty, and lacking explanations
regarding the sources of such uncertainty. Our approach employs Quantile
Regression Forests for generating interval predictions, alongside both local
and global variants of SHapley Additive Explanations for the examined
predictive process monitoring problem. The practical applicability of the
proposed methodology is substantiated through a real-world production planning
case study, emphasizing the potential of prescriptive analytics in refining
decision-making procedures. This paper accentuates the imperative of addressing
these challenges to fully harness the extensive and rich data resources
accessible for well-informed decision-making.",None,-1
6559fc97-e3eb-4b47-b1dc-12616404fb8f,Optimized Custom Dataset for Efficient Detection of Underwater Trash,0.580041,"Accurately quantifying and removing submerged underwater waste plays a
crucial role in safeguarding marine life and preserving the environment. While
detecting floating and surface debris is relatively straightforward,
quantifying submerged waste presents significant challenges due to factors like
light refraction, absorption, suspended particles, and color distortion. This
paper addresses these challenges by proposing the development of a custom
dataset and an efficient detection approach for submerged marine debris. The
dataset encompasses diverse underwater environments and incorporates
annotations for precise labeling of debris instances. Ultimately, the primary
objective of this custom dataset is to enhance the diversity of litter
instances and improve their detection accuracy in deep submerged environments
by leveraging state-of-the-art deep learning architectures.",None,-1
e8e9e04a-b730-4bc6-a92a-b99f976f187e,"What Should Be Balanced in a ""Balanced"" Face Recognition Dataset?",0.663357,"The issue of demographic disparities in face recognition accuracy has
attracted increasing attention in recent years. Various face image datasets
have been proposed as 'fair' or 'balanced' to assess the accuracy of face
recognition algorithms across demographics. These datasets typically balance
the number of identities and images across demographics. It is important to
note that the number of identities and images in an evaluation dataset are {\em
not} driving factors for 1-to-1 face matching accuracy. Moreover, balancing the
number of identities and images does not ensure balance in other factors known
to impact accuracy, such as head pose, brightness, and image quality. We
demonstrate these issues using several recently proposed datasets. To improve
the ability to perform less biased evaluations, we propose a bias-aware toolkit
that facilitates creation of cross-demographic evaluation datasets balanced on
factors mentioned in this paper.",None,-1
4a1dfc74-32d9-4edf-a043-3b92af0c6a81,SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains,0.483773,"Prompting pre-trained language models leads to promising results across
natural language processing tasks but is less effective when applied in
low-resource domains, due to the domain gap between the pre-training data and
the downstream task. In this work, we bridge this gap with a novel and
lightweight prompting methodology called SwitchPrompt for the adaptation of
language models trained on datasets from the general domain to diverse
low-resource domains. Using domain-specific keywords with a trainable gated
prompt, SwitchPrompt offers domain-oriented prompting, that is, effective
guidance on the target domains for general-domain language models. Our few-shot
experiments on three text classification benchmarks demonstrate the efficacy of
the general-domain pre-trained language models when used with SwitchPrompt.
They often even outperform their domain-specific counterparts trained with
baseline state-of-the-art prompting methods by up to 10.7% performance increase
in accuracy. This result indicates that SwitchPrompt effectively reduces the
need for domain-specific language model pre-training.",None,-1
49586f63-0878-40a1-9032-a7206385bb61,SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data,0.853472,"Text-to-SQL aims to automate the process of generating SQL queries on a
database from natural language text. In this work, we propose ""SQLPrompt"",
tailored to improve the few-shot prompting capabilities of Text-to-SQL for
Large Language Models (LLMs). Our methods include innovative prompt design,
execution-based consistency decoding strategy which selects the SQL with the
most consistent execution outcome among other SQL proposals, and a method that
aims to improve performance by diversifying the SQL proposals during
consistency selection with different prompt designs (""MixPrompt"") and
foundation models (""MixLLMs""). We show that \emph{SQLPrompt} outperforms
previous approaches for in-context learning with few labeled data by a large
margin, closing the gap with finetuning state-of-the-art with thousands of
labeled data.",None,-1
d4182153-1b07-433b-9655-db8cade90ede,Continual Learning as Computationally Constrained Reinforcement Learning,0.681186,"An agent that efficiently accumulates knowledge to develop increasingly
sophisticated skills over a long lifetime could advance the frontier of
artificial intelligence capabilities. The design of such agents, which remains
a long-standing challenge of artificial intelligence, is addressed by the
subject of continual learning. This monograph clarifies and formalizes concepts
of continual learning, introducing a framework and set of tools to stimulate
further research.",None,-1
025c6f6d-68cf-4bd0-9607-8aff0ba92b61,Zero-shot Transfer of Article-aware Legal Outcome Classification for European Court of Human Rights Cases,0.911355,"In this paper, we cast Legal Judgment Prediction on European Court of Human
Rights cases into an article-aware classification task, where the case outcome
is classified from a combined input of case facts and convention articles. This
configuration facilitates the model learning some legal reasoning ability in
mapping article text to specific case fact text. It also provides an
opportunity to evaluate the model's ability to generalize to zero-shot settings
when asked to classify the case outcome with respect to articles not seen
during training. We devise zero-shot experiments and apply domain adaptation
methods based on domain discrimination and Wasserstein distance. Our results
demonstrate that the article-aware architecture outperforms straightforward
fact classification. We also find that domain adaptation methods improve
zero-shot transfer performance, with article relatedness and encoder
pre-training influencing the effect.",None,-1
3de989ba-53d9-4a14-9d49-ca440f961108,Guideline Learning for In-context Information Extraction,0.665379,"Large language models (LLMs) can perform a new task by merely conditioning on
task instructions and a few input-output examples, without optimizing any
parameters. This is called In-Context Learning (ICL). In-context Information
Extraction (IE) has recently garnered attention in the research community.
However, the performance of In-context IE generally lags behind the
state-of-the-art supervised expert models. We highlight a key reason for this
shortfall: underspecified task description. The limited-length context
struggles to thoroughly express the intricate IE task instructions and various
edge cases, leading to misalignment in task comprehension with humans. In this
paper, we propose a Guideline Learning (GL) framework for In-context IE which
reflectively learns and follows guidelines. During the learning phrase, GL
automatically synthesizes a set of guidelines based on a few error cases, and
during inference, GL retrieves helpful guidelines for better ICL. Moreover, we
propose a self-consistency-based active learning method to enhance the
efficiency of GL. Experiments on event extraction and relation extraction show
that GL can significantly improve the performance of in-context IE.",None,-1
f3d540c7-6070-49ff-a88e-c9639519710c,How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model,0.721834,"Pre-trained language models can be surprisingly adept at tasks they were not
explicitly trained on, but how they implement these capabilities is poorly
understood. In this paper, we investigate the basic mathematical abilities
often acquired by pre-trained language models. Concretely, we use mechanistic
interpretability techniques to explain the (limited) mathematical abilities of
GPT-2 small. As a case study, we examine its ability to take in sentences such
as ""The war lasted from the year 1732 to the year 17"", and predict valid
two-digit end years (years > 32). We first identify a circuit, a small subset
of GPT-2 small's computational graph that computes this task's output. Then, we
explain the role of each circuit component, showing that GPT-2 small's final
multi-layer perceptrons boost the probability of end years greater than the
start year. Finally, we find related tasks that activate our circuit. Our
results suggest that GPT-2 small computes greater-than using a complex but
general mechanism that activates across diverse contexts.",None,-1
1efd5f21-051f-48f1-859a-9b463bcb442c,Machine Love,0.323229,"While ML generates much economic value, many of us have problematic
relationships with social media and other ML-powered applications. One reason
is that ML often optimizes for what we want in the moment, which is easy to
quantify but at odds with what is known scientifically about human flourishing.
Thus, through its impoverished models of us, ML currently falls far short of
its exciting potential, which is for it to help us to reach ours. While there
is no consensus on defining human flourishing, from diverse perspectives across
psychology, philosophy, and spiritual traditions, love is understood to be one
of its primary catalysts. Motivated by this view, this paper explores whether
there is a useful conception of love fitting for machines to embody, as
historically it has been generative to explore whether a nebulous concept, such
as life or intelligence, can be thoughtfully abstracted and reimagined, as in
the fields of machine intelligence or artificial life. This paper forwards a
candidate conception of machine love, inspired in particular by work in
positive psychology and psychotherapy: to provide unconditional support
enabling humans to autonomously pursue their own growth and development.
Through proof of concept experiments, this paper aims to highlight the need for
richer models of human flourishing in ML, provide an example framework through
which positive psychology can be combined with ML to realize a rough conception
of machine love, and demonstrate that current language models begin to enable
embodying qualitative humanistic principles. The conclusion is that though at
present ML may often serve to addict, distract, or divide us, an alternative
path may be opening up: We may align ML to support our growth, through it
helping us to align ourselves towards our highest aspirations.",None,-1
081bb7c8-485c-4197-894d-569948dcea1b,Description-Based Text Similarity,0.397039,"Identifying texts with a given semantics is central for many information
seeking scenarios. Similarity search over vector embeddings appear to be
central to this ability, yet the similarity reflected in current text
embeddings is corpus-driven, and is inconsistent and sub-optimal for many use
cases. What, then, is a good notion of similarity for effective retrieval of
text?
  We identify the need to search for texts based on abstract descriptions of
their content, and the corresponding notion of \emph{description based
similarity}. We demonstrate the inadequacy of current text embeddings and
propose an alternative model that significantly improves when used in standard
nearest neighbor search. The model is trained using positive and negative pairs
sourced through prompting a LLM, demonstrating how data from LLMs can be used
for creating new capabilities not immediately possible using the original
model.",None,-1
8d1c10c0-1f52-4180-b9e8-f99a82d35a3c,Mediapipe and CNNs for Real-Time ASL Gesture Recognition,0.726769,"This research paper describes a realtime system for identifying American Sign
Language (ASL) movements that employs modern computer vision and machine
learning approaches. The suggested method makes use of the Mediapipe library
for feature extraction and a Convolutional Neural Network (CNN) for ASL gesture
classification. The testing results show that the suggested system can detect
all ASL alphabets with an accuracy of 99.95%, indicating its potential for use
in communication devices for people with hearing impairments. The proposed
approach can also be applied to additional sign languages with similar hand
motions, potentially increasing the quality of life for people with hearing
loss. Overall, the study demonstrates the effectiveness of using Mediapipe and
CNN for real-time sign language recognition, making a significant contribution
to the field of computer vision and machine learning.",None,-1
348f8ebc-6fc5-449b-9e67-409c8eedb424,"""Im not Racist but..."": Discovering Bias in the Internal Knowledge of Large Language Models",0.105028,"Large language models (LLMs) have garnered significant attention for their
remarkable performance in a continuously expanding set of natural language
processing tasks. However, these models have been shown to harbor inherent
societal biases, or stereotypes, which can adversely affect their performance
in their many downstream applications. In this paper, we introduce a novel,
purely prompt-based approach to uncover hidden stereotypes within any arbitrary
LLM. Our approach dynamically generates a knowledge representation of internal
stereotypes, enabling the identification of biases encoded within the LLM's
internal knowledge. By illuminating the biases present in LLMs and offering a
systematic methodology for their analysis, our work contributes to advancing
transparency and promoting fairness in natural language processing systems.",None,-1
3ad61236-4360-4b7b-8056-ed9c48b7464e,WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models,0.703456,"This paper describes our submission to the MEDIQA-Chat 2023 shared task for
automatic clinical note generation from doctor-patient conversations. We report
results for two approaches: the first fine-tunes a pre-trained language model
(PLM) on the shared task data, and the second uses few-shot in-context learning
(ICL) with a large language model (LLM). Both achieve high performance as
measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and
first, respectively, of all submissions to the shared task. Expert human
scrutiny indicates that notes generated via the ICL-based approach with GPT-4
are preferred about as often as human-written notes, making it a promising path
toward automated note generation from doctor-patient conversations.",None,-1
9c2658a4-55fd-405a-b126-c56708caa914,STRIDE: Street View-based Environmental Feature Detection and Pedestrian Collision Prediction,0.421792,"This paper introduces a novel benchmark to study the impact and relationship
of built environment elements on pedestrian collision prediction, intending to
enhance environmental awareness in autonomous driving systems to prevent
pedestrian injuries actively. We introduce a built environment detection task
in large-scale panoramic images and a detection-based pedestrian collision
frequency prediction task. We propose a baseline method that incorporates a
collision prediction module into a state-of-the-art detection model to tackle
both tasks simultaneously. Our experiments demonstrate a significant
correlation between object detection of built environment elements and
pedestrian collision frequency prediction. Our results are a stepping stone
towards understanding the interdependencies between built environment
conditions and pedestrian safety.",None,-1
15262de6-fb3c-4ab7-a487-680341262664,Text Rendering Strategies for Pixel Language Models,0.0965348,"Pixel-based language models process text rendered as images, which allows
them to handle any script, making them a promising approach to open vocabulary
language modelling. However, recent approaches use text renderers that produce
a large set of almost-equivalent input patches, which may prove sub-optimal for
downstream tasks, due to redundancy in the input representations. In this
paper, we investigate four approaches to rendering text in the PIXEL model
(Rust et al., 2023), and find that simple character bigram rendering brings
improved performance on sentence-level tasks without compromising performance
on token-level or multilingual tasks. This new rendering strategy also makes it
possible to train a more compact model with only 22M parameters that performs
on par with the original 86M parameter model. Our analyses show that character
bigram rendering leads to a consistently better model but with an anisotropic
patch embedding space, driven by a patch frequency bias, highlighting the
connections between image patch- and tokenization-based language models.",None,-1
ea03fab3-4a6c-4888-ac73-62febe378dda,BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT,0.937252,"Recently, ChatGPT has gained significant attention in research due to its
ability to interact with humans effectively. The core idea behind this model is
reinforcement learning (RL) fine-tuning, a new paradigm that allows language
models to align with human preferences, i.e., InstructGPT. In this study, we
propose BadGPT, the first backdoor attack against RL fine-tuning in language
models. By injecting a backdoor into the reward model, the language model can
be compromised during the fine-tuning stage. Our initial experiments on movie
reviews, i.e., IMDB, demonstrate that an attacker can manipulate the generated
text through BadGPT.",None,-1
a43e0a62-d59e-4aaa-ab3c-902d6431b48d,Graph Agent: Explicit Reasoning Agent for Graphs,0.471165,"Graph embedding methods such as Graph Neural Networks (GNNs) and Graph
Transformers have contributed to the development of graph reasoning algorithms
for various tasks on knowledge graphs. However, the lack of interpretability
and explainability of graph embedding methods has limited their applicability
in scenarios requiring explicit reasoning. In this paper, we introduce the
Graph Agent (GA), an intelligent agent methodology of leveraging large language
models (LLMs), inductive-deductive reasoning modules, and long-term memory for
knowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning
and existing graph embedding methods to provide an innovative approach for
complex graph reasoning tasks. By converting graph structures into textual
data, GA enables LLMs to process, reason, and provide predictions alongside
human-interpretable explanations. The effectiveness of the GA was evaluated on
node classification and link prediction tasks. Results showed that GA reached
state-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and
89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to
existing GNN and transformer models, GA offered advantages of explicit
reasoning ability, free-of-training, easy adaption to various graph reasoning
tasks",None,-1
03083ea2-81c8-4219-991d-c2f7d271a425,PACO: Parts and Attributes of Common Objects,0.90905,"Object models are gradually progressing from predicting just category labels
to providing detailed descriptions of object instances. This motivates the need
for large datasets which go beyond traditional object masks and provide richer
annotations such as part masks and attributes. Hence, we introduce PACO: Parts
and Attributes of Common Objects. It spans 75 object categories, 456
object-part categories and 55 attributes across image (LVIS) and video (Ego4D)
datasets. We provide 641K part masks annotated across 260K object boxes, with
roughly half of them exhaustively annotated with attributes as well. We design
evaluation metrics and provide benchmark results for three tasks on the
dataset: part mask segmentation, object and part attribute prediction and
zero-shot instance detection. Dataset, models, and code are open-sourced at
https://github.com/facebookresearch/paco.",None,-1
b09148f1-e81f-4144-8e8d-f76a79b1bd6e,Robust Robot Planning for Human-Robot Collaboration,0.324454,"In human-robot collaboration, the objectives of the human are often unknown
to the robot. Moreover, even assuming a known objective, the human behavior is
also uncertain. In order to plan a robust robot behavior, a key preliminary
question is then: How to derive realistic human behaviors given a known
objective? A major issue is that such a human behavior should itself account
for the robot behavior, otherwise collaboration cannot happen. In this paper,
we rely on Markov decision models, representing the uncertainty over the human
objective as a probability distribution over a finite set of objective
functions (inducing a distribution over human behaviors). Based on this, we
propose two contributions: 1) an approach to automatically generate an
uncertain human behavior (a policy) for each given objective function while
accounting for possible robot behaviors; and 2) a robot planning algorithm that
is robust to the above-mentioned uncertainties and relies on solving a
partially observable Markov decision process (POMDP) obtained by reasoning on a
distribution over human behaviors. A co-working scenario allows conducting
experiments and presenting qualitative and quantitative results to evaluate our
approach.",None,-1
a9b615af-0bab-480a-866b-a0a6092896f6,MEnsA: Mix-up Ensemble Average for Unsupervised Multi Target Domain Adaptation on 3D Point Clouds,0.221199,"Unsupervised domain adaptation (UDA) addresses the problem of distribution
shift between the unlabelled target domain and labelled source domain. While
the single target domain adaptation (STDA) is well studied in the literature
for both 2D and 3D vision tasks, multi-target domain adaptation (MTDA) is
barely explored for 3D data despite its wide real-world applications such as
autonomous driving systems for various geographical and climatic conditions. We
establish an MTDA baseline for 3D point cloud data by proposing to mix the
feature representations from all domains together to achieve better domain
adaptation performance by an ensemble average, which we call Mixup Ensemble
Average or MEnsA. With the mixed representation, we use a domain classifier to
improve at distinguishing the feature representations of source domain from
those of target domains in a shared latent space. In empirical validations on
the challenging PointDA-10 dataset, we showcase a clear benefit of our simple
method over previous unsupervised STDA and MTDA methods by large margins (up to
17.10% and 4.76% on averaged over all domain shifts).",None,-1
a01673a1-01d0-4231-b385-2e7100bfcb05,On the Zero-Shot Generalization of Machine-Generated Text Detectors,0.676686,"The rampant proliferation of large language models, fluent enough to generate
text indistinguishable from human-written language, gives unprecedented
importance to the detection of machine-generated text. This work is motivated
by an important research question: How will the detectors of machine-generated
text perform on outputs of a new generator, that the detectors were not trained
on? We begin by collecting generation data from a wide range of LLMs, and train
neural detectors on data from each generator and test its performance on
held-out generators. While none of the detectors can generalize to all
generators, we observe a consistent and interesting pattern that the detectors
trained on data from a medium-size LLM can zero-shot generalize to the larger
version. As a concrete application, we demonstrate that robust detectors can be
built on an ensemble of training data from medium-sized models.",None,-1
c34131ba-f55f-4719-9b36-57808a304ca8,DiffuseGAE: Controllable and High-fidelity Image Manipulation from Disentangled Representation,0.207453,"Diffusion probabilistic models (DPMs) have shown remarkable results on
various image synthesis tasks such as text-to-image generation and image
inpainting. However, compared to other generative methods like VAEs and GANs,
DPMs lack a low-dimensional, interpretable, and well-decoupled latent code.
Recently, diffusion autoencoders (Diff-AE) were proposed to explore the
potential of DPMs for representation learning via autoencoding. Diff-AE
provides an accessible latent space that exhibits remarkable interpretability,
allowing us to manipulate image attributes based on latent codes from the
space. However, previous works are not generic as they only operated on a few
limited attributes. To further explore the latent space of Diff-AE and achieve
a generic editing pipeline, we proposed a module called Group-supervised
AutoEncoder(dubbed GAE) for Diff-AE to achieve better disentanglement on the
latent code. Our proposed GAE has trained via an attribute-swap strategy to
acquire the latent codes for multi-attribute image manipulation based on
examples. We empirically demonstrate that our method enables
multiple-attributes manipulation and achieves convincing sample quality and
attribute alignments, while significantly reducing computational requirements
compared to pixel-based approaches for representational decoupling. Code will
be released soon.",None,-1
703c49ec-3c41-44c6-9e29-98c9fc135519,DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields,0.696845,"Recent works such as BARF and GARF can bundle adjust camera poses with neural
radiance fields (NeRF) which is based on coordinate-MLPs. Despite the
impressive results, these methods cannot be applied to Generalizable NeRFs
(GeNeRFs) which require image feature extractions that are often based on more
complicated 3D CNN or transformer architectures. In this work, we first analyze
the difficulties of jointly optimizing camera poses with GeNeRFs, and then
further propose our DBARF to tackle these issues. Our DBARF which bundle
adjusts camera poses by taking a cost feature map as an implicit cost function
can be jointly trained with GeNeRFs in a self-supervised manner. Unlike BARF
and its follow-up works, which can only be applied to per-scene optimized NeRFs
and need accurate initial camera poses with the exception of forward-facing
scenes, our method can generalize across scenes and does not require any good
initialization. Experiments show the effectiveness and generalization ability
of our DBARF when evaluated on real-world datasets. Our code is available at
\url{https://aibluefisher.github.io/dbarf}.",None,-1
59bc0a5f-ec15-46d8-bd6a-69176e84ba90,Robust Evaluation of Diffusion-Based Adversarial Purification,0.62877,"We question the current evaluation practice on diffusion-based purification
methods. Diffusion-based purification methods aim to remove adversarial effects
from an input data point at test time. The approach gains increasing attention
as an alternative to adversarial training due to the disentangling between
training and testing. Well-known white-box attacks are often employed to
measure the robustness of the purification. However, it is unknown whether
these attacks are the most effective for the diffusion-based purification since
the attacks are often tailored for adversarial training. We analyze the current
practices and provide a new guideline for measuring the robustness of
purification methods against adversarial attacks. Based on our analysis, we
further propose a new purification strategy improving robustness compared to
the current diffusion-based purification methods.",None,-1
84fd2e98-8f56-4736-b9aa-6fab4a9edf4d,Posthoc Interpretation via Quantization,0.0750666,"In this paper, we introduce a new approach, called Posthoc Interpretation via
Quantization (PIQ), for interpreting decisions made by trained classifiers. Our
method utilizes vector quantization to transform the representations of a
classifier into a discrete, class-specific latent space. The class-specific
codebooks act as a bottleneck that forces the interpreter to focus on the parts
of the input data deemed relevant by the classifier for making a prediction.
Our model formulation also enables learning concepts by incorporating the
supervision of pretrained annotation models such as state-of-the-art image
segmentation models. We evaluated our method through quantitative and
qualitative studies involving black-and-white images, color images, and audio.
As a result of these studies we found that PIQ generates interpretations that
are more easily understood by participants to our user studies when compared to
several other interpretation methods in the literature.",None,-1
d1540931-988b-44ca-910d-b0e1ae0fbb93,Joint multi-modal Self-Supervised pre-training in Remote Sensing: Application to Methane Source Classification,0.0945512,"With the current ubiquity of deep learning methods to solve computer vision
and remote sensing specific tasks, the need for labelled data is growing
constantly. However, in many cases, the annotation process can be long and
tedious depending on the expertise needed to perform reliable annotations. In
order to alleviate this need for annotations, several self-supervised methods
have recently been proposed in the literature. The core principle behind these
methods is to learn an image encoder using solely unlabelled data samples. In
earth observation, there are opportunities to exploit domain-specific remote
sensing image data in order to improve these methods. Specifically, by
leveraging the geographical position associated with each image, it is possible
to cross reference a location captured from multiple sensors, leading to
multiple views of the same locations. In this paper, we briefly review the core
principles behind so-called joint-embeddings methods and investigate the usage
of multiple remote sensing modalities in self-supervised pre-training. We
evaluate the final performance of the resulting encoders on the task of methane
source classification.",None,-1
7393ecb2-09ac-48de-bf56-f4dd735913a2,Going beyond research datasets: Novel intent discovery in the industry setting,0.135949,"Novel intent discovery automates the process of grouping similar messages
(questions) to identify previously unknown intents. However, current research
focuses on publicly available datasets which have only the question field and
significantly differ from real-life datasets. This paper proposes methods to
improve the intent discovery pipeline deployed in a large e-commerce platform.
We show the benefit of pre-training language models on in-domain data: both
self-supervised and with weak supervision. We also devise the best method to
utilize the conversational structure (i.e., question and answer) of real-life
datasets during fine-tuning for clustering tasks, which we call Conv. All our
methods combined to fully utilize real-life datasets give up to 33pp
performance boost over state-of-the-art Constrained Deep Adaptive Clustering
(CDAC) model for question only. By comparison CDAC model for the question data
only gives only up to 13pp performance boost over the naive baseline.",None,-1
2e4cbdeb-c743-49eb-b417-2fb70f774aed,PromptMix: Text-to-image diffusion models enhance the performance of lightweight networks,0.0509325,"Many deep learning tasks require annotations that are too time consuming for
human operators, resulting in small dataset sizes. This is especially true for
dense regression problems such as crowd counting which requires the location of
every person in the image to be annotated. Techniques such as data augmentation
and synthetic data generation based on simulations can help in such cases. In
this paper, we introduce PromptMix, a method for artificially boosting the size
of existing datasets, that can be used to improve the performance of
lightweight networks. First, synthetic images are generated in an end-to-end
data-driven manner, where text prompts are extracted from existing datasets via
an image captioning deep network, and subsequently introduced to text-to-image
diffusion models. The generated images are then annotated using one or more
high-performing deep networks, and mixed with the real dataset for training the
lightweight network. By extensive experiments on five datasets and two tasks,
we show that PromptMix can significantly increase the performance of
lightweight networks by up to 26%.",None,-1
8d46191d-4819-4f65-8769-9b4c2141eda2,EventNet-ITA: Italian Frame Parsing for Events,0.316809,"This paper introduces EventNet-ITA, a large, multi-domain corpus annotated
full-text with event frames for Italian. Moreover, we present and thoroughly
evaluate an efficient multi-label sequence labeling approach for Frame Parsing.
Covering a wide range of individual, social and historical phenomena, with more
than 53,000 annotated sentences and over 200 modeled frames, EventNet-ITA
constitutes the first systematic attempt to provide the Italian language with a
publicly available resource for Frame Parsing of events, useful for a broad
spectrum of research and application tasks. Our approach achieves a promising
0.9 strict F1-score for frame classification and 0.72 for frame element
classification, on top of minimizing computational requirements. The annotated
corpus and the frame parsing model are released under open license.",None,-1
1938fdc3-1dd4-4add-93ef-786368060978,Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models,0.441321,"We investigate security concerns of the emergent instruction tuning paradigm,
that models are trained on crowdsourced datasets with task instructions to
achieve superior performance. Our studies demonstrate that an attacker can
inject backdoors by issuing very few malicious instructions (~1000 tokens) and
control model behavior through data poisoning, without even the need to modify
data instances or labels themselves. Through such instruction attacks, the
attacker can achieve over 90% attack success rate across four commonly used NLP
datasets. As an empirical study on instruction attacks, we systematically
evaluated unique perspectives of instruction attacks, such as poison transfer
where poisoned models can transfer to 15 diverse generative datasets in a
zero-shot manner; instruction transfer where attackers can directly apply
poisoned instruction on many other datasets; and poison resistance to continual
finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate
such backdoors to some degree. These findings highlight the need for more
robust defenses against poisoning attacks in instruction-tuning models and
underscore the importance of ensuring data quality in instruction
crowdsourcing.",None,-1
54eb8bcf-d2e3-48b3-8ae8-b72fc0df7e03,Linearity of Relation Decoding in Transformer Language Models,0.797188,"Much of the knowledge encoded in transformer language models (LMs) may be
expressed in terms of relations: relations between words and their synonyms,
entities and their attributes, etc. We show that, for a subset of relations,
this computation is well-approximated by a single linear transformation on the
subject representation. Linear relation representations may be obtained by
constructing a first-order approximation to the LM from a single prompt, and
they exist for a variety of factual, commonsense, and linguistic relations.
However, we also identify many cases in which LM predictions capture relational
knowledge accurately, but this knowledge is not linearly encoded in their
representations. Our results thus reveal a simple, interpretable, but
heterogeneously deployed knowledge representation strategy in transformer LMs.",None,-1
6be41e21-c646-47d5-bfb9-d0118f5c49ed,Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction,0.839892,"Accurate prediction of what types of patents that companies will apply for in
the next period of time can figure out their development strategies and help
them discover potential partners or competitors in advance. Although important,
this problem has been rarely studied in previous research due to the challenges
in modelling companies' continuously evolving preferences and capturing the
semantic correlations of classification codes. To fill in this gap, we propose
an event-based dynamic graph learning framework for patent application trend
prediction. In particular, our method is founded on the memorable
representations of both companies and patent classification codes. When a new
patent is observed, the representations of the related companies and
classification codes are updated according to the historical memories and the
currently encoded messages. Moreover, a hierarchical message passing mechanism
is provided to capture the semantic proximities of patent classification codes
by updating their representations along the hierarchical taxonomy. Finally, the
patent application trend is predicted by aggregating the representations of the
target company and classification codes from static, dynamic, and hierarchical
perspectives. Experiments on real-world data demonstrate the effectiveness of
our approach under various experimental conditions, and also reveal the
abilities of our method in learning semantics of classification codes and
tracking technology developing trajectories of companies.",None,-1
3e77ac94-2072-4ad0-9121-69c57ea3bd74,Bridging the Granularity Gap for Acoustic Modeling,0.325915,"While Transformer has become the de-facto standard for speech, modeling upon
the fine-grained frame-level features remains an open challenge of capturing
long-distance dependencies and distributing the attention weights. We propose
\textit{Progressive Down-Sampling} (PDS) which gradually compresses the
acoustic features into coarser-grained units containing more complete semantic
information, like text-level representation. In addition, we develop a
representation fusion method to alleviate information loss that occurs
inevitably during high compression. In this way, we compress the acoustic
features into 1/32 of the initial length while achieving better or comparable
performances on the speech recognition task. And as a bonus, it yields
inference speedups ranging from 1.20$\times$ to 1.47$\times$. By reducing the
modeling burden, we also achieve competitive results when training on the more
challenging speech translation task.",None,-1
998eaa36-6f26-4377-8aec-ff23d3067935,Multiagent Inverse Reinforcement Learning via Theory of Mind Reasoning,0.687686,"We approach the problem of understanding how people interact with each other
in collaborative settings, especially when individuals know little about their
teammates, via Multiagent Inverse Reinforcement Learning (MIRL), where the goal
is to infer the reward functions guiding the behavior of each individual given
trajectories of a team's behavior during some task. Unlike current MIRL
approaches, we do not assume that team members know each other's goals a
priori; rather, that they collaborate by adapting to the goals of others
perceived by observing their behavior, all while jointly performing a task. To
address this problem, we propose a novel approach to MIRL via Theory of Mind
(MIRL-ToM). For each agent, we first use ToM reasoning to estimate a posterior
distribution over baseline reward profiles given their demonstrated behavior.
We then perform MIRL via decentralized equilibrium by employing single-agent
Maximum Entropy IRL to infer a reward function for each agent, where we
simulate the behavior of other teammates according to the time-varying
distribution over profiles. We evaluate our approach in a simulated 2-player
search-and-rescue operation where the goal of the agents, playing different
roles, is to search for and evacuate victims in the environment. Our results
show that the choice of baseline profiles is paramount to the recovery of the
ground-truth rewards, and that MIRL-ToM is able to recover the rewards used by
agents interacting both with known and unknown teammates.",None,-1
5d4e39e3-2ffa-475d-86d9-1697dc5d4cb8,The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages,0.505754,"Gender biases in language generation systems are challenging to mitigate. One
possible source for these biases is gender representation disparities in the
training and evaluation data. Despite recent progress in documenting this
problem and many attempts at mitigating it, we still lack shared methodology
and tooling to report gender representation in large datasets. Such
quantitative reporting will enable further mitigation, e.g., via data
augmentation. This paper describes the Gender-GAP Pipeline (for Gender-Aware
Polyglot Pipeline), an automatic pipeline to characterize gender representation
in large-scale datasets for 55 languages. The pipeline uses a multilingual
lexicon of gendered person-nouns to quantify the gender representation in text.
We showcase it to report gender representation in WMT training data and
development data for the News task, confirming that current data is skewed
towards masculine representation. Having unbalanced datasets may indirectly
optimize our systems towards outperforming one gender over the others. We
suggest introducing our gender quantification pipeline in current datasets and,
ideally, modifying them toward a balanced representation.",None,-1
22b7c4b5-ec3d-4055-98d2-92e669764103,Unbalanced Optimal Transport for Unbalanced Word Alignment,0.0661258,"Monolingual word alignment is crucial to model semantic interactions between
sentences. In particular, null alignment, a phenomenon in which words have no
corresponding counterparts, is pervasive and critical in handling semantically
divergent sentences. Identification of null alignment is useful on its own to
reason about the semantic similarity of sentences by indicating there exists
information inequality. To achieve unbalanced word alignment that values both
alignment and null alignment, this study shows that the family of optimal
transport (OT), i.e., balanced, partial, and unbalanced OT, are natural and
powerful approaches even without tailor-made techniques. Our extensive
experiments covering unsupervised and supervised settings indicate that our
generic OT-based alignment methods are competitive against the
state-of-the-arts specially designed for word alignment, remarkably on
challenging datasets with high null alignment frequencies.",None,-1
44b6f04a-e9bb-4741-bed2-9e95ddda2bba,Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers,0.378468,"This paper introduces Content-aware Token Sharing (CTS), a token reduction
approach that improves the computational efficiency of semantic segmentation
networks that use Vision Transformers (ViTs). Existing works have proposed
token reduction approaches to improve the efficiency of ViT-based image
classification networks, but these methods are not directly applicable to
semantic segmentation, which we address in this work. We observe that, for
semantic segmentation, multiple image patches can share a token if they contain
the same semantic class, as they contain redundant information. Our approach
leverages this by employing an efficient, class-agnostic policy network that
predicts if image patches contain the same semantic class, and lets them share
a token if they do. With experiments, we explore the critical design choices of
CTS and show its effectiveness on the ADE20K, Pascal Context and Cityscapes
datasets, various ViT backbones, and different segmentation decoders. With
Content-aware Token Sharing, we are able to reduce the number of processed
tokens by up to 44%, without diminishing the segmentation quality.",None,-1
a0d1276e-ba07-43e7-b98b-3928a7c423de,A negation detection assessment of GPTs: analysis with the xNot360 dataset,0.16387,"Negation is a fundamental aspect of natural language, playing a critical role
in communication and comprehension. Our study assesses the negation detection
performance of Generative Pre-trained Transformer (GPT) models, specifically
GPT-2, GPT-3, GPT-3.5, and GPT-4. We focus on the identification of negation in
natural language using a zero-shot prediction approach applied to our custom
xNot360 dataset. Our approach examines sentence pairs labeled to indicate
whether the second sentence negates the first. Our findings expose a
considerable performance disparity among the GPT models, with GPT-4 surpassing
its counterparts and GPT-3.5 displaying a marked performance reduction. The
overall proficiency of the GPT models in negation detection remains relatively
modest, indicating that this task pushes the boundaries of their natural
language understanding capabilities. We not only highlight the constraints of
GPT models in handling negation but also emphasize the importance of logical
reliability in high-stakes domains such as healthcare, science, and law.",None,-1
b8af34ef-3619-4e15-8c76-443f98068aa9,Prototype Knowledge Distillation for Medical Segmentation with Missing Modality,0.646528,"Multi-modality medical imaging is crucial in clinical treatment as it can
provide complementary information for medical image segmentation. However,
collecting multi-modal data in clinical is difficult due to the limitation of
the scan time and other clinical situations. As such, it is clinically
meaningful to develop an image segmentation paradigm to handle this missing
modality problem. In this paper, we propose a prototype knowledge distillation
(ProtoKD) method to tackle the challenging problem, especially for the toughest
scenario when only single modal data can be accessed. Specifically, our ProtoKD
can not only distillate the pixel-wise knowledge of multi-modality data to
single-modality data but also transfer intra-class and inter-class feature
variations, such that the student model could learn more robust feature
representation from the teacher model and inference with only one single
modality data. Our method achieves state-of-the-art performance on BraTS
benchmark. The code is available at
\url{https://github.com/SakurajimaMaiii/ProtoKD}.",None,-1
e0bbee37-5f61-4bab-b1f1-6a1d9e37e5dd,LaMD: Latent Motion Diffusion for Video Generation,0.513936,"Generating coherent and natural movement is the key challenge in video
generation. This research proposes to condense video generation into a problem
of motion generation, to improve the expressiveness of motion and make video
generation more manageable. This can be achieved by breaking down the video
generation process into latent motion generation and video reconstruction. We
present a latent motion diffusion (LaMD) framework, which consists of a
motion-decomposed video autoencoder and a diffusion-based motion generator, to
implement this idea. Through careful design, the motion-decomposed video
autoencoder can compress patterns in movement into a concise latent motion
representation. Meanwhile, the diffusion-based motion generator is able to
efficiently generate realistic motion on a continuous latent space under
multi-modal conditions, at a cost that is similar to that of image diffusion
models. Results show that LaMD generates high-quality videos with a wide range
of motions, from stochastic dynamics to highly controllable movements. It
achieves new state-of-the-art performance on benchmark datasets, including
BAIR, Landscape and CATER-GENs, for Image-to-Video (I2V) and
Text-Image-to-Video (TI2V) generation. The source code of LaMD will be made
available soon.",None,-1
f93c2ab3-1c3b-4d6f-a4f7-1941aaed1d83,Event Temporal Relation Extraction with Bayesian Translational Model,0.425748,"Existing models to extract temporal relations between events lack a
principled method to incorporate external knowledge. In this study, we
introduce Bayesian-Trans, a Bayesian learning-based method that models the
temporal relation representations as latent variables and infers their values
via Bayesian inference and translational functions. Compared to conventional
neural approaches, instead of performing point estimation to find the best set
parameters, the proposed model infers the parameters' posterior distribution
directly, enhancing the model's capability to encode and express uncertainty
about the predictions. Experimental results on the three widely used datasets
show that Bayesian-Trans outperforms existing approaches for event temporal
relation extraction. We additionally present detailed analyses on uncertainty
quantification, comparison of priors, and ablation studies, illustrating the
benefits of the proposed approach.",None,-1
1b43fda2-e235-43fe-bf1b-a8fb3680ac9d,SpeechAlign: a Framework for Speech Translation Alignment Evaluation,0.343819,"Speech-to-Speech and Speech-to-Text translation are currently dynamic areas
of research. In our commitment to advance these fields, we present SpeechAlign,
a framework designed to evaluate the underexplored field of source-target
alignment in speech models. The SpeechAlign framework has two core components.
First, to tackle the absence of suitable evaluation datasets, we introduce the
Speech Gold Alignment dataset, built upon a English-German text translation
gold alignment dataset. Secondly, we introduce two novel metrics, Speech
Alignment Error Rate (SAER) and Time-weighted Speech Alignment Error Rate
(TW-SAER), which enable the evaluation of alignment quality within speech
models. While the former gives equal importance to each word, the latter
assigns weights based on the length of the words in the speech signal. By
publishing SpeechAlign we provide an accessible evaluation framework for model
assessment, and we employ it to benchmark open-source Speech Translation
models. In doing so, we contribute to the ongoing research progress within the
fields of Speech-to-Speech and Speech-to-Text translation.",None,-1
73d0fcba-315c-4df9-8199-1ea043ae175d,CoTracker: It is Better to Track Together,1.0,"We introduce CoTracker, a transformer-based model that tracks dense points in
a frame jointly across a video sequence. This differs from most existing
state-of-the-art approaches that track points independently, ignoring their
correlation. We show that joint tracking results in a significantly higher
tracking accuracy and robustness. We also provide several technical
innovations, including the concept of virtual tracks, which allows CoTracker to
track 70k points jointly and simultaneously. Furthermore, CoTracker operates
causally on short windows (hence, it is suitable for online tasks), but is
trained by unrolling the windows across longer video sequences, which enables
and significantly improves long-term tracking. We demonstrate qualitatively
impressive tracking results, where points can be tracked for a long time even
when they are occluded or leave the field of view. Quantitatively, CoTracker
outperforms all recent trackers on standard benchmarks, often by a substantial
margin.",None,-1
1b4c2583-f57b-4377-812b-f7db44970527,"Multi3WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",0.912754,"Creating high-quality annotated data for task-oriented dialog (ToD) is known
to be notoriously difficult, and the challenges are amplified when the goal is
to create equitable, culturally adapted, and large-scale ToD datasets for
multiple languages. Therefore, the current datasets are still very scarce and
suffer from limitations such as translation-based non-native dialogs with
translation artefacts, small scale, or lack of cultural adaptation, among
others. In this work, we first take stock of the current landscape of
multilingual ToD datasets, offering a systematic overview of their properties
and limitations. Aiming to reduce all the detected limitations, we then
introduce Multi3WOZ, a novel multilingual, multi-domain, multi-parallel ToD
dataset. It is large-scale and offers culturally adapted dialogs in 4 languages
to enable training and evaluation of multilingual and cross-lingual ToD
systems. We describe a complex bottom-up data collection process that yielded
the final dataset, and offer the first sets of baseline scores across different
ToD-related tasks for future reference, also highlighting its challenging
nature.",None,-1
27430a44-0872-43c7-9632-699633f98bb1,Calibrating LLM-Based Evaluator,0.366861,"Recent advancements in large language models (LLMs) on language modeling and
emergent capabilities make them a promising reference-free evaluator of natural
language generation quality, and a competent alternative to human evaluation.
However, hindered by the closed-source or high computational demand to host and
tune, there is a lack of practice to further calibrate an off-the-shelf
LLM-based evaluator towards better human alignment. In this work, we propose
AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate
and align an LLM-based evaluator toward human preference. Instead of explicitly
modeling human preferences, we first implicitly encompass them within a set of
human labels. Then, an initial set of scoring criteria is drafted by the
language model itself, leveraging in-context learning on different few-shot
examples. To further calibrate this set of criteria, we select the best
performers and re-draft them with self-refinement. Our experiments on multiple
text quality evaluation datasets illustrate a significant improvement in
correlation with expert evaluation through calibration. Our comprehensive
qualitative analysis conveys insightful intuitions and observations on the
essence of effective scoring criteria.",None,-1
14de8430-e7ff-41cc-afeb-6a563ef1fde8,CPLLM: Clinical Prediction with Large Language Models,0.721166,"We present Clinical Prediction with Large Language Models (CPLLM), a method
that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical
disease and readmission prediction. We utilized quantization and fine-tuned the
LLM using prompts. For diagnosis prediction, we predict whether patients will
be diagnosed with a target disease during their next visit or in the subsequent
diagnosis, leveraging their historical diagnosis records. We compared our
results to various baselines, including RETAIN, and Med-BERT, the current
state-of-the-art model for disease prediction using temporal structured EHR
data. In addition, We also evaluated CPLLM for patient hospital readmission
prediction and compared our method's performance with benchmark baselines. Our
experiments have shown that our proposed method, CPLLM, surpasses all the
tested models in terms of PR-AUC and ROC-AUC metrics, showing state-of-the-art
results for diagnosis prediction and patient hospital readmission prediction.
Such a method can be easily implemented and integrated into the clinical
process to help care providers estimate the next steps of patients",None,-1
3cb11b89-bcdd-4376-981f-4addd7a550c3,Point2Vec for Self-Supervised Representation Learning on Point Clouds,0.554849,"Recently, the self-supervised learning framework data2vec has shown inspiring
performance for various modalities using a masked student-teacher approach.
However, it remains open whether such a framework generalizes to the unique
challenges of 3D point clouds. To answer this question, we extend data2vec to
the point cloud domain and report encouraging results on several downstream
tasks. In an in-depth analysis, we discover that the leakage of positional
information reveals the overall object shape to the student even under heavy
masking and thus hampers data2vec to learn strong representations for point
clouds. We address this 3D-specific shortcoming by proposing point2vec, which
unleashes the full potential of data2vec-like pre-training on point clouds. Our
experiments show that point2vec outperforms other self-supervised methods on
shape classification and few-shot learning on ModelNet40 and ScanObjectNN,
while achieving competitive results on part segmentation on ShapeNetParts.
These results suggest that the learned representations are strong and
transferable, highlighting point2vec as a promising direction for
self-supervised learning of point cloud representations.",None,-1
7dcb6620-292d-40ab-934e-4ffe95699041,ACTOR: Active Learning with Annotator-specific Classification Heads to Embrace Human Label Variation,0.380552,"Label aggregation such as majority voting is commonly used to resolve
annotator disagreement in dataset creation. However, this may disregard
minority values and opinions. Recent studies indicate that learning from
individual annotations outperforms learning from aggregated labels, though they
require a considerable amount of annotation. Active learning, as an annotation
cost-saving strategy, has not been fully explored in the context of learning
from disagreement. We show that in the active learning setting, a multi-head
model performs significantly better than a single-head model in terms of
uncertainty estimation. By designing and evaluating acquisition functions with
annotator-specific heads on two datasets, we show that group-level entropy
works generally well on both datasets. Importantly, it achieves performance in
terms of both prediction and uncertainty estimation comparable to full-scale
training from disagreement, while saving up to 70% of the annotation budget.",None,-1
24628ce6-73a3-49a5-ab20-faa08a4eeddc,Backpropagation of Unrolled Solvers with Folded Optimization,0.61748,"The integration of constrained optimization models as components in deep
networks has led to promising advances on many specialized learning tasks. A
central challenge in this setting is backpropagation through the solution of an
optimization problem, which typically lacks a closed form. One typical strategy
is algorithm unrolling, which relies on automatic differentiation through the
operations of an iterative solver. While flexible and general, unrolling can
encounter accuracy and efficiency issues in practice. These issues can be
avoided by analytical differentiation of the optimization, but current
frameworks impose rigid requirements on the optimization problem's form. This
paper provides theoretical insights into the backward pass of unrolled
optimization, leading to a system for generating efficiently solvable
analytical models of backpropagation. Additionally, it proposes a unifying view
of unrolling and analytical differentiation through optimization mappings.
Experiments over various model-based learning tasks demonstrate the advantages
of the approach both computationally and in terms of enhanced expressiveness.",None,-1
549b646c-57bb-4550-b628-cfeaf5a94388,Automatically Identifying Relations Between Self-Admitted Technical Debt Across Different Sources,0.345399,"Self-Admitted Technical Debt or SATD can be found in various sources, such as
source code comments, commit messages, issue tracking systems, and pull
requests. Previous research has established the existence of relations between
SATD items in different sources; such relations can be useful for investigating
and improving SATD management. However, there is currently a lack of approaches
for automatically detecting these SATD relations. To address this, we proposed
and evaluated approaches for automatically identifying SATD relations across
different sources. Our findings show that our approach outperforms baseline
approaches by a large margin, achieving an average F1-score of 0.829 in
identifying relations between SATD items. Moreover, we explored the
characteristics of SATD relations in 103 open-source projects and describe nine
major cases in which related SATD is documented in a second source, and give a
quantitative overview of 26 kinds of relations.",None,-1
eb3626cc-df8f-4857-9f12-52aad1ebca59,Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation,0.255173,"Current reinforcement learning (RL) often suffers when solving a challenging
exploration problem where the desired outcomes or high rewards are rarely
observed. Even though curriculum RL, a framework that solves complex tasks by
proposing a sequence of surrogate tasks, shows reasonable results, most of the
previous works still have difficulty in proposing curriculum due to the absence
of a mechanism for obtaining calibrated guidance to the desired outcome state
without any prior domain knowledge. To alleviate it, we propose an uncertainty
& temporal distance-aware curriculum goal generation method for the
outcome-directed RL via solving a bipartite matching problem. It could not only
provide precisely calibrated guidance of the curriculum to the desired outcome
states but also bring much better sample efficiency and geometry-agnostic
curriculum goal proposal capability compared to previous curriculum RL methods.
We demonstrate that our algorithm significantly outperforms these prior methods
in a variety of challenging navigation tasks and robotic manipulation tasks in
a quantitative and qualitative way.",None,-1
a076f502-fae9-45da-a78d-1d398030f506,Toolformer: Language Models Can Teach Themselves to Use Tools,1.0,"Language models (LMs) exhibit remarkable abilities to solve new tasks from
just a few examples or textual instructions, especially at scale. They also,
paradoxically, struggle with basic functionality, such as arithmetic or factual
lookup, where much simpler and smaller models excel. In this paper, we show
that LMs can teach themselves to use external tools via simple APIs and achieve
the best of both worlds. We introduce Toolformer, a model trained to decide
which APIs to call, when to call them, what arguments to pass, and how to best
incorporate the results into future token prediction. This is done in a
self-supervised way, requiring nothing more than a handful of demonstrations
for each API. We incorporate a range of tools, including a calculator, a Q\&A
system, two different search engines, a translation system, and a calendar.
Toolformer achieves substantially improved zero-shot performance across a
variety of downstream tasks, often competitive with much larger models, without
sacrificing its core language modeling abilities.",None,-1
39459497-dd37-4086-80d7-748f7ea4475a,Calib-Anything: Zero-training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything,0.907193,"The research on extrinsic calibration between Light Detection and
Ranging(LiDAR) and camera are being promoted to a more accurate, automatic and
generic manner. Since deep learning has been employed in calibration, the
restrictions on the scene are greatly reduced. However, data driven method has
the drawback of low transfer-ability. It cannot adapt to dataset variations
unless additional training is taken. With the advent of foundation model, this
problem can be significantly mitigated. By using the Segment Anything
Model(SAM), we propose a novel LiDAR-camera calibration method, which requires
zero extra training and adapts to common scenes. With an initial guess, we
opimize the extrinsic parameter by maximizing the consistency of points that
are projected inside each image mask. The consistency includes three properties
of the point cloud: the intensity, normal vector and categories derived from
some segmentation methods. The experiments on different dataset have
demonstrated the generality and comparable accuracy of our method. The code is
available at https://github.com/OpenCalib/CalibAnything.",None,-1
521bb9e2-4f66-4791-ab28-cbc6c6d47a4e,"More Data Types More Problems: A Temporal Analysis of Complexity, Stability, and Sensitivity in Privacy Policies",0.523632,"Collecting personally identifiable information (PII) on data subjects has
become big business. Data brokers and data processors are part of a
multi-billion-dollar industry that profits from collecting, buying, and selling
consumer data. Yet there is little transparency in the data collection industry
which makes it difficult to understand what types of data are being collected,
used, and sold, and thus the risk to individual data subjects. In this study,
we examine a large textual dataset of privacy policies from 1997-2019 in order
to investigate the data collection activities of data brokers and data
processors. We also develop an original lexicon of PII-related terms
representing PII data types curated from legislative texts. This mesoscale
analysis looks at privacy policies overtime on the word, topic, and network
levels to understand the stability, complexity, and sensitivity of privacy
policies over time. We find that (1) privacy legislation correlates with
changes in stability and turbulence of PII data types in privacy policies; (2)
the complexity of privacy policies decreases over time and becomes more
regularized; (3) sensitivity rises over time and shows spikes that are
correlated with events when new privacy legislation is introduced.",None,-1
a87330f0-0900-4158-8a49-4d1e1e39032f,Machine-Created Universal Language for Cross-lingual Transfer,0.418419,"There are two primary approaches to addressing cross-lingual transfer:
multilingual pre-training, which implicitly aligns the hidden representations
of various languages, and translate-test, which explicitly translates different
languages into an intermediate language, such as English. Translate-test offers
better interpretability compared to multilingual pre-training. However, it has
lower performance than multilingual pre-training(Conneau and Lample, 2019;
Conneau et al, 2020) and struggles with word-level tasks due to translation
altering word order. As a result, we propose a new Machine-created Universal
Language (MUL) as an alternative intermediate language. MUL comprises a set of
discrete symbols forming a universal vocabulary and a natural language to MUL
translator for converting multiple natural languages to MUL. MUL unifies shared
concepts from various languages into a single universal word, enhancing
cross-language transfer. Additionally, MUL retains language-specific words and
word order, allowing the model to be easily applied to word-level tasks. Our
experiments demonstrate that translating into MUL yields improved performance
compared to multilingual pre-training, and our analysis indicates that MUL
possesses strong interpretability. The code is at:
https://github.com/microsoft/Unicoder/tree/master/MCUL.",None,-1
3ea37901-73e0-4978-b79e-ad9ecf704b62,DiagGPT: An LLM-based and Multi-agent Dialogue System with Automatic Topic Management for Flexible Task-Oriented Dialogue,0.15949,"A significant application of Large Language Models (LLMs), like ChatGPT, is
their deployment as chat agents, which respond to human inquiries across a
variety of domains. While current LLMs proficiently answer general questions,
they often fall short in complex diagnostic scenarios such as legal, medical,
or other specialized consultations. These scenarios typically require
Task-Oriented Dialogue (TOD), where an AI chat agent must proactively pose
questions and guide users toward specific goals or task completion. Previous
fine-tuning models have underperformed in TOD and the full potential of
conversational capability in current LLMs has not yet been fully explored. In
this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative
approach that extends LLMs to more TOD scenarios. In addition to guiding users
to complete tasks, DiagGPT can effectively manage the status of all topics
throughout the dialogue development. This feature enhances user experience and
offers a more flexible interaction in TOD. Our experiments demonstrate that
DiagGPT exhibits outstanding performance in conducting TOD with users, showing
its potential for practical applications in various fields.",None,-1
4211b2f7-bf07-43d2-9225-9256584f3d39,Prompting Neural Machine Translation with Translation Memories,0.113071,"Improving machine translation (MT) systems with translation memories (TMs) is
of great interest to practitioners in the MT community. However, previous
approaches require either a significant update of the model architecture and/or
additional training efforts to make the models well-behaved when TMs are taken
as additional input. In this paper, we present a simple but effective method to
introduce TMs into neural machine translation (NMT) systems. Specifically, we
treat TMs as prompts to the NMT model at test time, but leave the training
process unchanged. The result is a slight update of an existing NMT system,
which can be implemented in a few hours by anyone who is familiar with NMT.
Experimental results on several datasets demonstrate that our system
significantly outperforms strong baselines.",None,-1
410e36f5-13af-48a6-9504-dc5d5d9efde9,Simple diffusion: End-to-end diffusion for high resolution images,0.999643,"Currently, applying diffusion models in pixel space of high resolution images
is difficult. Instead, existing approaches focus on diffusion in lower
dimensional spaces (latent diffusion), or have multiple super-resolution levels
of generation referred to as cascades. The downside is that these approaches
add additional complexity to the diffusion framework.
  This paper aims to improve denoising diffusion for high resolution images
while keeping the model as simple as possible. The paper is centered around the
research question: How can one train a standard denoising diffusion models on
high resolution images, and still obtain performance comparable to these
alternate approaches?
  The four main findings are: 1) the noise schedule should be adjusted for high
resolution images, 2) It is sufficient to scale only a particular part of the
architecture, 3) dropout should be added at specific locations in the
architecture, and 4) downsampling is an effective strategy to avoid high
resolution feature maps. Combining these simple yet effective techniques, we
achieve state-of-the-art on image generation among diffusion models without
sampling modifiers on ImageNet.",None,-1
724faf02-7b3a-48bf-965f-97d5d083d192,Document Flattening: Beyond Concatenating Context for Document-Level Neural Machine Translation,0.901852,"Existing work in document-level neural machine translation commonly
concatenates several consecutive sentences as a pseudo-document, and then
learns inter-sentential dependencies. This strategy limits the model's ability
to leverage information from distant context. We overcome this limitation with
a novel Document Flattening (DocFlat) technique that integrates Flat-Batch
Attention (FBA) and Neural Context Gate (NCG) into Transformer model to utilize
information beyond the pseudo-document boundaries. FBA allows the model to
attend to all the positions in the batch and learns the relationships between
positions explicitly and NCG identifies the useful information from the distant
context. We conduct comprehensive experiments and analyses on three benchmark
datasets for English-German translation, and validate the effectiveness of two
variants of DocFlat. Empirical results show that our approach outperforms
strong baselines with statistical significance on BLEU, COMET and accuracy on
the contrastive test set. The analyses highlight that DocFlat is highly
effective in capturing the long-range information.",None,-1
9dd79f3e-487a-40c8-8b32-36858bb7088c,"TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT",0.723464,"Tables are prevalent in real-world databases, requiring significant time and
effort for humans to analyze and manipulate. The advancements in large language
models (LLMs) have made it possible to interact with tables using natural
language input, bringing this capability closer to reality. In this paper, we
present TableGPT, a unified fine-tuned framework that enables LLMs to
understand and operate on tables using external functional commands. It
introduces the capability to seamlessly interact with tables, enabling a wide
range of functionalities such as question answering, data manipulation (e.g.,
insert, delete, query, and modify operations), data visualization, analysis
report generation, and automated prediction. TableGPT aims to provide
convenience and accessibility to users by empowering them to effortlessly
leverage tabular data. At the core of TableGPT lies the novel concept of global
tabular representations, which empowers LLMs to gain a comprehensive
understanding of the entire table beyond meta-information. By jointly training
LLMs on both table and text modalities, TableGPT achieves a deep understanding
of tabular data and the ability to perform complex operations on tables through
chain-of-command instructions. Importantly, TableGPT offers the advantage of
being a self-contained system rather than relying on external API interfaces.
Moreover, it supports efficient data process flow, query rejection (when
appropriate) and private deployment, enabling faster domain data fine-tuning
and ensuring data privacy, which enhances the framework's adaptability to
specific use cases.",None,-1
65a244c6-b414-4503-a7c0-8909dc513e00,Gaze-Driven Sentence Simplification for Language Learners: Enhancing Comprehension and Readability,0.0187222,"Language learners should regularly engage in reading challenging materials as
part of their study routine. Nevertheless, constantly referring to dictionaries
is time-consuming and distracting. This paper presents a novel gaze-driven
sentence simplification system designed to enhance reading comprehension while
maintaining their focus on the content. Our system incorporates machine
learning models tailored to individual learners, combining eye gaze features
and linguistic features to assess sentence comprehension. When the system
identifies comprehension difficulties, it provides simplified versions by
replacing complex vocabulary and grammar with simpler alternatives via GPT-3.5.
We conducted an experiment with 19 English learners, collecting data on their
eye movements while reading English text. The results demonstrated that our
system is capable of accurately estimating sentence-level comprehension.
Additionally, we found that GPT-3.5 simplification improved readability in
terms of traditional readability metrics and individual word difficulty,
paraphrasing across different linguistic levels.",None,-1
31334cd0-68e3-45fa-8663-52d0564ef927,Adaptive manifold for imbalanced transductive few-shot learning,0.20804,"Transductive few-shot learning algorithms have showed substantially superior
performance over their inductive counterparts by leveraging the unlabeled
queries. However, the vast majority of such methods are evaluated on perfectly
class-balanced benchmarks. It has been shown that they undergo remarkable drop
in performance under a more realistic, imbalanced setting. To this end, we
propose a novel algorithm to address imbalanced transductive few-shot learning,
named Adaptive Manifold. Our method exploits the underlying manifold of the
labeled support examples and unlabeled queries by using manifold similarity to
predict the class probability distribution per query. It is parameterized by
one centroid per class as well as a set of graph-specific parameters that
determine the manifold. All parameters are optimized through a loss function
that can be tuned towards class-balanced or imbalanced distributions. The
manifold similarity shows substantial improvement over Euclidean distance,
especially in the 1-shot setting. Our algorithm outperforms or is on par with
other state of the art methods in three benchmark datasets, namely
miniImageNet, tieredImageNet and CUB, and three different backbones, namely
ResNet-18, WideResNet-28-10 and DenseNet-121. In certain cases, our algorithm
outperforms the previous state of the art by as much as 4.2%.",None,-1
cbf71df2-8383-4b2c-b900-be61847351d3,Towards Agile Text Classifiers for Everyone,0.377731,"Text-based safety classifiers are widely used for content moderation and
increasingly to tune generative language model behavior - a topic of growing
concern for the safety of digital assistants and chatbots. However, different
policies require different classifiers, and safety policies themselves improve
from iteration and adaptation. This paper introduces and evaluates methods for
agile text classification, whereby classifiers are trained using small,
targeted datasets that can be quickly developed for a particular policy.
Experimenting with 7 datasets from three safety-related domains, comprising 15
annotation schemes, led to our key finding: prompt-tuning large language
models, like PaLM 62B, with a labeled dataset of as few as 80 examples can
achieve state-of-the-art performance. We argue that this enables a paradigm
shift for text classification, especially for models supporting safer online
discourse. Instead of collecting millions of examples to attempt to create
universal safety classifiers over months or years, classifiers could be tuned
using small datasets, created by individuals or small organizations, tailored
for specific use cases, and iterated on and adapted in the time-span of a day.",None,-1
5c07fc11-befd-44c5-ae12-1766d76b2453,"LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",0.105555,"Large language models (LLMs) are popular for high-quality text generation but
can produce harmful content, even when aligned with human values through
reinforcement learning. Adversarial prompts can bypass their safety measures.
We propose LLM Self Defense, a simple approach to defend against these attacks
by having an LLM screen the induced responses. Our method does not require any
fine-tuning, input preprocessing, or iterative output generation. Instead, we
incorporate the generated content into a pre-defined prompt and employ another
instance of an LLM to analyze the text and predict whether it is harmful. We
test LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent
LLMs against various types of attacks, such as forcefully inducing affirmative
responses to prompts and prompt engineering attacks. Notably, LLM Self Defense
succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5
and Llama 2. The code is publicly available at
https://github.com/poloclub/llm-self-defense",None,-1
0778462b-3779-4011-a580-b648fbde1232,MoStGAN-V: Video Generation with Temporal Motion Styles,0.700254,"Video generation remains a challenging task due to spatiotemporal complexity
and the requirement of synthesizing diverse motions with temporal consistency.
Previous works attempt to generate videos in arbitrary lengths either in an
autoregressive manner or regarding time as a continuous signal. However, they
struggle to synthesize detailed and diverse motions with temporal coherence and
tend to generate repetitive scenes after a few time steps. In this work, we
argue that a single time-agnostic latent vector of style-based generator is
insufficient to model various and temporally-consistent motions. Hence, we
introduce additional time-dependent motion styles to model diverse motion
patterns. In addition, a Motion Style Attention modulation mechanism, dubbed as
MoStAtt, is proposed to augment frames with vivid dynamics for each specific
scale (i.e., layer), which assigns attention score for each motion style w.r.t
deconvolution filter weights in the target synthesis layer and softly attends
different motion styles for weight modulation. Experimental results show our
model achieves state-of-the-art performance on four unconditional $256^2$ video
synthesis benchmarks trained with only 3 frames per clip and produces better
qualitative results with respect to dynamic motions. Code and videos have been
made available at https://github.com/xiaoqian-shen/MoStGAN-V.",None,-1
917a23e9-8dc9-4d35-b903-6f95ce578cd3,Virtual Occlusions Through Implicit Depth,0.317054,"For augmented reality (AR), it is important that virtual assets appear to
`sit among' real world objects. The virtual element should variously occlude
and be occluded by real matter, based on a plausible depth ordering. This
occlusion should be consistent over time as the viewer's camera moves.
Unfortunately, small mistakes in the estimated scene depth can ruin the
downstream occlusion mask, and thereby the AR illusion. Especially in real-time
settings, depths inferred near boundaries or across time can be inconsistent.
In this paper, we challenge the need for depth-regression as an intermediate
step.
  We instead propose an implicit model for depth and use that to predict the
occlusion mask directly. The inputs to our network are one or more color
images, plus the known depths of any virtual geometry. We show how our
occlusion predictions are more accurate and more temporally stable than
predictions derived from traditional depth-estimation models. We obtain
state-of-the-art occlusion results on the challenging ScanNetv2 dataset and
superior qualitative results on real scenes.",None,-1
ea8b9b8f-21b9-4d9b-b8b6-8b16538f0a4d,Hallucination is the last thing you need,0.647517,"The legal profession necessitates a multidimensional approach that involves
synthesizing an in-depth comprehension of a legal issue with insightful
commentary based on personal experience, combined with a comprehensive
understanding of pertinent legislation, regulation, and case law, in order to
deliver an informed legal solution. The present offering with generative AI
presents major obstacles in replicating this, as current models struggle to
integrate and navigate such a complex interplay of understanding, experience,
and fact-checking procedures. It is noteworthy that where generative AI outputs
understanding and experience, which reflect the aggregate of various subjective
views on similar topics, this often deflects the model's attention from the
crucial legal facts, thereby resulting in hallucination. Hence, this paper
delves into the feasibility of three independent LLMs, each focused on
understanding, experience, and facts, synthesising as one single ensemble model
to effectively counteract the current challenges posed by the existing
monolithic generative AI models. We introduce an idea of mutli-length
tokenisation to protect key information assets like common law judgements, and
finally we interrogate the most advanced publicly available models for legal
hallucination, with some interesting results.",None,-1
22f388cc-078f-41f4-8e2a-1c62b4bec209,Diffusion Models for Memory-efficient Processing of 3D Medical Images,0.542007,"Denoising diffusion models have recently achieved state-of-the-art
performance in many image-generation tasks. They do, however, require a large
amount of computational resources. This limits their application to medical
tasks, where we often deal with large 3D volumes, like high-resolution
three-dimensional data. In this work, we present a number of different ways to
reduce the resource consumption for 3D diffusion models and apply them to a
dataset of 3D images. The main contribution of this paper is the
memory-efficient patch-based diffusion model \textit{PatchDDM}, which can be
applied to the total volume during inference while the training is performed
only on patches. While the proposed diffusion model can be applied to any image
generation tasks, we evaluate the method on the tumor segmentation task of the
BraTS2020 dataset and demonstrate that we can generate meaningful
three-dimensional segmentations.",None,-1
ba8f2c3c-4b8a-4f6a-bdf5-473f728934b0,CLIP the Gap: A Single Domain Generalization Approach for Object Detection,0.6474,"Single Domain Generalization (SDG) tackles the problem of training a model on
a single source domain so that it generalizes to any unseen target domain.
While this has been well studied for image classification, the literature on
SDG object detection remains almost non-existent. To address the challenges of
simultaneously learning robust object localization and representation, we
propose to leverage a pre-trained vision-language model to introduce semantic
domain concepts via textual prompts. We achieve this via a semantic
augmentation strategy acting on the features extracted by the detector
backbone, as well as a text-based classification loss. Our experiments evidence
the benefits of our approach, outperforming by 10% the only existing SDG object
detection method, Single-DGOD [49], on their own diverse weather-driving
benchmark.",None,-1
19c284ed-0a11-41cd-aa70-13ce0b0ede6d,Belief revision and incongruity: is it a joke?,0.0477713,"Incongruity often makes people laugh. You have to be smart to say stupid
things. It requires to be even smarter for understanding them. This paper is a
shameless attempt to formalize this intelligent behavior in the case of an
agent listening to a joke. All this is a matter of revision of beliefs,
surprise and violation of norms.",None,-1
0ff270e6-390b-413f-9816-896190b424cc,ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior Constraints,0.0363728,"Recent text-to-image generative models have enabled us to transform our words
into vibrant, captivating imagery. The surge of personalization techniques that
has followed has also allowed us to imagine unique concepts in new scenes.
However, an intriguing question remains: How can we generate a new, imaginary
concept that has never been seen before? In this paper, we present the task of
creative text-to-image generation, where we seek to generate new members of a
broad category (e.g., generating a pet that differs from all existing pets). We
leverage the under-studied Diffusion Prior models and show that the creative
generation problem can be formulated as an optimization process over the output
space of the diffusion prior, resulting in a set of ""prior constraints"". To
keep our generated concept from converging into existing members, we
incorporate a question-answering Vision-Language Model (VLM) that adaptively
adds new constraints to the optimization problem, encouraging the model to
discover increasingly more unique creations. Finally, we show that our prior
constraints can also serve as a strong mixing mechanism allowing us to create
hybrids between generated concepts, introducing even more flexibility into the
creative process.",None,-1
e7036f6f-fd17-4616-a987-5a9834ccded1,Do large language models solve verbal analogies like children do?,0.585665,"Analogy-making lies at the heart of human cognition. Adults solve analogies
such as \textit{Horse belongs to stable like chicken belongs to ...?} by
mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In
contrast, children often use association, e.g., answering \textit{egg}. This
paper investigates whether large language models (LLMs) solve verbal analogies
in A:B::C:? form using associations, similar to what children do. We use verbal
analogies extracted from an online adaptive learning environment, where 14,002
7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six
tested Dutch monolingual and multilingual LLMs performed around the same level
as children, with MGPT performing worst, around the 7-year-old level, and XLM-V
and GPT-3 the best, slightly above the 11-year-old level. However, when we
control for associative processes this picture changes and each model's
performance level drops 1-2 years. Further experiments demonstrate that
associative processes often underlie correctly solved analogies. We conclude
that the LLMs we tested indeed tend to solve verbal analogies by association
with C like children do.",None,-1
e92d41a9-4aa2-4ad3-8fc9-23ae5290b3ad,Ambiguous Medical Image Segmentation using Diffusion Models,0.976718,"Collective insights from a group of experts have always proven to outperform
an individual's best diagnostic for clinical tasks. For the task of medical
image segmentation, existing research on AI-based alternatives focuses more on
developing models that can imitate the best individual rather than harnessing
the power of expert groups. In this paper, we introduce a single diffusion
model-based approach that produces multiple plausible outputs by learning a
distribution over group insights. Our proposed model generates a distribution
of segmentation masks by leveraging the inherent stochastic sampling process of
diffusion using only minimal additional learning. We demonstrate on three
different medical image modalities- CT, ultrasound, and MRI that our model is
capable of producing several possible variants while capturing the frequencies
of their occurrences. Comprehensive results show that our proposed approach
outperforms existing state-of-the-art ambiguous segmentation networks in terms
of accuracy while preserving naturally occurring variation. We also propose a
new metric to evaluate the diversity as well as the accuracy of segmentation
predictions that aligns with the interest of clinical practice of collective
insights.",None,-1
041bf4c9-22ea-45d3-847c-ac51e9526cb3,Watch out Venomous Snake Species: A Solution to SnakeCLEF2023,0.779217,"The SnakeCLEF2023 competition aims to the development of advanced algorithms
for snake species identification through the analysis of images and
accompanying metadata. This paper presents a method leveraging utilization of
both images and metadata. Modern CNN models and strong data augmentation are
utilized to learn better representation of images. To relieve the challenge of
long-tailed distribution, seesaw loss is utilized in our method. We also design
a light model to calculate prior probabilities using metadata features
extracted from CLIP in post processing stage. Besides, we attach more
importance to venomous species by assigning venomous species labels to some
examples that model is uncertain about. Our method achieves 91.31% score of the
final metric combined of F1 and other metrics on private leaderboard, which is
the 1st place among the participators. The code is available at
https://github.com/xiaoxsparraw/CLEF2023.",None,-1
87b5c22b-2b75-4e23-a42b-86adf3c37c82,Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning,0.232074,"Text-based reinforcement learning agents have predominantly been neural
network-based models with embeddings-based representation, learning
uninterpretable policies that often do not generalize well to unseen games. On
the other hand, neuro-symbolic methods, specifically those that leverage an
intermediate formal representation, are gaining significant attention in
language understanding tasks. This is because of their advantages ranging from
inherent interpretability, the lesser requirement of training data, and being
generalizable in scenarios with unseen data. Therefore, in this paper, we
propose a modular, NEuro-Symbolic Textual Agent (NESTA) that combines a generic
semantic parser with a rule induction system to learn abstract interpretable
rules as policies. Our experiments on established text-based game benchmarks
show that the proposed NESTA method outperforms deep reinforcement
learning-based techniques by achieving better generalization to unseen test
games and learning from fewer training interactions.",None,-1
2bef8cf3-9145-4ac5-9628-a8c107431d71,Ladder Fine-tuning approach for SAM integrating complementary network,0.806241,"Recently, foundation models have been introduced demonstrating various tasks
in the field of computer vision. These models such as Segment Anything Model
(SAM) are generalized models trained using huge datasets. Currently, ongoing
research focuses on exploring the effective utilization of these generalized
models for specific domains, such as medical imaging. However, in medical
imaging, the lack of training samples due to privacy concerns and other factors
presents a major challenge for applying these generalized models to medical
image segmentation task. To address this issue, the effective fine tuning of
these models is crucial to ensure their optimal utilization. In this study, we
propose to combine a complementary Convolutional Neural Network (CNN) along
with the standard SAM network for medical image segmentation. To reduce the
burden of fine tuning large foundation model and implement cost-efficient
trainnig scheme, we focus only on fine-tuning the additional CNN network and
SAM decoder part. This strategy significantly reduces trainnig time and
achieves competitive results on publicly available dataset. The code is
available at https://github.com/11yxk/SAM-LST.",None,-1
36562f74-8171-4299-b050-80d639e2d9eb,Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence,0.777899,"Sentence-level representations are beneficial for various natural language
processing tasks. It is commonly believed that vector representations can
capture rich linguistic properties. Currently, large language models (LMs)
achieve state-of-the-art performance on sentence embedding. However, some
recent works suggest that vector representations from LMs can cause information
leakage. In this work, we further investigate the information leakage issue and
propose a generative embedding inversion attack (GEIA) that aims to reconstruct
input sequences based only on their sentence embeddings. Given the black-box
access to a language model, we treat sentence embeddings as initial tokens'
representations and train or fine-tune a powerful decoder model to decode the
whole sequences directly. We conduct extensive experiments to demonstrate that
our generative inversion attack outperforms previous embedding inversion
attacks in classification metrics and generates coherent and contextually
similar sentences as the original inputs.",None,-1
6b4dda8e-5528-4dff-af86-6f3d29555aa4,Scaling Open-Vocabulary Object Detection,0.948342,"Open-vocabulary object detection has benefited greatly from pretrained
vision-language models, but is still limited by the amount of available
detection training data. While detection training data can be expanded by using
Web image-text pairs as weak supervision, this has not been done at scales
comparable to image-level pretraining. Here, we scale up detection data with
self-training, which uses an existing detector to generate pseudo-box
annotations on image-text pairs. Major challenges in scaling self-training are
the choice of label space, pseudo-annotation filtering, and training
efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which
address these challenges. OWLv2 surpasses the performance of previous
state-of-the-art open-vocabulary detectors already at comparable training
scales (~10M examples). However, with OWL-ST, we can scale to over 1B examples,
yielding further large improvement: With an L/14 architecture, OWL-ST improves
AP on LVIS rare classes, for which the model has seen no human box annotations,
from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale
training for open-world localization, similar to what has been seen for image
classification and language modelling.",None,-1
801819f5-5a03-4066-b0d9-017b0c534e05,Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering,0.297078,"The final frontier for simulation is the accurate representation of complex,
real-world social systems. While agent-based modeling (ABM) seeks to study the
behavior and interactions of agents within a larger system, it is unable to
faithfully capture the full complexity of human-driven behavior. Large language
models (LLMs), like ChatGPT, have emerged as a potential solution to this
bottleneck by enabling researchers to explore human-driven interactions in
previously unimaginable ways. Our research investigates simulations of human
interactions using LLMs. Through prompt engineering, inspired by Park et al.
(2023), we present two simulations of believable proxies of human behavior: a
two-agent negotiation and a six-agent murder mystery game.",None,-1
bedbcc7a-04b1-4d9c-bfd8-f1e7827f6d30,CoSMo: A constructor specification language for Abstract Wikipedia's content selection process,0.264234,"Representing snippets of information abstractly is a task that needs to be
performed for various purposes, such as database view specification and the
first stage in the natural language generation pipeline for generative AI from
structured input, i.e., the content selection stage to determine what needs to
be verbalised. For the Abstract Wikipedia project, requirements analysis
revealed that such an abstract representation requires multilingual modelling,
content selection covering declarative content and functions, and both classes
and instances. There is no modelling language that meets either of the three
features, let alone a combination. Following a rigorous language design process
inclusive of broad stakeholder consultation, we created CoSMo, a novel {\sc
Co}ntent {\sc S}election {\sc Mo}deling language that meets these and other
requirements so that it may be useful both in Abstract Wikipedia as well as
other contexts. We describe the design process, rationale and choices, the
specification, and preliminary evaluation of the language.",None,-1
c9fc9946-a364-4787-9e41-87dd6d276b2f,Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,0.870795,"Object affordance is an important concept in hand-object interaction,
providing information on action possibilities based on human motor capacity and
objects' physical property thus benefiting tasks such as action anticipation
and robot imitation learning. However, the definition of affordance in existing
datasets often: 1) mix up affordance with object functionality; 2) confuse
affordance with goal-related action; and 3) ignore human motor capacity. This
paper proposes an efficient annotation scheme to address these issues by
combining goal-irrelevant motor actions and grasp types as affordance labels
and introducing the concept of mechanical action to represent the action
possibilities between two objects. We provide new annotations by applying this
scheme to the EPIC-KITCHENS dataset and test our annotation with tasks such as
affordance recognition, hand-object interaction hotspots prediction, and
cross-domain evaluation of affordance. The results show that models trained
with our annotation can distinguish affordance from other concepts, predict
fine-grained interaction possibilities on objects, and generalize through
different domains.",None,-1
a9a9fb57-dcad-4694-821e-89217b5c5d9e,RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks,0.949104,"Spiking Neural Networks (SNNs) as one of the biology-inspired models have
received much attention recently. It can significantly reduce energy
consumption since they quantize the real-valued membrane potentials to 0/1
spikes to transmit information thus the multiplications of activations and
weights can be replaced by additions when implemented on hardware. However,
this quantization mechanism will inevitably introduce quantization error, thus
causing catastrophic information loss. To address the quantization error
problem, we propose a regularizing membrane potential loss (RMP-Loss) to adjust
the distribution which is directly related to quantization error to a range
close to the spikes. Our method is extremely simple to implement and
straightforward to train an SNN. Furthermore, it is shown to consistently
outperform previous state-of-the-art methods over different network
architectures and datasets.",None,-1
92bdf0be-3937-4101-8de5-03f120fd6b30,Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning,0.999377,"The Mixture of Experts (MoE) is a widely known neural architecture where an
ensemble of specialized sub-models optimizes overall performance with a
constant computational cost. However, conventional MoEs pose challenges at
scale due to the need to store all experts in memory. In this paper, we push
MoE to the limit. We propose extremely parameter-efficient MoE by uniquely
combining MoE architecture with lightweight experts.Our MoE architecture
outperforms standard parameter-efficient fine-tuning (PEFT) methods and is on
par with full fine-tuning by only updating the lightweight experts -- less than
1% of an 11B parameters model. Furthermore, our method generalizes to unseen
tasks as it does not depend on any prior task knowledge. Our research
underscores the versatility of the mixture of experts architecture, showcasing
its ability to deliver robust performance even when subjected to rigorous
parameter constraints. Our code used in all the experiments is publicly
available here: https://github.com/for-ai/parameter-efficient-moe.",None,-1
a81f5eeb-fdff-4d34-a143-b3a81e41c9cd,PANet: LiDAR Panoptic Segmentation with Sparse Instance Proposal and Aggregation,0.309976,"Reliable LiDAR panoptic segmentation (LPS), including both semantic and
instance segmentation, is vital for many robotic applications, such as
autonomous driving. This work proposes a new LPS framework named PANet to
eliminate the dependency on the offset branch and improve the performance on
large objects, which are always over-segmented by clustering algorithms.
Firstly, we propose a non-learning Sparse Instance Proposal (SIP) module with
the ``sampling-shifting-grouping"" scheme to directly group thing points into
instances from the raw point cloud efficiently. More specifically, balanced
point sampling is introduced to generate sparse seed points with more uniform
point distribution over the distance range. And a shift module, termed bubble
shifting, is proposed to shrink the seed points to the clustered centers. Then
we utilize the connected component label algorithm to generate instance
proposals. Furthermore, an instance aggregation module is devised to integrate
potentially fragmented instances, improving the performance of the SIP module
on large objects. Extensive experiments show that PANet achieves
state-of-the-art performance among published works on the SemanticKITII
validation and nuScenes validation for the panoptic segmentation task.",None,-1
d549911e-12ed-4aca-a7f8-1418fcb51a4c,Learning-based Relational Object Matching Across Views,0.0709063,"Intelligent robots require object-level scene understanding to reason about
possible tasks and interactions with the environment. Moreover, many perception
tasks such as scene reconstruction, image retrieval, or place recognition can
benefit from reasoning on the level of objects. While keypoint-based matching
can yield strong results for finding correspondences for images with small to
medium view point changes, for large view point changes, matching semantically
on the object-level becomes advantageous. In this paper, we propose a
learning-based approach which combines local keypoints with novel object-level
features for matching object detections between RGB images. We train our
object-level matching features based on appearance and inter-frame and
cross-frame spatial relations between objects in an associative graph neural
network. We demonstrate our approach in a large variety of views on
realistically rendered synthetic images. Our approach compares favorably to
previous state-of-the-art object-level matching approaches and achieves
improved performance over a pure keypoint-based approach for large view-point
changes.",None,-1
0e584222-5620-4abe-ba8d-32983c6dacce,Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis,0.025219,"Recent advances in prompt engineering enable large language models (LLMs) to
solve multi-hop logical reasoning problems with impressive accuracy. However,
there is little existing work investigating the robustness of LLMs with
few-shot prompting techniques. Therefore, we introduce a systematic approach to
test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic
perturbations. We include perturbations at multiple levels of abstractions
(e.g. lexical perturbations such as typos, and semantic perturbations such as
the inclusion of intermediate reasoning steps in the questions) to conduct
behavioral analysis on the LLMs. Throughout our experiments, we find that
models are more sensitive to certain perturbations such as replacing words with
their synonyms. We also demonstrate that increasing the proportion of perturbed
exemplars in the prompts improves the robustness of few-shot prompting methods.",None,-1
19465124-1e23-4b6c-95c4-df2672ad8d6e,Understanding Expressivity of GNN in Rule Learning,0.318264,"Rule learning is critical to improving knowledge graph (KG) reasoning due to
their ability to provide logical and interpretable explanations. Recently,
Graph Neural Networks (GNNs) with tail entity scoring achieve the
state-of-the-art performance on KG reasoning. However, the theoretical
understandings for these GNNs are either lacking or focusing on
single-relational graphs, leaving what the kind of rules these GNNs can learn
an open problem. We propose to fill the above gap in this paper. Specifically,
GNNs with tail entity scoring are unified into a common framework. Then, we
analyze their expressivity by formally describing the rule structures they can
learn and theoretically demonstrating their superiority. These results further
inspire us to propose a novel labeling strategy to learn more rules in KG
reasoning. Experimental results are consistent with our theoretical findings
and verify the effectiveness of our proposed method. The code is publicly
available at https://github.com/LARS-research/Rule-learning-expressivity.",None,-1
17b9e2e3-b5a0-42de-ad2f-a5f587fa2fbd,MegaWika: Millions of reports and their sources across 50 diverse languages,0.277874,"To foster the development of new models for collaborative AI-assisted report
generation, we introduce MegaWika, consisting of 13 million Wikipedia articles
in 50 diverse languages, along with their 71 million referenced source
materials. We process this dataset for a myriad of applications, going beyond
the initial Wikipedia citation extraction and web scraping of content,
including translating non-English articles for cross-lingual applications and
providing FrameNet parses for automated semantic analysis. MegaWika is the
largest resource for sentence-level report generation and the only report
generation dataset that is multilingual. We manually analyze the quality of
this resource through a semantically stratified sample. Finally, we provide
baseline results and trained models for crucial steps in automated report
generation: cross-lingual question answering and citation retrieval.",None,-1
5c35a529-4e80-43e1-a353-8c50c32490ff,From Database Repairs to Causality in Databases and Beyond,0.705425,"We describe some recent approaches to score-based explanations for query
answers in databases. The focus is on work done by the author and
collaborators. Special emphasis is placed on the use of counterfactual
reasoning for score specification and computation. Several examples that
illustrate the flexibility of these methods are shown.",None,-1
9577dde8-d889-4920-aef2-72af8b2addf0,Sampling to Distill: Knowledge Transfer from Open-World Data,0.601225,"Data-Free Knowledge Distillation (DFKD) is a novel task that aims to train
high-performance student models using only the teacher network without original
training data. Despite encouraging results, existing DFKD methods rely heavily
on generation modules with high computational costs. Meanwhile, they ignore the
fact that the generated and original data exist domain shifts due to the lack
of supervision information. Moreover, knowledge is transferred through each
example, ignoring the implicit relationship among multiple examples. To this
end, we propose a novel Open-world Data Sampling Distillation (ODSD) method
without a redundant generation process. First, we try to sample open-world data
close to the original data's distribution by an adaptive sampling module. Then,
we introduce a low-noise representation to alleviate the domain shifts and
build a structured relationship of multiple data examples to exploit data
knowledge. Extensive experiments on CIFAR-10, CIFAR-100, NYUv2, and ImageNet
show that our ODSD method achieves state-of-the-art performance. Especially, we
improve 1.50\%-9.59\% accuracy on the ImageNet dataset compared with the
existing results.",None,-1
9b4ed27f-2023-4a8f-b11f-d702c026e8c4,An Entity-based Claim Extraction Pipeline for Real-world Biomedical Fact-checking,0.171613,"Existing fact-checking models for biomedical claims are typically trained on
synthetic or well-worded data and hardly transfer to social media content. This
mismatch can be mitigated by adapting the social media input to mimic the
focused nature of common training claims. To do so, Wuehrl & Klinger (2022)
propose to extract concise claims based on medical entities in the text.
However, their study has two limitations: First, it relies on gold-annotated
entities. Therefore, its feasibility for a real-world application cannot be
assessed since this requires detecting relevant entities automatically. Second,
they represent claim entities with the original tokens. This constitutes a
terminology mismatch which potentially limits the fact-checking performance. To
understand both challenges, we propose a claim extraction pipeline for medical
tweets that incorporates named entity recognition and terminology normalization
via entity linking. We show that automatic NER does lead to a performance drop
in comparison to using gold annotations but the fact-checking performance still
improves considerably over inputting the unchanged tweets. Normalizing entities
to their canonical forms does, however, not improve the performance.",None,-1
1b8d86ed-b028-4ce7-ac3c-c96833fef992,Identifying TBI Physiological States by Clustering Multivariate Clinical Time-Series Data,0.207111,"Determining clinically relevant physiological states from multivariate time
series data with missing values is essential for providing appropriate
treatment for acute conditions such as Traumatic Brain Injury (TBI),
respiratory failure, and heart failure. Utilizing non-temporal clustering or
data imputation and aggregation techniques may lead to loss of valuable
information and biased analyses. In our study, we apply the SLAC-Time
algorithm, an innovative self-supervision-based approach that maintains data
integrity by avoiding imputation or aggregation, offering a more useful
representation of acute patient states. By using SLAC-Time to cluster data in a
large research dataset, we identified three distinct TBI physiological states
and their specific feature profiles. We employed various clustering evaluation
metrics and incorporated input from a clinical domain expert to validate and
interpret the identified physiological states. Further, we discovered how
specific clinical events and interventions can influence patient states and
state transitions.",None,-1
6ab9c6d1-0338-4c35-86b6-1b6d536bc193,GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding,0.697726,"Humans subconsciously engage in geospatial reasoning when reading articles.
We recognize place names and their spatial relations in text and mentally
associate them with their physical locations on Earth. Although pretrained
language models can mimic this cognitive process using linguistic context, they
do not utilize valuable geospatial information in large, widely available
geographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a
geospatially grounded language model that enhances the understanding of
geo-entities in natural language. GeoLM leverages geo-entity mentions as
anchors to connect linguistic information in text corpora with geospatial
information extracted from geographical databases. GeoLM connects the two types
of context through contrastive learning and masked language modeling. It also
incorporates a spatial coordinate embedding mechanism to encode distance and
direction relations to capture geospatial context. In the experiment, we
demonstrate that GeoLM exhibits promising capabilities in supporting toponym
recognition, toponym linking, relation extraction, and geo-entity typing, which
bridge the gap between natural language processing and geospatial sciences. The
code is publicly available at https://github.com/knowledge-computing/geolm.",None,-1
f9b3fc5d-7444-47c2-b213-95641ffc8429,Level Generation Through Large Language Models,0.779704,"Large Language Models (LLMs) are powerful tools, capable of leveraging their
training on natural language to write stories, generate code, and answer
questions. But can they generate functional video game levels? Game levels,
with their complex functional constraints and spatial relationships in more
than one dimension, are very different from the kinds of data an LLM typically
sees during training. Datasets of game levels are also hard to come by,
potentially taxing the abilities of these data-hungry models. We investigate
the use of LLMs to generate levels for the game Sokoban, finding that LLMs are
indeed capable of doing so, and that their performance scales dramatically with
dataset size. We also perform preliminary experiments on controlling LLM level
generators and discuss promising areas for future work.",None,-1
f93657f0-ae39-4f56-b471-50b92fa79b65,Semantic Information Marketing in The Metaverse: A Learning-Based Contract Theory Framework,0.0670925,"In this paper, we address the problem of designing incentive mechanisms by a
virtual service provider (VSP) to hire sensing IoT devices to sell their
sensing data to help creating and rendering the digital copy of the physical
world in the Metaverse. Due to the limited bandwidth, we propose to use
semantic extraction algorithms to reduce the delivered data by the sensing IoT
devices. Nevertheless, mechanisms to hire sensing IoT devices to share their
data with the VSP and then deliver the constructed digital twin to the
Metaverse users are vulnerable to adverse selection problem. The adverse
selection problem, which is caused by information asymmetry between the system
entities, becomes harder to solve when the private information of the different
entities are multi-dimensional. We propose a novel iterative contract design
and use a new variant of multi-agent reinforcement learning (MARL) to solve the
modelled multi-dimensional contract problem. To demonstrate the effectiveness
of our algorithm, we conduct extensive simulations and measure several key
performance metrics of the contract for the Metaverse. Our results show that
our designed iterative contract is able to incentivize the participants to
interact truthfully, which maximizes the profit of the VSP with minimal
individual rationality (IR) and incentive compatibility (IC) violation rates.
Furthermore, the proposed learning-based iterative contract framework has
limited access to the private information of the participants, which is to the
best of our knowledge, the first of its kind in addressing the problem of
adverse selection in incentive mechanisms.",None,-1
dafb59b8-e8a4-42c0-bce2-4e8094c954f6,Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation,0.563514,"Previous studies have shown that leveraging domain index can significantly
boost domain adaptation performance (arXiv:2007.01807, arXiv:2202.03628).
However, such domain indices are not always available. To address this
challenge, we first provide a formal definition of domain index from the
probabilistic perspective, and then propose an adversarial variational Bayesian
framework that infers domain indices from multi-domain data, thereby providing
additional insight on domain relations and improving domain adaptation
performance. Our theoretical analysis shows that our adversarial variational
Bayesian framework finds the optimal domain index at equilibrium. Empirical
results on both synthetic and real data verify that our model can produce
interpretable domain indices which enable us to achieve superior performance
compared to state-of-the-art domain adaptation methods. Code is available at
https://github.com/Wang-ML-Lab/VDI.",None,-1
3382d2e5-fad7-43da-965a-61898bec5ec3,Human Feedback is not Gold Standard,0.573444,"Human feedback has become the de facto standard for evaluating the
performance of Large Language Models, and is increasingly being used as a
training objective. However, it is not clear which properties of a generated
output this single `preference' score captures. We hypothesise that preference
scores are subjective and open to undesirable biases. We critically analyse the
use of human feedback for both training and evaluation, to verify whether it
fully captures a range of crucial error criteria. We find that while preference
scores have fairly good coverage, they under-represent important aspects like
factuality. We further hypothesise that both preference scores and error
annotation may be affected by confounders, and leverage instruction-tuned
models to generate outputs that vary along two possible confounding dimensions:
assertiveness and complexity. We find that the assertiveness of an output skews
the perceived rate of factuality errors, indicating that human annotations are
not a fully reliable evaluation metric or training objective. Finally, we offer
preliminary evidence that using human feedback as a training objective
disproportionately increases the assertiveness of model outputs. We encourage
future work to carefully consider whether preference scores are well aligned
with the desired objective.",None,-1
dadb7ed3-f800-4598-b2db-05a16e7f68e9,Sampling-based Uncertainty Estimation for an Instance Segmentation Network,0.121466,"The examination of uncertainty in the predictions of machine learning (ML)
models is receiving increasing attention. One uncertainty modeling technique
used for this purpose is Monte-Carlo (MC)-Dropout, where repeated predictions
are generated for a single input. Therefore, clustering is required to describe
the resulting uncertainty, but only through efficient clustering is it possible
to describe the uncertainty from the model attached to each object. This
article uses Bayesian Gaussian Mixture (BGM) to solve this problem. In
addition, we investigate different values for the dropout rate and other
techniques, such as focal loss and calibration, which we integrate into the
Mask-RCNN model to obtain the most accurate uncertainty approximation of each
instance and showcase it graphically.",None,-1
6732d1a5-263c-4204-a14e-5e8e154c27e3,Kattis vs. ChatGPT: Assessment and Evaluation of Programming Tasks in the Age of Artificial Intelligence,0.650209,"AI-powered education technologies can support students and teachers in
computer science education. However, with the recent developments in generative
AI, and especially the increasingly emerging popularity of ChatGPT, the
effectiveness of using large language models for solving programming tasks has
been underexplored. The present study examines ChatGPT's ability to generate
code solutions at different difficulty levels for introductory programming
courses. We conducted an experiment where ChatGPT was tested on 127 randomly
selected programming problems provided by Kattis, an automatic software grading
tool for computer science programs, often used in higher education. The results
showed that ChatGPT independently could solve 19 out of 127 programming tasks
generated and assessed by Kattis. Further, ChatGPT was found to be able to
generate accurate code solutions for simple problems but encountered
difficulties with more complex programming tasks. The results contribute to the
ongoing debate on the utility of AI-powered tools in programming education.",None,-1
3ac6892d-1084-41e7-86c8-90c1a9ce64b2,Optimization of the location and design of urban green spaces,0.247241,"The recent promotion of sustainable urban planning combined with a growing
need for public interventions to improve well-being and health have led to an
increased collective interest for green spaces in and around cities. In
particular, parks have proven a wide range of benefits in urban areas. This
also means inequities in park accessibility may contribute to health
inequities. In this work, we showcase the application of classic tools from
Operations Research to assist decision-makers to improve parks' accessibility,
distribution and design. Given the context of public decision-making, we are
particularly concerned with equity and environmental justice, and are focused
on an advanced assessment of users' behavior through a spatial interaction
model. We present a two-stage fair facility location and design model, which
serves as a template model to assist public decision-makers at the city-level
for the planning of urban green spaces. The first-stage of the optimization
model is about the optimal city-budget allocation to neighborhoods based on a
data exposing inequality attributes. The second-stage seeks the optimal
location and design of parks for each neighborhood, and the objective consists
of maximizing the total expected probability of individuals visiting parks. We
show how to reformulate the latter as a mixed-integer linear program. We
further introduce a clustering method to reduce the size of the problem and
determine a close to optimal solution within reasonable time. The model is
tested using the case study of the city of Montreal and comparative results are
discussed in detail to justify the performance of the model.",None,-1
20cb24f6-c47e-4c72-b3ae-14f3d04911cb,Unified View of Damage leaves Planimetry & Analysis Using Digital Images Processing Techniques,0.690356,"The detection of leaf diseases in plants generally involves visual
observation of patterns appearing on the leaf surface. However, there are many
diseases that are distinguished based on very subtle changes in these visually
observable patterns. This paper attempts to identify plant leaf diseases using
image processing techniques. The focus of this study is on the detection of
citrus leaf canker disease. Canker is a bacterial infection of leaves. Symptoms
of citrus cankers include brown spots on the leaves, often with a watery or
oily appearance. The spots (called lesions in botany) are usually yellow. It is
surrounded by a halo of the leaves and is found on both the top and bottom of
the leaf. This paper describes various methods that have been used to detect
citrus leaf canker disease. The methods used are histogram comparison and
k-means clustering. Using these methods, citrus canker development was detected
based on histograms generated based on leaf patterns. The results thus obtained
can be used, after consultation with experts in the field of agriculture, to
identify suitable treatments for the processes used.",None,-1
3fe70238-365a-436b-892e-5908b256b89d,Generative Steganographic Flow,0.0777724,"Generative steganography (GS) is a new data hiding manner, featuring direct
generation of stego media from secret data. Existing GS methods are generally
criticized for their poor performances. In this paper, we propose a novel flow
based GS approach -- Generative Steganographic Flow (GSF), which provides
direct generation of stego images without cover image. We take the stego image
generation and secret data recovery process as an invertible transformation,
and build a reversible bijective mapping between input secret data and
generated stego images. In the forward mapping, secret data is hidden in the
input latent of Glow model to generate stego images. By reversing the mapping,
hidden data can be extracted exactly from generated stego images. Furthermore,
we propose a novel latent optimization strategy to improve the fidelity of
stego images. Experimental results show our proposed GSF has far better
performances than SOTA works.",None,-1
07f3d7de-7a35-4a93-9126-badd3db21cdb,Visual DNA: Representing and Comparing Images using Distributions of Neuron Activations,0.21916,"Selecting appropriate datasets is critical in modern computer vision.
However, no general-purpose tools exist to evaluate the extent to which two
datasets differ. For this, we propose representing images - and by extension
datasets - using Distributions of Neuron Activations (DNAs). DNAs fit
distributions, such as histograms or Gaussians, to activations of neurons in a
pre-trained feature extractor through which we pass the image(s) to represent.
This extractor is frozen for all datasets, and we rely on its generally
expressive power in feature space. By comparing two DNAs, we can evaluate the
extent to which two datasets differ with granular control over the comparison
attributes of interest, providing the ability to customise the way distances
are measured to suit the requirements of the task at hand. Furthermore, DNAs
are compact, representing datasets of any size with less than 15 megabytes. We
demonstrate the value of DNAs by evaluating their applicability on several
tasks, including conditional dataset comparison, synthetic image evaluation,
and transfer learning, and across diverse datasets, ranging from synthetic cat
images to celebrity faces and urban driving scenes.",None,-1
debfd6b9-86be-4b14-a369-87b23652d445,vMAP: Vectorised Object Mapping for Neural Field SLAM,0.868703,"We present vMAP, an object-level dense SLAM system using neural field
representations. Each object is represented by a small MLP, enabling efficient,
watertight object modelling without the need for 3D priors. As an RGB-D camera
browses a scene with no prior information, vMAP detects object instances
on-the-fly, and dynamically adds them to its map. Specifically, thanks to the
power of vectorised training, vMAP can optimise as many as 50 individual
objects in a single scene, with an extremely efficient training speed of 5Hz
map update. We experimentally demonstrate significantly improved scene-level
and object-level reconstruction quality compared to prior neural field SLAM
systems. Project page: https://kxhit.github.io/vMAP.",None,-1
2391bc66-374b-461a-a20d-c3e4aa705297,Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In,0.722883,"Retrieval augmentation can aid language models (LMs) in knowledge-intensive
tasks by supplying them with external information. Prior works on retrieval
augmentation usually jointly fine-tune the retriever and the LM, making them
closely coupled. In this paper, we explore the scheme of generic retrieval
plug-in: the retriever is to assist target LMs that may not be known beforehand
or are unable to be fine-tuned together. To retrieve useful documents for
unseen target LMs, we propose augmentation-adapted retriever (AAR), which
learns LM's preferences obtained from a known source LM. Experiments on the
MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM
is able to significantly improve the zero-shot generalization of larger target
LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates
that the preferences of different LMs overlap, enabling AAR trained with a
single source LM to serve as a generic plug-in for various target LMs. Our code
is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.",None,-1
0e5738d9-5b02-44b4-a37e-ebc925d3fcd7,Modern Bayesian Experimental Design,0.963316,"Bayesian experimental design (BED) provides a powerful and general framework
for optimizing the design of experiments. However, its deployment often poses
substantial computational challenges that can undermine its practical use. In
this review, we outline how recent advances have transformed our ability to
overcome these challenges and thus utilize BED effectively, before discussing
some key areas for future development in the field.",None,-1
28d36ef1-107e-499f-a2d3-3d5f7969a65e,Noisy Pair Corrector for Dense Retrieval,0.285776,"Most dense retrieval models contain an implicit assumption: the training
query-document pairs are exactly matched. Since it is expensive to annotate the
corpus manually, training pairs in real-world applications are usually
collected automatically, which inevitably introduces mismatched-pair noise. In
this paper, we explore an interesting and challenging problem in dense
retrieval, how to train an effective model with mismatched-pair noise. To solve
this problem, we propose a novel approach called Noisy Pair Corrector (NPC),
which consists of a detection module and a correction module. The detection
module estimates noise pairs by calculating the perplexity between annotated
positive and easy negative documents. The correction module utilizes an
exponential moving average (EMA) model to provide a soft supervised signal,
aiding in mitigating the effects of noise. We conduct experiments on
text-retrieval benchmarks Natural Question and TriviaQA, code-search benchmarks
StaQC and SO-DS. Experimental results show that NPC achieves excellent
performance in handling both synthetic and realistic noise.",None,-1
7eb4b0e1-b741-4447-8b52-62faef3d5e09,Rethinking Efficient Tuning Methods from a Unified Perspective,0.207803,"Parameter-efficient transfer learning (PETL) based on large-scale pre-trained
foundation models has achieved great success in various downstream
applications. Existing tuning methods, such as prompt, prefix, and adapter,
perform task-specific lightweight adjustments to different parts of the
original architecture. However, they take effect on only some parts of the
pre-trained models, i.e., only the feed-forward layers or the self-attention
layers, which leaves the remaining frozen structures unable to adapt to the
data distributions of downstream tasks. Further, the existing structures are
strongly coupled with the Transformers, hindering parameter-efficient
deployment as well as the design flexibility for new approaches. In this paper,
we revisit the design paradigm of PETL and derive a unified framework U-Tuning
for parameter-efficient transfer learning, which is composed of an operation
with frozen parameters and a unified tuner that adapts the operation for
downstream applications. The U-Tuning framework can simultaneously encompass
existing methods and derive new approaches for parameter-efficient transfer
learning, which prove to achieve on-par or better performances on CIFAR-100 and
FGVC datasets when compared with existing PETL methods.",None,-1
31e1b877-3aa9-4f3c-be89-bf4c3354c3b3,CLIPER: A Unified Vision-Language Framework for In-the-Wild Facial Expression Recognition,0.874825,"Facial expression recognition (FER) is an essential task for understanding
human behaviors. As one of the most informative behaviors of humans, facial
expressions are often compound and variable, which is manifested by the fact
that different people may express the same expression in very different ways.
However, most FER methods still use one-hot or soft labels as the supervision,
which lack sufficient semantic descriptions of facial expressions and are less
interpretable. Recently, contrastive vision-language pre-training (VLP) models
(e.g., CLIP) use text as supervision and have injected new vitality into
various computer vision tasks, benefiting from the rich semantics in text.
Therefore, in this work, we propose CLIPER, a unified framework for both static
and dynamic facial Expression Recognition based on CLIP. Besides, we introduce
multiple expression text descriptors (METD) to learn fine-grained expression
representations that make CLIPER more interpretable. We conduct extensive
experiments on several popular FER benchmarks and achieve state-of-the-art
performance, which demonstrates the effectiveness of CLIPER.",None,-1
96ab2208-18fa-4e58-9134-97a5884b4c85,"(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions",0.671933,"The concept of rationality is central to the field of artificial
intelligence. Whether we are seeking to simulate human reasoning, or the goal
is to achieve bounded optimality, we generally seek to make artificial agents
as rational as possible. Despite the centrality of the concept within AI, there
is no unified definition of what constitutes a rational agent. This article
provides a survey of rationality and irrationality in artificial intelligence,
and sets out the open questions in this area. The understanding of rationality
in other fields has influenced its conception within artificial intelligence,
in particular work in economics, philosophy and psychology. Focusing on the
behaviour of artificial agents, we consider irrational behaviours that can
prove to be optimal in certain scenarios. Some methods have been developed to
deal with irrational agents, both in terms of identification and interaction,
however work in this area remains limited. Methods that have up to now been
developed for other purposes, namely adversarial scenarios, may be adapted to
suit interactions with artificial agents. We further discuss the interplay
between human and artificial agents, and the role that rationality plays within
this interaction; many questions remain in this area, relating to potentially
irrational behaviour of both humans and artificial agents.",None,-1
7348bd54-95b3-46b8-8267-3cb8130adafd,Neural Compositional Rule Learning for Knowledge Graph Reasoning,0.498224,"Learning logical rules is critical to improving reasoning in KGs. This is due
to their ability to provide logical and interpretable explanations when used
for predictions, as well as their ability to generalize to other tasks,
domains, and data. While recent methods have been proposed to learn logical
rules, the majority of these methods are either restricted by their
computational complexity and can not handle the large search space of
large-scale KGs, or show poor generalization when exposed to data outside the
training set. In this paper, we propose an end-to-end neural model for learning
compositional logical rules called NCRL. NCRL detects the best compositional
structure of a rule body, and breaks it into small compositions in order to
infer the rule head. By recurrently merging compositions in the rule body with
a recurrent attention unit, NCRL finally predicts a single rule head.
Experimental results show that NCRL learns high-quality rules, as well as being
generalizable. Specifically, we show that NCRL is scalable, efficient, and
yields state-of-the-art results for knowledge graph completion on large-scale
KGs. Moreover, we test NCRL for systematic generalization by learning to reason
on small-scale observed graphs and evaluating on larger unseen ones.",None,-1
e788283b-285c-4112-aca6-d0095f90dd6c,FrameBERT: Conceptual Metaphor Detection with Frame Embedding Learning,0.891416,"In this paper, we propose FrameBERT, a RoBERTa-based model that can
explicitly learn and incorporate FrameNet Embeddings for concept-level metaphor
detection. FrameBERT not only achieves better or comparable performance to the
state-of-the-art, but also is more explainable and interpretable compared to
existing models, attributing to its ability of accounting for external
knowledge of FrameNet.",None,-1
c0ceff5e-439c-4ee8-8a38-47c96f411937,Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model,0.051095,"Model inversion attacks (MIAs) aim to recover private data from inaccessible
training sets of deep learning models, posing a privacy threat. MIAs primarily
focus on the white-box scenario where attackers have full access to the model's
structure and parameters. However, practical applications are usually in
black-box scenarios or label-only scenarios, i.e., the attackers can only
obtain the output confidence vectors or labels by accessing the model.
Therefore, the attack models in existing MIAs are difficult to effectively
train with the knowledge of the target model, resulting in sub-optimal attacks.
To the best of our knowledge, we pioneer the research of a powerful and
practical attack model in the label-only scenario.
  In this paper, we develop a novel MIA method, leveraging a conditional
diffusion model (CDM) to recover representative samples under the target label
from the training set. Two techniques are introduced: selecting an auxiliary
dataset relevant to the target model task and using predicted labels as
conditions to guide training CDM; and inputting target label, pre-defined
guidance strength, and random noise into the trained attack model to generate
and correct multiple results for final selection. This method is evaluated
using Learned Perceptual Image Patch Similarity as a new metric and as a
judgment basis for deciding the values of hyper-parameters. Experimental
results show that this method can generate similar and accurate samples to the
target label, outperforming generators of previous approaches.",None,-1
ac4829f8-a7cf-4f5f-83e2-cd5c5177c72b,A Simple and Plug-and-play Method for Unsupervised Sentence Representation Enhancement,0.148272,"Generating proper embedding of sentences through an unsupervised way is
beneficial to semantic matching and retrieval problems in real-world scenarios.
This paper presents Representation ALchemy (RepAL), an extremely simple
post-processing method that enhances sentence representations. The basic idea
in RepAL is to de-emphasize redundant information of sentence embedding
generated by pre-trained models. Through comprehensive experiments, we show
that RepAL is free of training and is a plug-and-play method that can be
combined with most existing unsupervised sentence learning models. We also
conducted in-depth analysis to understand RepAL.",None,-1
2ed88d2f-4df6-4462-86ef-bd9fe6849d3b,Manipulating Transfer Learning for Property Inference,0.0984513,"Transfer learning is a popular method for tuning pretrained (upstream) models
for different downstream tasks using limited data and computational resources.
We study how an adversary with control over an upstream model used in transfer
learning can conduct property inference attacks on a victim's tuned downstream
model. For example, to infer the presence of images of a specific individual in
the downstream training set. We demonstrate attacks in which an adversary can
manipulate the upstream model to conduct highly effective and specific property
inference attacks (AUC score $> 0.9$), without incurring significant
performance loss on the main task. The main idea of the manipulation is to make
the upstream model generate activations (intermediate features) with different
distributions for samples with and without a target property, thus enabling the
adversary to distinguish easily between downstream models trained with and
without training examples that have the target property. Our code is available
at https://github.com/yulongt23/Transfer-Inference.",None,-1
d3e65d23-02fe-41ad-9e02-694d5992dce1,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,0.464182,"With an excellent balance between speed and accuracy, cutting-edge YOLO
frameworks have become one of the most efficient algorithms for object
detection. However, the performance of using YOLO networks is scarcely
investigated in brain tumor detection. We propose a novel YOLO architecture
with Reparameterized Convolution based on channel Shuffle (RCS-YOLO). We
present RCS and a One-Shot Aggregation of RCS (RCS-OSA), which link feature
cascade and computation efficiency to extract richer information and reduce
time consumption. Experimental results on the brain tumor dataset Br35H show
that the proposed model surpasses YOLOv6, YOLOv7, and YOLOv8 in speed and
accuracy. Notably, compared with YOLOv7, the precision of RCS-YOLO improves by
1%, and the inference speed by 60% at 114.8 images detected per second (FPS).
Our proposed RCS-YOLO achieves state-of-the-art performance on the brain tumor
detection task. The code is available at https://github.com/mkang315/RCS-YOLO.",None,-1
67b4452a-d03d-42db-96b2-1a0b0e1cd93d,Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West,0.35998,"Large Language Models (LLMs), now used daily by millions of users, can encode
societal biases, exposing their users to representational harms. A large body
of scholarship on LLM bias exists but it predominantly adopts a Western-centric
frame and attends comparatively less to bias levels and potential harms in the
Global South. In this paper, we quantify stereotypical bias in popular LLMs
according to an Indian-centric frame and compare bias levels between the Indian
and Western contexts. To do this, we develop a novel dataset which we call
Indian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and
anti-stereotypical examples for caste and religion contexts. We find that the
majority of LLMs tested are strongly biased towards stereotypes in the Indian
context, especially as compared to the Western context. We finally investigate
Instruction Prompting as a simple intervention to mitigate such bias and find
that it significantly reduces both stereotypical and anti-stereotypical biases
in the majority of cases for GPT-3.5. The findings of this work highlight the
need for including more diverse voices when evaluating LLMs.",None,-1
0af58f09-837e-4d8b-822e-fe91c95d0895,Geometry-biased Transformers for Novel View Synthesis,0.151908,"We tackle the task of synthesizing novel views of an object given a few input
images and associated camera viewpoints. Our work is inspired by recent
'geometry-free' approaches where multi-view images are encoded as a (global)
set-latent representation, which is then used to predict the color for
arbitrary query rays. While this representation yields (coarsely) accurate
images corresponding to novel viewpoints, the lack of geometric reasoning
limits the quality of these outputs. To overcome this limitation, we propose
'Geometry-biased Transformers' (GBTs) that incorporate geometric inductive
biases in the set-latent representation-based inference to encourage multi-view
geometric consistency. We induce the geometric bias by augmenting the
dot-product attention mechanism to also incorporate 3D distances between rays
associated with tokens as a learnable bias. We find that this, along with
camera-aware embeddings as input, allows our models to generate significantly
more accurate outputs. We validate our approach on the real-world CO3D dataset,
where we train our system over 10 categories and evaluate its view-synthesis
ability for novel objects as well as unseen categories. We empirically validate
the benefits of the proposed geometric biases and show that our approach
significantly improves over prior works.",None,-1
3f5a7c57-bacf-416f-9865-d85de3a532b3,SSC-RS: Elevate LiDAR Semantic Scene Completion with Representation Separation and BEV Fusion,0.38652,"Semantic scene completion (SSC) jointly predicts the semantics and geometry
of the entire 3D scene, which plays an essential role in 3D scene understanding
for autonomous driving systems. SSC has achieved rapid progress with the help
of semantic context in segmentation. However, how to effectively exploit the
relationships between the semantic context in semantic segmentation and
geometric structure in scene completion remains under exploration. In this
paper, we propose to solve outdoor SSC from the perspective of representation
separation and BEV fusion. Specifically, we present the network, named SSC-RS,
which uses separate branches with deep supervision to explicitly disentangle
the learning procedure of the semantic and geometric representations. And a BEV
fusion network equipped with the proposed Adaptive Representation Fusion (ARF)
module is presented to aggregate the multi-scale features effectively and
efficiently. Due to the low computational burden and powerful representation
ability, our model has good generality while running in real-time. Extensive
experiments on SemanticKITTI demonstrate our SSC-RS achieves state-of-the-art
performance.",None,-1
f5bdbbc6-ea1e-4f23-b912-e446a9a248ff,Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models,0.149288,"The dynamic nature of knowledge in an ever-changing world presents challenges
for language models trained on static data; the model in the real world often
requires not only acquiring new knowledge but also overwriting outdated
information into updated ones. To study the ability of language models for
these time-dependent dynamics in human language, we introduce a novel task,
EvolvingQA, a temporally evolving question-answering benchmark designed for
training and evaluating LMs on an evolving Wikipedia database. The construction
of EvolvingQA is automated with our pipeline using large language models. We
uncover that existing continual learning baselines suffer from updating and
removing outdated knowledge. Our analysis suggests that models fail to rectify
knowledge due to small weight gradients. In addition, we elucidate that
language models particularly struggle to reflect the change of numerical or
temporal information. Our work aims to model the dynamic nature of real-world
information, suggesting faithful evaluations of the evolution-adaptability of
language models.",None,-1
0019f1e1-08d4-480d-9558-ab9737c0d2ba,GPT4GEO: How a Language Model Sees the World's Geography,0.967532,"Large language models (LLMs) have shown remarkable capabilities across a
broad range of tasks involving question answering and the generation of
coherent text and code. Comprehensively understanding the strengths and
weaknesses of LLMs is beneficial for safety, downstream applications and
improving performance. In this work, we investigate the degree to which GPT-4
has acquired factual geographic knowledge and is capable of using this
knowledge for interpretative reasoning, which is especially important for
applications that involve geographic data, such as geospatial analysis, supply
chain management, and disaster response. To this end, we design and conduct a
series of diverse experiments, starting from factual tasks such as location,
distance and elevation estimation to more complex questions such as generating
country outlines and travel networks, route finding under constraints and
supply chain analysis. We provide a broad characterisation of what GPT-4
(without plugins or Internet access) knows about the world, highlighting both
potentially surprising capabilities but also limitations.",None,-1
16e612c4-90ac-4dcc-905b-4a65cb29d921,MoniLog: An Automated Log-Based Anomaly Detection System for Cloud Computing Infrastructures,0.19313,"Within today's large-scale systems, one anomaly can impact millions of users.
Detecting such events in real-time is essential to maintain the quality of
services. It allows the monitoring team to prevent or diminish the impact of a
failure. Logs are a core part of software development and maintenance, by
recording detailed information at runtime. Such log data are universally
available in nearly all computer systems. They enable developers as well as
system maintainers to monitor and dissect anomalous events. For Cloud computing
companies and large online platforms in general, growth is linked to the
scaling potential. Automatizing the anomaly detection process is a promising
way to ensure the scalability of monitoring capacities regarding the increasing
volume of logs generated by modern systems. In this paper, we will introduce
MoniLog, a distributed approach to detect real-time anomalies within
large-scale environments. It aims to detect sequential and quantitative
anomalies within a multi-source log stream. MoniLog is designed to structure a
log stream and perform the monitoring of anomalous sequences. Its output
classifier learns from the administrator's actions to label and evaluate the
criticality level of anomalies.",None,-1
b412dc67-cfff-4a02-9721-df8e94776798,A Two-Stage Decoder for Efficient ICD Coding,0.117717,"Clinical notes in healthcare facilities are tagged with the International
Classification of Diseases (ICD) code; a list of classification codes for
medical diagnoses and procedures. ICD coding is a challenging multilabel text
classification problem due to noisy clinical document inputs and long-tailed
label distribution. Recent automated ICD coding efforts improve performance by
encoding medical notes and codes with additional data and knowledge bases.
However, most of them do not reflect how human coders generate the code: first,
the coders select general code categories and then look for specific
subcategories that are relevant to a patient's condition. Inspired by this, we
propose a two-stage decoding mechanism to predict ICD codes. Our model uses the
hierarchical properties of the codes to split the prediction into two steps: At
first, we predict the parent code and then predict the child code based on the
previous prediction. Experiments on the public MIMIC-III data set show that our
model performs well in single-model settings without external data or
knowledge.",None,-1
82e46d9f-313e-4385-8186-05551a297e05,Solving Multi-Agent Target Assignment and Path Finding with a Single Constraint Tree,0.312727,"Combined Target-Assignment and Path-Finding problem (TAPF) requires
simultaneously assigning targets to agents and planning collision-free paths
for agents from their start locations to their assigned targets. As a leading
approach to address TAPF, Conflict-Based Search with Target Assignment (CBS-TA)
leverages both K-best target assignments to create multiple search trees and
Conflict-Based Search (CBS) to resolve collisions in each search tree. While
being able to find an optimal solution, CBS-TA suffers from scalability due to
the duplicated collision resolution in multiple trees and the expensive
computation of K-best assignments. We therefore develop Incremental Target
Assignment CBS (ITA-CBS) to bypass these two computational bottlenecks. ITA-CBS
generates only a single search tree and avoids computing K-best assignments by
incrementally computing new 1-best assignments during the search. We show that,
in theory, ITA-CBS is guaranteed to find an optimal solution and, in practice,
is computationally efficient.",None,-1
8c669073-9af8-4c8f-abd4-11c08fd65775,Deep Image Harmonization with Globally Guided Feature Transformation and Relation Distillation,0.416548,"Given a composite image, image harmonization aims to adjust the foreground
illumination to be consistent with background. Previous methods have explored
transforming foreground features to achieve competitive performance. In this
work, we show that using global information to guide foreground feature
transformation could achieve significant improvement. Besides, we propose to
transfer the foreground-background relation from real images to composite
images, which can provide intermediate supervision for the transformed encoder
features. Additionally, considering the drawbacks of existing harmonization
datasets, we also contribute a ccHarmony dataset which simulates the natural
illumination variation. Extensive experiments on iHarmony4 and our contributed
dataset demonstrate the superiority of our method. Our ccHarmony dataset is
released at https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.",None,-1
4d8e4747-9a22-49ca-891e-60609c4f6700,RweetMiner: Automatic identification and categorization of help requests on twitter during disasters,0.511244,"Catastrophic events create uncertain situations for humanitarian
organizations locating and providing aid to affected people. Many people turn
to social media during disasters for requesting help and/or providing relief to
others. However, the majority of social media posts seeking help could not
properly be detected and remained concealed because often they are noisy and
ill-formed. Existing systems lack in planning an effective strategy for tweet
preprocessing and grasping the contexts of tweets. This research, first of all,
formally defines request tweets in the context of social networking sites,
hereafter rweets, along with their different primary types and sub-types. Our
main contributions are the identification and categorization of rweets. For
rweet identification, we employ two approaches, namely a rule-based and
logistic regression, and show their high precision and F1 scores. The rweets
classification into sub-types such as medical, food, and shelter, using
logistic regression shows promising results and outperforms existing works.
Finally, we introduce an architecture to store intermediate data to accelerate
the development process of the machine learning classifiers.",None,-1
436249c3-5db7-49d2-938a-84108f67e823,Integrating Audio-Visual Features for Multimodal Deepfake Detection,0.904969,"Deepfakes are AI-generated media in which an image or video has been
digitally modified. The advancements made in deepfake technology have led to
privacy and security issues. Most deepfake detection techniques rely on the
detection of a single modality. Existing methods for audio-visual detection do
not always surpass that of the analysis based on single modalities. Therefore,
this paper proposes an audio-visual-based method for deepfake detection, which
integrates fine-grained deepfake identification with binary classification. We
categorize the samples into four types by combining labels specific to each
single modality. This method enhances the detection under intra-domain and
cross-domain testing.",None,-1
67840f39-ffb0-47f5-97fb-593af0aa2059,Segment Anything Meets Point Tracking,0.999369,"The Segment Anything Model (SAM) has established itself as a powerful
zero-shot image segmentation model, enabled by efficient point-centric
annotation and prompt-based models. While click and brush interactions are both
well explored in interactive image segmentation, the existing methods on videos
focus on mask annotation and propagation. This paper presents SAM-PT, a novel
method for point-centric interactive video segmentation, empowered by SAM and
long-term point tracking. SAM-PT leverages robust and sparse point selection
and propagation techniques for mask generation. Compared to traditional
object-centric mask propagation strategies, we uniquely use point propagation
to exploit local structure information agnostic to object semantics. We
highlight the merits of point-based tracking through direct evaluation on the
zero-shot open-world Unidentified Video Objects (UVO) benchmark. Our
experiments on popular video object segmentation and multi-object segmentation
tracking benchmarks, including DAVIS, YouTube-VOS, and BDD100K, suggest that a
point-based segmentation tracker yields better zero-shot performance and
efficient interactions. We release our code that integrates different point
trackers and video segmentation benchmarks at https://github.com/SysCV/sam-pt.",None,-1
0efddf23-eb89-4283-8a07-914744f5c9d2,Applying Large Language Models for Causal Structure Learning in Non Small Cell Lung Cancer,0.656434,"Causal discovery is becoming a key part in medical AI research. These methods
can enhance healthcare by identifying causal links between biomarkers,
demographics, treatments and outcomes. They can aid medical professionals in
choosing more impactful treatments and strategies. In parallel, Large Language
Models (LLMs) have shown great potential in identifying patterns and generating
insights from text data. In this paper we investigate applying LLMs to the
problem of determining the directionality of edges in causal discovery.
Specifically, we test our approach on a deidentified set of Non Small Cell Lung
Cancer(NSCLC) patients that have both electronic health record and genomic
panel data. Graphs are validated using Bayesian Dirichlet estimators using
tabular data. Our result shows that LLMs can accurately predict the
directionality of edges in causal graphs, outperforming existing
state-of-the-art methods. These findings suggests that LLMs can play a
significant role in advancing causal discovery and help us better understand
complex systems.",None,-1
c22319e5-d834-475f-90cf-1ca5b8539d2c,Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment identification across diverse communities,0.619073,"The prevalence and impact of toxic discussions online have made content
moderation crucial.Automated systems can play a vital role in identifying
toxicity, and reducing the reliance on human moderation.Nevertheless,
identifying toxic comments for diverse communities continues to present
challenges that are addressed in this paper.The two-part goal of this study is
to(1)identify intuitive variances from annotator disagreement using
quantitative analysis and (2)model the subjectivity of these viewpoints.To
achieve our goal, we published a new
dataset\footnote{\url{https://github.com/XXX}} with expert annotators'
annotations and used two other public datasets to identify the subjectivity of
toxicity.Then leveraging the Large Language Model(LLM),we evaluate the model's
ability to mimic diverse viewpoints on toxicity by varying size of the training
data and utilizing same set of annotators as the test set used during model
training and a separate set of annotators as the test set.We conclude that
subjectivity is evident across all annotator groups, demonstrating the
shortcomings of majority-rule voting. Moving forward, subjective annotations
should serve as ground truth labels for training models for domains like
toxicity in diverse communities.",None,-1
a757ac8d-8b3c-4d24-ab59-bce4a04b382f,Syntax-Guided Transformers: Elevating Compositional Generalization and Grounding in Multimodal Environments,0.0453641,"Compositional generalization, the ability of intelligent models to
extrapolate understanding of components to novel compositions, is a fundamental
yet challenging facet in AI research, especially within multimodal
environments. In this work, we address this challenge by exploiting the
syntactic structure of language to boost compositional generalization. This
paper elevates the importance of syntactic grounding, particularly through
attention masking techniques derived from text input parsing. We introduce and
evaluate the merits of using syntactic information in the multimodal grounding
problem. Our results on grounded compositional generalization underscore the
positive impact of dependency parsing across diverse tasks when utilized with
Weight Sharing across the Transformer encoder. The results push the
state-of-the-art in multimodal grounding and parameter-efficient modeling and
provide insights for future research.",None,-1
71c5c518-2661-495d-8d92-c5b17b765e4f,Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model,0.999999,"In this paper, we rethink the low-light image enhancement task and propose a
physically explainable and generative diffusion model for low-light image
enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the
physical model and the generative network. Furthermore, we hope to supplement
and even deduce the information missing in the low-light image through the
generative network. Therefore, Diff-Retinex formulates the low-light image
enhancement problem into Retinex decomposition and conditional image
generation. In the Retinex decomposition, we integrate the superiority of
attention in Transformer and meticulously design a Retinex Transformer
decomposition network (TDN) to decompose the image into illumination and
reflectance maps. Then, we design multi-path generative diffusion networks to
reconstruct the normal-light Retinex probability distribution and solve the
various degradations in these components respectively, including dark
illumination, noise, color deviation, loss of scene contents, etc. Owing to
generative diffusion model, Diff-Retinex puts the restoration of low-light
subtle detail into practice. Extensive experiments conducted on real-world
low-light datasets qualitatively and quantitatively demonstrate the
effectiveness, superiority, and generalization of the proposed method.",None,-1
b11d6bd5-c050-4758-9e85-8beb8abf4589,Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems,0.361499,"This paper aims to serve as a comprehensive guide for researchers and
practitioners, offering insights into the current state, potential
applications, and future research directions for generative artificial
intelligence and foundation models within the context of intelligent vehicles.
As the automotive industry progressively integrates AI, generative artificial
intelligence technologies hold the potential to revolutionize user
interactions, delivering more immersive, intuitive, and personalised in-car
experiences. We provide an overview of current applications of generative
artificial intelligence in the automotive domain, emphasizing speech, audio,
vision, and multimodal interactions. We subsequently outline critical future
research areas, including domain adaptability, alignment, multimodal
integration and others, as well as, address the challenges and risks associated
with ethics. By fostering collaboration and addressing these research areas,
generative artificial intelligence can unlock its full potential, transforming
the driving experience and shaping the future of intelligent vehicles.",None,-1
50e34144-6ae8-489e-9812-bf7da19ac60f,Strategies for improving low resource speech to text translation relying on pre-trained ASR models,0.395068,"This paper presents techniques and findings for improving the performance of
low-resource speech to text translation (ST). We conducted experiments on both
simulated and real-low resource setups, on language pairs English - Portuguese,
and Tamasheq - French respectively. Using the encoder-decoder framework for ST,
our results show that a multilingual automatic speech recognition system acts
as a good initialization under low-resource scenarios. Furthermore, using the
CTC as an additional objective for translation during training and decoding
helps to reorder the internal representations and improves the final
translation. Through our experiments, we try to identify various factors
(initializations, objectives, and hyper-parameters) that contribute the most
for improvements in low-resource setups. With only 300 hours of pre-training
data, our model achieved 7.3 BLEU score on Tamasheq - French data,
outperforming prior published works from IWSLT 2022 by 1.6 points.",None,-1
10eccfb5-8eac-4d06-a311-988f25b9bef3,Semi-Supervised Semantic Segmentation With Region Relevance,0.314851,"Semi-supervised semantic segmentation aims to learn from a small amount of
labeled data and plenty of unlabeled ones for the segmentation task. The most
common approach is to generate pseudo-labels for unlabeled images to augment
the training data. However, the noisy pseudo-labels will lead to cumulative
classification errors and aggravate the local inconsistency in prediction. This
paper proposes a Region Relevance Network (RRN) to alleviate the problem
mentioned above. Specifically, we first introduce a local pseudo-label
filtering module that leverages discriminator networks to assess the accuracy
of the pseudo-label at the region level. A local selection loss is proposed to
mitigate the negative impact of wrong pseudo-labels in consistency
regularization training. In addition, we propose a dynamic region-loss
correction module, which takes the merit of network diversity to further rate
the reliability of pseudo-labels and correct the convergence direction of the
segmentation network with a dynamic region loss. Extensive experiments are
conducted on PASCAL VOC 2012 and Cityscapes datasets with varying amounts of
labeled data, demonstrating that our proposed approach achieves
state-of-the-art performance compared to current counterparts.",None,-1
48f0e101-f087-4b17-a2be-f17d75689585,Learning Visual Representations via Language-Guided Sampling,0.550505,"Although an object may appear in numerous contexts, we often describe it in a
limited number of ways. Language allows us to abstract away visual variation to
represent and communicate concepts. Building on this intuition, we propose an
alternative approach to visual representation learning: using language
similarity to sample semantically similar image pairs for contrastive learning.
Our approach diverges from image-based contrastive learning by sampling view
pairs using language similarity instead of hand-crafted augmentations or
learned clusters. Our approach also differs from image-text contrastive
learning by relying on pre-trained language models to guide the learning rather
than directly minimizing a cross-modal loss. Through a series of experiments,
we show that language-guided learning yields better features than image-based
and image-text representation learning approaches.",None,-1
6194704e-87a0-4b1e-9730-719a5139df9e,EM Pre-training for Multi-party Dialogue Response Generation,0.851785,"Dialogue response generation requires an agent to generate a response
according to the current dialogue history, in terms of which two-party
dialogues have been well studied, but leaving a great gap for multi-party
dialogues at the same time. Different from two-party dialogues where each
response is a direct reply to its previous utterance, the addressee of a
response utterance should be specified before it is generated in the
multi-party scenario. Thanks to the huge amount of two-party conversational
data, various pre-trained language models for two-party dialogue response
generation have been proposed. However, due to the lack of annotated addressee
labels in multi-party dialogue datasets, it is hard to use them to pre-train a
response generation model for multi-party dialogues. To tackle this obstacle,
we propose an Expectation-Maximization (EM) approach that iteratively performs
the expectation steps to generate addressee labels, and the maximization steps
to optimize a response generation model. Theoretical analyses and extensive
experiments have justified the feasibility and effectiveness of our proposed
method.",None,-1
8117a29a-1620-49d8-a616-f30646e8cfb8,Inference with Reference: Lossless Acceleration of Large Language Models,0.449274,"We propose LLMA, an LLM accelerator to losslessly speed up Large Language
Model (LLM) inference with references. LLMA is motivated by the observation
that there are abundant identical text spans between the decoding result by an
LLM and the reference that is available in many real world scenarios (e.g.,
retrieved documents). LLMA first selects a text span from the reference and
copies its tokens to the decoder and then efficiently checks the tokens'
appropriateness as the decoding result in parallel within one decoding step.
The improved computational parallelism allows LLMA to achieve over 2x speed-up
for LLMs with identical generation results as greedy decoding in many practical
generation scenarios where significant overlap between in-context reference and
outputs exists (e.g., search engines and multi-turn conversations).",None,-1
4f39e01d-f6cc-4d8a-b743-417fad104b08,CLAIMED -- the open source framework for building coarse-grained operators for accelerated discovery in science,0.260814,"In modern data-driven science, reproducibility and reusability are key
challenges. Scientists are well skilled in the process from data to
publication. Although some publication channels require source code and data to
be made accessible, rerunning and verifying experiments is usually hard due to
a lack of standards. Therefore, reusing existing scientific data processing
code from state-of-the-art research is hard as well. This is why we introduce
CLAIMED, which has a proven track record in scientific research for addressing
the repeatability and reusability issues in modern data-driven science. CLAIMED
is a framework to build reusable operators and scalable scientific workflows by
supporting the scientist to draw from previous work by re-composing workflows
from existing libraries of coarse-grained scientific operators. Although
various implementations exist, CLAIMED is programming language, scientific
library, and execution environment agnostic.",None,-1
aa9ee45d-2bc6-4264-9a98-1ad428344dfe,Understanding Self-Supervised Features for Learning Unsupervised Instance Segmentation,0.420911,"Self-supervised learning (SSL) can be used to solve complex visual tasks
without human labels. Self-supervised representations encode useful semantic
information about images, and as a result, they have already been used for
tasks such as unsupervised semantic segmentation. In this paper, we investigate
self-supervised representations for instance segmentation without any manual
annotations. We find that the features of different SSL methods vary in their
level of instance-awareness. In particular, DINO features, which are known to
be excellent semantic descriptors, lack behind MAE features in their
sensitivity for separating instances.",None,-1
1d3a267e-45f9-4692-ab7f-6e1106002638,Robust Wind Turbine Blade Segmentation from RGB Images in the Wild,0.780248,"With the relentless growth of the wind industry, there is an imperious need
to design automatic data-driven solutions for wind turbine maintenance. As
structural health monitoring mainly relies on visual inspections, the first
stage in any automatic solution is to identify the blade region on the image.
Thus, we propose a novel segmentation algorithm that strengthens the U-Net
results by a tailored loss, which pools the focal loss with a contiguity
regularization term. To attain top performing results, a set of additional
steps are proposed to ensure a reliable, generic, robust and efficient
algorithm. First, we leverage our prior knowledge on the images by filling the
holes enclosed by temporarily-classified blade pixels and by the image
boundaries. Subsequently, the mislead classified pixels are successfully
amended by training an on-the-fly random forest. Our algorithm demonstrates its
effectiveness reaching a non-trivial 97.39% of accuracy.",None,-1
1bdf6b79-c528-4613-88e6-7802fc06a2d0,Neural Kernel Surface Reconstruction,0.994412,"We present a novel method for reconstructing a 3D implicit surface from a
large-scale, sparse, and noisy point cloud. Our approach builds upon the
recently introduced Neural Kernel Fields (NKF) representation. It enjoys
similar generalization capabilities to NKF, while simultaneously addressing its
main limitations: (a) We can scale to large scenes through compactly supported
kernel functions, which enable the use of memory-efficient sparse linear
solvers. (b) We are robust to noise, through a gradient fitting solve. (c) We
minimize training requirements, enabling us to learn from any dataset of dense
oriented points, and even mix training data consisting of objects and scenes at
different scales. Our method is capable of reconstructing millions of points in
a few seconds, and handling very large scenes in an out-of-core fashion. We
achieve state-of-the-art results on reconstruction benchmarks consisting of
single objects, indoor scenes, and outdoor scenes.",None,-1
2a24a0eb-444a-417a-82c1-e9194cb51698,An End-to-End Neural Network for Image-to-Audio Transformation,0.217749,"This paper describes an end-to-end (E2E) neural architecture for the audio
rendering of small portions of display content on low resource personal
computing devices. It is intended to address the problem of accessibility for
vision-impaired or vision-distracted users at the hardware level. Neural
image-to-text (ITT) and text-to-speech (TTS) approaches are reviewed and a new
technique is introduced to efficiently integrate them in a way that is both
efficient and back-propagate-able, leading to a non-autoregressive E2E
image-to-speech (ITS) neural network that is efficient and trainable.
Experimental results are presented showing that, compared with the non-E2E
approach, the proposed E2E system is 29% faster and uses 19% fewer parameters
with a 2% reduction in phone accuracy. A future direction to address accuracy
is presented.",None,-1
3015c09e-e631-4e28-9779-a59d998d0310,Neural LiDAR Fields for Novel View Synthesis,0.750486,"We present Neural Fields for LiDAR (NFL), a method to optimise a neural field
scene representation from LiDAR measurements, with the goal of synthesizing
realistic LiDAR scans from novel viewpoints. NFL combines the rendering power
of neural fields with a detailed, physically motivated model of the LiDAR
sensing process, thus enabling it to accurately reproduce key sensor behaviors
like beam divergence, secondary returns, and ray dropping. We evaluate NFL on
synthetic and real LiDAR scans and show that it outperforms explicit
reconstruct-then-simulate methods as well as other NeRF-style methods on LiDAR
novel view synthesis task. Moreover, we show that the improved realism of the
synthesized views narrows the domain gap to real scans and translates to better
registration and semantic segmentation performance.",None,-1
ccb13440-515d-4eac-bcd3-e39c8b4d3667,DeGPR: Deep Guided Posterior Regularization for Multi-Class Cell Detection and Counting,0.523711,"Multi-class cell detection and counting is an essential task for many
pathological diagnoses. Manual counting is tedious and often leads to
inter-observer variations among pathologists. While there exist multiple,
general-purpose, deep learning-based object detection and counting methods,
they may not readily transfer to detecting and counting cells in medical
images, due to the limited data, presence of tiny overlapping objects, multiple
cell types, severe class-imbalance, minute differences in size/shape of cells,
etc. In response, we propose guided posterior regularization (DeGPR), which
assists an object detector by guiding it to exploit discriminative features
among cells. The features may be pathologist-provided or inferred directly from
visual data. We validate our model on two publicly available datasets (CoNSeP
and MoNuSAC), and on MuCeD, a novel dataset that we contribute. MuCeD consists
of 55 biopsy images of the human duodenum for predicting celiac disease. We
perform extensive experimentation with three object detection baselines on
three datasets to show that DeGPR is model-agnostic, and consistently improves
baselines obtaining up to 9% (absolute) mAP gains.",None,-1
75005647-a870-4954-83da-38116af62be9,MultiMediate'23: Engagement Estimation and Bodily Behaviour Recognition in Social Interactions,0.901612,"Automatic analysis of human behaviour is a fundamental prerequisite for the
creation of machines that can effectively interact with- and support humans in
social interactions. In MultiMediate'23, we address two key human social
behaviour analysis tasks for the first time in a controlled challenge:
engagement estimation and bodily behaviour recognition in social interactions.
This paper describes the MultiMediate'23 challenge and presents novel sets of
annotations for both tasks. For engagement estimation we collected novel
annotations on the NOvice eXpert Interaction (NOXI) database. For bodily
behaviour recognition, we annotated test recordings of the MPIIGroupInteraction
corpus with the BBSI annotation scheme. In addition, we present baseline
results for both challenge tasks.",None,-1
9e39c702-ec98-431e-8908-2271ce00ceb7,Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned LLMs Suggestions,0.311427,"In this project, we have investigated the use of advanced machine learning
methods, specifically fine-tuned large language models, for pre-annotating data
for a lexical extension task, namely adding descriptive words (verbs) to an
existing (but incomplete, as of yet) ontology of event types. Several research
questions have been focused on, from the investigation of a possible heuristics
to provide at least hints to annotators which verbs to include and which are
outside the current version of the ontology, to the possible use of the
automatic scores to help the annotators to be more efficient in finding a
threshold for identifying verbs that cannot be assigned to any existing class
and therefore they are to be used as seeds for a new class. We have also
carefully examined the correlation of the automatic scores with the human
annotation. While the correlation turned out to be strong, its influence on the
annotation proper is modest due to its near linearity, even though the mere
fact of such pre-annotation leads to relatively short annotation times.",None,-1
2f642455-cdc5-4fea-9fde-2bbe98461bf1,"Camera-Radar Perception for Autonomous Vehicles and ADAS: Concepts, Datasets and Metrics",0.590647,"One of the main paths towards the reduction of traffic accidents is the
increase in vehicle safety through driver assistance systems or even systems
with a complete level of autonomy. In these types of systems, tasks such as
obstacle detection and segmentation, especially the Deep Learning-based ones,
play a fundamental role in scene understanding for correct and safe navigation.
Besides that, the wide variety of sensors in vehicles nowadays provides a rich
set of alternatives for improvement in the robustness of perception in
challenging situations, such as navigation under lighting and weather adverse
conditions. Despite the current focus given to the subject, the literature
lacks studies on radar-based and radar-camera fusion-based perception. Hence,
this work aims to carry out a study on the current scenario of camera and
radar-based perception for ADAS and autonomous vehicles. Concepts and
characteristics related to both sensors, as well as to their fusion, are
presented. Additionally, we give an overview of the Deep Learning-based
detection and segmentation tasks, and the main datasets, metrics, challenges,
and open questions in vehicle perception.",None,-1
095ebbb6-235c-49dd-bbd1-ce202945da2c,Relighting Neural Radiance Fields with Shadow and Highlight Hints,0.839207,"This paper presents a novel neural implicit radiance representation for free
viewpoint relighting from a small set of unstructured photographs of an object
lit by a moving point light source different from the view position. We express
the shape as a signed distance function modeled by a multi layer perceptron. In
contrast to prior relightable implicit neural representations, we do not
disentangle the different reflectance components, but model both the local and
global reflectance at each point by a second multi layer perceptron that, in
addition, to density features, the current position, the normal (from the
signed distace function), view direction, and light position, also takes shadow
and highlight hints to aid the network in modeling the corresponding high
frequency light transport effects. These hints are provided as a suggestion,
and we leave it up to the network to decide how to incorporate these in the
final relit result. We demonstrate and validate our neural implicit
representation on synthetic and real scenes exhibiting a wide variety of
shapes, material properties, and global illumination light transport.",None,-1
28edcd43-2c50-4ee3-845e-4cdd3755f857,HanoiT: Enhancing Context-aware Translation via Selective Context,0.25694,"Context-aware neural machine translation aims to use the document-level
context to improve translation quality. However, not all words in the context
are helpful. The irrelevant or trivial words may bring some noise and distract
the model from learning the relationship between the current sentence and the
auxiliary context. To mitigate this problem, we propose a novel end-to-end
encoder-decoder model with a layer-wise selection mechanism to sift and refine
the long document context. To verify the effectiveness of our method, extensive
experiments and extra quantitative analysis are conducted on four
document-level machine translation benchmarks. The experimental results
demonstrate that our model significantly outperforms previous models on all
datasets via the soft selection mechanism.",None,-1
c83e8d8f-309c-4bd8-96b7-fba99dabaca4,Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems using Differentially Private Language Models,0.625143,"We address the challenge of ensuring differential privacy (DP) guarantees in
training deep retrieval systems. Training these systems often involves the use
of contrastive-style losses, which are typically non-per-example decomposable,
making them difficult to directly DP-train with since common techniques require
per-example gradients. To address this issue, we propose an approach that
prioritizes ensuring query privacy prior to training a deep retrieval system.
Our method employs DP language models (LMs) to generate private synthetic
queries representative of the original data. These synthetic queries can be
used in downstream retrieval system training without compromising privacy. Our
approach demonstrates a significant enhancement in retrieval quality compared
to direct DP-training, all while maintaining query-level privacy guarantees.
This work highlights the potential of harnessing LMs to overcome limitations in
standard DP-training methods.",None,-1
b99065e6-7848-4ee7-81de-a6543b20b833,Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints,0.636655,"Local feature extraction is a standard approach in computer vision for
tackling important tasks such as image matching and retrieval. The core
assumption of most methods is that images undergo affine transformations,
disregarding more complicated effects such as non-rigid deformations.
Furthermore, incipient works tailored for non-rigid correspondence still rely
on keypoint detectors designed for rigid transformations, hindering performance
due to the limitations of the detector. We propose DALF (Deformation-Aware
Local Features), a novel deformation-aware network for jointly detecting and
describing keypoints, to handle the challenging problem of matching deformable
surfaces. All network components work cooperatively through a feature fusion
approach that enforces the descriptors' distinctiveness and invariance.
Experiments using real deforming objects showcase the superiority of our
method, where it delivers 8% improvement in matching scores compared to the
previous best results. Our approach also enhances the performance of two
real-world applications: deformable object retrieval and non-rigid 3D surface
registration. Code for training, inference, and applications are publicly
available at https://verlab.dcc.ufmg.br/descriptors/dalf_cvpr23.",None,-1
cb22b410-77c7-48ce-850c-a0783514e9cf,Emergent and Predictable Memorization in Large Language Models,0.383328,"Memorization, or the tendency of large language models (LLMs) to output
entire sequences from their training data verbatim, is a key concern for safely
deploying language models. In particular, it is vital to minimize a model's
memorization of sensitive datapoints such as those containing personal
identifiable information (PII). The prevalence of such undesirable memorization
can pose issues for model trainers, and may even require discarding an
otherwise functional model. We therefore seek to predict which sequences will
be memorized before a large model's full train-time by extrapolating the
memorization behavior of lower-compute trial runs. We measure memorization of
the Pythia model suite and plot scaling laws for forecasting memorization,
allowing us to provide equi-compute recommendations to maximize the reliability
(recall) of such predictions. We additionally provide further novel discoveries
on the distribution of memorization scores across models and data. We release
all code and data necessary to reproduce the results in this paper at
https://github.com/EleutherAI/pythia",None,-1
5e321a0d-e458-4bc0-99fc-572c2272b692,Three Bricks to Consolidate Watermarks for Large Language Models,0.982427,"The task of discerning between generated and natural texts is increasingly
challenging. In this context, watermarking emerges as a promising technique for
ascribing generated text to a specific model. It alters the sampling generation
process so as to leave an invisible trace in the generated output, facilitating
later detection. This research consolidates watermarks for large language
models based on three theoretical and empirical considerations. First, we
introduce new statistical tests that offer robust theoretical guarantees which
remain valid even at low false-positive rates (less than 10$^{\text{-6}}$).
Second, we compare the effectiveness of watermarks using classical benchmarks
in the field of natural language processing, gaining insights into their
real-world applicability. Third, we develop advanced detection schemes for
scenarios where access to the LLM is available, as well as multi-bit
watermarking.",None,-1
b684bd6d-51d4-434b-9665-4ce415b6febc,The sample complexity of multi-distribution learning,0.550959,"Multi-distribution learning generalizes the classic PAC learning to handle
data coming from multiple distributions. Given a set of $k$ data distributions
and a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis
that minimizes the maximum population loss over $k$ distributions, up to
$\epsilon$ additive error. In this paper, we settle the sample complexity of
multi-distribution learning by giving an algorithm of sample complexity
$\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$. This matches the
lower bound up to sub-polynomial factor and resolves the COLT 2023 open problem
of Awasthi, Haghtalab and Zhao [AHZ23].",None,-1
0bdaa4d0-aef2-4486-b074-81d08a591b85,Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT -- A Text-to-SQL Parsing Comparison,0.624209,"The success of ChatGPT has ignited an AI race, with researchers striving to
develop new large language models (LLMs) that can match or surpass the language
understanding and generation abilities of commercial ones. In recent times, a
number of models have emerged, claiming performance near that of GPT-3.5 or
GPT-4 through various instruction-tuning methods. As practitioners of
Text-to-SQL parsing, we are grateful for their valuable contributions to
open-source research. However, it is important to approach these claims with a
sense of scrutiny and ascertain the actual effectiveness of these models.
Therefore, we pit six popular large language models against each other,
systematically evaluating their Text-to-SQL parsing capability on nine
benchmark datasets with five different prompting strategies, covering both
zero-shot and few-shot scenarios. Regrettably, the open-sourced models fell
significantly short of the performance achieved by closed-source models like
GPT-3.5, highlighting the need for further work to bridge the performance gap
between these models.",None,-1
5d156219-2e5b-42ba-aacb-33bbd4fc3452,Asynchronous training of quantum reinforcement learning,0.612224,"The development of quantum machine learning (QML) has received a lot of
interest recently thanks to developments in both quantum computing (QC) and
machine learning (ML). One of the ML paradigms that can be utilized to address
challenging sequential decision-making issues is reinforcement learning (RL).
It has been demonstrated that classical RL can successfully complete many
difficult tasks. A leading method of building quantum RL agents relies on the
variational quantum circuits (VQC). However, training QRL algorithms with VQCs
requires significant amount of computational resources. This issue hurdles the
exploration of various QRL applications. In this paper, we approach this
challenge through asynchronous training QRL agents. Specifically, we choose the
asynchronous training of advantage actor-critic variational quantum policies.
We demonstrate the results via numerical simulations that within the tasks
considered, the asynchronous training of QRL agents can reach performance
comparable to or superior than classical agents with similar model sizes and
architectures.",None,-1
cdc1788a-1771-4fbe-b83c-8ffd13e1e224,Explaining Vision and Language through Graphs of Events in Space and Time,0.131155,"Artificial Intelligence makes great advances today and starts to bridge the
gap between vision and language. However, we are still far from understanding,
explaining and controlling explicitly the visual content from a linguistic
perspective, because we still lack a common explainable representation between
the two domains. In this work we come to address this limitation and propose
the Graph of Events in Space and Time (GEST), by which we can represent, create
and explain, both visual and linguistic stories. We provide a theoretical
justification of our model and an experimental validation, which proves that
GEST can bring a solid complementary value along powerful deep learning models.
In particular, GEST can help improve at the content-level the generation of
videos from text, by being easily incorporated into our novel video generation
engine. Additionally, by using efficient graph matching techniques, the GEST
graphs can also improve the comparisons between texts at the semantic level.",None,-1
b139d6d8-cfa8-47f2-8484-964bf7197af4,Framework for Quality Evaluation of Smart Roadside Infrastructure Sensors for Automated Driving Applications,0.684205,"The use of smart roadside infrastructure sensors is highly relevant for
future applications of connected and automated vehicles. External sensor
technology in the form of intelligent transportation system stations (ITS-Ss)
can provide safety-critical real-time information about road users in the form
of a digital twin. The choice of sensor setups has a major influence on the
downstream function as well as the data quality. To date, there is insufficient
research on which sensor setups result in which levels of ITS-S data quality.
We present a novel approach to perform detailed quality assessment for smart
roadside infrastructure sensors. Our framework is multimodal across different
sensor types and is evaluated on the DAIR-V2X dataset. We analyze the
composition of different lidar and camera sensors and assess them in terms of
accuracy, latency, and reliability. The evaluations show that the framework can
be used reliably for several future ITS-S applications.",None,-1
e3d5b709-0960-4100-9d4f-1ce009e71447,DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation,0.998551,"Text-to-image diffusion models pre-trained on billions of image-text pairs
have recently enabled 3D content creation by optimizing a randomly initialized
differentiable 3D representation with score distillation. However, the
optimization process suffers slow convergence and the resultant 3D models often
exhibit two limitations: (a) quality concerns such as missing attributes and
distorted shape and texture; (b) extremely low diversity comparing to
text-guided image synthesis. In this paper, we show that the conflict between
the 3D optimization process and uniform timestep sampling in score distillation
is the main reason for these limitations. To resolve this conflict, we propose
to prioritize timestep sampling with monotonically non-increasing functions,
which aligns the 3D optimization process with the sampling process of diffusion
model. Extensive experiments show that our simple redesign significantly
improves 3D content creation with faster convergence, better quality and
diversity.",None,-1
a2c94a0e-e01b-48c5-b371-67a3a2e3c3b6,GridMM: Grid Memory Map for Vision-and-Language Navigation,0.784122,"Vision-and-language navigation (VLN) enables the agent to navigate to a
remote location following the natural language instruction in 3D environments.
To represent the previously visited environment, most approaches for VLN
implement memory using recurrent states, topological maps, or top-down semantic
maps. In contrast to these approaches, we build the top-down egocentric and
dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited
environment. From a global perspective, historical observations are projected
into a unified grid map in a top-down view, which can better represent the
spatial relations of the environment. From a local perspective, we further
propose an instruction relevance aggregation method to capture fine-grained
visual clues in each grid region. Extensive experiments are conducted on both
the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE
dataset in the continuous environments, showing the superiority of our proposed
method.",None,-1
053238ca-2472-4770-a9e8-094419d4b261,Scalable Prompt Generation for Semi-supervised Learning with Language Models,0.652661,"Prompt-based learning methods in semi-supervised learning (SSL) settings have
been shown to be effective on multiple natural language understanding (NLU)
datasets and tasks in the literature. However, manually designing multiple
prompts and verbalizers requires domain knowledge and human effort, making it
difficult and expensive to scale across different datasets. In this paper, we
propose two methods to automatically design multiple prompts and integrate
automatic verbalizer in SSL settings without sacrificing performance. The first
method uses various demonstration examples with learnable continuous prompt
tokens to create diverse prompt models. The second method uses a varying number
of soft prompt tokens to encourage language models to learn different prompts.
For the verbalizer, we use the prototypical verbalizer to replace the manual
one. In summary, we obtained the best average accuracy of 73.2% (a relative
improvement of 2.52% over even the previous state-of-the-art SSL method with
manual prompts and verbalizers) in different few-shot learning settings.",None,-1
2b0174bd-3d01-4ff5-9b9a-69248a28ad31,Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero,0.707133,"Artificial Intelligence (AI) systems have made remarkable progress, attaining
super-human performance across various domains. This presents us with an
opportunity to further human knowledge and improve human expert performance by
leveraging the hidden knowledge encoded within these highly performant AI
systems. Yet, this knowledge is often hard to extract, and may be hard to
understand or learn from. Here, we show that this is possible by proposing a
new method that allows us to extract new chess concepts in AlphaZero, an AI
system that mastered the game of chess via self-play without human supervision.
Our analysis indicates that AlphaZero may encode knowledge that extends beyond
the existing human knowledge, but knowledge that is ultimately not beyond human
grasp, and can be successfully learned from. In a human study, we show that
these concepts are learnable by top human experts, as four top chess
grandmasters show improvements in solving the presented concept prototype
positions. This marks an important first milestone in advancing the frontier of
human knowledge by leveraging AI; a development that could bear profound
implications and help us shape how we interact with AI systems across many AI
applications.",None,-1
1f921a5f-631e-41c1-8748-bd19c212ad46,Navigating to Objects Specified by Images,0.968765,"Images are a convenient way to specify which particular object instance an
embodied agent should navigate to. Solving this task requires semantic visual
reasoning and exploration of unknown environments. We present a system that can
perform this task in both simulation and the real world. Our modular method
solves sub-tasks of exploration, goal instance re-identification, goal
localization, and local navigation. We re-identify the goal instance in
egocentric vision using feature-matching and localize the goal instance by
projecting matched features to a map. Each sub-task is solved using
off-the-shelf components requiring zero fine-tuning. On the HM3D
InstanceImageNav benchmark, this system outperforms a baseline end-to-end RL
policy 7x and a state-of-the-art ImageNav model 2.3x (56% vs 25% success). We
deploy this system to a mobile robot platform and demonstrate effective
real-world performance, achieving an 88% success rate across a home and an
office environment.",None,-1
b511e8c1-c184-47bf-b5a5-13ae49084721,Knowledge-based Reasoning and Learning under Partial Observability in Ad Hoc Teamwork,0.364152,"Ad hoc teamwork refers to the problem of enabling an agent to collaborate
with teammates without prior coordination. Data-driven methods represent the
state of the art in ad hoc teamwork. They use a large labeled dataset of prior
observations to model the behavior of other agent types and to determine the ad
hoc agent's behavior. These methods are computationally expensive, lack
transparency, and make it difficult to adapt to previously unseen changes,
e.g., in team composition. Our recent work introduced an architecture that
determined an ad hoc agent's behavior based on non-monotonic logical reasoning
with prior commonsense domain knowledge and predictive models of other agents'
behavior that were learned from limited examples. In this paper, we
substantially expand the architecture's capabilities to support: (a) online
selection, adaptation, and learning of the models that predict the other
agents' behavior; and (b) collaboration with teammates in the presence of
partial observability and limited communication. We illustrate and
experimentally evaluate the capabilities of our architecture in two simulated
multiagent benchmark domains for ad hoc teamwork: Fort Attack and Half Field
Offense. We show that the performance of our architecture is comparable or
better than state of the art data-driven baselines in both simple and complex
scenarios, particularly in the presence of limited training data, partial
observability, and changes in team composition.",None,-1
71c696dd-9ca4-49ed-9690-cf0a5f8f48b7,Sentence Identification with BOS and EOS Label Combinations,0.0518499,"The sentence is a fundamental unit in many NLP applications. Sentence
segmentation is widely used as the first preprocessing task, where an input
text is split into consecutive sentences considering the end of the sentence
(EOS) as their boundaries. This task formulation relies on a strong assumption
that the input text consists only of sentences, or what we call the sentential
units (SUs). However, real-world texts often contain non-sentential units
(NSUs) such as metadata, sentence fragments, nonlinguistic markers, etc. which
are unreasonable or undesirable to be treated as a part of an SU. To tackle
this issue, we formulate a novel task of sentence identification, where the
goal is to identify SUs while excluding NSUs in a given text. To conduct
sentence identification, we propose a simple yet effective method which
combines the beginning of the sentence (BOS) and EOS labels to determine the
most probable SUs and NSUs based on dynamic programming. To evaluate this task,
we design an automatic, language-independent procedure to convert the Universal
Dependencies corpora into sentence identification benchmarks. Finally, our
experiments on the sentence identification task demonstrate that our proposed
method generally outperforms sentence segmentation baselines which only utilize
EOS labels.",None,-1
f2f9aa8b-9bf2-42fa-be5b-0facff42f9e9,A New Class of Explanations for Classifiers with Non-Binary Features,0.423838,"Two types of explanations have been receiving increased attention in the
literature when analyzing the decisions made by classifiers. The first type
explains why a decision was made and is known as a sufficient reason for the
decision, also an abductive explanation or a PI-explanation. The second type
explains why some other decision was not made and is known as a necessary
reason for the decision, also a contrastive or counterfactual explanation.
These explanations were defined for classifiers with binary, discrete and, in
some cases, continuous features. We show that these explanations can be
significantly improved in the presence of non-binary features, leading to a new
class of explanations that relay more information about decisions and the
underlying classifiers. Necessary and sufficient reasons were also shown to be
the prime implicates and implicants of the complete reason for a decision,
which can be obtained using a quantification operator. We show that our
improved notions of necessary and sufficient reasons are also prime implicates
and implicants but for an improved notion of complete reason obtained by a new
quantification operator that we also define and study.",None,-1
16ec8d0c-1f73-445f-a88d-0e2d46473ca2,GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts,0.984922,"Large language models (LLMs) have recently experienced tremendous popularity
and are widely used from casual conversations to AI-driven programming.
However, despite their considerable success, LLMs are not entirely reliable and
can give detailed guidance on how to conduct harmful or illegal activities.
While safety measures can reduce the risk of such outputs, adversarial
jailbreak attacks can still exploit LLMs to produce harmful content. These
jailbreak templates are typically manually crafted, making large-scale testing
challenging.
  In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing
framework inspired by the AFL fuzzing framework. Instead of manual engineering,
GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.
At its core, GPTFuzz starts with human-written templates as initial seeds, then
mutates them to produce new templates. We detail three key components of
GPTFuzz: a seed selection strategy for balancing efficiency and variability,
mutate operators for creating semantically equivalent or similar sentences, and
a judgment model to assess the success of a jailbreak attack.
  We evaluate GPTFuzz against various commercial and open-source LLMs,
including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our
results indicate that GPTFuzz consistently produces jailbreak templates with a
high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz
achieves over 90% attack success rates against ChatGPT and Llama-2 models, even
with suboptimal initial seed templates. We anticipate that GPTFuzz will be
instrumental for researchers and practitioners in examining LLM robustness and
will encourage further exploration into enhancing LLM safety.",None,-1
6b545fbc-a771-400c-9674-246938aad95d,f-Divergence Minimization for Sequence-Level Knowledge Distillation,0.928757,"Knowledge distillation (KD) is the process of transferring knowledge from a
large model to a small one. It has gained increasing attention in the natural
language processing community, driven by the demands of compressing
ever-growing language models. In this work, we propose an f-DISTILL framework,
which formulates sequence-level knowledge distillation as minimizing a
generalized f-divergence function. We propose four distilling variants under
our framework and show that existing SeqKD and ENGINE approaches are
approximations of our f-DISTILL methods. We further derive step-wise
decomposition for our f-DISTILL, reducing intractable sequence-level divergence
to word-level losses that can be computed in a tractable manner. Experiments
across four datasets show that our methods outperform existing KD approaches,
and that our symmetric distilling losses can better force the student to learn
from the teacher distribution.",None,-1
2a10a22f-fe98-43b4-a5c2-91000c66bba1,Generative Plug and Play: Posterior Sampling for Inverse Problems,0.747906,"Over the past decade, Plug-and-Play (PnP) has become a popular method for
reconstructing images using a modular framework consisting of a forward and
prior model. The great strength of PnP is that an image denoiser can be used as
a prior model while the forward model can be implemented using more traditional
physics-based approaches. However, a limitation of PnP is that it reconstructs
only a single deterministic image.
  In this paper, we introduce Generative Plug-and-Play (GPnP), a generalization
of PnP to sample from the posterior distribution. As with PnP, GPnP has a
modular framework using a physics-based forward model and an image denoising
prior model. However, in GPnP these models are extended to become proximal
generators, which sample from associated distributions. GPnP applies these
proximal generators in alternation to produce samples from the posterior. We
present experimental simulations using the well-known BM3D denoiser. Our
results demonstrate that the GPnP method is robust, easy to implement, and
produces intuitively reasonable samples from the posterior for sparse
interpolation and tomographic reconstruction. Code to accompany this paper is
available at https://github.com/gbuzzard/generative-pnp-allerton .",None,-1
c208c0f6-9a34-4fdf-9384-b4d8cf0bc54b,Semiconductor Fab Scheduling with Self-Supervised and Reinforcement Learning,0.506828,"Semiconductor manufacturing is a notoriously complex and costly multi-step
process involving a long sequence of operations on expensive and
quantity-limited equipment. Recent chip shortages and their impacts have
highlighted the importance of semiconductors in the global supply chains and
how reliant on those our daily lives are. Due to the investment cost,
environmental impact, and time scale needed to build new factories, it is
difficult to ramp up production when demand spikes.
  This work introduces a method to successfully learn to schedule a
semiconductor manufacturing facility more efficiently using deep reinforcement
and self-supervised learning. We propose the first adaptive scheduling approach
to handle complex, continuous, stochastic, dynamic, modern semiconductor
manufacturing models. Our method outperforms the traditional hierarchical
dispatching strategies typically used in semiconductor manufacturing plants,
substantially reducing each order's tardiness and time until completion. As a
result, our method yields a better allocation of resources in the semiconductor
manufacturing process.",None,-1
943392e8-f1e9-427f-aadb-00ac6b6a1776,Rationale-Enhanced Language Models are Better Continual Relation Learners,0.926837,"Continual relation extraction (CRE) aims to solve the problem of catastrophic
forgetting when learning a sequence of newly emerging relations. Recent CRE
studies have found that catastrophic forgetting arises from the model's lack of
robustness against future analogous relations. To address the issue, we
introduce rationale, i.e., the explanations of relation classification results
generated by large language models (LLM), into CRE task. Specifically, we
design the multi-task rationale tuning strategy to help the model learn current
relations robustly. We also conduct contrastive rationale replay to further
distinguish analogous relations. Experimental results on two standard
benchmarks demonstrate that our method outperforms the state-of-the-art CRE
models.",None,-1
de95f5a7-d7cf-4fa3-9745-949292701b67,ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation,0.558938,"Compositional generalization benchmarks for semantic parsing seek to assess
whether models can accurately compute meanings for novel sentences, but
operationalize this in terms of logical form (LF) prediction. This raises the
concern that semantically irrelevant details of the chosen LFs could shape
model performance. We argue that this concern is realized for the COGS
benchmark. COGS poses generalization splits that appear impossible for
present-day models, which could be taken as an indictment of those models.
However, we show that the negative results trace to incidental features of COGS
LFs. Converting these LFs to semantically equivalent ones and factoring out
capabilities unrelated to semantic interpretation, we find that even baseline
models get traction. A recent variable-free translation of COGS LFs suggests
similar conclusions, but we observe this format is not semantically equivalent;
it is incapable of accurately representing some COGS meanings. These findings
inform our proposal for ReCOGS, a modified version of COGS that comes closer to
assessing the target semantic capabilities while remaining very challenging.
Overall, our results reaffirm the importance of compositional generalization
and careful benchmark task design.",None,-1
