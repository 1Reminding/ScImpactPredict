id,title,TNCSI,cites,abstract,OA,authors_title
0af66ae3-daee-482d-8b6a-636598799910,INF: Implicit Neural Fusion for LiDAR and Camera,0.568435,4,"Sensor fusion has become a popular topic in robotics. However, conventional
fusion methods encounter many difficulties, such as data representation
differences, sensor variations, and extrinsic calibration. For example, the
calibration methods used for LiDAR-camera fusion often require manual operation
and auxiliary calibration targets. Implicit neural representations (INRs) have
been developed for 3D scenes, and the volume density distribution involved in
an INR unifies the scene information obtained by different types of sensors.
Therefore, we propose implicit neural fusion (INF) for LiDAR and camera. INF
first trains a neural density field of the target scene using LiDAR frames.
Then, a separate neural color field is trained using camera images and the
trained neural density field. Along with the training process, INF both
estimates LiDAR poses and optimizes extrinsic parameters. Our experiments
demonstrate the high accuracy and stable performance of the proposed method.",None,-1
f681a533-1b4e-408c-813b-60995b2e9110,Eye Disease Classification Using Deep Learning Techniques,0.5359,6,"Eye is the essential sense organ for vision function. Due to the fact that
certain eye disorders might result in vision loss, it is essential to diagnose
and treat eye diseases early on. By identifying common eye illnesses and
performing an eye check, eye care providers can safeguard patients against
vision loss or blindness. Convolutional neural networks (CNN) and transfer
learning were employed in this study to discriminate between a normal eye and
one with diabetic retinopathy, cataract, or glaucoma disease. Using transfer
learning for multi-class classification, high accuracy was achieved at 94%
while the traditional CNN achieved 84% rate.",None,-1
318cb870-d93d-4a53-bb05-25906548ef91,A recommender for the management of chronic pain in patients undergoing spinal cord stimulation,0.426041,1,"Spinal cord stimulation (SCS) is a therapeutic approach used for the
management of chronic pain. It involves the delivery of electrical impulses to
the spinal cord via an implanted device, which when given suitable stimulus
parameters can mask or block pain signals. Selection of optimal stimulation
parameters usually happens in the clinic under the care of a provider whereas
at-home SCS optimization is managed by the patient. In this paper, we propose a
recommender system for the management of pain in chronic pain patients
undergoing SCS. In particular, we use a contextual multi-armed bandit (CMAB)
approach to develop a system that recommends SCS settings to patients with the
aim of improving their condition. These recommendations, sent directly to
patients though a digital health ecosystem, combined with a patient monitoring
system closes the therapeutic loop around a chronic pain patient over their
entire patient journey. We evaluated the system in a cohort of SCS-implanted
ENVISION study subjects (Clinicaltrials.gov ID: NCT03240588) using a
combination of quality of life metrics and Patient States (PS), a novel measure
of holistic outcomes. SCS recommendations provided statistically significant
improvement in clinical outcomes (pain and/or QoL) in 85\% of all subjects
(N=21). Among subjects in moderate PS (N=7) prior to receiving recommendations,
100\% showed statistically significant improvements and 5/7 had improved PS
dwell time. This analysis suggests SCS patients may benefit from SCS
recommendations, resulting in additional clinical improvement on top of
benefits already received from SCS therapy.",None,-1
f5b0bead-8a60-4baa-ab2d-75ef5eeaea2e,Speech Translation with Foundation Models and Optimal Transport: UPC at IWSLT23,0.291153,2,"This paper describes the submission of the UPC Machine Translation group to
the IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems
utilize foundation models for speech (wav2vec 2.0) and text (mBART50). We
incorporate a Siamese pretraining step of the speech and text encoders with CTC
and Optimal Transport, to adapt the speech representations to the space of the
text model, thus maximizing transfer learning from MT. After this pretraining,
we fine-tune our system end-to-end on ST, with Cross Entropy and Knowledge
Distillation. Apart from the available ST corpora, we create synthetic data
with SegAugment to better adapt our models to the custom segmentations of the
IWSLT test sets. Our best single model obtains 31.2 BLEU points on MuST-C
tst-COMMON, 29.8 points on IWLST.tst2020 and 33.4 points on the newly released
IWSLT.ACLdev2023.",None,-1
ee2384f0-de01-4030-81fd-8d2b89017c36,Let's reward step by step: Step-Level reward model as the Navigators for Reasoning,0.50468,13,"Recent years have seen considerable advancements in multi-step reasoning with
Large Language Models (LLMs). The previous studies have elucidated the merits
of integrating feedback or search mechanisms during model inference to improve
the reasoning accuracy. The Process-Supervised Reward Model (PRM), typically
furnishes LLMs with step-by-step feedback during the training phase, akin to
Proximal Policy Optimization (PPO) or reject sampling. Our objective is to
examine the efficacy of PRM in the inference phase to help discern the optimal
solution paths for multi-step tasks such as mathematical reasoning and code
generation. To this end, we propose a heuristic greedy search algorithm that
employs the step-level feedback from PRM to optimize the reasoning pathways
explored by LLMs. This tailored PRM demonstrated enhanced results compared to
the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH.
Additionally, to explore the versatility of our approach, we develop a novel
method to automatically generate step-level reward dataset for coding tasks and
observed similar improved performance in the code generation tasks. Thus
highlighting the robust nature of our reward-model-based approach to inference
for reasoning tasks.",None,-1
68ab1f0a-0a67-4fda-8a22-2011a111283f,3D Registration with Maximal Cliques,0.974826,37,"As a fundamental problem in computer vision, 3D point cloud registration
(PCR) aims to seek the optimal pose to align a point cloud pair. In this paper,
we present a 3D registration method with maximal cliques (MAC). The key insight
is to loosen the previous maximum clique constraint, and mine more local
consensus information in a graph for accurate pose hypotheses generation: 1) A
compatibility graph is constructed to render the affinity relationship between
initial correspondences. 2) We search for maximal cliques in the graph, each of
which represents a consensus set. We perform node-guided clique selection then,
where each node corresponds to the maximal clique with the greatest graph
weight. 3) Transformation hypotheses are computed for the selected cliques by
the SVD algorithm and the best hypothesis is used to perform registration.
Extensive experiments on U3M, 3DMatch, 3DLoMatch and KITTI demonstrate that MAC
effectively increases registration accuracy, outperforms various
state-of-the-art methods and boosts the performance of deep-learned methods.
MAC combined with deep-learned methods achieves state-of-the-art registration
recall of 95.7% / 78.9% on 3DMatch / 3DLoMatch.",None,-1
830bbf8a-1612-4289-b2be-3a78ddd06876,ProAgent: Building Proactive Cooperative Agents with Large Language Models,0.999988,30,"Building agents with adaptive behavior in cooperative tasks stands as a
paramount goal in the realm of multi-agent systems. Current approaches to
developing cooperative agents rely primarily on learning-based methods, whose
policy generalization depends heavily on the diversity of teammates they
interact with during the training phase. Such reliance, however, constrains the
agents' capacity for strategic adaptation when cooperating with unfamiliar
teammates, which becomes a significant challenge in zero-shot coordination
scenarios. To address this challenge, we propose ProAgent, a novel framework
that harnesses large language models (LLMs) to create proactive agents capable
of dynamically adapting their behavior to enhance cooperation with teammates.
ProAgent can analyze the present state, and infer the intentions of teammates
from observations. It then updates its beliefs in alignment with the teammates'
subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of
modularity and interpretability, making it easily integrated into various of
coordination scenarios. Experimental evaluations conducted within the
Overcooked-AI environment unveil the remarkable performance superiority of
ProAgent, outperforming five methods based on self-play and population-based
training when cooperating with AI agents. Furthermore, in partnered with human
proxy models, its performance exhibits an average improvement exceeding 10%
compared to the current state-of-the-art method. For more information about our
project, please visit~\url{https://pku-proagent.github.io}.",None,-1
dc45fdb5-2609-41ba-bb21-f4be135b97a2,Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection,0.989824,11,"This paper investigates the effectiveness of large language models (LLMs) in
email spam detection by comparing prominent models from three distinct
families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we
examine well-established machine learning techniques for spam detection, such
as Na\""ive Bayes and LightGBM, as baseline methods. We assess the performance
of these models across four public datasets, utilizing different numbers of
training samples (full training set and few-shot settings). Our findings reveal
that, in the majority of cases, LLMs surpass the performance of the popular
baseline techniques, particularly in few-shot scenarios. This adaptability
renders LLMs uniquely suited to spam detection tasks, where labeled samples are
limited in number and models require frequent updates. Additionally, we
introduce Spam-T5, a Flan-T5 model that has been specifically adapted and
fine-tuned for the purpose of detecting email spam. Our results demonstrate
that Spam-T5 surpasses baseline models and other LLMs in the majority of
scenarios, particularly when there are a limited number of training samples
available. Our code is publicly available at
https://github.com/jpmorganchase/emailspamdetection.",None,-1
72b39733-826f-4d2a-9f13-e2fabd1dea70,On the Generalization Ability of Retrieval-Enhanced Transformers,0.172787,3,"Recent work on the Retrieval-Enhanced Transformer (RETRO) model has shown
that off-loading memory from trainable weights to a retrieval database can
significantly improve language modeling and match the performance of
non-retrieval models that are an order of magnitude larger in size. It has been
suggested that at least some of this performance gain is due to non-trivial
generalization based on both model weights and retrieval. In this paper, we try
to better understand the relative contributions of these two components. We
find that the performance gains from retrieval largely originate from
overlapping tokens between the database and the test data, suggesting less
non-trivial generalization than previously assumed. More generally, our results
point to the challenges of evaluating the generalization of retrieval-augmented
language models such as RETRO, as even limited token overlap may significantly
decrease test-time loss. We release our code and model at
https://github.com/TobiasNorlund/retro",None,-1
ded85fc3-b467-4ca0-a013-b67504a5674d,Examining Autoexposure for Challenging Scenes,0.864665,1,"Autoexposure (AE) is a critical step applied by camera systems to ensure
properly exposed images. While current AE algorithms are effective in well-lit
environments with constant illumination, these algorithms still struggle in
environments with bright light sources or scenes with abrupt changes in
lighting. A significant hurdle in developing new AE algorithms for challenging
environments, especially those with time-varying lighting, is the lack of
suitable image datasets. To address this issue, we have captured a new 4D
exposure dataset that provides a large solution space (i.e., shutter speed
range from (1/500 to 15 seconds) over a temporal sequence with moving objects,
bright lights, and varying lighting. In addition, we have designed a software
platform to allow AE algorithms to be used in a plug-and-play manner with the
dataset. Our dataset and associate platform enable repeatable evaluation of
different AE algorithms and provide a much-needed starting point to develop
better AE methods. We examine several existing AE strategies using our dataset
and show that most users prefer a simple saliency method for challenging
lighting conditions.",None,-1
3fd2cd28-a675-4967-a4fb-79f252b94923,Algebraic Positional Encodings,0.0808471,1,"We introduce a novel positional encoding strategy for Transformer-style
models, addressing the shortcomings of existing, often ad hoc, approaches. Our
framework provides a flexible mapping from the algebraic specification of a
domain to an interpretation as orthogonal operators. This design preserves the
algebraic characteristics of the source domain, ensuring that the model upholds
the desired structural properties. Our scheme can accommodate various
structures, including sequences, grids and trees, as well as their
compositions. We conduct a series of experiments to demonstrate the practical
applicability of our approach. Results suggest performance on par with or
surpassing the current state-of-the-art, without hyperparameter optimizations
or ``task search'' of any kind. Code will be made available at
\url{github.com/konstantinosKokos/UnitaryPE}.",None,-1
4414a9fe-1c77-4925-b3c4-8a7cedf71da1,Ensemble Distillation for Unsupervised Constituency Parsing,0.69139,2,"We investigate the unsupervised constituency parsing task, which organizes
words and phrases of a sentence into a hierarchical structure without using
linguistically annotated data. We observe that existing unsupervised parsers
capture differing aspects of parsing structures, which can be leveraged to
enhance unsupervised parsing performance. To this end, we propose a notion of
""tree averaging,"" based on which we further propose a novel ensemble method for
unsupervised parsing. To improve inference efficiency, we further distill the
ensemble knowledge into a student model; such an ensemble-then-distill process
is an effective approach to mitigate the over-smoothing problem existing in
common multi-teacher distilling methods. Experiments show that our method
surpasses all previous approaches, consistently demonstrating its effectiveness
and robustness across various runs, with different ensemble components, and
under domain-shift conditions.",None,-1
2ea12f41-25e7-4be6-a2c7-9d8754dc99b7,Japanese Lexical Complexity for Non-Native Readers: A New Dataset,0.694203,5,"Lexical complexity prediction (LCP) is the task of predicting the complexity
of words in a text on a continuous scale. It plays a vital role in simplifying
or annotating complex words to assist readers. To study lexical complexity in
Japanese, we construct the first Japanese LCP dataset. Our dataset provides
separate complexity scores for Chinese/Korean annotators and others to address
the readers' L1-specific needs. In the baseline experiment, we demonstrate the
effectiveness of a BERT-based system for Japanese LCP.",None,-1
2d3becbb-ae71-4142-acc9-8e8d158e69c7,Professional Basketball Player Behavior Synthesis via Planning with Diffusion,0.917915,3,"Dynamically planning in multi-agent systems has been explored to improve
decision-making in various domains. Professional basketball serves as a
compelling example of a dynamic spatio-temporal game, encompassing both
concealed strategic policies and decision-making. However, processing the
diverse on-court signals and navigating the vast space of potential actions and
outcomes makes it difficult for existing approaches to swiftly identify optimal
strategies in response to evolving circumstances. In this study, we first
formulate the sequential decision-making process as a conditional trajectory
generation process. We further introduce PLAYBEST (PLAYer BEhavior SynThesis),
a method for enhancing player decision-making. We extend the state-of-the-art
generative model, diffusion probabilistic model, to learn challenging
multi-agent environmental dynamics from historical National Basketball
Association (NBA) player motion tracking data. To incorporate data-driven
strategies, an auxiliary value function is trained using the play-by-play data
with corresponding rewards acting as the plan guidance. To accomplish
reward-guided trajectory generation, conditional sampling is introduced to
condition the diffusion model on the value function and conduct
classifier-guided sampling. We validate the effectiveness of PLAYBEST via
comprehensive simulation studies from real-world data, contrasting the
generated trajectories and play strategies with those employed by professional
basketball teams. Our results reveal that the model excels at generating
high-quality basketball trajectories that yield efficient plays, surpassing
conventional planning techniques in terms of adaptability, flexibility, and
overall performance. Moreover, the synthesized play strategies exhibit a
remarkable alignment with professional tactics, highlighting the model's
capacity to capture the intricate dynamics of basketball games.",None,-1
af54fc6b-ea2c-419c-92f2-e6047ee5e958,Applying a Color Palette with Local Control using Diffusion Models,0.164646,3,"We demonstrate two novel editing procedures in the context of fantasy art.
Palette transfer applies a specified reference palette to a given image. For
fantasy art, the desired change in palette can be very large, leading to huge
changes in the ``look'' of the art. We show that a pipeline of vector
quantization; matching; and ``dequantization'' (using a diffusion model)
produces successful extreme palette transfers. A novel training loss measures
the match between color distribution in control and generated images even when
a ground truth target is not available. This measurably improves performance.
Segment control allows an artist to move one or more image segments, and to
optionally specify the desired color of the result. The combination of these
two types of edit yields valuable workflows. We demonstrate our methods on the
challenging Yu-Gi-Oh card art dataset.",None,-1
35726a7b-18f2-4b2b-8bd7-2365f5229608,Open-WikiTable: Dataset for Open Domain Question Answering with Complex Reasoning over Table,0.269592,6,"Despite recent interest in open domain question answering (ODQA) over tables,
many studies still rely on datasets that are not truly optimal for the task
with respect to utilizing structural nature of table. These datasets assume
answers reside as a single cell value and do not necessitate exploring over
multiple cells such as aggregation, comparison, and sorting. Thus, we release
Open-WikiTable, the first ODQA dataset that requires complex reasoning over
tables. Open-WikiTable is built upon WikiSQL and WikiTableQuestions to be
applicable in the open-domain setting. As each question is coupled with both
textual answers and SQL queries, Open-WikiTable opens up a wide range of
possibilities for future research, as both reader and parser methods can be
applied. The dataset and code are publicly available.",None,-1
c61cf47d-4c3f-4872-8072-a86c934f107f,Amodal Intra-class Instance Segmentation: Synthetic Datasets and Benchmark,0.542788,1,"Images of realistic scenes often contain intra-class objects that are heavily
occluded from each other, making the amodal perception task that requires
parsing the occluded parts of the objects challenging. Although important for
downstream tasks such as robotic grasping systems, the lack of large-scale
amodal datasets with detailed annotations makes it difficult to model
intra-class occlusions explicitly. This paper introduces two new amodal
datasets for image amodal completion tasks, which contain a total of over 267K
images of intra-class occlusion scenarios, annotated with multiple masks,
amodal bounding boxes, dual order relations and full appearance for instances
and background. We also present a point-supervised scheme with layer priors for
amodal instance segmentation specifically designed for intra-class occlusion
scenarios. Experiments show that our weakly supervised approach outperforms the
SOTA fully supervised methods, while our layer priors design exhibits
remarkable performance improvements in the case of intra-class occlusion in
both synthetic and real images.",None,-1
2ed45341-7203-47d9-9323-66dba52cac6f,A Probabilistic Rotation Representation for Symmetric Shapes With an Efficiently Computable Bingham Loss Function,0.145137,1,"In recent years, a deep learning framework has been widely used for object
pose estimation. While quaternion is a common choice for rotation
representation, it cannot represent the ambiguity of the observation. In order
to handle the ambiguity, the Bingham distribution is one promising solution.
However, it requires complicated calculation when yielding the negative
log-likelihood (NLL) loss. An alternative easy-to-implement loss function has
been proposed to avoid complex computations but has difficulty expressing
symmetric distribution. In this paper, we introduce a fast-computable and
easy-to-implement NLL loss function for Bingham distribution. We also create
the inference network and show that our loss function can capture the symmetric
property of target objects from their point clouds.",None,-1
bc00821c-9f61-49f8-9908-7ef0fe6207bc,No Compromise in Solution Quality: Speeding Up Belief-dependent Continuous POMDPs via Adaptive Multilevel Simplification,0.208141,2,"Continuous POMDPs with general belief-dependent rewards are notoriously
difficult to solve online. In this paper, we present a complete provable theory
of adaptive multilevel simplification for the setting of a given externally
constructed belief tree and MCTS that constructs the belief tree on the fly
using an exploration technique. Our theory allows to accelerate POMDP planning
with belief-dependent rewards without any sacrifice in the quality of the
obtained solution. We rigorously prove each theoretical claim in the proposed
unified theory. Using the general theoretical results, we present three
algorithms to accelerate continuous POMDP online planning with belief-dependent
rewards. Our two algorithms, SITH-BSP and LAZY-SITH-BSP, can be utilized on top
of any method that constructs a belief tree externally. The third algorithm,
SITH-PFT, is an anytime MCTS method that permits to plug-in any exploration
technique. All our methods are guaranteed to return exactly the same optimal
action as their unsimplified equivalents. We replace the costly computation of
information-theoretic rewards with novel adaptive upper and lower bounds which
we derive in this paper, and are of independent interest. We show that they are
easy to calculate and can be tightened by the demand of our algorithms. Our
approach is general; namely, any bounds that monotonically converge to the
reward can be utilized to achieve significant speedup without any loss in
performance. Our theory and algorithms support the challenging setting of
continuous states, actions, and observations. The beliefs can be parametric or
general and represented by weighted particles. We demonstrate in simulation a
significant speedup in planning compared to baseline approaches with guaranteed
identical performance.",None,-1
6427d9db-6a1b-4699-b150-1f49a375cdcf,Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,0.80379,3,"Intracerebral hemorrhage (ICH) is the second most common and deadliest form
of stroke. Despite medical advances, predicting treat ment outcomes for ICH
remains a challenge. This paper proposes a novel prognostic model that utilizes
both imaging and tabular data to predict treatment outcome for ICH. Our model
is trained on observational data collected from non-randomized controlled
trials, providing reliable predictions of treatment success. Specifically, we
propose to employ a variational autoencoder model to generate a low-dimensional
prognostic score, which can effectively address the selection bias resulting
from the non-randomized controlled trials. Importantly, we develop a
variational distributions combination module that combines the information from
imaging data, non-imaging clinical data, and treatment assignment to accurately
generate the prognostic score. We conducted extensive experiments on a
real-world clinical dataset of intracerebral hemorrhage. Our proposed method
demonstrates a substantial improvement in treatment outcome prediction compared
to existing state-of-the-art approaches. Code is available at
https://github.com/med-air/TOP-GPM",None,-1
e295caf9-2f38-481e-b650-b04c774889b4,Logarithm-transform aided Gaussian Sampling for Few-Shot Learning,0.0855107,1,"Few-shot image classification has recently witnessed the rise of
representation learning being utilised for models to adapt to new classes using
only a few training examples. Therefore, the properties of the representations,
such as their underlying probability distributions, assume vital importance.
Representations sampled from Gaussian distributions have been used in recent
works, [19] to train classifiers for few-shot classification. These methods
rely on transforming the distributions of experimental data to approximate
Gaussian distributions for their functioning. In this paper, I propose a novel
Gaussian transform, that outperforms existing methods on transforming
experimental data into Gaussian-like distributions. I then utilise this novel
transformation for few-shot image classification and show significant gains in
performance, while sampling lesser data.",None,-1
5e1be299-0d97-4aa2-9736-3d1b1aca6d7c,Full Parameter Fine-tuning for Large Language Models with Limited Resources,0.75408,53,"Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but demand massive GPU resources for training. Lowering the threshold for
LLMs training would encourage greater participation from researchers,
benefiting both academia and society. While existing approaches have focused on
parameter-efficient fine-tuning, which tunes or adds a small number of
parameters, few have addressed the challenge of tuning the full parameters of
LLMs with limited resources. In this work, we propose a new optimizer,
LOw-Memory Optimization (LOMO), which fuses the gradient computation and the
parameter update in one step to reduce memory usage. By integrating LOMO with
existing memory saving techniques, we reduce memory usage to 10.8% compared to
the standard approach (DeepSpeed solution). Consequently, our approach enables
the full parameter fine-tuning of a 65B model on a single machine with 8 RTX
3090, each with 24GB memory.Code and data are available at
https://github.com/OpenLMLab/LOMO.",None,-1
f09e2bbe-7964-424f-9d19-69456968b634,SA-BEV: Generating Semantic-Aware Bird's-Eye-View Feature for Multi-view 3D Object Detection,0.820325,11,"Recently, the pure camera-based Bird's-Eye-View (BEV) perception provides a
feasible solution for economical autonomous driving. However, the existing
BEV-based multi-view 3D detectors generally transform all image features into
BEV features, without considering the problem that the large proportion of
background information may submerge the object information. In this paper, we
propose Semantic-Aware BEV Pooling (SA-BEVPool), which can filter out
background information according to the semantic segmentation of image features
and transform image features into semantic-aware BEV features. Accordingly, we
propose BEV-Paste, an effective data augmentation strategy that closely matches
with semantic-aware BEV feature. In addition, we design a Multi-Scale
Cross-Task (MSCT) head, which combines task-specific and cross-task information
to predict depth distribution and semantic segmentation more accurately,
further improving the quality of semantic-aware BEV feature. Finally, we
integrate the above modules into a novel multi-view 3D object detection
framework, namely SA-BEV. Experiments on nuScenes show that SA-BEV achieves
state-of-the-art performance. Code has been available at
https://github.com/mengtan00/SA-BEV.git.",None,-1
c3dac146-66c7-48b8-86cd-a117d79b4b23,Generative Prompt Model for Weakly Supervised Object Localization,0.992687,18,"Weakly supervised object localization (WSOL) remains challenging when
learning object localization models from image category labels. Conventional
methods that discriminatively train activation models ignore representative yet
less discriminative object parts. In this study, we propose a generative prompt
model (GenPromp), defining the first generative pipeline to localize less
discriminative object parts by formulating WSOL as a conditional image
denoising procedure. During training, GenPromp converts image category labels
to learnable prompt embeddings which are fed to a generative model to
conditionally recover the input image with noise and learn representative
embeddings. During inference, enPromp combines the representative embeddings
with discriminative embeddings (queried from an off-the-shelf vision-language
model) for both representative and discriminative capacity. The combined
embeddings are finally used to generate multi-scale high-quality attention
maps, which facilitate localizing full object extent. Experiments on
CUB-200-2011 and ILSVRC show that GenPromp respectively outperforms the best
discriminative models by 5.2% and 5.6% (Top-1 Loc), setting a solid baseline
for WSOL with the generative model. Code is available at
https://github.com/callsys/GenPromp.",None,-1
db32f365-6b7f-45ba-93b0-aab94912525c,Knowledge-grounded Natural Language Recommendation Explanation,0.238709,2,"Explanations accompanied by a recommendation can assist users in
understanding the decision made by recommendation systems, which in turn
increases a user's confidence and trust in the system. Recently, research has
focused on generating natural language explanations in a human-readable format.
Thus far, the proposed approaches leverage item reviews written by users, which
are often subjective, sparse in language, and unable to account for new items
that have not been purchased or reviewed before. Instead, we aim to generate
fact-grounded recommendation explanations that are objectively described with
item features while implicitly considering a user's preferences, based on the
user's purchase history. To achieve this, we propose a knowledge graph (KG)
approach to natural language explainable recommendation. Our approach draws on
user-item features through a novel collaborative filtering-based KG
representation to produce fact-grounded, personalized explanations, while
jointly learning user-item representations for recommendation scoring.
Experimental results show that our approach consistently outperforms previous
state-of-the-art models on natural language explainable recommendation.",None,-1
1906acd9-98ce-4e6d-a684-c05a95ce2743,Generalisation Through Negation and Predicate Invention,0.411443,2,"The ability to generalise from a small number of examples is a fundamental
challenge in machine learning. To tackle this challenge, we introduce an
inductive logic programming (ILP) approach that combines negation and predicate
invention. Combining these two features allows an ILP system to generalise
better by learning rules with universally quantified body-only variables. We
implement our idea in NOPI, which can learn normal logic programs with
predicate invention, including Datalog programs with stratified negation. Our
experimental results on multiple domains show that our approach can improve
predictive accuracies and learning times.",None,-1
373f8b4e-f315-4e56-822e-882ef0bfa04b,Speech-based Slot Filling using Large Language Models,0.353223,1,"Recently, advancements in large language models (LLMs) have shown an
unprecedented ability across various language tasks. This paper investigates
the potential application of LLMs to slot filling with noisy ASR
transcriptions, via both in-context learning and task-specific fine-tuning.
Dedicated prompt designs and fine-tuning approaches are proposed to improve the
robustness of LLMs for slot filling with noisy ASR transcriptions. Moreover, a
linearised knowledge injection (LKI) scheme is also proposed to integrate
dynamic external knowledge into LLMs. Experiments were performed on SLURP to
quantify the performance of LLMs, including GPT-3.5-turbo, GPT-4, LLaMA-13B and
Vicuna-13B (v1.1 and v1.5) with different ASR error rates. The use of the
proposed fine-tuning together with the LKI scheme for LLaMA-13B achieved an
8.3% absolute SLU-F1 improvement compared to the strong Flan-T5-base baseline
system on a limited data setup.",None,-1
408d331f-aaf3-4bec-bd1f-6a45468c89ed,Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach,0.863341,9,"The remarkable progress in Large Language Models (LLMs) opens up new avenues
for addressing planning and decision-making problems in Multi-Agent Systems
(MAS). However, as the number of agents increases, the issues of hallucination
in LLMs and coordination in MAS have become increasingly prominent.
Additionally, the efficient utilization of tokens emerges as a critical
consideration when employing LLMs to facilitate the interactions among a
substantial number of agents. In this paper, we develop a modular framework
called LLaMAC to mitigate these challenges. LLaMAC implements a value
distribution encoding similar to that found in the human brain, utilizing
internal and external feedback mechanisms to facilitate collaboration and
iterative reasoning among its modules. Through evaluations involving system
resource allocation and robot grid transportation, we demonstrate the
considerable advantages afforded by our proposed approach.",None,-1
50721d16-87ae-4a06-aeaa-868b1f80ae8f,Improving Interpersonal Communication by Simulating Audiences with Language Models,0.986421,5,"How do we communicate with others to achieve our goals? We use our prior
experience or advice from others, or construct a candidate utterance by
predicting how it will be received. However, our experiences are limited and
biased, and reasoning about potential outcomes can be difficult and cognitively
challenging. In this paper, we explore how we can leverage Large Language Model
(LLM) simulations to help us communicate better. We propose the
Explore-Generate-Simulate (EGS) framework, which takes as input any scenario
where an individual is communicating to an audience with a goal they want to
achieve. EGS (1) explores the solution space by producing a diverse set of
advice relevant to the scenario, (2) generates communication candidates
conditioned on subsets of the advice, and (3) simulates the reactions from
various audiences to determine both the best candidate and advice to use. We
evaluate the framework on eight scenarios spanning the ten fundamental
processes of interpersonal communication. For each scenario, we collect a
dataset of human evaluations across candidates and baselines, and showcase that
our framework's chosen candidate is preferred over popular generation
mechanisms including Chain-of-Thought. We also find that audience simulations
achieve reasonably high agreement with human raters across 5 of the 8
scenarios. Finally, we demonstrate the generality of our framework by applying
it to real-world scenarios described by users on web forums. Through
evaluations and demonstrations, we show that EGS enhances the effectiveness and
outcomes of goal-oriented communication across a variety of situations, thus
opening up new possibilities for the application of large language models in
revolutionizing communication and decision-making processes.",None,-1
c8d37e8e-bac8-4f77-af04-92467afd177f,Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling,0.937721,14,"As the size of pre-trained speech recognition models increases, running these
large models in low-latency or resource-constrained environments becomes
challenging. In this work, we leverage pseudo-labelling to assemble a
large-scale open-source dataset which we use to distill the Whisper model into
a smaller variant, called Distil-Whisper. Using a simple word error rate (WER)
heuristic, we select only the highest quality pseudo-labels for training. The
distilled model is 5.8 times faster with 51% fewer parameters, while performing
to within 1% WER on out-of-distribution test data in a zero-shot transfer
setting. Distil-Whisper maintains the robustness of the Whisper model to
difficult acoustic conditions, while being less prone to hallucination errors
on long-form audio. Distil-Whisper is designed to be paired with Whisper for
speculative decoding, yielding a 2 times speed-up while mathematically ensuring
the same outputs as the original model. To facilitate further research in this
domain, we make our training code, inference code and models publicly
accessible.",None,-1
0ba6d750-4b77-4b9f-965d-ab64fbd9abe6,GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions,0.812818,8,"There is growing interest in systems that generate captions for scientific
figures. However, assessing these systems output poses a significant challenge.
Human evaluation requires academic expertise and is costly, while automatic
evaluation depends on often low-quality author-written captions. This paper
investigates using large language models (LLMs) as a cost-effective,
reference-free method for evaluating figure captions. We first constructed
SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600
scientific figure captions, both original and machine-made, for 600 arXiv
figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption
based on its potential to aid reader understanding, given relevant context such
as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot
evaluator, outperformed all other models and even surpassed assessments made by
Computer Science and Informatics undergraduates, achieving a Kendall
correlation score of 0.401 with Ph.D. students rankings",None,-1
1bc59dfe-51ef-4f3c-8e8f-ab6f2de7896f,Danish Foundation Models,0.0684772,2,"Large language models, sometimes referred to as foundation models, have
transformed multiple fields of research. However, smaller languages risk
falling behind due to high training costs and small incentives for large
companies to train these models. To combat this, the Danish Foundation Models
project seeks to provide and maintain open, well-documented, and high-quality
foundation models for the Danish language. This is achieved through broad
cooperation with public and private institutions, to ensure high data quality
and applicability of the trained models. We present the motivation of the
project, the current status, and future perspectives.",None,-1
3306f17b-daf9-404b-a176-6d1ad67458ca,InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling,0.898925,10,"Cross-lingual topic models have been prevalent for cross-lingual text
analysis by revealing aligned latent topics. However, most existing methods
suffer from producing repetitive topics that hinder further analysis and
performance decline caused by low-coverage dictionaries. In this paper, we
propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).
Instead of the direct alignment in previous work, we propose a topic alignment
with mutual information method. This works as a regularization to properly
align topics and prevent degenerate topic representations of words, which
mitigates the repetitive topic issue. To address the low-coverage dictionary
issue, we further propose a cross-lingual vocabulary linking method that finds
more linked cross-lingual words for topic alignment beyond the translations of
a given dictionary. Extensive experiments on English, Chinese, and Japanese
datasets demonstrate that our method outperforms state-of-the-art baselines,
producing more coherent, diverse, and well-aligned topics and showing better
transferability for cross-lingual classification tasks.",None,-1
2048b443-a5cc-489c-9dc5-2ed1aa035b14,One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data,0.527608,7,"There is a recent growing interest in applying Deep Learning techniques to
tabular data, in order to replicate the success of other Artificial
Intelligence areas in this structured domain. Specifically interesting is the
case in which tabular data have a time dependence, such as, for instance
financial transactions. However, the heterogeneity of the tabular values, in
which categorical elements are mixed with numerical items, makes this
adaptation difficult. In this paper we propose a Transformer architecture to
represent heterogeneous time-dependent tabular data, in which numerical
features are represented using a set of frequency functions and the whole
network is uniformly trained with a unique loss function.",None,-1
ce3a0e18-803b-4a75-bd54-4cb2f8c8ae57,When SAM Meets Sonar Images,0.405191,8,"Segment Anything Model (SAM) has revolutionized the way of segmentation.
However, SAM's performance may decline when applied to tasks involving domains
that differ from natural images. Nonetheless, by employing fine-tuning
techniques, SAM exhibits promising capabilities in specific domains, such as
medicine and planetary science. Notably, there is a lack of research on the
application of SAM to sonar imaging. In this paper, we aim to address this gap
by conducting a comprehensive investigation of SAM's performance on sonar
images. Specifically, we evaluate SAM using various settings on sonar images.
Additionally, we fine-tune SAM using effective methods both with prompts and
for semantic segmentation, thereby expanding its applicability to tasks
requiring automated segmentation. Experimental results demonstrate a
significant improvement in the performance of the fine-tuned SAM.",None,-1
7f4cc4fa-a305-486e-be2d-b4d1cd8d5cd9,Non-autoregressive Machine Translation with Probabilistic Context-free Grammar,0.420422,1,"Non-autoregressive Transformer(NAT) significantly accelerates the inference
of neural machine translation. However, conventional NAT models suffer from
limited expression power and performance degradation compared to autoregressive
(AT) models due to the assumption of conditional independence among target
tokens. To address these limitations, we propose a novel approach called
PCFG-NAT, which leverages a specially designed Probabilistic Context-Free
Grammar (PCFG) to enhance the ability of NAT models to capture complex
dependencies among output tokens. Experimental results on major machine
translation benchmarks demonstrate that PCFG-NAT further narrows the gap in
translation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a
deeper understanding of the generated sentences, addressing the lack of
satisfactory explainability in neural machine translation.Code is publicly
available at https://github.com/ictnlp/PCFG-NAT.",None,-1
45dad6c7-cf35-4fd4-8855-68a72e925204,A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI),0.808963,8,"Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.",None,-1
300aab74-9b60-4cc7-aa10-a670d569a226,Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs,0.438003,6,"Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency
multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic.
However, MLPs rely exclusively on the node features and fail to capture the
graph structural information. Previous methods address this issue by processing
graph edges into extra inputs for MLPs, but such graph structures may be
unavailable for various scenarios. To this end, we propose a Prototype-Guided
Knowledge Distillation~(PGKD) method, which does not require graph
edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the
graph structural information in GNN teachers, and distill such information from
GNNs to MLPs via prototypes in an edge-free setting. Experimental results on
popular graph benchmarks demonstrate the effectiveness and robustness of the
proposed PGKD.",None,-1
7a160d23-ecec-4417-9fe2-4de0f8fdb54f,MDP: A Generalized Framework for Text-Guided Image Editing by Manipulating the Diffusion Path,0.362531,11,"Image generation using diffusion can be controlled in multiple ways. In this
paper, we systematically analyze the equations of modern generative diffusion
networks to propose a framework, called MDP, that explains the design space of
suitable manipulations. We identify 5 different manipulations, including
intermediate latent, conditional embedding, cross attention maps, guidance, and
predicted noise. We analyze the corresponding parameters of these manipulations
and the manipulation schedule. We show that some previous editing methods fit
nicely into our framework. Particularly, we identified one specific
configuration as a new type of control by manipulating the predicted noise,
which can perform higher-quality edits than previous work for a variety of
local and global edits.",None,-1
ccef4170-a06a-4089-8bab-02e32a8194db,Making Metadata More FAIR Using Large Language Models,0.280008,1,"With the global increase in experimental data artifacts, harnessing them in a
unified fashion leads to a major stumbling block - bad metadata. To bridge this
gap, this work presents a Natural Language Processing (NLP) informed
application, called FAIRMetaText, that compares metadata. Specifically,
FAIRMetaText analyzes the natural language descriptions of metadata and
provides a mathematical similarity measure between two terms. This measure can
then be utilized for analyzing varied metadata, by suggesting terms for
compliance or grouping similar terms for identification of replaceable terms.
The efficacy of the algorithm is presented qualitatively and quantitatively on
publicly available research artifacts and demonstrates large gains across
metadata related tasks through an in-depth study of a wide variety of Large
Language Models (LLMs). This software can drastically reduce the human effort
in sifting through various natural language metadata while employing several
experimental datasets on the same topic.",None,-1
e1951d0d-4ccb-4760-a5d2-2fd29c2869be,Small but Mighty: Enhancing 3D Point Clouds Semantic Segmentation with U-Next Framework,0.670807,6,"We study the problem of semantic segmentation of large-scale 3D point clouds.
In recent years, significant research efforts have been directed toward local
feature aggregation, improved loss functions and sampling strategies. While the
fundamental framework of point cloud semantic segmentation has been largely
overlooked, with most existing approaches rely on the U-Net architecture by
default. In this paper, we propose U-Next, a small but mighty framework
designed for point cloud semantic segmentation. The key to this framework is to
learn multi-scale hierarchical representations from semantically similar
feature maps. Specifically, we build our U-Next by stacking multiple U-Net
$L^1$ codecs in a nested and densely arranged manner to minimize the semantic
gap, while simultaneously fusing the feature maps across scales to effectively
recover the fine-grained details. We also devised a multi-level deep
supervision mechanism to further smooth gradient propagation and facilitate
network optimization. Extensive experiments conducted on three large-scale
benchmarks including S3DIS, Toronto3D, and SensatUrban demonstrate the
superiority and the effectiveness of the proposed U-Next architecture. Our
U-Next architecture shows consistent and visible performance improvements
across different tasks and baseline models, indicating its great potential to
serve as a general framework for future research.",None,-1
a2c94a0e-e01b-48c5-b371-67a3a2e3c3b6,GridMM: Grid Memory Map for Vision-and-Language Navigation,0.784122,12,"Vision-and-language navigation (VLN) enables the agent to navigate to a
remote location following the natural language instruction in 3D environments.
To represent the previously visited environment, most approaches for VLN
implement memory using recurrent states, topological maps, or top-down semantic
maps. In contrast to these approaches, we build the top-down egocentric and
dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited
environment. From a global perspective, historical observations are projected
into a unified grid map in a top-down view, which can better represent the
spatial relations of the environment. From a local perspective, we further
propose an instruction relevance aggregation method to capture fine-grained
visual clues in each grid region. Extensive experiments are conducted on both
the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE
dataset in the continuous environments, showing the superiority of our proposed
method.",None,-1
1a352dad-3f5e-48ed-af85-902e260ce4b6,MagicEdit: High-Fidelity and Temporally Coherent Video Editing,0.810554,34,"In this report, we present MagicEdit, a surprisingly simple yet effective
solution to the text-guided video editing task. We found that high-fidelity and
temporally coherent video-to-video translation can be achieved by explicitly
disentangling the learning of content, structure and motion signals during
training. This is in contradict to most existing methods which attempt to
jointly model both the appearance and temporal representation within a single
framework, which we argue, would lead to degradation in per-frame quality.
Despite its simplicity, we show that MagicEdit supports various downstream
video editing tasks, including video stylization, local editing, video-MagicMix
and video outpainting.",None,-1
105c463f-5be5-42e5-8edd-a67dd45c6da9,ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation,0.851785,42,"The ability to accurately locate and navigate to a specific object is a
crucial capability for embodied agents that operate in the real world and
interact with objects to complete tasks. Such object navigation tasks usually
require large-scale training in visual environments with labeled objects, which
generalizes poorly to novel objects in unknown environments. In this work, we
present a novel zero-shot object navigation method, Exploration with Soft
Commonsense constraints (ESC), that transfers commonsense knowledge in
pre-trained models to open-world object navigation without any navigation
experience nor any other training on the visual environments. First, ESC
leverages a pre-trained vision and language model for open-world prompt-based
grounding and a pre-trained commonsense language model for room and object
reasoning. Then ESC converts commonsense knowledge into navigation actions by
modeling it as soft logic predicates for efficient exploration. Extensive
experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method
improves significantly over baselines, and achieves new state-of-the-art
results for zero-shot object navigation (e.g., 288% relative Success Rate
improvement than CoW on MP3D).",None,-1
da8cf9cb-b07a-4009-9845-0510c54a24c5,RILS: Masked Visual Reconstruction in Language Semantic Space,0.138554,6,"Both masked image modeling (MIM) and natural language supervision have
facilitated the progress of transferable visual pre-training. In this work, we
seek the synergy between two paradigms and study the emerging properties when
MIM meets natural language supervision. To this end, we present a novel masked
visual Reconstruction In Language semantic Space (RILS) pre-training framework,
in which sentence representations, encoded by the text encoder, serve as
prototypes to transform the vision-only signals into patch-sentence
probabilities as semantically meaningful MIM reconstruction targets. The vision
models can therefore capture useful components with structured information by
predicting proper semantic of masked tokens. Better visual representations
could, in turn, improve the text encoder via the image-text alignment
objective, which is essential for the effective MIM target transformation.
Extensive experimental results demonstrate that our method not only enjoys the
best of previous MIM and CLIP but also achieves further improvements on various
tasks due to their mutual benefits. RILS exhibits advanced transferability on
downstream classification, detection, and segmentation, especially for low-shot
regimes. Code will be made available at https://github.com/hustvl/RILS.",None,-1
17b9e2e3-b5a0-42de-ad2f-a5f587fa2fbd,MegaWika: Millions of reports and their sources across 50 diverse languages,0.277874,3,"To foster the development of new models for collaborative AI-assisted report
generation, we introduce MegaWika, consisting of 13 million Wikipedia articles
in 50 diverse languages, along with their 71 million referenced source
materials. We process this dataset for a myriad of applications, going beyond
the initial Wikipedia citation extraction and web scraping of content,
including translating non-English articles for cross-lingual applications and
providing FrameNet parses for automated semantic analysis. MegaWika is the
largest resource for sentence-level report generation and the only report
generation dataset that is multilingual. We manually analyze the quality of
this resource through a semantically stratified sample. Finally, we provide
baseline results and trained models for crucial steps in automated report
generation: cross-lingual question answering and citation retrieval.",None,-1
5c0c9f4f-ad41-4473-b427-03050ea2d618,Blind Video Deflickering by Neural Filtering with a Flawed Atlas,0.379799,16,"Many videos contain flickering artifacts. Common causes of flicker include
video processing algorithms, video generation algorithms, and capturing videos
under specific situations. Prior work usually requires specific guidance such
as the flickering frequency, manual annotations, or extra consistent videos to
remove the flicker. In this work, we propose a general flicker removal
framework that only receives a single flickering video as input without
additional guidance. Since it is blind to a specific flickering type or
guidance, we name this ""blind deflickering."" The core of our approach is
utilizing the neural atlas in cooperation with a neural filtering strategy. The
neural atlas is a unified representation for all frames in a video that
provides temporal consistency guidance but is flawed in many cases. To this
end, a neural network is trained to mimic a filter to learn the consistent
features (e.g., color, brightness) and avoid introducing the artifacts in the
atlas. To validate our method, we construct a dataset that contains diverse
real-world flickering videos. Extensive experiments show that our method
achieves satisfying deflickering performance and even outperforms baselines
that use extra guidance on a public benchmark.",None,-1
f5bdbbc6-ea1e-4f23-b912-e446a9a248ff,Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models,0.149288,2,"The dynamic nature of knowledge in an ever-changing world presents challenges
for language models trained on static data; the model in the real world often
requires not only acquiring new knowledge but also overwriting outdated
information into updated ones. To study the ability of language models for
these time-dependent dynamics in human language, we introduce a novel task,
EvolvingQA, a temporally evolving question-answering benchmark designed for
training and evaluating LMs on an evolving Wikipedia database. The construction
of EvolvingQA is automated with our pipeline using large language models. We
uncover that existing continual learning baselines suffer from updating and
removing outdated knowledge. Our analysis suggests that models fail to rectify
knowledge due to small weight gradients. In addition, we elucidate that
language models particularly struggle to reflect the change of numerical or
temporal information. Our work aims to model the dynamic nature of real-world
information, suggesting faithful evaluations of the evolution-adaptability of
language models.",None,-1
123ae158-a17c-498f-8433-6ce30a7cfbc7,Efficient and Accurate Co-Visible Region Localization with Matching Key-Points Crop (MKPC): A Two-Stage Pipeline for Enhancing Image Matching Performance,0.102954,1,"Image matching is a classic and fundamental task in computer vision. In this
paper, under the hypothesis that the areas outside the co-visible regions carry
little information, we propose a matching key-points crop (MKPC) algorithm. The
MKPC locates, proposes and crops the critical regions, which are the co-visible
areas with great efficiency and accuracy. Furthermore, building upon MKPC, we
propose a general two-stage pipeline for image matching, which is compatible to
any image matching models or combinations. We experimented with plugging
SuperPoint + SuperGlue into the two-stage pipeline, whose results show that our
method enhances the performance for outdoor pose estimations. What's more, in a
fair comparative condition, our method outperforms the SOTA on Image Matching
Challenge 2022 Benchmark, which represents the hardest outdoor benchmark of
image matching currently.",None,-1
2523a02e-126f-4dfe-9fcc-71123f6f51f3,Mixture Encoder for Joint Speech Separation and Recognition,0.38205,2,"Multi-speaker automatic speech recognition (ASR) is crucial for many
real-world applications, but it requires dedicated modeling techniques.
Existing approaches can be divided into modular and end-to-end methods. Modular
approaches separate speakers and recognize each of them with a single-speaker
ASR system. End-to-end models process overlapped speech directly in a single,
powerful neural network. This work proposes a middle-ground approach that
leverages explicit speech separation similarly to the modular approach but also
incorporates mixture speech information directly into the ASR module in order
to mitigate the propagation of errors made by the speech separator. We also
explore a way to exchange cross-speaker context information through a layer
that combines information of the individual speakers. Our system is optimized
through separate and joint training stages and achieves a relative improvement
of 7% in word error rate over a purely modular setup on the SMS-WSJ task.",None,-1
55f9ff49-13d3-4074-9fc8-89df4f4b4e75,Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data,0.424263,8,"Chatbots have become popular in educational settings, revolutionizing how
students interact with material and how teachers teach. We present
Curriculum-Driven EduBot, a framework for developing a chatbot that combines
the interactive features of chatbots with the systematic material of English
textbooks to assist students in enhancing their conversational skills. We begin
by extracting pertinent topics from textbooks and then using large language
models to generate dialogues related to these topics. We then fine-tune an
open-source LLM using our generated conversational data to create our
curriculum-driven chatbot. User studies demonstrate that our chatbot
outperforms ChatGPT in leading curriculum-based dialogues and adapting its
dialogue to match the user's English proficiency level. By combining
traditional textbook methodologies with conversational AI, our approach offers
learners an interactive tool that aligns with their curriculum and provides
user-tailored conversation practice. This facilitates meaningful student-bot
dialogues and enriches the overall learning experience within the curriculum's
pedagogical framework.",None,-1
437b4bb2-a15a-432a-b0f2-b5f7141d801d,SGAligner : 3D Scene Alignment with Scene Graphs,0.580902,9,"Building 3D scene graphs has recently emerged as a topic in scene
representation for several embodied AI applications to represent the world in a
structured and rich manner. With their increased use in solving downstream
tasks (eg, navigation and room rearrangement), can we leverage and recycle them
for creating 3D maps of environments, a pivotal step in agent operation? We
focus on the fundamental problem of aligning pairs of 3D scene graphs whose
overlap can range from zero to partial and can contain arbitrary changes. We
propose SGAligner, the first method for aligning pairs of 3D scene graphs that
is robust to in-the-wild scenarios (ie, unknown overlap -- if any -- and
changes in the environment). We get inspired by multi-modality knowledge graphs
and use contrastive learning to learn a joint, multi-modal embedding space. We
evaluate on the 3RScan dataset and further showcase that our method can be used
for estimating the transformation between pairs of 3D scenes. Since benchmarks
for these tasks are missing, we create them on this dataset. The code,
benchmark, and trained models are available on the project website.",None,-1
4756b0af-6a8b-4ac3-bb04-15402fbc4504,Enabling AI-Generated Content (AIGC) Services in Wireless Edge Networks,0.83648,49,"Artificial Intelligence-Generated Content (AIGC) refers to the use of AI to
automate the information creation process while fulfilling the personalized
requirements of users. However, due to the instability of AIGC models, e.g.,
the stochastic nature of diffusion models, the quality and accuracy of the
generated content can vary significantly. In wireless edge networks, the
transmission of incorrectly generated content may unnecessarily consume network
resources. Thus, a dynamic AIGC service provider (ASP) selection scheme is
required to enable users to connect to the most suited ASP, improving the
users' satisfaction and quality of generated content. In this article, we first
review the AIGC techniques and their applications in wireless networks. We then
present the AIGC-as-a-service (AaaS) concept and discuss the challenges in
deploying AaaS at the edge networks. Yet, it is essential to have performance
metrics to evaluate the accuracy of AIGC services. Thus, we introduce several
image-based perceived quality evaluation metrics. Then, we propose a general
and effective model to illustrate the relationship between computational
resources and user-perceived quality evaluation metrics. To achieve efficient
AaaS and maximize the quality of generated content in wireless edge networks,
we propose a deep reinforcement learning-enabled algorithm for optimal ASP
selection. Simulation results show that the proposed algorithm can provide a
higher quality of generated content to users and achieve fewer crashed tasks by
comparing with four benchmarks, i.e., overloading-avoidance, random,
round-robin policies, and the upper-bound schemes.",None,-1
06377f8b-a474-4e06-b9b7-b411f6d8d71a,Local Implicit Normalizing Flow for Arbitrary-Scale Image Super-Resolution,0.656719,11,"Flow-based methods have demonstrated promising results in addressing the
ill-posed nature of super-resolution (SR) by learning the distribution of
high-resolution (HR) images with the normalizing flow. However, these methods
can only perform a predefined fixed-scale SR, limiting their potential in
real-world applications. Meanwhile, arbitrary-scale SR has gained more
attention and achieved great progress. Nonetheless, previous arbitrary-scale SR
methods ignore the ill-posed problem and train the model with per-pixel L1
loss, leading to blurry SR outputs. In this work, we propose ""Local Implicit
Normalizing Flow"" (LINF) as a unified solution to the above problems. LINF
models the distribution of texture details under different scaling factors with
normalizing flow. Thus, LINF can generate photo-realistic HR images with rich
texture details in arbitrary scale factors. We evaluate LINF with extensive
experiments and show that LINF achieves the state-of-the-art perceptual quality
compared with prior arbitrary-scale SR methods.",None,-1
245c269b-998a-48b1-b2b9-39aa2a682543,GeoNet: Benchmarking Unsupervised Adaptation across Geographies,0.492142,10,"In recent years, several efforts have been aimed at improving the robustness
of vision models to domains and environments unseen during training. An
important practical problem pertains to models deployed in a new geography that
is under-represented in the training dataset, posing a direct challenge to fair
and inclusive computer vision. In this paper, we study the problem of
geographic robustness and make three main contributions. First, we introduce a
large-scale dataset GeoNet for geographic adaptation containing benchmarks
across diverse tasks like scene recognition (GeoPlaces), image classification
(GeoImNet) and universal adaptation (GeoUniDA). Second, we investigate the
nature of distribution shifts typical to the problem of geographic adaptation
and hypothesize that the major source of domain shifts arise from significant
variations in scene context (context shift), object design (design shift) and
label distribution (prior shift) across geographies. Third, we conduct an
extensive evaluation of several state-of-the-art unsupervised domain adaptation
algorithms and architectures on GeoNet, showing that they do not suffice for
geographical adaptation, and that large-scale pre-training using large vision
models also does not lead to geographic robustness. Our dataset is publicly
available at https://tarun005.github.io/GeoNet.",None,-1
033fe709-1ba5-4a13-b2ac-e3c5dd28f680,PanelNet: Understanding 360 Indoor Environment via Panel Representation,0.756741,5,"Indoor 360 panoramas have two essential properties. (1) The panoramas are
continuous and seamless in the horizontal direction. (2) Gravity plays an
important role in indoor environment design. By leveraging these properties, we
present PanelNet, a framework that understands indoor environments using a
novel panel representation of 360 images. We represent an equirectangular
projection (ERP) as consecutive vertical panels with corresponding 3D panel
geometry. To reduce the negative impact of panoramic distortion, we incorporate
a panel geometry embedding network that encodes both the local and global
geometric features of a panel. To capture the geometric context in room design,
we introduce Local2Global Transformer, which aggregates local information
within a panel and panel-wise global context. It greatly improves the model
performance with low training overhead. Our method outperforms existing methods
on indoor 360 depth estimation and shows competitive results against
state-of-the-art approaches on the task of indoor layout estimation and
semantic segmentation.",None,-1
a5198a90-83ea-4161-9783-236cd7e9ae34,From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models,0.949464,113,"Language models (LMs) are pretrained on diverse data sources, including news,
discussion forums, books, and online encyclopedias. A significant portion of
this data includes opinions and perspectives which, on one hand, celebrate
democracy and diversity of ideas, and on the other hand are inherently socially
biased. Our work develops new methods to (1) measure political biases in LMs
trained on such corpora, along social and economic axes, and (2) measure the
fairness of downstream NLP models trained on top of politically biased LMs. We
focus on hate speech and misinformation detection, aiming to empirically
quantify the effects of political (social, economic) biases in pretraining data
on the fairness of high-stakes social-oriented tasks. Our findings reveal that
pretrained LMs do have political leanings that reinforce the polarization
present in pretraining corpora, propagating social biases into hate speech
predictions and misinformation detectors. We discuss the implications of our
findings for NLP research and propose future directions to mitigate unfairness.",None,-1
7c222d4a-36b5-474f-a801-f9dc8031efce,How Well Do Text Embedding Models Understand Syntax?,0.102901,1,"Text embedding models have significantly contributed to advancements in
natural language processing by adeptly capturing semantic properties of textual
data. However, the ability of these models to generalize across a wide range of
syntactic contexts remains under-explored. In this paper, we first develop an
evaluation set, named \textbf{SR}, to scrutinize the capability for syntax
understanding of text embedding models from two crucial syntactic aspects:
Structural heuristics, and Relational understanding among concepts, as revealed
by the performance gaps in previous studies. Our findings reveal that existing
text embedding models have not sufficiently addressed these syntactic
understanding challenges, and such ineffectiveness becomes even more apparent
when evaluated against existing benchmark datasets. Furthermore, we conduct
rigorous analysis to unearth factors that lead to such limitations and examine
why previous evaluations fail to detect such ineffectiveness. Lastly, we
propose strategies to augment the generalization ability of text embedding
models in diverse syntactic scenarios. This study serves to highlight the
hurdles associated with syntactic generalization and provides pragmatic
guidance for boosting model performance across varied syntactic contexts.",None,-1
412790f0-d139-4c02-81d7-ca17b396a540,CoReFace: Sample-Guided Contrastive Regularization for Deep Face Recognition,0.588239,3,"The discriminability of feature representation is the key to open-set face
recognition. Previous methods rely on the learnable weights of the
classification layer that represent the identities. However, the evaluation
process learns no identity representation and drops the classifier from
training. This inconsistency could confuse the feature encoder in understanding
the evaluation goal and hinder the effect of identity-based methods. To
alleviate the above problem, we propose a novel approach namely Contrastive
Regularization for Face recognition (CoReFace) to apply image-level
regularization in feature representation learning. Specifically, we employ
sample-guided contrastive learning to regularize the training with the
image-image relationship directly, which is consistent with the evaluation
process. To integrate contrastive learning into face recognition, we augment
embeddings instead of images to avoid the image quality degradation. Then, we
propose a novel contrastive loss for the representation distribution by
incorporating an adaptive margin and a supervised contrastive mask to generate
steady loss values and avoid the collision with the classification supervision
signal. Finally, we discover and solve the semantically repetitive signal
problem in contrastive learning by exploring new pair coupling protocols.
Extensive experiments demonstrate the efficacy and efficiency of our CoReFace
which is highly competitive with the state-of-the-art approaches.",None,-1
de9197a2-50dc-47dd-95fb-3661838f3378,QAID: Question Answering Inspired Few-shot Intent Detection,0.486213,5,"Intent detection with semantically similar fine-grained intents is a
challenging task. To address it, we reformulate intent detection as a
question-answering retrieval task by treating utterances and intent names as
questions and answers. To that end, we utilize a question-answering retrieval
architecture and adopt a two stages training schema with batch contrastive
loss. In the pre-training stage, we improve query representations through
self-supervised training. Then, in the fine-tuning stage, we increase
contextualized token-level similarity scores between queries and answers from
the same intent. Our results on three few-shot intent detection benchmarks
achieve state-of-the-art performance.",None,-1
a2f065ce-2dfd-49cd-be51-1e37130d7f63,Ontology-aware Network for Zero-shot Sketch-based Image Retrieval,0.0495931,1,"Zero-Shot Sketch-Based Image Retrieval (ZSSBIR) is an emerging task. The
pioneering work focused on the modal gap but ignored inter-class information.
Although recent work has begun to consider the triplet-based or contrast-based
loss to mine inter-class information, positive and negative samples need to be
carefully selected, or the model is prone to lose modality-specific
information. To respond to these issues, an Ontology-Aware Network (OAN) is
proposed. Specifically, the smooth inter-class independence learning mechanism
is put forward to maintain inter-class peculiarity. Meanwhile,
distillation-based consistency preservation is utilized to keep
modality-specific information. Extensive experiments have demonstrated the
superior performance of our algorithm on two challenging Sketchy and Tu-Berlin
datasets.",None,-1
9b555769-aaf6-4456-8d72-43d5f6ec83d9,HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation,0.628845,18,"In this paper, we study Text-to-3D content generation leveraging 2D diffusion
priors to enhance the quality and detail of the generated 3D models. Recent
progress (Magic3D) in text-to-3D has shown that employing high-resolution
(e.g., 512 x 512) renderings can lead to the production of high-quality 3D
models using latent diffusion priors. To enable rendering at even higher
resolutions, which has the potential to further augment the quality and detail
of the models, we propose a novel approach that combines multiple noise
estimation processes with a pretrained 2D diffusion prior. Distinct from the
Bar-Tal et al.s' study which binds multiple denoised results to generate images
from texts, our approach integrates the computation of scoring distillation
losses such as SDS loss and VSD loss which are essential techniques for the 3D
content generation with 2D diffusion priors. We experimentally evaluated the
proposed approach. The results show that the proposed approach can generate
high-quality details compared to the baselines.",None,-1
3000f47a-cde5-4902-8f37-53356a8c4c55,Towards interpretable quantum machine learning via single-photon quantum walks,0.25549,2,"Variational quantum algorithms represent a promising approach to quantum
machine learning where classical neural networks are replaced by parametrized
quantum circuits. However, both approaches suffer from a clear limitation, that
is a lack of interpretability. Here, we present a variational method to
quantize projective simulation (PS), a reinforcement learning model aimed at
interpretable artificial intelligence. Decision making in PS is modeled as a
random walk on a graph describing the agent's memory. To implement the
quantized model, we consider quantum walks of single photons in a lattice of
tunable Mach-Zehnder interferometers trained via variational algorithms. Using
an example from transfer learning, we show that the quantized PS model can
exploit quantum interference to acquire capabilities beyond those of its
classical counterpart. Finally, we discuss the role of quantum interference for
training and tracing the decision making process, paving the way for
realizations of interpretable quantum learning agents.",None,-1
7b27cb6d-2815-4f75-b814-443849b749b3,Unbiased Scene Graph Generation in Videos,0.580101,12,"The task of dynamic scene graph generation (SGG) from videos is complicated
and challenging due to the inherent dynamics of a scene, temporal fluctuation
of model predictions, and the long-tailed distribution of the visual
relationships in addition to the already existing challenges in image-based
SGG. Existing methods for dynamic SGG have primarily focused on capturing
spatio-temporal context using complex architectures without addressing the
challenges mentioned above, especially the long-tailed distribution of
relationships. This often leads to the generation of biased scene graphs. To
address these challenges, we introduce a new framework called TEMPURA: TEmporal
consistency and Memory Prototype guided UnceRtainty Attenuation for unbiased
dynamic SGG. TEMPURA employs object-level temporal consistencies via
transformer-based sequence modeling, learns to synthesize unbiased relationship
representations using memory-guided training, and attenuates the predictive
uncertainty of visual relations using a Gaussian Mixture Model (GMM). Extensive
experiments demonstrate that our method achieves significant (up to 10% in some
cases) performance gain over existing methods highlighting its superiority in
generating more unbiased scene graphs.",None,-1
2730fd68-6de0-41ff-81da-3eed8c364417,Leverage Points in Modality Shifts: Comparing Language-only and Multimodal Word Representations,0.314135,2,"Multimodal embeddings aim to enrich the semantic information in neural
representations of language compared to text-only models. While different
embeddings exhibit different applicability and performance on downstream tasks,
little is known about the systematic representation differences attributed to
the visual modality. Our paper compares word embeddings from three
vision-and-language models (CLIP, OpenCLIP and Multilingual CLIP) and three
text-only models, with static (FastText) as well as contextual representations
(multilingual BERT; XLM-RoBERTa). This is the first large-scale study of the
effect of visual grounding on language representations, including 46 semantic
parameters. We identify meaning properties and relations that characterize
words whose embeddings are most affected by the inclusion of visual modality in
the training data; that is, points where visual grounding turns out most
important. We find that the effect of visual modality correlates most with
denotational semantic properties related to concreteness, but is also detected
for several specific semantic classes, as well as for valence, a
sentiment-related connotational property of linguistic expressions.",None,-1
25a40d11-57b3-4d1c-935d-e136e0eb2e25,No Offense Taken: Eliciting Offensiveness from Language Models,0.0700523,2,"This work was completed in May 2022.
  For safe and reliable deployment of language models in the real world,
testing needs to be robust. This robustness can be characterized by the
difficulty and diversity of the test cases we evaluate these models on.
Limitations in human-in-the-loop test case generation has prompted an advent of
automated test case generation approaches. In particular, we focus on Red
Teaming Language Models with Language Models by Perez et al.(2022). Our
contributions include developing a pipeline for automated test case generation
via red teaming that leverages publicly available smaller language models
(LMs), experimenting with different target LMs and red classifiers, and
generating a corpus of test cases that can help in eliciting offensive
responses from widely deployed LMs and identifying their failure modes.",None,-1
66ac3c44-3fe0-45d5-86e5-9bb1ce71a566,Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks,0.458681,1,"We introduce a novel and efficient method for Event Coreference Resolution
(ECR) applied to a lower-resourced language domain. By framing ECR as a graph
reconstruction task, we are able to combine deep semantic embeddings with
structural coreference chain knowledge to create a parameter-efficient family
of Graph Autoencoder models (GAE). Our method significantly outperforms
classical mention-pair methods on a large Dutch event coreference corpus in
terms of overall score, efficiency and training speed. Additionally, we show
that our models are consistently able to classify more difficult coreference
links and are far more robust in low-data settings when compared to
transformer-based mention-pair coreference algorithms.",None,-1
f286d550-6714-4678-af77-307a768d96c3,Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News,0.550671,3,"This paper explains the participation of team Hitachi to SemEval-2023 Task 3
""Detecting the genre, the framing, and the persuasion techniques in online news
in a multi-lingual setup.'' Based on the multilingual, multi-task nature of the
task and the low-resource setting, we investigated different cross-lingual and
multi-task strategies for training the pretrained language models. Through
extensive experiments, we found that (a) cross-lingual/multi-task training, and
(b) collecting an external balanced dataset, can benefit the genre and framing
detection. We constructed ensemble models from the results and achieved the
highest macro-averaged F1 scores in Italian and Russian genre categorization
subtasks.",None,-1
e0d0f77b-f284-43e5-b544-ec098664043b,DIFFQG: Generating Questions to Summarize Factual Changes,0.15028,3,"Identifying the difference between two versions of the same article is useful
to update knowledge bases and to understand how articles evolve. Paired texts
occur naturally in diverse situations: reporters write similar news stories and
maintainers of authoritative websites must keep their information up to date.
We propose representing factual changes between paired documents as
question-answer pairs, where the answer to the same question differs between
two versions. We find that question-answer pairs can flexibly and concisely
capture the updated contents. Provided with paired documents, annotators
identify questions that are answered by one passage but answered differently or
cannot be answered by the other. We release DIFFQG which consists of 759 QA
pairs and 1153 examples of paired passages with no factual change. These
questions are intended to be both unambiguous and information-seeking and
involve complex edits, pushing beyond the capabilities of current question
generation and factual change detection systems. Our dataset summarizes the
changes between two versions of the document as questions and answers, studying
automatic update summarization in a novel way.",None,-1
54eb8bcf-d2e3-48b3-8ae8-b72fc0df7e03,Linearity of Relation Decoding in Transformer Language Models,0.797188,44,"Much of the knowledge encoded in transformer language models (LMs) may be
expressed in terms of relations: relations between words and their synonyms,
entities and their attributes, etc. We show that, for a subset of relations,
this computation is well-approximated by a single linear transformation on the
subject representation. Linear relation representations may be obtained by
constructing a first-order approximation to the LM from a single prompt, and
they exist for a variety of factual, commonsense, and linguistic relations.
However, we also identify many cases in which LM predictions capture relational
knowledge accurately, but this knowledge is not linearly encoded in their
representations. Our results thus reveal a simple, interpretable, but
heterogeneously deployed knowledge representation strategy in transformer LMs.",None,-1
fcdf5379-b4f4-4dfb-bc99-da6a383dde11,ARB: Advanced Reasoning Benchmark for Large Language Models,0.592191,25,"Large Language Models (LLMs) have demonstrated remarkable performance on
various quantitative reasoning and knowledge benchmarks. However, many of these
benchmarks are losing utility as LLMs get increasingly high scores, despite not
yet reaching expert performance in these domains. We introduce ARB, a novel
benchmark composed of advanced reasoning problems in multiple fields. ARB
presents a more challenging test than prior benchmarks, featuring problems in
mathematics, physics, biology, chemistry, and law. As a subset of ARB, we
introduce a challenging set of math and physics problems which require advanced
symbolic reasoning and domain knowledge. We evaluate recent models such as
GPT-4 and Claude on ARB and demonstrate that current models score well below
50% on more demanding tasks. In order to improve both automatic and assisted
evaluation capabilities, we introduce a rubric-based evaluation approach,
allowing GPT-4 to score its own intermediate reasoning steps. Further, we
conduct a human evaluation of the symbolic subset of ARB, finding promising
agreement between annotators and GPT-4 rubric evaluation scores.",None,-1
0e36df8a-bf6e-43fe-b44e-008c8af9052c,StableVideo: Text-driven Consistency-aware Diffusion Video Editing,0.982773,83,"Diffusion-based methods can generate realistic images and videos, but they
struggle to edit existing objects in a video while preserving their appearance
over time. This prevents diffusion models from being applied to natural video
editing in practical scenarios. In this paper, we tackle this problem by
introducing temporal dependency to existing text-driven diffusion models, which
allows them to generate consistent appearance for the edited objects.
Specifically, we develop a novel inter-frame propagation mechanism for
diffusion video editing, which leverages the concept of layered representations
to propagate the appearance information from one frame to the next. We then
build up a text-driven video editing framework based on this mechanism, namely
StableVideo, which can achieve consistency-aware video editing. Extensive
experiments demonstrate the strong editing capability of our approach. Compared
with state-of-the-art video editing methods, our approach shows superior
qualitative and quantitative results. Our code is available at
\href{https://github.com/rese1f/StableVideo}{this https URL}.",None,-1
b4d4f21f-643a-4cd5-a076-56145c225a67,A Meta-heuristic Approach to Estimate and Explain Classifier Uncertainty,0.0685311,1,"Trust is a crucial factor affecting the adoption of machine learning (ML)
models. Qualitative studies have revealed that end-users, particularly in the
medical domain, need models that can express their uncertainty in
decision-making allowing users to know when to ignore the model's
recommendations. However, existing approaches for quantifying decision-making
uncertainty are not model-agnostic, or they rely on complex statistical
derivations that are not easily understood by laypersons or end-users, making
them less useful for explaining the model's decision-making process. This work
proposes a set of class-independent meta-heuristics that can characterize the
complexity of an instance in terms of factors are mutually relevant to both
human and ML decision-making. The measures are integrated into a meta-learning
framework that estimates the risk of misclassification. The proposed framework
outperformed predicted probabilities in identifying instances at risk of being
misclassified. The proposed measures and framework hold promise for improving
model development for more complex instances, as well as providing a new means
of model abstention and explanation.",None,-1
3850aa9e-9080-4efd-92ca-54dadc673df9,Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision,0.212929,1,"We address the challenging problem of jointly inferring the 3D flow and
volumetric densities moving in a fluid from a monocular input video with a deep
neural network. Despite the complexity of this task, we show that it is
possible to train the corresponding networks without requiring any 3D ground
truth for training. In the absence of ground truth data we can train our model
with observations from real-world capture setups instead of relying on
synthetic reconstructions. We make this unsupervised training approach possible
by first generating an initial prototype volume which is then moved and
transported over time without the need for volumetric supervision. Our approach
relies purely on image-based losses, an adversarial discriminator network, and
regularization. Our method can estimate long-term sequences in a stable manner,
while achieving closely matching targets for inputs such as rising smoke
plumes.",None,-1
cbc54d82-035d-4bec-bc02-dc5bb774a868,Hierarchical-Hyperplane Kernels for Actively Learning Gaussian Process Models of Nonstationary Systems,0.329501,3,"Learning precise surrogate models of complex computer simulations and
physical machines often require long-lasting or expensive experiments.
Furthermore, the modeled physical dependencies exhibit nonlinear and
nonstationary behavior. Machine learning methods that are used to produce the
surrogate model should therefore address these problems by providing a scheme
to keep the number of queries small, e.g. by using active learning and be able
to capture the nonlinear and nonstationary properties of the system. One way of
modeling the nonstationarity is to induce input-partitioning, a principle that
has proven to be advantageous in active learning for Gaussian processes.
However, these methods either assume a known partitioning, need to introduce
complex sampling schemes or rely on very simple geometries. In this work, we
present a simple, yet powerful kernel family that incorporates a partitioning
that: i) is learnable via gradient-based methods, ii) uses a geometry that is
more flexible than previous ones, while still being applicable in the low data
regime. Thus, it provides a good prior for active learning procedures. We
empirically demonstrate excellent performance on various active learning tasks.",None,-1
0dd2d5dd-12f4-4790-a7ac-26aa98289137,Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene Graphs,0.620706,4,"Dynamic scene graphs generated from video clips could help enhance the
semantic visual understanding in a wide range of challenging tasks such as
environmental perception, autonomous navigation, and task planning of
self-driving vehicles and mobile robots. In the process of temporal and spatial
modeling during dynamic scene graph generation, it is particularly intractable
to learn time-variant relations in dynamic scene graphs among frames. In this
paper, we propose a Time-variant Relation-aware TRansformer (TR$^2$), which
aims to model the temporal change of relations in dynamic scene graphs.
Explicitly, we leverage the difference of text embeddings of prompted sentences
about relation labels as the supervision signal for relations. In this way,
cross-modality feature guidance is realized for the learning of time-variant
relations. Implicitly, we design a relation feature fusion module with a
transformer and an additional message token that describes the difference
between adjacent frames. Extensive experiments on the Action Genome dataset
prove that our TR$^2$ can effectively model the time-variant relations. TR$^2$
significantly outperforms previous state-of-the-art methods under two different
settings by 2.1% and 2.6% respectively.",None,-1
532ea431-a48b-4e2d-a370-5fea53aaf935,Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing,0.589758,3,"Environmental damage has been of much concern, particularly in coastal areas
and the oceans, given climate change and the drastic effects of pollution and
extreme climate events. Our present-day analytical capabilities, along with
advancements in information acquisition techniques such as remote sensing, can
be utilised for the management and study of coral reef ecosystems. In this
paper, we present Reef-Insight, an unsupervised machine learning framework that
features advanced clustering methods and remote sensing for reef habitat
mapping. Our framework compares different clustering methods for reef habitat
mapping using remote sensing data. We evaluate four major clustering approaches
based on qualitative and visual assessments which include k-means, hierarchical
clustering, Gaussian mixture model, and density-based clustering. We utilise
remote sensing data featuring the One Tree Island reef in Australia's Southern
Great Barrier Reef. Our results indicate that clustering methods using remote
sensing data can well identify benthic and geomorphic clusters in reefs when
compared with other studies. Our results indicate that Reef-Insight can
generate detailed reef habitat maps outlining distinct reef habitats and has
the potential to enable further insights for reef restoration projects.",None,-1
110d1388-0a92-4d59-a28a-d155a14cef7b,Inductive reasoning in humans and large language models,0.34523,16,"The impressive recent performance of large language models has led many to
wonder to what extent they can serve as models of general intelligence or are
similar to human cognition. We address this issue by applying GPT-3.5 and GPT-4
to a classic problem in human inductive reasoning known as property induction.
Over two experiments, we elicit human judgments on a range of property
induction tasks spanning multiple domains. Although GPT-3.5 struggles to
capture many aspects of human behaviour, GPT-4 is much more successful: for the
most part, its performance qualitatively matches that of humans, and the only
notable exception is its failure to capture the phenomenon of premise
non-monotonicity. Our work demonstrates that property induction allows for
interesting comparisons between human and machine intelligence and provides two
large datasets that can serve as benchmarks for future work in this vein.",None,-1
48d2a9ff-63bd-428b-8b3f-a19166efbaf4,Data Acquisition: A New Frontier in Data-centric AI,0.801211,3,"As Machine Learning (ML) systems continue to grow, the demand for relevant
and comprehensive datasets becomes imperative. There is limited study on the
challenges of data acquisition due to ad-hoc processes and lack of consistent
methodologies. We first present an investigation of current data marketplaces,
revealing lack of platforms offering detailed information about datasets,
transparent pricing, standardized data formats. With the objective of inciting
participation from the data-centric AI community, we then introduce the DAM
challenge, a benchmark to model the interaction between the data providers and
acquirers. The benchmark was released as a part of DataPerf. Our evaluation of
the submitted strategies underlines the need for effective data acquisition
strategies in ML.",None,-1
3373057b-7ea9-4dd2-9534-eeb99a33e508,AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,0.419645,60,"AutoGen is an open-source framework that allows developers to build LLM
applications via multiple agents that can converse with each other to
accomplish tasks. AutoGen agents are customizable, conversable, and can operate
in various modes that employ combinations of LLMs, human inputs, and tools.
Using AutoGen, developers can also flexibly define agent interaction behaviors.
Both natural language and computer code can be used to program flexible
conversation patterns for different applications. AutoGen serves as a generic
infrastructure to build diverse applications of various complexities and LLM
capacities. Empirical studies demonstrate the effectiveness of the framework in
many example applications, with domains ranging from mathematics, coding,
question answering, operations research, online decision-making, entertainment,
etc.",None,-1
dbd84ac9-cd52-4b0b-8fde-32483b90863d,Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation,0.993873,11,"One critical challenge in 6D object pose estimation from a single RGBD image
is efficient integration of two different modalities, i.e., color and depth. In
this work, we tackle this problem by a novel Deep Fusion Transformer~(DFTr)
block that can aggregate cross-modality features for improving pose estimation.
Unlike existing fusion methods, the proposed DFTr can better model
cross-modality semantic correlation by leveraging their semantic similarity,
such that globally enhanced features from different modalities can be better
integrated for improved information extraction. Moreover, to further improve
robustness and efficiency, we introduce a novel weighted vector-wise voting
algorithm that employs a non-iterative global optimization strategy for precise
3D keypoint localization while achieving near real-time inference. Extensive
experiments show the effectiveness and strong generalization capability of our
proposed 3D keypoint voting algorithm. Results on four widely used benchmarks
also demonstrate that our method outperforms the state-of-the-art methods by
large margins.",None,-1
affe6301-adb0-4340-b312-657a4214995b,NeSIG: A Neuro-Symbolic Method for Learning to Generate Planning Problems,0.276554,2,"In the field of Automated Planning there is often the need for a set of
planning problems from a particular domain, e.g., to be used as training data
for Machine Learning or as benchmarks in planning competitions. In most cases,
these problems are created either by hand or by a domain-specific generator,
putting a burden on the human designers. In this paper we propose NeSIG, to the
best of our knowledge the first domain-independent method for automatically
generating planning problems that are valid, diverse and difficult to solve. We
formulate problem generation as a Markov Decision Process and train two
generative policies with Deep Reinforcement Learning to generate problems with
the desired properties. We conduct experiments on several classical domains,
comparing our method with handcrafted domain-specific generators that generate
valid and diverse problems but do not optimize difficulty. The results show
NeSIG is able to automatically generate valid problems of greater difficulty
than the competitor approaches, while maintaining good diversity.",None,-1
21e6612a-cdb5-46aa-b354-3f19d9bd74da,When Brain-inspired AI Meets AGI,0.999829,57,"Artificial General Intelligence (AGI) has been a long-standing goal of
humanity, with the aim of creating machines capable of performing any
intellectual task that humans can do. To achieve this, AGI researchers draw
inspiration from the human brain and seek to replicate its principles in
intelligent machines. Brain-inspired artificial intelligence is a field that
has emerged from this endeavor, combining insights from neuroscience,
psychology, and computer science to develop more efficient and powerful AI
systems. In this article, we provide a comprehensive overview of brain-inspired
AI from the perspective of AGI. We begin with the current progress in
brain-inspired AI and its extensive connection with AGI. We then cover the
important characteristics for both human intelligence and AGI (e.g., scaling,
multimodality, and reasoning). We discuss important technologies toward
achieving AGI in current AI systems, such as in-context learning and prompt
tuning. We also investigate the evolution of AGI systems from both algorithmic
and infrastructural perspectives. Finally, we explore the limitations and
future of AGI.",None,-1
9553bb85-3f31-4d3a-a5f9-795e796f1fbc,GMNLP at SemEval-2023 Task 12: Sentiment Analysis with Phylogeny-Based Adapters,0.169866,2,"This report describes GMU's sentiment analysis system for the SemEval-2023
shared task AfriSenti-SemEval. We participated in all three sub-tasks:
Monolingual, Multilingual, and Zero-Shot. Our approach uses models initialized
with AfroXLMR-large, a pre-trained multilingual language model trained on
African languages and fine-tuned correspondingly. We also introduce augmented
training data along with original training data. Alongside finetuning, we
perform phylogeny-based adapter tuning to create several models and ensemble
the best models for the final submission. Our system achieves the best F1-score
on track 5: Amharic, with 6.2 points higher F1-score than the second-best
performing system on this track. Overall, our system ranks 5th among the 10
systems participating in all 15 tracks.",None,-1
65373303-923d-4721-9bfc-ba5a1b0a4c73,"OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning",0.517995,17,"Large language models (LLMs) often struggle with maintaining accuracy
throughout multiple multiple reasoning steps, especially in mathematical
reasoning where an error in earlier steps can propagate to subsequent ones and
it ultimately leading to an incorrect answer. To reduce error propagation,
guided decoding is employed to direct the LM decoding on a step-by-step basis.
We argue that in guided decoding, assessing the potential of an incomplete
reasoning path can be more advantageous than simply ensuring per-step
correctness, as the former approach leads towards a correct final answer. This
transforms the task into a $\textit{value estimation}$ problem in planning.
  Inspired by the findings that $\textit{outcome supervision for guided
decoding essentially acts as a value model}$, we propose Outcome-supervised
Value Model (OVM) that employs outcome supervision for training a value model,
which prioritizes steps that lead to accurate conclusions. Furthermore, the OVM
eliminates the need for labor-intensive annotations of step-level correctness,
thereby significantly enhancing its scalability. Our experiments on two
multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate
the superior performance of the OVM model. Notably, in GSM8K, our
$\textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B
parameters}$; especially it does not utilize GPT-4 or code execution. These
findings offer a novel perspective on the role of outcome supervision in
training value models for multi-step reasoning tasks and provide theoretical
justification for its advantage in value estimation for guided decoding.",None,-1
afb55724-d334-4cee-8c70-946b9f6981dd,PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents,0.214524,31,"Strategies such as chain-of-thought prompting improve the performance of
large language models (LLMs) on complex reasoning tasks by decomposing input
examples into intermediate steps. However, it remains unclear how to apply such
methods to reason over long input documents, in which both the decomposition
and the output of each intermediate step are non-trivial to obtain. In this
work, we propose PEARL, a prompting framework to improve reasoning over long
documents, which consists of three stages: action mining, plan formulation, and
plan execution. More specifically, given a question about a long document,
PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE,
FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain
the answer. Each stage of PEARL is implemented via zero-shot or few-shot
prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate
PEARL on a challenging subset of the QuALITY dataset, which contains questions
that require complex reasoning over long narrative texts. PEARL outperforms
zero-shot and chain-of-thought prompting on this dataset, and ablation
experiments show that each stage of PEARL is critical to its performance.
Overall, PEARL is a first step towards leveraging LLMs to reason over long
documents.",None,-1
7a0c98d0-0334-4be9-976f-61e38002f467,Local Optima Correlation Assisted Adaptive Operator Selection,0.381692,2,"For solving combinatorial optimisation problems with metaheuristics,
different search operators are applied for sampling new solutions in the
neighbourhood of a given solution. It is important to understand the
relationship between operators for various purposes, e.g., adaptively deciding
when to use which operator to find optimal solutions efficiently. However, it
is difficult to theoretically analyse this relationship, especially in the
complex solution space of combinatorial optimisation problems. In this paper,
we propose to empirically analyse the relationship between operators in terms
of the correlation between their local optima and develop a measure for
quantifying their relationship. The comprehensive analyses on a wide range of
capacitated vehicle routing problem benchmark instances show that there is a
consistent pattern in the correlation between commonly used operators. Based on
this newly proposed local optima correlation metric, we propose a novel
approach for adaptively selecting among the operators during the search
process. The core intention is to improve search efficiency by preventing
wasting computational resources on exploring neighbourhoods where the local
optima have already been reached. Experiments on randomly generated instances
and commonly used benchmark datasets are conducted. Results show that the
proposed approach outperforms commonly used adaptive operator selection
methods.",None,-1
66710081-6ead-4fa8-bb5b-7d9184826cf9,Continual Domain Adaptation through Pruning-aided Domain-specific Weight Modulation,0.167413,2,"In this paper, we propose to develop a method to address unsupervised domain
adaptation (UDA) in a practical setting of continual learning (CL). The goal is
to update the model on continually changing domains while preserving
domain-specific knowledge to prevent catastrophic forgetting of past-seen
domains. To this end, we build a framework for preserving domain-specific
features utilizing the inherent model capacity via pruning. We also perform
effective inference using a novel batch-norm based metric to predict the final
model parameters to be used accurately. Our approach achieves not only
state-of-the-art performance but also prevents catastrophic forgetting of past
domains significantly. Our code is made publicly available.",None,-1
4fa6e12e-8c87-4bc3-ac9d-3d87f1437b06,Generating Topic Pages for Scientific Concepts Using Scientific Publications,0.314966,2,"In this paper, we describe Topic Pages, an inventory of scientific concepts
and information around them extracted from a large collection of scientific
books and journals. The main aim of Topic Pages is to provide all the necessary
information to the readers to understand scientific concepts they come across
while reading scholarly content in any scientific domain. Topic Pages are a
collection of automatically generated information pages using NLP and ML, each
corresponding to a scientific concept. Each page contains three pieces of
information: a definition, related concepts, and the most relevant snippets,
all extracted from scientific peer-reviewed publications. In this paper, we
discuss the details of different components to extract each of these elements.
The collection of pages in production contains over 360,000 Topic Pages across
20 different scientific domains with an average of 23 million unique visits per
month, constituting it a popular source for scientific information.",None,-1
c4b2be82-c549-4ee6-abea-f1fbb1046cf9,Less than One-shot: Named Entity Recognition via Extremely Weak Supervision,0.731505,2,"We study the named entity recognition (NER) problem under the extremely weak
supervision (XWS) setting, where only one example entity per type is given in a
context-free way. While one can see that XWS is lighter than one-shot in terms
of the amount of supervision, we propose a novel method X-NER that can
outperform the state-of-the-art one-shot NER methods. We first mine entity
spans that are similar to the example entities from an unlabelled training
corpus. Instead of utilizing entity span representations from language models,
we find it more effective to compare the context distributions before and after
the span is replaced by the entity example. We then leverage the top-ranked
spans as pseudo-labels to train an NER tagger. Extensive experiments and
analyses on 4 NER datasets show the superior end-to-end NER performance of
X-NER, outperforming the state-of-the-art few-shot methods with 1-shot
supervision and ChatGPT annotations significantly. Finally, our X-NER possesses
several notable properties, such as inheriting the cross-lingual abilities of
the underlying language models.",None,-1
4190f9cb-75f7-4678-acdd-703af1dfa76f,AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL,0.921708,7,"LLMs are being increasingly used for planning-style tasks, but their
capabilities for planning and reasoning are poorly understood. We present
AutoPlanBench, a novel method for automatically converting planning benchmarks
written in PDDL into textual descriptions and offer a benchmark dataset created
with our method. We show that while the best LLM planners do well on some
planning tasks, others remain out of reach of current methods.",None,-1
14de8430-e7ff-41cc-afeb-6a563ef1fde8,CPLLM: Clinical Prediction with Large Language Models,0.721166,8,"We present Clinical Prediction with Large Language Models (CPLLM), a method
that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical
disease and readmission prediction. We utilized quantization and fine-tuned the
LLM using prompts. For diagnosis prediction, we predict whether patients will
be diagnosed with a target disease during their next visit or in the subsequent
diagnosis, leveraging their historical diagnosis records. We compared our
results to various baselines, including RETAIN, and Med-BERT, the current
state-of-the-art model for disease prediction using temporal structured EHR
data. In addition, We also evaluated CPLLM for patient hospital readmission
prediction and compared our method's performance with benchmark baselines. Our
experiments have shown that our proposed method, CPLLM, surpasses all the
tested models in terms of PR-AUC and ROC-AUC metrics, showing state-of-the-art
results for diagnosis prediction and patient hospital readmission prediction.
Such a method can be easily implemented and integrated into the clinical
process to help care providers estimate the next steps of patients",None,-1
e5fd0c97-f76c-4834-9ea0-0369eff338e1,Leveraging Importance Weights in Subset Selection,0.436638,3,"We present a subset selection algorithm designed to work with arbitrary model
families in a practical batch setting. In such a setting, an algorithm can
sample examples one at a time but, in order to limit overhead costs, is only
able to update its state (i.e. further train model weights) once a large enough
batch of examples is selected. Our algorithm, IWeS, selects examples by
importance sampling where the sampling probability assigned to each example is
based on the entropy of models trained on previously selected batches. IWeS
admits significant performance improvement compared to other subset selection
algorithms for seven publicly available datasets. Additionally, it is
competitive in an active learning setting, where the label information is not
available at selection time. We also provide an initial theoretical analysis to
support our importance weighting approach, proving generalization and sampling
rate bounds.",None,-1
4c91ffd5-0b32-41f1-934e-e1537be7d8cb,MiDaS v3.1 -- A Model Zoo for Robust Monocular Relative Depth Estimation,0.999803,44,"We release MiDaS v3.1 for monocular depth estimation, offering a variety of
new models based on different encoder backbones. This release is motivated by
the success of transformers in computer vision, with a large variety of
pretrained vision transformers now available. We explore how using the most
promising vision transformers as image encoders impacts depth estimation
quality and runtime of the MiDaS architecture. Our investigation also includes
recent convolutional approaches that achieve comparable quality to vision
transformers in image classification tasks. While the previous release MiDaS
v3.0 solely leverages the vanilla vision transformer ViT, MiDaS v3.1 offers
additional models based on BEiT, Swin, SwinV2, Next-ViT and LeViT. These models
offer different performance-runtime tradeoffs. The best model improves the
depth estimation quality by 28% while efficient models enable downstream tasks
requiring high frame rates. We also describe the general process for
integrating new backbones. A video summarizing the work can be found at
https://youtu.be/UjaeNNFf9sE and the code is available at
https://github.com/isl-org/MiDaS.",None,-1
68a1633d-e406-4095-a139-3ab507a8fa07,Unpaired Image-to-Image Translation via Neural Schrödinger Bridge,0.860262,15,"Diffusion models are a powerful class of generative models which simulate
stochastic differential equations (SDEs) to generate data from noise. While
diffusion models have achieved remarkable progress, they have limitations in
unpaired image-to-image (I2I) translation tasks due to the Gaussian prior
assumption. Schr\""{o}dinger Bridge (SB), which learns an SDE to translate
between two arbitrary distributions, have risen as an attractive solution to
this problem. Yet, to our best knowledge, none of SB models so far have been
successful at unpaired translation between high-resolution images. In this
work, we propose Unpaired Neural Schr\""{o}dinger Bridge (UNSB), which expresses
the SB problem as a sequence of adversarial learning problems. This allows us
to incorporate advanced discriminators and regularization to learn a SB between
unpaired data. We show that UNSB is scalable and successfully solves various
unpaired I2I translation tasks. Code: \url{https://github.com/cyclomon/UNSB}",None,-1
d2790db7-75a5-49ec-bc2f-432930b1081e,FinnWoodlands Dataset,0.552329,4,"While the availability of large and diverse datasets has contributed to
significant breakthroughs in autonomous driving and indoor applications,
forestry applications are still lagging behind and new forest datasets would
most certainly contribute to achieving significant progress in the development
of data-driven methods for forest-like scenarios. This paper introduces a
forest dataset called \textit{FinnWoodlands}, which consists of RGB stereo
images, point clouds, and sparse depth maps, as well as ground truth manual
annotations for semantic, instance, and panoptic segmentation.
\textit{FinnWoodlands} comprises a total of 4226 objects manually annotated,
out of which 2562 objects (60.6\%) correspond to tree trunks classified into
three different instance categories, namely ""Spruce Tree"", ""Birch Tree"", and
""Pine Tree"". Besides tree trunks, we also annotated ""Obstacles"" objects as
instances as well as the semantic stuff classes ""Lake"", ""Ground"", and ""Track"".
Our dataset can be used in forestry applications where a holistic
representation of the environment is relevant. We provide an initial benchmark
using three models for instance segmentation, panoptic segmentation, and depth
completion, and illustrate the challenges that such unstructured scenarios
introduce.",None,-1
0c997f02-0a72-4b79-bf35-6dab6c67a977,Semantic-Preserving Augmentation for Robust Image-Text Retrieval,0.0232218,1,"Image text retrieval is a task to search for the proper textual descriptions
of the visual world and vice versa. One challenge of this task is the
vulnerability to input image and text corruptions. Such corruptions are often
unobserved during the training, and degrade the retrieval model decision
quality substantially. In this paper, we propose a novel image text retrieval
technique, referred to as robust visual semantic embedding (RVSE), which
consists of novel image-based and text-based augmentation techniques called
semantic preserving augmentation for image (SPAugI) and text (SPAugT). Since
SPAugI and SPAugT change the original data in a way that its semantic
information is preserved, we enforce the feature extractors to generate
semantic aware embedding vectors regardless of the corruption, improving the
model robustness significantly. From extensive experiments using benchmark
datasets, we show that RVSE outperforms conventional retrieval schemes in terms
of image-text retrieval performance.",None,-1
1fe40536-c45c-42cf-bdf1-4399019e1730,OLISIA: a Cascade System for Spoken Dialogue State Tracking,0.443177,2,"Though Dialogue State Tracking (DST) is a core component of spoken dialogue
systems, recent work on this task mostly deals with chat corpora, disregarding
the discrepancies between spoken and written language.In this paper, we propose
OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR)
model and a DST model. We introduce several adaptations in the ASR and DST
modules to improve integration and robustness to spoken conversations.With
these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to
evaluate spoken DST. We conduct an in-depth analysis of the results and find
that normalizing the ASR outputs and adapting the DST inputs through data
augmentation, along with increasing the pre-trained models size all play an
important role in reducing the performance discrepancy between written and
spoken conversations.",None,-1
b2e7570b-ba6e-42a4-a5fa-ae57f88f9990,Inferring Fluid Dynamics via Inverse Rendering,0.414274,2,"Humans have a strong intuitive understanding of physical processes such as
fluid falling by just a glimpse of such a scene picture, i.e., quickly derived
from our immersive visual experiences in memory. This work achieves such a
photo-to-fluid-dynamics reconstruction functionality learned from unannotated
videos, without any supervision of ground-truth fluid dynamics. In a nutshell,
a differentiable Euler simulator modeled with a ConvNet-based pressure
projection solver, is integrated with a volumetric renderer, supporting
end-to-end/coherent differentiable dynamic simulation and rendering. By
endowing each sampled point with a fluid volume value, we derive a NeRF-like
differentiable renderer dedicated from fluid data; and thanks to this
volume-augmented representation, fluid dynamics could be inversely inferred
from the error signal between the rendered result and ground-truth video frame
(i.e., inverse rendering). Experiments on our generated Fluid Fall datasets and
DPI Dam Break dataset are conducted to demonstrate both effectiveness and
generalization ability of our method.",None,-1
eaf3ef46-30d3-4cbc-b2e9-d864b0e798da,UKP-SQuARE v3: A Platform for Multi-Agent QA Research,0.200196,1,"The continuous development of Question Answering (QA) datasets has drawn the
research community's attention toward multi-domain models. A popular approach
is to use multi-dataset models, which are models trained on multiple datasets
to learn their regularities and prevent overfitting to a single dataset.
However, with the proliferation of QA models in online repositories such as
GitHub or Hugging Face, an alternative is becoming viable. Recent works have
demonstrated that combining expert agents can yield large performance gains
over multi-dataset models. To ease research in multi-agent models, we extend
UKP-SQuARE, an online platform for QA research, to support three families of
multi-agent systems: i) agent selection, ii) early-fusion of agents, and iii)
late-fusion of agents. We conduct experiments to evaluate their inference speed
and discuss the performance vs. speed trade-off compared to multi-dataset
models. UKP-SQuARE is open-source and publicly available at
http://square.ukp-lab.de.",None,-1
d13c2278-08eb-4c91-9100-dfd13caadf0d,Householder Projector for Unsupervised Latent Semantics Discovery,0.733369,4,"Generative Adversarial Networks (GANs), especially the recent style-based
generators (StyleGANs), have versatile semantics in the structured latent
space. Latent semantics discovery methods emerge to move around the latent code
such that only one factor varies during the traversal. Recently, an
unsupervised method proposed a promising direction to directly use the
eigenvectors of the projection matrix that maps latent codes to features as the
interpretable directions. However, one overlooked fact is that the projection
matrix is non-orthogonal and the number of eigenvectors is too large. The
non-orthogonality would entangle semantic attributes in the top few
eigenvectors, and the large dimensionality might result in meaningless
variations among the directions even if the matrix is orthogonal. To avoid
these issues, we propose Householder Projector, a flexible and general low-rank
orthogonal matrix representation based on Householder transformations, to
parameterize the projection matrix. The orthogonality guarantees that the
eigenvectors correspond to disentangled interpretable semantics, while the
low-rank property encourages that each identified direction has meaningful
variations. We integrate our projector into pre-trained StyleGAN2/StyleGAN3 and
evaluate the models on several benchmarks. Within only $1\%$ of the original
training steps for fine-tuning, our projector helps StyleGANs to discover more
disentangled and precise semantic attributes without sacrificing image
fidelity.",None,-1
588ba8b6-b9ca-4cf4-a579-f66a52aece3f,Painterly Image Harmonization using Diffusion Model,0.75003,7,"Painterly image harmonization aims to insert photographic objects into
paintings and obtain artistically coherent composite images. Previous methods
for this task mainly rely on inference optimization or generative adversarial
network, but they are either very time-consuming or struggling at fine control
of the foreground objects (e.g., texture and content details). To address these
issues, we propose a novel Painterly Harmonization stable Diffusion model
(PHDiffusion), which includes a lightweight adaptive encoder and a Dual Encoder
Fusion (DEF) module. Specifically, the adaptive encoder and the DEF module
first stylize foreground features within each encoder. Then, the stylized
foreground features from both encoders are combined to guide the harmonization
process. During training, besides the noise loss in diffusion model, we
additionally employ content loss and two style losses, i.e., AdaIN style loss
and contrastive style loss, aiming to balance the trade-off between style
migration and content preservation. Compared with the state-of-the-art models
from related fields, our PHDiffusion can stylize the foreground more
sufficiently and simultaneously retain finer content. Our code and model are
available at https://github.com/bcmi/PHDiffusion-Painterly-Image-Harmonization.",None,-1
1c830846-96e3-4d83-a030-a475d6d5d1c4,Visual Material Characteristics Learning for Circular Healthcare,0.648669,2,"The linear take-make-dispose paradigm at the foundations of our traditional
economy is proving to be unsustainable due to waste pollution and material
supply uncertainties. Hence, increasing the circularity of material flows is
necessary. In this paper, we make a step towards circular healthcare by
developing several vision systems targeting three main circular economy tasks:
resources mapping and quantification, waste sorting, and disassembly. The
performance of our systems demonstrates that representation-learning vision can
improve the recovery chain, where autonomous systems are key enablers due to
the contamination risks. We also published two fully-annotated datasets for
image segmentation and for key-point tracking in disassembly operations of
inhalers and glucose meters. The datasets and source code are publicly
available.",None,-1
19600611-a03c-4308-a197-fd197fffd178,First-Order Stable Model Semantics with Intensional Functions,0.29648,6,"In classical logic, nonBoolean fluents, such as the location of an object,
can be naturally described by functions. However, this is not the case in
answer set programs, where the values of functions are pre-defined, and
nonmonotonicity of the semantics is related to minimizing the extents of
predicates but has nothing to do with functions. We extend the first-order
stable model semantics by Ferraris, Lee, and Lifschitz to allow intensional
functions -- functions that are specified by a logic program just like
predicates are specified. We show that many known properties of the stable
model semantics are naturally extended to this formalism and compare it with
other related approaches to incorporating intensional functions. Furthermore,
we use this extension as a basis for defining Answer Set Programming Modulo
Theories (ASPMT), analogous to the way that Satisfiability Modulo Theories
(SMT) is defined, allowing for SMT-like effective first-order reasoning in the
context of ASP. Using SMT solving techniques involving functions, ASPMT can be
applied to domains containing real numbers and alleviates the grounding
problem. We show that other approaches to integrating ASP and CSP/SMT can be
related to special cases of ASPMT in which functions are limited to
non-intensional ones.",None,-1
75bdd410-73a0-44a3-9713-b8d20aea4772,Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion,0.993121,53,"We introduce a method for generating realistic pedestrian trajectories and
full-body animations that can be controlled to meet user-defined goals. We draw
on recent advances in guided diffusion modeling to achieve test-time
controllability of trajectories, which is normally only associated with
rule-based systems. Our guided diffusion model allows users to constrain
trajectories through target waypoints, speed, and specified social groups while
accounting for the surrounding environment context. This trajectory diffusion
model is integrated with a novel physics-based humanoid controller to form a
closed-loop, full-body pedestrian animation system capable of placing large
crowds in a simulated environment with varying terrains. We further propose
utilizing the value function learned during RL training of the animation
controller to guide diffusion to produce trajectories better suited for
particular scenarios such as collision avoidance and traversing uneven terrain.
Video results are available on the project page at
https://nv-tlabs.github.io/trace-pace .",None,-1
09e0987c-b81c-42f7-a804-1eee2665ef19,Developing Speech Processing Pipelines for Police Accountability,0.103583,1,"Police body-worn cameras have the potential to improve accountability and
transparency in policing. Yet in practice, they result in millions of hours of
footage that is never reviewed. We investigate the potential of large
pre-trained speech models for facilitating reviews, focusing on ASR and officer
speech detection in footage from traffic stops. Our proposed pipeline includes
training data alignment and filtering, fine-tuning with resource constraints,
and combining officer speech detection with ASR for a fully automated approach.
We find that (1) fine-tuning strongly improves ASR performance on officer
speech (WER=12-13%), (2) ASR on officer speech is much more accurate than on
community member speech (WER=43.55-49.07%), (3) domain-specific tasks like
officer speech detection and diarization remain challenging. Our work offers
practical applications for reviewing body camera footage and general guidance
for adapting pre-trained speech models to noisy multi-speaker domains.",None,-1
25dcc8ee-78b1-4c67-b430-fc220e929e64,Predictable MDP Abstraction for Unsupervised Model-Based RL,0.390385,8,"A key component of model-based reinforcement learning (RL) is a dynamics
model that predicts the outcomes of actions. Errors in this predictive model
can degrade the performance of model-based controllers, and complex Markov
decision processes (MDPs) can present exceptionally difficult prediction
problems. To mitigate this issue, we propose predictable MDP abstraction (PMA):
instead of training a predictive model on the original MDP, we train a model on
a transformed MDP with a learned action space that only permits predictable,
easy-to-model actions, while covering the original state-action space as much
as possible. As a result, model learning becomes easier and more accurate,
which allows robust, stable model-based planning or model-based RL. This
transformation is learned in an unsupervised manner, before any task is
specified by the user. Downstream tasks can then be solved with model-based
control in a zero-shot fashion, without additional environment interactions. We
theoretically analyze PMA and empirically demonstrate that PMA leads to
significant improvements over prior unsupervised model-based RL approaches in a
range of benchmark environments. Our code and videos are available at
https://seohong.me/projects/pma/",None,-1
19b3e1f2-dc41-4295-ad40-621e88976c64,X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs,0.725467,2,"Understanding when two pieces of text convey the same information is a goal
touching many subproblems in NLP, including textual entailment and
fact-checking. This problem becomes more complex when those two pieces of text
are in different languages. Here, we introduce X-PARADE (Cross-lingual
Paragraph-level Analysis of Divergences and Entailments), the first
cross-lingual dataset of paragraph-level information divergences. Annotators
label a paragraph in a target language at the span level and evaluate it with
respect to a corresponding paragraph in a source language, indicating whether a
given piece of information is the same, new, or new but can be inferred. This
last notion establishes a link with cross-language NLI. Aligned paragraphs are
sourced from Wikipedia pages in different languages, reflecting real
information divergences observed in the wild. Armed with our dataset, we
investigate a diverse set of approaches for this problem, including token
alignment from machine translation, textual entailment methods that localize
their decisions, and prompting LLMs. Our results show that these methods vary
in their capability to handle inferable information, but they all fall short of
human performance.",None,-1
972f433e-c71c-433c-841f-cf95fbbdcec9,Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis,0.924298,11,"Generative latent diffusion models have been established as state-of-the-art
in data generation. One promising application is generation of realistic
synthetic medical imaging data for open data sharing without compromising
patient privacy. Despite the promise, the capacity of such models to memorize
sensitive patient training data and synthesize samples showing high resemblance
to training data samples is relatively unexplored. Here, we assess the
memorization capacity of 3D latent diffusion models on photon-counting coronary
computed tomography angiography and knee magnetic resonance imaging datasets.
To detect potential memorization of training samples, we utilize
self-supervised models based on contrastive learning. Our results suggest that
such latent diffusion models indeed memorize training data, and there is a dire
need for devising strategies to mitigate memorization.",None,-1
4643260f-afb6-4ce5-8676-8c20e3626424,Unveiling the Multi-Annotation Process: Examining the Influence of Annotation Quantity and Instance Difficulty on Model Performance,0.0713055,2,"The NLP community has long advocated for the construction of multi-annotator
datasets to better capture the nuances of language interpretation,
subjectivity, and ambiguity. This paper conducts a retrospective study to show
how performance scores can vary when a dataset expands from a single annotation
per instance to multiple annotations. We propose a novel multi-annotator
simulation process to generate datasets with varying annotation budgets. We
show that similar datasets with the same annotation budget can lead to varying
performance gains. Our findings challenge the popular belief that models
trained on multi-annotation examples always lead to better performance than
models trained on single or few-annotation examples.",None,-1
107b6a79-8e93-44f9-ac26-c6665a7908b0,Rounding Meets Approximate Model Counting,0.231499,4,"The problem of model counting, also known as #SAT, is to compute the number
of models or satisfying assignments of a given Boolean formula $F$. Model
counting is a fundamental problem in computer science with a wide range of
applications. In recent years, there has been a growing interest in using
hashing-based techniques for approximate model counting that provide
$(\varepsilon, \delta)$-guarantees: i.e., the count returned is within a
$(1+\varepsilon)$-factor of the exact count with confidence at least
$1-\delta$. While hashing-based techniques attain reasonable scalability for
large enough values of $\delta$, their scalability is severely impacted for
smaller values of $\delta$, thereby preventing their adoption in application
domains that require estimates with high confidence.
  The primary contribution of this paper is to address the Achilles heel of
hashing-based techniques: we propose a novel approach based on rounding that
allows us to achieve a significant reduction in runtime for smaller values of
$\delta$. The resulting counter, called RoundMC, achieves a substantial runtime
performance improvement over the current state-of-the-art counter, ApproxMC. In
particular, our extensive evaluation over a benchmark suite consisting of 1890
instances shows that RoundMC solves 204 more instances than ApproxMC, and
achieves a $4\times$ speedup over ApproxMC.",None,-1
37a5fc65-21d0-47a5-8f90-d6a5ac798f9d,KG-BERTScore: Incorporating Knowledge Graph into BERTScore for Reference-Free Machine Translation Evaluation,0.401574,5,"BERTScore is an effective and robust automatic metric for referencebased
machine translation evaluation. In this paper, we incorporate multilingual
knowledge graph into BERTScore and propose a metric named KG-BERTScore, which
linearly combines the results of BERTScore and bilingual named entity matching
for reference-free machine translation evaluation. From the experimental
results on WMT19 QE as a metric without references shared tasks, our metric
KG-BERTScore gets higher overall correlation with human judgements than the
current state-of-the-art metrics for reference-free machine translation
evaluation.1 Moreover, the pre-trained multilingual model used by KG-BERTScore
and the parameter for linear combination are also studied in this paper.",None,-1
7ebd676f-7359-498c-b348-0b4f152a6a89,Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction,0.686121,2,"Knowledge graph construction (KGC) is a multifaceted undertaking involving
the extraction of entities, relations, and events. Traditionally, large
language models (LLMs) have been viewed as solitary task-solving agents in this
complex landscape. However, this paper challenges this paradigm by introducing
a novel framework, CooperKGC. Departing from the conventional approach,
CooperKGC establishes a collaborative processing network, assembling a KGC
collaboration team capable of concurrently addressing entity, relation, and
event extraction tasks. Our experiments unequivocally demonstrate that
fostering collaboration and information interaction among diverse agents within
CooperKGC yields superior results compared to individual cognitive processes
operating in isolation. Importantly, our findings reveal that the collaboration
facilitated by CooperKGC enhances knowledge selection, correction, and
aggregation capabilities across multiple rounds of interactions.",None,-1
10483fb6-f8f6-46db-a34f-e683a98f72c6,SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images,0.753719,19,"Visual question answering on document images that contain textual, visual,
and layout information, called document VQA, has received much attention
recently. Although many datasets have been proposed for developing document VQA
systems, most of the existing datasets focus on understanding the content
relationships within a single image and not across multiple images. In this
study, we propose a new multi-image document VQA dataset, SlideVQA, containing
2.6k+ slide decks composed of 52k+ slide images and 14.5k questions about a
slide deck. SlideVQA requires complex reasoning, including single-hop,
multi-hop, and numerical reasoning, and also provides annotated arithmetic
expressions of numerical answers for enhancing the ability of numerical
reasoning. Moreover, we developed a new end-to-end document VQA model that
treats evidence selection and question answering in a unified
sequence-to-sequence format. Experiments on SlideVQA show that our model
outperformed existing state-of-the-art QA models, but that it still has a large
gap behind human performance. We believe that our dataset will facilitate
research on document VQA.",None,-1
af88e78d-80e6-46c2-b925-5c174b7e26ac,DetermiNet: A Large-Scale Diagnostic Dataset for Complex Visually-Grounded Referencing using Determiners,0.344929,1,"State-of-the-art visual grounding models can achieve high detection accuracy,
but they are not designed to distinguish between all objects versus only
certain objects of interest. In natural language, in order to specify a
particular object or set of objects of interest, humans use determiners such as
""my"", ""either"" and ""those"". Determiners, as an important word class, are a type
of schema in natural language about the reference or quantity of the noun.
Existing grounded referencing datasets place much less emphasis on determiners,
compared to other word classes such as nouns, verbs and adjectives. This makes
it difficult to develop models that understand the full variety and complexity
of object referencing. Thus, we have developed and released the DetermiNet
dataset , which comprises 250,000 synthetically generated images and captions
based on 25 determiners. The task is to predict bounding boxes to identify
objects of interest, constrained by the semantics of the given determiner. We
find that current state-of-the-art visual grounding models do not perform well
on the dataset, highlighting the limitations of existing models on reference
and quantification tasks.",None,-1
6840e3f3-2b24-45dc-afd2-e933ad3d9a9c,Linking Alternative Fuel Vehicles Adoption with Socioeconomic Status and Air Quality Index,0.550671,2,"This is a study on the potential widespread usage of alternative fuel
vehicles, linking them with the socio-economic status of the respective
consumers as well as the impact on the resulting air quality index. Research in
this area aims to leverage machine learning techniques in order to promote
appropriate policies for the proliferation of alternative fuel vehicles such as
electric vehicles with due justice to different population groups. Pearson
correlation coefficient is deployed in the modeling the relationships between
socio-economic data, air quality index and data on alternative fuel vehicles.
Linear regression is used to conduct predictive modeling on air quality index
as per the adoption of alternative fuel vehicles, based on socio-economic
factors. This work exemplifies artificial intelligence for social good.",None,-1
2cced6ad-d68d-43cb-8848-771d1f3e11d9,FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model,0.677035,67,"Recently, conditional diffusion models have gained popularity in numerous
applications due to their exceptional generation ability. However, many
existing methods are training-required. They need to train a time-dependent
classifier or a condition-dependent score estimator, which increases the cost
of constructing conditional diffusion models and is inconvenient to transfer
across different conditions. Some current works aim to overcome this limitation
by proposing training-free solutions, but most can only be applied to a
specific category of tasks and not to more general conditions. In this work, we
propose a training-Free conditional Diffusion Model (FreeDoM) used for various
conditions. Specifically, we leverage off-the-shelf pre-trained networks, such
as a face detection model, to construct time-independent energy functions,
which guide the generation process without requiring training. Furthermore,
because the construction of the energy function is very flexible and adaptable
to various conditions, our proposed FreeDoM has a broader range of applications
than existing training-free methods. FreeDoM is advantageous in its simplicity,
effectiveness, and low cost. Experiments demonstrate that FreeDoM is effective
for various conditions and suitable for diffusion models of diverse data
domains, including image and latent code domains.",None,-1
e459f9de-0218-460c-b0f1-44db5ebb679b,Distributional Multi-Objective Decision Making,0.269101,2,"For effective decision support in scenarios with conflicting objectives, sets
of potentially optimal solutions can be presented to the decision maker. We
explore both what policies these sets should contain and how such sets can be
computed efficiently. With this in mind, we take a distributional approach and
introduce a novel dominance criterion relating return distributions of policies
directly. Based on this criterion, we present the distributional undominated
set and show that it contains optimal policies otherwise ignored by the Pareto
front. In addition, we propose the convex distributional undominated set and
prove that it comprises all policies that maximise expected utility for
multivariate risk-averse decision makers. We propose a novel algorithm to learn
the distributional undominated set and further contribute pruning operators to
reduce the set to the convex distributional undominated set. Through
experiments, we demonstrate the feasibility and effectiveness of these methods,
making this a valuable new approach for decision support in real-world
problems.",None,-1
efeb2ce9-8f50-47ff-af1e-9bed64f94293,Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models,0.133321,13,"Large Language Models (LLMs) have achieved remarkable success in reasoning
tasks with the development of prompting methods. However, existing prompting
approaches cannot reuse insights of solving similar problems and suffer from
accumulated errors in multi-step reasoning, since they prompt LLMs to reason
\textit{from scratch}. To address these issues, we propose
\textbf{\textit{Thought Propagation} (TP)}, which explores the analogous
problems and leverages their solutions to enhance the complex reasoning ability
of LLMs. These analogous problems are related to the input one, with reusable
solutions and problem-solving strategies. Thus, it is promising to propagate
insights of solving previous analogous problems to inspire new problem-solving.
To achieve this, TP first prompts LLMs to propose and solve a set of analogous
problems that are related to the input one. Then, TP reuses the results of
analogous problems to directly yield a new solution or derive a
knowledge-intensive plan for execution to amend the initial solution obtained
from scratch. TP is compatible with existing prompting approaches, allowing
plug-and-play generalization and enhancement in a wide range of tasks without
much labor in task-specific prompt engineering. Experiments across three
challenging tasks demonstrate TP enjoys a substantial improvement over the
baselines by an average of 12\% absolute increase in finding the optimal
solutions in Shortest-path Reasoning, 13\% improvement of human preference in
Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent
Planning.",None,-1
4c03c942-6228-4a74-80d1-3218b6828b4d,Multi-label Video Classification for Underwater Ship Inspection,0.248645,2,"Today ship hull inspection including the examination of the external coating,
detection of defects, and other types of external degradation such as corrosion
and marine growth is conducted underwater by means of Remotely Operated
Vehicles (ROVs). The inspection process consists of a manual video analysis
which is a time-consuming and labor-intensive process. To address this, we
propose an automatic video analysis system using deep learning and computer
vision to improve upon existing methods that only consider spatial information
on individual frames in underwater ship hull video inspection. By exploring the
benefits of adding temporal information and analyzing frame-based classifiers,
we propose a multi-label video classification model that exploits the
self-attention mechanism of transformers to capture spatiotemporal attention in
consecutive video frames. Our proposed method has demonstrated promising
results and can serve as a benchmark for future research and development in
underwater video inspection applications.",None,-1
bc5ccdec-85f9-4f75-b6ba-8d615b436ef2,HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition,0.445753,6,"State-of-the-art ASR systems have achieved promising results by modeling
local and global interactions separately. While the former can be computed
efficiently, global interactions are usually modeled via attention mechanisms,
which are expensive for long input sequences. Here, we address this by
extending HyperMixer, an efficient alternative to attention exhibiting linear
complexity, to the Conformer architecture for speech recognition, leading to
HyperConformer. In particular, multi-head HyperConformer achieves comparable or
higher recognition performance while being more efficient than Conformer in
terms of inference speed, memory, parameter count, and available training data.
HyperConformer achieves a word error rate of 2.9% on Librispeech test-clean
with less than 8M neural parameters and a peak memory during training of 5.7GB,
hence trainable with accessible hardware. Encoder speed is between 38% on
mid-length speech and 56% on long speech faster than an equivalent Conformer.
(The HyperConformer recipe is publicly available in:
https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriSpeech/ASR/transformer/)",None,-1
3609cba3-f5b8-4229-83b4-a7fdb02ad343,Generative Spoken Language Model based on continuous word-sized audio tokens,0.198475,3,"In NLP, text language models based on words or subwords are known to
outperform their character-based counterparts. Yet, in the speech community,
the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter
than a phoneme). Taking inspiration from word-based LM, we introduce a
Generative Spoken Language Model (GSLM) based on word-size continuous-valued
audio embeddings that can generate diverse and expressive language output. This
is obtained by replacing lookup table for lexical types with a Lexical
Embedding function, the cross entropy loss by a contrastive loss, and
multinomial sampling by k-NN sampling. The resulting model is the first
generative language model based on word-size continuous embeddings. Its
performance is on par with discrete unit GSLMs regarding generation quality as
measured by automatic metrics and subjective human judgements. Moreover, it is
five times more memory efficient thanks to its large 200ms units. In addition,
the embeddings before and after the Lexical Embedder are phonetically and
semantically interpretable.",None,-1
20a0c98c-1543-4fb1-a875-43dbe1ab0cf7,T1: Scaling Diffusion Probabilistic Fields to High-Resolution on Unified Visual Modalities,0.0643313,1,"Diffusion Probabilistic Field (DPF) models the distribution of continuous
functions defined over metric spaces. While DPF shows great potential for
unifying data generation of various modalities including images, videos, and 3D
geometry, it does not scale to a higher data resolution. This can be attributed
to the ``scaling property'', where it is difficult for the model to capture
local structures through uniform sampling. To this end, we propose a new model
comprising of a view-wise sampling algorithm to focus on local structure
learning, and incorporating additional guidance, e.g., text description, to
complement the global geometry. The model can be scaled to generate
high-resolution data while unifying multiple modalities. Experimental results
on data generation in various modalities demonstrate the effectiveness of our
model, as well as its potential as a foundation framework for scalable
modality-unified visual content generation.",None,-1
86cd77fa-a76a-49bd-9537-15b7a34e6dd7,Self-Influence Guided Data Reweighting for Language Model Pre-training,0.358725,8,"Language Models (LMs) pre-trained with self-supervision on large text corpora
have become the default starting point for developing models for various NLP
tasks. Once the pre-training corpus has been assembled, all data samples in the
corpus are treated with equal importance during LM pre-training. However, due
to varying levels of relevance and quality of data, equal importance to all the
data samples may not be the optimal choice. While data reweighting has been
explored in the context of task-specific supervised learning and LM
fine-tuning, model-driven reweighting for pre-training data has not been
explored. We fill this important gap and propose PRESENCE, a method for jointly
reweighting samples by leveraging self-influence (SI) scores as an indicator of
sample importance and pre-training. PRESENCE promotes novelty and stability for
model pre-training. Through extensive analysis spanning multiple model sizes,
datasets, and tasks, we present PRESENCE as an important first step in the
research direction of sample reweighting for pre-training language models.",None,-1
53cd14e9-3227-453e-b5cf-3acf16bd47bd,SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation,0.479121,11,"This paper studies referring video object segmentation (RVOS) by boosting
video-level visual-linguistic alignment. Recent approaches model the RVOS task
as a sequence prediction problem and perform multi-modal interaction as well as
segmentation for each frame separately. However, the lack of a global view of
video content leads to difficulties in effectively utilizing inter-frame
relationships and understanding textual descriptions of object temporal
variations. To address this issue, we propose Semantic-assisted Object Cluster
(SOC), which aggregates video content and textual guidance for unified temporal
modeling and cross-modal alignment. By associating a group of frame-level
object embeddings with language tokens, SOC facilitates joint space learning
across modalities and time steps. Moreover, we present multi-modal contrastive
supervision to help construct well-aligned joint space at the video level. We
conduct extensive experiments on popular RVOS benchmarks, and our method
outperforms state-of-the-art competitors on all benchmarks by a remarkable
margin. Besides, the emphasis on temporal coherence enhances the segmentation
stability and adaptability of our method in processing text expressions with
temporal variations. Code will be available.",None,-1
1a4ea541-b398-4feb-84c9-62727ab986f1,Decision-Making Under Uncertainty: Beyond Probabilities,0.654283,8,"This position paper reflects on the state-of-the-art in decision-making under
uncertainty. A classical assumption is that probabilities can sufficiently
capture all uncertainty in a system. In this paper, the focus is on the
uncertainty that goes beyond this classical interpretation, particularly by
employing a clear distinction between aleatoric and epistemic uncertainty. The
paper features an overview of Markov decision processes (MDPs) and extensions
to account for partial observability and adversarial behavior. These models
sufficiently capture aleatoric uncertainty but fail to account for epistemic
uncertainty robustly. Consequently, we present a thorough overview of so-called
uncertainty models that exhibit uncertainty in a more robust interpretation. We
show several solution techniques for both discrete and continuous models,
ranging from formal verification, over control-based abstractions, to
reinforcement learning. As an integral part of this paper, we list and discuss
several key challenges that arise when dealing with rich types of uncertainty
in a model-based fashion.",None,-1
1082180b-9fc7-479c-b7e8-e735fa9301a1,MedDiff: Generating Electronic Health Records using Accelerated Denoising Diffusion Model,0.900333,18,"Due to patient privacy protection concerns, machine learning research in
healthcare has been undeniably slower and limited than in other application
domains. High-quality, realistic, synthetic electronic health records (EHRs)
can be leveraged to accelerate methodological developments for research
purposes while mitigating privacy concerns associated with data sharing. The
current state-of-the-art model for synthetic EHR generation is generative
adversarial networks, which are notoriously difficult to train and can suffer
from mode collapse. Denoising Diffusion Probabilistic Models, a class of
generative models inspired by statistical thermodynamics, have recently been
shown to generate high-quality synthetic samples in certain domains. It is
unknown whether these can generalize to generation of large-scale,
high-dimensional EHRs. In this paper, we present a novel generative model based
on diffusion models that is the first successful application on electronic
health records. Our model proposes a mechanism to perform class-conditional
sampling to preserve label information. We also introduce a new sampling
strategy to accelerate the inference speed. We empirically show that our model
outperforms existing state-of-the-art synthetic EHR generation methods.",None,-1
ee0666d5-5ceb-4f89-b8d2-7db84f07f11b,Deep Learning Mental Health Dialogue System,0.942598,8,"Mental health counseling remains a major challenge in modern society due to
cost, stigma, fear, and unavailability. We posit that generative artificial
intelligence (AI) models designed for mental health counseling could help
improve outcomes by lowering barriers to access. To this end, we have developed
a deep learning (DL) dialogue system called Serena. The system consists of a
core generative model and post-processing algorithms. The core generative model
is a 2.7 billion parameter Seq2Seq Transformer fine-tuned on thousands of
transcripts of person-centered-therapy (PCT) sessions. The series of
post-processing algorithms detects contradictions, improves coherency, and
removes repetitive answers. Serena is implemented and deployed on
\url{https://serena.chat}, which currently offers limited free services. While
the dialogue system is capable of responding in a qualitatively empathetic and
engaging manner, occasionally it displays hallucination and long-term
incoherence. Overall, we demonstrate that a deep learning mental health
dialogue system has the potential to provide a low-cost and effective
complement to traditional human counselors with less barriers to access.",None,-1
aefe32e1-5105-4422-881e-5638c59a22d6,Structured Thoughts Automaton: First Formalized Execution Model for Auto-Regressive Language Models,0.2957,2,"In recent months, Language Models (LMs) have become a part of daily
discourse, with focus on OpenAI and the potential of Artificial General
Intelligence (AGI). Furthermore, the leaking of LLama's weights to the public
has led to an influx of innovations demonstrating the impressive capabilities
of generative LMs. While we believe that AGI is still a distant goal, we
recognize the potential of LMs in solving tasks such as searching complex
documents, compiling reports with basic analysis, and providing assistance in
problem-solving. In this paper, we propose formalizing the execution model of
language models. We investigate current execution models, to find that this
formalism has received little attention, and present our contribution: the
first formalized execution model for LMs. We introduce a new algorithm for
sampling the predictions of LMs, which we use to build a reliable and
inspectable execution model. We introduce a low-level language to write
""cognitive program"" for this execution model. We hope to shed light on the need
for execution models for LMs and encourage further research in this area.",None,-1
d9cec052-6f8c-4fa9-936b-d3f3a1908965,SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input Views,0.0754857,2,"We study to generate novel views of indoor scenes given sparse input views.
The challenge is to achieve both photorealism and view consistency. We present
SparseGNV: a learning framework that incorporates 3D structures and image
generative models to generate novel views with three modules. The first module
builds a neural point cloud as underlying geometry, providing contextual
information and guidance for the target novel view. The second module utilizes
a transformer-based network to map the scene context and the guidance into a
shared latent space and autoregressively decodes the target view in the form of
discrete image tokens. The third module reconstructs the tokens into the image
of the target view. SparseGNV is trained across a large indoor scene dataset to
learn generalizable priors. Once trained, it can efficiently generate novel
views of an unseen indoor scene in a feed-forward manner. We evaluate SparseGNV
on both real-world and synthetic indoor scenes and demonstrate that it
outperforms state-of-the-art methods based on either neural radiance fields or
conditional image generation.",None,-1
eec7e77f-96f4-4d9e-9b75-ae818fa3fb27,Spatiotemporally Consistent HDR Indoor Lighting Estimation,0.758518,4,"We propose a physically-motivated deep learning framework to solve a general
version of the challenging indoor lighting estimation problem. Given a single
LDR image with a depth map, our method predicts spatially consistent lighting
at any given image position. Particularly, when the input is an LDR video
sequence, our framework not only progressively refines the lighting prediction
as it sees more regions, but also preserves temporal consistency by keeping the
refinement smooth. Our framework reconstructs a spherical Gaussian lighting
volume (SGLV) through a tailored 3D encoder-decoder, which enables spatially
consistent lighting prediction through volume ray tracing, a hybrid blending
network for detailed environment maps, an in-network Monte-Carlo rendering
layer to enhance photorealism for virtual object insertion, and recurrent
neural networks (RNN) to achieve temporally consistent lighting prediction with
a video sequence as the input. For training, we significantly enhance the
OpenRooms public dataset of photorealistic synthetic indoor scenes with around
360K HDR environment maps of much higher resolution and 38K video sequences,
rendered with GPU-based path tracing. Experiments show that our framework
achieves lighting prediction with higher quality compared to state-of-the-art
single-image or video-based methods, leading to photorealistic AR applications
such as object insertion.",None,-1
34732c58-4a07-4322-9965-97578f8e0793,Backdoor Defense via Deconfounded Representation Learning,0.763817,14,"Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor
attacks, where attackers embed hidden backdoors in the DNN model by injecting a
few poisoned examples into the training dataset. While extensive efforts have
been made to detect and remove backdoors from backdoored DNNs, it is still not
clear whether a backdoor-free clean model can be directly obtained from
poisoned datasets. In this paper, we first construct a causal graph to model
the generation process of poisoned data and find that the backdoor attack acts
as the confounder, which brings spurious associations between the input images
and target labels, making the model predictions less reliable. Inspired by the
causal understanding, we propose the Causality-inspired Backdoor Defense (CBD),
to learn deconfounded representations for reliable classification.
Specifically, a backdoored model is intentionally trained to capture the
confounding effects. The other clean model dedicates to capturing the desired
causal effects by minimizing the mutual information with the confounding
representations from the backdoored model and employing a sample-wise
re-weighting scheme. Extensive experiments on multiple benchmark datasets
against 6 state-of-the-art attacks verify that our proposed defense method is
effective in reducing backdoor threats while maintaining high accuracy in
predicting benign samples. Further analysis shows that CBD can also resist
potential adaptive attacks. The code is available at
\url{https://github.com/zaixizhang/CBD}.",None,-1
f55cee29-4381-45ff-aa3c-77fc6160b341,Effect of latent space distribution on the segmentation of images with multiple annotations,0.0960392,3,"We propose the Generalized Probabilistic U-Net, which extends the
Probabilistic U-Net by allowing more general forms of the Gaussian distribution
as the latent space distribution that can better approximate the uncertainty in
the reference segmentations. We study the effect the choice of latent space
distribution has on capturing the variation in the reference segmentations for
lung tumors and white matter hyperintensities in the brain. We show that the
choice of distribution affects the sample diversity of the predictions and
their overlap with respect to the reference segmentations. We have made our
implementation available at
https://github.com/ishaanb92/GeneralizedProbabilisticUNet",None,-1
25682e1a-cbde-45ff-a98b-3bc368356fec,Knowledge Graphs: Opportunities and Challenges,0.994904,67,"With the explosive growth of artificial intelligence (AI) and big data, it
has become vitally important to organize and represent the enormous volume of
knowledge appropriately. As graph data, knowledge graphs accumulate and convey
knowledge of the real world. It has been well-recognized that knowledge graphs
effectively represent complex information; hence, they rapidly gain the
attention of academia and industry in recent years. Thus to develop a deeper
understanding of knowledge graphs, this paper presents a systematic overview of
this field. Specifically, we focus on the opportunities and challenges of
knowledge graphs. We first review the opportunities of knowledge graphs in
terms of two aspects: (1) AI systems built upon knowledge graphs; (2) potential
application fields of knowledge graphs. Then, we thoroughly discuss severe
technical challenges in this field, such as knowledge graph embeddings,
knowledge acquisition, knowledge graph completion, knowledge fusion, and
knowledge reasoning. We expect that this survey will shed new light on future
research and the development of knowledge graphs.",None,-1
1123bc36-a85e-4fd9-a4ff-8668622fb17d,Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,0.994785,127,"This paper demonstrates an approach for learning highly semantic image
representations without relying on hand-crafted data-augmentations. We
introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a
non-generative approach for self-supervised learning from images. The idea
behind I-JEPA is simple: from a single context block, predict the
representations of various target blocks in the same image. A core design
choice to guide I-JEPA towards producing semantic representations is the
masking strategy; specifically, it is crucial to (a) sample target blocks with
sufficiently large scale (semantic), and to (b) use a sufficiently informative
(spatially distributed) context block. Empirically, when combined with Vision
Transformers, we find I-JEPA to be highly scalable. For instance, we train a
ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong
downstream performance across a wide range of tasks, from linear classification
to object counting and depth prediction.",None,-1
cee00b2d-e747-4eb4-997a-7ddc991a7900,Neural Episodic Control with State Abstraction,0.346652,8,"Existing Deep Reinforcement Learning (DRL) algorithms suffer from sample
inefficiency. Generally, episodic control-based approaches are solutions that
leverage highly-rewarded past experiences to improve sample efficiency of DRL
algorithms. However, previous episodic control-based approaches fail to utilize
the latent information from the historical behaviors (e.g., state transitions,
topological similarities, etc.) and lack scalability during DRL training. This
work introduces Neural Episodic Control with State Abstraction (NECSA), a
simple but effective state abstraction-based episodic control containing a more
comprehensive episodic memory, a novel state evaluation, and a multi-step state
analysis. We evaluate our approach to the MuJoCo and Atari tasks in OpenAI gym
domains. The experimental results indicate that NECSA achieves higher sample
efficiency than the state-of-the-art episodic control-based approaches. Our
data and code are available at the project
website\footnote{\url{https://sites.google.com/view/drl-necsa}}.",None,-1
d8ed4701-f02a-4614-ab07-d55d9680dbb8,Multi-Scale Occ: 4th Place Solution for CVPR 2023 3D Occupancy Prediction Challenge,0.400175,4,"In this report, we present the 4th place solution for CVPR 2023 3D occupancy
prediction challenge. We propose a simple method called Multi-Scale Occ for
occupancy prediction based on lift-splat-shoot framework, which introduces
multi-scale image features for generating better multi-scale 3D voxel features
with temporal fusion of multiple past frames. Post-processing including model
ensemble, test-time augmentation, and class-wise thresh are adopted to further
boost the final performance. As shown on the leaderboard, our proposed
occupancy prediction method ranks the 4th place with 49.36 mIoU.",None,-1
1cacd27e-1f34-4029-9d10-1333527bf39b,NormMark: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery,0.961583,4,"Norms, which are culturally accepted guidelines for behaviours, can be
integrated into conversational models to generate utterances that are
appropriate for the socio-cultural context. Existing methods for norm
recognition tend to focus only on surface-level features of dialogues and do
not take into account the interactions within a conversation. To address this
issue, we propose NormMark, a probabilistic generative Markov model to carry
the latent features throughout a dialogue. These features are captured by
discrete and continuous latent variables conditioned on the conversation
history, and improve the model's ability in norm recognition. The model is
trainable on weakly annotated data using the variational technique. On a
dataset with limited norm annotations, we show that our approach achieves
higher F1 score, outperforming current state-of-the-art methods, including
GPT3.",None,-1
111f7146-c3c3-40be-9c6d-135feca19dae,Separating Invisible Sounds Toward Universal Audiovisual Scene-Aware Sound Separation,0.867137,2,"The audio-visual sound separation field assumes visible sources in videos,
but this excludes invisible sounds beyond the camera's view. Current methods
struggle with such sounds lacking visible cues. This paper introduces a novel
""Audio-Visual Scene-Aware Separation"" (AVSA-Sep) framework. It includes a
semantic parser for visible and invisible sounds and a separator for
scene-informed separation. AVSA-Sep successfully separates both sound types,
with joint training and cross-modal alignment enhancing effectiveness.",None,-1
17cfa229-95d9-46d7-9561-905462d20bd0,Multimodal Subtask Graph Generation from Instructional Videos,0.694446,6,"Real-world tasks consist of multiple inter-dependent subtasks (e.g., a dirty
pan needs to be washed before it can be used for cooking). In this work, we aim
to model the causal dependencies between such subtasks from instructional
videos describing the task. This is a challenging problem since complete
information about the world is often inaccessible from videos, which demands
robust learning mechanisms to understand the causal structure of events. We
present Multimodal Subtask Graph Generation (MSG2), an approach that constructs
a Subtask Graph defining the dependency between a task's subtasks relevant to a
task from noisy web videos. Graphs generated by our multimodal approach are
closer to human-annotated graphs compared to prior approaches. MSG2 further
performs the downstream task of next subtask prediction 85% and 30% more
accurately than recent video transformer models in the ProceL and CrossTask
datasets, respectively.",None,-1
07821150-0c2d-46ea-acc1-8d8f7f0ad66f,Can SAM Segment Polyps?,0.983868,45,"Recently, Meta AI Research releases a general Segment Anything Model (SAM),
which has demonstrated promising performance in several segmentation tasks. As
we know, polyp segmentation is a fundamental task in the medical imaging field,
which plays a critical role in the diagnosis and cure of colorectal cancer. In
particular, applying SAM to the polyp segmentation task is interesting. In this
report, we evaluate the performance of SAM in segmenting polyps, in which SAM
is under unprompted settings. We hope this report will provide insights to
advance this polyp segmentation field and promote more interesting works in the
future. This project is publicly at https://github.com/taozh2017/SAMPolyp.",None,-1
448de281-1067-475c-b4e1-9452c1741433,Meta-Learned Models of Cognition,0.725674,15,"Meta-learning is a framework for learning learning algorithms through
repeated interactions with an environment as opposed to designing them by hand.
In recent years, this framework has established itself as a promising tool for
building models of human cognition. Yet, a coherent research program around
meta-learned models of cognition is still missing. The purpose of this article
is to synthesize previous work in this field and establish such a research
program. We rely on three key pillars to accomplish this goal. We first point
out that meta-learning can be used to construct Bayes-optimal learning
algorithms. This result not only implies that any behavioral phenomenon that
can be explained by a Bayesian model can also be explained by a meta-learned
model but also allows us to draw strong connections to the rational analysis of
cognition. We then discuss several advantages of the meta-learning framework
over traditional Bayesian methods. In particular, we argue that meta-learning
can be applied to situations where Bayesian inference is impossible and that it
enables us to make rational models of cognition more realistic, either by
incorporating limited computational resources or neuroscientific knowledge.
Finally, we reexamine prior studies from psychology and neuroscience that have
applied meta-learning and put them into the context of these new insights. In
summary, our work highlights that meta-learning considerably extends the scope
of rational analysis and thereby of cognitive theories more generally.",None,-1
55216313-4c32-4a6b-ad86-690389bff7c7,Multi-modal Latent Space Learning for Chain-of-Thought Reasoning in Language Models,0.817643,3,"Chain-of-thought (CoT) reasoning has exhibited impressive performance in
language models for solving complex tasks and answering questions. However,
many real-world questions require multi-modal information, such as text and
images. Previous research on multi-modal CoT has primarily focused on
extracting fixed image features from off-the-shelf vision models and then
fusing them with text using attention mechanisms. This approach has limitations
because these vision models were not designed for complex reasoning tasks and
do not align well with language thoughts. To overcome this limitation, we
introduce a novel approach for multi-modal CoT reasoning that utilizes latent
space learning via diffusion processes to generate effective image features
that align with language thoughts. Our method fuses image features and text
representations at a deep level and improves the complex reasoning ability of
multi-modal CoT. We demonstrate the efficacy of our proposed method on
multi-modal ScienceQA and machine translation benchmarks, achieving
state-of-the-art performance on ScienceQA. Overall, our approach offers a more
robust and effective solution for multi-modal reasoning in language models,
enhancing their ability to tackle complex real-world problems.",None,-1
fee9de19-890d-4f7b-8ff6-24b0b21bd493,Can GPT-3 Perform Statutory Reasoning?,0.655884,57,"Statutory reasoning is the task of reasoning with facts and statutes, which
are rules written in natural language by a legislature. It is a basic legal
skill. In this paper we explore the capabilities of the most capable GPT-3
model, text-davinci-003, on an established statutory-reasoning dataset called
SARA. We consider a variety of approaches, including dynamic few-shot
prompting, chain-of-thought prompting, and zero-shot prompting. While we
achieve results with GPT-3 that are better than the previous best published
results, we also identify several types of clear errors it makes. We
investigate why these errors happen. We discover that GPT-3 has imperfect prior
knowledge of the actual U.S. statutes on which SARA is based. More importantly,
we create simple synthetic statutes, which GPT-3 is guaranteed not to have seen
during training. We find GPT-3 performs poorly at answering straightforward
questions about these simple synthetic statutes.",None,-1
2ec4e85d-3029-4b05-bac0-cf6c00a3aa2a,Syntactically Robust Training on Partially-Observed Data for Open Information Extraction,0.516583,4,"Open Information Extraction models have shown promising results with
sufficient supervision. However, these models face a fundamental challenge that
the syntactic distribution of training data is partially observable in
comparison to the real world. In this paper, we propose a syntactically robust
training framework that enables models to be trained on a syntactic-abundant
distribution based on diverse paraphrase generation. To tackle the intrinsic
problem of knowledge deformation of paraphrasing, two algorithms based on
semantic similarity matching and syntactic tree walking are used to restore the
expressionally transformed knowledge. The training framework can be generally
applied to other syntactic partial observable domains. Based on the proposed
framework, we build a new evaluation set called CaRB-AutoPara, a syntactically
diverse dataset consistent with the real-world setting for validating the
robustness of the models. Experiments including a thorough analysis show that
the performance of the model degrades with the increase of the difference in
syntactic distribution, while our framework gives a robust boundary. The source
code is publicly available at https://github.com/qijimrc/RobustOIE.",None,-1
e7fd5928-e3cb-4714-82b2-0f465ed16f58,Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-identification,0.998303,34,"For the visible-infrared person re-identification (VIReID) task, one of the
major challenges is the modality gaps between visible (VIS) and infrared (IR)
images. However, the training samples are usually limited, while the modality
gaps are too large, which leads that the existing methods cannot effectively
mine diverse cross-modality clues. To handle this limitation, we propose a
novel augmentation network in the embedding space, called diverse embedding
expansion network (DEEN). The proposed DEEN can effectively generate diverse
embeddings to learn the informative feature representations and reduce the
modality discrepancy between the VIS and IR images. Moreover, the VIReID model
may be seriously affected by drastic illumination changes, while all the
existing VIReID datasets are captured under sufficient illumination without
significant light changes. Thus, we provide a low-light cross-modality (LLCM)
dataset, which contains 46,767 bounding boxes of 1,064 identities captured by 9
RGB/IR cameras. Extensive experiments on the SYSU-MM01, RegDB and LLCM datasets
show the superiority of the proposed DEEN over several other state-of-the-art
methods. The code and dataset are released at: https://github.com/ZYK100/LLCM",None,-1
0a2e20b0-0bdc-4398-a6be-60fdc74b7920,Generalized Universal Domain Adaptation with Generative Flow Networks,0.483344,7,"We introduce a new problem in unsupervised domain adaptation, termed as
Generalized Universal Domain Adaptation (GUDA), which aims to achieve precise
prediction of all target labels including unknown categories. GUDA bridges the
gap between label distribution shift-based and label space mismatch-based
variants, essentially categorizing them as a unified problem, guiding to a
comprehensive framework for thoroughly solving all the variants. The key
challenge of GUDA is developing and identifying novel target categories while
estimating the target label distribution. To address this problem, we take
advantage of the powerful exploration capability of generative flow networks
and propose an active domain adaptation algorithm named GFlowDA, which selects
diverse samples with probabilities proportional to a reward function. To
enhance the exploration capability and effectively perceive the target label
distribution, we tailor the states and rewards, and introduce an efficient
solution for parent exploration and state transition. We also propose a
training paradigm for GUDA called Generalized Universal Adversarial Network
(GUAN), which involves collaborative optimization between GUAN and GFlowNet.
Theoretical analysis highlights the importance of exploration, and extensive
experiments on benchmark datasets demonstrate the superiority of GFlowDA.",None,-1
1a8f8059-fa6f-41ac-aef5-93e13646e2fe,System-status-aware Adaptive Network for Online Streaming Video Understanding,0.453128,9,"Recent years have witnessed great progress in deep neural networks for
real-time applications. However, most existing works do not explicitly consider
the general case where the device's state and the available resources fluctuate
over time, and none of them investigate or address the impact of varying
computational resources for online video understanding tasks. This paper
proposes a System-status-aware Adaptive Network (SAN) that considers the
device's real-time state to provide high-quality predictions with low delay.
Usage of our agent's policy improves efficiency and robustness to fluctuations
of the system status. On two widely used video understanding tasks, SAN obtains
state-of-the-art performance while constantly keeping processing delays low.
Moreover, training such an agent on various types of hardware configurations is
not easy as the labeled training data might not be available, or can be
computationally prohibitive. To address this challenging problem, we propose a
Meta Self-supervised Adaptation (MSA) method that adapts the agent's policy to
new hardware configurations at test-time, allowing for easy deployment of the
model onto other unseen hardware platforms.",None,-1
2d3e37f6-2701-4471-947e-a40145faca14,Weakly Supervised Monocular 3D Object Detection using Multi-View Projection and Direction Consistency,0.673689,7,"Monocular 3D object detection has become a mainstream approach in automatic
driving for its easy application. A prominent advantage is that it does not
need LiDAR point clouds during the inference. However, most current methods
still rely on 3D point cloud data for labeling the ground truths used in the
training phase. This inconsistency between the training and inference makes it
hard to utilize the large-scale feedback data and increases the data collection
expenses. To bridge this gap, we propose a new weakly supervised monocular 3D
objection detection method, which can train the model with only 2D labels
marked on images. To be specific, we explore three types of consistency in this
task, i.e. the projection, multi-view and direction consistency, and design a
weakly-supervised architecture based on these consistencies. Moreover, we
propose a new 2D direction labeling method in this task to guide the model for
accurate rotation direction prediction. Experiments show that our
weakly-supervised method achieves comparable performance with some fully
supervised methods. When used as a pre-training method, our model can
significantly outperform the corresponding fully-supervised baseline with only
1/3 3D labels. https://github.com/weakmono3d/weakmono3d",None,-1
a164849c-f363-4c2d-b513-155eaeef2e23,SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues,0.816174,5,"In second language vocabulary learning, existing works have primarily focused
on either the learning interface or scheduling personalized retrieval practices
to maximize memory retention. However, the learning content, i.e., the
information presented on flashcards, has mostly remained constant. Keyword
mnemonic is a notable learning strategy that relates new vocabulary to existing
knowledge by building an acoustic and imagery link using a keyword that sounds
alike. Beyond that, producing verbal and visual cues associated with the
keyword to facilitate building these links requires a manual process and is not
scalable. In this paper, we explore an opportunity to use large language models
to automatically generate verbal and visual cues for keyword mnemonics. Our
approach, an end-to-end pipeline for auto-generating verbal and visual cues,
can automatically generate highly memorable cues. We investigate the
effectiveness of our approach via a human participant experiment by comparing
it with manually generated cues.",None,-1
0db9dd00-daec-46cf-bf0a-28257ead3523,Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet,0.737485,26,"The rapid adoption of generative Artificial Intelligence (AI) tools that can
generate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have
put the societal impacts of these technologies at the center of public debate.
These tools are possible due to the massive amount of data (text and images)
that is publicly available through the Internet. At the same time, these
generative AI tools become content creators that are already contributing to
the data that is available to train future models. Therefore, future versions
of generative AI tools will be trained with a mix of human-created and
AI-generated content, causing a potential feedback loop between generative AI
and public data repositories. This interaction raises many questions: how will
future versions of generative AI tools behave when trained on a mixture of real
and AI generated data? Will they evolve and improve with the new data sets or
on the contrary will they degrade? Will evolution introduce biases or reduce
diversity in subsequent generations of generative AI tools? What are the
societal implications of the possible degradation of these models? Can we
mitigate the effects of this feedback loop? In this document, we explore the
effect of this interaction and report some initial results using simple
diffusion models trained with various image datasets. Our results show that the
quality and diversity of the generated images can degrade over time suggesting
that incorporating AI-created data can have undesired effects on future
versions of generative models.",None,-1
5341b3ae-61d4-4599-9f35-19e085c04e7e,DiffusionPoser: Real-time Human Motion Reconstruction From Arbitrary Sparse Sensors Using Autoregressive Diffusion,0.104187,1,"Motion capture from a limited number of body-worn sensors, such as inertial
measurement units (IMUs) and pressure insoles, has important applications in
health, human performance, and entertainment. Recent work has focused on
accurately reconstructing whole-body motion from a specific sensor
configuration using six IMUs. While a common goal across applications is to use
the minimal number of sensors to achieve required accuracy, the optimal
arrangement of the sensors might differ from application to application. We
propose a single diffusion model, DiffusionPoser, which reconstructs human
motion in real-time from an arbitrary combination of sensors, including IMUs
placed at specified locations, and, pressure insoles. Unlike existing methods,
our model grants users the flexibility to determine the number and arrangement
of sensors tailored to the specific activity of interest, without the need for
retraining. A novel autoregressive inferencing scheme ensures real-time motion
reconstruction that closely aligns with measured sensor signals. The generative
nature of DiffusionPoser ensures realistic behavior, even for
degrees-of-freedom not directly measured. Qualitative results can be found on
our website: https://diffusionposer.github.io/.",None,-1
792e3d30-4093-49a0-83f2-a3ecc79fec89,Hint of Thought prompting: an explainable and zero-shot approach to reasoning tasks with LLMs,0.0465604,2,"As a way of communicating with users and any LLMs like GPT or PaLM2,
prompting becomes an increasingly important research topic for better
utilization of LLMs. Although simple prompting performs well on single-step
questions, it cannot permanently activate the correct knowledge path for
multi-step reasoning tasks. The chain of thought (CoT), which often contains
zero-shot CoT and few-shot CoT, is a recently developed prompting method that
can explain the reasoning process to the LLM and outperforms simple prompting
in three challenging reasoning tasks, including arithmetic, symbolic, and
commonsense reasoning. In this paper, we propose a novel hint of thought (HoT)
prompting with explainability and zero-shot generalization. First, it is
decomposed into the following three steps: explainable sub-questions, logical
reasoning, and answer extraction. Second, such three steps are sequentially
ordered in the format of step-by-step hints, which can be easily adjusted and
explained to different tasks. Finally, experimental results demonstrate that
our HoT prompting has a significant advantage on the zero-shot reasoning task
compared to existing zero-shot CoT. We did zero-shot experiments on math tasks
like GSM8K, ADDSUB, AQUA, SVAMP and commonsense tasks such as StrategyQA. In
particular, the accuracy of the proposed HoT prompting is improved with GSM8K
from 40.50% to 67.80%, with AQUA from 31.9% to 46.4%, with SVAMP from 63.7% to
76.9%, and with ADDSUB from 74.7% to 87.34%, respectively, which even defeats
the competitive PoT approach on GSM8k, AQUA, and SVAMP.",None,-1
42ebdd39-2154-498a-b19b-9dd8bd9d897f,HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation,0.814038,11,"Monocular 3D human pose and shape estimation is an ill-posed problem since
multiple 3D solutions can explain a 2D image of a subject. Recent approaches
predict a probability distribution over plausible 3D pose and shape parameters
conditioned on the image. We show that these approaches exhibit a trade-off
between three key properties: (i) accuracy - the likelihood of the ground-truth
3D solution under the predicted distribution, (ii) sample-input consistency -
the extent to which 3D samples from the predicted distribution match the
visible 2D image evidence, and (iii) sample diversity - the range of plausible
3D solutions modelled by the predicted distribution. Our method, HuManiFlow,
predicts simultaneously accurate, consistent and diverse distributions. We use
the human kinematic tree to factorise full body pose into ancestor-conditioned
per-body-part pose distributions in an autoregressive manner. Per-body-part
distributions are implemented using normalising flows that respect the manifold
structure of SO(3), the Lie group of per-body-part poses. We show that
ill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,
and employ only probabilistic training losses. Code is available at:
https://github.com/akashsengupta1997/HuManiFlow.",None,-1
7f27630c-65b1-462b-ad1c-11a8d01f9f27,Testing the Channels of Convolutional Neural Networks,0.0419453,1,"Neural networks have complex structures, and thus it is hard to understand
their inner workings and ensure correctness. To understand and debug
convolutional neural networks (CNNs) we propose techniques for testing the
channels of CNNs. We design FtGAN, an extension to GAN, that can generate test
data with varying the intensity (i.e., sum of the neurons) of a channel of a
target CNN. We also proposed a channel selection algorithm to find
representative channels for testing. To efficiently inspect the target CNN's
inference computations, we define unexpectedness score, which estimates how
similar the inference computation of the test data is to that of the training
data. We evaluated FtGAN with five public datasets and showed that our
techniques successfully identify defective channels in five different CNN
models.",None,-1
1788a401-9851-47db-aaed-26c8cc8ea901,FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge,0.81512,10,"Detecting factual errors in textual information, whether generated by large
language models (LLM) or curated by humans, is crucial for making informed
decisions. LLMs' inability to attribute their claims to external knowledge and
their tendency to hallucinate makes it difficult to rely on their responses.
Humans, too, are prone to factual errors in their writing. Since manual
detection and correction of factual errors is labor-intensive, developing an
automatic approach can greatly reduce human effort. We present FLEEK, a
prototype tool that automatically extracts factual claims from text, gathers
evidence from external knowledge sources, evaluates the factuality of each
claim, and suggests revisions for identified errors using the collected
evidence. Initial empirical evaluation on fact error detection (77-85\% F1)
shows the potential of FLEEK. A video demo of FLEEK can be found at
https://youtu.be/NapJFUlkPdQ.",None,-1
ff6b941a-5440-4706-bd5a-89bec0588c33,PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs,0.219989,1,"Temporal facts, the facts for characterizing events that hold in specific
time periods, are attracting rising attention in the knowledge graph (KG)
research communities. In terms of quality management, the introduction of time
restrictions brings new challenges to maintaining the temporal consistency of
KGs and detecting potential temporal conflicts. Previous studies rely on
manually enumerated temporal constraints to detect conflicts, which are
labor-intensive and may have granularity issues. We start from the common
pattern of temporal facts and constraints and propose a pattern-based temporal
constraint mining method, PaTeCon. PaTeCon uses automatically determined graph
patterns and their relevant statistical information over the given KG instead
of human experts to generate time constraints. Specifically, PaTeCon
dynamically attaches class restriction to candidate constraints according to
their measuring scores.We evaluate PaTeCon on two large-scale datasets based on
Wikidata and Freebase respectively. The experimental results show that
pattern-based automatic constraint mining is powerful in generating valuable
temporal constraints.",None,-1
25aa282d-2848-4100-9a36-bc21ec3700ea,Scalable Knowledge Graph Construction and Inference on Human Genome Variants,0.439751,1,"Real-world knowledge can be represented as a graph consisting of entities and
relationships between the entities. The need for efficient and scalable
solutions arises when dealing with vast genomic data, like RNA-sequencing.
Knowledge graphs offer a powerful approach for various tasks in such
large-scale genomic data, such as analysis and inference. In this work,
variant-level information extracted from the RNA-sequences of vaccine-na\""ive
COVID-19 patients have been represented as a unified, large knowledge graph.
Variant call format (VCF) files containing the variant-level information were
annotated to include further information for each variant. The data records in
the annotated files were then converted to Resource Description Framework (RDF)
triples. Each VCF file obtained had an associated CADD scores file that
contained the raw and Phred-scaled scores for each variant. An ontology was
defined for the VCF and CADD scores files. Using this ontology and the
extracted information, a large, scalable knowledge graph was created. Available
graph storage was then leveraged to query and create datasets for further
downstream tasks. We also present a case study using the knowledge graph and
perform a classification task using graph machine learning. We also draw
comparisons between different Graph Neural Networks (GNNs) for the case study.",None,-1
5107cfa5-73e0-444f-a6f3-94f4ae6b7d56,MatFuse: Controllable Material Generation with Diffusion Models,0.877781,8,"Creating high-quality materials in computer graphics is a challenging and
time-consuming task, which requires great expertise. To simplify this process,
we introduce MatFuse, a unified approach that harnesses the generative power of
diffusion models for creation and editing of 3D materials. Our method
integrates multiple sources of conditioning, including color palettes,
sketches, text, and pictures, enhancing creative possibilities and granting
fine-grained control over material synthesis. Additionally, MatFuse enables
map-level material editing capabilities through latent manipulation by means of
a multi-encoder compression model which learns a disentangled latent
representation for each map. We demonstrate the effectiveness of MatFuse under
multiple conditioning settings and explore the potential of material editing.
Finally, we assess the quality of the generated materials both quantitatively
in terms of CLIP-IQA and FID scores and qualitatively by conducting a user
study. Source code for training MatFuse and supplemental materials are publicly
available at https://gvecchio.com/matfuse.",None,-1
516fa994-9136-4539-abaa-b5ec7c35c491,OpenContrails: Benchmarking Contrail Detection on GOES-16 ABI,0.27567,13,"Contrails (condensation trails) are line-shaped ice clouds caused by aircraft
and are likely the largest contributor of aviation-induced climate change.
Contrail avoidance is potentially an inexpensive way to significantly reduce
the climate impact of aviation. An automated contrail detection system is an
essential tool to develop and evaluate contrail avoidance systems. In this
paper, we present a human-labeled dataset named OpenContrails to train and
evaluate contrail detection models based on GOES-16 Advanced Baseline Imager
(ABI) data. We propose and evaluate a contrail detection model that
incorporates temporal context for improved detection accuracy. The human
labeled dataset and the contrail detection outputs are publicly available on
Google Cloud Storage at gs://goes_contrails_dataset.",None,-1
548c7c1a-f9e7-48ed-a6eb-96148c1a19a2,Hyperbolic Audio-visual Zero-shot Learning,0.239753,3,"Audio-visual zero-shot learning aims to classify samples consisting of a pair
of corresponding audio and video sequences from classes that are not present
during training. An analysis of the audio-visual data reveals a large degree of
hyperbolicity, indicating the potential benefit of using a hyperbolic
transformation to achieve curvature-aware geometric learning, with the aim of
exploring more complex hierarchical data structures for this task. The proposed
approach employs a novel loss function that incorporates cross-modality
alignment between video and audio features in the hyperbolic space.
Additionally, we explore the use of multiple adaptive curvatures for hyperbolic
projections. The experimental results on this very challenging task demonstrate
that our proposed hyperbolic approach for zero-shot learning outperforms the
SOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL
achieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%,
respectively.",None,-1
86519ba7-ff50-4ccf-94b7-d0132df651df,Denoising Diffusion for 3D Hand Pose Estimation from Images,0.424571,2,"Hand pose estimation from a single image has many applications. However,
approaches to full 3D body pose estimation are typically trained on day-to-day
activities or actions. As such, detailed hand-to-hand interactions are poorly
represented, especially during motion. We see this in the failure cases of
techniques such as OpenPose or MediaPipe. However, accurate hand pose
estimation is crucial for many applications where the global body motion is
less important than accurate hand pose estimation.
  This paper addresses the problem of 3D hand pose estimation from monocular
images or sequences. We present a novel end-to-end framework for 3D hand
regression that employs diffusion models that have shown excellent ability to
capture the distribution of data for generative purposes. Moreover, we enforce
kinematic constraints to ensure realistic poses are generated by incorporating
an explicit forward kinematic layer as part of the network. The proposed model
provides state-of-the-art performance when lifting a 2D single-hand image to
3D. However, when sequence data is available, we add a Transformer module over
a temporal window of consecutive frames to refine the results, overcoming
jittering and further increasing accuracy.
  The method is quantitatively and qualitatively evaluated showing
state-of-the-art robustness, generalization, and accuracy on several different
datasets.",None,-1
2cf5bcc5-77ed-4b40-802d-92e3a924bfc8,VITAL: Vision Transformer Neural Networks for Accurate Smartphone Heterogeneity Resilient Indoor Localization,0.760349,5,"Wi-Fi fingerprinting-based indoor localization is an emerging embedded
application domain that leverages existing Wi-Fi access points (APs) in
buildings to localize users with smartphones. Unfortunately, the heterogeneity
of wireless transceivers across diverse smartphones carried by users has been
shown to reduce the accuracy and reliability of localization algorithms. In
this paper, we propose a novel framework based on vision transformer neural
networks called VITAL that addresses this important challenge. Experiments
indicate that VITAL can reduce the uncertainty created by smartphone
heterogeneity while improving localization accuracy from 41% to 68% over the
best-known prior works. We also demonstrate the generalizability of our
approach and propose a data augmentation technique that can be integrated into
most deep learning-based localization frameworks to improve accuracy.",None,-1
d39038f4-b144-4f65-8ebe-ee4eef7afed0,A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training,0.487528,6,"Mixture-of-Experts (MoE) is a neural network architecture that adds sparsely
activated expert blocks to a base model, increasing the number of parameters
without impacting computational costs. However, current distributed deep
learning frameworks are limited in their ability to train high-quality MoE
models with large base models. In this work, we present DeepSpeed-TED, a novel,
three-dimensional, hybrid parallel algorithm that combines data, tensor, and
expert parallelism to enable the training of MoE models with 4 to 8x larger
base models than the current state-of-the-art. We also describe memory
optimizations in the optimizer step, and communication optimizations that
eliminate unnecessary data movement. We implement our approach in DeepSpeed and
achieve speedups of 26% over a baseline (i.e. without our communication
optimizations) when training a 40 billion parameter MoE model (6.7 billion base
model with 16 experts) on 128 V100 GPUs.",None,-1
46b2f48a-53f3-40c6-a349-270477f6d28b,GLEN: General-Purpose Event Detection for Thousands of Types,0.772151,10,"The progress of event extraction research has been hindered by the absence of
wide-coverage, large-scale datasets. To make event extraction systems more
accessible, we build a general-purpose event detection dataset GLEN, which
covers 205K event mentions with 3,465 different types, making it more than 20x
larger in ontology than today's largest event dataset. GLEN is created by
utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and
PropBank rolesets. This enables us to use the abundant existing annotation for
PropBank as distant supervision. In addition, we also propose a new multi-stage
event detection model CEDAR specifically designed to handle the large ontology
size in GLEN. We show that our model exhibits superior performance compared to
a range of baselines including InstructGPT. Finally, we perform error analysis
and show that label noise is still the largest challenge for improving
performance for this new dataset. Our dataset, code, and models are released at
\url{https://github.com/ZQS1943/GLEN}.}",None,-1
e43ea477-02ed-4329-b4b3-3bb9cd64d994,Colo-SCRL: Self-Supervised Contrastive Representation Learning for Colonoscopic Video Retrieval,0.608015,5,"Colonoscopic video retrieval, which is a critical part of polyp treatment,
has great clinical significance for the prevention and treatment of colorectal
cancer. However, retrieval models trained on action recognition datasets
usually produce unsatisfactory retrieval results on colonoscopic datasets due
to the large domain gap between them. To seek a solution to this problem, we
construct a large-scale colonoscopic dataset named Colo-Pair for medical
practice. Based on this dataset, a simple yet effective training method called
Colo-SCRL is proposed for more robust representation learning. It aims to
refine general knowledge from colonoscopies through masked autoencoder-based
reconstruction and momentum contrast to improve retrieval performance. To the
best of our knowledge, this is the first attempt to employ the contrastive
learning paradigm for medical video retrieval. Empirical results show that our
method significantly outperforms current state-of-the-art methods in the
colonoscopic video retrieval task.",None,-1
053fe506-fbca-4c79-a943-7a51701e2f10,Do Generative Large Language Models need billions of parameters?,0.0656669,7,"This paper presents novel systems and methodologies for the development of
efficient large language models (LLMs). It explores the trade-offs between
model size, performance, and computational resources, with the aim of
maximizing the efficiency of these AI systems. The research explores novel
methods that allow different parts of the model to share parameters, reducing
the total number of unique parameters required. This approach ensures that the
model remains compact without sacrificing its ability to learn and represent
complex language structures. This study provides valuable insights and tools
for creating more efficient and effective LLMs, contributing to a more
sustainable and accessible future for AI language modeling.",None,-1
28172fd6-5e5c-4a94-a384-981918e13e0f,Jointly Optimizing Translations and Speech Timing to Improve Isochrony in Automatic Dubbing,0.110815,4,"Automatic dubbing (AD) is the task of translating the original speech in a
video into target language speech. The new target language speech should
satisfy isochrony; that is, the new speech should be time aligned with the
original video, including mouth movements, pauses, hand gestures, etc. In this
paper, we propose training a model that directly optimizes both the translation
as well as the speech duration of the generated translations. We show that this
system generates speech that better matches the timing of the original speech,
compared to prior work, while simplifying the system architecture.",None,-1
15d76b52-51a1-4fcd-a3bd-e1c012d37aad,Break It Down: Evidence for Structural Compositionality in Neural Networks,0.718099,17,"Though modern neural networks have achieved impressive performance in both
vision and language tasks, we know little about the functions that they
implement. One possibility is that neural networks implicitly break down
complex tasks into subroutines, implement modular solutions to these
subroutines, and compose them into an overall solution to a task - a property
we term structural compositionality. Another possibility is that they may
simply learn to match new inputs to learned templates, eliding task
decomposition entirely. Here, we leverage model pruning techniques to
investigate this question in both vision and language across a variety of
architectures, tasks, and pretraining regimens. Our results demonstrate that
models often implement solutions to subroutines via modular subnetworks, which
can be ablated while maintaining the functionality of other subnetworks. This
suggests that neural networks may be able to learn compositionality, obviating
the need for specialized symbolic mechanisms.",None,-1
d3d46d7a-ed6e-43b0-8b3a-90b5330dc315,EpiK-Eval: Evaluation for Language Models as Epistemic Models,0.0192019,2,"In the age of artificial intelligence, the role of large language models
(LLMs) is becoming increasingly central. Despite their growing prevalence,
their capacity to consolidate knowledge from different training documents - a
crucial ability in numerous applications - remains unexplored. This paper
presents the first study examining the capability of LLMs to effectively
combine such information within their parameter space. We introduce EpiK-Eval,
a novel question-answering benchmark tailored to evaluate LLMs' proficiency in
formulating a coherent and consistent knowledge representation from segmented
narratives. Evaluations across various LLMs reveal significant weaknesses in
this domain. We contend that these shortcomings stem from the intrinsic nature
of prevailing training objectives. Consequently, we advocate for refining the
approach towards knowledge consolidation, as it harbors the potential to
dramatically improve their overall effectiveness and performance. The findings
from this study offer insights for developing more robust and reliable LLMs.
Our code and benchmark are available at
https://github.com/chandar-lab/EpiK-Eval",None,-1
5048975c-ff77-4b25-b8c1-96499cfb5465,EgoVM: Achieving Precise Ego-Localization using Lightweight Vectorized Maps,0.133816,2,"Accurate and reliable ego-localization is critical for autonomous driving. In
this paper, we present EgoVM, an end-to-end localization network that achieves
comparable localization accuracy to prior state-of-the-art methods, but uses
lightweight vectorized maps instead of heavy point-based maps. To begin with,
we extract BEV features from online multi-view images and LiDAR point cloud.
Then, we employ a set of learnable semantic embeddings to encode the semantic
types of map elements and supervise them with semantic segmentation, to make
their feature representation consistent with BEV features. After that, we feed
map queries, composed of learnable semantic embeddings and coordinates of map
elements, into a transformer decoder to perform cross-modality matching with
BEV features. Finally, we adopt a robust histogram-based pose solver to
estimate the optimal pose by searching exhaustively over candidate poses. We
comprehensively validate the effectiveness of our method using both the
nuScenes dataset and a newly collected dataset. The experimental results show
that our method achieves centimeter-level localization accuracy, and
outperforms existing methods using vectorized maps by a large margin.
Furthermore, our model has been extensively tested in a large fleet of
autonomous vehicles under various challenging urban scenes.",None,-1
48a9ee20-3414-4ace-8f8a-7744adf94bd7,Mind the Backbone: Minimizing Backbone Distortion for Robust Object Detection,0.0367524,1,"Building object detectors that are robust to domain shifts is critical for
real-world applications. Prior approaches fine-tune a pre-trained backbone and
risk overfitting it to in-distribution (ID) data and distorting features useful
for out-of-distribution (OOD) generalization. We propose to use Relative
Gradient Norm (RGN) as a way to measure the vulnerability of a backbone to
feature distortion, and show that high RGN is indeed correlated with lower OOD
performance. Our analysis of RGN yields interesting findings: some backbones
lose OOD robustness during fine-tuning, but others gain robustness because
their architecture prevents the parameters from changing too much from the
initial model. Given these findings, we present recipes to boost OOD robustness
for both types of backbones. Specifically, we investigate regularization and
architectural choices for minimizing gradient updates so as to prevent the
tuned backbone from losing generalizable features. Our proposed techniques
complement each other and show substantial improvements over baselines on
diverse architectures and datasets. Code is available at
https://github.com/VisionLearningGroup/mind_back.",None,-1
c194530d-0e99-440c-9621-51f96e8960b3,Reusable Slotwise Mechanisms,0.101032,3,"Agents with the ability to comprehend and reason about the dynamics of
objects would be expected to exhibit improved robustness and generalization in
novel scenarios. However, achieving this capability necessitates not only an
effective scene representation but also an understanding of the mechanisms
governing interactions among object subsets. Recent studies have made
significant progress in representing scenes using object slots. In this work,
we introduce Reusable Slotwise Mechanisms, or RSM, a framework that models
object dynamics by leveraging communication among slots along with a modular
architecture capable of dynamically selecting reusable mechanisms for
predicting the future states of each object slot. Crucially, RSM leverages the
Central Contextual Information (CCI), enabling selected mechanisms to access
the remaining slots through a bottleneck, effectively allowing for modeling of
higher order and complex interactions that might require a sparse subset of
objects. Experimental results demonstrate the superior performance of RSM
compared to state-of-the-art methods across various future prediction and
related downstream tasks, including Visual Question Answering and action
planning. Furthermore, we showcase RSM's Out-of-Distribution generalization
ability to handle scenes in intricate scenarios.",None,-1
279b5d67-3c60-485f-9a9e-db1bf4ef984d,Does Federated Learning Really Need Backpropagation?,0.510565,14,"Federated learning (FL) is a general principle for decentralized clients to
train a server model collectively without sharing local data. FL is a promising
framework with practical applications, but its standard training paradigm
requires the clients to backpropagate through the model to compute gradients.
Since these clients are typically edge devices and not fully trusted, executing
backpropagation on them incurs computational and storage overhead as well as
white-box vulnerability. In light of this, we develop backpropagation-free
federated learning, dubbed BAFFLE, in which backpropagation is replaced by
multiple forward processes to estimate gradients. BAFFLE is 1) memory-efficient
and easily fits uploading bandwidth; 2) compatible with inference-only hardware
optimization and model quantization or pruning; and 3) well-suited to trusted
execution environments, because the clients in BAFFLE only execute forward
propagation and return a set of scalars to the server. Empirically we use
BAFFLE to train deep models from scratch or to finetune pretrained models,
achieving acceptable results. Code is available in
https://github.com/FengHZ/BAFFLE.",None,-1
1823c64c-36ac-45b9-b22e-66524fd57687,Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of Early-bird Students towards Three Diagnostic Objectives,0.548665,1,"Cognitive diagnosis seeks to estimate the cognitive states of students by
exploring their logged practice quiz data. It plays a pivotal role in
personalized learning guidance within intelligent education systems. In this
paper, we focus on an important, practical, yet often underexplored task:
domain-level zero-shot cognitive diagnosis (DZCD), which arises due to the
absence of student practice logs in newly launched domains. Recent cross-domain
diagnostic models have been demonstrated to be a promising strategy for DZCD.
These methods primarily focus on how to transfer student states across domains.
However, they might inadvertently incorporate non-transferable information into
student representations, thereby limiting the efficacy of knowledge transfer.
To tackle this, we propose Zero-1-to-3, a domain-level zero-shot cognitive
diagnosis framework via one batch of early-bird students towards three
diagnostic objectives. Our approach initiates with pre-training a diagnosis
model with dual regularizers, which decouples student states into domain-shared
and domain-specific parts. The shared cognitive signals can be transferred to
the target domain, enriching the cognitive priors for the new domain, which
ensures the cognitive state propagation objective. Subsequently, we devise a
strategy to generate simulated practice logs for cold-start students through
analyzing the behavioral patterns from early-bird students, fulfilling the
domain-adaption goal. Consequently, we refine the cognitive states of
cold-start students as diagnostic outcomes via virtual data, aligning with the
diagnosis-oriented goal. Finally, extensive experiments on six real-world
datasets highlight the efficacy of our model for DZCD and its practical
application in question recommendation. The code is publicly available at
https://github.com/bigdata-ustc/Zero-1-to-3.",None,-1
9d3b3c92-929c-4a03-afc4-8d31131e4435,From Fake to Hyperpartisan News Detection Using Domain Adaptation,0.105839,1,"Unsupervised Domain Adaptation (UDA) is a popular technique that aims to
reduce the domain shift between two data distributions. It was successfully
applied in computer vision and natural language processing. In the current
work, we explore the effects of various unsupervised domain adaptation
techniques between two text classification tasks: fake and hyperpartisan news
detection. We investigate the knowledge transfer from fake to hyperpartisan
news detection without involving target labels during training. Thus, we
evaluate UDA, cluster alignment with a teacher, and cross-domain contrastive
learning. Extensive experiments show that these techniques improve performance,
while including data augmentation further enhances the results. In addition, we
combine clustering and topic modeling algorithms with UDA, resulting in
improved performances compared to the initial UDA setup.",None,-1
b34e937b-ac3c-470f-95c6-4e5778595921,Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs,0.924968,16,"Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and
can solve different tasks due to their emergent ability and generalizability.
However, LLMs sometimes lack domain-specific knowledge to perform tasks, which
would also cause hallucination during inference. In some previous works,
additional modules like graph neural networks (GNNs) are trained on retrieved
knowledge from external knowledge bases, aiming to mitigate the problem of
lacking domain-specific knowledge. However, incorporating additional modules:
1) would need retraining additional modules when encountering novel domains; 2)
would become a bottleneck since LLMs' strong abilities are not fully utilized
for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver
(KSL), to teach LLMs to search for essential knowledge from external knowledge
bases by harnessing their own strong generalizability. Specifically, we design
a simple yet effective prompt to transform retrieval into a multi-hop decision
sequence, which empowers LLMs with searching knowledge ability in zero-shot
manner. Additionally, KSL is able to provide complete retrieval paths and
therefore increase explainability of LLMs' reasoning processes. We conduct
experiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and
found that our approach improves LLM baseline performance by a relatively large
margin.",None,-1
cb0a78ce-1b50-42f0-8b31-c9f4749c4075,Robust Domain Misinformation Detection via Multi-modal Feature Alignment,0.738281,3,"Social media misinformation harms individuals and societies and is
potentialized by fast-growing multi-modal content (i.e., texts and images),
which accounts for higher ""credibility"" than text-only news pieces. Although
existing supervised misinformation detection methods have obtained acceptable
performances in key setups, they may require large amounts of labeled data from
various events, which can be time-consuming and tedious. In turn, directly
training a model by leveraging a publicly available dataset may fail to
generalize due to domain shifts between the training data (a.k.a. source
domains) and the data from target domains. Most prior work on domain shift
focuses on a single modality (e.g., text modality) and ignores the scenario
where sufficient unlabeled target domain data may not be readily available in
an early stage. The lack of data often happens due to the dynamic propagation
trend (i.e., the number of posts related to fake news increases slowly before
catching the public attention). We propose a novel robust domain and
cross-modal approach (\textbf{RDCM}) for multi-modal misinformation detection.
It reduces the domain shift by aligning the joint distribution of textual and
visual modalities through an inter-domain alignment module and bridges the
semantic gap between both modalities through a cross-modality alignment module.
We also propose a framework that simultaneously considers application scenarios
of domain generalization (in which the target domain data is unavailable) and
domain adaptation (in which unlabeled target domain data is available).
Evaluation results on two public multi-modal misinformation detection datasets
(Pheme and Twitter Datasets) evince the superiority of the proposed model. The
formal implementation of this paper can be found in this link:
https://github.com/less-and-less-bugs/RDCM",None,-1
557524e5-1068-44ba-846e-90ea8e6288fa,SpaDen : Sparse and Dense Keypoint Estimation for Real-World Chart Understanding,0.112715,1,"We introduce a novel bottom-up approach for the extraction of chart data. Our
model utilizes images of charts as inputs and learns to detect keypoints (KP),
which are used to reconstruct the components within the plot area. Our novelty
lies in detecting a fusion of continuous and discrete KP as predicted heatmaps.
A combination of sparse and dense per-pixel objectives coupled with a uni-modal
self-attention-based feature-fusion layer is applied to learn KP embeddings.
Further leveraging deep metric learning for unsupervised clustering, allows us
to segment the chart plot area into various objects. By further matching the
chart components to the legend, we are able to obtain the data series names. A
post-processing threshold is applied to the KP embeddings to refine the object
reconstructions and improve accuracy. Our extensive experiments include an
evaluation of different modules for KP estimation and the combination of deep
layer aggregation and corner pooling approaches. The results of our experiments
provide extensive evaluation for the task of real-world chart data extraction.",None,-1
e15ca023-9a9b-4b9f-bc27-1554be923c74,PE-YOLO: Pyramid Enhancement Network for Dark Object Detection,0.327322,7,"Current object detection models have achieved good results on many benchmark
datasets, detecting objects in dark conditions remains a large challenge. To
address this issue, we propose a pyramid enhanced network (PENet) and joint it
with YOLOv3 to build a dark object detection framework named PE-YOLO. Firstly,
PENet decomposes the image into four components of different resolutions using
the Laplacian pyramid. Specifically we propose a detail processing module (DPM)
to enhance the detail of images, which consists of context branch and edge
branch. In addition, we propose a low-frequency enhancement filter (LEF) to
capture low-frequency semantics and prevent high-frequency noise. PE-YOLO
adopts an end-to-end joint training approach and only uses normal detection
loss to simplify the training process. We conduct experiments on the low-light
object detection dataset ExDark to demonstrate the effectiveness of ours. The
results indicate that compared with other dark detectors and low-light
enhancement models, PE-YOLO achieves the advanced results, achieving 78.0% in
mAP and 53.6 in FPS, respectively, which can adapt to object detection under
different low-light conditions. The code is available at
https://github.com/XiangchenYin/PE-YOLO.",None,-1
35516420-c7fb-4879-ac4f-b4fdde42611e,Mastering Strategy Card Game (Hearthstone) with Improved Techniques,0.596312,5,"Strategy card game is a well-known genre that is demanding on the intelligent
game-play and can be an ideal test-bench for AI. Previous work combines an
end-to-end policy function and an optimistic smooth fictitious play, which
shows promising performances on the strategy card game Legend of Code and
Magic. In this work, we apply such algorithms to Hearthstone, a famous
commercial game that is more complicated in game rules and mechanisms. We
further propose several improved techniques and consequently achieve
significant progress. For a machine-vs-human test we invite a Hearthstone
streamer whose best rank was top 10 of the official league in China region that
is estimated to be of millions of players. Our models defeat the human player
in all Best-of-5 tournaments of full games (including both deck building and
battle), showing a strong capability of decision making.",None,-1
58dd213b-7d01-4846-95dc-770d3f34739c,Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations,0.2913,4,"Computer simulations offer a robust toolset for exploring complex systems
across various disciplines. A particularly impactful approach within this realm
is Agent-Based Modeling (ABM), which harnesses the interactions of individual
agents to emulate intricate system dynamics. ABM's strength lies in its
bottom-up methodology, illuminating emergent phenomena by modeling the
behaviors of individual components of a system. Yet, ABM has its own set of
challenges, notably its struggle with modeling natural language instructions
and common sense in mathematical equations or rules. This paper seeks to
transcend these boundaries by integrating Large Language Models (LLMs) like GPT
into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based
Modeling (SABM). Building upon the concept of smart agents -- entities
characterized by their intelligence, adaptability, and computation ability --
we explore in the direction of utilizing LLM-powered agents to simulate
real-world scenarios with increased nuance and realism. In this comprehensive
exploration, we elucidate the state of the art of ABM, introduce SABM's
potential and methodology, and present three case studies (source codes
available at https://github.com/Roihn/SABM), demonstrating the SABM methodology
and validating its effectiveness in modeling real-world systems. Furthermore,
we cast a vision towards several aspects of the future of SABM, anticipating a
broader horizon for its applications. Through this endeavor, we aspire to
redefine the boundaries of computer simulations, enabling a more profound
understanding of complex systems.",None,-1
454c2e5d-35d6-4ea7-9f00-7c9643a4f63d,RSFDM-Net: Real-time Spatial and Frequency Domains Modulation Network for Underwater Image Enhancement,0.582375,4,"Underwater images typically experience mixed degradations of brightness and
structure caused by the absorption and scattering of light by suspended
particles. To address this issue, we propose a Real-time Spatial and Frequency
Domains Modulation Network (RSFDM-Net) for the efficient enhancement of colors
and details in underwater images. Specifically, our proposed conditional
network is designed with Adaptive Fourier Gating Mechanism (AFGM) and
Multiscale Convolutional Attention Module (MCAM) to generate vectors carrying
low-frequency background information and high-frequency detail features, which
effectively promote the network to model global background information and
local texture details. To more precisely correct the color cast and low
saturation of the image, we introduce a Three-branch Feature Extraction (TFE)
block in the primary net that processes images pixel by pixel to integrate the
color information extended by the same channel (R, G, or B). This block
consists of three small branches, each of which has its own weights. Extensive
experiments demonstrate that our network significantly outperforms over
state-of-the-art methods in both visual quality and quantitative metrics.",None,-1
73d83f65-4eae-4532-b1c1-aa839bdabab5,Scene Graph as Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation with Visual Scene Hallucination,0.949528,27,"In this work, we investigate a more realistic unsupervised multimodal machine
translation (UMMT) setup, inference-time image-free UMMT, where the model is
trained with source-text image pairs, and tested with only source-text inputs.
First, we represent the input images and texts with the visual and language
scene graphs (SG), where such fine-grained vision-language features ensure a
holistic understanding of the semantics. To enable pure-text input during
inference, we devise a visual scene hallucination mechanism that dynamically
generates pseudo visual SG from the given textual SG. Several SG-pivoting based
learning objectives are introduced for unsupervised translation training. On
the benchmark Multi30K data, our SG-based method outperforms the
best-performing baseline by significant BLEU scores on the task and setup,
helping yield translations with better completeness, relevance and fluency
without relying on paired images. Further in-depth analyses reveal how our
model advances in the task setting.",None,-1
84d0437b-9cb2-4210-87c3-7801687f7071,ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories,0.0874346,2,"Recently, Large Language Models (LLMs) have been serving as general-purpose
interfaces, posing a significant demand for comprehensive visual knowledge.
However, it remains unclear how well current LLMs and their visually augmented
counterparts (VaLMs) can master visual commonsense knowledge. To investigate
this, we propose ImageNetVC, a human-annotated dataset specifically designed
for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet
categories. Utilizing ImageNetVC, we benchmark the fundamental visual
commonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze
the factors affecting the visual commonsense knowledge of large-scale models,
providing insights into the development of language models enriched with visual
commonsense knowledge. Our code and dataset are available at
https://github.com/hemingkx/ImageNetVC.",None,-1
9e01afc5-76d2-4a2b-83ee-ac24a186502f,Developing an Informal-Formal Persian Corpus,0.182572,2,"Informal language is a style of spoken or written language frequently used in
casual conversations, social media, weblogs, emails and text messages. In
informal writing, the language faces some lexical and/or syntactic changes
varying among different languages. Persian is one of the languages with many
differences between its formal and informal styles of writing, thus developing
informal language processing tools for this language seems necessary. Such a
converter needs a large aligned parallel corpus of colloquial-formal sentences
which can be useful for linguists to extract a regulated grammar and
orthography for colloquial Persian as is done for the formal language. In this
paper we explain our methodology in building a parallel corpus of 50,000
sentence pairs with alignments in the word/phrase level. The sentences were
attempted to cover almost all kinds of lexical and syntactic changes between
informal and formal Persian, therefore both methods of exploring and collecting
from the different resources of informal scripts and following the phonological
and morphological patterns of changes were applied to find as much instances as
possible. The resulting corpus has about 530,000 alignments and a dictionary
containing 49,397 word and phrase pairs.",None,-1
24628ce6-73a3-49a5-ab20-faa08a4eeddc,Backpropagation of Unrolled Solvers with Folded Optimization,0.61748,6,"The integration of constrained optimization models as components in deep
networks has led to promising advances on many specialized learning tasks. A
central challenge in this setting is backpropagation through the solution of an
optimization problem, which typically lacks a closed form. One typical strategy
is algorithm unrolling, which relies on automatic differentiation through the
operations of an iterative solver. While flexible and general, unrolling can
encounter accuracy and efficiency issues in practice. These issues can be
avoided by analytical differentiation of the optimization, but current
frameworks impose rigid requirements on the optimization problem's form. This
paper provides theoretical insights into the backward pass of unrolled
optimization, leading to a system for generating efficiently solvable
analytical models of backpropagation. Additionally, it proposes a unifying view
of unrolling and analytical differentiation through optimization mappings.
Experiments over various model-based learning tasks demonstrate the advantages
of the approach both computationally and in terms of enhanced expressiveness.",None,-1
ab275335-6ee0-46a3-9360-e2c0835c26b9,UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning,0.407268,8,"Multimodal learning aims to imitate human beings to acquire complementary
information from multiple modalities for various downstream tasks. However,
traditional aggregation-based multimodal fusion methods ignore the
inter-modality relationship, treat each modality equally, suffer sensor noise,
and thus reduce multimodal learning performance. In this work, we propose a
novel multimodal contrastive method to explore more reliable multimodal
representations under the weak supervision of unimodal predicting.
Specifically, we first capture task-related unimodal representations and the
unimodal predictions from the introduced unimodal predicting task. Then the
unimodal representations are aligned with the more effective one by the
designed multimodal contrastive method under the supervision of the unimodal
predictions. Experimental results with fused features on two image-text
classification benchmarks UPMC-Food-101 and N24News show that our proposed
Unimodality-Supervised MultiModal Contrastive UniS-MMC learning method
outperforms current state-of-the-art multimodal methods. The detailed ablation
study and analysis further demonstrate the advantage of our proposed method.",None,-1
5486c0ea-eaba-4bdc-8e8d-e637ce8960f1,TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage,0.950213,14,"With recent advancements in natural language processing, Large Language
Models (LLMs) have emerged as powerful tools for various real-world
applications. Despite their prowess, the intrinsic generative abilities of LLMs
may prove insufficient for handling complex tasks which necessitate a
combination of task planning and the usage of external tools. In this paper, we
first propose a structured framework tailored for LLM-based AI Agents and
discuss the crucial capabilities necessary for tackling intricate problems.
Within this framework, we design two distinct types of agents (i.e., one-step
agent and sequential agent) to execute the inference process. Subsequently, we
instantiate the framework using various LLMs and evaluate their Task Planning
and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings
and challenges, our goal is to provide a helpful resource for researchers and
practitioners to leverage the power of LLMs in their AI applications. Our study
emphasizes the substantial potential of these models, while also identifying
areas that need more investigation and improvement.",None,-1
baf227fb-0cec-413a-9575-8260d6de821b,Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media,0.264947,11,"Stance detection predicts attitudes towards targets in texts and has gained
attention with the rise of social media. Traditional approaches include
conventional machine learning, early deep neural networks, and pre-trained
fine-tuning models. However, with the evolution of very large pre-trained
language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face
deployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not
requiring backpropagation training, has emerged as a promising alternative.
This paper examines CoT's effectiveness in stance detection tasks,
demonstrating its superior accuracy and discussing associated challenges.",None,-1
ab8af330-a810-473a-a0cb-380c7542519e,Diffusion-based Document Layout Generation,0.637846,9,"We develop a diffusion-based approach for various document layout sequence
generation. Layout sequences specify the contents of a document design in an
explicit format. Our novel diffusion-based approach works in the sequence
domain rather than the image domain in order to permit more complex and
realistic layouts. We also introduce a new metric, Document Earth Mover's
Distance (Doc-EMD). By considering similarity between heterogeneous categories
document designs, we handle the shortcomings of prior document metrics that
only evaluate the same category of layouts. Our empirical analysis shows that
our diffusion-based approach is comparable to or outperforming other previous
methods for layout generation across various document datasets. Moreover, our
metric is capable of differentiating documents better than previous metrics for
specific cases.",None,-1
863772b6-e442-4475-943e-bdeb7a024a27,Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge,0.156176,2,"We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge
using Large Language Models (LLMs) as a system of multiple expert agents. Using
the flexibility of LLMs to be prompted to do various novel tasks using
zero-shot, few-shot, context-grounded prompting, we explore the feasibility of
using LLMs to solve the ARC Challenge. We firstly convert the input image into
multiple suitable text-based abstraction spaces. We then utilise the
associative power of LLMs to derive the input-output relationship and map this
to actions in the form of a working program, similar to Voyager / Ghost in the
MineCraft. In addition, we use iterative environmental feedback in order to
guide LLMs to solve the task. Our proposed approach achieves 50 solves out of
111 training set problems (45%) with just three abstraction spaces - grid,
object and pixel - and we believe that with more abstraction spaces and
learnable actions, we will be able to solve more.",None,-1
da9a8f48-7fef-4427-b645-a81363e6c648,Is Centralized Training with Decentralized Execution Framework Centralized Enough for MARL?,0.493521,4,"Centralized Training with Decentralized Execution (CTDE) has recently emerged
as a popular framework for cooperative Multi-Agent Reinforcement Learning
(MARL), where agents can use additional global state information to guide
training in a centralized way and make their own decisions only based on
decentralized local policies. Despite the encouraging results achieved, CTDE
makes an independence assumption on agent policies, which limits agents to
adopt global cooperative information from each other during centralized
training. Therefore, we argue that existing CTDE methods cannot fully utilize
global information for training, leading to an inefficient joint-policy
exploration and even suboptimal results. In this paper, we introduce a novel
Centralized Advising and Decentralized Pruning (CADP) framework for multi-agent
reinforcement learning, that not only enables an efficacious message exchange
among agents during training but also guarantees the independent policies for
execution. Firstly, CADP endows agents the explicit communication channel to
seek and take advices from different agents for more centralized training. To
further ensure the decentralized execution, we propose a smooth model pruning
mechanism to progressively constraint the agent communication into a closed one
without degradation in agent cooperation capability. Empirical evaluations on
StarCraft II micromanagement and Google Research Football benchmarks
demonstrate that the proposed framework achieves superior performance compared
with the state-of-the-art counterparts. Our code will be made publicly
available.",None,-1
e9288167-a720-4b71-9225-d2d0184e1ba0,Putting People in Their Place: Affordance-Aware Human Insertion into Scenes,0.944966,21,"We study the problem of inferring scene affordances by presenting a method
for realistically inserting people into scenes. Given a scene image with a
marked region and an image of a person, we insert the person into the scene
while respecting the scene affordances. Our model can infer the set of
realistic poses given the scene context, re-pose the reference person, and
harmonize the composition. We set up the task in a self-supervised fashion by
learning to re-pose humans in video clips. We train a large-scale diffusion
model on a dataset of 2.4M video clips that produces diverse plausible poses
while respecting the scene context. Given the learned human-scene composition,
our model can also hallucinate realistic people and scenes when prompted
without conditioning and also enables interactive editing. A quantitative
evaluation shows that our method synthesizes more realistic human appearance
and more natural human-scene interactions than prior work.",None,-1
3ad04aec-89a6-4543-ae6a-e262c328cea5,Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT,0.320401,2,"The rapid proliferation of ChatGPT has incited debates regarding its impact
on human writing. Amid concerns about declining writing standards, this study
investigates the role of ChatGPT in facilitating academic writing, especially
among language learners. Using a case study approach, this study examines the
experiences of Kailing, a doctoral student, who integrates ChatGPT throughout
their academic writing process. The study employs activity theory as a lens for
understanding writing with generative AI tools and data analyzed includes
semi-structured interviews, writing samples, and GPT logs. Results indicate
that Kailing effectively collaborates with ChatGPT across various writing
stages while preserving her distinct authorial voice and agency. This
underscores the potential of AI tools such as ChatGPT to enhance academic
writing for language learners without overshadowing individual authenticity.
This case study offers a critical exploration of how ChatGPT is utilized in the
academic writing process and the preservation of a student's authentic voice
when engaging with the tool.",None,-1
1eb43731-4517-4666-a1ff-dafbb537967e,X-ReCoSa: Multi-Scale Context Aggregation For Multi-Turn Dialogue Generation,0.0552052,1,"In multi-turn dialogue generation, responses are not only related to the
topic and background of the context but also related to words and phrases in
the sentences of the context. However, currently widely used hierarchical
dialog models solely rely on context representations from the utterance-level
encoder, ignoring the sentence representations output by the word-level
encoder. This inevitably results in a loss of information while decoding and
generating. In this paper, we propose a new dialog model X-ReCoSa to tackle
this problem which aggregates multi-scale context information for hierarchical
dialog models. Specifically, we divide the generation decoder into upper and
lower parts, namely the intention part and the generation part. Firstly, the
intention part takes context representations as input to generate the intention
of the response. Then the generation part generates words depending on sentence
representations. Therefore, the hierarchical information has been fused into
response generation. we conduct experiments on the English dataset DailyDialog.
Experimental results exhibit that our method outperforms baseline models on
both automatic metric-based and human-based evaluations.",None,-1
894e0d35-456b-4abf-857d-fa1b92a75417,TemporAI: Facilitating Machine Learning Innovation in Time Domain Tasks for Medicine,0.169698,3,"TemporAI is an open source Python software library for machine learning (ML)
tasks involving data with a time component, focused on medicine and healthcare
use cases. It supports data in time series, static, and eventmodalities and
provides an interface for prediction, causal inference, and time-to-event
analysis, as well as common preprocessing utilities and model interpretability
methods. The library aims to facilitate innovation in the medical ML space by
offering a standardized temporal setting toolkit for model development,
prototyping and benchmarking, bridging the gaps in the ML research, healthcare
professional, medical/pharmacological industry, and data science communities.
TemporAI is available on GitHub (https://github.com/vanderschaarlab/temporai)
and we welcome community engagement through use, feedback, and code
contributions.",None,-1
0862c267-515b-48fa-a93e-36a4be70f851,Pedestrian Behavior Maps for Safety Advisories: CHAMP Framework and Real-World Data Analysis,0.385562,1,"It is critical for vehicles to prevent any collisions with pedestrians.
Current methods for pedestrian collision prevention focus on integrating visual
pedestrian detectors with Automatic Emergency Braking (AEB) systems which can
trigger warnings and apply brakes as a pedestrian enters a vehicle's path.
Unfortunately, pedestrian-detection-based systems can be hindered in certain
situations such as night-time or when pedestrians are occluded. Our system
addresses such issues using an online, map-based pedestrian detection
aggregation system where common pedestrian locations are learned after repeated
passes of locations. Using a carefully collected and annotated dataset in La
Jolla, CA, we demonstrate the system's ability to learn pedestrian zones and
generate advisory notices when a vehicle is approaching a pedestrian despite
challenges like dark lighting or pedestrian occlusion. Using the number of
correct advisories, false advisories, and missed advisories to define precision
and recall performance metrics, we evaluate our system and discuss future
positive effects with further data collection. We have made our code available
at https://github.com/s7desai/ped-mapping, and a video demonstration of the
CHAMP system at https://youtu.be/dxeCrS_Gpkw.",None,-1
5772ea3f-1484-445c-b791-973835991dbd,Safe POMDP Online Planning via Shielding,0.440463,1,"Partially observable Markov decision processes (POMDPs) have been widely used
in many robotic applications for sequential decision-making under uncertainty.
POMDP online planning algorithms such as Partially Observable Monte-Carlo
Planning (POMCP) can solve very large POMDPs with the goal of maximizing the
expected return. But the resulting policies cannot provide safety guarantees
which are imperative for real-world safety-critical tasks (e.g., autonomous
driving). In this work, we consider safety requirements represented as
almost-sure reach-avoid specifications (i.e., the probability to reach a set of
goal states is one and the probability to reach a set of unsafe states is
zero). We compute shields that restrict unsafe actions which would violate the
almost-sure reach-avoid specifications. We then integrate these shields into
the POMCP algorithm for safe POMDP online planning. We propose four distinct
shielding methods, differing in how the shields are computed and integrated,
including factored variants designed to improve scalability. Experimental
results on a set of benchmark domains demonstrate that the proposed shielding
methods successfully guarantee safety (unlike the baseline POMCP without
shielding) on large POMDPs, with negligible impact on the runtime for online
planning.",None,-1
98418b38-0dfe-49d7-9910-82416d7df107,Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations,0.406486,6,"Human-annotated labels and explanations are critical for training explainable
NLP models. However, unlike human-annotated labels whose quality is easier to
calibrate (e.g., with a majority vote), human-crafted free-form explanations
can be quite subjective. Before blindly using them as ground truth to train ML
models, a vital question needs to be asked: How do we evaluate a
human-annotated explanation's quality? In this paper, we build on the view that
the quality of a human-annotated explanation can be measured based on its
helpfulness (or impairment) to the ML models' performance for the desired NLP
tasks for which the annotations were collected. In comparison to the commonly
used Simulatability score, we define a new metric that can take into
consideration the helpfulness of an explanation for model performance at both
fine-tuning and inference. With the help of a unified dataset format, we
evaluated the proposed metric on five datasets (e.g., e-SNLI) against two model
architectures (T5 and BART), and the results show that our proposed metric can
objectively evaluate the quality of human-annotated explanations, while
Simulatability falls short.",None,-1
d9c409a8-3d5d-42f9-a173-ab199f49a135,Diversity Enhanced Narrative Question Generation for Storybooks,0.145919,2,"Question generation (QG) from a given context can enhance comprehension,
engagement, assessment, and overall efficacy in learning or conversational
environments. Despite recent advancements in QG, the challenge of enhancing or
measuring the diversity of generated questions often remains unaddressed. In
this paper, we introduce a multi-question generation model (mQG), which is
capable of generating multiple, diverse, and answerable questions by focusing
on context and questions. To validate the answerability of the generated
questions, we employ a SQuAD2.0 fine-tuned question answering model,
classifying the questions as answerable or not. We train and evaluate mQG on
the FairytaleQA dataset, a well-structured QA dataset based on storybooks, with
narrative questions. We further apply a zero-shot adaptation on the TellMeWhy
and SQuAD1.1 datasets. mQG shows promising results across various evaluation
metrics, among strong baselines.",None,-1
316036a6-acb8-4f6b-bca2-3635688fd1ee,Inline Citation Classification using Peripheral Context and Time-evolving Augmentation,0.0572609,2,"Citation plays a pivotal role in determining the associations among research
articles. It portrays essential information in indicative, supportive, or
contrastive studies. The task of inline citation classification aids in
extrapolating these relationships; However, existing studies are still immature
and demand further scrutiny. Current datasets and methods used for inline
citation classification only use citation-marked sentences constraining the
model to turn a blind eye to domain knowledge and neighboring contextual
sentences. In this paper, we propose a new dataset, named 3Cext, which along
with the cited sentences, provides discourse information using the vicinal
sentences to analyze the contrasting and entailing relationships as well as
domain information. We propose PeriCite, a Transformer-based deep neural
network that fuses peripheral sentences and domain knowledge. Our model
achieves the state-of-the-art on the 3Cext dataset by +0.09 F1 against the best
baseline. We conduct extensive ablations to analyze the efficacy of the
proposed dataset and model fusion methods.",None,-1
7df14410-b4d2-4d79-89f5-9af3c1a9f8f9,Semantic-aware Occlusion Filtering Neural Radiance Fields in the Wild,0.194684,4,"We present a learning framework for reconstructing neural scene
representations from a small number of unconstrained tourist photos. Since each
image contains transient occluders, decomposing the static and transient
components is necessary to construct radiance fields with such in-the-wild
photographs where existing methods require a lot of training data. We introduce
SF-NeRF, aiming to disentangle those two components with only a few images
given, which exploits semantic information without any supervision. The
proposed method contains an occlusion filtering module that predicts the
transient color and its opacity for each pixel, which enables the NeRF model to
solely learn the static scene representation. This filtering module learns the
transient phenomena guided by pixel-wise semantic features obtained by a
trainable image encoder that can be trained across multiple scenes to learn the
prior of transient objects. Furthermore, we present two techniques to prevent
ambiguous decomposition and noisy results of the filtering module. We
demonstrate that our method outperforms state-of-the-art novel view synthesis
methods on Phototourism dataset in a few-shot setting.",None,-1
9a7ad560-f426-47b1-8ce8-5be8080fc9c4,Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images,0.990278,17,"Photos serve as a way for humans to record what they experience in their
daily lives, and they are often regarded as trustworthy sources of information.
However, there is a growing concern that the advancement of artificial
intelligence (AI) technology may produce fake photos, which can create
confusion and diminish trust in photographs. This study aims to comprehensively
evaluate agents for distinguishing state-of-the-art AI-generated visual
content. Our study benchmarks both human capability and cutting-edge fake image
detection AI algorithms, using a newly collected large-scale fake image dataset
Fake2M. In our human perception evaluation, titled HPBench, we discovered that
humans struggle significantly to distinguish real photos from AI-generated
ones, with a misclassification rate of 38.7%. Along with this, we conduct the
model capability of AI-Generated images detection evaluation MPBench and the
top-performing model from MPBench achieves a 13% failure rate under the same
setting used in the human evaluation. We hope that our study can raise
awareness of the potential risks of AI-generated images and facilitate further
research to prevent the spread of false information. More information can refer
to https://github.com/Inf-imagine/Sentry.",None,-1
16ec8d0c-1f73-445f-a88d-0e2d46473ca2,GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts,0.984922,128,"Large language models (LLMs) have recently experienced tremendous popularity
and are widely used from casual conversations to AI-driven programming.
However, despite their considerable success, LLMs are not entirely reliable and
can give detailed guidance on how to conduct harmful or illegal activities.
While safety measures can reduce the risk of such outputs, adversarial
jailbreak attacks can still exploit LLMs to produce harmful content. These
jailbreak templates are typically manually crafted, making large-scale testing
challenging.
  In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing
framework inspired by the AFL fuzzing framework. Instead of manual engineering,
GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.
At its core, GPTFuzz starts with human-written templates as initial seeds, then
mutates them to produce new templates. We detail three key components of
GPTFuzz: a seed selection strategy for balancing efficiency and variability,
mutate operators for creating semantically equivalent or similar sentences, and
a judgment model to assess the success of a jailbreak attack.
  We evaluate GPTFuzz against various commercial and open-source LLMs,
including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our
results indicate that GPTFuzz consistently produces jailbreak templates with a
high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz
achieves over 90% attack success rates against ChatGPT and Llama-2 models, even
with suboptimal initial seed templates. We anticipate that GPTFuzz will be
instrumental for researchers and practitioners in examining LLM robustness and
will encourage further exploration into enhancing LLM safety.",None,-1
5673ed1a-3716-4e91-acd6-5494be1507c1,Exploring Attention Map Reuse for Efficient Transformer Neural Networks,0.103733,2,"Transformer-based deep neural networks have achieved great success in various
sequence applications due to their powerful ability to model long-range
dependency. The key module of Transformer is self-attention (SA) which extracts
features from the entire sequence regardless of the distance between positions.
Although SA helps Transformer performs particularly well on long-range tasks,
SA requires quadratic computation and memory complexity with the input sequence
length. Recently, attention map reuse, which groups multiple SA layers to share
one attention map, has been proposed and achieved significant speedup for
speech recognition models. In this paper, we provide a comprehensive study on
attention map reuse focusing on its ability to accelerate inference. We compare
the method with other SA compression techniques and conduct a breakdown
analysis of its advantages for a long sequence. We demonstrate the
effectiveness of attention map reuse by measuring the latency on both CPU and
GPU platforms.",None,-1
6e1b2922-d240-4c61-b47d-0e3be6bf219c,Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization,0.102421,3,"A series of datasets and models have been proposed for summaries generated
for well-formatted documents such as news articles. Dialogue summaries,
however, have been under explored. In this paper, we present the first dataset
with fine-grained factual error annotations named DIASUMFACT. We define
fine-grained factual error detection as a sentence-level multi-label
classification problem, and we evaluate two state-of-the-art (SOTA) models on
our dataset. Both models yield sub-optimal results, with a macro-averaged F1
score of around 0.25 over 6 error classes. We further propose an unsupervised
model ENDERANKER via candidate ranking using pretrained encoder-decoder models.
Our model performs on par with the SOTA models while requiring fewer resources.
These observations confirm the challenges in detecting factual errors from
dialogue summaries, which call for further studies, for which our dataset and
results offer a solid foundation.",None,-1
e4238ba8-ac5a-400b-aa8c-189fea083455,MADiff: Offline Multi-agent Learning with Diffusion Models,0.749543,10,"Diffusion model (DM) recently achieved huge success in various scenarios
including offline reinforcement learning, where the diffusion planner learn to
generate desired trajectories during online evaluations. However, despite the
effectiveness in single-agent learning, it remains unclear how DMs can operate
in multi-agent problems, where agents can hardly complete teamwork without good
coordination by independently modeling each agent's trajectories. In this
paper, we propose MADiff, a novel generative multi-agent learning framework to
tackle this problem. MADiff is realized with an attention-based diffusion model
to model the complex coordination among behaviors of multiple agents. To the
best of our knowledge, MADiff is the first diffusion-based multi-agent learning
framework, which behaves as both a decentralized policy and a centralized
controller. During decentralized executions, MADiff simultaneously performs
teammate modeling, and the centralized controller can also be applied in
multi-agent trajectory predictions. Our experiments show the superior
performance of MADiff compared to baseline algorithms in a wide range of
multi-agent learning tasks, which emphasizes the effectiveness of MADiff in
modeling complex multi-agent interactions. Our code is available at
https://github.com/zbzhu99/madiff.",None,-1
b174a17f-8317-4e11-bdf1-58caa8ccd107,Revisiting Non-Autoregressive Translation at Scale,0.285918,2,"In real-world systems, scaling has been critical for improving the
translation quality in autoregressive translation (AT), which however has not
been well studied for non-autoregressive translation (NAT). In this work, we
bridge the gap by systematically studying the impact of scaling on NAT
behaviors. Extensive experiments on six WMT benchmarks over two advanced NAT
models show that scaling can alleviate the commonly-cited weaknesses of NAT
models, resulting in better translation performance. To reduce the side-effect
of scaling on decoding speed, we empirically investigate the impact of NAT
encoder and decoder on the translation performance. Experimental results on the
large-scale WMT20 En-De show that the asymmetric architecture (e.g. bigger
encoder and smaller decoder) can achieve comparable performance with the
scaling model, while maintaining the superiority of decoding speed with
standard NAT models. To this end, we establish a new benchmark by validating
scaled NAT models on the scaled dataset, which can be regarded as a strong
baseline for future works. We release code and system outputs at
https://github.com/DeepLearnXMU/Scaling4NAT.",None,-1
432fc815-8193-4892-a05e-b05ce14450f7,AIMS: All-Inclusive Multi-Level Segmentation,0.191332,6,"Despite the progress of image segmentation for accurate visual entity
segmentation, completing the diverse requirements of image editing applications
for different-level region-of-interest selections remains unsolved. In this
paper, we propose a new task, All-Inclusive Multi-Level Segmentation (AIMS),
which segments visual regions into three levels: part, entity, and relation
(two entities with some semantic relationships). We also build a unified AIMS
model through multi-dataset multi-task training to address the two major
challenges of annotation inconsistency and task correlation. Specifically, we
propose task complementarity, association, and prompt mask encoder for
three-level predictions. Extensive experiments demonstrate the effectiveness
and generalization capacity of our method compared to other state-of-the-art
methods on a single dataset or the concurrent work on segmenting anything. We
will make our code and training model publicly available.",None,-1
cd4f1993-d5bf-410a-9c2e-59a00e7a17d8,Towards Coding Social Science Datasets with Language Models,0.449646,4,"Researchers often rely on humans to code (label, annotate, etc.) large sets
of texts. This kind of human coding forms an important part of social science
research, yet the coding process is both resource intensive and highly variable
from application to application. In some cases, efforts to automate this
process have achieved human-level accuracies, but to achieve this, these
attempts frequently rely on thousands of hand-labeled training examples, which
makes them inapplicable to small-scale research studies and costly for large
ones. Recent advances in a specific kind of artificial intelligence tool -
language models (LMs) - provide a solution to this problem. Work in computer
science makes it clear that LMs are able to classify text, without the cost (in
financial terms and human effort) of alternative methods. To demonstrate the
possibilities of LMs in this area of political science, we use GPT-3, one of
the most advanced LMs, as a synthetic coder and compare it to human coders. We
find that GPT-3 can match the performance of typical human coders and offers
benefits over other machine learning methods of coding text. We find this
across a variety of domains using very different coding procedures. This
provides exciting evidence that language models can serve as a critical advance
in the coding of open-ended texts in a variety of applications.",None,-1
464fa096-ecb1-46e7-99ee-861d54445766,Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,0.996279,21,"Reconstructing deformable tissues from endoscopic stereo videos in robotic
surgery is crucial for various clinical applications. However, existing methods
relying only on implicit representations are computationally expensive and
require dozens of hours, which limits further practical applications. To
address this challenge, we introduce LerPlane, a novel method for fast and
accurate reconstruction of surgical scenes under a single-viewpoint setting.
LerPlane treats surgical procedures as 4D volumes and factorizes them into
explicit 2D planes of static and dynamic fields, leading to a compact memory
footprint and significantly accelerated optimization. The efficient
factorization is accomplished by fusing features obtained through linear
interpolation of each plane and enables using lightweight neural networks to
model surgical scenes. Besides, LerPlane shares static fields, significantly
reducing the workload of dynamic tissue modeling. We also propose a novel
sample scheme to boost optimization and improve performance in regions with
tool occlusion and large motions. Experiments on DaVinci robotic surgery videos
demonstrate that LerPlane accelerates optimization by over 100$\times$ while
maintaining high quality across various non-rigid deformations, showing
significant promise for future intraoperative surgery applications.",None,-1
7cc35a0d-5120-4bab-acd6-bac873eb079d,DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars,0.904779,20,"We present DINAR, an approach for creating realistic rigged fullbody avatars
from single RGB images. Similarly to previous works, our method uses neural
textures combined with the SMPL-X body model to achieve photo-realistic quality
of avatars while keeping them easy to animate and fast to infer. To restore the
texture, we use a latent diffusion model and show how such model can be trained
in the neural texture space. The use of the diffusion model allows us to
realistically reconstruct large unseen regions such as the back of a person
given the frontal view. The models in our pipeline are trained using 2D images
and videos only. In the experiments, our approach achieves state-of-the-art
rendering quality and good generalization to new poses and viewpoints. In
particular, the approach improves state-of-the-art on the SnapshotPeople public
benchmark.",None,-1
d2642e77-eb36-4334-add5-be7b874623a0,RM-Depth: Unsupervised Learning of Recurrent Monocular Depth in Dynamic Scenes,0.922636,35,"Unsupervised methods have showed promising results on monocular depth
estimation. However, the training data must be captured in scenes without
moving objects. To push the envelope of accuracy, recent methods tend to
increase their model parameters. In this paper, an unsupervised learning
framework is proposed to jointly predict monocular depth and complete 3D motion
including the motions of moving objects and camera. (1) Recurrent modulation
units are used to adaptively and iteratively fuse encoder and decoder features.
This not only improves the single-image depth inference but also does not
overspend model parameters. (2) Instead of using a single set of filters for
upsampling, multiple sets of filters are devised for the residual upsampling.
This facilitates the learning of edge-preserving filters and leads to the
improved performance. (3) A warping-based network is used to estimate a motion
field of moving objects without using semantic priors. This breaks down the
requirement of scene rigidity and allows to use general videos for the
unsupervised learning. The motion field is further regularized by an
outlier-aware training loss. Despite the depth model just uses a single image
in test time and 2.97M parameters, it achieves state-of-the-art results on the
KITTI and Cityscapes benchmarks.",None,-1
b929f8d6-370f-473c-82cb-cd5aee3a040b,Efficient Diffusion Training via Min-SNR Weighting Strategy,0.740573,73,"Denoising diffusion models have been a mainstream approach for image
generation, however, training these models often suffers from slow convergence.
In this paper, we discovered that the slow convergence is partly due to
conflicting optimization directions between timesteps. To address this issue,
we treat the diffusion training as a multi-task learning problem, and introduce
a simple yet effective approach referred to as Min-SNR-$\gamma$. This method
adapts loss weights of timesteps based on clamped signal-to-noise ratios, which
effectively balances the conflicts among timesteps. Our results demonstrate a
significant improvement in converging speed, 3.4$\times$ faster than previous
weighting strategies. It is also more effective, achieving a new record FID
score of 2.06 on the ImageNet $256\times256$ benchmark using smaller
architectures than that employed in previous state-of-the-art. The code is
available at https://github.com/TiankaiHang/Min-SNR-Diffusion-Training.",None,-1
c9fc9946-a364-4787-9e41-87dd6d276b2f,Fine-grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,0.870795,9,"Object affordance is an important concept in hand-object interaction,
providing information on action possibilities based on human motor capacity and
objects' physical property thus benefiting tasks such as action anticipation
and robot imitation learning. However, the definition of affordance in existing
datasets often: 1) mix up affordance with object functionality; 2) confuse
affordance with goal-related action; and 3) ignore human motor capacity. This
paper proposes an efficient annotation scheme to address these issues by
combining goal-irrelevant motor actions and grasp types as affordance labels
and introducing the concept of mechanical action to represent the action
possibilities between two objects. We provide new annotations by applying this
scheme to the EPIC-KITCHENS dataset and test our annotation with tasks such as
affordance recognition, hand-object interaction hotspots prediction, and
cross-domain evaluation of affordance. The results show that models trained
with our annotation can distinguish affordance from other concepts, predict
fine-grained interaction possibilities on objects, and generalize through
different domains.",None,-1
97ce64fe-16ed-4668-bb82-9778831a3f06,SEM-CS: Semantic CLIPStyler for Text-Based Image Style Transfer,0.0920407,1,"CLIPStyler demonstrated image style transfer with realistic textures using
only the style text description (instead of requiring a reference style image).
However, the ground semantics of objects in style transfer output is lost due
to style spillover on salient and background objects (content mismatch) or
over-stylization. To solve this, we propose Semantic CLIPStyler (Sem-CS) that
performs semantic style transfer. Sem-CS first segments the content image into
salient and non-salient objects and then transfers artistic style based on a
given style text description. The semantic style transfer is achieved using
global foreground loss (for salient objects) and global background loss (for
non-salient objects). Our empirical results, including DISTS, NIMA and user
study scores, show that our proposed framework yields superior qualitative and
quantitative performance.",None,-1
11b8238d-ba49-4ab4-b777-38934625aa00,Zero-1-to-3: Zero-shot One Image to 3D Object,1.0,528,"We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an
object given just a single RGB image. To perform novel view synthesis in this
under-constrained setting, we capitalize on the geometric priors that
large-scale diffusion models learn about natural images. Our conditional
diffusion model uses a synthetic dataset to learn controls of the relative
camera viewpoint, which allow new images to be generated of the same object
under a specified camera transformation. Even though it is trained on a
synthetic dataset, our model retains a strong zero-shot generalization ability
to out-of-distribution datasets as well as in-the-wild images, including
impressionist paintings. Our viewpoint-conditioned diffusion approach can
further be used for the task of 3D reconstruction from a single image.
Qualitative and quantitative experiments show that our method significantly
outperforms state-of-the-art single-view 3D reconstruction and novel view
synthesis models by leveraging Internet-scale pre-training.",None,-1
27a37f05-2c13-408e-9a2a-573c2f1481c5,From Temporal to Contemporaneous Iterative Causal Discovery in the Presence of Latent Confounders,0.236359,5,"We present a constraint-based algorithm for learning causal structures from
observational time-series data, in the presence of latent confounders. We
assume a discrete-time, stationary structural vector autoregressive process,
with both temporal and contemporaneous causal relations. One may ask if
temporal and contemporaneous relations should be treated differently. The
presented algorithm gradually refines a causal graph by learning long-term
temporal relations before short-term ones, where contemporaneous relations are
learned last. This ordering of causal relations to be learnt leads to a
reduction in the required number of statistical tests. We validate this
reduction empirically and demonstrate that it leads to higher accuracy for
synthetic data and more plausible causal graphs for real-world data compared to
state-of-the-art algorithms.",None,-1
4621f945-84e3-4d4e-aca6-fbc04146c665,Attention-based Part Assembly for 3D Volumetric Shape Modeling,0.0783098,1,"Modeling a 3D volumetric shape as an assembly of decomposed shape parts is
much more challenging, but semantically more valuable than direct
reconstruction from a full shape representation. The neural network needs to
implicitly learn part relations coherently, which is typically performed by
dedicated network layers that can generate transformation matrices for each
part. In this paper, we propose a VoxAttention network architecture for
attention-based part assembly. We further propose a variant of using
channel-wise part attention and show the advantages of this approach.
Experimental results show that our method outperforms most state-of-the-art
methods for the part relation-aware 3D shape modeling task.",None,-1
8b39ac8d-a365-4513-ad16-d7e127e1a6b1,Attention-based Spatial-Temporal Graph Convolutional Recurrent Networks for Traffic Forecasting,0.313696,2,"Traffic forecasting is one of the most fundamental problems in transportation
science and artificial intelligence. The key challenge is to effectively model
complex spatial-temporal dependencies and correlations in modern traffic data.
Existing methods, however, cannot accurately model both long-term and
short-term temporal correlations simultaneously, limiting their expressive
power on complex spatial-temporal patterns. In this paper, we propose a novel
spatial-temporal neural network framework: Attention-based Spatial-Temporal
Graph Convolutional Recurrent Network (ASTGCRN), which consists of a graph
convolutional recurrent module (GCRN) and a global attention module. In
particular, GCRN integrates gated recurrent units and adaptive graph
convolutional networks for dynamically learning graph structures and capturing
spatial dependencies and local temporal relationships. To effectively extract
global temporal dependencies, we design a temporal attention layer and
implement it as three independent modules based on multi-head self-attention,
transformer, and informer respectively. Extensive experiments on five real
traffic datasets have demonstrated the excellent predictive performance of all
our three models with all their average MAE, RMSE and MAPE across the test
datasets lower than the baseline methods.",None,-1
9a9fc235-a21c-4b63-bdd5-1fd314e16dc8,Evaluating Self-Supervised Speech Representations for Indigenous American Languages,0.618935,4,"The application of self-supervision to speech representation learning has
garnered significant interest in recent years, due to its scalability to large
amounts of unlabeled data. However, much progress, both in terms of
pre-training and downstream evaluation, has remained concentrated in
monolingual models that only consider English. Few models consider other
languages, and even fewer consider indigenous ones. In our submission to the
New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR
corpus for Quechua, an indigenous South American Language. We benchmark the
efficacy of large SSL models on Quechua, along with 6 other indigenous
languages such as Guarani and Bribri, on low-resource ASR. Our results show
surprisingly strong performance by state-of-the-art SSL models, showing the
potential generalizability of large-scale models to real-world data.",None,-1
aae4e8ce-d4ed-421d-b7a0-e58a739c35db,mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection,0.512968,5,"This paper presents the winning system for the zero-shot Spanish framing
detection task, which also achieves competitive places in eight additional
languages. The challenge of the framing detection task lies in identifying a
set of 14 frames when only a few or zero samples are available, i.e., a
multilingual multi-label few- or zero-shot setting. Our developed solution
employs a pre-training procedure based on multilingual Transformers using a
label-aware contrastive loss function. In addition to describing the system, we
perform an embedding space analysis and ablation study to demonstrate how our
pre-training procedure supports framing detection to advance computational
framing analysis.",None,-1
a7fa4484-ea07-43c6-b02b-107cf530fa7d,"Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today",0.530422,9,"Recent investigations show that large language models (LLMs), specifically
GPT-4, not only have remarkable capabilities in common Natural Language
Processing (NLP) tasks but also exhibit human-level performance on various
professional and academic benchmarks. However, whether GPT-4 can be directly
used in practical applications and replace traditional artificial intelligence
(AI) tools in specialized domains requires further experimental validation. In
this paper, we explore the potential of LLMs such as GPT-4 to outperform
traditional AI tools in dementia diagnosis. Comprehensive comparisons between
GPT-4 and traditional AI tools are conducted to examine their diagnostic
accuracy in a clinical setting. Experimental results on two real clinical
datasets show that, although LLMs like GPT-4 demonstrate potential for future
advancements in dementia diagnosis, they currently do not surpass the
performance of traditional AI tools. The interpretability and faithfulness of
GPT-4 are also evaluated by comparison with real doctors. We discuss the
limitations of GPT-4 in its current state and propose future research
directions to enhance GPT-4 in dementia diagnosis.",None,-1
6d3687aa-f587-44c2-901e-84553a2dd335,With a Little Help from the Authors: Reproducing Human Evaluation of an MT Error Detector,0.628339,3,"This work presents our efforts to reproduce the results of the human
evaluation experiment presented in the paper of Vamvas and Sennrich (2022),
which evaluated an automatic system detecting over- and undertranslations
(translations containing more or less information than the original) in machine
translation (MT) outputs. Despite the high quality of the documentation and
code provided by the authors, we discuss some problems we found in reproducing
the exact experimental setup and offer recommendations for improving
reproducibility. Our replicated results generally confirm the conclusions of
the original study, but in some cases, statistically significant differences
were observed, suggesting a high variability of human annotation.",None,-1
e7c2293d-cd16-4ce5-b747-9026b2fff2df,A general Markov decision process formalism for action-state entropy-regularized reward maximization,0.364478,3,"Previous work has separately addressed different forms of action, state and
action-state entropy regularization, pure exploration and space occupation.
These problems have become extremely relevant for regularization,
generalization, speeding up learning and providing robust solutions at
unprecedented levels. However, solutions of those problems are hectic, ranging
from convex and non-convex optimization, and unconstrained optimization to
constrained optimization. Here we provide a general dual function formalism
that transforms the constrained optimization problem into an unconstrained
convex one for any mixture of action and state entropies. The cases with pure
action entropy and pure state entropy are understood as limits of the mixture.",None,-1
90ceea1d-e48c-43cd-a0b0-1cf7d78d36b5,Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization,0.843981,18,"Backdoor defense, which aims to detect or mitigate the effect of malicious
triggers introduced by attackers, is becoming increasingly critical for machine
learning security and integrity. Fine-tuning based on benign data is a natural
defense to erase the backdoor effect in a backdoored model. However, recent
studies show that, given limited benign data, vanilla fine-tuning has poor
defense performance. In this work, we provide a deep study of fine-tuning the
backdoored model from the neuron perspective and find that backdoorrelated
neurons fail to escape the local minimum in the fine-tuning process. Inspired
by observing that the backdoorrelated neurons often have larger norms, we
propose FTSAM, a novel backdoor defense paradigm that aims to shrink the norms
of backdoor-related neurons by incorporating sharpness-aware minimization with
fine-tuning. We demonstrate the effectiveness of our method on several
benchmark datasets and network architectures, where it achieves
state-of-the-art defense performance. Overall, our work provides a promising
avenue for improving the robustness of machine learning models against backdoor
attacks.",None,-1
35695ea5-d504-4764-ae7b-ffd0fed78c1e,Deep Quantum Error Correction,0.441488,5,"Quantum error correction codes (QECC) are a key component for realizing the
potential of quantum computing. QECC, as its classical counterpart (ECC),
enables the reduction of error rates, by distributing quantum logical
information across redundant physical qubits, such that errors can be detected
and corrected. In this work, we efficiently train novel {\emph{end-to-end}}
deep quantum error decoders. We resolve the quantum measurement collapse by
augmenting syndrome decoding to predict an initial estimate of the system
noise, which is then refined iteratively through a deep neural network. The
logical error rates calculated over finite fields are directly optimized via a
differentiable objective, enabling efficient decoding under the constraints
imposed by the code. Finally, our architecture is extended to support faulty
syndrome measurement, by efficient decoding of repeated syndrome sampling. The
proposed method demonstrates the power of neural decoders for QECC by achieving
state-of-the-art accuracy, outperforming {for small distance topological
codes,} the existing {end-to-end }neural and classical decoders, which are
often computationally prohibitive.",None,-1
c0696015-48c4-411b-9ead-c51425ad1574,Towards a Deeper Understanding of Concept Bottleneck Models Through End-to-End Explanation,0.122003,3,"Concept Bottleneck Models (CBMs) first map raw input(s) to a vector of
human-defined concepts, before using this vector to predict a final
classification. We might therefore expect CBMs capable of predicting concepts
based on distinct regions of an input. In doing so, this would support human
interpretation when generating explanations of the model's outputs to visualise
input features corresponding to concepts. The contribution of this paper is
threefold: Firstly, we expand on existing literature by looking at relevance
both from the input to the concept vector, confirming that relevance is
distributed among the input features, and from the concept vector to the final
classification where, for the most part, the final classification is made using
concepts predicted as present. Secondly, we report a quantitative evaluation to
measure the distance between the maximum input feature relevance and the ground
truth location; we perform this with the techniques, Layer-wise Relevance
Propagation (LRP), Integrated Gradients (IG) and a baseline gradient approach,
finding LRP has a lower average distance than IG. Thirdly, we propose using the
proportion of relevance as a measurement for explaining concept importance.",None,-1
8598d2c4-3e7b-428f-9e7c-2c70ead956a3,MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning,0.050339,1,"Prompt-based learning has shown considerable promise in reformulating various
downstream tasks as cloze problems by combining original input with a
predetermined template. This approach demonstrates its effectiveness,
especially in few-shot learning scenarios, where the model is trained on a
scarce amount of data. Despite its successes, the limited templates and text in
few-shot prompt-based learning scenarios leave significant room for performance
improvement. Moreover, existing methods sometimes resort to model ensembles,
which, while effective, could potentially hamper model efficiency due to
increased computational demands. To address these issues, we introduce MixPro,
an augmentation method designed to augment both the vanilla input text and the
templates. We implement this through the token-level, the sentence-level, and
the template-level Mixup strategies. The experimental results on five few-shot
datasets show that MixPro outperforms other augmentation baselines, improving
model performance by an average of 5.08% compared to before augmentation.",None,-1
61c7d0b4-0953-4b42-a33f-49743aaaa552,Joint Dense-Point Representation for Contour-Aware Graph Segmentation,0.0767834,1,"We present a novel methodology that combines graph and dense segmentation
techniques by jointly learning both point and pixel contour representations,
thereby leveraging the benefits of each approach. This addresses deficiencies
in typical graph segmentation methods where misaligned objectives restrict the
network from learning discriminative vertex and contour features. Our joint
learning strategy allows for rich and diverse semantic features to be encoded,
while alleviating common contour stability issues in dense-based approaches,
where pixel-level objectives can lead to anatomically implausible topologies.
In addition, we identify scenarios where correct predictions that fall on the
contour boundary are penalised and address this with a novel hybrid contour
distance loss. Our approach is validated on several Chest X-ray datasets,
demonstrating clear improvements in segmentation stability and accuracy against
a variety of dense- and point-based methods. Our source code is freely
available at: www.github.com/kitbransby/Joint_Graph_Segmentation",None,-1
86a8794b-7066-43e4-a95e-e7dbfa476ba4,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,0.188807,2,"Transfer learning is a critical technique in training deep neural networks
for the challenging medical image segmentation task that requires enormous
resources. With the abundance of medical image data, many research institutions
release models trained on various datasets that can form a huge pool of
candidate source models to choose from. Hence, it's vital to estimate the
source models' transferability (i.e., the ability to generalize across
different downstream tasks) for proper and efficient model reuse. To make up
for its deficiency when applying transfer learning to medical image
segmentation, in this paper, we therefore propose a new Transferability
Estimation (TE) method. We first analyze the drawbacks of using the existing TE
algorithms for medical image segmentation and then design a source-free TE
framework that considers both class consistency and feature variety for better
estimation. Extensive experiments show that our method surpasses all current
algorithms for transferability estimation in medical image segmentation. Code
is available at https://github.com/EndoluminalSurgicalVision-IMR/CCFV",None,-1
0401b320-4b18-4f19-a017-07c8b6825ec5,Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation,0.401611,1,"This paper reports on the use of prompt engineering and GPT-3.5 for
biomedical query-focused multi-document summarisation. Using GPT-3.5 and
appropriate prompts, our system achieves top ROUGE-F1 results in the task of
obtaining short-paragraph-sized answers to biomedical questions in the 2023
BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in
other domains: 1) Prompts that incorporated few-shot samples generally improved
on their counterpart zero-shot variants; 2) The largest improvement was
achieved by retrieval augmented generation. The fact that these prompts allow
our top runs to rank within the top two runs of BioASQ 11b demonstrate the
power of using adequate prompts for Large Language Models in general, and
GPT-3.5 in particular, for query-focused summarisation.",None,-1
eb6d5345-5cb9-486f-b0d1-accdf50d56c2,Modelling Electricity Consumption in Irish Dairy Farms Using Agent-Based Modelling,0.132235,2,"Dairy farming can be an energy intensive form of farming. Understanding the
factors affecting electricity consumption on dairy farms is crucial for farm
owners and energy providers. In order to accurately estimate electricity
demands in dairy farms, it is necessary to develop a model. In this research
paper, an agent-based model is proposed to model the electricity consumption of
Irish dairy farms. The model takes into account various factors that affect the
energy consumption of dairy farms, including herd size, number of milking
machines, and time of year. The outputs are validated using existing
state-of-the-art dairy farm modelling frameworks. The proposed agent-based
model is fully explainable, which is an advantage over other Artificial
Intelligence techniques, e.g. deep learning.",None,-1
16721b8a-f536-49bc-b825-d8b4be5f0cfc,Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication,0.443566,1,"Individuals with complex communication needs (CCN) often rely on augmentative
and alternative communication (AAC) systems to have conversations and
communique their wants. Such systems allow message authoring by arranging
pictograms in sequence. However, the difficulty of finding the desired item to
complete a sentence can increase as the user's vocabulary increases. This paper
proposes using BERTimbau, a Brazilian Portuguese version of BERT, for pictogram
prediction in AAC systems. To finetune BERTimbau, we constructed an AAC corpus
for Brazilian Portuguese to use as a training corpus. We tested different
approaches to representing a pictogram for prediction: as a word (using
pictogram captions), as a concept (using a dictionary definition), and as a set
of synonyms (using related terms). We also evaluated the usage of images for
pictogram prediction. The results demonstrate that using embeddings computed
from the pictograms' caption, synonyms, or definitions have a similar
performance. Using synonyms leads to lower perplexity, but using captions leads
to the highest accuracies. This paper provides insight into how to represent a
pictogram for prediction using a BERT-like model and the potential of using
images for pictogram prediction.",None,-1
512fd9af-6dec-4f89-b293-3aed968580d9,Syllable-level lyrics generation from melody exploiting character-level language model,0.0215351,1,"The generation of lyrics tightly connected to accompanying melodies involves
establishing a mapping between musical notes and syllables of lyrics. This
process requires a deep understanding of music constraints and semantic
patterns at syllable-level, word-level, and sentence-level semantic meanings.
However, pre-trained language models specifically designed at the syllable
level are publicly unavailable. To solve these challenging issues, we propose
to exploit fine-tuning character-level language models for syllable-level
lyrics generation from symbolic melody. In particular, our method endeavors to
incorporate linguistic knowledge of the language model into the beam search
process of a syllable-level Transformer generator network. Additionally, by
exploring ChatGPT-based evaluation for generated lyrics, along with human
subjective evaluation, we demonstrate that our approach enhances the coherence
and correctness of the generated lyrics, eliminating the need to train
expensive new language models.",None,-1
dd49e829-4c07-4cfe-8d9e-f7e66f26d063,Dual-Alignment Pre-training for Cross-lingual Sentence Embedding,0.808211,7,"Recent studies have shown that dual encoder models trained with the
sentence-level translation ranking task are effective methods for cross-lingual
sentence embedding. However, our research indicates that token-level alignment
is also crucial in multilingual scenarios, which has not been fully explored
previously. Based on our findings, we propose a dual-alignment pre-training
(DAP) framework for cross-lingual sentence embedding that incorporates both
sentence-level and token-level alignment. To achieve this, we introduce a novel
representation translation learning (RTL) task, where the model learns to use
one-side contextualized token representation to reconstruct its translation
counterpart. This reconstruction objective encourages the model to embed
translation information into the token representation. Compared to other
token-level alignment methods such as translation language modeling, RTL is
more suitable for dual encoder architectures and is computationally efficient.
Extensive experiments on three sentence-level cross-lingual benchmarks
demonstrate that our approach can significantly improve sentence embedding. Our
code is available at https://github.com/ChillingDream/DAP.",None,-1
ff62ba9c-abbd-4147-a6ed-60935f356c1f,MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions,0.684426,9,"Audio-driven portrait animation aims to synthesize portrait videos that are
conditioned by given audio. Animating high-fidelity and multimodal video
portraits has a variety of applications. Previous methods have attempted to
capture different motion modes and generate high-fidelity portrait videos by
training different models or sampling signals from given videos. However,
lacking correlation learning between lip-sync and other movements (e.g., head
pose/eye blinking) usually leads to unnatural results. In this paper, we
propose a unified system for multi-person, diverse, and high-fidelity talking
portrait generation. Our method contains three stages, i.e., 1) Mapping-Once
network with Dual Attentions (MODA) generates talking representation from given
audio. In MODA, we design a dual-attention module to encode accurate mouth
movements and diverse modalities. 2) Facial composer network generates dense
and detailed face landmarks, and 3) temporal-guided renderer syntheses stable
videos. Extensive evaluations demonstrate that the proposed system produces
more natural and realistic video portraits compared to previous methods.",None,-1
a5cd9dda-415b-464e-951f-440fb29e6a9a,Jigsaw: Learning to Assemble Multiple Fractured Objects,0.869065,4,"Automated assembly of 3D fractures is essential in orthopedics, archaeology,
and our daily life. This paper presents Jigsaw, a novel framework for
assembling physically broken 3D objects from multiple pieces. Our approach
leverages hierarchical features of global and local geometry to match and align
the fracture surfaces. Our framework consists of four components: (1) front-end
point feature extractor with attention layers, (2) surface segmentation to
separate fracture and original parts, (3) multi-parts matching to find
correspondences among fracture surface points, and (4) robust global alignment
to recover the global poses of the pieces. We show how to jointly learn
segmentation and matching and seamlessly integrate feature matching and
rigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and
achieve superior performance compared to state-of-the-art methods. Our method
also generalizes well to diverse fracture modes, objects, and unseen instances.
To the best of our knowledge, this is the first learning-based method designed
specifically for 3D fracture assembly over multiple pieces. Our code is
available at https://jiaxin-lu.github.io/Jigsaw/.",None,-1
1fbb359d-41c8-4518-8b60-ab5f139269be,Devil's on the Edges: Selective Quad Attention for Scene Graph Generation,0.66183,15,"Scene graph generation aims to construct a semantic graph structure from an
image such that its nodes and edges respectively represent objects and their
relationships. One of the major challenges for the task lies in the presence of
distracting objects and relationships in images; contextual reasoning is
strongly distracted by irrelevant objects or backgrounds and, more importantly,
a vast number of irrelevant candidate relations. To tackle the issue, we
propose the Selective Quad Attention Network (SQUAT) that learns to select
relevant object pairs and disambiguate them via diverse contextual
interactions. SQUAT consists of two main components: edge selection and quad
attention. The edge selection module selects relevant object pairs, i.e., edges
in the scene graph, which helps contextual reasoning, and the quad attention
module then updates the edge features using both edge-to-node and edge-to-edge
cross-attentions to capture contextual information between objects and object
pairs. Experiments demonstrate the strong performance and robustness of SQUAT,
achieving the state of the art on the Visual Genome and Open Images v6
benchmarks.",None,-1
6194704e-87a0-4b1e-9730-719a5139df9e,EM Pre-training for Multi-party Dialogue Response Generation,0.851785,7,"Dialogue response generation requires an agent to generate a response
according to the current dialogue history, in terms of which two-party
dialogues have been well studied, but leaving a great gap for multi-party
dialogues at the same time. Different from two-party dialogues where each
response is a direct reply to its previous utterance, the addressee of a
response utterance should be specified before it is generated in the
multi-party scenario. Thanks to the huge amount of two-party conversational
data, various pre-trained language models for two-party dialogue response
generation have been proposed. However, due to the lack of annotated addressee
labels in multi-party dialogue datasets, it is hard to use them to pre-train a
response generation model for multi-party dialogues. To tackle this obstacle,
we propose an Expectation-Maximization (EM) approach that iteratively performs
the expectation steps to generate addressee labels, and the maximization steps
to optimize a response generation model. Theoretical analyses and extensive
experiments have justified the feasibility and effectiveness of our proposed
method.",None,-1
4b4517b6-3d6a-4138-a5fe-0fd6bd048a4c,Video Object Segmentation in Panoptic Wild Scenes,0.299364,6,"In this paper, we introduce semi-supervised video object segmentation (VOS)
to panoptic wild scenes and present a large-scale benchmark as well as a
baseline method for it. Previous benchmarks for VOS with sparse annotations are
not sufficient to train or evaluate a model that needs to process all possible
objects in real-world scenarios. Our new benchmark (VIPOSeg) contains
exhaustive object annotations and covers various real-world object categories
which are carefully divided into subsets of thing/stuff and seen/unseen classes
for comprehensive evaluation. Considering the challenges in panoptic VOS, we
propose a strong baseline method named panoptic object association with
transformers (PAOT), which uses panoptic identification to associate objects
with a pyramid architecture on multiple scales. Experimental results show that
VIPOSeg can not only boost the performance of VOS models by panoptic training
but also evaluate them comprehensively in panoptic scenes. Previous methods for
classic VOS still need to improve in performance and efficiency when dealing
with panoptic scenes, while our PAOT achieves SOTA performance with good
efficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in the
VOT2022 challenge. Our dataset is available at
https://github.com/yoxu515/VIPOSeg-Benchmark.",None,-1
65c85e70-187e-484f-be9c-6c4103d74d45,Multi-dimensional Signal Recovery using Low-rank Deconvolution,0.0706259,1,"In this work we present Low-rank Deconvolution, a powerful framework for
low-level feature-map learning for efficient signal representation with
application to signal recovery. Its formulation in multi-linear algebra
inherits properties from convolutional sparse coding and low-rank approximation
methods as in this setting signals are decomposed in a set of filters convolved
with a set of low-rank tensors. We show its advantages by learning compressed
video representations and solving image in-painting problems.",None,-1
c2192a1d-e5ed-4c14-917e-af103daa271c,Explore the difficulty of words and its influential attributes based on the Wordle game,0.681788,3,"We adopt the distribution and expectation of guessing times in game Wordle as
metrics to predict the difficulty of words and explore their influence factors.
In order to predictthe difficulty distribution, we use Monte Carlo to simulate
the guessing process of players and then narrow the gap between raw and actual
distribution of guessing times for each word with Markov which generates the
associativity of words. Afterwards, we take advantage of lasso regression to
predict the deviation of guessing times expectation and quadratic programming
to obtain the correction of the original distribution.To predict the difficulty
levels, we first use hierarchical clustering to classify the difficulty levels
based on the expectation of guessing times. Afterwards we downscale the
variables of lexical attributes based on factor analysis. Significant factors
include the number of neighboring words, letter similarity, sub-string
similarity, and word frequency. Finally, we build the relationship between
lexical attributes and difficulty levels through ordered logistic regression.",None,-1
1e8e252f-9d96-41c5-a84a-0bfdf04a647d,Automatic Spell Checker and Correction for Under-represented Spoken Languages: Case Study on Wolof,0.93919,2,"This paper presents a spell checker and correction tool specifically designed
for Wolof, an under-represented spoken language in Africa. The proposed spell
checker leverages a combination of a trie data structure, dynamic programming,
and the weighted Levenshtein distance to generate suggestions for misspelled
words. We created novel linguistic resources for Wolof, such as a lexicon and a
corpus of misspelled words, using a semi-automatic approach that combines
manual and automatic annotation methods. Despite the limited data available for
the Wolof language, the spell checker's performance showed a predictive
accuracy of 98.31% and a suggestion accuracy of 93.33%. Our primary focus
remains the revitalization and preservation of Wolof as an Indigenous and
spoken language in Africa, providing our efforts to develop novel linguistic
resources. This work represents a valuable contribution to the growth of
computational tools and resources for the Wolof language and provides a strong
foundation for future studies in the automatic spell checking and correction
field.",None,-1
75874c14-a8d1-477d-bfa2-efa4a26fe71a,ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text Translation,0.779296,6,"Joint speech-language training is challenging due to the large demand for
training data and GPU consumption, as well as the modality gap between speech
and language. We present ComSL, a speech-language model built atop a composite
architecture of public pretrained speech-only and language-only models and
optimized data-efficiently for spoken language tasks. Particularly, we propose
to incorporate cross-modality learning into transfer learning and conduct them
simultaneously for downstream tasks in a multi-task learning manner. Our
approach has demonstrated effectiveness in end-to-end speech-to-text
translation tasks, achieving a new state-of-the-art average BLEU score of 31.5
on the multilingual speech to English text translation task for 21 languages,
as measured on the public CoVoST2 evaluation set.",None,-1
5ffb6005-0194-46f9-8387-eb92fa897003,Memotion 3: Dataset on Sentiment and Emotion Analysis of Codemixed Hindi-English Memes,0.946003,9,"Memes are the new-age conveyance mechanism for humor on social media sites.
Memes often include an image and some text. Memes can be used to promote
disinformation or hatred, thus it is crucial to investigate in details. We
introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other
prevalent datasets in the domain, including prior iterations of Memotion,
Memotion 3 introduces Hindi-English Codemixed memes while prior works in the
area were limited to only the English memes. We describe the Memotion task, the
data collection and the dataset creation methodologies. We also provide a
baseline for the task. The baseline code and dataset will be made available at
https://github.com/Shreyashm16/Memotion-3.0",None,-1
8c6e74b4-3ced-4c9f-8602-dbd5a42b5ca0,Fine-tuning Large Enterprise Language Models via Ontological Reasoning,0.323921,11,"Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to
diverse goals, thanks to task-specific training data. Task specificity should
go hand in hand with domain orientation, that is, the specialization of an LLM
to accurately address the tasks of a given realm of interest. However, models
are usually fine-tuned over publicly available data or, at most, over ground
data from databases, ignoring business-level definitions and domain experience.
On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and
augment such domain knowledge via ontological reasoning. With the goal of
combining LLM flexibility with the domain orientation of EKGs, we propose a
novel neurosymbolic architecture that leverages the power of ontological
reasoning to build task- and domain-specific corpora for LLM fine-tuning.",None,-1
3ee93835-ecdf-4105-8e08-c69373b03e69,ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation,0.766259,3,"Recent advancements in dense out-of-distribution (OOD) detection have
primarily focused on scenarios where the training and testing datasets share a
similar domain, with the assumption that no domain shift exists between them.
However, in real-world situations, domain shift often exits and significantly
affects the accuracy of existing out-of-distribution (OOD) detection models. In
this work, we propose a dual-level OOD detection framework to handle domain
shift and semantic shift jointly. The first level distinguishes whether domain
shift exists in the image by leveraging global low-level features, while the
second level identifies pixels with semantic shift by utilizing dense
high-level feature maps. In this way, we can selectively adapt the model to
unseen domains as well as enhance model's capacity in detecting novel classes.
We validate the efficacy of our proposed method on several OOD segmentation
benchmarks, including those with significant domain shifts and those without,
observing consistent performance improvements across various baseline models.
Code is available at
${\href{https://github.com/gaozhitong/ATTA}{https://github.com/gaozhitong/ATTA}}$.",None,-1
555f4b64-52fb-4565-99bd-e4f4926c6484,The Change You Want to See (Now in 3D),0.322405,3,"The goal of this paper is to detect what has changed, if anything, between
two ""in the wild"" images of the same 3D scene acquired from different camera
positions and at different temporal instances. The open-set nature of this
problem, occlusions/dis-occlusions due to the shift in viewpoint, and the lack
of suitable training datasets, presents substantial challenges in devising a
solution.
  To address this problem, we contribute a change detection model that is
trained entirely on synthetic data and is class-agnostic, yet it is performant
out-of-the-box on real world images without requiring fine-tuning. Our solution
entails a ""register and difference"" approach that leverages self-supervised
frozen embeddings and feature differences, which allows the model to generalise
to a wide variety of scenes and domains. The model is able to operate directly
on two RGB images, without requiring access to ground truth camera intrinsics,
extrinsics, depth maps, point clouds, or additional before-after images.
Finally, we collect and release a new evaluation dataset consisting of
real-world image pairs with human-annotated differences and demonstrate the
efficacy of our method. The code, datasets and pre-trained model can be found
at: https://github.com/ragavsachdeva/CYWS-3D",None,-1
6bf408eb-6d69-4465-aeb2-f96384f4ce3a,"Can I say, now machines can think?",0.0552321,3,"Generative AI techniques have opened the path for new generations of machines
in diverse domains. These machines have various capabilities for example, they
can produce images, generate answers or stories, and write codes based on the
""prompts"" only provided by users. These machines are considered 'thinking
minds' because they have the ability to generate human-like responses. In this
study, we have analyzed and explored the capabilities of artificial
intelligence-enabled machines. We have revisited on Turing's concept of
thinking machines and compared it with recent technological advancements. The
objections and consequences of the thinking machines are also discussed in this
study, along with available techniques to evaluate machines' cognitive
capabilities. We have concluded that Turing Test is a critical aspect of
evaluating machines' ability. However, there are other aspects of intelligence
too, and AI machines exhibit most of these aspects.",None,-1
336e12f8-646e-4825-8136-71fa7f885438,Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning,0.268384,2,"We propose a model-free reinforcement learning architecture, called
distributed attentional actor architecture after conditional attention (DA6-X),
to provide better interpretability of conditional coordinated behaviors. The
underlying principle involves reusing the saliency vector, which represents the
conditional states of the environment, such as the global position of agents.
Hence, agents with DA6-X flexibility built into their policy exhibit superior
performance by considering the additional information in the conditional states
during the decision-making process. The effectiveness of the proposed method
was experimentally evaluated by comparing it with conventional methods in an
objects collection game. By visualizing the attention weights from DA6-X, we
confirmed that agents successfully learn situation-dependent coordinated
behaviors by correctly identifying various conditional states, leading to
improved interpretability of agents along with superior performance.",None,-1
3563cc4c-c01f-49d9-9749-f4355d2fa976,Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT,0.360932,12,"Prompts have been proven to play a crucial role in large language models, and
in recent years, vision models have also been using prompts to improve
scalability for multiple downstream tasks. In this paper, we focus on adapting
prompt design based on instruction tuning into a visual transformer model for
image classification which we called Instruction-ViT. The key idea is to
implement multi-modal prompts (text or image prompt) related to category
information to guide the fine-tuning of the model. Based on the experiments of
several image captionining tasks, the performance and domain adaptability were
improved. Our work provided an innovative strategy to fuse multi-modal prompts
with better performance and faster adaptability for visual classification
models.",None,-1
14dbe66c-4c32-4504-83ea-42904882f32e,Spatial-temporal Transformer for Affective Behavior Analysis,0.625558,3,"The in-the-wild affective behavior analysis has been an important study. In
this paper, we submit our solutions for the 5th Workshop and Competition on
Affective Behavior Analysis in-the-wild (ABAW), which includes V-A Estimation,
Facial Expression Classification and AU Detection Sub-challenges. We propose a
Transformer Encoder with Multi-Head Attention framework to learn the
distribution of both the spatial and temporal features. Besides, there are
virious effective data augmentation strategies employed to alleviate the
problems of sample imbalance during model training. The results fully
demonstrate the effectiveness of our proposed model based on the Aff-Wild2
dataset.",None,-1
6e955b31-6eb2-4e49-8de1-cf80f357b697,Linguistic ambiguity analysis in ChatGPT,0.335841,17,"Linguistic ambiguity is and has always been one of the main challenges in
Natural Language Processing (NLP) systems. Modern Transformer architectures
like BERT, T5 or more recently InstructGPT have achieved some impressive
improvements in many NLP fields, but there is still plenty of work to do.
Motivated by the uproar caused by ChatGPT, in this paper we provide an
introduction to linguistic ambiguity, its varieties and their relevance in
modern NLP, and perform an extensive empiric analysis. ChatGPT strengths and
weaknesses are revealed, as well as strategies to get the most of this model.",None,-1
f3d540c7-6070-49ff-a88e-c9639519710c,How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model,0.721834,60,"Pre-trained language models can be surprisingly adept at tasks they were not
explicitly trained on, but how they implement these capabilities is poorly
understood. In this paper, we investigate the basic mathematical abilities
often acquired by pre-trained language models. Concretely, we use mechanistic
interpretability techniques to explain the (limited) mathematical abilities of
GPT-2 small. As a case study, we examine its ability to take in sentences such
as ""The war lasted from the year 1732 to the year 17"", and predict valid
two-digit end years (years > 32). We first identify a circuit, a small subset
of GPT-2 small's computational graph that computes this task's output. Then, we
explain the role of each circuit component, showing that GPT-2 small's final
multi-layer perceptrons boost the probability of end years greater than the
start year. Finally, we find related tasks that activate our circuit. Our
results suggest that GPT-2 small computes greater-than using a complex but
general mechanism that activates across diverse contexts.",None,-1
bbb1bdd0-dd36-45bf-9b7d-fd71db9ea223,Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue,0.0942663,1,"The demand for multimodal dialogue systems has been rising in various
domains, emphasizing the importance of interpreting multimodal inputs from
conversational and situational contexts. We explore three methods to tackle
this problem and evaluate them on the largest situated dialogue dataset, SIMMC
2.1. Our best method, scene-dialogue alignment, improves the performance by
~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and
discussion regarding the limitation of our methods and the potential directions
for future works. Our code is publicly available at
https://github.com/holylovenia/multimodal-object-identification.",None,-1
030edee2-468d-4f1a-b0a5-4b65bd4e9c63,DeDrift: Robust Similarity Search under Content Drift,0.811387,4,"The statistical distribution of content uploaded and searched on media
sharing sites changes over time due to seasonal, sociological and technical
factors. We investigate the impact of this ""content drift"" for large-scale
similarity search tools, based on nearest neighbor search in embedding space.
Unless a costly index reconstruction is performed frequently, content drift
degrades the search accuracy and efficiency. The degradation is especially
severe since, in general, both the query and database distributions change.
  We introduce and analyze real-world image and video datasets for which
temporal information is available over a long time period. Based on the
learnings, we devise DeDrift, a method that updates embedding quantizers to
continuously adapt large-scale indexing structures on-the-fly. DeDrift almost
eliminates the accuracy degradation due to the query and database content drift
while being up to 100x faster than a full index reconstruction.",None,-1
e0842ac5-ecec-404d-a23d-2b214b2e98e9,Target-Side Augmentation for Document-Level Machine Translation,0.696954,5,"Document-level machine translation faces the challenge of data sparsity due
to its long input length and a small amount of training data, increasing the
risk of learning spurious patterns. To address this challenge, we propose a
target-side augmentation method, introducing a data augmentation (DA) model to
generate many potential translations for each source document. Learning on
these wider range translations, an MT model can learn a smoothed distribution,
thereby reducing the risk of data sparsity. We demonstrate that the DA model,
which estimates the posterior distribution, largely improves the MT
performance, outperforming the previous best system by 2.30 s-BLEU on News and
achieving new state-of-the-art on News and Europarl benchmarks. Our code is
available at https://github.com/baoguangsheng/target-side-augmentation.",None,-1
bff30f11-9b84-46ca-99fe-c1c5c14abf2a,Large Language Model Enhanced Multi-Agent Systems for 6G Communications,0.897881,7,"The rapid development of the Large Language Model (LLM) presents huge
opportunities for 6G communications, e.g., network optimization and management
by allowing users to input task requirements to LLMs by nature language.
However, directly applying native LLMs in 6G encounters various challenges,
such as a lack of private communication data and knowledge, limited logical
reasoning, evaluation, and refinement abilities. Integrating LLMs with the
capabilities of retrieval, planning, memory, evaluation and reflection in
agents can greatly enhance the potential of LLMs for 6G communications. To this
end, we propose a multi-agent system with customized communication knowledge
and tools for solving communication related tasks using natural language,
comprising three components: (1) Multi-agent Data Retrieval (MDR), which
employs the condensate and inference agents to refine and summarize
communication knowledge from the knowledge base, expanding the knowledge
boundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning
(MCP), which utilizes multiple planning agents to generate feasible solutions
for the communication related task from different perspectives based on the
retrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which
utilizes the evaluation agent to assess the solutions, and applies the
reflexion agent and refinement agent to provide improvement suggestions for
current solutions. Finally, we validate the effectiveness of the proposed
multi-agent system by designing a semantic communication system, as a case
study of 6G communications.",None,-1
5b6e9662-2a40-4da6-ab30-6da274a82e2d,On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence,0.999999,80,"Large pre-trained models, also known as foundation models (FMs), are trained
in a task-agnostic manner on large-scale data and can be adapted to a wide
range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.
Despite their successes in language and vision tasks, we have yet seen an
attempt to develop foundation models for geospatial artificial intelligence
(GeoAI). In this work, we explore the promises and challenges of developing
multimodal foundation models for GeoAI. We first investigate the potential of
many existing FMs by testing their performances on seven tasks across multiple
geospatial subdomains including Geospatial Semantics, Health Geography, Urban
Geography, and Remote Sensing. Our results indicate that on several geospatial
tasks that only involve text modality such as toponym recognition, location
description recognition, and US state-level/county-level dementia time series
forecasting, these task-agnostic LLMs can outperform task-specific
fully-supervised models in a zero-shot or few-shot learning setting. However,
on other geospatial tasks, especially tasks that involve multiple data
modalities (e.g., POI-based urban function classification, street view
image-based urban noise intensity classification, and remote sensing image
scene classification), existing foundation models still underperform
task-specific models. Based on these observations, we propose that one of the
major challenges of developing a FM for GeoAI is to address the multimodality
nature of geospatial tasks. After discussing the distinct challenges of each
geospatial data modality, we suggest the possibility of a multimodal foundation
model which can reason over various types of geospatial data through geospatial
alignments. We conclude this paper by discussing the unique risks and
challenges to develop such a model for GeoAI.",None,-1
d82953c7-048e-417d-80db-7fe9d7f22744,Inter-connection: Effective Connection between Pre-trained Encoder and Decoder for Speech Translation,0.460028,4,"In end-to-end speech translation, speech and text pre-trained models improve
translation quality. Recently proposed models simply connect the pre-trained
models of speech and text as encoder and decoder. Therefore, only the
information from the final layer of encoders is input to the decoder. Since it
is clear that the speech pre-trained model outputs different information from
each layer, the simple connection method cannot fully utilize the information
that the speech pre-trained model has. In this study, we propose an
inter-connection mechanism that aggregates the information from each layer of
the speech pre-trained model by weighted sums and inputs into the decoder. This
mechanism increased BLEU by approximately 2 points in en-de, en-ja, and en-zh
by increasing parameters by 2K when the speech pre-trained model was frozen.
Furthermore, we investigated the contribution of each layer for each language
by visualizing layer weights and found that the contributions were different.",None,-1
3ce39ec7-ccee-42b2-94ed-eb887bb14f7a,When can transformers reason with abstract symbols?,0.18202,6,"We investigate the capabilities of transformer models on relational reasoning
tasks. In these tasks, models are trained on a set of strings encoding abstract
relations, and are then tested out-of-distribution on data that contains
symbols that did not appear in the training dataset. We prove that for any
relational reasoning task in a large family of tasks, transformers learn the
abstract relations and generalize to the test set when trained by gradient
descent on sufficiently large quantities of training data. This is in contrast
to classical fully-connected networks, which we prove fail to learn to reason.
Our results inspire modifications of the transformer architecture that add only
two trainable parameters per head, and that we empirically demonstrate improve
data efficiency for learning to reason.",None,-1
2f61d197-84af-4cc2-a9ea-035e90786534,DiffNAS: Bootstrapping Diffusion Models by Prompting for Better Architectures,0.0991601,3,"Diffusion models have recently exhibited remarkable performance on synthetic
data. After a diffusion path is selected, a base model, such as UNet, operates
as a denoising autoencoder, primarily predicting noises that need to be
eliminated step by step. Consequently, it is crucial to employ a model that
aligns with the expected budgets to facilitate superior synthetic performance.
In this paper, we meticulously analyze the diffusion model and engineer a base
model search approach, denoted ""DiffNAS"". Specifically, we leverage GPT-4 as a
supernet to expedite the search, supplemented with a search memory to enhance
the results. Moreover, we employ RFID as a proxy to promptly rank the
experimental outcomes produced by GPT-4. We also adopt a rapid-convergence
training strategy to boost search efficiency. Rigorous experimentation
corroborates that our algorithm can augment the search efficiency by 2 times
under GPT-based scenarios, while also attaining a performance of 2.82 with 0.37
improvement in FID on CIFAR10 relative to the benchmark IDDPM algorithm.",None,-1
dbc0de06-f948-4bd6-b44b-b2b95ae02cd2,Can Large Language Models Change User Preference Adversarially?,0.0499808,7,"Pretrained large language models (LLMs) are becoming increasingly powerful
and ubiquitous in mainstream applications such as being a personal assistant, a
dialogue model, etc. As these models become proficient in deducing user
preferences and offering tailored assistance, there is an increasing concern
about the ability of these models to influence, modify and in the extreme case
manipulate user preference adversarially. The issue of lack of interpretability
in these models in adversarial settings remains largely unsolved. This work
tries to study adversarial behavior in user preferences from the lens of
attention probing, red teaming and white-box analysis. Specifically, it
provides a bird's eye view of existing literature, offers red teaming samples
for dialogue models like ChatGPT and GODEL and probes the attention mechanism
in the latter for non-adversarial and adversarial settings.",None,-1
c5b7e139-9d2c-4379-b514-5bca98fd9c22,Explainable Depression Detection via Head Motion Patterns,0.106086,2,"While depression has been studied via multimodal non-verbal behavioural cues,
head motion behaviour has not received much attention as a biomarker. This
study demonstrates the utility of fundamental head-motion units, termed
\emph{kinemes}, for depression detection by adopting two distinct approaches,
and employing distinctive features: (a) discovering kinemes from head motion
data corresponding to both depressed patients and healthy controls, and (b)
learning kineme patterns only from healthy controls, and computing statistics
derived from reconstruction errors for both the patient and control classes.
Employing machine learning methods, we evaluate depression classification
performance on the \emph{BlackDog} and \emph{AVEC2013} datasets. Our findings
indicate that: (1) head motion patterns are effective biomarkers for detecting
depressive symptoms, and (2) explanatory kineme patterns consistent with prior
findings can be observed for the two classes. Overall, we achieve peak F1
scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 for binary
classification over episodic \emph{thin-slices}, and a peak F1 of 0.72 over
videos for AVEC2013.",None,-1
c910e498-3397-48b5-90cc-18fa0af9d42c,Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments,0.0832761,1,"Trustworthiness of artificially intelligent agents is vital for the
acceptance of human-machine teaming in industrial manufacturing environments.
Predictable behaviours and explainable (and understandable) rationale allow
humans collaborating with (and building) these agents to understand their
motivations and therefore validate decisions that are made. To that aim, we
make use of G\""ardenfors's cognitively inspired Conceptual Space framework to
represent the agent's knowledge using concepts as convex regions in a space
spanned by inherently comprehensible quality dimensions. A simple typicality
quantification model is built on top of it to determine fuzzy category
membership and classify instances interpretably. We apply it on a use case from
the manufacturing domain, using objects' physical properties obtained from
cobots' onboard sensors and utilisation properties from crowdsourced
commonsense knowledge available at public knowledge bases. Such flexible
knowledge representation based on property decomposition allows for
data-efficient representation learning of typically highly specialist or
specific manufacturing artefacts. In such a setting, traditional data-driven
(e.g., computer vision-based) classification approaches would struggle due to
training data scarcity. This allows for comprehensibility of an AI agent's
acquired knowledge by the human collaborator thus contributing to
trustworthiness. We situate our approach within an existing explainability
framework specifying explanation desiderata. We provide arguments for our
system's applicability and appropriateness for different roles of human agents
collaborating with the AI system throughout its design, validation, and
operation.",None,-1
2694b83f-66d1-42f0-8596-4e263b56ed79,Implementation of a noisy hyperlink removal system: A semantic and relatedness approach,0.446838,2,"As the volume of data on the web grows, the web structure graph, which is a
graph representation of the web, continues to evolve. The structure of this
graph has gradually shifted from content-based to non-content-based.
Furthermore, spam data, such as noisy hyperlinks, in the web structure graph
adversely affect the speed and efficiency of information retrieval and link
mining algorithms. Previous works in this area have focused on removing noisy
hyperlinks using structural and string approaches. However, these approaches
may incorrectly remove useful links or be unable to detect noisy hyperlinks in
certain circumstances. In this paper, a data collection of hyperlinks is
initially constructed using an interactive crawler. The semantic and
relatedness structure of the hyperlinks is then studied through semantic web
approaches and tools such as the DBpedia ontology. Finally, the removal process
of noisy hyperlinks is carried out using a reasoner on the DBpedia ontology.
Our experiments demonstrate the accuracy and ability of semantic web
technologies to remove noisy hyperlinks",None,-1
d8dd4692-7f0d-4eb8-89eb-bfe4d270bfc3,Bi-ViT: Pushing the Limit of Vision Transformer Quantization,0.0524398,1,"Vision transformers (ViTs) quantization offers a promising prospect to
facilitate deploying large pre-trained networks on resource-limited devices.
Fully-binarized ViTs (Bi-ViT) that pushes the quantization of ViTs to its limit
remain largely unexplored and a very challenging task yet, due to their
unacceptable performance. Through extensive empirical analyses, we identify the
severe drop in ViT binarization is caused by attention distortion in
self-attention, which technically stems from the gradient vanishing and ranking
disorder. To address these issues, we first introduce a learnable scaling
factor to reactivate the vanished gradients and illustrate its effectiveness
through theoretical and experimental analyses. We then propose a ranking-aware
distillation method to rectify the disordered ranking in a teacher-student
framework. Bi-ViT achieves significant improvements over popular DeiT and Swin
backbones in terms of Top-1 accuracy and FLOPs. For example, with DeiT-Tiny and
Swin-Tiny, our method significantly outperforms baselines by 22.1% and 21.4%
respectively, while 61.5x and 56.1x theoretical acceleration in terms of FLOPs
compared with real-valued counterparts on ImageNet.",None,-1
93fdc74c-4b6b-49b0-ade0-cce65a7a5e65,Zero-shot Referring Image Segmentation with Global-Local Context Features,0.695557,20,"Referring image segmentation (RIS) aims to find a segmentation mask given a
referring expression grounded to a region of the input image. Collecting
labelled datasets for this task, however, is notoriously costly and
labor-intensive. To overcome this issue, we propose a simple yet effective
zero-shot referring image segmentation method by leveraging the pre-trained
cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded
to the input text, we propose a mask-guided visual encoder that captures global
and local contextual information of an input image. By utilizing instance masks
obtained from off-the-shelf mask proposal techniques, our method is able to
segment fine-detailed Istance-level groundings. We also introduce a
global-local text encoder where the global feature captures complex
sentence-level semantics of the entire input expression while the local feature
focuses on the target noun phrase extracted by a dependency parser. In our
experiments, the proposed method outperforms several zero-shot baselines of the
task and even the weakly supervised referring expression segmentation method
with substantial margins. Our code is available at
https://github.com/Seonghoon-Yu/Zero-shot-RIS.",None,-1
0526d4dc-5f53-40a1-8700-e8a22928ee7b,Sample Attackability in Natural Language Adversarial Attacks,0.10873,1,"Adversarial attack research in natural language processing (NLP) has made
significant progress in designing powerful attack methods and defence
approaches. However, few efforts have sought to identify which source samples
are the most attackable or robust, i.e. can we determine for an unseen target
model, which samples are the most vulnerable to an adversarial attack. This
work formally extends the definition of sample attackability/robustness for NLP
attacks. Experiments on two popular NLP datasets, four state of the art models
and four different NLP adversarial attack methods, demonstrate that sample
uncertainty is insufficient for describing characteristics of attackable/robust
samples and hence a deep learning based detector can perform much better at
identifying the most attackable and robust samples for an unseen target model.
Nevertheless, further analysis finds that there is little agreement in which
samples are considered the most attackable/robust across different NLP attack
methods, explaining a lack of portability of attackability detection methods
across attack methods.",None,-1
b5d4916e-4285-46b9-8cc7-e7329d8fb39e,Human Pose Estimation in Extremely Low-Light Conditions,0.670756,8,"We study human pose estimation in extremely low-light images. This task is
challenging due to the difficulty of collecting real low-light images with
accurate labels, and severely corrupted inputs that degrade prediction quality
significantly. To address the first issue, we develop a dedicated camera system
and build a new dataset of real low-light images with accurate pose labels.
Thanks to our camera system, each low-light image in our dataset is coupled
with an aligned well-lit image, which enables accurate pose labeling and is
used as privileged information during training. We also propose a new model and
a new training strategy that fully exploit the privileged information to learn
representation insensitive to lighting conditions. Our method demonstrates
outstanding performance on real extremely low light images, and extensive
analyses validate that both of our model and dataset contribute to the success.",None,-1
3d5b5051-e937-4715-94c7-23d0f114146f,Removing Image Artifacts From Scratched Lens Protectors,0.0993372,1,"A protector is placed in front of the camera lens for mobile devices to avoid
damage, while the protector itself can be easily scratched accidentally,
especially for plastic ones. The artifacts appear in a wide variety of
patterns, making it difficult to see through them clearly. Removing image
artifacts from the scratched lens protector is inherently challenging due to
the occasional flare artifacts and the co-occurring interference within mixed
artifacts. Though different methods have been proposed for some specific
distortions, they seldom consider such inherent challenges. In our work, we
consider the inherent challenges in a unified framework with two cooperative
modules, which facilitate the performance boost of each other. We also collect
a new dataset from the real world to facilitate training and evaluation
purposes. The experimental results demonstrate that our method outperforms the
baselines qualitatively and quantitatively. The code and datasets will be
released after acceptance.",None,-1
4a9d769b-20a5-475a-a9a0-a92e24912d49,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,0.998479,223,"We propose MM-Vet, an evaluation benchmark that examines large multimodal
models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various
intriguing abilities, such as solving math problems written on the blackboard,
reasoning about events and celebrities in news images, and explaining visual
jokes. Rapid model advancements pose challenges to evaluation benchmark
development. Problems include: (1) How to systematically structure and evaluate
the complicated multimodal tasks; (2) How to design evaluation metrics that
work well across question and answer types; and (3) How to give model insights
beyond a simple performance ranking. To this end, we present MM-Vet, designed
based on the insight that the intriguing ability to solve complicated tasks is
often achieved by a generalist model being able to integrate different core
vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and
examines the 16 integrations of interest derived from the capability
combination. For evaluation metrics, we propose an LLM-based evaluator for
open-ended outputs. The evaluator enables the evaluation across different
question types and answer styles, resulting in a unified scoring metric. We
evaluate representative LMMs on MM-Vet, providing insights into the
capabilities of different LMM system paradigms and models. Code and data are
available at https://github.com/yuweihao/MM-Vet.",None,-1
b1d600ce-3138-4f40-8de0-af231f4c757f,Physical Deep Reinforcement Learning: Safety and Unknown Unknowns,0.265044,3,"In this paper, we propose the Phy-DRL: a physics-model-regulated deep
reinforcement learning framework for safety-critical autonomous systems. The
Phy-DRL is unique in three innovations: i) proactive unknown-unknowns training,
ii) conjunctive residual control (i.e., integration of data-driven control and
physics-model-based control) and safety- \& stability-sensitive reward, and
iii) physics-model-based neural network editing, including link editing and
activation editing. Thanks to the concurrent designs, the Phy-DRL is able to 1)
tolerate unknown-unknowns disturbances, 2) guarantee mathematically provable
safety and stability, and 3) strictly comply with physical knowledge pertaining
to Bellman equation and reward. The effectiveness of the Phy-DRL is finally
validated by an inverted pendulum and a quadruped robot. The experimental
results demonstrate that compared with purely data-driven DRL, Phy-DRL features
remarkably fewer learning parameters, accelerated training and enlarged reward,
while offering enhanced model robustness and safety assurance.",None,-1
7e51949a-72c6-47bd-a587-20cd39a738e5,"Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",0.994502,72,"Large Language Models (LLMs) have made remarkable strides in various tasks.
Whether LLMs are competitive few-shot solvers for information extraction (IE)
tasks, however, remains an open problem. In this work, we aim to provide a
thorough answer to this question. Through extensive experiments on nine
datasets across four IE tasks, we demonstrate that current advanced LLMs
consistently exhibit inferior performance, higher latency, and increased budget
requirements compared to fine-tuned SLMs under most settings. Therefore, we
conclude that LLMs are not effective few-shot information extractors in
general. Nonetheless, we illustrate that with appropriate prompting strategies,
LLMs can effectively complement SLMs and tackle challenging samples that SLMs
struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm
to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as
filters and LLMs serve as rerankers. By prompting LLMs to rerank a small
portion of difficult samples identified by SLMs, our preliminary system
consistently achieves promising improvements (2.4% F1-gain on average) on
various IE tasks, with an acceptable time and cost investment.",None,-1
774e73e6-4454-48e3-a609-b9e91be93d2c,Face Transformer: Towards High Fidelity and Accurate Face Swapping,0.359632,5,"Face swapping aims to generate swapped images that fuse the identity of
source faces and the attributes of target faces. Most existing works address
this challenging task through 3D modelling or generation using generative
adversarial networks (GANs), but 3D modelling suffers from limited
reconstruction accuracy and GANs often struggle in preserving subtle yet
important identity details of source faces (e.g., skin colors, face features)
and structural attributes of target faces (e.g., face shapes, facial
expressions). This paper presents Face Transformer, a novel face swapping
network that can accurately preserve source identities and target attributes
simultaneously in the swapped face images. We introduce a transformer network
for the face swapping task, which learns high-quality semantic-aware
correspondence between source and target faces and maps identity features of
source faces to the corresponding region in target faces. The high-quality
semantic-aware correspondence enables smooth and accurate transfer of source
identity information with minimal modification of target shapes and
expressions. In addition, our Face Transformer incorporates a multi-scale
transformation mechanism for preserving the rich fine facial details. Extensive
experiments show that our Face Transformer achieves superior face swapping
performance qualitatively and quantitatively.",None,-1
4187c73e-0e56-4590-9283-043e809e0531,ICICLE: Interpretable Class Incremental Continual Learning,0.676481,15,"Continual learning enables incremental learning of new tasks without
forgetting those previously learned, resulting in positive knowledge transfer
that can enhance performance on both new and old tasks. However, continual
learning poses new challenges for interpretability, as the rationale behind
model predictions may change over time, leading to interpretability concept
drift. We address this problem by proposing Interpretable Class-InCremental
LEarning (ICICLE), an exemplar-free approach that adopts a prototypical
part-based approach. It consists of three crucial novelties: interpretability
regularization that distills previously learned concepts while preserving
user-friendly positive reasoning; proximity-based prototype initialization
strategy dedicated to the fine-grained setting; and task-recency bias
compensation devoted to prototypical parts. Our experimental results
demonstrate that ICICLE reduces the interpretability concept drift and
outperforms the existing exemplar-free methods of common class-incremental
learning when applied to concept-based models.",None,-1
949a95e1-6b90-4d67-99bd-5c88d80b4ad9,Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages,0.553307,8,"Sentiment analysis (SA) systems are used in many products and hundreds of
languages. Gender and racial biases are well-studied in English SA systems, but
understudied in other languages, with few resources for such studies. To remedy
this, we build a counterfactual evaluation corpus for gender and racial/migrant
bias in four languages. We demonstrate its usefulness by answering a simple but
important question that an engineer might need to answer when deploying a
system: What biases do systems import from pre-trained models when compared to
a baseline with no pre-training? Our evaluation corpus, by virtue of being
counterfactual, not only reveals which models have less bias, but also
pinpoints changes in model bias behaviour, which enables more targeted
mitigation strategies. We release our code and evaluation corpora to facilitate
future research.",None,-1
ad57c0fc-1f01-4819-97c7-b83c929139b0,VLN-Trans: Translator for the Vision and Language Navigation Agent,0.371693,5,"Language understanding is essential for the navigation agent to follow
instructions. We observe two kinds of issues in the instructions that can make
the navigation task challenging: 1. The mentioned landmarks are not
recognizable by the navigation agent due to the different vision abilities of
the instructor and the modeled agent. 2. The mentioned landmarks are applicable
to multiple targets, thus not distinctive for selecting the target among the
candidate viewpoints. To deal with these issues, we design a translator module
for the navigation agent to convert the original instructions into
easy-to-follow sub-instruction representations at each step. The translator
needs to focus on the recognizable and distinctive landmarks based on the
agent's visual abilities and the observed visual environment. To achieve this
goal, we create a new synthetic sub-instruction dataset and design specific
tasks to train the translator and the navigation agent. We evaluate our
approach on Room2Room~(R2R), Room4room~(R4R), and Room2Room Last (R2R-Last)
datasets and achieve state-of-the-art results on multiple benchmarks.",None,-1
b40df4fc-d6a7-4e3b-9e37-607eeb9236cc,FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding,0.726271,12,"Although Domain Adaptation in Semantic Scene Segmentation has shown
impressive improvement in recent years, the fairness concerns in the domain
adaptation have yet to be well defined and addressed. In addition, fairness is
one of the most critical aspects when deploying the segmentation models into
human-related real-world applications, e.g., autonomous driving, as any unfair
predictions could influence human safety. In this paper, we propose a novel
Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In
particular, from the proposed formulated fairness objective, a new adaptation
framework will be introduced based on the fair treatment of class
distributions. Moreover, to generally model the context of structural
dependency, a new conditional structural constraint is introduced to impose the
consistency of predicted segmentation. Thanks to the proposed Conditional
Structure Network, the self-attention mechanism has sufficiently modeled the
structural information of segmentation. Through the ablation studies, the
proposed method has shown the performance improvement of the segmentation
models and promoted fairness in the model predictions. The experimental results
on the two standard benchmarks, i.e., SYNTHIA $\to$ Cityscapes and GTA5 $\to$
Cityscapes, have shown that our method achieved State-of-the-Art (SOTA)
performance.",None,-1
5164e31d-2180-455b-a740-289601e59bcf,Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation,0.463245,56,"Despite efforts to align large language models to produce harmless responses,
they are still vulnerable to jailbreak prompts that elicit unrestricted
behaviour. In this work, we investigate persona modulation as a black-box
jailbreaking method to steer a target model to take on personalities that are
willing to comply with harmful instructions. Rather than manually crafting
prompts for each persona, we automate the generation of jailbreaks using a
language model assistant. We demonstrate a range of harmful completions made
possible by persona modulation, including detailed instructions for
synthesising methamphetamine, building a bomb, and laundering money. These
automated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is
185 times larger than before modulation (0.23%). These prompts also transfer to
Claude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,
respectively. Our work reveals yet another vulnerability in commercial large
language models and highlights the need for more comprehensive safeguards.",None,-1
2d2e915b-6705-4ae1-8d92-1d55bbede320,Benchmarking Large Language Model Capabilities for Conditional Generation,0.562593,20,"Pre-trained large language models (PLMs) underlie most new developments in
natural language processing. They have shifted the field from
application-specific model pipelines to a single model that is adapted to a
wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside
techniques like few-shot learning, have additionally shifted the output
modality to generation instead of classification or regression. Despite their
ubiquitous use, the generation quality of language models is rarely evaluated
when these models are introduced. Additionally, it is unclear how existing
generation tasks--while they can be used to compare systems at a high
level--relate to the real world use cases for which people have been adopting
them. In this work, we discuss how to adapt existing application-specific
generation benchmarks to PLMs and provide an in-depth, empirical study of the
limitations and capabilities of PLMs in natural language generation tasks along
dimensions such as scale, architecture, input and output language. Our results
show that PLMs differ in their applicability to different data regimes and
their generalization to multiple languages and inform which PLMs to use for a
given generation task setup. We share best practices to be taken into
consideration when benchmarking generation capabilities during the development
of upcoming PLMs.",None,-1
5f8aca1b-c073-4415-aae7-560527aca2ae,NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support,0.590679,10,"We present a method for generating high-quality watertight manifold meshes
from multi-view input images. Existing volumetric rendering methods are robust
in optimization but tend to generate noisy meshes with poor topology.
Differentiable rasterization-based methods can generate high-quality meshes but
are sensitive to initialization. Our method combines the benefits of both
worlds; we take the geometry initialization obtained from neural volumetric
fields, and further optimize the geometry as well as a compact neural texture
representation with differentiable rasterizers. Through extensive experiments,
we demonstrate that our method can generate accurate mesh reconstructions with
faithful appearance that are comparable to previous volume rendering methods
while being an order of magnitude faster in rendering. We also show that our
generated mesh and neural texture reconstruction is compatible with existing
graphics pipelines and enables downstream 3D applications such as simulation.
Project page: https://sarahweiii.github.io/neumanifold/",None,-1
328f3f24-eca9-4f3e-bdbb-58a3d536aa9f,Hybrid-Supervised Dual-Search: Leveraging Automatic Learning for Loss-free Multi-Exposure Image Fusion,0.170893,1,"Multi-exposure image fusion (MEF) has emerged as a prominent solution to
address the limitations of digital imaging in representing varied exposure
levels. Despite its advancements, the field grapples with challenges, notably
the reliance on manual designs for network structures and loss functions, and
the constraints of utilizing simulated reference images as ground truths.
Consequently, current methodologies often suffer from color distortions and
exposure artifacts, further complicating the quest for authentic image
representation. In addressing these challenges, this paper presents a
Hybrid-Supervised Dual-Search approach for MEF, dubbed HSDS-MEF, which
introduces a bi-level optimization search scheme for automatic design of both
network structures and loss functions. More specifically, we harnesses a unique
dual research mechanism rooted in a novel weighted structure refinement
architecture search. Besides, a hybrid supervised contrast constraint
seamlessly guides and integrates with searching process, facilitating a more
adaptive and comprehensive search for optimal loss functions. We realize the
state-of-the-art performance in comparison to various competitive schemes,
yielding a 10.61% and 4.38% improvement in Visual Information Fidelity (VIF)
for general and no-reference scenarios, respectively, while providing results
with high contrast, rich details and colors.",None,-1
80278fe2-1da0-43c8-91de-c30366e8d126,"One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning",0.451123,14,"Fine-tuning pre-trained language models for multiple tasks tends to be
expensive in terms of storage. To mitigate this, parameter-efficient transfer
learning (PETL) methods have been proposed to address this issue, but they
still require a significant number of parameters and storage when being applied
to broader ranges of tasks. To achieve even greater storage reduction, we
propose PROPETL, a novel method that enables efficient sharing of a single PETL
module which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)
across layers and tasks. We then learn binary masks to select different
sub-networks from the shared prototype network and apply them as PETL modules
into different layers. We find that the binary masks can determine crucial
information from the network, which is often ignored in previous studies. Our
work can also be seen as a type of pruning method, where we find that
overparameterization also exists in the seemingly small PETL modules. We
evaluate PROPETL on various downstream tasks and show that it can outperform
other PETL methods with approximately 10% of the parameter storage required by
the latter.",None,-1
481957ef-fba7-473e-81c0-b0e51451978b,De-coupling and De-positioning Dense Self-supervised Learning,0.0858518,2,"Dense Self-Supervised Learning (SSL) methods address the limitations of using
image-level feature representations when handling images with multiple objects.
Although the dense features extracted by employing segmentation maps and
bounding boxes allow networks to perform SSL for each object, we show that they
suffer from coupling and positional bias, which arise from the receptive field
increasing with layer depth and zero-padding. We address this by introducing
three data augmentation strategies, and leveraging them in (i) a decoupling
module that aims to robustify the network to variations in the object's
surroundings, and (ii) a de-positioning module that encourages the network to
discard positional object information. We demonstrate the benefits of our
method on COCO and on a new challenging benchmark, OpenImage-MINI, for object
classification, semantic segmentation, and object detection. Our extensive
experiments evidence the better generalization of our method compared to the
SOTA dense SSL methods",None,-1
875e91c1-b608-476a-a920-811f86c97fd5,HSCNet++: Hierarchical Scene Coordinate Classification and Regression for Visual Localization with Transformer,0.492002,4,"Visual localization is critical to many applications in computer vision and
robotics. To address single-image RGB localization, state-of-the-art
feature-based methods match local descriptors between a query image and a
pre-built 3D model. Recently, deep neural networks have been exploited to
regress the mapping between raw pixels and 3D coordinates in the scene, and
thus the matching is implicitly performed by the forward pass through the
network. However, in a large and ambiguous environment, learning such a
regression task directly can be difficult for a single network. In this work,
we present a new hierarchical scene coordinate network to predict pixel scene
coordinates in a coarse-to-fine manner from a single RGB image. The proposed
method, which is an extension of HSCNet, allows us to train compact models
which scale robustly to large environments. It sets a new state-of-the-art for
single-image localization on the 7-Scenes, 12 Scenes, Cambridge Landmarks
datasets, and the combined indoor scenes.",None,-1
2a0c2f20-4e05-40cb-9b59-16c2f9479cc9,Simple Token-Level Confidence Improves Caption Correctness,0.143343,5,"The ability to judge whether a caption correctly describes an image is a
critical part of vision-language understanding. However, state-of-the-art
models often misinterpret the correctness of fine-grained details, leading to
errors in outputs such as hallucinating objects in generated captions or poor
compositional reasoning. In this work, we explore Token-Level Confidence, or
TLC, as a simple yet surprisingly effective method to assess caption
correctness. Specifically, we fine-tune a vision-language model on image
captioning, input an image and proposed caption to the model, and aggregate
either algebraic or learned token confidences over words or sequences to
estimate image-caption consistency. Compared to sequence-level scores from
pretrained models, TLC with algebraic confidence measures achieves a relative
improvement in accuracy by 10% on verb understanding in SVO-Probes and
outperforms prior state-of-the-art in image and group scores for compositional
reasoning in Winoground by a relative 37% and 9%, respectively. When training
data are available, a learned confidence estimator provides further improved
performance, reducing object hallucination rates in MS COCO Captions by a
relative 30% over the original model and setting a new state-of-the-art.",None,-1
7bd908f0-8db6-4b81-b884-ed1ece8ee6b9,Towards Generative Modeling of Urban Flow through Knowledge-enhanced Denoising Diffusion,0.846209,7,"Although generative AI has been successful in many areas, its ability to
model geospatial data is still underexplored. Urban flow, a typical kind of
geospatial data, is critical for a wide range of urban applications. Existing
studies mostly focus on predictive modeling of urban flow that predicts the
future flow based on historical flow data, which may be unavailable in
data-sparse areas or newly planned regions. Some other studies aim to predict
OD flow among regions but they fail to model dynamic changes of urban flow over
time. In this work, we study a new problem of urban flow generation that
generates dynamic urban flow for regions without historical flow data. To
capture the effect of multiple factors on urban flow, such as region features
and urban environment, we employ diffusion model to generate urban flow for
regions under different conditions. We first construct an urban knowledge graph
(UKG) to model the urban environment and relationships between regions, based
on which we design a knowledge-enhanced spatio-temporal diffusion model
(KSTDiff) to generate urban flow for each region. Specifically, to accurately
generate urban flow for regions with different flow volumes, we design a novel
diffusion process guided by a volume estimator, which is learnable and
customized for each region. Moreover, we propose a knowledge-enhanced denoising
network to capture the spatio-temporal dependencies of urban flow as well as
the impact of urban environment in the denoising process. Extensive experiments
on four real-world datasets validate the superiority of our model over
state-of-the-art baselines in urban flow generation. Further in-depth studies
demonstrate the utility of generated urban flow data and the ability of our
model for long-term flow generation and urban flow prediction. Our code is
released at: https://github.com/tsinghua-fib-lab/KSTDiff-Urban-flow-generation.",None,-1
d40efe27-73fd-4a11-b823-a83e42bb3c12,DaliID: Distortion-Adaptive Learned Invariance for Identification Models,0.221516,2,"In unconstrained scenarios, face recognition and person re-identification are
subject to distortions such as motion blur, atmospheric turbulence, or
upsampling artifacts. To improve robustness in these scenarios, we propose a
methodology called Distortion-Adaptive Learned Invariance for Identification
(DaliID) models. We contend that distortion augmentations, which degrade image
quality, can be successfully leveraged to a greater degree than has been shown
in the literature. Aided by an adaptive weighting schedule, a novel distortion
augmentation is applied at severe levels during training. This training
strategy increases feature-level invariance to distortions and decreases domain
shift to unconstrained scenarios. At inference, we use a magnitude-weighted
fusion of features from parallel models to retain robustness across the range
of images. DaliID models achieve state-of-the-art (SOTA) for both face
recognition and person re-identification on seven benchmark datasets, including
IJB-S, TinyFace, DeepChange, and MSMT17. Additionally, we provide recaptured
evaluation data at a distance of 750+ meters and further validate on real
long-distance face imagery.",None,-1
edf89e5d-4d05-4505-9944-95675490c348,CASP-Net: Rethinking Video Saliency Prediction from an Audio-VisualConsistency Perceptual Perspective,0.666411,8,"Incorporating the audio stream enables Video Saliency Prediction (VSP) to
imitate the selective attention mechanism of human brain. By focusing on the
benefits of joint auditory and visual information, most VSP methods are capable
of exploiting semantic correlation between vision and audio modalities but
ignoring the negative effects due to the temporal inconsistency of audio-visual
intrinsics. Inspired by the biological inconsistency-correction within
multi-sensory information, in this study, a consistency-aware audio-visual
saliency prediction network (CASP-Net) is proposed, which takes a comprehensive
consideration of the audio-visual semantic interaction and consistent
perception. In addition a two-stream encoder for elegant association between
video frames and corresponding sound source, a novel consistency-aware
predictive coding is also designed to improve the consistency within audio and
visual representations iteratively. To further aggregate the multi-scale
audio-visual information, a saliency decoder is introduced for the final
saliency map generation. Substantial experiments demonstrate that the proposed
CASP-Net outperforms the other state-of-the-art methods on six challenging
audio-visual eye-tracking datasets. For a demo of our system please see our
project webpage.",None,-1
b81e8df8-d227-4d4b-ab63-7e532b8d4d2c,An adaptive large neighborhood search heuristic for the multi-port continuous berth allocation problem,0.0772219,1,"In this paper, we study a problem that integrates the vessel scheduling
problem with the berth allocation into a collaborative problem denoted as the
multi-port continuous berth allocation problem (MCBAP). This problem optimizes
the berth allocation of a set of ships simultaneously in multiple ports while
also considering the sailing speed of ships between ports. Due to the highly
combinatorial character of the problem, exact methods struggle to scale to
large-size instances, which points to exploring heuristic methods. We present a
mixed-integer problem formulation for the MCBAP and introduce an adaptive large
neighborhood search (ALNS) algorithm enhanced with a local search procedure to
solve it. The computational results highlight the method's suitability for
larger instances by providing high-quality solutions in short computational
times. Practical insights indicate that the carriers' and terminal operators'
operational costs are impacted in different ways by fuel prices, external ships
at port, and the modeling of a continuous quay.",None,-1
381acd67-de1a-41c4-be2a-758b2130b511,Uncertainty-guided Boundary Learning for Imbalanced Social Event Detection,0.379843,2,"Real-world social events typically exhibit a severe class-imbalance
distribution, which makes the trained detection model encounter a serious
generalization challenge. Most studies solve this problem from the frequency
perspective and emphasize the representation or classifier learning for tail
classes. While in our observation, compared to the rarity of classes, the
calibrated uncertainty estimated from well-trained evidential deep learning
networks better reflects model performance. To this end, we propose a novel
uncertainty-guided class imbalance learning framework - UCL$_{SED}$, and its
variant - UCL-EC$_{SED}$, for imbalanced social event detection tasks. We aim
to improve the overall model performance by enhancing model generalization to
those uncertain classes. Considering performance degradation usually comes from
misclassifying samples as their confusing neighboring classes, we focus on
boundary learning in latent space and classifier learning with high-quality
uncertainty estimation. First, we design a novel uncertainty-guided contrastive
learning loss, namely UCL and its variant - UCL-EC, to manipulate
distinguishable representation distribution for imbalanced data. During
training, they force all classes, especially uncertain ones, to adaptively
adjust a clear separable boundary in the feature space. Second, to obtain more
robust and accurate class uncertainty, we combine the results of multi-view
evidential classifiers via the Dempster-Shafer theory under the supervision of
an additional calibration method. We conduct experiments on three severely
imbalanced social event datasets including Events2012\_100, Events2018\_100,
and CrisisLexT\_7. Our model significantly improves social event representation
and classification tasks in almost all classes, especially those uncertain
ones.",None,-1
7531b7e9-8802-4fa9-9bc7-153e2ed1f63b,Advancing Beyond Identification: Multi-bit Watermark for Large Language Models,0.476187,4,"We show the viability of tackling misuses of large language models beyond the
identification of machine-generated text. While existing zero-bit watermark
methods focus on detection only, some malicious misuses demand tracing the
adversary user for counteracting them. To address this, we propose Multi-bit
Watermark via Position Allocation, embedding traceable multi-bit information
during language model generation. Through allocating tokens onto different
parts of the messages, we embed longer messages in high corruption settings
without added latency. By independently embedding sub-units of messages, the
proposed method outperforms the existing works in terms of robustness and
latency. Leveraging the benefits of zero-bit watermarking, our method enables
robust extraction of the watermark without any model access, embedding and
extraction of long messages ($\geq$ 32-bit) without finetuning, and maintaining
text quality, while allowing zero-bit detection all at the same time. Code is
released here: https://github.com/bangawayoo/mb-lm-watermarking",None,-1
9c5a545a-c5ca-4a49-9009-9827f5b1d431,GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4,0.999999,25,"This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to
detect translation quality errors, specifically for the quality estimation
setting without the need for human reference translations. Based on the power
of large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting
technique, querying the GPT-4 model to mark error quality spans. Compared to
previous works, our method has language-agnostic prompts, thus avoiding the
need for manual prompt preparation for new languages.
  While preliminary results indicate that GEMBA-MQM achieves state-of-the-art
accuracy for system ranking, we advise caution when using it in academic works
to demonstrate improvements over other methods due to its dependence on the
proprietary, black-box GPT model.",None,-1
013e25ac-6062-4e70-a439-d8ccdef4e229,Segment Anything,1.0,2892,"We introduce the Segment Anything (SA) project: a new task, model, and
dataset for image segmentation. Using our efficient model in a data collection
loop, we built the largest segmentation dataset to date (by far), with over 1
billion masks on 11M licensed and privacy respecting images. The model is
designed and trained to be promptable, so it can transfer zero-shot to new
image distributions and tasks. We evaluate its capabilities on numerous tasks
and find that its zero-shot performance is impressive -- often competitive with
or even superior to prior fully supervised results. We are releasing the
Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and
11M images at https://segment-anything.com to foster research into foundation
models for computer vision.",None,-1
d169df25-e301-47f8-a582-c86533b92689,Challenges in Context-Aware Neural Machine Translation,0.390509,3,"Context-aware neural machine translation involves leveraging information
beyond sentence-level context to resolve inter-sentential discourse
dependencies and improve document-level translation quality, and has given rise
to a number of recent techniques. However, despite well-reasoned intuitions,
most context-aware translation models show only modest improvements over
sentence-level systems. In this work, we investigate several challenges that
impede progress within this field, relating to discourse phenomena, context
usage, model architectures, and document-level evaluation. To address these
problems, we propose a more realistic setting for document-level translation,
called paragraph-to-paragraph (para2para) translation, and collect a new
dataset of Chinese-English novels to promote future research.",None,-1
56b3948c-77ff-49fd-996a-40544cf92289,Multiview Identifiers Enhanced Generative Retrieval,0.182888,9,"Instead of simply matching a query to pre-existing passages, generative
retrieval generates identifier strings of passages as the retrieval target. At
a cost, the identifier must be distinctive enough to represent a passage.
Current approaches use either a numeric ID or a text piece (such as a title or
substrings) as the identifier. However, these identifiers cannot cover a
passage's content well. As such, we are motivated to propose a new type of
identifier, synthetic identifiers, that are generated based on the content of a
passage and could integrate contextualized information that text pieces lack.
Furthermore, we simultaneously consider multiview identifiers, including
synthetic identifiers, titles, and substrings. These views of identifiers
complement each other and facilitate the holistic ranking of passages from
multiple perspectives. We conduct a series of experiments on three public
datasets, and the results indicate that our proposed approach performs the best
in generative retrieval, demonstrating its effectiveness and robustness.",None,-1
ad768f02-eff4-4b4a-8512-21e3a7369468,Top-Down Visual Attention from Analysis by Synthesis,0.970215,16,"Current attention algorithms (e.g., self-attention) are stimulus-driven and
highlight all the salient objects in an image. However, intelligent agents like
humans often guide their attention based on the high-level task at hand,
focusing only on task-related objects. This ability of task-guided top-down
attention provides task-adaptive representation and helps the model generalize
to various tasks. In this paper, we consider top-down attention from a classic
Analysis-by-Synthesis (AbS) perspective of vision. Prior work indicates a
functional equivalence between visual attention and sparse reconstruction; we
show that an AbS visual system that optimizes a similar sparse reconstruction
objective modulated by a goal-directed top-down signal naturally simulates
top-down attention. We further propose Analysis-by-Synthesis Vision Transformer
(AbSViT), which is a top-down modulated ViT model that variationally
approximates AbS, and achieves controllable top-down attention. For real-world
applications, AbSViT consistently improves over baselines on Vision-Language
tasks such as VQA and zero-shot retrieval where language guides the top-down
attention. AbSViT can also serve as a general backbone, improving performance
on classification, semantic segmentation, and model robustness.",None,-1
12fac069-f2aa-489d-8780-067fde504ab1,Coronary Artery Semantic Labeling using Edge Attention Graph Matching Network,0.147856,3,"Coronary artery disease (CAD) is one of the primary causes leading deaths
worldwide. The presence of atherosclerotic lesions in coronary arteries is the
underlying pathophysiological basis of CAD, and accurate extraction of
individual arterial branches using invasive coronary angiography (ICA) is
crucial for stenosis detection and CAD diagnosis. We propose an innovative
approach called the Edge Attention Graph Matching Network (EAGMN) for coronary
artery semantic labeling. By converting the coronary artery semantic
segmentation task into a graph node similarity comparison task, identifying the
node-to-node correspondence would assign semantic labels for each arterial
branch. More specifically, The EAGMN utilizes the association graph constructed
from the two individual graphs as input. Experimental results indicate the
EAGMN achieved a weighted accuracy of 0.8653, a weighted precision of 0.8656, a
weighted recall of 0.8653 and a weighted F1-score of 0.8643. Furthermore, we
employ ZORRO to provide interpretability and explainability of the graph
matching for artery semantic labeling. These findings highlight the potential
of the EAGMN for accurate and efficient coronary artery semantic labeling using
ICAs. By leveraging the inherent characteristics of ICAs and incorporating
graph matching techniques, our proposed model provides a promising solution for
improving CAD diagnosis and treatment",None,-1
053238ca-2472-4770-a9e8-094419d4b261,Scalable Prompt Generation for Semi-supervised Learning with Language Models,0.652661,9,"Prompt-based learning methods in semi-supervised learning (SSL) settings have
been shown to be effective on multiple natural language understanding (NLU)
datasets and tasks in the literature. However, manually designing multiple
prompts and verbalizers requires domain knowledge and human effort, making it
difficult and expensive to scale across different datasets. In this paper, we
propose two methods to automatically design multiple prompts and integrate
automatic verbalizer in SSL settings without sacrificing performance. The first
method uses various demonstration examples with learnable continuous prompt
tokens to create diverse prompt models. The second method uses a varying number
of soft prompt tokens to encourage language models to learn different prompts.
For the verbalizer, we use the prototypical verbalizer to replace the manual
one. In summary, we obtained the best average accuracy of 73.2% (a relative
improvement of 2.52% over even the previous state-of-the-art SSL method with
manual prompts and verbalizers) in different few-shot learning settings.",None,-1
187eaf48-8361-4dc1-ad5f-65215ab24814,DAVA: Disentangling Adversarial Variational Autoencoder,0.428561,5,"The use of well-disentangled representations offers many advantages for
downstream tasks, e.g. an increased sample efficiency, or better
interpretability. However, the quality of disentangled interpretations is often
highly dependent on the choice of dataset-specific hyperparameters, in
particular the regularization strength. To address this issue, we introduce
DAVA, a novel training procedure for variational auto-encoders. DAVA completely
alleviates the problem of hyperparameter selection. We compare DAVA to models
with optimal hyperparameters. Without any hyperparameter tuning, DAVA is
competitive on a diverse range of commonly used datasets. Underlying DAVA, we
discover a necessary condition for unsupervised disentanglement, which we call
PIPE. We demonstrate the ability of PIPE to positively predict the performance
of downstream models in abstract reasoning. We also thoroughly investigate
correlations with existing supervised and unsupervised metrics. The code is
available at https://github.com/besterma/dava.",None,-1
c6adb5a6-be18-44b8-9ab1-2aa108bf2796,CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification,0.318747,8,"Chain-of-thought (CoT) prompting enables large language models (LLMs) to
solve complex reasoning tasks by generating an explanation before the final
prediction. Despite it's promising ability, a critical downside of CoT
prompting is that the performance is greatly affected by the factuality of the
generated explanation. To improve the correctness of the explanations,
fine-tuning language models with explanation data is needed. However, there
exists only a few datasets that can be used for such approaches, and no data
collection tool for building them. Thus, we introduce CoTEVer, a tool-kit for
annotating the factual correctness of generated explanations and collecting
revision data of wrong explanations. Furthermore, we suggest several use cases
where the data collected with CoTEVer can be utilized for enhancing the
faithfulness of explanations. Our toolkit is publicly available at
https://github.com/SeungoneKim/CoTEVer.",None,-1
0fa08589-f852-46e7-a896-ca1d81e3c9a5,Story Shaping: Teaching Agents Human-like Behavior with Stories,0.110959,4,"Reward design for reinforcement learning agents can be difficult in
situations where one not only wants the agent to achieve some effect in the
world but where one also cares about how that effect is achieved. For example,
we might wish for an agent to adhere to a tacit understanding of commonsense,
align itself to a preference for how to behave for purposes of safety, or
taking on a particular role in an interactive game. Storytelling is a mode for
communicating tacit procedural knowledge. We introduce a technique, Story
Shaping, in which a reinforcement learning agent infers tacit knowledge from an
exemplar story of how to accomplish a task and intrinsically rewards itself for
performing actions that make its current environment adhere to that of the
inferred story world. Specifically, Story Shaping infers a knowledge graph
representation of the world state from observations, and also infers a
knowledge graph from the exemplar story. An intrinsic reward is generated based
on the similarity between the agent's inferred world state graph and the
inferred story world graph. We conducted experiments in text-based games
requiring commonsense reasoning and shaping the behaviors of agents as virtual
game characters.",None,-1
9d3b6de3-9f49-48b1-8abf-b0671446a27e,Enhancing Embedding Representations of Biomedical Data using Logic Knowledge,0.0823712,1,"Knowledge Graph Embeddings (KGE) have become a quite popular class of models
specifically devised to deal with ontologies and graph structure data, as they
can implicitly encode statistical dependencies between entities and relations
in a latent space. KGE techniques are particularly effective for the biomedical
domain, where it is quite common to deal with large knowledge graphs underlying
complex interactions between biological and chemical objects. Recently in the
literature, the PharmKG dataset has been proposed as one of the most
challenging knowledge graph biomedical benchmark, with hundreds of thousands of
relational facts between genes, diseases and chemicals. Despite KGEs can scale
to very large relational domains, they generally fail at representing more
complex relational dependencies between facts, like logic rules, which may be
fundamental in complex experimental settings. In this paper, we exploit logic
rules to enhance the embedding representations of KGEs on the PharmKG dataset.
To this end, we adopt Relational Reasoning Network (R2N), a recently proposed
neural-symbolic approach showing promising results on knowledge graph
completion tasks. An R2N uses the available logic rules to build a neural
architecture that reasons over KGE latent representations. In the experiments,
we show that our approach is able to significantly improve the current
state-of-the-art on the PharmKG dataset. Finally, we provide an ablation study
to experimentally compare the effect of alternative sets of rules according to
different selection criteria and varying the number of considered rules.",None,-1
29ac76b7-5b6f-4e0c-abaf-05c071c95f12,Joint fMRI Decoding and Encoding with Latent Embedding Alignment,0.670227,4,"The connection between brain activity and corresponding visual stimuli is
crucial in comprehending the human brain. While deep generative models have
exhibited advancement in recovering brain recordings by generating images
conditioned on fMRI signals, accomplishing high-quality generation with
consistent semantics continues to pose challenges. Moreover, the prediction of
brain activity from visual stimuli remains a formidable undertaking. In this
paper, we introduce a unified framework that addresses both fMRI decoding and
encoding. Commencing with the establishment of two latent spaces capable of
representing and reconstructing fMRI signals and visual images, respectively,
we proceed to align the fMRI signals and visual images within the latent space,
thereby enabling a bidirectional transformation between the two domains. Our
Latent Embedding Alignment (LEA) model concurrently recovers visual stimuli
from fMRI signals and predicts brain activity from images within a unified
framework. The performance of LEA surpasses that of existing methods on
multiple benchmark fMRI decoding and encoding datasets. By integrating fMRI
decoding and encoding, LEA offers a comprehensive solution for modeling the
intricate relationship between brain activity and visual stimuli.",None,-1
83c1b25c-f397-42f5-a3b2-bad4f56811cf,Rigorously Assessing Natural Language Explanations of Neurons,0.840006,12,"Natural language is an appealing medium for explaining how large language
models process and store information, but evaluating the faithfulness of such
explanations is challenging. To help address this, we develop two modes of
evaluation for natural language explanations that claim individual neurons
represent a concept in a text input. In the observational mode, we evaluate
claims that a neuron $a$ activates on all and only input strings that refer to
a concept picked out by the proposed explanation $E$. In the intervention mode,
we construe $E$ as a claim that the neuron $a$ is a causal mediator of the
concept denoted by $E$. We apply our framework to the GPT-4-generated
explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the
most confident explanations have high error rates and little to no causal
efficacy. We close the paper by critically assessing whether natural language
is a good choice for explanations and whether neurons are the best level of
analysis.",None,-1
6e556d17-5604-4d72-910d-9918d538a10d,Improving Fairness in Adaptive Social Exergames via Shapley Bandits,0.356117,2,"Algorithmic fairness is an essential requirement as AI becomes integrated in
society. In the case of social applications where AI distributes resources,
algorithms often must make decisions that will benefit a subset of users,
sometimes repeatedly or exclusively, while attempting to maximize specific
outcomes. How should we design such systems to serve users more fairly? This
paper explores this question in the case where a group of users works toward a
shared goal in a social exergame called Step Heroes. We identify adverse
outcomes in traditional multi-armed bandits (MABs) and formalize the Greedy
Bandit Problem. We then propose a solution based on a new type of
fairness-aware multi-armed bandit, Shapley Bandits. It uses the Shapley Value
for increasing overall player participation and intervention adherence rather
than the maximization of total group output, which is traditionally achieved by
favoring only high-performing participants. We evaluate our approach via a user
study (n=46). Our results indicate that our Shapley Bandits effectively
mediates the Greedy Bandit Problem and achieves better user retention and
motivation across the participants.",None,-1
050fae2f-c30e-4f3b-98a2-b19827248fab,Understanding and Unifying Fourteen Attribution Methods with Taylor Interactions,0.603966,13,"Various attribution methods have been developed to explain deep neural
networks (DNNs) by inferring the attribution/importance/contribution score of
each input variable to the final output. However, existing attribution methods
are often built upon different heuristics. There remains a lack of a unified
theoretical understanding of why these methods are effective and how they are
related. To this end, for the first time, we formulate core mechanisms of
fourteen attribution methods, which were designed on different heuristics, into
the same mathematical system, i.e., the system of Taylor interactions.
Specifically, we prove that attribution scores estimated by fourteen
attribution methods can all be reformulated as the weighted sum of two types of
effects, i.e., independent effects of each individual input variable and
interaction effects between input variables. The essential difference among the
fourteen attribution methods mainly lies in the weights of allocating different
effects. Based on the above findings, we propose three principles for a fair
allocation of effects to evaluate the faithfulness of the fourteen attribution
methods.",None,-1
d0ca14fd-ee0f-4cfa-928b-5592b5ec4835,Improving Continual Relation Extraction by Distinguishing Analogous Semantics,0.628304,7,"Continual relation extraction (RE) aims to learn constantly emerging
relations while avoiding forgetting the learned relations. Existing works store
a small number of typical samples to re-train the model for alleviating
forgetting. However, repeatedly replaying these samples may cause the
overfitting problem. We conduct an empirical study on existing works and
observe that their performance is severely affected by analogous relations. To
address this issue, we propose a novel continual extraction model for analogous
relations. Specifically, we design memory-insensitive relation prototypes and
memory augmentation to overcome the overfitting problem. We also introduce
integrated training and focal knowledge distillation to enhance the performance
on analogous relations. Experimental results show the superiority of our model
and demonstrate its effectiveness in distinguishing analogous relations and
overcoming overfitting.",None,-1
ea8d2ed7-d9d8-4dfd-a554-c9ccf54436af,DRM-IR: Task-Adaptive Deep Unfolding Network for All-In-One Image Restoration,0.148624,1,"Existing All-In-One image restoration (IR) methods usually lack flexible
modeling on various types of degradation, thus impeding the restoration
performance. To achieve All-In-One IR with higher task dexterity, this work
proposes an efficient Dynamic Reference Modeling paradigm (DRM-IR), which
consists of task-adaptive degradation modeling and model-based image restoring.
Specifically, these two subtasks are formalized as a pair of entangled
reference-based maximum a posteriori (MAP) inferences, which are optimized
synchronously in an unfolding-based manner. With the two cascaded subtasks,
DRM-IR first dynamically models the task-specific degradation based on a
reference image pair and further restores the image with the collected
degradation statistics. Besides, to bridge the semantic gap between the
reference and target degraded images, we further devise a Degradation Prior
Transmitter (DPT) that restrains the instance-specific feature differences.
DRM-IR explicitly provides superior flexibility for All-in-One IR while being
interpretable. Extensive experiments on multiple benchmark datasets show that
our DRM-IR achieves state-of-the-art in All-In-One IR.",None,-1
17882a95-3b11-4b10-be08-4d610c8df7e5,Where are We in Event-centric Emotion Analysis? Bridging Emotion Role Labeling and Appraisal-based Approaches,0.751249,1,"The term emotion analysis in text subsumes various natural language
processing tasks which have in common the goal to enable computers to
understand emotions. Most popular is emotion classification in which one or
multiple emotions are assigned to a predefined textual unit. While such setting
is appropriate for identifying the reader's or author's emotion, emotion role
labeling adds the perspective of mentioned entities and extracts text spans
that correspond to the emotion cause. The underlying emotion theories agree on
one important point; that an emotion is caused by some internal or external
event and comprises several subcomponents, including the subjective feeling and
a cognitive evaluation. We therefore argue that emotions and events are related
in two ways. (1) Emotions are events; and this perspective is the fundament in
natural language processing for emotion role labeling. (2) Emotions are caused
by events; a perspective that is made explicit with research how to incorporate
psychological appraisal theories in NLP models to interpret events. These two
research directions, role labeling and (event-focused) emotion classification,
have by and large been tackled separately. In this paper, we contextualize both
perspectives and discuss open research questions.",None,-1
0a3ddc7e-af74-4ff6-9776-043d0730c99b,Synthesizing a Progression of Subtasks for Block-Based Visual Programming Tasks,0.40238,2,"Block-based visual programming environments play an increasingly important
role in introducing computing concepts to K-12 students. In recent years, they
have also gained popularity in neuro-symbolic AI, serving as a benchmark to
evaluate general problem-solving and logical reasoning skills. The open-ended
and conceptual nature of these visual programming tasks make them challenging,
both for state-of-the-art AI agents as well as for novice programmers. A
natural approach to providing assistance for problem-solving is breaking down a
complex task into a progression of simpler subtasks; however, this is not
trivial given that the solution codes are typically nested and have non-linear
execution behavior. In this paper, we formalize the problem of synthesizing
such a progression for a given reference block-based visual programming task.
We propose a novel synthesis algorithm that generates a progression of subtasks
that are high-quality, well-spaced in terms of their complexity, and solving
this progression leads to solving the reference task. We show the utility of
our synthesis algorithm in improving the efficacy of AI agents (in this case,
neural program synthesizers) for solving tasks in the Karel programming
environment. Then, we conduct a user study to demonstrate that our synthesized
progression of subtasks can assist a novice programmer in solving tasks in the
Hour of Code: Maze Challenge by Code-dot-org.",None,-1
7b6b396d-4059-489a-bb84-bafe2fae036a,Rethinking Boundary Detection in Deep Learning Models for Medical Image Segmentation,0.6673,12,"Medical image segmentation is a fundamental task in the community of medical
image analysis. In this paper, a novel network architecture, referred to as
Convolution, Transformer, and Operator (CTO), is proposed. CTO employs a
combination of Convolutional Neural Networks (CNNs), Vision Transformer (ViT),
and an explicit boundary detection operator to achieve high recognition
accuracy while maintaining an optimal balance between accuracy and efficiency.
The proposed CTO follows the standard encoder-decoder segmentation paradigm,
where the encoder network incorporates a popular CNN backbone for capturing
local semantic information, and a lightweight ViT assistant for integrating
long-range dependencies. To enhance the learning capacity on boundary, a
boundary-guided decoder network is proposed that uses a boundary mask obtained
from a dedicated boundary detection operator as explicit supervision to guide
the decoding learning process. The performance of the proposed method is
evaluated on six challenging medical image segmentation datasets, demonstrating
that CTO achieves state-of-the-art accuracy with a competitive model
complexity.",None,-1
3ecf2d5d-2262-4fb5-bac9-68a4d720f641,Exploiting Language Models as a Source of Knowledge for Cognitive Agents,0.694346,3,"Large language models (LLMs) provide capabilities far beyond sentence
completion, including question answering, summarization, and natural-language
inference. While many of these capabilities have potential application to
cognitive systems, our research is exploiting language models as a source of
task knowledge for cognitive agents, that is, agents realized via a cognitive
architecture. We identify challenges and opportunities for using language
models as an external knowledge source for cognitive systems and possible ways
to improve the effectiveness of knowledge extraction by integrating extraction
with cognitive architecture capabilities, highlighting with examples from our
recent work in this area.",None,-1
cd496a84-2e9e-4bb5-96af-167d1dc07ec3,CipherSniffer: Classifying Cipher Types,0.0419295,1,"Ciphers are a powerful tool for encrypting communication. There are many
different cipher types, which makes it computationally expensive to solve a
cipher using brute force. In this paper, we frame the decryption task as a
classification problem. We first create a dataset of transpositions,
substitutions, text reversals, word reversals, sentence shifts, and unencrypted
text. Then, we evaluate the performance of various tokenizer-model combinations
on this task.",None,-1
22e31256-f09c-44c9-a5b3-404081c6a402,Diversity is Strength: Mastering Football Full Game with Interactive Reinforcement Learning of Multiple AIs,0.0339019,1,"Training AI with strong and rich strategies in multi-agent environments
remains an important research topic in Deep Reinforcement Learning (DRL). The
AI's strength is closely related to its diversity of strategies, and this
relationship can guide us to train AI with both strong and rich strategies. To
prove this point, we propose Diversity is Strength (DIS), a novel DRL training
framework that can simultaneously train multiple kinds of AIs. These AIs are
linked through an interconnected history model pool structure, which enhances
their capabilities and strategy diversities. We also design a model evaluation
and screening scheme to select the best models to enrich the model pool and
obtain the final AI. The proposed training method provides diverse,
generalizable, and strong AI strategies without using human data. We tested our
method in an AI competition based on Google Research Football (GRF) and won the
5v5 and 11v11 tracks. The method enables a GRF AI to have a high level on both
5v5 and 11v11 tracks for the first time, which are under complex multi-agent
environments. The behavior analysis shows that the trained AI has rich
strategies, and the ablation experiments proved that the designed modules
benefit the training process.",None,-1
0428226a-c7d1-4ac3-a2d4-17ec1d3e0756,Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models,0.351141,13,"Large-scale image generation models, with impressive quality made possible by
the vast amount of data available on the Internet, raise social concerns that
these models may generate harmful or copyrighted content. The biases and
harmfulness arise throughout the entire training process and are hard to
completely remove, which have become significant hurdles to the safe deployment
of these models. In this paper, we propose a method called SDD to prevent
problematic content generation in text-to-image diffusion models. We
self-distill the diffusion model to guide the noise estimate conditioned on the
target removal concept to match the unconditional one. Compared to the previous
methods, our method eliminates a much greater proportion of harmful content
from the generated images without degrading the overall image quality.
Furthermore, our method allows the removal of multiple concepts at once,
whereas previous works are limited to removing a single concept at a time.",None,-1
3d6b4c6b-c9a0-431b-81ef-e2b8668510b0,Aligning Speakers: Evaluating and Visualizing Text-based Diarization Using Efficient Multiple Sequence Alignment (Extended Version),0.261559,1,"This paper presents a novel evaluation approach to text-based speaker
diarization (SD), tackling the limitations of traditional metrics that do not
account for any contextual information in text. Two new metrics are proposed,
Text-based Diarization Error Rate and Diarization F1, which perform utterance-
and word-level evaluations by aligning tokens in reference and hypothesis
transcripts. Our metrics encompass more types of errors compared to existing
ones, allowing us to make a more comprehensive analysis in SD. To align tokens,
a multiple sequence alignment algorithm is introduced that supports multiple
sequences in the reference while handling high-dimensional alignment to the
hypothesis using dynamic programming. Our work is packaged into two tools,
align4d providing an API for our alignment algorithm and TranscribeView for
visualizing and evaluating SD errors, which can greatly aid in the creation of
high-quality data, fostering the advancement of dialogue systems.",None,-1
08b6bf16-98ed-41fd-9fbf-5bae5d4dad40,Robust Single Rotation Averaging Revisited,0.0737986,1,"In this work, we propose a novel method for robust single rotation averaging
that can efficiently handle an extremely large fraction of outliers. Our
approach is to minimize the total truncated least unsquared deviations (TLUD)
cost of geodesic distances. The proposed algorithm consists of three steps:
First, we consider each input rotation as a potential initial solution and
choose the one that yields the least sum of truncated chordal deviations. Next,
we obtain the inlier set using the initial solution and compute its chordal
$L_2$-mean. Finally, starting from this estimate, we iteratively compute the
geodesic $L_1$-mean of the inliers using the Weiszfeld algorithm on $SO(3)$. An
extensive evaluation shows that our method is robust against up to 99% outliers
given a sufficient number of accurate inliers, outperforming the current state
of the art.",None,-1
3680fda7-cd0a-4ddc-8681-ad0a68deaeb6,Language-enhanced RNR-Map: Querying Renderable Neural Radiance Field maps with natural language,0.262078,3,"We present Le-RNR-Map, a Language-enhanced Renderable Neural Radiance map for
Visual Navigation with natural language query prompts. The recently proposed
RNR-Map employs a grid structure comprising latent codes positioned at each
pixel. These latent codes, which are derived from image observation, enable: i)
image rendering given a camera pose, since they are converted to Neural
Radiance Field; ii) image navigation and localization with astonishing
accuracy. On top of this, we enhance RNR-Map with CLIP-based embedding latent
codes, allowing natural language search without additional label data. We
evaluate the effectiveness of this map in single and multi-object searches. We
also investigate its compatibility with a Large Language Model as an
""affordance query resolver"". Code and videos are available at
https://intelligolabs.github.io/Le-RNR-Map/",None,-1
5ee6e792-b5c1-4bc4-b66e-653dc66c4728,A Template Is All You Meme,0.654088,1,"Memes are a modern form of communication and meme templates possess a base
semantics that is customizable by whomever posts it on social media. Machine
learning systems struggle with memes, which is likely due to such systems
having insufficient context to understand memes, as there is more to memes than
the obvious image and text. Here, to aid understanding of memes, we release a
knowledge base of memes and information found on www.knowyourmeme.com, which we
call the Know Your Meme Knowledge Base (KYMKB), composed of more than 54,000
images. The KYMKB includes popular meme templates, examples of each template,
and detailed information about the template. We hypothesize that meme templates
can be used to inject models with the context missing from previous approaches.
To test our hypothesis, we create a non-parametric majority-based classifier,
which we call Template-Label Counter (TLC). We find TLC more effective than or
competitive with fine-tuned baselines. To demonstrate the power of meme
templates and the value of both our knowledge base and method, we conduct
thorough classification experiments and exploratory data analysis in the
context of five meme analysis tasks.",None,-1
d3aa6148-d5cb-4989-822f-029272139dac,Speaker attribution in German parliamentary debates with QLoRA-adapted large language models,0.147704,1,"The growing body of political texts opens up new opportunities for rich
insights into political dynamics and ideologies but also increases the workload
for manual analysis. Automated speaker attribution, which detects who said what
to whom in a speech event and is closely related to semantic role labeling, is
an important processing step for computational text analysis. We study the
potential of the large language model family Llama 2 to automate speaker
attribution in German parliamentary debates from 2017-2021. We fine-tune Llama
2 with QLoRA, an efficient training strategy, and observe our approach to
achieve competitive performance in the GermEval 2023 Shared Task On Speaker
Attribution in German News Articles and Parliamentary Debates. Our results shed
light on the capabilities of large language models in automating speaker
attribution, revealing a promising avenue for computational analysis of
political discourse and the development of semantic role labeling systems.",None,-1
9392bd2e-6c8a-4ff7-a513-d5e101275e7f,EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression,0.0823509,1,"We propose an unsupervised method to extract keywords and keyphrases from
texts based on a pre-trained language model (LM) and Shannon's information
maximization. Specifically, our method extracts phrases having the highest
conditional entropy under the LM. The resulting set of keyphrases turns out to
solve a relevant information-theoretic problem: if provided as side
information, it leads to the expected minimal binary code length in compressing
the text using the LM and an entropy encoder. Alternately, the resulting set is
an approximation via a causal LM to the set of phrases that minimize the
entropy of the text when conditioned upon it. Empirically, the method provides
results comparable to the most commonly used methods in various keyphrase
extraction benchmark challenges.",None,-1
0c284c18-c843-4e48-b6a4-1e8801cebfbf,Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model Interpretability in Medicine,0.862896,15,"One of the major barriers to using large language models (LLMs) in medicine
is the perception they use uninterpretable methods to make clinical decisions
that are inherently different from the cognitive processes of clinicians. In
this manuscript we develop novel diagnostic reasoning prompts to study whether
LLMs can perform clinical reasoning to accurately form a diagnosis. We find
that GPT4 can be prompted to mimic the common clinical reasoning processes of
clinicians without sacrificing diagnostic accuracy. This is significant because
an LLM that can use clinical reasoning to provide an interpretable rationale
offers physicians a means to evaluate whether LLMs can be trusted for patient
care. Novel prompting methods have the potential to expose the black box of
LLMs, bringing them one step closer to safe and effective use in medicine.",None,-1
528246c4-3b7f-4d29-9450-d80d21d1a61a,Multistage Spatial Context Models for Learned Image Compression,0.621367,6,"Recent state-of-the-art Learned Image Compression methods feature spatial
context models, achieving great rate-distortion improvements over hyperprior
methods. However, the autoregressive context model requires serial decoding,
limiting runtime performance. The Checkerboard context model allows parallel
decoding at a cost of reduced RD performance. We present a series of multistage
spatial context models allowing both fast decoding and better RD performance.
We split the latent space into square patches and decode serially within each
patch while different patches are decoded in parallel. The proposed method
features a comparable decoding speed to Checkerboard while reaching the RD
performance of Autoregressive and even also outperforming Autoregressive.
Inside each patch, the decoding order must be carefully decided as a bad order
negatively impacts performance; therefore, we also propose a decoding order
optimization algorithm.",None,-1
e42ffc92-dd20-4f9a-a78c-3b3036d49db6,Cardinality Estimation over Knowledge Graphs with Embeddings and Graph Neural Networks,0.248729,1,"Cardinality Estimation over Knowledge Graphs (KG) is crucial for query
optimization, yet remains a challenging task due to the semi-structured nature
and complex correlations of typical Knowledge Graphs. In this work, we propose
GNCE, a novel approach that leverages knowledge graph embeddings and Graph
Neural Networks (GNN) to accurately predict the cardinality of conjunctive
queries. GNCE first creates semantically meaningful embeddings for all entities
in the KG, which are then integrated into the given query, which is processed
by a GNN to estimate the cardinality of the query. We evaluate GNCE on several
KGs in terms of q-Error and demonstrate that it outperforms state-of-the-art
approaches based on sampling, summaries, and (machine) learning in terms of
estimation accuracy while also having lower execution time and less parameters.
Additionally, we show that GNCE can inductively generalise to unseen entities,
making it suitable for use in dynamic query processing scenarios. Our proposed
approach has the potential to significantly improve query optimization and
related applications that rely on accurate cardinality estimates of conjunctive
queries.",None,-1
2ecd0e5f-d6bf-4b20-b297-8d6f1671a5f6,Less is More for Long Document Summary Evaluation by LLMs,0.804249,13,"Large Language Models (LLMs) have shown promising performance in summary
evaluation tasks, yet they face challenges such as high computational costs and
the Lost-in-the-Middle problem where important information in the middle of
long documents is often overlooked. To address these issues, this paper
introduces a novel approach, Extract-then-Evaluate, which involves extracting
key sentences from a long source document and then evaluating the summary by
prompting LLMs. The results reveal that the proposed method not only
significantly reduces evaluation costs but also exhibits a higher correlation
with human evaluations. Furthermore, we provide practical recommendations for
optimal document length and sentence extraction methods, contributing to the
development of cost-effective yet more accurate methods for LLM-based text
generation evaluation.",None,-1
54424484-994a-47e2-88f3-3c53d59e367e,Hierarchical Graph Neural Networks for Causal Discovery and Root Cause Localization,0.432964,11,"In this paper, we propose REASON, a novel framework that enables the
automatic discovery of both intra-level (i.e., within-network) and inter-level
(i.e., across-network) causal relationships for root cause localization. REASON
consists of Topological Causal Discovery and Individual Causal Discovery. The
Topological Causal Discovery component aims to model the fault propagation in
order to trace back to the root causes. To achieve this, we propose novel
hierarchical graph neural networks to construct interdependent causal networks
by modeling both intra-level and inter-level non-linear causal relations. Based
on the learned interdependent causal networks, we then leverage random walks
with restarts to model the network propagation of a system fault. The
Individual Causal Discovery component focuses on capturing abrupt change
patterns of a single system entity. This component examines the temporal
patterns of each entity's metric data (i.e., time series), and estimates its
likelihood of being a root cause based on the Extreme Value theory. Combining
the topological and individual causal scores, the top K system entities are
identified as root causes. Extensive experiments on three real-world datasets
with case studies demonstrate the effectiveness and superiority of the proposed
framework.",None,-1
15eafe64-a7ef-4435-aa1e-6efb6788a5a9,"Once Detected, Never Lost: Surpassing Human Performance in Offline LiDAR based 3D Object Detection",0.748567,9,"This paper aims for high-performance offline LiDAR-based 3D object detection.
We first observe that experienced human annotators annotate objects from a
track-centric perspective. They first label the objects with clear shapes in a
track, and then leverage the temporal coherence to infer the annotations of
obscure objects. Drawing inspiration from this, we propose a high-performance
offline detector in a track-centric perspective instead of the conventional
object-centric perspective. Our method features a bidirectional tracking module
and a track-centric learning module. Such a design allows our detector to infer
and refine a complete track once the object is detected at a certain moment. We
refer to this characteristic as ""onCe detecTed, neveR Lost"" and name the
proposed system CTRL. Extensive experiments demonstrate the remarkable
performance of our method, surpassing the human-level annotating accuracy and
the previous state-of-the-art methods in the highly competitive Waymo Open
Dataset without model ensemble. The code will be made publicly available at
https://github.com/tusen-ai/SST.",None,-1
92da8ce6-9a5c-41b2-ab49-695383ea5ed1,Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain,0.826762,19,"Adapting pretrained language models to novel domains, such as clinical
applications, traditionally involves retraining their entire set of parameters.
Parameter-Efficient Fine-Tuning (PEFT) techniques for fine-tuning language
models significantly reduce computational requirements by selectively
fine-tuning small subsets of parameters. In this study, we propose a two-step
PEFT framework and evaluate it in the clinical domain. Our approach combines a
specialised PEFT adapter layer designed for clinical domain adaptation with
another adapter specialised for downstream tasks. We evaluate the framework on
multiple clinical outcome prediction datasets, comparing it to clinically
trained language models. Our framework achieves a better AUROC score averaged
across all clinical downstream tasks compared to clinical language models. In
particular, we observe large improvements of 4-5% AUROC in large-scale
multilabel classification tasks, such as diagnoses and procedures
classification. To our knowledge, this study is the first to provide an
extensive empirical analysis of the interplay between PEFT techniques and
domain adaptation in an important real-world domain of clinical applications.",None,-1
16e612c4-90ac-4dcc-905b-4a65cb29d921,MoniLog: An Automated Log-Based Anomaly Detection System for Cloud Computing Infrastructures,0.19313,9,"Within today's large-scale systems, one anomaly can impact millions of users.
Detecting such events in real-time is essential to maintain the quality of
services. It allows the monitoring team to prevent or diminish the impact of a
failure. Logs are a core part of software development and maintenance, by
recording detailed information at runtime. Such log data are universally
available in nearly all computer systems. They enable developers as well as
system maintainers to monitor and dissect anomalous events. For Cloud computing
companies and large online platforms in general, growth is linked to the
scaling potential. Automatizing the anomaly detection process is a promising
way to ensure the scalability of monitoring capacities regarding the increasing
volume of logs generated by modern systems. In this paper, we will introduce
MoniLog, a distributed approach to detect real-time anomalies within
large-scale environments. It aims to detect sequential and quantitative
anomalies within a multi-source log stream. MoniLog is designed to structure a
log stream and perform the monitoring of anomalous sequences. Its output
classifier learns from the administrator's actions to label and evaluate the
criticality level of anomalies.",None,-1
28f25fce-c97a-4f82-8864-0e8fc2df7abd,Enabling Human-Centered AI: A Methodological Perspective,0.399003,1,"Human-centered AI (HCAI) is a design philosophy that advocates prioritizing
humans in designing, developing, and deploying intelligent systems, aiming to
maximize the benefits of AI to humans and avoid potential adverse impacts.
While HCAI continues to influence, the lack of guidance on methodology in
practice makes its adoption challenging. This paper proposes a comprehensive
HCAI framework based on our previous work with integrated components, including
design goals, design principles, implementation approaches, interdisciplinary
teams, HCAI methods, and HCAI processes. This paper also presents a
""three-layer"" approach to facilitate the implementation of the framework. We
believe this systematic and executable framework can overcome the weaknesses in
current HCAI frameworks and the challenges currently faced in practice, putting
it into action to enable HCAI further.",None,-1
966e68b8-5ade-47ab-8724-9894fa617362,Pareto Optimal Learning for Estimating Large Language Model Errors,0.0582456,3,"Large Language Models (LLMs) have shown impressive abilities in many
applications. When a concrete and precise answer is desired, it is important to
have a quantitative estimation of the potential error rate. However, this can
be challenging due to the text-in-text-out nature of generative models. We
present a method based on Pareto optimization that generates a risk score to
estimate the probability of error in an LLM response by integrating multiple
sources of information. We prove theoretically that the error estimator
optimized in our framework aligns with the LLM and the information sources in
an Pareto optimal manner. Experimental results show that the risk scores
estimated by our method are well correlated with the true LLM error rate, thus
facilitating error correction. By dynamically combining with prompting
strategies such as self-verification and information retrieval, we demonstrate
the proposed method can be utilized to increase the performance of an LLM,
surpassing state-of-the-art task specific models.",None,-1
9417ba60-4e6b-4aa0-95dd-96f98e09181a,Rotation-Constrained Cross-View Feature Fusion for Multi-View Appearance-based Gaze Estimation,0.0726096,1,"Appearance-based gaze estimation has been actively studied in recent years.
However, its generalization performance for unseen head poses is still a
significant limitation for existing methods. This work proposes a generalizable
multi-view gaze estimation task and a cross-view feature fusion method to
address this issue. In addition to paired images, our method takes the relative
rotation matrix between two cameras as additional input. The proposed network
learns to extract rotatable feature representation by using relative rotation
as a constraint and adaptively fuses the rotatable features via stacked fusion
modules. This simple yet efficient approach significantly improves
generalization performance under unseen head poses without significantly
increasing computational cost. The model can be trained with random
combinations of cameras without fixing the positioning and can generalize to
unseen camera pairs during inference. Through experiments using multiple
datasets, we demonstrate the advantage of the proposed method over baseline
methods, including state-of-the-art domain generalization approaches. The code
will be available at https://github.com/ut-vision/Rot-MVGaze.",None,-1
2f8e45d3-ea21-4f78-af1f-abe371422082,Emotion-Cause Pair Extraction as Question Answering,0.716379,4,"The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all
potential emotion-cause pairs of a document without any annotation of emotion
or cause clauses. Previous approaches on ECPE have tried to improve
conventional two-step processing schemes by using complex architectures for
modeling emotion-cause interaction. In this paper, we cast the ECPE task to the
question answering (QA) problem and propose simple yet effective BERT-based
solutions to tackle it. Given a document, our Guided-QA model first predicts
the best emotion clause using a fixed question. Then the predicted emotion is
used as a question to predict the most potential cause for the emotion. We
evaluate our model on a standard ECPE corpus. The experimental results show
that despite its simplicity, our Guided-QA achieves promising results and is
easy to reproduce. The code of Guided-QA is also provided.",None,-1
15a5dac8-0374-42c0-aac5-55e37df5cf5b,Automatic Number Plate Recognition using Random Forest Classifier,0.59768,11,"Automatic Number Plate Recognition System (ANPRS) is a mass surveillance
embedded system that recognizes the number plate of the vehicle. This system is
generally used for traffic management applications. It should be very efficient
in detecting the number plate in noisy as well as in low illumination and also
within required time frame. This paper proposes a number plate recognition
method by processing vehicle's rear or front image. After image is captured,
processing is divided into four steps which are Pre-Processing, Number plate
localization, Character segmentation and Character recognition. Pre-Processing
enhances the image for further processing, number plate localization extracts
the number plate region from the image, character segmentation separates the
individual characters from the extracted number plate and character recognition
identifies the optical characters by using random forest classification
algorithm. Experimental results reveal that the accuracy of this method is
90.9%.",None,-1
39c02478-24c2-4b79-87c1-d0036af71e8a,Ultra-High Resolution Segmentation with Ultra-Rich Context: A Novel Benchmark,0.912546,10,"With the increasing interest and rapid development of methods for Ultra-High
Resolution (UHR) segmentation, a large-scale benchmark covering a wide range of
scenes with full fine-grained dense annotations is urgently needed to
facilitate the field. To this end, the URUR dataset is introduced, in the
meaning of Ultra-High Resolution dataset with Ultra-Rich Context. As the name
suggests, URUR contains amounts of images with high enough resolution (3,008
images of size 5,120x5,120), a wide range of complex scenes (from 63 cities),
rich-enough context (1 million instances with 8 categories) and fine-grained
annotations (about 80 billion manually annotated pixels), which is far superior
to all the existing UHR datasets including DeepGlobe, Inria Aerial, UDD, etc..
Moreover, we also propose WSDNet, a more efficient and effective framework for
UHR segmentation especially with ultra-rich context. Specifically, multi-level
Discrete Wavelet Transform (DWT) is naturally integrated to release computation
burden while preserve more spatial details, along with a Wavelet Smooth Loss
(WSL) to reconstruct original structured context and texture with a smooth
constrain. Experiments on several UHR datasets demonstrate its state-of-the-art
performance. The dataset is available at https://github.com/jankyee/URUR.",None,-1
3b97a5f4-aa26-4f91-a9d0-3a060dd32833,From Big to Small Without Losing It All: Text Augmentation with ChatGPT for Efficient Sentiment Analysis,0.220811,1,"In the era of artificial intelligence, data is gold but costly to annotate.
The paper demonstrates a groundbreaking solution to this dilemma using ChatGPT
for text augmentation in sentiment analysis. We leverage ChatGPT's generative
capabilities to create synthetic training data that significantly improves the
performance of smaller models, making them competitive with, or even
outperforming, their larger counterparts. This innovation enables models to be
both efficient and effective, thereby reducing computational cost, inference
time, and memory usage without compromising on quality. Our work marks a key
advancement in the cost-effective development and deployment of robust
sentiment analysis models.",None,-1
1bfb44ad-7df9-4767-ad48-df5bcfe11e9e,PixelRNN: In-pixel Recurrent Neural Networks for End-to-end-optimized Perception with Neural Sensors,0.671496,2,"Conventional image sensors digitize high-resolution images at fast frame
rates, producing a large amount of data that needs to be transmitted off the
sensor for further processing. This is challenging for perception systems
operating on edge devices, because communication is power inefficient and
induces latency. Fueled by innovations in stacked image sensor fabrication,
emerging sensor-processors offer programmability and minimal processing
capabilities directly on the sensor. We exploit these capabilities by
developing an efficient recurrent neural network architecture, PixelRNN, that
encodes spatio-temporal features on the sensor using purely binary operations.
PixelRNN reduces the amount of data to be transmitted off the sensor by a
factor of 64x compared to conventional systems while offering competitive
accuracy for hand gesture recognition and lip reading tasks. We experimentally
validate PixelRNN using a prototype implementation on the SCAMP-5
sensor-processor platform.",None,-1
e17eb813-8abc-46c9-a54c-87f2d1afca1c,Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation,0.41364,28,"Causal reasoning ability is crucial for numerous NLP applications. Despite
the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear
how well ChatGPT performs in causal reasoning. In this paper, we conduct the
first comprehensive evaluation of the ChatGPT's causal reasoning capabilities.
Experiments show that ChatGPT is not a good causal reasoner, but a good causal
explainer. Besides, ChatGPT has a serious hallucination on causal reasoning,
possibly due to the reporting biases between causal and non-causal
relationships in natural language, as well as ChatGPT's upgrading processes,
such as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (CoT)
techniques can further exacerbate such causal hallucination. Additionally, the
causal reasoning ability of ChatGPT is sensitive to the words used to express
the causal concept in prompts, and close-ended prompts perform better than
open-ended prompts. For events in sentences, ChatGPT excels at capturing
explicit causality rather than implicit causality, and performs better in
sentences with lower event density and smaller lexical distance between events.
The code is available on https://github.com/ArrogantL/ChatGPT4CausalReasoning .",None,-1
5b13000a-8097-460e-911e-725786f8b68d,Learning to Paraphrase Sentences to Different Complexity Levels,0.32436,5,"While sentence simplification is an active research topic in NLP, its
adjacent tasks of sentence complexification and same-level paraphrasing are
not. To train models on all three tasks, we present two new unsupervised
datasets. We compare these datasets, one labeled by a weak classifier and the
other by a rule-based approach, with a single supervised dataset. Using these
three datasets for training, we perform extensive experiments on both
multitasking and prompting strategies. Compared to other systems trained on
unsupervised parallel data, models trained on our weak classifier labeled
dataset achieve state-of-the-art performance on the ASSET simplification
benchmark. Our models also outperform previous work on sentence level
targeting. Finally, we establish how a handful of Large Language Models perform
on these tasks under a zero-shot setting.",None,-1
5d687106-e916-4e94-a0ac-62163029ada6,Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring,0.775477,9,"Automated essay scoring (AES) aims to score essays written for a given
prompt, which defines the writing topic. Most existing AES systems assume to
grade essays of the same prompt as used in training and assign only a holistic
score. However, such settings conflict with real-education situations;
pre-graded essays for a particular prompt are lacking, and detailed trait
scores of sub-rubrics are required. Thus, predicting various trait scores of
unseen-prompt essays (called cross-prompt essay trait scoring) is a remaining
challenge of AES. In this paper, we propose a robust model: prompt- and trait
relation-aware cross-prompt essay trait scorer. We encode prompt-aware essay
representation by essay-prompt attention and utilizing the topic-coherence
feature extracted by the topic-modeling mechanism without access to labeled
data; therefore, our model considers the prompt adherence of an essay, even in
a cross-prompt setting. To facilitate multi-trait scoring, we design
trait-similarity loss that encapsulates the correlations of traits. Experiments
prove the efficacy of our model, showing state-of-the-art results for all
prompts and traits. Significant improvements in low-resource-prompt and
inferior traits further indicate our model's strength.",None,-1
11b4405b-198a-49c4-82b4-40d3f56b51b6,Interaction-Aware Decision-Making for Autonomous Vehicles in Forced Merging Scenario Leveraging Social Psychology Factors,0.275522,3,"Understanding the intention of vehicles in the surrounding traffic is crucial
for an autonomous vehicle to successfully accomplish its driving tasks in
complex traffic scenarios such as highway forced merging. In this paper, we
consider a behavioral model that incorporates both social behaviors and
personal objectives of the interacting drivers. Leveraging this model, we
develop a receding-horizon control-based decision-making strategy, that
estimates online the other drivers' intentions using Bayesian filtering and
incorporates predictions of nearby vehicles' behaviors under uncertain
intentions. The effectiveness of the proposed decision-making strategy is
demonstrated and evaluated based on simulation studies in comparison with a
game theoretic controller and a real-world traffic dataset.",None,-1
ecc2553b-c305-44a5-a57e-c301b5ee91ff,UDApter -- Efficient Domain Adaptation Using Adapters,0.452783,8,"We propose two methods to make unsupervised domain adaptation (UDA) more
parameter efficient using adapters, small bottleneck layers interspersed with
every layer of the large-scale pre-trained language model (PLM). The first
method deconstructs UDA into a two-step process: first by adding a domain
adapter to learn domain-invariant information and then by adding a task adapter
that uses domain-invariant information to learn task representations in the
source domain. The second method jointly learns a supervised classifier while
reducing the divergence measure. Compared to strong baselines, our simple
methods perform well in natural language inference (MNLI) and the cross-domain
sentiment classification task. We even outperform unsupervised domain
adaptation methods such as DANN and DSN in sentiment classification, and we are
within 0.85% F1 for natural language inference task, by fine-tuning only a
fraction of the full model parameters. We release our code at
https://github.com/declare-lab/domadapter",None,-1
063b5448-7421-4732-a03c-63388e4fc65d,Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation,0.930704,29,"Recent advances in machine translation (MT) have shown that Minimum Bayes
Risk (MBR) decoding can be a powerful alternative to beam search decoding,
especially when combined with neural-based utility functions. However, the
performance of MBR decoding depends heavily on how and how many candidates are
sampled from the model. In this paper, we explore how different sampling
approaches for generating candidate lists for MBR decoding affect performance.
We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k
sampling. Based on our insights into their limitations, we experiment with the
recently proposed epsilon-sampling approach, which prunes away all tokens with
a probability smaller than epsilon, ensuring that each token in a sample
receives a fair probability mass. Through extensive human evaluations, we
demonstrate that MBR decoding based on epsilon-sampling significantly
outperforms not only beam search decoding, but also MBR decoding with all other
tested sampling methods across four language pairs.",None,-1
c16a27bf-09a9-4221-b7be-4e48a88cf5de,Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning,0.929842,10,"In cross-lingual named entity recognition (NER), self-training is commonly
used to bridge the linguistic gap by training on pseudo-labeled target-language
data. However, due to sub-optimal performance on target languages, the pseudo
labels are often noisy and limit the overall performance. In this work, we aim
to improve self-training for cross-lingual NER by combining representation
learning and pseudo label refinement in one coherent framework. Our proposed
method, namely ContProto mainly comprises two components: (1) contrastive
self-training and (2) prototype-based pseudo-labeling. Our contrastive
self-training facilitates span classification by separating clusters of
different classes, and enhances cross-lingual transferability by producing
closely-aligned representations between the source and target language.
Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of
pseudo labels during training. We evaluate ContProto on multiple transfer
pairs, and experimental results show our method brings in substantial
improvements over current state-of-the-art methods.",None,-1
5282df2b-1575-4589-84f4-4ce5131b5644,"First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models",0.0375026,3,"Many NLP researchers are experiencing an existential crisis triggered by the
astonishing success of ChatGPT and other systems based on large language models
(LLMs). After such a disruptive change to our understanding of the field, what
is left to do? Taking a historical lens, we look for guidance from the first
era of LLMs, which began in 2005 with large $n$-gram models for machine
translation (MT). We identify durable lessons from the first era, and more
importantly, we identify evergreen problems where NLP researchers can continue
to make meaningful contributions in areas where LLMs are ascendant. We argue
that disparities in scale are transient and researchers can work to reduce
them; that data, rather than hardware, is still a bottleneck for many
applications; that meaningful realistic evaluation is still an open problem;
and that there is still room for speculative approaches.",None,-1
13ddb44b-e595-4055-aebe-6ffe6d9ae293,Reward Design with Language Models,0.959137,108,"Reward design in reinforcement learning (RL) is challenging since specifying
human notions of desired behavior may be difficult via reward functions or
require many expert demonstrations. Can we instead cheaply design rewards using
a natural language interface? This paper explores how to simplify reward design
by prompting a large language model (LLM) such as GPT-3 as a proxy reward
function, where the user provides a textual prompt containing a few examples
(few-shot) or a description (zero-shot) of the desired behavior. Our approach
leverages this proxy reward function in an RL framework. Specifically, users
specify a prompt once at the beginning of training. During training, the LLM
evaluates an RL agent's behavior against the desired behavior described by the
prompt and outputs a corresponding reward signal. The RL agent then uses this
reward to update its behavior. We evaluate whether our approach can train
agents aligned with user objectives in the Ultimatum Game, matrix games, and
the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents
trained with our framework are well-aligned with the user's objectives and
outperform RL agents trained with reward functions learned via supervised
learning",None,-1
1cc7e869-cd68-456f-b041-27c8dfe75f35,"SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations",0.999633,14,"Large language models (LLMs) have been widely applied in various fields due
to their excellent capability for memorizing knowledge and chain of thought
(CoT). When these language models are applied in the field of psychological
counseling, they often rush to provide universal advice. However, when users
seek psychological support, they need to gain empathy, trust, understanding and
comfort, rather than just reasonable advice. To this end, we constructed a
multi-turn empathetic conversation dataset of more than 2 million samples, in
which the input is the multi-turn conversation context, and the target is
empathetic responses that cover expressions such as questioning, comfort,
recognition, listening, trust, emotional support, etc. Experiments have shown
that the empathy ability of LLMs can be significantly enhanced when finetuning
by using multi-turn dialogue history and responses that are closer to the
expression of a psychological consultant.",None,-1
6435b849-f1e9-4116-9493-a888670f3c52,Consistency Analysis of ChatGPT,0.478367,27,"ChatGPT has gained a huge popularity since its introduction. Its positive
aspects have been reported through many media platforms, and some analyses even
showed that ChatGPT achieved a decent grade in professional exams, adding extra
support to the claim that AI can now assist and even replace humans in
industrial fields. Others, however, doubt its reliability and trustworthiness.
This paper investigates the trustworthiness of ChatGPT and GPT-4 regarding
logically consistent behaviour, focusing specifically on semantic consistency
and the properties of negation, symmetric, and transitive consistency. Our
findings suggest that while both models appear to show an enhanced language
understanding and reasoning ability, they still frequently fall short of
generating logically consistent predictions. We also ascertain via experiments
that prompt designing, few-shot learning and employing larger large language
models (LLMs) are unlikely to be the ultimate solution to resolve the
inconsistency issue of LLMs.",None,-1
e4398fb7-e904-4958-8eb3-2759e8f2abbe,Revisiting Machine Translation for Cross-lingual Classification,0.917145,22,"Machine Translation (MT) has been widely used for cross-lingual
classification, either by translating the test set into English and running
inference with a monolingual model (translate-test), or translating the
training set into the target languages and finetuning a multilingual model
(translate-train). However, most research in the area focuses on the
multilingual models rather than the MT component. We show that, by using a
stronger MT system and mitigating the mismatch between training on original
text and running inference on machine translated text, translate-test can do
substantially better than previously assumed. The optimal approach, however, is
highly task dependent, as we identify various sources of cross-lingual transfer
gap that affect different tasks and approaches differently. Our work calls into
question the dominance of multilingual models for cross-lingual classification,
and prompts to pay more attention to MT-based baselines.",None,-1
4c092c35-ac15-4232-b130-297c703d42c4,Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading,0.793979,34,"Large language models (LLMs) have advanced in large strides due to the
effectiveness of the self-attention mechanism that processes and compares all
tokens at once. However, this mechanism comes with a fundamental issue -- the
predetermined context window is bound to be limited. Despite attempts to extend
the context window through methods like extrapolating the positional embedding,
using recurrence, or selectively retrieving essential parts of the long
sequence, long-text understanding continues to be a challenge. We propose an
alternative approach which instead treats the LLM as an interactive agent,
allowing it to decide how to read the text via iterative prompting. We
introduce MemWalker, a method that first processes the long context into a tree
of summary nodes. Upon receiving a query, the model navigates this tree in
search of relevant information, and responds once it gathers sufficient
information. On long-text question answering tasks our method outperforms
baseline approaches that use long context windows, recurrence, and retrieval.
We show that, beyond effective reading, MemWalker enhances explainability by
highlighting the reasoning steps as it interactively reads the text;
pinpointing the relevant text segments related to the query.",None,-1
5818da8c-8ffa-46b0-823c-113c9b2a7dd2,Evaluating Online Bandit Exploration In Large-Scale Recommender System,0.442494,6,"Bandit learning has been an increasingly popular design choice for
recommender system. Despite the strong interest in bandit learning from the
community, there remains multiple bottlenecks that prevent many bandit learning
approaches from productionalization. One major bottleneck is how to test the
effectiveness of bandit algorithm with fairness and without data leakage.
Different from supervised learning algorithms, bandit learning algorithms
emphasize greatly on the data collection process through their explorative
nature. Such explorative behavior may induce unfair evaluation in a classic A/B
test setting. In this work, we apply upper confidence bound (UCB) to our large
scale short video recommender system and present a test framework for the
production bandit learning life-cycle with a new set of metrics. Extensive
experiment results show that our experiment design is able to fairly evaluate
the performance of bandit learning in the recommender system.",None,-1
91b84020-471f-4630-be51-79d592225579,Preference-conditioned Pixel-based AI Agent For Game Testing,0.544551,2,"The game industry is challenged to cope with increasing growth in demand and
game complexity while maintaining acceptable quality standards for released
games. Classic approaches solely depending on human efforts for quality
assurance and game testing do not scale effectively in terms of time and cost.
Game-testing AI agents that learn by interaction with the environment have the
potential to mitigate these challenges with good scalability properties on time
and costs. However, most recent work in this direction depends on game state
information for the agent's state representation, which limits generalization
across different game scenarios. Moreover, game test engineers usually prefer
exploring a game in a specific style, such as exploring the golden path.
However, current game testing AI agents do not provide an explicit way to
satisfy such a preference. This paper addresses these limitations by proposing
an agent design that mainly depends on pixel-based state observations while
exploring the environment conditioned on a user's preference specified by
demonstration trajectories. In addition, we propose an imitation learning
method that couples self-supervised and supervised learning objectives to
enhance the quality of imitation behaviors. Our agent significantly outperforms
state-of-the-art pixel-based game testing agents over exploration coverage and
test execution quality when evaluated on a complex open-world environment
resembling many aspects of real AAA games.",None,-1
4fd8aa3a-3383-446b-9b53-8dd17c2b9411,FSNet: Redesign Self-Supervised MonoDepth for Full-Scale Depth Prediction for Autonomous Driving,0.0604004,1,"Predicting accurate depth with monocular images is important for low-cost
robotic applications and autonomous driving. This study proposes a
comprehensive self-supervised framework for accurate scale-aware depth
prediction on autonomous driving scenes utilizing inter-frame poses obtained
from inertial measurements. In particular, we introduce a Full-Scale depth
prediction network named FSNet. FSNet contains four important improvements over
existing self-supervised models: (1) a multichannel output representation for
stable training of depth prediction in driving scenarios, (2) an
optical-flow-based mask designed for dynamic object removal, (3) a
self-distillation training strategy to augment the training process, and (4) an
optimization-based post-processing algorithm in test time, fusing the results
from visual odometry. With this framework, robots and vehicles with only one
well-calibrated camera can collect sequences of training image frames and
camera poses, and infer accurate 3D depths of the environment without extra
labeling work or 3D data. Extensive experiments on the KITTI dataset, KITTI-360
dataset and the nuScenes dataset demonstrate the potential of FSNet. More
visualizations are presented in \url{https://sites.google.com/view/fsnet/home}",None,-1
4e00127b-6c93-4b15-8876-0443df6a13d3,The Big Data Myth: Using Diffusion Models for Dataset Generation to Train Deep Detection Models,0.415661,4,"Despite the notable accomplishments of deep object detection models, a major
challenge that persists is the requirement for extensive amounts of training
data. The process of procuring such real-world data is a laborious undertaking,
which has prompted researchers to explore new avenues of research, such as
synthetic data generation techniques. This study presents a framework for the
generation of synthetic datasets by fine-tuning pretrained stable diffusion
models. The synthetic datasets are then manually annotated and employed for
training various object detection models. These detectors are evaluated on a
real-world test set of 331 images and compared against a baseline model that
was trained on real-world images. The results of this study reveal that the
object detection models trained on synthetic data perform similarly to the
baseline model. In the context of apple detection in orchards, the average
precision deviation with the baseline ranges from 0.09 to 0.12. This study
illustrates the potential of synthetic data generation techniques as a viable
alternative to the collection of extensive training data for the training of
deep models.",None,-1
6314f4b5-cdad-47f1-9985-21bfd193da6e,Multilingual Sentence Transformer as A Multilingual Word Aligner,0.41291,4,"Multilingual pretrained language models (mPLMs) have shown their
effectiveness in multilingual word alignment induction. However, these methods
usually start from mBERT or XLM-R. In this paper, we investigate whether
multilingual sentence Transformer LaBSE is a strong multilingual word aligner.
This idea is non-trivial as LaBSE is trained to learn language-agnostic
sentence-level embeddings, while the alignment extraction task requires the
more fine-grained word-level embeddings to be language-agnostic. We demonstrate
that the vanilla LaBSE outperforms other mPLMs currently used in the alignment
task, and then propose to finetune LaBSE on parallel corpus for further
improvement. Experiment results on seven language pairs show that our best
aligner outperforms previous state-of-the-art models of all varieties. In
addition, our aligner supports different language pairs in a single model, and
even achieves new state-of-the-art on zero-shot language pairs that does not
appear in the finetuning process.",None,-1
ee5e2a23-a20e-404e-b57c-6c02f49d0cc0,RPTQ: Reorder-based Post-training Quantization for Large Language Models,0.915786,43,"Large-scale language models (LLMs) have demonstrated impressive performance,
but their deployment presents challenges due to their significant memory usage.
This issue can be alleviated through quantization. In this paper, we identify
that the challenge in quantizing activations in LLMs arises from varying ranges
across channels, rather than solely the presence of outliers. To address this
challenge, we introduce a quantization method called RPTQ, which utilizes a
reorder-based approach. By rearranging the channels and quantizing them in
clusters, RPTQ effectively mitigates the impact of range differences between
channels. To minimize the overhead of the reorder operation, we fuse it into
the layer norm operation and weights in linear layers. In our experiments, RPTQ
achieved a significant breakthrough by utilizing 3-bit activation in LLMs for
the first time, resulting in a substantial reduction in memory usage. For
instance, quantizing OPT-175b can lead to a memory consumption reduction of up
to 80%.",None,-1
07c276ed-2cd2-4541-8407-683881cdf45c,Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance,0.828616,17,"ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that
are slated to promise different applications in diverse areas. In education,
these AI technologies have been tested for applications in assessment and
teaching. In assessment, AI has long been used in automated essay scoring and
automated item generation. One psychometric property that these tools must have
to assist or replace humans in assessment is high reliability in terms of
agreement between AI scores and human raters. In this paper, we measure the
reliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and
trained humans in perceiving and rating the complexity of writing prompts.
Intraclass correlation (ICC) as a performance metric showed that the
inter-reliability of both the OpenAI ChatGPT and the Google Bard were low
against the gold standard of human ratings.",None,-1
55d6ee9c-a49c-499c-a286-76296bdd8f3c,Unsupervised Chunking with Hierarchical RNN,0.1506,2,"In Natural Language Processing (NLP), predicting linguistic structures, such
as parsing and chunking, has mostly relied on manual annotations of syntactic
structures. This paper introduces an unsupervised approach to chunking, a
syntactic task that involves grouping words in a non-hierarchical manner. We
present a two-layer Hierarchical Recurrent Neural Network (HRNN) designed to
model word-to-chunk and chunk-to-sentence compositions. Our approach involves a
two-stage training process: pretraining with an unsupervised parser and
finetuning on downstream NLP tasks. Experiments on the CoNLL-2000 dataset
reveal a notable improvement over existing unsupervised methods, enhancing
phrase F1 score by up to 6 percentage points. Further, finetuning with
downstream tasks results in an additional performance improvement.
Interestingly, we observe that the emergence of the chunking structure is
transient during the neural model's downstream-task training. This study
contributes to the advancement of unsupervised syntactic structure discovery
and opens avenues for further research in linguistic theory.",None,-1
78373268-0602-4c69-b764-9cf467374179,ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing,0.893339,2,"English and Chinese, known as resource-rich languages, have witnessed the
strong development of transformer-based language models for natural language
processing tasks. Although Vietnam has approximately 100M people speaking
Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,
performed well on general Vietnamese NLP tasks, including POS tagging and named
entity recognition. These pre-trained language models are still limited to
Vietnamese social media tasks. In this paper, we present the first monolingual
pre-trained language model for Vietnamese social media texts, ViSoBERT, which
is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese
social media texts using XLM-R architecture. Moreover, we explored our
pre-trained model on five important natural language downstream tasks on
Vietnamese social media texts: emotion recognition, hate speech detection,
sentiment analysis, spam reviews detection, and hate speech spans detection.
Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses
the previous state-of-the-art models on multiple Vietnamese social media tasks.
Our ViSoBERT model is available only for research purposes.",None,-1
9b8b91ac-2511-4207-9553-06abcc48c3d3,CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer,0.760784,9,"Content affinity loss including feature and pixel affinity is a main problem
which leads to artifacts in photorealistic and video style transfer. This paper
proposes a new framework named CAP-VSTNet, which consists of a new reversible
residual network and an unbiased linear transform module, for versatile style
transfer. This reversible residual network can not only preserve content
affinity but not introduce redundant information as traditional reversible
networks, and hence facilitate better stylization. Empowered by Matting
Laplacian training loss which can address the pixel affinity loss problem led
by the linear transform, the proposed framework is applicable and effective on
versatile style transfer. Extensive experiments show that CAP-VSTNet can
produce better qualitative and quantitative results in comparison with the
state-of-the-art methods.",None,-1
49aa567a-80f7-45a6-889d-cbbb7b7be718,S-TREK: Sequential Translation and Rotation Equivariant Keypoints for local feature extraction,0.637307,3,"In this work we introduce S-TREK, a novel local feature extractor that
combines a deep keypoint detector, which is both translation and rotation
equivariant by design, with a lightweight deep descriptor extractor. We train
the S-TREK keypoint detector within a framework inspired by reinforcement
learning, where we leverage a sequential procedure to maximize a reward
directly related to keypoint repeatability. Our descriptor network is trained
following a ""detect, then describe"" approach, where the descriptor loss is
evaluated only at those locations where keypoints have been selected by the
already trained detector. Extensive experiments on multiple benchmarks confirm
the effectiveness of our proposed method, with S-TREK often outperforming other
state-of-the-art methods in terms of repeatability and quality of the recovered
poses, especially when dealing with in-plane rotations.",None,-1
412edb14-1476-4cb2-bd3e-8b8ae3ee2dd9,A Novel Approach for Auto-Formulation of Optimization Problems,0.622167,7,"In the Natural Language for Optimization (NL4Opt) NeurIPS 2022 competition,
competitors focus on improving the accessibility and usability of optimization
solvers, with the aim of subtask 1: recognizing the semantic entities that
correspond to the components of the optimization problem; subtask 2: generating
formulations for the optimization problem. In this paper, we present the
solution of our team. First, we treat subtask 1 as a named entity recognition
(NER) problem with the solution pipeline including pre-processing methods,
adversarial training, post-processing methods and ensemble learning. Besides,
we treat subtask 2 as a generation problem with the solution pipeline including
specially designed prompts, adversarial training, post-processing methods and
ensemble learning. Our proposed methods have achieved the F1-score of 0.931 in
subtask 1 and the accuracy of 0.867 in subtask 2, which won the fourth and
third places respectively in this competition. Our code is available at
https://github.com/bigdata-ustc/nl4opt.",None,-1
7d0674aa-1c14-41b9-9533-5f3ad44dc2fc,PDPU: An Open-Source Posit Dot-Product Unit for Deep Learning Applications,0.439626,3,"Posit has been a promising alternative to the IEEE-754 floating point format
for deep learning applications due to its better trade-off between dynamic
range and accuracy. However, hardware implementation of posit arithmetic
requires further exploration, especially for the dot-product operations
dominated in deep neural networks (DNNs). It has been implemented by either the
combination of multipliers and an adder tree or cascaded fused multiply-add
units, leading to poor computational efficiency and excessive hardware
overhead. To address this issue, we propose an open-source posit dot-product
unit, namely PDPU, that facilitates resource-efficient and high-throughput
dot-product hardware implementation. PDPU not only features the fused and
mixed-precision architecture that eliminates redundant latency and hardware
resources, but also has a fine-grained 6-stage pipeline, improving
computational efficiency. A configurable PDPU generator is further developed to
meet the diverse needs of various DNNs for computational accuracy. Experimental
results evaluated under the 28nm CMOS process show that PDPU reduces area,
latency, and power by up to 43%, 64%, and 70%, respectively, compared to the
existing implementations. Hence, PDPU has great potential as the computing core
of posit-based accelerators for deep learning applications.",None,-1
5634cb2e-7ed9-4cd7-81ec-eda36471618a,"MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models",0.150861,1,"In online forums like Reddit, users share their experiences with medical
conditions and treatments, including making claims, asking questions, and
discussing the effects of treatments on their health. Building systems to
understand this information can effectively monitor the spread of
misinformation and verify user claims. The Task-8 of the 2023 International
Workshop on Semantic Evaluation focused on medical applications, specifically
extracting patient experience- and medical condition-related entities from user
posts on social media. The Reddit Health Online Talk (RedHot) corpus contains
posts from medical condition-related subreddits with annotations characterizing
the patient experience and medical conditions. In Subtask-1, patient experience
is characterized by personal experience, questions, and claims. In Subtask-2,
medical conditions are characterized by population, intervention, and outcome.
For the automatic extraction of patient experiences and medical condition
information, as a part of the challenge, we proposed language-model-based
extraction systems that ranked $3^{rd}$ on both subtasks' leaderboards. In this
work, we describe our approach and, in addition, explore the automatic
extraction of this information using domain-specific language models and the
inclusion of external knowledge.",None,-1
0b8243b7-14ea-4066-a4b9-19e5f89e140d,Detecting and Mitigating Hallucinations in Multilingual Summarisation,0.859355,21,"Hallucinations pose a significant challenge to the reliability of neural
models for abstractive summarisation. While automatically generated summaries
may be fluent, they often lack faithfulness to the original document. This
issue becomes even more pronounced in low-resource settings, such as
cross-lingual transfer. With the existing faithful metrics focusing on English,
even measuring the extent of this phenomenon in cross-lingual settings is hard.
To address this, we first develop a novel metric, mFACT, evaluating the
faithfulness of non-English summaries, leveraging translation-based transfer
from multiple English faithfulness metrics. We then propose a simple but
effective method to reduce hallucinations with a cross-lingual transfer, which
weighs the loss of each training example by its faithfulness score. Through
extensive experiments in multiple languages, we demonstrate that mFACT is the
metric that is most suited to detect hallucinations. Moreover, we find that our
proposed loss weighting method drastically increases both performance and
faithfulness according to both automatic and human evaluation when compared to
strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset
are available at https://github.com/yfqiu-nlp/mfact-summ.",None,-1
ebea1eae-4901-4744-9e60-32f410e03e55,1st Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation,0.397605,3,"Video panoptic segmentation is a challenging task that serves as the
cornerstone of numerous downstream applications, including video editing and
autonomous driving. We believe that the decoupling strategy proposed by DVIS
enables more effective utilization of temporal information for both ""thing"" and
""stuff"" objects. In this report, we successfully validated the effectiveness of
the decoupling strategy in video panoptic segmentation. Finally, our method
achieved a VPQ score of 51.4 and 53.7 in the development and test phases,
respectively, and ultimately ranked 1st in the VPS track of the 2nd PVUW
Challenge. The code is available at https://github.com/zhang-tao-whu/DVIS",None,-1
e1e56022-18f9-4013-aa01-2eb523235857,A Study on Knowledge Distillation from Weak Teacher for Scaling Up Pre-trained Language Models,0.205354,2,"Distillation from Weak Teacher (DWT) is a method of transferring knowledge
from a smaller, weaker teacher model to a larger student model to improve its
performance. Previous studies have shown that DWT can be effective in the
vision domain and natural language processing (NLP) pre-training stage.
Specifically, DWT shows promise in practical scenarios, such as enhancing new
generation or larger models using pre-trained yet older or smaller models and
lacking a resource budget. However, the optimal conditions for using DWT have
yet to be fully investigated in NLP pre-training. Therefore, this study
examines three key factors to optimize DWT, distinct from those used in the
vision domain or traditional knowledge distillation. These factors are: (i) the
impact of teacher model quality on DWT effectiveness, (ii) guidelines for
adjusting the weighting value for DWT loss, and (iii) the impact of parameter
remapping as a student model initialization technique for DWT.",None,-1
a404d560-d5e0-40ab-bdb9-9986fd34e201,Low-Light Image Enhancement by Learning Contrastive Representations in Spatial and Frequency Domains,0.606564,5,"Images taken under low-light conditions tend to suffer from poor visibility,
which can decrease image quality and even reduce the performance of the
downstream tasks. It is hard for a CNN-based method to learn generalized
features that can recover normal images from the ones under various unknow
low-light conditions. In this paper, we propose to incorporate the contrastive
learning into an illumination correction network to learn abstract
representations to distinguish various low-light conditions in the
representation space, with the purpose of enhancing the generalizability of the
network. Considering that light conditions can change the frequency components
of the images, the representations are learned and compared in both spatial and
frequency domains to make full advantage of the contrastive learning. The
proposed method is evaluated on LOL and LOL-V2 datasets, the results show that
the proposed method achieves better qualitative and quantitative results
compared with other state-of-the-arts.",None,-1
1d84b629-6046-4f7b-9671-940424035835,Exploring Lottery Prompts for Pre-trained Language Models,0.0875761,4,"Consistently scaling pre-trained language models (PLMs) imposes substantial
burdens on model adaptation, necessitating more efficient alternatives to
conventional fine-tuning. Given the advantage of prompting in the zero-shot
setting and the observed performance fluctuation among different prompts, we
explore the instance-level prompt and their generalizability. By searching
through the prompt space, we first validate the assumption that for every
instance, there is almost always a lottery prompt that induces the correct
prediction from the PLM, and such prompt can be obtained at a low cost thanks
to the inherent ability of PLMs. Meanwhile, we find that some strong lottery
prompts have high performance over the whole training set, and they are
equipped with distinguishable linguistic features. Lastly, we attempt to
generalize the searched strong lottery prompts to unseen data with prompt
ensembling method without any parameter tuning. Experiments are conducted on
various types of NLP classification tasks and demonstrate that the proposed
method can achieve comparable results with other gradient-free and
optimization-free baselines.",None,-1
04e664ce-07c7-40ff-ba3b-3632296bd709,Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN),0.809814,6,"Recently, there has been a remarkable increase in the interest towards
skeleton-based action recognition within the research community, owing to its
various advantageous features, including computational efficiency,
representative features, and illumination invariance. Despite this, researchers
continue to explore and investigate the most optimal way to represent human
actions through skeleton representation and the extracted features. As a
result, the growth and availability of human action recognition datasets have
risen substantially. In addition, deep learning-based algorithms have gained
widespread popularity due to the remarkable advancements in various computer
vision tasks. Most state-of-the-art contributions in skeleton-based action
recognition incorporate a Graph Neural Network (GCN) architecture for
representing the human body and extracting features. Our research demonstrates
that Convolutional Neural Networks (CNNs) can attain comparable results to GCN,
provided that the proper training techniques, augmentations, and optimizers are
applied. Our approach has been rigorously validated, and we have achieved a
score of 95% on the NTU-60 dataset",None,-1
fb9b9b64-7d6a-48c2-b13e-d60baf507c11,Language Embeddings Sometimes Contain Typological Generalizations,0.154159,7,"To what extent can neural network models learn generalizations about language
structure, and how do we find out what they have learned? We explore these
questions by training neural models for a range of natural language processing
tasks on a massively multilingual dataset of Bible translations in 1295
languages. The learned language representations are then compared to existing
typological databases as well as to a novel set of quantitative syntactic and
morphological features obtained through annotation projection. We conclude that
some generalizations are surprisingly close to traditional features from
linguistic typology, but that most of our models, as well as those of previous
work, do not appear to have made linguistically meaningful generalizations.
Careful attention to details in the evaluation turns out to be essential to
avoid false positives. Furthermore, to encourage continued work in this field,
we release several resources covering most or all of the languages in our data:
(i) multiple sets of language representations, (ii) multilingual word
embeddings, (iii) projected and predicted syntactic and morphological features,
(iv) software to provide linguistically sound evaluations of language
representations.",None,-1
2a113652-6bef-4ac6-bb50-03ed1aec7307,DehazeNeRF: Multiple Image Haze Removal and 3D Shape Reconstruction using Neural Radiance Fields,0.709705,6,"Neural radiance fields (NeRFs) have demonstrated state-of-the-art performance
for 3D computer vision tasks, including novel view synthesis and 3D shape
reconstruction. However, these methods fail in adverse weather conditions. To
address this challenge, we introduce DehazeNeRF as a framework that robustly
operates in hazy conditions. DehazeNeRF extends the volume rendering equation
by adding physically realistic terms that model atmospheric scattering. By
parameterizing these terms using suitable networks that match the physical
properties, we introduce effective inductive biases, which, together with the
proposed regularizations, allow DehazeNeRF to demonstrate successful multi-view
haze removal, novel view synthesis, and 3D shape reconstruction where existing
approaches fail.",None,-1
fc32ea84-9f47-44db-919f-2b936430b176,WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset,0.487612,2,"We present WebQAmGaze, a multilingual low-cost eye-tracking-while-reading
dataset, designed as the first webcam-based eye-tracking corpus of reading to
support the development of explainable computational language processing
models. WebQAmGaze includes webcam eye-tracking data from 600 participants of a
wide age range naturally reading English, German, Spanish, and Turkish texts.
Each participant performs two reading tasks composed of five texts each, a
normal reading and an information-seeking task, followed by a comprehension
question. We compare the collected webcam data to high-quality eye-tracking
recordings. The results show a moderate to strong correlation between the eye
movement measures obtained with the webcam compared to those obtained with a
commercial eye-tracking device. When validating the data, we find that higher
fixation duration on relevant text spans accurately indicates correctness when
answering the corresponding questions. This dataset advances webcam-based
reading studies and opens avenues to low-cost and diverse data collection.
WebQAmGaze is beneficial to learn about the cognitive processes behind
question-answering and to apply these insights to computational models of
language understanding.",None,-1
28d900e9-0a55-48b1-b46d-d89e29f56a2b,Delving into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling,0.382962,7,"Normalizing flows (NFs) provide a powerful tool to construct an expressive
distribution by a sequence of trackable transformations of a base distribution
and form a probabilistic model of underlying data. Rotation, as an important
quantity in computer vision, graphics, and robotics, can exhibit many
ambiguities when occlusion and symmetry occur and thus demands such
probabilistic models. Though much progress has been made for NFs in Euclidean
space, there are no effective normalizing flows without discontinuity or
many-to-one mapping tailored for SO(3) manifold. Given the unique non-Euclidean
properties of the rotation manifold, adapting the existing NFs to SO(3)
manifold is non-trivial. In this paper, we propose a novel normalizing flow on
SO(3) by combining a Mobius transformation-based coupling layer and a
quaternion affine transformation. With our proposed rotation normalizing flows,
one can not only effectively express arbitrary distributions on SO(3), but also
conditionally build the target distribution given input observations. Extensive
experiments show that our rotation normalizing flows significantly outperform
the baselines on both unconditional and conditional tasks.",None,-1
e4900e60-7639-4952-a938-2a3eec4c4079,MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation,0.969988,194,"Recent advances in text-to-image generation with diffusion models present
transformative capabilities in image quality. However, user controllability of
the generated image, and fast adaptation to new tasks still remains an open
challenge, currently mostly addressed by costly and long re-training and
fine-tuning or ad-hoc adaptations to specific image generation tasks. In this
work, we present MultiDiffusion, a unified framework that enables versatile and
controllable image generation, using a pre-trained text-to-image diffusion
model, without any further training or finetuning. At the center of our
approach is a new generation process, based on an optimization task that binds
together multiple diffusion generation processes with a shared set of
parameters or constraints. We show that MultiDiffusion can be readily applied
to generate high quality and diverse images that adhere to user-provided
controls, such as desired aspect ratio (e.g., panorama), and spatial guiding
signals, ranging from tight segmentation masks to bounding boxes. Project
webpage: https://multidiffusion.github.io",None,-1
c41f6e7a-b61b-4d9d-a5dd-a760ae7c6a98,"""Are you telling me to put glasses on the dog?'' Content-Grounded Annotation of Instruction Clarification Requests in the CoDraw Dataset",0.126238,3,"Instruction Clarification Requests are a mechanism to solve communication
problems, which is very functional in instruction-following interactions.
Recent work has argued that the CoDraw dataset is a valuable source of
naturally occurring iCRs. Beyond identifying when iCRs should be made, dialogue
models should also be able to generate them with suitable form and content. In
this work, we introduce CoDraw-iCR (v2), extending the existing iCR identifiers
with fine-grained information grounded in the underlying dialogue game items
and possible actions. Our annotation can serve to model and evaluate repair
capabilities of dialogue agents.",None,-1
ebc68e73-5430-4201-865b-1ce8911bec62,PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts,0.275396,3,"A key component of modern conversational systems is the Dialogue State
Tracker (or DST), which models a user's goals and needs. Toward building more
robust and reliable DSTs, we introduce a prompt-based learning approach to
automatically generate effective adversarial examples to probe DST models. Two
key characteristics of this approach are: (i) it only needs the output of the
DST with no need for model parameters, and (ii) it can learn to generate
natural language utterances that can target any DST. Through experiments over
state-of-the-art DSTs, the proposed framework leads to the greatest reduction
in accuracy and the best attack success rate while maintaining good fluency and
a low perturbation ratio. We also show how much the generated adversarial
examples can bolster a DST through adversarial training. These results indicate
the strength of prompt-based attacks on DSTs and leave open avenues for
continued refinement.",None,-1
ca1dfba9-1ad9-423f-99be-8198cf112159,Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge,0.195953,28,"Large language models (LLMs) have been widely studied for their ability to
store and utilize positive knowledge. However, negative knowledge, such as
""lions don't live in the ocean"", is also ubiquitous in the world but rarely
mentioned explicitly in the text. What do LLMs know about negative knowledge?
This work examines the ability of LLMs to negative commonsense knowledge. We
design a constrained keywords-to-sentence generation task (CG) and a Boolean
question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs
frequently fail to generate valid sentences grounded in negative commonsense
knowledge, yet they can correctly answer polar yes-or-no questions. We term
this phenomenon the belief conflict of LLMs. Our further analysis shows that
statistical shortcuts and negation reporting bias from language modeling
pre-training cause this conflict.",None,-1
30f4d544-6c5e-4b74-a671-ec8736761146,LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models,0.252246,10,"Prompt engineering is a powerful tool used to enhance the performance of
pre-trained models on downstream tasks. For example, providing the prompt
""Let's think step by step"" improved GPT-3's reasoning accuracy to 63% on
MutiArith while prompting ""a photo of"" filled with a class name enables CLIP to
achieve $80$\% zero-shot accuracy on ImageNet. While previous research has
explored prompt learning for the visual modality, analyzing what constitutes a
good visual prompt specifically for image recognition is limited. In addition,
existing visual prompt tuning methods' generalization ability is worse than
text-only prompting tuning. This paper explores our key insight: synthetic text
images are good visual prompts for vision-language models! To achieve that, we
propose our LoGoPrompt, which reformulates the classification objective to the
visual prompt selection and addresses the chicken-and-egg challenge of first
adding synthetic text images as class-wise visual prompts or predicting the
class first. Without any trainable visual prompt parameters, experimental
results on 16 datasets demonstrate that our method consistently outperforms
state-of-the-art methods in few-shot learning, base-to-new generalization, and
domain generalization.",None,-1
68f25062-a3f4-452c-961f-586480e02c90,Unsupervised 3D registration through optimization-guided cyclical self-training,0.293736,4,"State-of-the-art deep learning-based registration methods employ three
different learning strategies: supervised learning, which requires costly
manual annotations, unsupervised learning, which heavily relies on hand-crafted
similarity metrics designed by domain experts, or learning from synthetic data,
which introduces a domain shift. To overcome the limitations of these
strategies, we propose a novel self-supervised learning paradigm for
unsupervised registration, relying on self-training. Our idea is based on two
key insights. Feature-based differentiable optimizers 1) perform reasonable
registration even from random features and 2) stabilize the training of the
preceding feature extraction network on noisy labels. Consequently, we propose
cyclical self-training, where pseudo labels are initialized as the displacement
fields inferred from random features and cyclically updated based on more and
more expressive features from the learning feature extractor, yielding a
self-reinforcement effect. We evaluate the method for abdomen and lung
registration, consistently surpassing metric-based supervision and
outperforming diverse state-of-the-art competitors. Source code is available at
https://github.com/multimodallearning/reg-cyclical-self-train.",None,-1
0ad1e116-2dab-4cdf-b0e7-9ee46544ff30,Deep Neural Networks for Encrypted Inference with TFHE,0.465609,13,"Fully homomorphic encryption (FHE) is an encryption method that allows to
perform computation on encrypted data, without decryption. FHE preserves the
privacy of the users of online services that handle sensitive data, such as
health data, biometrics, credit scores and other personal information. A common
way to provide a valuable service on such data is through machine learning and,
at this time, Neural Networks are the dominant machine learning model for
unstructured data. In this work we show how to construct Deep Neural Networks
(DNN) that are compatible with the constraints of TFHE, an FHE scheme that
allows arbitrary depth computation circuits. We discuss the constraints and
show the architecture of DNNs for two computer vision tasks. We benchmark the
architectures using the Concrete stack, an open-source implementation of TFHE.",None,-1
22838a7a-6b4a-41b1-a88a-67595e60cbab,SSN: Stockwell Scattering Network for SAR Image Change Detection,0.554084,5,"Recently, synthetic aperture radar (SAR) image change detection has become an
interesting yet challenging direction due to the presence of speckle noise.
Although both traditional and modern learning-driven methods attempted to
overcome this challenge, deep convolutional neural networks (DCNNs)-based
methods are still hindered by the lack of interpretability and the requirement
of large computation power. To overcome this drawback, wavelet scattering
network (WSN) and Fourier scattering network (FSN) are proposed. Combining
respective merits of WSN and FSN, we propose Stockwell scattering network (SSN)
based on Stockwell transform which is widely applied against noisy signals and
shows advantageous characteristics in speckle reduction. The proposed SSN
provides noise-resilient feature representation and obtains state-of-art
performance in SAR image change detection as well as high computational
efficiency. Experimental results on three real SAR image datasets demonstrate
the effectiveness of the proposed method.",None,-1
588f19f8-610f-4f7b-8104-5f926d53d6bf,LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation,0.975885,76,"Recently, diffusion models have achieved great success in image synthesis.
However, when it comes to the layout-to-image generation where an image often
has a complex scene of multiple objects, how to make strong control over both
the global layout map and each detailed object remains a challenging task. In
this paper, we propose a diffusion model named LayoutDiffusion that can obtain
higher generation quality and greater controllability than the previous works.
To overcome the difficult multimodal fusion of image and layout, we propose to
construct a structural image patch with region information and transform the
patched image into a special layout to fuse with the normal layout in a unified
form. Moreover, Layout Fusion Module (LFM) and Object-aware Cross Attention
(OaCA) are proposed to model the relationship among multiple objects and
designed to be object-aware and position-sensitive, allowing for precisely
controlling the spatial related information. Extensive experiments show that
our LayoutDiffusion outperforms the previous SOTA methods on FID, CAS by
relatively 46.35%, 26.70% on COCO-stuff and 44.29%, 41.82% on VG. Code is
available at https://github.com/ZGCTroy/LayoutDiffusion.",None,-1
a5a934c7-95b9-4cdf-bf97-aac8fa7d2d2e,Localized Region Contrast for Enhancing Self-Supervised Learning in Medical Image Segmentation,0.166288,2,"Recent advancements in self-supervised learning have demonstrated that
effective visual representations can be learned from unlabeled images. This has
led to increased interest in applying self-supervised learning to the medical
domain, where unlabeled images are abundant and labeled images are difficult to
obtain. However, most self-supervised learning approaches are modeled as image
level discriminative or generative proxy tasks, which may not capture the finer
level representations necessary for dense prediction tasks like multi-organ
segmentation. In this paper, we propose a novel contrastive learning framework
that integrates Localized Region Contrast (LRC) to enhance existing
self-supervised pre-training methods for medical image segmentation. Our
approach involves identifying Super-pixels by Felzenszwalb's algorithm and
performing local contrastive learning using a novel contrastive sampling loss.
Through extensive experiments on three multi-organ segmentation datasets, we
demonstrate that integrating LRC to an existing self-supervised method in a
limited annotation setting significantly improves segmentation performance.
Moreover, we show that LRC can also be applied to fully-supervised pre-training
methods to further boost performance.",None,-1
988bc631-5b4d-42c4-ae0c-e5f8795e476c,Automated Action Model Acquisition from Narrative Texts,0.596158,2,"Action models, which take the form of precondition/effect axioms, facilitate
causal and motivational connections between actions for AI agents. Action model
acquisition has been identified as a bottleneck in the application of planning
technology, especially within narrative planning. Acquiring action models from
narrative texts in an automated way is essential, but challenging because of
the inherent complexities of such texts. We present NaRuto, a system that
extracts structured events from narrative text and subsequently generates
planning-language-style action models based on predictions of commonsense event
relations, as well as textual contradictions and similarities, in an
unsupervised manner. Experimental results in classical narrative planning
domains show that NaRuto can generate action models of significantly better
quality than existing fully automated methods, and even on par with those of
semi-automated methods.",None,-1
43642c09-dddc-492d-8e31-1a7aa7a8faf4,Open-TI: Open Traffic Intelligence with Augmented Language Model,0.791498,6,"Transportation has greatly benefited the cities' development in the modern
civilization process. Intelligent transportation, leveraging advanced computer
algorithms, could further increase people's daily commuting efficiency.
However, intelligent transportation, as a cross-discipline, often requires
practitioners to comprehend complicated algorithms and obscure neural networks,
bringing a challenge for the advanced techniques to be trusted and deployed in
practical industries. Recognizing the expressiveness of the pre-trained large
language models, especially the potential of being augmented with abilities to
understand and execute intricate commands, we introduce Open-TI. Serving as a
bridge to mitigate the industry-academic gap, Open-TI is an innovative model
targeting the goal of Turing Indistinguishable Traffic Intelligence, it is
augmented with the capability to harness external traffic analysis packages
based on existing conversations. Marking its distinction, Open-TI is the first
method capable of conducting exhaustive traffic analysis from scratch -
spanning from map data acquisition to the eventual execution in complex
simulations. Besides, Open-TI is able to conduct task-specific embodiment like
training and adapting the traffic signal control policies (TSC), explore demand
optimizations, etc. Furthermore, we explored the viability of LLMs directly
serving as control agents, by understanding the expected intentions from
Open-TI, we designed an agent-to-agent communication mode to support Open-TI
conveying messages to ChatZero (control agent), and then the control agent
would choose from the action space to proceed the execution. We eventually
provide the formal implementation structure, and the open-ended design invites
further community-driven enhancements.",None,-1
757cf691-43db-4921-8fac-335899298882,Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection,0.999987,80,"In this paper, we propose a long-sequence modeling framework, named
StreamPETR, for multi-view 3D object detection. Built upon the sparse query
design in the PETR series, we systematically develop an object-centric temporal
mechanism. The model is performed in an online manner and the long-term
historical information is propagated through object queries frame by frame.
Besides, we introduce a motion-aware layer normalization to model the movement
of the objects. StreamPETR achieves significant performance improvements only
with negligible computation cost, compared to the single-frame baseline. On the
standard nuScenes benchmark, it is the first online multi-view method that
achieves comparable performance (67.6% NDS & 65.3% AMOTA) with lidar-based
methods. The lightweight version realizes 45.0% mAP and 31.7 FPS, outperforming
the state-of-the-art method (SOLOFusion) by 2.3% mAP and 1.8x faster FPS. Code
has been available at https://github.com/exiawsh/StreamPETR.git.",None,-1
19b9c7c5-2dff-499d-97b9-865f257b5056,Online Map Vectorization for Autonomous Driving: A Rasterization Perspective,0.588363,13,"Vectorized high-definition (HD) map is essential for autonomous driving,
providing detailed and precise environmental information for advanced
perception and planning. However, current map vectorization methods often
exhibit deviations, and the existing evaluation metric for map vectorization
lacks sufficient sensitivity to detect these deviations. To address these
limitations, we propose integrating the philosophy of rasterization into map
vectorization. Specifically, we introduce a new rasterization-based evaluation
metric, which has superior sensitivity and is better suited to real-world
autonomous driving scenarios. Furthermore, we propose MapVR (Map Vectorization
via Rasterization), a novel framework that applies differentiable rasterization
to vectorized outputs and then performs precise and geometry-aware supervision
on rasterized HD maps. Notably, MapVR designs tailored rasterization strategies
for various geometric shapes, enabling effective adaptation to a wide range of
map elements. Experiments show that incorporating rasterization into map
vectorization greatly enhances performance with no extra computational cost
during inference, leading to more accurate map perception and ultimately
promoting safer autonomous driving.",None,-1
b0c623db-e04e-4946-8a20-842fea707bf2,A Comparison of Speech Data Augmentation Methods Using S3PRL Toolkit,0.301043,2,"Data augmentations are known to improve robustness in speech-processing
tasks. In this study, we summarize and compare different data augmentation
strategies using S3PRL toolkit. We explore how HuBERT and wav2vec perform using
different augmentation techniques (SpecAugment, Gaussian Noise, Speed
Perturbation) for Phoneme Recognition (PR) and Automatic Speech Recognition
(ASR) tasks. We evaluate model performance in terms of phoneme error rate (PER)
and word error rate (WER). From the experiments, we observed that SpecAugment
slightly improves the performance of HuBERT and wav2vec on the original
dataset. Also, we show that models trained using the Gaussian Noise and Speed
Perturbation dataset are more robust when tested with augmented test sets.",None,-1
0b34e9f0-e956-4c52-ace5-25cbbe02fb12,Entity-Based Evaluation of Political Bias in Automatic Summarization,0.096465,1,"Growing literature has shown that NLP systems may encode social biases;
however, the political bias of summarization models remains relatively unknown.
In this work, we use an entity replacement method to investigate the portrayal
of politicians in automatically generated summaries of news articles. We
develop an entity-based computational framework to assess the sensitivities of
several extractive and abstractive summarizers to the politicians Donald Trump
and Joe Biden. We find consistent differences in these summaries upon entity
replacement, such as reduced emphasis of Trump's presence in the context of the
same article and a more individualistic representation of Trump with respect to
the collective US government (i.e., administration). These summary
dissimilarities are most prominent when the entity is heavily featured in the
source article. Our characterization provides a foundation for future studies
of bias in summarization and for normative discussions on the ideal qualities
of automatic summaries.",None,-1
586d95d3-7920-42b2-9d43-4e9c346e72c6,YOGA: Deep Object Detection in the Wild with Lightweight Feature Learning and Multiscale Attention,0.0984836,3,"We introduce YOGA, a deep learning based yet lightweight object detection
model that can operate on low-end edge devices while still achieving
competitive accuracy. The YOGA architecture consists of a two-phase feature
learning pipeline with a cheap linear transformation, which learns feature maps
using only half of the convolution filters required by conventional
convolutional neural networks. In addition, it performs multi-scale feature
fusion in its neck using an attention mechanism instead of the naive
concatenation used by conventional detectors. YOGA is a flexible model that can
be easily scaled up or down by several orders of magnitude to fit a broad range
of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets
with other over 10 state-of-the-art object detectors. The results show that
YOGA strikes the best trade-off between model size and accuracy (up to 22%
increase of AP and 23-34% reduction of parameters and FLOPs), making it an
ideal choice for deployment in the wild on low-end edge devices. This is
further affirmed by our hardware implementation and evaluation on NVIDIA Jetson
Nano.",None,-1
cef3e81c-ad83-4504-85ff-e4940bd13abe,ModelScope Text-to-Video Technical Report,0.996967,154,"This paper introduces ModelScopeT2V, a text-to-video synthesis model that
evolves from a text-to-image synthesis model (i.e., Stable Diffusion).
ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame
generation and smooth movement transitions. The model could adapt to varying
frame numbers during training and inference, rendering it suitable for both
image-text and video-text datasets. ModelScopeT2V brings together three
components (i.e., VQGAN, a text encoder, and a denoising UNet), totally
comprising 1.7 billion parameters, in which 0.5 billion parameters are
dedicated to temporal capabilities. The model demonstrates superior performance
over state-of-the-art methods across three evaluation metrics. The code and an
online demo are available at
\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.",None,-1
27c2d2ad-b15f-4e55-910e-134412b005b6,Longformer: Longitudinal Transformer for Alzheimer's Disease Classification with Structural MRIs,0.383848,3,"Structural magnetic resonance imaging (sMRI) is widely used for brain
neurological disease diagnosis; while longitudinal MRIs are often collected to
monitor and capture disease progression, as clinically used in diagnosing
Alzheimer's disease (AD). However, most current methods neglect AD's
progressive nature and only take a single sMRI for recognizing AD. In this
paper, we consider the problem of leveraging the longitudinal MRIs of a subject
for AD identification. To capture longitudinal changes in sMRIs, we propose a
novel model Longformer, a spatiotemporal transformer network that performs
attention mechanisms spatially on sMRIs at each time point and integrates brain
region features over time to obtain longitudinal embeddings for classification.
Our Longformer achieves state-of-the-art performance on two binary
classification tasks of separating different stages of AD using the ADNI
dataset. Our source code is available at https://github.com/Qybc/LongFormer.",None,-1
5ff73c35-7f94-41a8-be12-74e6000157be,Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data,0.76178,40,"Large language models (LLMs) provide a new way to build chatbots by accepting
natural language prompts. Yet, it is unclear how to design prompts to power
chatbots to carry on naturalistic conversations while pursuing a given goal,
such as collecting self-report data from users. We explore what design factors
of prompts can help steer chatbots to talk naturally and collect data reliably.
To this aim, we formulated four prompt designs with different structures and
personas. Through an online study (N = 48) where participants conversed with
chatbots driven by different designs of prompts, we assessed how prompt designs
and conversation topics affected the conversation flows and users' perceptions
of chatbots. Our chatbots covered 79% of the desired information slots during
conversations, and the designs of prompts and topics significantly influenced
the conversation flows and the data collection performance. We discuss the
opportunities and challenges of building chatbots with LLMs.",None,-1
3c2d17f5-fc30-45a6-b8f3-d0ce725312b8,`It is currently hodgepodge'': Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values,0.801903,21,"Recently, the AI/ML research community has indicated an urgent need to
establish Responsible AI (RAI) values and practices as part of the AI/ML
lifecycle. Several organizations and communities are responding to this call by
sharing RAI guidelines. However, there are gaps in awareness, deliberation, and
execution of such practices for multi-disciplinary ML practitioners. This work
contributes to the discussion by unpacking co-production challenges faced by
practitioners as they align their RAI values. We interviewed 23 individuals,
across 10 organizations, tasked to ship AI/ML based products while upholding
RAI norms and found that both top-down and bottom-up institutional structures
create burden for different roles preventing them from upholding RAI values, a
challenge that is further exacerbated when executing conflicted values. We
share multiple value levers used as strategies by the practitioners to resolve
their challenges. We end our paper with recommendations for inclusive and
equitable RAI value-practices, creating supportive organizational structures
and opportunities to further aid practitioners.",None,-1
6af29915-cc0b-49ff-9ed9-02a42477f0c3,Variational Bayes Made Easy,0.0626198,1,"Variational Bayes is a popular method for approximate inference but its
derivation can be cumbersome. To simplify the process, we give a 3-step recipe
to identify the posterior form by explicitly looking for linearity with respect
to expectations of well-known distributions. We can then directly write the
update by simply ``reading-off'' the terms in front of those expectations. The
recipe makes the derivation easier, faster, shorter, and more general.",None,-1
26106c7d-143e-443e-9c5e-e3394a4ebe00,PrivateLoRA For Efficient Privacy Preserving LLM,0.0603557,4,"End users face a choice between privacy and efficiency in current Large
Language Model (LLM) service paradigms. In cloud-based paradigms, users are
forced to compromise data locality for generation quality and processing speed.
Conversely, edge device paradigms maintain data locality but fail to deliver
satisfactory performance. In this work, we propose a novel LLM service paradigm
that distributes privacy-sensitive computation on edge devices and shared
computation in the cloud. Only activations are transmitted between the central
cloud and edge devices to ensure data locality. Our core innovation,
PrivateLoRA, addresses the challenging communication overhead by exploiting the
low rank of residual activations, achieving over 95% communication reduction.
Consequently, PrivateLoRA effectively maintains data locality and is extremely
resource efficient. Under standard 5G networks, PrivateLoRA achieves throughput
over 300% of device-only solutions for 7B models and over 80% of an A100 GPU
for 33B models. PrivateLoRA also provides tuning performance comparable to LoRA
for advanced personalization. Our approach democratizes access to
state-of-the-art generative AI for edge devices, paving the way for more
tailored LLM experiences for the general public. To our knowledge, our proposed
framework is the first efficient and privacy-preserving LLM solution in the
literature.",None,-1
20710eae-3940-4fb3-99c6-a2ede4a0b381,RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems,0.999399,50,"Large Language Models (LLMs) have greatly advanced code auto-completion
systems, with a potential for substantial productivity enhancements for
developers. However, current benchmarks mainly focus on single-file tasks,
leaving an assessment gap for more complex, real-world, multi-file programming
scenarios. To fill this gap, we introduce RepoBench, a new benchmark
specifically designed for evaluating repository-level code auto-completion
systems. RepoBench supports both Python and Java and consists of three
interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code
Completion), and RepoBench-P (Pipeline). Each task respectively measures the
system's ability to retrieve the most relevant code snippets from other files
as cross-file context, predict the next line of code with cross-file and
in-file context, and handle complex tasks that require a combination of both
retrieval and next-line prediction. RepoBench aims to facilitate a more
complete comparison of performance and encouraging continuous improvement in
auto-completion systems. RepoBench is publicly available at
https://github.com/Leolty/repobench.",None,-1
1d35f337-77b8-42b4-ab6f-849ba70a973f,Switch-BERT: Learning to Model Multimodal Interactions by Switching Attention and Input,0.0649115,1,"The ability to model intra-modal and inter-modal interactions is fundamental
in multimodal machine learning. The current state-of-the-art models usually
adopt deep learning models with fixed structures. They can achieve exceptional
performances on specific tasks, but face a particularly challenging problem of
modality mismatch because of diversity of input modalities and their fixed
structures. In this paper, we present \textbf{Switch-BERT} for joint vision and
language representation learning to address this problem. Switch-BERT extends
BERT architecture by introducing learnable layer-wise and cross-layer
interactions. It learns to optimize attention from a set of attention modes
representing these interactions. One specific property of the model is that it
learns to attend outputs from various depths, therefore mitigates the modality
mismatch problem. We present extensive experiments on visual question
answering, image-text retrieval and referring expression comprehension
experiments. Results confirm that, whereas alternative architectures including
ViLBERT and UNITER may excel in particular tasks, Switch-BERT can consistently
achieve better or comparable performances than the current state-of-the-art
models in these tasks. Ablation studies indicate that the proposed model
achieves superior performances due to its ability in learning task-specific
multimodal interactions.",None,-1
01102b36-16f9-48e0-b02e-1ed8ca698c2e,Characterizing Structural Hardness of Logic Programs: What makes Cycles and Reachability Hard for Treewidth?,0.0928021,2,"Answer Set Programming (ASP) is a problem modeling and solving framework for
several problems in KR with growing industrial applications. Also for studies
of computational complexity and deeper insights into the hardness and its
sources, ASP has been attracting researchers for many years. These studies
resulted in fruitful characterizations in terms of complexity classes,
fine-grained insights in form of dichotomy-style results, as well as detailed
parameterized complexity landscapes. Recently, this lead to a novel result
establishing that for the measure treewidth, which captures structural density
of a program, the evaluation of the well-known class of normal programs is
expected to be slightly harder than deciding satisfiability (SAT). However, it
is unclear how to utilize this structural power of ASP. This paper deals with a
novel reduction from SAT to normal ASP that goes beyond well-known encodings:
We explicitly utilize the structural power of ASP, whereby we sublinearly
decrease the treewidth, which probably cannot be significantly improved. Then,
compared to existing results, this characterizes hardness in a fine-grained way
by establishing the required functional dependency of the dependency graph's
cycle length (SCC size) on the treewidth.",None,-1
b4de9704-ae8f-4483-8bae-81f6dcd813a8,MITFAS: Mutual Information based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition,0.521802,3,"We present a novel approach for action recognition in UAV videos. Our
formulation is designed to handle occlusion and viewpoint changes caused by the
movement of a UAV. We use the concept of mutual information to compute and
align the regions corresponding to human action or motion in the temporal
domain. This enables our recognition model to learn from the key features
associated with the motion. We also propose a novel frame sampling method that
uses joint mutual information to acquire the most informative frame sequence in
UAV videos. We have integrated our approach with X3D and evaluated the
performance on multiple datasets. In practice, we achieve 18.9% improvement in
Top-1 accuracy over current state-of-the-art methods on UAV-Human(Li et al.,
2021), 7.3% improvement on Drone-Action(Perera et al., 2019), and 7.16%
improvement on NEC Drones(Choi et al., 2020).",None,-1
af25cc0b-8592-4fe5-97f6-ea218a39a7b9,Frustratingly Simple but Effective Zero-shot Detection and Segmentation: Analysis and a Strong Baseline,0.272519,5,"Methods for object detection and segmentation often require abundant
instance-level annotations for training, which are time-consuming and expensive
to collect. To address this, the task of zero-shot object detection (or
segmentation) aims at learning effective methods for identifying and localizing
object instances for the categories that have no supervision available.
Constructing architectures for these tasks requires choosing from a myriad of
design options, ranging from the form of the class encoding used to transfer
information from seen to unseen categories, to the nature of the function being
optimized for learning. In this work, we extensively study these design
choices, and carefully construct a simple yet extremely effective zero-shot
recognition method. Through extensive experiments on the MSCOCO dataset on
object detection and segmentation, we highlight that our proposed method
outperforms existing, considerably more complex, architectures. Our findings
and method, which we propose as a competitive future baseline, point towards
the need to revisit some of the recent design trends in zero-shot detection /
segmentation.",None,-1
64d191f1-7481-459e-8468-065342716ff9,Leveraging Summary Guidance on Medical Report Summarization,0.204942,3,"This study presents three deidentified large medical text datasets, named
DISCHARGE, ECHO and RADIOLOGY, which contain 50K, 16K and 378K pairs of report
and summary that are derived from MIMIC-III, respectively. We implement
convincing baselines of automated abstractive summarization on the proposed
datasets with pre-trained encoder-decoder language models, including BERT2BERT,
T5-large and BART. Further, based on the BART model, we leverage the sampled
summaries from the train set as prior knowledge guidance, for encoding
additional contextual representations of the guidance with the encoder and
enhancing the decoding representations in the decoder. The experimental results
confirm the improvement of ROUGE scores and BERTScore made by the proposed
method, outperforming the larger model T5-large.",None,-1
474ff3bd-81c3-407c-a20c-48d672c516bd,Affordance Grounding from Demonstration Video to Target Image,0.574253,12,"Humans excel at learning from expert demonstrations and solving their own
problems. To equip intelligent robots and assistants, such as AR glasses, with
this ability, it is essential to ground human hand interactions (i.e.,
affordances) from demonstration videos and apply them to a target image like a
user's AR glass view. The video-to-image affordance grounding task is
challenging due to (1) the need to predict fine-grained affordances, and (2)
the limited training data, which inadequately covers video-image discrepancies
and negatively impacts grounding. To tackle them, we propose Affordance
Transformer (Afformer), which has a fine-grained transformer-based decoder that
gradually refines affordance grounding. Moreover, we introduce Mask Affordance
Hand (MaskAHand), a self-supervised pre-training technique for synthesizing
video-image data and simulating context changes, enhancing affordance grounding
across video-image discrepancies. Afformer with MaskAHand pre-training achieves
state-of-the-art performance on multiple benchmarks, including a substantial
37% improvement on the OPRA dataset. Code is made available at
https://github.com/showlab/afformer.",None,-1
7aaa21a1-5cfa-47dd-b28f-3f4ccf5bb45a,Improving Long Context Document-Level Machine Translation,0.721162,5,"Document-level context for neural machine translation (NMT) is crucial to
improve the translation consistency and cohesion, the translation of ambiguous
inputs, as well as several other linguistic phenomena. Many works have been
published on the topic of document-level NMT, but most restrict the system to
only local context, typically including just the one or two preceding sentences
as additional information. This might be enough to resolve some ambiguous
inputs, but it is probably not sufficient to capture some document-level
information like the topic or style of a conversation. When increasing the
context size beyond just the local context, there are two challenges: (i)
the~memory usage increases exponentially (ii) the translation performance
starts to degrade. We argue that the widely-used attention mechanism is
responsible for both issues. Therefore, we propose a constrained attention
variant that focuses the attention on the most relevant parts of the sequence,
while simultaneously reducing the memory consumption. For evaluation, we
utilize targeted test sets in combination with novel evaluation techniques to
analyze the translations in regards to specific discourse-related phenomena. We
find that our approach is a good compromise between sentence-level NMT vs
attending to the full context, especially in low resource scenarios.",None,-1
bf20226d-ab2d-4041-8d38-b66d06860182,ChOiRe: Characterizing and Predicting Human Opinions with Chain of Opinion Reasoning,0.0580586,1,"Aligning language models (LMs) with human opinion is challenging yet vital to
enhance their grasp of human values, preferences, and beliefs. We present
ChOiRe, a four-step framework to predict human opinion which differentially
models the user explicit personae (i.e. demographic or ideological attributes)
that are manually declared, and implicit personae inferred from user historical
opinions. ChOiRe consists of (i) an LM analyzing the user explicit personae to
filter out irrelevant attributes; (ii) the LM ranking the implicit persona
opinions into a preferential list; (iii) Chain-of-Opinion (CoO) reasoning,
where the LM sequentially analyzes the explicit personae and the most relevant
implicit personae to perform opinion prediction; (iv) and where ChOiRe executes
Step (iii) CoO multiple times with increasingly larger lists of implicit
personae to overcome insufficient personae information to infer a final result.
ChOiRe achieves new state-of-the-art effectiveness with limited inference
calls, improving previous techniques significantly by 3.22%. We also show that
ChOiRe Steps (i) and (ii) can significantly better fine-tune opinion-aligned
models, by up to 18.44%.",None,-1
e58f457f-72e2-436a-a639-64d178cb0e59,Beyond Grids: Exploring Elastic Input Sampling for Vision Transformers,0.069257,1,"Vision transformers have excelled in various computer vision tasks but mostly
rely on rigid input sampling using a fixed-size grid of patches. This limits
their applicability in real-world problems, such as in the field of robotics
and UAVs, where one can utilize higher input elasticity to boost model
performance and efficiency. Our paper addresses this limitation by formalizing
the concept of input elasticity for vision transformers and introducing an
evaluation protocol, including dedicated metrics for measuring input
elasticity. Moreover, we propose modifications to the transformer architecture
and training regime, which increase its elasticity. Through extensive
experimentation, we spotlight opportunities and challenges associated with
input sampling strategies.",None,-1
2fdd73d2-c377-42a7-b2a7-0d581626c37f,XPert: Peripheral Circuit & Neural Architecture Co-search for Area and Energy-efficient Xbar-based Computing,0.17714,2,"The hardware-efficiency and accuracy of Deep Neural Networks (DNNs)
implemented on In-memory Computing (IMC) architectures primarily depend on the
DNN architecture and the peripheral circuit parameters. It is therefore
essential to holistically co-search the network and peripheral parameters to
achieve optimal performance. To this end, we propose XPert, which co-searches
network architecture in tandem with peripheral parameters such as the type and
precision of analog-to-digital converters, crossbar column sharing and the
layer-specific input precision using an optimization-based design space
exploration. Compared to VGG16 baselines, XPert achieves 10.24x (4.7x) lower
EDAP, 1.72x (1.62x) higher TOPS/W,1.93x (3x) higher TOPS/mm2 at 92.46% (56.7%)
accuracy for CIFAR10 (TinyImagenet) datasets. The code for this paper is
available at https://github.com/Intelligent-Computing-Lab-Yale/XPert.",None,-1
87159d43-ee16-45d5-92fa-a849a259ee65,M2C: Towards Automatic Multimodal Manga Complement,0.461898,3,"Multimodal manga analysis focuses on enhancing manga understanding with
visual and textual features, which has attracted considerable attention from
both natural language processing and computer vision communities. Currently,
most comics are hand-drawn and prone to problems such as missing pages, text
contamination, and aging, resulting in missing comic text content and seriously
hindering human comprehension. In other words, the Multimodal Manga Complement
(M2C) task has not been investigated, which aims to handle the aforementioned
issues by providing a shared semantic space for vision and language
understanding. To this end, we first propose the Multimodal Manga Complement
task by establishing a new M2C benchmark dataset covering two languages. First,
we design a manga argumentation method called MCoT to mine event knowledge in
comics with large language models. Then, an effective baseline FVP-M$^{2}$
using fine-grained visual prompts is proposed to support manga complement.
Extensive experimental results show the effectiveness of FVP-M$^{2}$ method for
Multimodal Mange Complement.",None,-1
16ac2cce-ef0b-44d5-b1d6-ff7ecc63e3a2,Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification,0.45858,6,"Authorship verification (AV) is a fundamental task in natural language
processing (NLP) and computational linguistics, with applications in forensic
analysis, plagiarism detection, and identification of deceptive content.
Existing AV techniques, including traditional stylometric and deep learning
approaches, face limitations in terms of data requirements and lack of
explainability. To address these limitations, this paper proposes PromptAV, a
novel technique that leverages Large-Language Models (LLMs) for AV by providing
step-by-step stylometric explanation prompts. PromptAV outperforms
state-of-the-art baselines, operates effectively with limited training data,
and enhances interpretability through intuitive explanations, showcasing its
potential as an effective and interpretable solution for the AV task.",None,-1
730c477e-bd88-48cb-a18b-c331a82ecb13,Detecting Stance of Authorities towards Rumors in Arabic Tweets: A Preliminary Study,0.30504,4,"A myriad of studies addressed the problem of rumor verification in Twitter by
either utilizing evidence from the propagation networks or external evidence
from the Web. However, none of these studies exploited evidence from trusted
authorities. In this paper, we define the task of detecting the stance of
authorities towards rumors in tweets, i.e., whether a tweet from an authority
agrees, disagrees, or is unrelated to the rumor. We believe the task is useful
to augment the sources of evidence utilized by existing rumor verification
systems. We construct and release the first Authority STance towards Rumors
(AuSTR) dataset, where evidence is retrieved from authority timelines in Arabic
Twitter. Due to the relatively limited size of our dataset, we study the
usefulness of existing datasets for stance detection in our task. We show that
existing datasets are somewhat useful for the task; however, they are clearly
insufficient, which motivates the need to augment them with annotated data
constituting stance of authorities from Twitter.",None,-1
8ff35999-ede5-4875-85b3-aaed8de7652c,Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models,0.289356,11,"Generating intermediate steps, or Chain of Thought (CoT), is an effective way
to significantly improve language models' (LM) multi-step reasoning capability.
However, the CoT lengths can grow rapidly with the problem complexity, easily
exceeding the maximum context size. Instead of increasing the context limit,
which has already been heavily investigated, we explore an orthogonal
direction: making LMs divide a problem into multiple contexts. We propose a new
inference framework, called Recursion of Thought (RoT), which introduces
several special tokens that the models can output to trigger context-related
operations. Extensive experiments with multiple architectures including GPT-3
show that RoT dramatically improves LMs' inference capability to solve
problems, whose solution consists of hundreds of thousands of tokens.",None,-1
a1aae083-0e4f-40a7-a727-cc9cc210580b,OPE-SR: Orthogonal Position Encoding for Designing a Parameter-free Upsampling Module in Arbitrary-scale Image Super-Resolution,0.624222,7,"Implicit neural representation (INR) is a popular approach for
arbitrary-scale image super-resolution (SR), as a key component of INR,
position encoding improves its representation ability. Motivated by position
encoding, we propose orthogonal position encoding (OPE) - an extension of
position encoding - and an OPE-Upscale module to replace the INR-based
upsampling module for arbitrary-scale image super-resolution. Same as INR, our
OPE-Upscale Module takes 2D coordinates and latent code as inputs; however it
does not require training parameters. This parameter-free feature allows the
OPE-Upscale Module to directly perform linear combination operations to
reconstruct an image in a continuous manner, achieving an arbitrary-scale image
reconstruction. As a concise SR framework, our method has high computing
efficiency and consumes less memory comparing to the state-of-the-art (SOTA),
which has been confirmed by extensive experiments and evaluations. In addition,
our method has comparable results with SOTA in arbitrary scale image
super-resolution. Last but not the least, we show that OPE corresponds to a set
of orthogonal basis, justifying our design principle.",None,-1
0ce77957-9909-469a-b44e-f1291dbf0cf6,UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation,0.0968665,1,"Multiple Object Tracking (MOT) aims to find bounding boxes and identities of
targeted objects in consecutive video frames. While fully-supervised MOT
methods have achieved high accuracy on existing datasets, they cannot
generalize well on a newly obtained dataset or a new unseen domain. In this
work, we first address the MOT problem from the cross-domain point of view,
imitating the process of new data acquisition in practice. Then, a new
cross-domain MOT adaptation from existing datasets is proposed without any
pre-defined human knowledge in understanding and modeling objects. It can also
learn and update itself from the target data feedback. The intensive
experiments are designed on four challenging settings, including MOTSynth to
MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then
prove the adaptability of the proposed self-supervised learning strategy. The
experiments also show superior performance on tracking metrics MOTA and IDF1,
compared to fully supervised, unsupervised, and self-supervised
state-of-the-art methods.",None,-1
4ece9c20-d878-4b1d-9ea2-89a7c83414cd,Artificial Intelligence for Drug Discovery: Are We There Yet?,0.908088,20,"Drug discovery is adapting to novel technologies such as data science,
informatics, and artificial intelligence (AI) to accelerate effective treatment
development while reducing costs and animal experiments. AI is transforming
drug discovery, as indicated by increasing interest from investors, industrial
and academic scientists, and legislators. Successful drug discovery requires
optimizing properties related to pharmacodynamics, pharmacokinetics, and
clinical outcomes. This review discusses the use of AI in the three pillars of
drug discovery: diseases, targets, and therapeutic modalities, with a focus on
small molecule drugs. AI technologies, such as generative chemistry, machine
learning, and multi-property optimization, have enabled several compounds to
enter clinical trials. The scientific community must carefully vet known
information to address the reproducibility crisis. The full potential of AI in
drug discovery can only be realized with sufficient ground truth and
appropriate human intervention at later pipeline stages.",None,-1
248dbd96-bb35-432a-99ee-a055b3f6957f,Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems,0.418351,5,"The paper surveys automated scientific discovery, from equation discovery and
symbolic regression to autonomous discovery systems and agents. It discusses
the individual approaches from a ""big picture"" perspective and in context, but
also discusses open issues and recent topics like the various roles of deep
neural networks in this area, aiding in the discovery of human-interpretable
knowledge. Further, we will present closed-loop scientific discovery systems,
starting with the pioneering work on the Adam system up to current efforts in
fields from material science to astronomy. Finally, we will elaborate on
autonomy from a machine learning perspective, but also in analogy to the
autonomy levels in autonomous driving. The maximal level, level five, is
defined to require no human intervention at all in the production of scientific
knowledge. Achieving this is one step towards solving the Nobel Turing Grand
Challenge to develop AI Scientists: AI systems capable of making Nobel-quality
scientific discoveries highly autonomously at a level comparable, and possibly
superior, to the best human scientists by 2050.",None,-1
4c9e43c7-10f9-4d10-9b8a-98f12a373707,A Federated Approach for Hate Speech Detection,0.164071,1,"Hate speech detection has been the subject of high research attention, due to
the scale of content created on social media. In spite of the attention and the
sensitive nature of the task, privacy preservation in hate speech detection has
remained under-studied. The majority of research has focused on centralised
machine learning infrastructures which risk leaking data. In this paper, we
show that using federated machine learning can help address privacy the
concerns that are inherent to hate speech detection while obtaining up to 6.81%
improvement in terms of F1-score.",None,-1
fe24820b-f211-47e9-818e-9d895f572ad0,Summarizing Stream Data for Memory-Constrained Online Continual Learning,0.619467,5,"Replay-based methods have proved their effectiveness on online continual
learning by rehearsing past samples from an auxiliary memory. With many efforts
made on improving training schemes based on the memory, however, the
information carried by each sample in the memory remains under-investigated.
Under circumstances with restricted storage space, the informativeness of the
memory becomes critical for effective replay. Although some works design
specific strategies to select representative samples, by only employing a small
number of original images, the storage space is still not well utilized. To
this end, we propose to Summarize the knowledge from the Stream Data (SSD) into
more informative samples by distilling the training characteristics of real
images. Through maintaining the consistency of training gradients and
relationship to the past tasks, the summarized samples are more representative
for the stream data compared to the original images. Extensive experiments are
conducted on multiple online continual learning benchmarks to support that the
proposed SSD method significantly enhances the replay effects. We demonstrate
that with limited extra computational overhead, SSD provides more than 3%
accuracy boost for sequential CIFAR-100 under extremely restricted memory
buffer. Code in https://github.com/vimar-gu/SSD.",None,-1
2fe2dcf7-20c4-46be-a892-9f18d9baeff7,Approximate Thompson Sampling via Epistemic Neural Networks,0.201065,11,"Thompson sampling (TS) is a popular heuristic for action selection, but it
requires sampling from a posterior distribution. Unfortunately, this can become
computationally intractable in complex environments, such as those modeled
using neural networks. Approximate posterior samples can produce effective
actions, but only if they reasonably approximate joint predictive distributions
of outputs across inputs. Notably, accuracy of marginal predictive
distributions does not suffice. Epistemic neural networks (ENNs) are designed
to produce accurate joint predictive distributions. We compare a range of ENNs
through computational experiments that assess their performance in
approximating TS across bandit and reinforcement learning environments. The
results indicate that ENNs serve this purpose well and illustrate how the
quality of joint predictive distributions drives performance. Further, we
demonstrate that the \textit{epinet} -- a small additive network that estimates
uncertainty -- matches the performance of large ensembles at orders of
magnitude lower computational cost. This enables effective application of TS
with computation that scales gracefully to complex environments.",None,-1
18ed9c6e-ca3f-49a6-a64a-1ac3b4f339bd,Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps,0.0582988,1,"Precise localization is critical for autonomous vehicles. We present a
self-supervised learning method that employs Transformers for the first time
for the task of outdoor localization using LiDAR data. We propose a pre-text
task that reorganizes the slices of a $360^\circ$ LiDAR scan to leverage its
axial properties. Our model, called Slice Transformer, employs multi-head
attention while systematically processing the slices. To the best of our
knowledge, this is the first instance of leveraging multi-head attention for
outdoor point clouds. We additionally introduce the Perth-WA dataset, which
provides a large-scale LiDAR map of Perth city in Western Australia, covering
$\sim$4km$^2$ area. Localization annotations are provided for Perth-WA. The
proposed localization method is thoroughly evaluated on Perth-WA and
Appollo-SouthBay datasets. We also establish the efficacy of our
self-supervised learning approach for the common downstream task of object
classification using ModelNet40 and ScanNN datasets. The code and Perth-WA data
will be publicly released.",None,-1
a2fd89e8-cb28-43e4-8552-171e45675a44,Generating a Structured Summary of Numerous Academic Papers: Dataset and Method,0.404529,6,"Writing a survey paper on one research topic usually needs to cover the
salient content from numerous related papers, which can be modeled as a
multi-document summarization (MDS) task. Existing MDS datasets usually focus on
producing the structureless summary covering a few input documents. Meanwhile,
previous structured summary generation works focus on summarizing a single
document into a multi-section summary. These existing datasets and methods
cannot meet the requirements of summarizing numerous academic papers into a
structured summary. To deal with the scarcity of available data, we propose
BigSurvey, the first large-scale dataset for generating comprehensive summaries
of numerous academic papers on each topic. We collect target summaries from
more than seven thousand survey papers and utilize their 430 thousand reference
papers' abstracts as input documents. To organize the diverse content from
dozens of input documents and ensure the efficiency of processing long text
sequences, we propose a summarization method named category-based alignment and
sparse transformer (CAST). The experimental results show that our CAST method
outperforms various advanced summarization methods.",None,-1
12181ffa-ac0b-463f-937d-e879fc3a0728,Zero-shot Causal Graph Extrapolation from Text via LLMs,0.808854,6,"We evaluate the ability of large language models (LLMs) to infer causal
relations from natural language. Compared to traditional natural language
processing and deep learning techniques, LLMs show competitive performance in a
benchmark of pairwise relations without needing (explicit) training samples.
This motivates us to extend our approach to extrapolating causal graphs through
iterated pairwise queries. We perform a preliminary analysis on a benchmark of
biomedical abstracts with ground-truth causal graphs validated by experts. The
results are promising and support the adoption of LLMs for such a crucial step
in causal inference, especially in medical domains, where the amount of
scientific text to analyse might be huge, and the causal statements are often
implicit.",None,-1
dc92d9c7-d3a7-4f83-9fab-88905e36f3b1,MyStyle++: A Controllable Personalized Generative Prior,0.186803,4,"In this paper, we propose an approach to obtain a personalized generative
prior with explicit control over a set of attributes. We build upon MyStyle, a
recently introduced method, that tunes the weights of a pre-trained StyleGAN
face generator on a few images of an individual. This system allows
synthesizing, editing, and enhancing images of the target individual with high
fidelity to their facial features. However, MyStyle does not demonstrate
precise control over the attributes of the generated images. We propose to
address this problem through a novel optimization system that organizes the
latent space in addition to tuning the generator. Our key contribution is to
formulate a loss that arranges the latent codes, corresponding to the input
images, along a set of specific directions according to their attributes. We
demonstrate that our approach, dubbed MyStyle++, is able to synthesize, edit,
and enhance images of an individual with great control over the attributes,
while preserving the unique facial characteristics of that individual.",None,-1
414b2813-3462-45c8-a969-897c83caac87,IH-ViT: Vision Transformer-based Integrated Circuit Appear-ance Defect Detection,0.324223,1,"For the problems of low recognition rate and slow recognition speed of
traditional detection methods in IC appearance defect detection, we propose an
IC appearance defect detection algo-rithm IH-ViT. Our proposed model takes
advantage of the respective strengths of CNN and ViT to acquire image features
from both local and global aspects, and finally fuses the two features for
decision making to determine the class of defects, thus obtaining better
accuracy of IC defect recognition. To address the problem that IC appearance
defects are mainly reflected in the dif-ferences in details, which are
difficult to identify by traditional algorithms, we improved the tra-ditional
ViT by performing an additional convolution operation inside the batch. For the
problem of information imbalance of samples due to diverse sources of data
sets, we adopt a dual-channel image segmentation technique to further improve
the accuracy of IC appearance defects. Finally, after testing, our proposed
hybrid IH-ViT model achieved 72.51% accuracy, which is 2.8% and 6.06% higher
than ResNet50 and ViT models alone. The proposed algorithm can quickly and
accurately detect the defect status of IC appearance and effectively improve
the productivity of IC packaging and testing companies.",None,-1
8adaa9e3-91d7-4819-932a-e801787e0234,Asymptotic Convergence and Performance of Multi-Agent Q-Learning Dynamics,0.806154,12,"Achieving convergence of multiple learning agents in general $N$-player games
is imperative for the development of safe and reliable machine learning (ML)
algorithms and their application to autonomous systems. Yet it is known that,
outside the bounds of simple two-player games, convergence cannot be taken for
granted.
  To make progress in resolving this problem, we study the dynamics of smooth
Q-Learning, a popular reinforcement learning algorithm which quantifies the
tendency for learning agents to explore their state space or exploit their
payoffs. We show a sufficient condition on the rate of exploration such that
the Q-Learning dynamics is guaranteed to converge to a unique equilibrium in
any game. We connect this result to games for which Q-Learning is known to
converge with arbitrary exploration rates, including weighted Potential games
and weighted zero sum polymatrix games.
  Finally, we examine the performance of the Q-Learning dynamic as measured by
the Time Averaged Social Welfare, and comparing this with the Social Welfare
achieved by the equilibrium. We provide a sufficient condition whereby the
Q-Learning dynamic will outperform the equilibrium even if the dynamics do not
converge.",None,-1
f9d4a823-335d-4ecb-8a03-4eaa93f2d323,Cost-Efficient Prompt Engineering for Unsupervised Entity Resolution,0.819605,3,"Entity Resolution (ER) is the problem of semi-automatically determining when
two entities refer to the same underlying entity, with applications ranging
from healthcare to e-commerce. Traditional ER solutions required considerable
manual expertise, including domain-specific feature engineering, as well as
identification and curation of training data. Recently released large language
models (LLMs) provide an opportunity to make ER more seamless and
domain-independent. However, it is also well known that LLMs can pose risks,
and that the quality of their outputs can depend on how prompts are engineered.
Unfortunately, a systematic experimental study on the effects of different
prompting methods for addressing unsupervised ER, using LLMs like ChatGPT, has
been lacking thus far. This paper aims to address this gap by conducting such a
study. We consider some relatively simple and cost-efficient ER prompt
engineering methods and apply them to ER on two real-world datasets widely used
in the community. We use an extensive set of experimental results to show that
an LLM like GPT3.5 is viable for high-performing unsupervised ER, and
interestingly, that more complicated and detailed (and hence, expensive)
prompting methods do not necessarily outperform simpler approaches. We provide
brief discussions on qualitative and error analysis, including a study of the
inter-consistency of different prompting methods to determine whether they
yield stable outputs. Finally, we consider some limitations of LLMs when
applied to ER.",None,-1
d82b62e8-13cf-4685-a4b2-6f1c10349ef4,Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems,0.412486,1,"Transferring the knowledge of large language models (LLMs) is a promising
technique to incorporate linguistic knowledge into end-to-end automatic speech
recognition (ASR) systems. However, existing works only transfer a single
representation of LLM (e.g. the last layer of pretrained BERT), while the
representation of a text is inherently non-unique and can be obtained variously
from different layers, contexts and models. In this work, we explore a wide
range of techniques to obtain and transfer multiple representations of LLMs
into a transducer-based ASR system. While being conceptually simple, we show
that transferring multiple representations of LLMs can be an effective
alternative to transferring only a single representation.",None,-1
1c9adb41-06b9-46e7-a14a-0c1570011645,RobArch: Designing Robust Architectures against Adversarial Attacks,0.302191,4,"Adversarial Training is the most effective approach for improving the
robustness of Deep Neural Networks (DNNs). However, compared to the large body
of research in optimizing the adversarial training process, there are few
investigations into how architecture components affect robustness, and they
rarely constrain model capacity. Thus, it is unclear where robustness precisely
comes from. In this work, we present the first large-scale systematic study on
the robustness of DNN architecture components under fixed parameter budgets.
Through our investigation, we distill 18 actionable robust network design
guidelines that empower model developers to gain deep insights. We demonstrate
these guidelines' effectiveness by introducing the novel Robust Architecture
(RobArch) model that instantiates the guidelines to build a family of
top-performing models across parameter capacities against strong adversarial
attacks. RobArch achieves the new state-of-the-art AutoAttack accuracy on the
RobustBench ImageNet leaderboard. The code is available at
$\href{https://github.com/ShengYun-Peng/RobArch}{\text{this url}}$.",None,-1
fe72b49f-7493-44b1-89a5-fcfb88d33e0a,Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation,0.946985,13,"News summary generation is an important task in the field of intelligence
analysis, which can provide accurate and comprehensive information to help
people better understand and respond to complex real-world events. However,
traditional news summary generation methods face some challenges, which are
limited by the model itself and the amount of training data, as well as the
influence of text noise, making it difficult to generate reliable information
accurately. In this paper, we propose a new paradigm for news summary
generation using LLM with powerful natural language understanding and
generative capabilities. We use LLM to extract multiple structured event
patterns from the events contained in news paragraphs, evolve the event pattern
population with genetic algorithm, and select the most adaptive event pattern
to input into the LLM to generate news summaries. A News Summary Generator
(NSG) is designed to select and evolve the event pattern populations and
generate news summaries. The experimental results show that the news summary
generator is able to generate accurate and reliable news summaries with some
generalization ability.",None,-1
64dff90f-4246-47eb-b102-907d3b7c7719,A Green(er) World for A.I,0.211709,6,"As research and practice in artificial intelligence (A.I.) grow in leaps and
bounds, the resources necessary to sustain and support their operations also
grow at an increasing pace. While innovations and applications from A.I. have
brought significant advances, from applications to vision and natural language
to improvements to fields like medical imaging and materials engineering, their
costs should not be neglected. As we embrace a world with ever-increasing
amounts of data as well as research and development of A.I. applications, we
are sure to face an ever-mounting energy footprint to sustain these
computational budgets, data storage needs, and more. But, is this sustainable
and, more importantly, what kind of setting is best positioned to nurture such
sustainable A.I. in both research and practice? In this paper, we outline our
outlook for Green A.I. -- a more sustainable, energy-efficient and energy-aware
ecosystem for developing A.I. across the research, computing, and practitioner
communities alike -- and the steps required to arrive there. We present a
bird's eye view of various areas for potential changes and improvements from
the ground floor of AI's operational and hardware optimizations for
datacenters/HPCs to the current incentive structures in the world of A.I.
research and practice, and more. We hope these points will spur further
discussion, and action, on some of these issues and their potential solutions.",None,-1
a3be6093-c511-4caf-9615-00bdd578f91f,mPMR: A Multilingual Pre-trained Machine Reader at Scale,0.665015,4,"We present multilingual Pre-trained Machine Reader (mPMR), a novel method for
multilingual machine reading comprehension (MRC)-style pre-training. mPMR aims
to guide multilingual pre-trained language models (mPLMs) to perform natural
language understanding (NLU) including both sequence classification and span
extraction in multiple languages. To achieve cross-lingual generalization when
only source-language fine-tuning data is available, existing mPLMs solely
transfer NLU capability from a source language to target languages. In
contrast, mPMR allows the direct inheritance of multilingual NLU capability
from the MRC-style pre-training to downstream tasks. Therefore, mPMR acquires
better NLU capability for target languages. mPMR also provides a unified solver
for tackling cross-lingual span extraction and sequence classification, thereby
enabling the extraction of rationales to explain the sentence-pair
classification process.",None,-1
c7e14867-65cd-446c-8f49-de4b925209af,Is Writing Prompts Really Making Art?,0.565379,13,"In recent years Generative Machine Learning systems have advanced
significantly. A current wave of generative systems use text prompts to create
complex imagery, video, even 3D datasets. The creators of these systems claim a
revolution in bringing creativity and art to anyone who can type a prompt. In
this position paper, we question the basis for these claims, dividing our
analysis into three areas: the limitations of linguistic descriptions,
implications of the dataset, and lastly, matters of materiality and embodiment.
We conclude with an analysis of the creative possibilities enabled by
prompt-based systems, asking if they can be considered a new artistic medium.",None,-1
55a14660-8568-4d54-b8a3-8bf42886c950,Deblurring Masked Autoencoder is Better Recipe for Ultrasound Image Recognition,0.234017,2,"Masked autoencoder (MAE) has attracted unprecedented attention and achieves
remarkable performance in many vision tasks. It reconstructs random masked
image patches (known as proxy task) during pretraining and learns meaningful
semantic representations that can be transferred to downstream tasks. However,
MAE has not been thoroughly explored in ultrasound imaging. In this work, we
investigate the potential of MAE for ultrasound image recognition. Motivated by
the unique property of ultrasound imaging in high noise-to-signal ratio, we
propose a novel deblurring MAE approach that incorporates deblurring into the
proxy task during pretraining. The addition of deblurring facilitates the
pretraining to better recover the subtle details presented in the ultrasound
images, thus improving the performance of the downstream classification task.
Our experimental results demonstrate the effectiveness of our deblurring MAE,
achieving state-of-the-art performance in ultrasound image classification.
Overall, our work highlights the potential of MAE for ultrasound image
recognition and presents a novel approach that incorporates deblurring to
further improve its effectiveness.",None,-1
42da42b9-4cd6-4aff-8d53-cd0f0a48776d,Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting,0.103059,8,"Recent work has shown how to prompt large language models with explanations
to obtain strong performance on textual reasoning tasks, i.e., the
chain-of-thought paradigm. However, subtly different explanations can yield
widely varying downstream task accuracy. Explanations that have not been
""tuned"" for a task, such as off-the-shelf explanations written by nonexperts,
may lead to mediocre performance. This paper tackles the problem of how to
optimize explanation-infused prompts in a blackbox fashion. We first generate
sets of candidate explanations for each example in the prompt using a
leave-one-out scheme, then find an effective combination of these explanations
with a two-stage framework. We first evaluate explanations for each in-context
example in isolation according to two proxy metrics, log likelihood and
accuracy on new examples. Then, we search over combinations of explanations to
find one that yields high performance against a silver-labeled development set.
Across four textual reasoning tasks spanning question answering, mathematical
reasoning, and natural language inference, results show that our proxy metrics
correlate with ground truth accuracy and our overall method can effectively
improve prompts over crowdworker annotations and naive search strategies",None,-1
e421a733-eeb5-4b60-b6b3-2275c580fd3a,Decoding Stumpers: Large Language Models vs. Human Problem-Solvers,0.0324808,3,"This paper investigates the problem-solving capabilities of Large Language
Models (LLMs) by evaluating their performance on stumpers, unique single-step
intuition problems that pose challenges for human solvers but are easily
verifiable. We compare the performance of four state-of-the-art LLMs
(Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our
findings reveal that the new-generation LLMs excel in solving stumpers and
surpass human performance. However, humans exhibit superior skills in verifying
solutions to the same problems. This research enhances our understanding of
LLMs' cognitive abilities and provides insights for enhancing their
problem-solving potential across various domains.",None,-1
b9d3238d-182e-4e0c-846f-4b176c029b84,BackTrack: Robust template update via Backward Tracking of candidate template,0.104122,1,"Variations of target appearance such as deformations, illumination variance,
occlusion, etc., are the major challenges of visual object tracking that
negatively impact the performance of a tracker. An effective method to tackle
these challenges is template update, which updates the template to reflect the
change of appearance in the target object during tracking. However, with
template updates, inadequate quality of new templates or inappropriate timing
of updates may induce a model drift problem, which severely degrades the
tracking performance. Here, we propose BackTrack, a robust and reliable method
to quantify the confidence of the candidate template by backward tracking it on
the past frames. Based on the confidence score of candidates from BackTrack, we
can update the template with a reliable candidate at the right time while
rejecting unreliable candidates. BackTrack is a generic template update scheme
and is applicable to any template-based trackers. Extensive experiments on
various tracking benchmarks verify the effectiveness of BackTrack over existing
template update algorithms, as it achieves SOTA performance on various tracking
benchmarks.",None,-1
28ab206a-67ec-47e6-a126-e7033b37818d,How Efficient Are Today's Continual Learning Algorithms?,0.566887,11,"Supervised Continual learning involves updating a deep neural network (DNN)
from an ever-growing stream of labeled data. While most work has focused on
overcoming catastrophic forgetting, one of the major motivations behind
continual learning is being able to efficiently update a network with new
information, rather than retraining from scratch on the training dataset as it
grows over time. Despite recent continual learning methods largely solving the
catastrophic forgetting problem, there has been little attention paid to the
efficiency of these algorithms. Here, we study recent methods for incremental
class learning and illustrate that many are highly inefficient in terms of
compute, memory, and storage. Some methods even require more compute than
training from scratch! We argue that for continual learning to have real-world
applicability, the research community cannot ignore the resources used by these
algorithms. There is more to continual learning than mitigating catastrophic
forgetting.",None,-1
355b1df0-1bcc-44ed-b221-62caaee5f813,FengWu: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead,1.0,88,"We present FengWu, an advanced data-driven global medium-range weather
forecast system based on Artificial Intelligence (AI). Different from existing
data-driven weather forecast methods, FengWu solves the medium-range forecast
problem from a multi-modal and multi-task perspective. Specifically, a deep
learning architecture equipped with model-specific encoder-decoders and
cross-modal fusion Transformer is elaborately designed, which is learned under
the supervision of an uncertainty loss to balance the optimization of different
predictors in a region-adaptive manner. Besides this, a replay buffer mechanism
is introduced to improve medium-range forecast performance. With 39-year data
training based on the ERA5 reanalysis, FengWu is able to accurately reproduce
the atmospheric dynamics and predict the future land and atmosphere states at
37 vertical levels on a 0.25{\deg} latitude-longitude resolution. Hindcasts of
6-hourly weather in 2018 based on ERA5 demonstrate that FengWu performs better
than GraphCast in predicting 80\% of the 880 reported predictands, e.g.,
reducing the root mean square error (RMSE) of 10-day lead global z500
prediction from 733 to 651 $m^{2}/s^2$. In addition, the inference cost of each
iteration is merely 600ms on NVIDIA Tesla A100 hardware. The results suggest
that FengWu can significantly improve the forecast skill and extend the
skillful global medium-range weather forecast out to 10.75 days lead (with ACC
of z500 > 0.6) for the first time.",None,-1
63c0fdc1-0b30-4979-8491-ac642d0a1b2b,Towards Enriched Controllability for Educational Question Generation,0.18935,4,"Question Generation (QG) is a task within Natural Language Processing (NLP)
that involves automatically generating questions given an input, typically
composed of a text and a target answer. Recent work on QG aims to control the
type of generated questions so that they meet educational needs. A remarkable
example of controllability in educational QG is the generation of questions
underlying certain narrative elements, e.g., causal relationship, outcome
resolution, or prediction. This study aims to enrich controllability in QG by
introducing a new guidance attribute: question explicitness. We propose to
control the generation of explicit and implicit wh-questions from
children-friendly stories. We show preliminary evidence of controlling QG via
question explicitness alone and simultaneously with another target attribute:
the question's narrative element. The code is publicly available at
github.com/bernardoleite/question-generation-control.",None,-1
71b9bc2d-f155-4121-a9cd-42698bf4e524,Constraint and Union for Partially-Supervised Temporal Sentence Grounding,0.596228,10,"Temporal sentence grounding aims to detect the event timestamps described by
the natural language query from given untrimmed videos. The existing
fully-supervised setting achieves great performance but requires expensive
annotation costs; while the weakly-supervised setting adopts cheap labels but
performs poorly. To pursue high performance with less annotation cost, this
paper introduces an intermediate partially-supervised setting, i.e., only
short-clip or even single-frame labels are available during training. To take
full advantage of partial labels, we propose a novel quadruple constraint
pipeline to comprehensively shape event-query aligned representations, covering
intra- and inter-samples, uni- and multi-modalities. The former raises
intra-cluster compactness and inter-cluster separability; while the latter
enables event-background separation and event-query gather. To achieve more
powerful performance with explicit grounding optimization, we further introduce
a partial-full union framework, i.e., bridging with an additional
fully-supervised branch, to enjoy its impressive grounding bonus, and be robust
to partial annotations. Extensive experiments and ablations on Charades-STA and
ActivityNet Captions demonstrate the significance of partial supervision and
our superior performance.",None,-1
a7bd2828-24d0-4f86-a22a-8472e9fee1ac,Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences,0.477925,27,"As a natural language assistant, ChatGPT is capable of performing various
tasks, including but not limited to article generation, code completion, and
data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable
level of accuracy and reliability in terms of content evaluation, exhibiting
the capability of mimicking human preferences. To further explore ChatGPT's
potential in this regard, a study is conducted to assess its ability to rank
content. In order to do so, a test set consisting of prompts is created,
covering a wide range of use cases, and five models are utilized to generate
corresponding responses. ChatGPT is then instructed to rank the responses
generated by these models. The results on the test set show that ChatGPT's
ranking preferences are consistent with human to a certain extent. This
preliminary experimental finding implies that ChatGPT's zero-shot ranking
capability could be used to reduce annotation pressure in a number of ranking
tasks.",None,-1
444e4256-e8fd-4535-a685-b2d30c5c5535,Hallucination Reduction in Long Input Text Summarization,0.311939,3,"Hallucination in text summarization refers to the phenomenon where the model
generates information that is not supported by the input source document.
Hallucination poses significant obstacles to the accuracy and reliability of
the generated summaries. In this paper, we aim to reduce hallucinated outputs
or hallucinations in summaries of long-form text documents. We have used the
PubMed dataset, which contains long scientific research documents and their
abstracts. We have incorporated the techniques of data filtering and joint
entity and summary generation (JAENS) in the fine-tuning of the Longformer
Encoder-Decoder (LED) model to minimize hallucinations and thereby improve the
quality of the generated summary. We have used the following metrics to measure
factual consistency at the entity level: precision-source, and F1-target. Our
experiments show that the fine-tuned LED model performs well in generating the
paper abstract. Data filtering techniques based on some preprocessing steps
reduce entity-level hallucinations in the generated summaries in terms of some
of the factual consistency metrics.",None,-1
28edcd43-2c50-4ee3-845e-4cdd3755f857,HanoiT: Enhancing Context-aware Translation via Selective Context,0.25694,4,"Context-aware neural machine translation aims to use the document-level
context to improve translation quality. However, not all words in the context
are helpful. The irrelevant or trivial words may bring some noise and distract
the model from learning the relationship between the current sentence and the
auxiliary context. To mitigate this problem, we propose a novel end-to-end
encoder-decoder model with a layer-wise selection mechanism to sift and refine
the long document context. To verify the effectiveness of our method, extensive
experiments and extra quantitative analysis are conducted on four
document-level machine translation benchmarks. The experimental results
demonstrate that our model significantly outperforms previous models on all
datasets via the soft selection mechanism.",None,-1
35c5ece9-7ce0-44d9-9cf4-68535ee2c790,Learning Empirical Bregman Divergence for Uncertain Distance Representation,0.0917093,1,"Deep metric learning techniques have been used for visual representation in
various supervised and unsupervised learning tasks through learning embeddings
of samples with deep networks. However, classic approaches, which employ a
fixed distance metric as a similarity function between two embeddings, may lead
to suboptimal performance for capturing the complex data distribution. The
Bregman divergence generalizes measures of various distance metrics and arises
throughout many fields of deep metric learning. In this paper, we first show
how deep metric learning loss can arise from the Bregman divergence. We then
introduce a novel method for learning empirical Bregman divergence directly
from data based on parameterizing the convex function underlying the Bregman
divergence with a deep learning setting. We further experimentally show that
our approach performs effectively on five popular public datasets compared to
other SOTA deep metric learning methods, particularly for pattern recognition
problems.",None,-1
382b877d-95e7-4e78-b8f3-ca31cef05832,Super-Resolution of License Plate Images Using Attention Modules and Sub-Pixel Convolution Layers,0.487694,7,"Recent years have seen significant developments in the field of License Plate
Recognition (LPR) through the integration of deep learning techniques and the
increasing availability of training data. Nevertheless, reconstructing license
plates (LPs) from low-resolution (LR) surveillance footage remains challenging.
To address this issue, we introduce a Single-Image Super-Resolution (SISR)
approach that integrates attention and transformer modules to enhance the
detection of structural and textural features in LR images. Our approach
incorporates sub-pixel convolution layers (also known as PixelShuffle) and a
loss function that uses an Optical Character Recognition (OCR) model for
feature extraction. We trained the proposed architecture on synthetic images
created by applying heavy Gaussian noise to high-resolution LP images from two
public datasets, followed by bicubic downsampling. As a result, the generated
images have a Structural Similarity Index Measure (SSIM) of less than 0.10. Our
results show that our approach for reconstructing these low-resolution
synthesized images outperforms existing ones in both quantitative and
qualitative measures. Our code is publicly available at
https://github.com/valfride/lpr-rsr-ext/",None,-1
16050b1e-cccf-4969-9271-a83a594ad6e1,"What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems",0.583029,5,"Machine learning has made remarkable advancements, but confidently utilising
learning-enabled components in safety-critical domains still poses challenges.
Among the challenges, it is known that a rigorous, yet practical, way of
achieving safety guarantees is one of the most prominent. In this paper, we
first discuss the engineering and research challenges associated with the
design and verification of such systems. Then, based on the observation that
existing works cannot actually achieve provable guarantees, we promote a
two-step verification method for the ultimate achievement of provable
statistical guarantees.",None,-1
cb36f705-6cb3-45bd-9c76-eb08465fc87c,Data quality dimensions for fair AI,0.275873,2,"AI systems are not intrinsically neutral and biases trickle in any type of
technological tool. In particular when dealing with people, AI algorithms
reflect technical errors originating with mislabeled data. As they feed wrong
and discriminatory classifications, perpetuating structural racism and
marginalization, these systems are not systematically guarded against bias. In
this article we consider the problem of bias in AI systems from the point of
view of Information Quality dimensions. We illustrate potential improvements of
a bias mitigation tool in gender classification errors, referring to two
typically difficult contexts: the classification of non-binary individuals and
the classification of transgender individuals. The identification of data
quality dimensions to implement in bias mitigation tool may help achieve more
fairness. Hence, we propose to consider this issue in terms of completeness,
consistency, timeliness and reliability, and offer some theoretical results.",None,-1
73c4a4a0-7ada-4bc7-8fc6-135d1d3ae9ce,Calibration-free BEV Representation for Infrastructure Perception,0.417321,9,"Effective BEV object detection on infrastructure can greatly improve traffic
scenes understanding and vehicle-toinfrastructure (V2I) cooperative perception.
However, cameras installed on infrastructure have various postures, and
previous BEV detection methods rely on accurate calibration, which is difficult
for practical applications due to inevitable natural factors (e.g., wind and
snow). In this paper, we propose a Calibration-free BEV Representation (CBR)
network, which achieves 3D detection based on BEV representation without
calibration parameters and additional depth supervision. Specifically, we
utilize two multi-layer perceptrons for decoupling the features from
perspective view to front view and birdeye view under boxes-induced foreground
supervision. Then, a cross-view feature fusion module matches features from
orthogonal views according to similarity and conducts BEV feature enhancement
with front view features. Experimental results on DAIR-V2X demonstrate that CBR
achieves acceptable performance without any camera parameters and is naturally
not affected by calibration noises. We hope CBR can serve as a baseline for
future research addressing practical challenges of infrastructure perception.",None,-1
d0b8f626-cd59-43a7-b2f0-a06160eb57bf,Retrosynthetic Planning with Dual Value Networks,0.638891,7,"Retrosynthesis, which aims to find a route to synthesize a target molecule
from commercially available starting materials, is a critical task in drug
discovery and materials design. Recently, the combination of ML-based
single-step reaction predictors with multi-step planners has led to promising
results. However, the single-step predictors are mostly trained offline to
optimize the single-step accuracy, without considering complete routes. Here,
we leverage reinforcement learning (RL) to improve the single-step predictor,
by using a tree-shaped MDP to optimize complete routes. Specifically, we
propose a novel online training algorithm, called Planning with Dual Value
Networks (PDVN), which alternates between the planning phase and updating
phase. In PDVN, we construct two separate value networks to predict the
synthesizability and cost of molecules, respectively. To maintain the
single-step accuracy, we design a two-branch network structure for the
single-step predictor. On the widely-used USPTO dataset, our PDVN algorithm
improves the search success rate of existing multi-step planners (e.g.,
increasing the success rate from 85.79% to 98.95% for Retro*, and reducing the
number of model calls by half while solving 99.47% molecules for RetroGraph).
Additionally, PDVN helps find shorter synthesis routes (e.g., reducing the
average route length from 5.76 to 4.83 for Retro*, and from 5.63 to 4.78 for
RetroGraph). Our code is available at \url{https://github.com/DiXue98/PDVN}.",None,-1
9c7167f8-d492-4819-8cb7-7c882609d00d,Cross-modal Place Recognition in Image Databases using Event-based Sensors,0.208087,1,"Visual place recognition is an important problem towards global localization
in many robotics tasks. One of the biggest challenges is that it may suffer
from illumination or appearance changes in surrounding environments. Event
cameras are interesting alternatives to frame-based sensors as their high
dynamic range enables robust perception in difficult illumination conditions.
However, current event-based place recognition methods only rely on event
information, which restricts downstream applications of VPR. In this paper, we
present the first cross-modal visual place recognition framework that is
capable of retrieving regular images from a database given an event query. Our
method demonstrates promising results with respect to the state-of-the-art
frame-based and event-based methods on the Brisbane-Event-VPR dataset under
different scenarios. We also verify the effectiveness of the combination of
retrieval and classification, which can boost performance by a large margin.",None,-1
1052e758-8531-4ebf-999d-41c63d8e79cc,Removing RLHF Protections in GPT-4 via Fine-Tuning,0.837009,40,"As large language models (LLMs) have increased in their capabilities, so does
their potential for dual use. To reduce harmful outputs, produces and vendors
of LLMs have used reinforcement learning with human feedback (RLHF). In tandem,
LLM vendors have been increasingly enabling fine-tuning of their most powerful
models. However, concurrent work has shown that fine-tuning can remove RLHF
protections. We may expect that the most powerful models currently available
(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the
contrary: fine-tuning allows attackers to remove RLHF protections with as few
as 340 examples and a 95% success rate. These training examples can be
automatically generated with weaker models. We further show that removing RLHF
protections does not decrease usefulness on non-censored outputs, providing
evidence that our fine-tuning strategy does not decrease usefulness despite
using weaker models to generate training data. Our results show the need for
further research on protections on LLMs.",None,-1
09dbd41b-cfd1-476b-aa02-f12f1dbf60e8,Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation,0.510616,27,"New Natural Langauge Process~(NLP) benchmarks are urgently needed to align
with the rapid development of large language models (LLMs). We present Xiezhi,
the most comprehensive evaluation suite designed to assess holistic domain
knowledge. Xiezhi comprises multiple-choice questions across 516 diverse
disciplines ranging from 13 different subjects with 249,587 questions and
accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k
questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results
indicate that LLMs exceed average performance of humans in science,
engineering, agronomy, medicine, and art, but fall short in economics,
jurisprudence, pedagogy, literature, history, and management. We anticipate
Xiezhi will help analyze important strengths and shortcomings of LLMs, and the
benchmark is released in~\url{https://github.com/MikeGu721/XiezhiBenchmark}.",None,-1
4e4b47df-d744-415f-93e0-c4735e71c669,Tackling Clutter in Radar Data -- Label Generation and Detection Using PointNet++,0.770778,7,"Radar sensors employed for environment perception, e.g. in autonomous
vehicles, output a lot of unwanted clutter. These points, for which no
corresponding real objects exist, are a major source of errors in following
processing steps like object detection or tracking. We therefore present two
novel neural network setups for identifying clutter. The input data, network
architectures and training configuration are adjusted specifically for this
task. Special attention is paid to the downsampling of point clouds composed of
multiple sensor scans. In an extensive evaluation, the new setups display
substantially better performance than existing approaches. Because there is no
suitable public data set in which clutter is annotated, we design a method to
automatically generate the respective labels. By applying it to existing data
with object annotations and releasing its code, we effectively create the first
freely available radar clutter data set representing real-world driving
scenarios. Code and instructions are accessible at
www.github.com/kopp-j/clutter-ds.",None,-1
08254823-1b2e-4c25-b001-d3302bb2683e,"System identification of neural systems: If we got it right, would we know?",0.671737,7,"Artificial neural networks are being proposed as models of parts of the
brain. The networks are compared to recordings of biological neurons, and good
performance in reproducing neural responses is considered to support the
model's validity. A key question is how much this system identification
approach tells us about brain computation. Does it validate one model
architecture over another? We evaluate the most commonly used comparison
techniques, such as a linear encoding model and centered kernel alignment, to
correctly identify a model by replacing brain recordings with known ground
truth models. System identification performance is quite variable; it also
depends significantly on factors independent of the ground truth architecture,
such as stimuli images. In addition, we show the limitations of using
functional similarity scores in identifying higher-level architectural motifs.",None,-1
46a69a3f-265b-4732-9f56-1242f81537a4,Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies,0.392004,10,"Recent work has shown the promise of creating generalist, transformer-based,
models for language, vision, and sequential decision-making problems. To create
such models, we generally require centralized training objectives, data, and
compute. It is of interest if we can more flexibly create generalist policies
by merging together multiple, task-specific, individually trained policies. In
this work, we take a preliminary step in this direction through merging, or
averaging, subsets of Decision Transformers in parameter space trained on
different MuJoCo locomotion problems, forming multi-task models without
centralized training. We also demonstrate the importance of various
methodological choices when merging policies, such as utilizing common
pre-trained initializations, increasing model capacity, and utilizing Fisher
information for weighting parameter importance. In general, we believe research
in this direction could help democratize and distribute the process that forms
multi-task robotics policies. Our implementation is available at
https://github.com/daniellawson9999/merging-decision-transformers.",None,-1
8bb984d7-841e-44bd-9ec9-09da24bd561a,Explanations for Automatic Speech Recognition,0.468319,3,"We address quality assessment for neural network based ASR by providing
explanations that help increase our understanding of the system and ultimately
help build trust in the system. Compared to simple classification labels,
explaining transcriptions is more challenging as judging their correctness is
not straightforward and transcriptions as a variable-length sequence is not
handled by existing interpretable machine learning models. We provide an
explanation for an ASR transcription as a subset of audio frames that is both a
minimal and sufficient cause of the transcription. To do this, we adapt
existing explainable AI (XAI) techniques from image classification-Statistical
Fault Localisation(SFL) and Causal. Additionally, we use an adapted version of
Local Interpretable Model-Agnostic Explanations (LIME) for ASR as a baseline in
our experiments. We evaluate the quality of the explanations generated by the
proposed techniques over three different ASR ,Google API, the baseline model of
Sphinx, Deepspeech and 100 audio samples from the Commonvoice dataset.",None,-1
d278869c-f130-4ecd-b54e-a0385fda8d8d,RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation,0.819555,5,"Grammatical Error Correction (GEC) systems play a vital role in assisting
people with their daily writing tasks. However, users may sometimes come across
a GEC system that initially performs well but fails to correct errors when the
inputs are slightly modified. To ensure an ideal user experience, a reliable
GEC system should have the ability to provide consistent and accurate
suggestions when encountering irrelevant context perturbations, which we refer
to as context robustness. In this paper, we introduce RobustGEC, a benchmark
designed to evaluate the context robustness of GEC systems. RobustGEC comprises
5,000 GEC cases, each with one original error-correct sentence pair and five
variants carefully devised by human annotators. Utilizing RobustGEC, we reveal
that state-of-the-art GEC systems still lack sufficient robustness against
context perturbations. In addition, we propose a simple yet effective method
for remitting this issue.",None,-1
fbf6503c-fc8e-4c48-b437-2143ae61bdf3,Performance Prediction for Multi-hop Questions,0.0536708,2,"We study the problem of Query Performance Prediction (QPP) for open-domain
multi-hop Question Answering (QA), where the task is to estimate the difficulty
of evaluating a multi-hop question over a corpus. Despite the extensive
research on predicting the performance of ad-hoc and QA retrieval models, there
has been a lack of study on the estimation of the difficulty of multi-hop
questions. The problem is challenging due to the multi-step nature of the
retrieval process, potential dependency of the steps and the reasoning
involved. To tackle this challenge, we propose multHP, a novel pre-retrieval
method for predicting the performance of open-domain multi-hop questions. Our
extensive evaluation on the largest multi-hop QA dataset using several modern
QA systems shows that the proposed model is a strong predictor of the
performance, outperforming traditional single-hop QPP models. Additionally, we
demonstrate that our approach can be effectively used to optimize the
parameters of QA systems, such as the number of documents to be retrieved,
resulting in improved overall retrieval performance.",None,-1
55a0d6c1-0773-4ff6-98fa-c3647a5ecf9c,Training Socially Aligned Language Models on Simulated Social Interactions,0.30797,13,"Social alignment in AI systems aims to ensure that these models behave
according to established societal values. However, unlike humans, who derive
consensus on value judgments through social interaction, current language
models (LMs) are trained to rigidly replicate their training corpus in
isolation, leading to subpar generalization in unfamiliar scenarios and
vulnerability to adversarial attacks. This work presents a novel training
paradigm that permits LMs to learn from simulated social interactions. In
comparison to existing methodologies, our approach is considerably more
scalable and efficient, demonstrating superior performance in alignment
benchmarks and human evaluations. This paradigm shift in the training of LMs
brings us a step closer to developing AI systems that can robustly and
accurately reflect societal norms and values.",None,-1
3edcfb06-3927-4b02-ad2f-1ca02f30651e,Efficient Bayesian Computational Imaging with a Surrogate Score-Based Prior,0.524051,10,"We propose a surrogate function for efficient use of score-based priors for
Bayesian inverse imaging. Recent work turned score-based diffusion models into
probabilistic priors for solving ill-posed imaging problems by appealing to an
ODE-based log-probability function. However, evaluating this function is
computationally inefficient and inhibits posterior estimation of
high-dimensional images. Our proposed surrogate prior is based on the evidence
lower-bound of a score-based diffusion model. We demonstrate the surrogate
prior on variational inference for efficient approximate posterior sampling of
large images. Compared to the exact prior in previous work, our surrogate prior
accelerates optimization of the variational image distribution by at least two
orders of magnitude. We also find that our principled approach achieves
higher-fidelity images than non-Bayesian baselines that involve
hyperparameter-tuning at inference. Our work establishes a practical path
forward for using score-based diffusion models as general-purpose priors for
imaging.",None,-1
d896c856-04cb-4e71-9e18-761d0e897c55,ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,0.999713,168,"Text evaluation has historically posed significant challenges, often
demanding substantial labor and time cost. With the emergence of large language
models (LLMs), researchers have explored LLMs' potential as alternatives for
human evaluation. While these single-agent-based approaches show promise,
experimental results suggest that further advancements are needed to bridge the
gap between their current effectiveness and human-level evaluation quality.
Recognizing that best practices of human evaluation processes often involve
multiple human annotators collaborating in the evaluation, we resort to a
multi-agent debate framework, moving beyond single-agent prompting strategies.
The multi-agent-based approach enables a group of LLMs to synergize with an
array of intelligent counterparts, harnessing their distinct capabilities and
expertise to enhance efficiency and effectiveness in handling intricate tasks.
In this paper, we construct a multi-agent referee team called ChatEval to
autonomously discuss and evaluate the quality of generated responses from
different models on open-ended questions and traditional natural language
generation (NLG) tasks. Our analysis shows that ChatEval transcends mere
textual scoring, offering a human-mimicking evaluation process for reliable
assessments. Our code is available at https://github.com/chanchimin/ChatEval.",None,-1
7f26dde6-3957-49ac-addb-10d6053b1c43,MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems,0.736872,3,"MeetEval is an open-source toolkit to evaluate all kinds of meeting
transcription systems. It provides a unified interface for the computation of
commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER
along other WER definitions. We extend the cpWER computation by a temporal
constraint to ensure that only words are identified as correct when the
temporal alignment is plausible. This leads to a better quality of the matching
of the hypothesis string to the reference string that more closely resembles
the actual transcription quality, and a system is penalized if it provides poor
time annotations. Since word-level timing information is often not available,
we present a way to approximate exact word-level timings from segment-level
timings (e.g., a sentence) and show that the approximation leads to a similar
WER as a matching with exact word-level annotations. At the same time, the time
constraint leads to a speedup of the matching algorithm, which outweighs the
additional overhead caused by processing the time stamps.",None,-1
12fdb36a-29af-4597-868d-887cee29680d,Exploiting Inductive Bias in Transformer for Point Cloud Classification and Segmentation,0.200436,2,"Discovering inter-point connection for efficient high-dimensional feature
extraction from point coordinate is a key challenge in processing point cloud.
Most existing methods focus on designing efficient local feature extractors
while ignoring global connection, or vice versa. In this paper, we design a new
Inductive Bias-aided Transformer (IBT) method to learn 3D inter-point
relations, which considers both local and global attentions. Specifically,
considering local spatial coherence, local feature learning is performed
through Relative Position Encoding and Attentive Feature Pooling. We
incorporate the learned locality into the Transformer module. The local feature
affects value component in Transformer to modulate the relationship between
channels of each point, which can enhance self-attention mechanism with
locality based channel interaction. We demonstrate its superiority
experimentally on classification and segmentation tasks. The code is available
at: https://github.com/jiamang/IBT",None,-1
073a5ae1-42ba-466e-b768-4ac004f3bd36,Koala: An Index for Quantifying Overlaps with Pre-training Corpora,0.614354,8,"In very recent years more attention has been placed on probing the role of
pre-training data in Large Language Models (LLMs) downstream behaviour. Despite
the importance, there is no public tool that supports such analysis of
pre-training corpora at large scale. To help research in this space, we launch
Koala, a searchable index over large pre-training corpora using compressed
suffix arrays with highly efficient compression rate and search support. In its
first release we index the public proportion of OPT 175B pre-training data.
Koala provides a framework to do forensic analysis on the current and future
benchmarks as well as to assess the degree of memorization in the output from
the LLMs. Koala is available for public use at
https://koala-index.erc.monash.edu/.",None,-1
2033dfba-014b-42c5-967d-ae4b58f6a4c5,Competitions in AI -- Robustly Ranking Solvers Using Statistical Resampling,0.0366877,1,"Solver competitions play a prominent role in assessing and advancing the
state of the art for solving many problems in AI and beyond. Notably, in many
areas of AI, competitions have had substantial impact in guiding research and
applications for many years, and for a solver to be ranked highly in a
competition carries considerable weight. But to which extent can we expect
competition results to generalise to sets of problem instances different from
those used in a particular competition? This is the question we investigate
here, using statistical resampling techniques. We show that the rankings
resulting from the standard interpretation of competition results can be very
sensitive to even minor changes in the benchmark instance set used as the basis
for assessment and can therefore not be expected to carry over to other samples
from the same underlying instance distribution. To address this problem, we
introduce a novel approach to statistically meaningful analysis of competition
results based on resampling performance data. Our approach produces confidence
intervals of competition scores as well as statistically robust solver rankings
with bounded error. Applied to recent SAT, AI planning and computer vision
competitions, our analysis reveals frequent statistical ties in solver
performance as well as some inversions of ranks compared to the official
results based on simple scoring.",None,-1
78bda044-c2ee-47fb-a495-766bb0ee7092,Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,0.524583,8,"Real-world fact verification task aims to verify the factuality of a claim by
retrieving evidence from the source document. The quality of the retrieved
evidence plays an important role in claim verification. Ideally, the retrieved
evidence should be faithful (reflecting the model's decision-making process in
claim verification) and plausible (convincing to humans), and can improve the
accuracy of verification task. Although existing approaches leverage the
similarity measure of semantic or surface form between claims and documents to
retrieve evidence, they all rely on certain heuristics that prevent them from
satisfying all three requirements. In light of this, we propose a fact
verification model named ReRead to retrieve evidence and verify claim that: (1)
Train the evidence retriever to obtain interpretable evidence (i.e.,
faithfulness and plausibility criteria); (2) Train the claim verifier to
revisit the evidence retrieved by the optimized evidence retriever to improve
the accuracy. The proposed system is able to achieve significant improvements
upon best-reported models under different settings.",None,-1
550bd658-64d7-4c9e-95eb-626ba8cf9110,Conditional Denoising Diffusion for Sequential Recommendation,0.624146,12,"Generative models have attracted significant interest due to their ability to
handle uncertainty by learning the inherent data distributions. However, two
prominent generative models, namely Generative Adversarial Networks (GANs) and
Variational AutoEncoders (VAEs), exhibit challenges that impede achieving
optimal performance in sequential recommendation tasks. Specifically, GANs
suffer from unstable optimization, while VAEs are prone to posterior collapse
and over-smoothed generations. The sparse and noisy nature of sequential
recommendation further exacerbates these issues. In response to these
limitations, we present a conditional denoising diffusion model, which includes
a sequence encoder, a cross-attentive denoising decoder, and a step-wise
diffuser. This approach streamlines the optimization and generation process by
dividing it into easier and tractable steps in a conditional autoregressive
manner. Furthermore, we introduce a novel optimization schema that incorporates
both cross-divergence loss and contrastive loss. This novel training schema
enables the model to generate high-quality sequence/item representations and
meanwhile precluding collapse. We conducted comprehensive experiments on four
benchmark datasets, and the superior performance achieved by our model attests
to its efficacy.",None,-1
390e4f97-3bfa-4d05-b25c-581489fd5222,Rethinking the BERT-like Pretraining for DNA Sequences,0.692554,6,"With the success of large-scale pretraining in NLP, there is an increasing
trend of applying it to the domain of life sciences. In particular, pretraining
methods based on DNA sequences have garnered growing attention due to their
potential to capture generic information about genes. However, existing
pretraining methods for DNA sequences largely rely on direct adoptions of BERT
pretraining from NLP, lacking a comprehensive understanding and a specifically
tailored approach. To address this research gap, we first conducted a series of
exploratory experiments and gained several insightful observations: 1) In the
fine-tuning phase of downstream tasks, when using K-mer overlapping
tokenization instead of K-mer non-overlapping tokenization, both overlapping
and non-overlapping pretraining weights show consistent performance
improvement.2) During the pre-training process, using K-mer overlapping
tokenization quickly produces clear K-mer embeddings and reduces the loss to a
very low level, while using K-mer non-overlapping tokenization results in less
distinct embeddings and continuously decreases the loss. 3) Using overlapping
tokenization causes the self-attention in the intermediate layers of
pre-trained models to tend to overly focus on certain tokens, reflecting that
these layers are not adequately optimized. In summary, overlapping tokenization
can benefit the fine-tuning of downstream tasks but leads to inadequate
pretraining with fast convergence. To unleash the pretraining potential, we
introduce a novel approach called RandomMask, which gradually increases the
task difficulty of BERT-like pretraining by continuously expanding its mask
boundary, forcing the model to learn more knowledge. RandomMask is simple but
effective, achieving top-tier performance across 26 datasets of 28 datasets
spanning 7 downstream tasks.",None,-1
5a8b9f48-0114-43d8-9cab-ae257fd3de88,Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling,0.0634513,3,"With their increasing size, large language models (LLMs) are becoming
increasingly good at language understanding tasks. But even with high
performance on specific downstream task, LLMs fail at simple linguistic tests
for negation or quantifier understanding. Previous work on quantifier
understanding in LLMs show inverse scaling in understanding few-type
quantifiers. In this paper, we question the claims of of previous work and show
that it is a result of inappropriate testing methodology. We also present
alternate methods to measure quantifier comprehension in LLMs and show that
LLMs are able to better understand the difference between the meaning of
few-type and most-type quantifiers as their size increases, although they are
not particularly good at it. We also observe inverse scaling for most-type
quantifier understanding, which is contrary to human psycho-linguistic
experiments and previous work, where the model's understanding of most-type
quantifier gets worse as the model size increases. We do this evaluation on
models ranging from 125M-175B parameters, which suggests that LLMs do not do as
well as expected with quantifiers. We also discuss the possible reasons for
this and the relevance of quantifier understanding in evaluating language
understanding in LLMs.",None,-1
17ce13c1-d17c-4c1d-abf3-170ce72dada0,Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,0.470174,24,"State-of-the-art supervised NLP models achieve high accuracy but are also
susceptible to failures on inputs from low-data regimes, such as domains that
are not represented in training data. As an approximation to collecting
ground-truth labels for the specific domain, we study the use of large language
models (LLMs) for annotating inputs and improving the generalization of NLP
models. Specifically, given a budget for LLM annotations, we present an
algorithm for sampling the most informative inputs to annotate and retrain the
NLP model. We find that popular active learning strategies such as
uncertainty-based sampling do not work well. Instead, we propose a sampling
strategy based on the difference in prediction scores between the base model
and the finetuned NLP model, utilizing the fact that most NLP models are
finetuned from a base model. Experiments with classification (semantic
similarity) and ranking (semantic search) tasks show that our sampling strategy
leads to significant gains in accuracy for both the training and target
domains.",None,-1
4bf9dd1e-e9f0-428c-80cb-a2018070114d,CEMFormer: Learning to Predict Driver Intentions from In-Cabin and External Cameras via Spatial-Temporal Transformers,0.943269,8,"Driver intention prediction seeks to anticipate drivers' actions by analyzing
their behaviors with respect to surrounding traffic environments. Existing
approaches primarily focus on late-fusion techniques, and neglect the
importance of maintaining consistency between predictions and prevailing
driving contexts. In this paper, we introduce a new framework called Cross-View
Episodic Memory Transformer (CEMFormer), which employs spatio-temporal
transformers to learn unified memory representations for an improved driver
intention prediction. Specifically, we develop a spatial-temporal encoder to
integrate information from both in-cabin and external camera views, along with
episodic memory representations to continuously fuse historical data.
Furthermore, we propose a novel context-consistency loss that incorporates
driving context as an auxiliary supervision signal to improve prediction
performance. Comprehensive experiments on the Brain4Cars dataset demonstrate
that CEMFormer consistently outperforms existing state-of-the-art methods in
driver intention prediction.",None,-1
1aa31c45-35a0-4f12-ac98-33e3574a78ac,Tracking Anything with Decoupled Video Segmentation,0.999225,32,"Training data for video segmentation are expensive to annotate. This impedes
extensions of end-to-end algorithms to new video segmentation tasks, especially
in large-vocabulary settings. To 'track anything' without training on video
data for every individual task, we develop a decoupled video segmentation
approach (DEVA), composed of task-specific image-level segmentation and
class/task-agnostic bi-directional temporal propagation. Due to this design, we
only need an image-level model for the target task (which is cheaper to train)
and a universal temporal propagation model which is trained once and
generalizes across tasks. To effectively combine these two modules, we use
bi-directional propagation for (semi-)online fusion of segmentation hypotheses
from different frames to generate a coherent segmentation. We show that this
decoupled formulation compares favorably to end-to-end approaches in several
data-scarce tasks including large-vocabulary video panoptic segmentation,
open-world video segmentation, referring video segmentation, and unsupervised
video object segmentation. Code is available at:
https://hkchengrex.github.io/Tracking-Anything-with-DEVA",None,-1
ea6be990-ddd4-417e-8edb-1d1f45cd8728,From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for Conversational Exploratory Search,0.569733,2,"Exploratory search is an open-ended information retrieval process that aims
at discovering knowledge about a topic or domain rather than searching for a
specific answer or piece of information. Conversational interfaces are
particularly suitable for supporting exploratory search, allowing users to
refine queries and examine search results through interactive dialogues. In
addition to conversational search interfaces, knowledge graphs are also useful
in supporting information exploration due to their rich semantic representation
of data items. In this study, we demonstrate the synergistic effects of
combining knowledge graphs and conversational interfaces for exploratory
search, bridging the gap between structured and unstructured information
retrieval. To this end, we propose a knowledge-driven dialogue system for
exploring news articles by asking natural language questions and using the
graph structure to navigate between related topics. Based on a user study with
54 participants, we empirically evaluate the effectiveness of the graph-based
exploratory search and discuss design implications for developing such systems.",None,-1
1353278c-7be5-4a80-ae14-4de6bae5aadd,"Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models",0.601129,6,"Modern recommender systems lie at the heart of complex ecosystems that couple
the behavior of users, content providers, advertisers, and other actors.
Despite this, the focus of the majority of recommender research -- and most
practical recommenders of any import -- is on the local, myopic optimization of
the recommendations made to individual users. This comes at a significant cost
to the long-term utility that recommenders could generate for its users. We
argue that explicitly modeling the incentives and behaviors of all actors in
the system -- and the interactions among them induced by the recommender's
policy -- is strictly necessary if one is to maximize the value the system
brings to these actors and improve overall ecosystem ""health"". Doing so
requires: optimization over long horizons using techniques such as
reinforcement learning; making inevitable tradeoffs in the utility that can be
generated for different actors using the methods of social choice; reducing
information asymmetry, while accounting for incentives and strategic behavior,
using the tools of mechanism design; better modeling of both user and
item-provider behaviors by incorporating notions from behavioral economics and
psychology; and exploiting recent advances in generative and foundation models
to make these mechanisms interpretable and actionable. We propose a conceptual
framework that encompasses these elements, and articulate a number of research
challenges that emerge at the intersection of these different disciplines.",None,-1
1bdda410-c25c-4301-9a06-2647c97b6e04,Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach,0.313559,11,"Large language models (LLMs) encode a vast amount of world knowledge acquired
from massive text datasets. Recent studies have demonstrated that LLMs can
assist an embodied agent in solving complex sequential decision making tasks by
providing high-level instructions. However, interactions with LLMs can be
time-consuming. In many practical scenarios, it requires a significant amount
of storage space that can only be deployed on remote cloud servers.
Additionally, using commercial LLMs can be costly since they may charge based
on usage frequency. In this paper, we explore how to enable intelligent
cost-effective interactions between a down stream task oriented agent and an
LLM. We find that this problem can be naturally formulated by a Markov decision
process (MDP), and propose When2Ask, a reinforcement learning based approach
that learns when it is necessary to query LLMs for high-level instructions to
accomplish a target task. On one side, When2Ask discourages unnecessary
redundant interactions, while on the other side, it enables the agent to
identify and follow useful instructions from the LLM. This enables the agent to
halt an ongoing plan and transition to a more suitable one based on new
environmental observations. Experiments on MiniGrid and Habitat environments
that entail planning sub-goals demonstrate that When2Ask learns to solve target
tasks with only a few necessary interactions with the LLM, significantly
reducing interaction costs in testing environments compared with baseline
methods. Our code is available at: https://github.com/ZJLAB-AMMI/LLM4RL.",None,-1
118469da-598e-498d-8898-b4f2bdf48d3c,Protecting Society from AI Misuse: When are Restrictions on Capabilities Warranted?,0.380979,18,"Artificial intelligence (AI) systems will increasingly be used to cause harm
as they grow more capable. In fact, AI systems are already starting to be used
to automate fraudulent activities, violate human rights, create harmful fake
images, and identify dangerous toxins. To prevent some misuses of AI, we argue
that targeted interventions on certain capabilities will be warranted. These
restrictions may include controlling who can access certain types of AI models,
what they can be used for, whether outputs are filtered or can be traced back
to their user, and the resources needed to develop them. We also contend that
some restrictions on non-AI capabilities needed to cause harm will be required.
Though capability restrictions risk reducing use more than misuse (facing an
unfavorable Misuse-Use Tradeoff), we argue that interventions on capabilities
are warranted when other interventions are insufficient, the potential harm
from misuse is high, and there are targeted ways to intervene on capabilities.
We provide a taxonomy of interventions that can reduce AI misuse, focusing on
the specific steps required for a misuse to cause harm (the Misuse Chain), and
a framework to determine if an intervention is warranted. We apply this
reasoning to three examples: predicting novel toxins, creating harmful images,
and automating spear phishing campaigns.",None,-1
af7b005e-3443-48e4-9881-1b975705fc3d,"SITReg: Multi-resolution architecture for symmetric, inverse consistent, and topology preserving image registration",0.13657,1,"Deep learning has emerged as a strong alternative for classical iterative
methods for deformable medical image registration, where the goal is to find a
mapping between the coordinate systems of two images. Popular classical image
registration methods enforce the useful inductive biases of symmetricity,
inverse consistency, and topology preservation by construct. However, while
many deep learning registration methods encourage these properties via loss
functions, no earlier methods enforce all of them by construct. Here, we
propose a novel registration architecture based on extracting multi-resolution
feature representations which is by construct symmetric, inverse consistent,
and topology preserving. We also develop an implicit layer for memory efficient
inversion of the deformation fields. Our method achieves state-of-the-art
registration accuracy on two datasets.",None,-1
b6a11d2e-a72d-4b16-8faa-5453ce954ece,Object-centric Inference for Language Conditioned Placement: A Foundation Model based Approach,0.223472,2,"We focus on the task of language-conditioned object placement, in which a
robot should generate placements that satisfy all the spatial relational
constraints in language instructions. Previous works based on rule-based
language parsing or scene-centric visual representation have restrictions on
the form of instructions and reference objects or require large amounts of
training data. We propose an object-centric framework that leverages foundation
models to ground the reference objects and spatial relations for placement,
which is more sample efficient and generalizable. Experiments indicate that our
model can achieve a 97.75% success rate of placement with only ~0.26M trainable
parameters. Besides, our method generalizes better to both unseen objects and
instructions. Moreover, with only 25% training data, we still outperform the
top competing approach.",None,-1
21412aad-71a0-4e2f-b5e0-efafbadcfa8f,NeRD: Neural field-based Demosaicking,0.359782,1,"We introduce NeRD, a new demosaicking method for generating full-color images
from Bayer patterns. Our approach leverages advancements in neural fields to
perform demosaicking by representing an image as a coordinate-based neural
network with sine activation functions. The inputs to the network are spatial
coordinates and a low-resolution Bayer pattern, while the outputs are the
corresponding RGB values. An encoder network, which is a blend of ResNet and
U-net, enhances the implicit neural representation of the image to improve its
quality and ensure spatial consistency through prior learning. Our experimental
results demonstrate that NeRD outperforms traditional and state-of-the-art
CNN-based methods and significantly closes the gap to transformer-based
methods.",None,-1
afe649ec-16c7-49e6-8f47-2e2385daf555,MVPSNet: Fast Generalizable Multi-view Photometric Stereo,0.980765,10,"We propose a fast and generalizable solution to Multi-view Photometric Stereo
(MVPS), called MVPSNet. The key to our approach is a feature extraction network
that effectively combines images from the same view captured under multiple
lighting conditions to extract geometric features from shading cues for stereo
matching. We demonstrate these features, termed `Light Aggregated Feature Maps'
(LAFM), are effective for feature matching even in textureless regions, where
traditional multi-view stereo methods fail. Our method produces similar
reconstruction results to PS-NeRF, a state-of-the-art MVPS method that
optimizes a neural network per-scene, while being 411$\times$ faster (105
seconds vs. 12 hours) in inference. Additionally, we introduce a new synthetic
dataset for MVPS, sMVPS, which is shown to be effective to train a
generalizable MVPS method.",None,-1
1881b070-d585-424b-8823-e0e6b2404ac8,"Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",0.0514618,2,"Recent explorations with commercial Large Language Models (LLMs) have shown
that non-expert users can jailbreak LLMs by simply manipulating their prompts;
resulting in degenerate output behavior, privacy and security breaches,
offensive outputs, and violations of content regulator policies. Limited
studies have been conducted to formalize and analyze these attacks and their
mitigations. We bridge this gap by proposing a formalism and a taxonomy of
known (and possible) jailbreaks. We survey existing jailbreak methods and their
effectiveness on open-source and commercial LLMs (such as GPT-based models,
OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak
detection in terms of their effectiveness against known attacks. For further
analysis, we release a dataset of model outputs across 3700 jailbreak prompts
over 4 tasks.",None,-1
0ec16f31-15e9-4109-8b88-7fc1081aaf24,TCSloT: Text Guided 3D Context and Slope Aware Triple Network for Dental Implant Position Prediction,0.845083,3,"In implant prosthesis treatment, the surgical guide of implant is used to
ensure accurate implantation. However, such design heavily relies on the manual
location of the implant position. When deep neural network has been proposed to
assist the dentist in locating the implant position, most of them take a single
slice as input, which do not fully explore 3D contextual information and
ignoring the influence of implant slope. In this paper, we design a Text Guided
3D Context and Slope Aware Triple Network (TCSloT) which enables the perception
of contextual information from multiple adjacent slices and awareness of
variation of implant slopes. A Texture Variation Perception (TVP) module is
correspondingly elaborated to process the multiple slices and capture the
texture variation among slices and a Slope-Aware Loss (SAL) is proposed to
dynamically assign varying weights for the regression head. Additionally, we
design a conditional text guidance (CTG) module to integrate the text condition
(i.e., left, middle and right) from the CLIP for assisting the implant position
prediction. Extensive experiments on a dental implant dataset through five-fold
cross-validation demonstrated that the proposed TCSloT achieves superior
performance than existing methods.",None,-1
d80b9044-6d03-40be-b3d2-37975767e620,Clothes-Invariant Feature Learning by Causal Intervention for Clothes-Changing Person Re-identification,0.468754,2,"Clothes-invariant feature extraction is critical to the clothes-changing
person re-identification (CC-ReID). It can provide discriminative identity
features and eliminate the negative effects caused by the confounder--clothing
changes. But we argue that there exists a strong spurious correlation between
clothes and human identity, that restricts the common likelihood-based ReID
method P(Y|X) to extract clothes-irrelevant features. In this paper, we propose
a new Causal Clothes-Invariant Learning (CCIL) method to achieve
clothes-invariant feature learning by modeling causal intervention P(Y|do(X)).
This new causality-based model is inherently invariant to the confounder in the
causal view, which can achieve the clothes-invariant features and avoid the
barrier faced by the likelihood-based methods. Extensive experiments on three
CC-ReID benchmarks, including PRCC, LTCC, and VC-Clothes, demonstrate the
effectiveness of our approach, which achieves a new state of the art.",None,-1
e8e8a38a-9643-45f8-b5fd-18238e485882,Visual Imitation Learning with Patch Rewards,0.320653,8,"Visual imitation learning enables reinforcement learning agents to learn to
behave from expert visual demonstrations such as videos or image sequences,
without explicit, well-defined rewards. Previous research either adopted
supervised learning techniques or induce simple and coarse scalar rewards from
pixels, neglecting the dense information contained in the image demonstrations.
In this work, we propose to measure the expertise of various local regions of
image samples, or called \textit{patches}, and recover multi-dimensional
\textit{patch rewards} accordingly. Patch reward is a more precise rewarding
characterization that serves as a fine-grained expertise measurement and visual
explainability tool. Specifically, we present Adversarial Imitation Learning
with Patch Rewards (PatchAIL), which employs a patch-based discriminator to
measure the expertise of different local parts from given images and provide
patch rewards. The patch-based knowledge is also used to regularize the
aggregated reward and stabilize the training. We evaluate our method on
DeepMind Control Suite and Atari tasks. The experiment results have
demonstrated that PatchAIL outperforms baseline methods and provides valuable
interpretations for visual demonstrations.",None,-1
76637139-777b-4a2a-b091-8cbf78eb507d,Paint it Black: Generating paintings from text descriptions,0.0179519,1,"Two distinct tasks - generating photorealistic pictures from given text
prompts and transferring the style of a painting to a real image to make it
appear as though it were done by an artist, have been addressed many times, and
several approaches have been proposed to accomplish them. However, the
intersection of these two, i.e., generating paintings from a given caption, is
a relatively unexplored area with little data available. In this paper, we have
explored two distinct strategies and have integrated them together. First
strategy is to generate photorealistic images and then apply style transfer and
the second strategy is to train an image generation model on real images with
captions and then fine-tune it on captioned paintings later. These two models
are evaluated using different metrics as well as a user study is conducted to
get human feedback on the produced results.",None,-1
78977f9e-5f6e-41ee-bff3-3a3834dd1aed,Contrastive Learning for Low-light Raw Denoising,0.248261,1,"Image/video denoising in low-light scenes is an extremely challenging problem
due to limited photon count and high noise. In this paper, we propose a novel
approach with contrastive learning to address this issue. Inspired by the
success of contrastive learning used in some high-level computer vision tasks,
we bring in this idea to the low-level denoising task. In order to achieve this
goal, we introduce a new denoising contrastive regularization (DCR) to exploit
the information of noisy images and clean images. In the feature space, DCR
makes the denoised image closer to the clean image and far away from the noisy
image. In addition, we build a new feature embedding network called Wnet, which
is more effective to extract high-frequency information. We conduct the
experiments on a real low-light dataset that captures still images taken on a
moonless clear night in 0.6 millilux and videos under starlight (no moon
present, <0.001 lux). The results show that our method can achieve a higher
PSNR and better visual quality compared with existing methods",None,-1
07e2d51f-bba6-4d65-9cfa-696e11e26947,Unleash the Potential of 3D Point Cloud Modeling with A Calibrated Local Geometry-driven Distance Metric,0.115577,1,"Quantifying the dissimilarity between two unstructured 3D point clouds is a
challenging task, with existing metrics often relying on measuring the distance
between corresponding points that can be either inefficient or ineffective. In
this paper, we propose a novel distance metric called Calibrated Local Geometry
Distance (CLGD), which computes the difference between the underlying 3D
surfaces calibrated and induced by a set of reference points. By associating
each reference point with two given point clouds through computing its
directional distances to them, the difference in directional distances of an
identical reference point characterizes the geometric difference between a
typical local region of the two point clouds. Finally, CLGD is obtained by
averaging the directional distance differences of all reference points. We
evaluate CLGD on various optimization and unsupervised learning-based tasks,
including shape reconstruction, rigid registration, scene flow estimation, and
feature representation. Extensive experiments show that CLGD achieves
significantly higher accuracy under all tasks in a memory and computationally
efficient manner, compared with existing metrics. As a generic metric, CLGD has
the potential to advance 3D point cloud modeling. The source code is publicly
available at https://github.com/rsy6318/CLGD.",None,-1
4641647a-0731-4d57-b8a8-f56136d0ed9d,Together We Make Sense -- Learning Meta-Sense Embeddings from Pretrained Static Sense Embeddings,0.078704,1,"Sense embedding learning methods learn multiple vectors for a given ambiguous
word, corresponding to its different word senses. For this purpose, different
methods have been proposed in prior work on sense embedding learning that use
different sense inventories, sense-tagged corpora and learning methods.
However, not all existing sense embeddings cover all senses of ambiguous words
equally well due to the discrepancies in their training resources. To address
this problem, we propose the first-ever meta-sense embedding method --
Neighbour Preserving Meta-Sense Embeddings, which learns meta-sense embeddings
by combining multiple independently trained source sense embeddings such that
the sense neighbourhoods computed from the source embeddings are preserved in
the meta-embedding space. Our proposed method can combine source sense
embeddings that cover different sets of word senses. Experimental results on
Word Sense Disambiguation (WSD) and Word-in-Context (WiC) tasks show that the
proposed meta-sense embedding method consistently outperforms several
competitive baselines.",None,-1
611c4edd-2ada-42da-bd6d-2889aadbb062,On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective,0.990781,166,"ChatGPT is a recent chatbot service released by OpenAI and is receiving
increasing attention over the past few months. While evaluations of various
aspects of ChatGPT have been done, its robustness, i.e., the performance to
unexpected inputs, is still unclear to the public. Robustness is of particular
concern in responsible AI, especially for safety-critical applications. In this
paper, we conduct a thorough evaluation of the robustness of ChatGPT from the
adversarial and out-of-distribution (OOD) perspective. To do so, we employ the
AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart
review and DDXPlus medical diagnosis datasets for OOD evaluation. We select
several popular foundation models as baselines. Results show that ChatGPT shows
consistent advantages on most adversarial and OOD classification and
translation tasks. However, the absolute performance is far from perfection,
which suggests that adversarial and OOD robustness remains a significant threat
to foundation models. Moreover, ChatGPT shows astounding performance in
understanding dialogue-related texts and we find that it tends to provide
informal suggestions for medical tasks instead of definitive answers. Finally,
we present in-depth discussions of possible research directions.",None,-1
2fbde41c-5b4d-4080-bed7-df5873e27150,XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters,0.814564,46,"In recent years, pre-trained language models have undergone rapid development
with the emergence of large-scale models. However, there is a lack of
open-sourced chat models specifically designed for the Chinese language,
especially in the field of Chinese finance, at the scale of hundreds of
billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese
chat model to date, built upon the BLOOM-176B architecture. Additionally, we
propose a novel training method called hybrid-tuning to mitigate catastrophic
forgetting. By combining general-domain with domain-specific knowledge and
integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable
of providing accurate and contextually appropriate responses in the Chinese
financial domain.",None,-1
78f83136-9f37-484a-95bd-fc6844fb75b4,SCL(FOL) Can Simulate Non-Redundant Superposition Clause Learning,0.0407158,1,"We show that SCL(FOL) can simulate the derivation of non-redundant clauses by
superposition for first-order logic without equality. Superposition-based
reasoning is performed with respect to a fixed reduction ordering. The
completeness proof of superposition relies on the grounding of the clause set.
It builds a ground partial model according to the fixed ordering, where minimal
false ground instances of clauses then trigger non-redundant superposition
inferences. We define a respective strategy for the SCL calculus such that
clauses learned by SCL and superposition inferences coincide. From this
perspective the SCL calculus can be viewed as a generalization of the
superposition calculus.",None,-1
c97620f7-de25-4a41-8711-43b6bcd54b06,"Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5",0.699482,3,"The use of large language models (LLMs) in healthcare is gaining popularity,
but their practicality and safety in clinical settings have not been thoroughly
assessed. In high-stakes environments like medical settings, trust and safety
are critical issues for LLMs. To address these concerns, we present an approach
to evaluate the performance and trustworthiness of a GPT3.5 model for medical
image protocol assignment. We compare it with a fine-tuned BERT model and a
radiologist. In addition, we have a radiologist review the GPT3.5 output to
evaluate its decision-making process. Our evaluation dataset consists of 4,700
physician entries across 11 imaging protocol classes spanning the entire head.
Our findings suggest that the GPT3.5 performance falls behind BERT and a
radiologist. However, GPT3.5 outperforms BERT in its ability to explain its
decision, detect relevant word indicators, and model calibration. Furthermore,
by analyzing the explanations of GPT3.5 for misclassifications, we reveal
systematic errors that need to be resolved to enhance its safety and
suitability for clinical use.",None,-1
7d04af9f-8b91-4082-942d-e9222f8987c5,PU-EdgeFormer: Edge Transformer for Dense Prediction in Point Cloud Upsampling,0.0756398,1,"Despite the recent development of deep learning-based point cloud upsampling,
most MLP-based point cloud upsampling methods have limitations in that it is
difficult to train the local and global structure of the point cloud at the
same time. To solve this problem, we present a combined graph convolution and
transformer for point cloud upsampling, denoted by PU-EdgeFormer. The proposed
method constructs EdgeFormer unit that consists of graph convolution and
multi-head self-attention modules. We employ graph convolution using EdgeConv,
which learns the local geometry and global structure of point cloud better than
existing point-to-feature method. Through in-depth experiments, we confirmed
that the proposed method has better point cloud upsampling performance than the
existing state-of-the-art method in both subjective and objective aspects. The
code is available at https://github.com/dohoon2045/PU-EdgeFormer.",None,-1
14390025-5d8f-49ab-8f0b-0111db5614e6,Relevance Feedback with Brain Signals,0.562997,1,"The Relevance Feedback (RF) process relies on accurate and real-time
relevance estimation of feedback documents to improve retrieval performance.
Since collecting explicit relevance annotations imposes an extra burden on the
user, extensive studies have explored using pseudo-relevance signals and
implicit feedback signals as substitutes. However, such signals are indirect
indicators of relevance and suffer from complex search scenarios where user
interactions are absent or biased.
  Recently, the advances in portable and high-precision brain-computer
interface (BCI) devices have shown the possibility to monitor user's brain
activities during search process. Brain signals can directly reflect user's
psychological responses to search results and thus it can act as additional and
unbiased RF signals. To explore the effectiveness of brain signals in the
context of RF, we propose a novel RF framework that combines BCI-based
relevance feedback with pseudo-relevance signals and implicit signals to
improve the performance of document re-ranking. The experimental results on the
user study dataset show that incorporating brain signals leads to significant
performance improvement in our RF framework. Besides, we observe that brain
signals perform particularly well in several hard search scenarios, especially
when implicit signals as feedback are missing or noisy. This reveals when and
how to exploit brain signals in the context of RF.",None,-1
da82b35c-3008-4d6c-89d6-3f371aac5a32,Progressively Optimized Local Radiance Fields for Robust View Synthesis,0.852064,58,"We present an algorithm for reconstructing the radiance field of a
large-scale scene from a single casually captured video. The task poses two
core challenges. First, most existing radiance field reconstruction approaches
rely on accurate pre-estimated camera poses from Structure-from-Motion
algorithms, which frequently fail on in-the-wild videos. Second, using a
single, global radiance field with finite representational capacity does not
scale to longer trajectories in an unbounded scene. For handling unknown poses,
we jointly estimate the camera poses with radiance field in a progressive
manner. We show that progressive optimization significantly improves the
robustness of the reconstruction. For handling large unbounded scenes, we
dynamically allocate new local radiance fields trained with frames within a
temporal window. This further improves robustness (e.g., performs well even
under moderate pose drifts) and allows us to scale to large scenes. Our
extensive evaluation on the Tanks and Temples dataset and our collected outdoor
dataset, Static Hikes, show that our approach compares favorably with the
state-of-the-art.",None,-1
83328c3d-4b31-4588-ac5b-8fa1c3c91d60,Gaussian Latent Representations for Uncertainty Estimation using Mahalanobis Distance in Deep Classifiers,0.317834,2,"Recent works show that the data distribution in a network's latent space is
useful for estimating classification uncertainty and detecting
Out-of-distribution (OOD) samples. To obtain a well-regularized latent space
that is conducive for uncertainty estimation, existing methods bring in
significant changes to model architectures and training procedures. In this
paper, we present a lightweight, fast, and high-performance regularization
method for Mahalanobis distance-based uncertainty prediction, and that requires
minimal changes to the network's architecture. To derive Gaussian latent
representation favourable for Mahalanobis Distance calculation, we introduce a
self-supervised representation learning method that separates in-class
representations into multiple Gaussians. Classes with non-Gaussian
representations are automatically identified and dynamically clustered into
multiple new classes that are approximately Gaussian. Evaluation on standard
OOD benchmarks shows that our method achieves state-of-the-art results on OOD
detection with minimal inference time, and is very competitive on predictive
probability calibration. Finally, we show the applicability of our method to a
real-life computer vision use case on microorganism classification.",None,-1
bbc536b0-3c5b-4020-b915-7d7b1078a113,Contextual Object Detection with Multimodal Large Language Models,0.855345,36,"Recent Multimodal Large Language Models (MLLMs) are remarkable in
vision-language tasks, such as image captioning and question answering, but
lack the essential perception ability, i.e., object detection. In this work, we
address this limitation by introducing a novel research problem of contextual
object detection -- understanding visible objects within different human-AI
interactive contexts. Three representative scenarios are investigated,
including the language cloze test, visual captioning, and question answering.
Moreover, we present ContextDET, a unified multimodal model that is capable of
end-to-end differentiable modeling of visual-language contexts, so as to
locate, identify, and associate visual objects with language inputs for
human-AI interaction. Our ContextDET involves three key submodels: (i) a visual
encoder for extracting visual representations, (ii) a pre-trained LLM for
multimodal context decoding, and (iii) a visual decoder for predicting bounding
boxes given contextual object words. The new generate-then-detect framework
enables us to detect object words within human vocabulary. Extensive
experiments show the advantages of ContextDET on our proposed CODE benchmark,
open-vocabulary detection, and referring image segmentation. Github:
https://github.com/yuhangzang/ContextDET.",None,-1
738b281f-0770-4f84-90d2-3e0e46097059,Vehicle-Infrastructure Cooperative 3D Object Detection via Feature Flow Prediction,0.967612,17,"Cooperatively utilizing both ego-vehicle and infrastructure sensor data can
significantly enhance autonomous driving perception abilities. However,
temporal asynchrony and limited wireless communication in traffic environments
can lead to fusion misalignment and impact detection performance. This paper
proposes Feature Flow Net (FFNet), a novel cooperative detection framework that
uses a feature flow prediction module to address these issues in
vehicle-infrastructure cooperative 3D object detection. Rather than
transmitting feature maps extracted from still-images, FFNet transmits feature
flow, which leverages the temporal coherence of sequential infrastructure
frames to predict future features and compensate for asynchrony. Additionally,
we introduce a self-supervised approach to enable FFNet to generate feature
flow with feature prediction ability. Experimental results demonstrate that our
proposed method outperforms existing cooperative detection methods while
requiring no more than 1/10 transmission cost of raw data on the DAIR-V2X
dataset when temporal asynchrony exceeds 200$ms$. The code is available at
\href{https://github.com/haibao-yu/FFNet-VIC3D}{https://github.com/haibao-yu/FFNet-VIC3D}.",None,-1
bda6f0c6-55d1-4f9b-b3e0-fbfce7750201,Conceptual Views on Tree Ensemble Classifiers,0.334503,3,"Random Forests and related tree-based methods are popular for supervised
learning from table based data. Apart from their ease of parallelization, their
classification performance is also superior. However, this performance,
especially parallelizability, is offset by the loss of explainability.
Statistical methods are often used to compensate for this disadvantage. Yet,
their ability for local explanations, and in particular for global
explanations, is limited. In the present work we propose an algebraic method,
rooted in lattice theory, for the (global) explanation of tree ensembles. In
detail, we introduce two novel conceptual views on tree ensemble classifiers
and demonstrate their explanatory capabilities on Random Forests that were
trained with standard parameters.",None,-1
3c9e903e-3a6c-4d67-b3c5-792da4aae2a8,Pure Monte Carlo Counterfactual Regret Minimization,0.859653,3,"Counterfactual Regret Minimization (CFR) and its variants are the best
algorithms so far for solving large-scale incomplete information games.
However, we believe that there are two problems with CFR: First, matrix
multiplication is required in CFR iteration, and the time complexity of one
iteration is too high; Secondly, the game characteristics in the real world are
different. Just using one CFR algorithm will not be perfectly suitable for all
game problems.
  For these two problems, this paper proposes a new algorithm called Pure CFR
(PCFR) based on CFR. PCFR can be seen as a combination of CFR and Fictitious
Play (FP), inheriting the concept of counterfactual regret (value) from CFR,
and using the best response strategy instead of the regret matching strategy
for the next iteration. This algorithm has three advantages. First, PCFR can be
combined with any CFR variant. The resulting Pure MCCFR (PMCCFR) can
significantly reduce the time and space complexity of one iteration. Secondly,
our experiments show that the convergence speed of the PMCCFR is 2$\sim$3 times
that of the MCCFR. Finally, there is a type of game that is very suitable for
PCFR. We call this type of game clear-game, which is characterized by a high
proportion of dominated strategies. Experiments show that in clear-game, the
convergence rate of PMCCFR is two orders of magnitude higher than that of
MCCFR.",None,-1
235e7826-06b4-4aff-b002-b905ebdcde9b,Estimating the Causal Effect of Early ArXiving on Paper Acceptance,0.373347,3,"What is the effect of releasing a preprint of a paper before it is submitted
for peer review? No randomized controlled trial has been conducted, so we turn
to observational data to answer this question. We use data from the ICLR
conference (2018--2022) and apply methods from causal inference to estimate the
effect of arXiving a paper before the reviewing period (early arXiving) on its
acceptance to the conference. Adjusting for confounders such as topic, authors,
and quality, we may estimate the causal effect. However, since quality is a
challenging construct to estimate, we use the negative outcome control method,
using paper citation count as a control variable to debias the quality
confounding effect. Our results suggest that early arXiving may have a small
effect on a paper's chances of acceptance. However, this effect (when existing)
does not differ significantly across different groups of authors, as grouped by
author citation count and institute rank. This suggests that early arXiving
does not provide an advantage to any particular group.",None,-1
d933e028-9874-47c6-942e-1db42d375a72,PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking,0.999102,35,"We introduce PointOdyssey, a large-scale synthetic dataset, and data
generation framework, for the training and evaluation of long-term fine-grained
tracking algorithms. Our goal is to advance the state-of-the-art by placing
emphasis on long videos with naturalistic motion. Toward the goal of
naturalism, we animate deformable characters using real-world motion capture
data, we build 3D scenes to match the motion capture environments, and we
render camera viewpoints using trajectories mined via structure-from-motion on
real videos. We create combinatorial diversity by randomizing character
appearance, motion profiles, materials, lighting, 3D assets, and atmospheric
effects. Our dataset currently includes 104 videos, averaging 2,000 frames
long, with orders of magnitude more correspondence annotations than prior work.
We show that existing methods can be trained from scratch in our dataset and
outperform the published variants. Finally, we introduce modifications to the
PIPs point tracking method, greatly widening its temporal receptive field,
which improves its performance on PointOdyssey as well as on two real-world
benchmarks. Our data and code are publicly available at:
https://pointodyssey.com",None,-1
e7575bf0-629a-4775-b45c-5120937c3a9d,Semantic Compression With Large Language Models,0.485915,14,"The rise of large language models (LLMs) is revolutionizing information
retrieval, question answering, summarization, and code generation tasks.
However, in addition to confidently presenting factually inaccurate information
at times (known as ""hallucinations""), LLMs are also inherently limited by the
number of input and output tokens that can be processed at once, making them
potentially less effective on tasks that require processing a large set or
continuous stream of information. A common approach to reducing the size of
data is through lossless or lossy compression. Yet, in some cases it may not be
strictly necessary to perfectly recover every detail from the original data, as
long as a requisite level of semantic precision or intent is conveyed.
  This paper presents three contributions to research on LLMs. First, we
present the results from experiments exploring the viability of approximate
compression using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT
interfaces. Second, we investigate and quantify the capability of LLMs to
compress text and code, as well as to recall and manipulate compressed
representations of prompts. Third, we present two novel metrics -- Exact
Reconstructive Effectiveness (ERE) and Semantic Reconstruction Effectiveness
(SRE) -- that quantify the level of preserved intent between text compressed
and decompressed by the LLMs we studied. Our initial results indicate that
GPT-4 can effectively compress and reconstruct text while preserving the
semantic essence of the original text, providing a path to leverage
$\sim$5$\times$ more tokens than present limits allow.",None,-1
1b5aab62-d1ab-4b00-a834-fb937c32a1f8,GAM : Gradient Attention Module of Optimization for Point Clouds Analysis,0.646464,4,"In point cloud analysis tasks, the existing local feature aggregation
descriptors (LFAD) are unable to fully utilize information in the neighborhood
of central points. Previous methods rely solely on Euclidean distance to
constrain the local aggregation process, which can be easily affected by
abnormal points and cannot adequately fit with the original geometry of the
point cloud. We believe that fine-grained geometric information (FGGI) is
significant for the aggregation of local features. Therefore, we propose a
gradient-based local attention module, termed as Gradient Attention Module
(GAM), to address the aforementioned problem. Our proposed GAM simplifies the
process that extracts gradient information in the neighborhood and uses the
Zenith Angle matrix and Azimuth Angle matrix as explicit representation, which
accelerates the module by 35X. Comprehensive experiments were conducted on five
benchmark datasets to demonstrate the effectiveness and generalization
capability of the proposed GAM for 3D point cloud analysis. Especially on S3DIS
dataset, GAM achieves the best performance among current point-based models
with mIoU/OA/mAcc of 74.4%/90.6%/83.2%, respectively.",None,-1
5c91001f-6685-441d-923b-d7efacd9c901,Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting,0.925944,66,"Large language models (LLMs) demonstrate impressive multilingual capability,
but their performance varies substantially across different languages. In this
work, we introduce a simple yet effective method, called cross-lingual-thought
prompting (XLT), to systematically improve the multilingual capability of LLMs.
Specifically, XLT is a generic template prompt that stimulates cross-lingual
and logical reasoning skills to enhance task performance across languages. We
conduct comprehensive evaluations on 7 typical benchmarks related to reasoning,
understanding, and generation tasks, covering both high-resource and
low-resource languages. Experimental results show that XLT not only remarkably
enhances the performance of various multilingual tasks but also significantly
reduces the gap between the average performance and the best performance of
each task in different languages. Notably, XLT brings over 10 points of average
improvement in arithmetic reasoning and open-domain question-answering tasks.",None,-1
b719fdd5-ca5e-4aaf-9232-041d2f5964c7,iDML: Incentivized Decentralized Machine Learning,0.313745,3,"With the rising emergence of decentralized and opportunistic approaches to
machine learning, end devices are increasingly tasked with training deep
learning models on-devices using crowd-sourced data that they collect
themselves. These approaches are desirable from a resource consumption
perspective and also from a privacy preservation perspective. When the devices
benefit directly from the trained models, the incentives are implicit -
contributing devices' resources are incentivized by the availability of the
higher-accuracy model that results from collaboration. However, explicit
incentive mechanisms must be provided when end-user devices are asked to
contribute their resources (e.g., computation, communication, and data) to a
task performed primarily for the benefit of others, e.g., training a model for
a task that a neighbor device needs but the device owner is uninterested in. In
this project, we propose a novel blockchain-based incentive mechanism for
completely decentralized and opportunistic learning architectures. We leverage
a smart contract not only for providing explicit incentives to end devices to
participate in decentralized learning but also to create a fully decentralized
mechanism to inspect and reflect on the behavior of the learning architecture.",None,-1
2c877e2b-fe89-4297-83be-3598f3ed357c,When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP,0.113399,5,"Despite its crucial role in research experiments, code correctness is often
presumed only on the basis of the perceived quality of results. This assumption
comes with the risk of erroneous outcomes and potentially misleading findings.
To address this issue, we posit that the current focus on reproducibility
should go hand in hand with the emphasis on software quality. We present a case
study in which we identify and fix three bugs in widely used implementations of
the state-of-the-art Conformer architecture. Through experiments on speech
recognition and translation in various languages, we demonstrate that the
presence of bugs does not prevent the achievement of good and reproducible
results, which however can lead to incorrect conclusions that potentially
misguide future research. As a countermeasure, we propose a Code-quality
Checklist and release pangoliNN, a library dedicated to testing neural models,
with the goal of promoting coding best practices and improving research
software quality within the NLP community.",None,-1
eb423407-f085-4f8f-a3a9-7f90b40e9229,PassGPT: Password Modeling and (Guided) Generation with Large Language Models,0.476516,2,"Large language models (LLMs) successfully model natural language from vast
amounts of text without the need for explicit supervision. In this paper, we
investigate the efficacy of LLMs in modeling passwords. We present PassGPT, a
LLM trained on password leaks for password generation. PassGPT outperforms
existing methods based on generative adversarial networks (GAN) by guessing
twice as many previously unseen passwords. Furthermore, we introduce the
concept of guided password generation, where we leverage PassGPT sampling
procedure to generate passwords matching arbitrary constraints, a feat lacking
in current GAN-based strategies. Lastly, we conduct an in-depth analysis of the
entropy and probability distribution that PassGPT defines over passwords and
discuss their use in enhancing existing password strength estimators.",None,-1
bf3efa89-d99e-4c67-b75c-123eec820c74,Multi-Architecture Multi-Expert Diffusion Models,0.210236,12,"In this paper, we address the performance degradation of efficient diffusion
models by introducing Multi-architecturE Multi-Expert diffusion models (MEME).
We identify the need for tailored operations at different time-steps in
diffusion processes and leverage this insight to create compact yet
high-performing models. MEME assigns distinct architectures to different
time-step intervals, balancing convolution and self-attention operations based
on observed frequency characteristics. We also introduce a soft interval
assignment strategy for comprehensive training. Empirically, MEME operates 3.3
times faster than baselines while improving image generation quality (FID
scores) by 0.62 (FFHQ) and 0.37 (CelebA). Though we validate the effectiveness
of assigning more optimal architecture per time-step, where efficient models
outperform the larger models, we argue that MEME opens a new design choice for
diffusion models that can be easily applied in other scenarios, such as large
multi-expert models.",None,-1
814aecbd-b7fc-43d1-8ecd-6e71e8463323,Semi-supervised Parametric Real-world Image Harmonization,0.783875,14,"Learning-based image harmonization techniques are usually trained to undo
synthetic random global transformations applied to a masked foreground in a
single ground truth photo. This simulated data does not model many of the
important appearance mismatches (illumination, object boundaries, etc.) between
foreground and background in real composites, leading to models that do not
generalize well and cannot model complex local changes. We propose a new
semi-supervised training strategy that addresses this problem and lets us learn
complex local appearance harmonization from unpaired real composites, where
foreground and background come from different images. Our model is fully
parametric. It uses RGB curves to correct the global colors and tone and a
shading map to model local variations. Our method outperforms previous work on
established benchmarks and real composites, as shown in a user study, and
processes high-resolution images interactively.",None,-1
34d88ae1-ab73-4101-9487-c7914168e312,Intention-Aware Decision-Making for Mixed Intersection Scenarios,0.670909,4,"This paper presents a white-box intention-aware decision-making for the
handling of interactions between a pedestrian and an automated vehicle (AV) in
an unsignalized street crossing scenario. Moreover, a design framework has been
developed, which enables automated parameterization of the decision-making.
This decision-making is designed in such a manner that it can understand
pedestrians in urban traffic and can react accordingly to their intentions.
That way, a human-like response to the actions of the pedestrian is ensured,
leading to a higher acceptance of AVs. The core notion of this paper is that
the intention prediction of the pedestrian to cross the street and
decision-making are divided into two subsystems. On the one hand, the intention
detection is a data-driven, black-box model. Thus, it can model the complex
behavior of the pedestrians. On the other hand, the decision-making is a
white-box model to ensure traceability and to enable a rapid verification and
validation of AVs. This white-box decision-making provides human-like behavior
and a guaranteed prevention of deadlocks. An additional benefit is that the
proposed decision-making requires low computational resources only enabling
real world usage. The automated parameterization uses a particle swarm
optimization and compares two different models of the pedestrian: The social
force model and the Markov decision process model. Consequently, a rapid design
of the decision-making is possible and different pedestrian behaviors can be
taken into account. The results reinforce the applicability of the proposed
intention-aware decision-making.",None,-1
2561f2d4-73ee-4395-98f5-33efb51a4949,Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models,0.998772,41,"We present the first framework to solve linear inverse problems leveraging
pre-trained latent diffusion models. Previously proposed algorithms (such as
DPS and DDRM) only apply to pixel-space diffusion models. We theoretically
analyze our algorithm showing provable sample recovery in a linear model
setting. The algorithmic insight obtained from our analysis extends to more
general settings often considered in practice. Experimentally, we outperform
previously proposed posterior sampling algorithms in a wide variety of problems
including random inpainting, block inpainting, denoising, deblurring,
destriping, and super-resolution.",None,-1
96974dab-c9e1-48db-aec1-a717b75bd24a,Smooth Non-Stationary Bandits,0.611182,5,"In many applications of online decision making, the environment is
non-stationary and it is therefore crucial to use bandit algorithms that handle
changes. Most existing approaches are designed to protect against non-smooth
changes, constrained only by total variation or Lipschitzness over time, where
they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice
environments are often changing {\bf smoothly}, so such algorithms may incur
higher-than-necessary regret in these settings and do not leverage information
on the rate of change. We study a non-stationary two-armed bandits problem
where we assume that an arm's mean reward is a $\beta$-H\""older function over
(normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously
differentiable. We show the first separation between the smooth and non-smooth
regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$.
We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound
for any integer $\beta\ge 1$, which matches our upper bound for $\beta=2$.",None,-1
280d2ea4-4092-4c16-a8e6-b5353e56b050,Markov Decision Processes with Time-Varying Geometric Discounting,0.185081,1,"Canonical models of Markov decision processes (MDPs) usually consider
geometric discounting based on a constant discount factor. While this standard
modeling approach has led to many elegant results, some recent studies indicate
the necessity of modeling time-varying discounting in certain applications.
This paper studies a model of infinite-horizon MDPs with time-varying discount
factors. We take a game-theoretic perspective -- whereby each time step is
treated as an independent decision maker with their own (fixed) discount factor
-- and we study the subgame perfect equilibrium (SPE) of the resulting game as
well as the related algorithmic problems. We present a constructive proof of
the existence of an SPE and demonstrate the EXPTIME-hardness of computing an
SPE. We also turn to the approximate notion of $\epsilon$-SPE and show that an
$\epsilon$-SPE exists under milder assumptions. An algorithm is presented to
compute an $\epsilon$-SPE, of which an upper bound of the time complexity, as a
function of the convergence property of the time-varying discount factor, is
provided.",None,-1
e96b92e4-b0d1-4f91-8e09-037d79b299db,StyleGenes: Discrete and Efficient Latent Distributions for GANs,0.0787412,1,"We propose a discrete latent distribution for Generative Adversarial Networks
(GANs). Instead of drawing latent vectors from a continuous prior, we sample
from a finite set of learnable latents. However, a direct parametrization of
such a distribution leads to an intractable linear increase in memory in order
to ensure sufficient sample diversity. We address this key issue by taking
inspiration from the encoding of information in biological organisms. Instead
of learning a separate latent vector for each sample, we split the latent space
into a set of genes. For each gene, we train a small bank of gene variants.
Thus, by independently sampling a variant for each gene and combining them into
the final latent vector, our approach can represent a vast number of unique
latent samples from a compact set of learnable parameters. Interestingly, our
gene-inspired latent encoding allows for new and intuitive approaches to
latent-space exploration, enabling conditional sampling from our
unconditionally trained model. Moreover, our approach preserves
state-of-the-art photo-realism while achieving better disentanglement than the
widely-used StyleMapping network.",None,-1
d1c6327a-c1bb-47f2-b6c3-258d6f541b58,Impact of translation on biomedical information extraction from real-life clinical notes,0.54724,1,"The objective of our study is to determine whether using English tools to
extract and normalize French medical concepts on translations provides
comparable performance to French models trained on a set of annotated French
clinical notes. We compare two methods: a method involving French language
models and a method involving English language models. For the native French
method, the Named Entity Recognition (NER) and normalization steps are
performed separately. For the translated English method, after the first
translation step, we compare a two-step method and a terminology-oriented
method that performs extraction and normalization at the same time. We used
French, English and bilingual annotated datasets to evaluate all steps (NER,
normalization and translation) of our algorithms. Concerning the results, the
native French method performs better than the translated English one with a
global f1 score of 0.51 [0.47;0.55] against 0.39 [0.34;0.44] and 0.38
[0.36;0.40] for the two English methods tested. In conclusion, despite the
recent improvement of the translation models, there is a significant
performance difference between the two approaches in favor of the native French
method which is more efficient on French medical texts, even with few annotated
documents.",None,-1
9a5cd29e-5777-4a3d-962c-8f3bdcc9b39e,"Pre-train, Adapt and Detect: Multi-Task Adapter Tuning for Camouflaged Object Detection",0.0558728,1,"Camouflaged object detection (COD), aiming to segment camouflaged objects
which exhibit similar patterns with the background, is a challenging task. Most
existing works are dedicated to establishing specialized modules to identify
camouflaged objects with complete and fine details, while the boundary can not
be well located for the lack of object-related semantics. In this paper, we
propose a novel ``pre-train, adapt and detect"" paradigm to detect camouflaged
objects. By introducing a large pre-trained model, abundant knowledge learned
from massive multi-modal data can be directly transferred to COD. A lightweight
parallel adapter is inserted to adjust the features suitable for the downstream
COD task. Extensive experiments on four challenging benchmark datasets
demonstrate that our method outperforms existing state-of-the-art COD models by
large margins. Moreover, we design a multi-task learning scheme for tuning the
adapter to exploit the shareable knowledge across different semantic classes.
Comprehensive experimental results showed that the generalization ability of
our model can be substantially improved with multi-task adapter initialization
on source tasks and multi-task adaptation on target tasks.",None,-1
02e84961-3f15-4aee-af29-f1c539cf2534,Explaining CLIP through Co-Creative Drawings and Interaction,0.436432,2,"This paper analyses a visual archive of drawings produced by an interactive
robotic art installation where audience members narrated their dreams into a
system powered by CLIPdraw deep learning (DL) model that interpreted and
transformed their dreams into images. The resulting archive of prompt-image
pairs were examined and clustered based on concept representation accuracy. As
a result of the analysis, the paper proposes four groupings for describing and
explaining CLIP-generated results: clear concept, text-to-text as image,
indeterminacy and confusion, and lost in translation. This article offers a
glimpse into a collection of dreams interpreted, mediated and given form by
Artificial Intelligence (AI), showcasing oftentimes unexpected, visually
compelling or, indeed, the dream-like output of the system, with the emphasis
on processes and results of translations between languages, sign-systems and
various modules of the installation. In the end, the paper argues that proposed
clusters support better understanding of the neural model.",None,-1
050ecfef-44b5-4b87-b414-67b8f8b31002,Active RIS-aided EH-NOMA Networks: A Deep Reinforcement Learning Approach,0.257679,2,"An active reconfigurable intelligent surface (RIS)-aided multi-user downlink
communication system is investigated, where non-orthogonal multiple access
(NOMA) is employed to improve spectral efficiency, and the active RIS is
powered by energy harvesting (EH). The problem of joint control of the RIS's
amplification matrix and phase shift matrix is formulated to maximize the
communication success ratio with considering the quality of service (QoS)
requirements of users, dynamic communication state, and dynamic available
energy of RIS. To tackle this non-convex problem, a cascaded deep learning
algorithm namely long short-term memory-deep deterministic policy gradient
(LSTM-DDPG) is designed. First, an advanced LSTM based algorithm is developed
to predict users' dynamic communication state. Then, based on the prediction
results, a DDPG based algorithm is proposed to joint control the amplification
matrix and phase shift matrix of the RIS. Finally, simulation results verify
the accuracy of the prediction of the proposed LSTM algorithm, and demonstrate
that the LSTM-DDPG algorithm has a significant advantage over other benchmark
algorithms in terms of communication success ratio performance.",None,-1
0c3803a5-a66d-455b-ae51-3125c278666a,AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes,0.698791,5,"We propose attribute-aware multimodal entity linking, where the input is a
mention described with a text and image, and the goal is to predict the
corresponding target entity from a multimodal knowledge base (KB) where each
entity is also described with a text description, a visual image and a set of
attributes and values. To support this research, we construct AMELI, a
large-scale dataset consisting of 18,472 reviews and 35,598 products. To
establish baseline performance on AMELI, we experiment with the current
state-of-the-art multimodal entity linking approaches and our enhanced
attribute-aware model and demonstrate the importance of incorporating the
attribute information into the entity linking process. To be best of our
knowledge, we are the first to build benchmark dataset and solutions for the
attribute-aware multimodal entity linking task. Datasets and codes will be made
publicly available.",None,-1
9f813322-73cd-41d7-ad1d-d1848a95e692,Narrative as a Dynamical System,0.0940448,1,"There is increasing evidence that human activity in general, and narrative in
particular, can be treated as a dynamical system in the physics sense; a system
whose evolution is described by an action integral, such that the average of
all possible paths from point A to point B is given by the extremum of the
action. We create by construction three such paths by averaging about 500
different narratives, and we show that the average path is consistent with an
action principle.",None,-1
cea89fb8-1044-454a-869d-9f383fc762fd,Complex Mixer for MedMNIST Classification Decathlon,0.189127,2,"With the development of the medical image field, researchers seek to develop
a class of datasets to block the need for medical knowledge, such as
\text{MedMNIST} (v2). MedMNIST (v2) includes a large number of small-sized (28
$\times$ 28 or 28 $\times$ 28 $\times$ 28) medical samples and the
corresponding expert annotations (class label). The existing baseline model
(Google AutoML Vision, ResNet-50+3D) can reach an average accuracy of over 70\%
on MedMNIST (v2) datasets, which is comparable to the performance of expert
decision-making. Nevertheless, we note that there are two insurmountable
obstacles to modeling on MedMNIST (v2): 1) the raw images are cropped to low
scales may cause effective recognition information to be dropped and the
classifier to have difficulty in tracing accurate decision boundaries; 2) the
labelers' subjective insight may cause many uncertainties in the label space.
To address these issues, we develop a Complex Mixer (C-Mixer) with a
pre-training framework to alleviate the problem of insufficient information and
uncertainty in the label space by introducing an incentive imaginary matrix and
a self-supervised scheme with random masking. Our method (incentive learning
and self-supervised learning with masking) shows surprising potential on both
the standard MedMNIST (v2) dataset, the customized weakly supervised datasets,
and other image enhancement tasks.",None,-1
f0ea9543-faf6-41f4-b00d-96adbb47555d,An interpretability framework for Similar case matching,0.101782,1,"Similar Case Matching (SCM) plays a pivotal role in the legal system by
facilitating the efficient identification of similar cases for legal
professionals. While previous research has primarily concentrated on enhancing
the performance of SCM models, the aspect of interpretability has been
neglected. To bridge the gap, this study proposes an integrated pipeline
framework for interpretable SCM. The framework comprises four modules: judicial
feature sentence identification, case matching, feature sentence alignment, and
conflict resolution. In contrast to current SCM methods, our framework first
extracts feature sentences within a legal case that contain essential
information. Then it conducts case matching based on these extracted features.
Subsequently, our framework aligns the corresponding sentences in two legal
cases to provide evidence of similarity. In instances where the results of case
matching and feature sentence alignment exhibit conflicts, the conflict
resolution module resolves these inconsistencies. The experimental results show
the effectiveness of our proposed framework, establishing a new benchmark for
interpretable SCM.",None,-1
d748dee7-60e6-44ae-914c-c351029c3583,Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation,0.961397,40,"Large language models (LLMs) demonstrate remarkable machine translation (MT)
abilities via prompting, even though they were not explicitly trained for this
task. However, even given the incredible quantities of data they are trained
on, LLMs can struggle to translate inputs with rare words, which are common in
low resource or domain transfer scenarios. We show that LLM prompting can
provide an effective solution for rare words as well, by using prior knowledge
from bilingual dictionaries to provide control hints in the prompts. We propose
a novel method, DiPMT, that provides a set of possible translations for a
subset of the input words, thereby enabling fine-grained phrase-level prompted
control of the LLM. Extensive experiments show that DiPMT outperforms the
baseline both in low-resource MT, as well as for out-of-domain MT. We further
provide a qualitative analysis of the benefits and limitations of this
approach, including the overall level of controllability that is achieved.",None,-1
655db86e-20f8-4281-ab3e-5a9b8de4fb29,Chat Translation Error Detection for Assisting Cross-lingual Communications,0.357306,2,"In this paper, we describe the development of a communication support system
that detects erroneous translations to facilitate crosslingual communications
due to the limitations of current machine chat translation methods. We trained
an error detector as the baseline of the system and constructed a new
Japanese-English bilingual chat corpus, BPersona-chat, which comprises
multiturn colloquial chats augmented with crowdsourced quality ratings. The
error detector can serve as an encouraging foundation for more advanced
erroneous translation detection systems.",None,-1
4afe136f-0f50-462d-973b-6d4e3770eb37,Dual Aggregation Transformer for Image Super-Resolution,0.999533,48,"Transformer has recently gained considerable popularity in low-level vision
tasks, including image super-resolution (SR). These networks utilize
self-attention along different dimensions, spatial or channel, and achieve
impressive performance. This inspires us to combine the two dimensions in
Transformer for a more powerful representation capability. Based on the above
idea, we propose a novel Transformer model, Dual Aggregation Transformer (DAT),
for image SR. Our DAT aggregates features across spatial and channel
dimensions, in the inter-block and intra-block dual manner. Specifically, we
alternately apply spatial and channel self-attention in consecutive Transformer
blocks. The alternate strategy enables DAT to capture the global context and
realize inter-block feature aggregation. Furthermore, we propose the adaptive
interaction module (AIM) and the spatial-gate feed-forward network (SGFN) to
achieve intra-block feature aggregation. AIM complements two self-attention
mechanisms from corresponding dimensions. Meanwhile, SGFN introduces additional
non-linear spatial information in the feed-forward network. Extensive
experiments show that our DAT surpasses current methods. Code and models are
obtainable at https://github.com/zhengchen1999/DAT.",None,-1
646c3bd4-b6f4-4256-a881-9ced55f391a8,An Investigation of LLMs' Inefficacy in Understanding Converse Relations,0.237218,4,"Large Language Models (LLMs) have achieved remarkable success in many formal
language oriented tasks, such as structural data-to-text and semantic parsing.
However current benchmarks mostly follow the data distribution of the
pre-training data of LLMs. Therefore, a natural question rises that do LLMs
really understand the structured semantics of formal languages. In this paper,
we investigate this problem on a special case, converse binary relation. We
introduce a new benchmark ConvRe focusing on converse relations, which contains
17 relations and 1240 triples extracted from popular knowledge graph completion
datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are
formulated as multi-choice question answering to evaluate LLMs' ability to
determine the matching between relations and associated text. For the
evaluation protocol, apart from different prompting methods, we further
introduce variants to the test text and few-shot example text. We conduct
experiments on three popular LLM families and have observed various scaling
trends. The results suggest that LLMs often resort to shortcut learning and
still face challenges on our proposed benchmark.",None,-1
e66e6d24-3c96-48f9-9a81-ad9e57d041fc,AutoAD: Movie Description in Context,0.447745,18,"The objective of this paper is an automatic Audio Description (AD) model that
ingests movies and outputs AD in text form. Generating high-quality movie AD is
challenging due to the dependency of the descriptions on context, and the
limited amount of training data available. In this work, we leverage the power
of pretrained foundation models, such as GPT and CLIP, and only train a mapping
network that bridges the two models for visually-conditioned text generation.
In order to obtain high-quality AD, we make the following four contributions:
(i) we incorporate context from the movie clip, AD from previous clips, as well
as the subtitles; (ii) we address the lack of training data by pretraining on
large-scale datasets, where visual or contextual information is unavailable,
e.g. text-only AD without movies or visual captioning datasets without context;
(iii) we improve on the currently available AD datasets, by removing label
noise in the MAD dataset, and adding character naming information; and (iv) we
obtain strong results on the movie AD task compared with previous methods.",None,-1
91d11530-46ee-4fb0-bd4d-213e3f0bb548,PolyGNN: Polyhedron-based Graph Neural Network for 3D Building Reconstruction from Point Clouds,0.0825751,1,"We present PolyGNN, a polyhedron-based graph neural network for 3D building
reconstruction from point clouds. PolyGNN learns to assemble primitives
obtained by polyhedral decomposition via graph node classification, achieving a
watertight, compact, and weakly semantic reconstruction. To effectively
represent arbitrary-shaped polyhedra in the neural network, we propose three
different sampling strategies to select representative points as
polyhedron-wise queries, enabling efficient occupancy inference. Furthermore,
we incorporate the inter-polyhedron adjacency to enhance the classification of
the graph nodes. We also observe that existing city-building models are
abstractions of the underlying instances. To address this abstraction gap and
provide a fair evaluation of the proposed method, we develop our method on a
large-scale synthetic dataset covering 500k+ buildings with well-defined ground
truths of polyhedral class labels. We further conduct a transferability
analysis across cities and on real-world point clouds. Both qualitative and
quantitative results demonstrate the effectiveness of our method, particularly
its efficiency for large-scale reconstructions. The source code and data of our
work are available at https://github.com/chenzhaiyu/polygnn.",None,-1
6dc11160-4e2b-4be1-bc94-f971ecea63ef,"Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation",0.7539,3,"Interpreting ancient Chinese has been the key to comprehending vast Chinese
literature, tradition, and civilization. In this paper, we propose Erya for
ancient Chinese translation. From a dataset perspective, we collect, clean, and
classify ancient Chinese materials from various sources, forming the most
extensive ancient Chinese resource to date. From a model perspective, we devise
Erya training method oriented towards ancient Chinese. We design two
jointly-working tasks: disyllabic aligned substitution (DAS) and dual masked
language model (DMLM). From an evaluation perspective, we build a benchmark to
judge ancient Chinese translation quality in different scenarios and evaluate
the ancient Chinese translation capacities of various existing models. Our
model exhibits remarkable zero-shot performance across five domains, with over
+12.0 BLEU against GPT-3.5 models and better human evaluation results than
ERNIE Bot. Subsequent fine-tuning further shows the superior transfer
capability of Erya model with +6.2 BLEU gain. We release all the
above-mentioned resources at https://github.com/RUCAIBox/Erya.",None,-1
06e8c969-e172-4ed3-ba90-22917af15654,LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation,0.956689,26,"Empowering chatbots in the field of mental health is receiving increasing
amount of attention, while there still lacks exploration in developing and
evaluating chatbots in psychiatric outpatient scenarios. In this work, we focus
on exploring the potential of ChatGPT in powering chatbots for psychiatrist and
patient simulation. We collaborate with psychiatrists to identify objectives
and iteratively develop the dialogue system to closely align with real-world
scenarios. In the evaluation experiments, we recruit real psychiatrists and
patients to engage in diagnostic conversations with the chatbots, collecting
their ratings for assessment. Our findings demonstrate the feasibility of using
ChatGPT-powered chatbots in psychiatric scenarios and explore the impact of
prompt designs on chatbot behavior and user experience.",None,-1
7ff91b47-6503-4058-b899-7508872dc4d1,Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields,0.149891,3,"Neural radiance fields (NeRF) appeared recently as a powerful tool to
generate realistic views of objects and confined areas. Still, they face
serious challenges with open scenes, where the camera has unrestricted movement
and content can appear at any distance. In such scenarios, current
NeRF-inspired models frequently yield hazy or pixelated outputs, suffer slow
training times, and might display irregularities, because of the challenging
task of reconstructing an extensive scene from a limited number of images. We
propose a new framework to boost the performance of NeRF-based architectures
yielding significantly superior outcomes compared to the prior work. Our
solution overcomes several obstacles that plagued earlier versions of NeRF,
including handling multiple video inputs, selecting keyframes, and extracting
poses from real-world frames that are ambiguous and symmetrical. Furthermore,
we applied our framework, dubbed as ""Pre-NeRF 360"", to enable the use of the
Nutrition5k dataset in NeRF and introduce an updated version of this dataset,
known as the N5k360 dataset.",None,-1
51342b4a-2330-4dd7-bf7a-4c0d65b97c3a,Evaluating Large Language Models at Evaluating Instruction Following,0.505608,64,"As research in large language models (LLMs) continues to accelerate,
LLM-based evaluation has emerged as a scalable and cost-effective alternative
to human evaluations for comparing the ever increasing list of models. This
paper investigates the efficacy of these ``LLM evaluators'', particularly in
using them to assess instruction following, a metric that gauges how closely
generated text adheres to the given instruction. We introduce a challenging
meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM
evaluator in discerning instruction-following outputs. The authors manually
curated 419 pairs of outputs, one adhering to instructions while the other
diverging, yet may possess deceptive qualities that mislead an LLM evaluator,
e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover
that different evaluators (i.e., combinations of LLMs and prompts) exhibit
distinct performance on LLMBar and even the highest-scoring ones have
substantial room for improvement. We also present a novel suite of prompting
strategies that further close the gap between LLM and human evaluators. With
LLMBar, we hope to offer more insight into LLM evaluators and foster future
research in developing better instruction-following models.",None,-1
0976db13-164d-4cca-8097-33fbb67590f5,"Wireless Channel Charting: Theory, Practice, and Applications",0.623191,7,"Channel charting is a recently proposed framework that applies dimensionality
reduction to channel state information (CSI) in wireless systems with the goal
of associating a pseudo-position to each mobile user in a low-dimensional
space: the channel chart. Channel charting summarizes the entire CSI dataset in
a self-supervised manner, which opens up a range of applications that are tied
to user location. In this article, we introduce the theoretical underpinnings
of channel charting and present an overview of recent algorithmic developments
and experimental results obtained in the field. We furthermore discuss concrete
application examples of channel charting to network- and user-related
applications, and we provide a perspective on future developments and
challenges as well as the role of channel charting in next-generation wireless
networks.",None,-1
0cb72cf6-6c5d-4d99-a7d4-aecc686c8d67,"Syntax and Semantics Meet in the ""Middle"": Probing the Syntax-Semantics Interface of LMs Through Agentivity",0.417362,2,"Recent advances in large language models have prompted researchers to examine
their abilities across a variety of linguistic tasks, but little has been done
to investigate how models handle the interactions in meaning across words and
larger syntactic forms -- i.e. phenomena at the intersection of syntax and
semantics. We present the semantic notion of agentivity as a case study for
probing such interactions. We created a novel evaluation dataset by utilitizing
the unique linguistic properties of a subset of optionally transitive English
verbs. This dataset was used to prompt varying sizes of three model classes to
see if they are sensitive to agentivity at the lexical level, and if they can
appropriately employ these word-level priors given a specific syntactic
context. Overall, GPT-3 text-davinci-003 performs extremely well across all
experiments, outperforming all other models tested by far. In fact, the results
are even better correlated with human judgements than both syntactic and
semantic corpus statistics. This suggests that LMs may potentially serve as
more useful tools for linguistic annotation, theory testing, and discovery than
select corpora for certain tasks. Code is available at
https://github.com/lindiatjuatja/lm_sem",None,-1
e0044dce-6a16-4763-9538-c6655956fe5b,TextPainter: Multimodal Text Image Generation with Visual-harmony and Text-comprehension for Poster Design,0.168603,7,"Text design is one of the most critical procedures in poster design, as it
relies heavily on the creativity and expertise of humans to design text images
considering the visual harmony and text-semantic. This study introduces
TextPainter, a novel multimodal approach that leverages contextual visual
information and corresponding text semantics to generate text images.
Specifically, TextPainter takes the global-local background image as a hint of
style and guides the text image generation with visual harmony. Furthermore, we
leverage the language model and introduce a text comprehension module to
achieve both sentence-level and word-level style variations. Besides, we
construct the PosterT80K dataset, consisting of about 80K posters annotated
with sentence-level bounding boxes and text contents. We hope this dataset will
pave the way for further research on multimodal text image generation.
Extensive quantitative and qualitative experiments demonstrate that TextPainter
can generate visually-and-semantically-harmonious text images for posters.",None,-1
f3b3efca-d816-458c-887d-3f4c3c9bd67d,A Data Fusion Framework for Multi-Domain Morality Learning,0.774198,15,"Language models can be trained to recognize the moral sentiment of text,
creating new opportunities to study the role of morality in human life. As
interest in language and morality has grown, several ground truth datasets with
moral annotations have been released. However, these datasets vary in the
method of data collection, domain, topics, instructions for annotators, etc.
Simply aggregating such heterogeneous datasets during training can yield models
that fail to generalize well. We describe a data fusion framework for training
on multiple heterogeneous datasets that improve performance and
generalizability. The model uses domain adversarial training to align the
datasets in feature space and a weighted loss function to deal with label
shift. We show that the proposed framework achieves state-of-the-art
performance in different datasets compared to prior works in morality
inference.",None,-1
7f47211d-52e8-4106-8d28-218096f4d638,Fairlearn: Assessing and Improving Fairness of AI Systems,0.987112,27,"Fairlearn is an open source project to help practitioners assess and improve
fairness of artificial intelligence (AI) systems. The associated Python
library, also named fairlearn, supports evaluation of a model's output across
affected populations and includes several algorithms for mitigating fairness
issues. Grounded in the understanding that fairness is a sociotechnical
challenge, the project integrates learning resources that aid practitioners in
considering a system's broader societal context.",None,-1
cd895266-672d-405f-8488-0eb2a2889cc6,"Presence of informal language, such as emoticons, hashtags, and slang, impact the performance of sentiment analysis models on social media text?",0.150183,2,"This study aimed to investigate the influence of the presence of informal
language, such as emoticons and slang, on the performance of sentiment analysis
models applied to social media text. A convolutional neural network (CNN) model
was developed and trained on three datasets: a sarcasm dataset, a sentiment
dataset, and an emoticon dataset. The model architecture was held constant for
all experiments and the model was trained on 80% of the data and tested on 20%.
The results revealed that the model achieved an accuracy of 96.47% on the
sarcasm dataset, with the lowest accuracy for class 1. On the sentiment
dataset, the model achieved an accuracy of 95.28%. The amalgamation of sarcasm
and sentiment datasets improved the accuracy of the model to 95.1%, and the
addition of emoticon dataset has a slight positive impact on the accuracy of
the model to 95.37%. The study suggests that the presence of informal language
has a restricted impact on the performance of sentiment analysis models applied
to social media text. However, the inclusion of emoticon data to the model can
enhance the accuracy slightly.",None,-1
60991cc5-9476-4635-a238-c4069fe84b9c,Illumination Controllable Dehazing Network based on Unsupervised Retinex Embedding,0.152213,1,"On the one hand, the dehazing task is an illposedness problem, which means
that no unique solution exists. On the other hand, the dehazing task should
take into account the subjective factor, which is to give the user selectable
dehazed images rather than a single result. Therefore, this paper proposes a
multi-output dehazing network by introducing illumination controllable ability,
called IC-Dehazing. The proposed IC-Dehazing can change the illumination
intensity by adjusting the factor of the illumination controllable module,
which is realized based on the interpretable Retinex theory. Moreover, the
backbone dehazing network of IC-Dehazing consists of a Transformer with double
decoders for high-quality image restoration. Further, the prior-based loss
function and unsupervised training strategy enable IC-Dehazing to complete the
parameter learning process without the need for paired data. To demonstrate the
effectiveness of the proposed IC-Dehazing, quantitative and qualitative
experiments are conducted on image dehazing, semantic segmentation, and object
detection tasks. Code is available at
https://github.com/Xiaofeng-life/ICDehazing.",None,-1
3b63b9cc-587e-406c-9d6b-5b38fa49a143,UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective,0.33026,5,"We propose a new paradigm for universal information extraction (IE) that is
compatible with any schema format and applicable to a list of IE tasks, such as
named entity recognition, relation extraction, event extraction and sentiment
analysis. Our approach converts the text-based IE tasks as the token-pair
problem, which uniformly disassembles all extraction targets into joint span
detection, classification and association problems with a unified extractive
framework, namely UniEX. UniEX can synchronously encode schema-based prompt and
textual information, and collaboratively learn the generalized knowledge from
pre-defined information using the auto-encoder language models. We develop a
traffine attention mechanism to integrate heterogeneous factors including
tasks, labels and inside tokens, and obtain the extraction target via a scoring
matrix. Experiment results show that UniEX can outperform generative universal
IE models in terms of performance and inference-speed on $14$ benchmarks IE
datasets with the supervised setting. The state-of-the-art performance in
low-resource scenarios also verifies the transferability and effectiveness of
UniEX.",None,-1
30babebb-e99f-4bd8-b8ad-fb4b77ea8baa,Single-Trajectory Distributionally Robust Reinforcement Learning,0.240047,4,"As a framework for sequential decision-making, Reinforcement Learning (RL)
has been regarded as an essential component leading to Artificial General
Intelligence (AGI). However, RL is often criticized for having the same
training environment as the test one, which also hinders its application in the
real world. To mitigate this problem, Distributionally Robust RL (DRRL) is
proposed to improve the worst performance in a set of environments that may
contain the unknown test environment. Due to the nonlinearity of the robustness
goal, most of the previous work resort to the model-based approach, learning
with either an empirical distribution learned from the data or a simulator that
can be sampled infinitely, which limits their applications in simple dynamics
environments. In contrast, we attempt to design a DRRL algorithm that can be
trained along a single trajectory, i.e., no repeated sampling from a state.
Based on the standard Q-learning, we propose distributionally robust Q-learning
with the single trajectory (DRQ) and its average-reward variant named
differential DRQ. We provide asymptotic convergence guarantees and experiments
for both settings, demonstrating their superiority in the perturbed
environments against the non-robust ones.",None,-1
41a93393-5999-42de-a750-dce5632edbad,On the Possibilities of AI-Generated Text Detection,0.985796,81,"Our work addresses the critical issue of distinguishing text generated by
Large Language Models (LLMs) from human-produced text, a task essential for
numerous applications. Despite ongoing debate about the feasibility of such
differentiation, we present evidence supporting its consistent achievability,
except when human and machine text distributions are indistinguishable across
their entire support. Drawing from information theory, we argue that as
machine-generated text approximates human-like quality, the sample size needed
for detection increases. We establish precise sample complexity bounds for
detecting AI-generated text, laying groundwork for future research aimed at
developing advanced, multi-sample detectors. Our empirical evaluations across
multiple datasets (Xsum, Squad, IMDb, and Kaggle FakeNews) confirm the
viability of enhanced detection methods. We test various state-of-the-art text
generators, including GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, and
Llama-2-70B-Chat-HF, against detectors, including oBERTa-Large/Base-Detector,
GPTZero. Our findings align with OpenAI's empirical data related to sequence
length, marking the first theoretical substantiation for these observations.",None,-1
143b519a-f32d-455f-985a-ef34d8e44479,Diverse Inpainting and Editing with GAN Inversion,0.793123,12,"Recent inversion methods have shown that real images can be inverted into
StyleGAN's latent space and numerous edits can be achieved on those images
thanks to the semantically rich feature representations of well-trained GAN
models. However, extensive research has also shown that image inversion is
challenging due to the trade-off between high-fidelity reconstruction and
editability. In this paper, we tackle an even more difficult task, inverting
erased images into GAN's latent space for realistic inpaintings and editings.
Furthermore, by augmenting inverted latent codes with different latent samples,
we achieve diverse inpaintings. Specifically, we propose to learn an encoder
and mixing network to combine encoded features from erased images with
StyleGAN's mapped features from random samples. To encourage the mixing network
to utilize both inputs, we train the networks with generated data via a novel
set-up. We also utilize higher-rate features to prevent color inconsistencies
between the inpainted and unerased parts. We run extensive experiments and
compare our method with state-of-the-art inversion and inpainting methods.
Qualitative metrics and visual comparisons show significant improvements.",None,-1
127b26d0-be2c-4596-87cc-792c0393a85b,Why think step by step? Reasoning emerges from the locality of experience,0.489774,41,"Humans have a powerful and mysterious capacity to reason. Working through a
set of mental steps enables us to make inferences we would not be capable of
making directly even though we get no additional data from the world.
Similarly, when large language models generate intermediate steps (a chain of
thought) before answering a question, they often produce better answers than
they would directly. We investigate why and how chain-of-thought reasoning is
useful in language models, testing the hypothesis that reasoning is effective
when training data consists of overlapping local clusters of variables that
influence each other strongly. These training conditions enable the chaining of
accurate local inferences to estimate relationships between variables that were
not seen together in training. We prove that there will exist a ""reasoning
gap"", where reasoning through intermediate variables reduces bias, for the
simple case of an autoregressive density estimator trained on local samples
from a chain-structured probabilistic model. We then test our hypothesis
experimentally in more complex models, training an autoregressive language
model on samples from Bayes nets but only including a subset of variables in
each sample. We test language models' ability to match conditional
probabilities with and without intermediate reasoning steps, finding that
intermediate steps are only helpful when the training data is locally
structured with respect to dependencies between variables. The combination of
locally structured observations and reasoning is much more data-efficient than
training on all variables. Our results illustrate how the effectiveness of
reasoning step by step is rooted in the local statistical structure of the
training data.",None,-1
b25f1a29-3e74-4d31-8440-b771ca4291f2,Exploration with Principles for Diverse AI Supervision,0.0936138,1,"Training large transformers using next-token prediction has given rise to
groundbreaking advancements in AI. While this generative AI approach has
produced impressive results, it heavily leans on human supervision. Even
state-of-the-art AI models like ChatGPT depend on fine-tuning through human
demonstrations, demanding extensive human input and domain expertise. This
strong reliance on human oversight poses a significant hurdle to the
advancement of AI innovation. To address this limitation, we propose a novel
paradigm termed Exploratory AI (EAI) aimed at autonomously generating
high-quality training data. Drawing inspiration from unsupervised reinforcement
learning (RL) pretraining, EAI achieves exploration within the natural language
space. We accomplish this by harnessing large language models to assess the
novelty of generated content. Our approach employs two key components: an actor
that generates novel content following exploration principles and a critic that
evaluates the generated content, offering critiques to guide the actor.
Empirical evaluations demonstrate that EAI significantly boosts model
performance on complex reasoning tasks, addressing the limitations of
human-intensive supervision.",None,-1
4ca71203-0ad7-496d-aef1-902a1c768946,GNN-based Passenger Request Prediction,0.777338,4,"Passenger request prediction is essential for operations planning, control,
and management in ride-sharing platforms. While the demand prediction problem
has been studied extensively, the Origin-Destination (OD) flow prediction of
passengers has received less attention from the research community. This paper
develops a Graph Neural Network framework along with the Attention Mechanism to
predict the OD flow of passengers. The proposed framework exploits various
linear and non-linear dependencies that arise among requests originating from
different locations and captures the repetition pattern and the contextual data
of that place. Moreover, the optimal size of the grid cell that covers the road
network and preserves the complexity and accuracy of the model is determined.
Extensive simulations are conducted to examine the characteristics of our
proposed approach and its various components. The results show the superior
performance of our proposed model compared to the existing baselines.",None,-1
2d35debc-2b6c-4de0-b2ab-69c0c0ab718c,DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices,0.75417,10,"Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap
between a labeled source dataset and an unlabelled target dataset for
regression problems. Recent works mostly focus on learning a deep feature
encoder by minimizing the discrepancy between source and target features. In
this work, we present a different perspective for the DAR problem by analyzing
the closed-form ordinary least square~(OLS) solution to the linear regressor in
the deep domain adaptation context. Rather than aligning the original feature
embedding space, we propose to align the inverse Gram matrix of the features,
which is motivated by its presence in the OLS solution and the Gram matrix's
ability to capture the feature correlations. Specifically, we propose a simple
yet effective DAR method which leverages the pseudo-inverse low-rank property
to align the scale and angle in a selected subspace generated by the
pseudo-inverse Gram matrix of the two domains. We evaluate our method on three
domain adaptation regression benchmarks. Experimental results demonstrate that
our method achieves state-of-the-art performance. Our code is available at
https://github.com/ismailnejjar/DARE-GRAM.",None,-1
626db1a1-1e4e-44f1-98af-5b4521b57fe8,In-Contextual Gender Bias Suppression for Large Language Models,0.388836,2,"Despite their impressive performance in a wide range of NLP tasks, Large
Language Models (LLMs) have been reported to encode worrying-levels of gender
biases. Prior work has proposed debiasing methods that require human labelled
examples, data augmentation and fine-tuning of LLMs, which are computationally
costly. Moreover, one might not even have access to the model parameters for
performing debiasing such as in the case of closed LLMs such as GPT-4. To
address this challenge, we propose bias suppression that prevents biased
generations of LLMs by simply providing textual preambles constructed from
manually designed templates and real-world statistics, without accessing to
model parameters. We show that, using CrowsPairs dataset, our textual preambles
covering counterfactual statements can suppress gender biases in English LLMs
such as LLaMA2. Moreover, we find that gender-neutral descriptions of
gender-biased objects can also suppress their gender biases. Moreover, we show
that bias suppression has acceptable adverse effect on downstream task
performance with HellaSwag and COPA.",None,-1
3a19c674-bb1d-42c6-b0bb-22697abf95a7,Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks,0.605689,14,"Large Language Models (LLMs) have demonstrated impressive text generation
capabilities, prompting us to reconsider the future of human-AI co-creation and
how humans interact with LLMs. In this paper, we present a spectrum of content
generation tasks and their corresponding human-AI interaction patterns. These
tasks include: 1) fixed-scope content curation tasks with minimal human-AI
interactions, 2) independent creative tasks with precise human-AI interactions,
and 3) complex and interdependent creative tasks with iterative human-AI
interactions. We encourage the generative AI and HCI research communities to
focus on the more complex and interdependent tasks, which require greater
levels of human involvement.",None,-1
905ab3fa-3fe8-4323-87b9-a672d21cee08,Self-Supervised Learning for Multimodal Non-Rigid 3D Shape Matching,0.441101,13,"The matching of 3D shapes has been extensively studied for shapes represented
as surface meshes, as well as for shapes represented as point clouds. While
point clouds are a common representation of raw real-world 3D data (e.g. from
laser scanners), meshes encode rich and expressive topological information, but
their creation typically requires some form of (often manual) curation. In
turn, methods that purely rely on point clouds are unable to meet the matching
quality of mesh-based methods that utilise the additional topological
structure. In this work we close this gap by introducing a self-supervised
multimodal learning strategy that combines mesh-based functional map
regularisation with a contrastive loss that couples mesh and point cloud data.
Our shape matching approach allows to obtain intramodal correspondences for
triangle meshes, complete point clouds, and partially observed point clouds, as
well as correspondences across these data modalities. We demonstrate that our
method achieves state-of-the-art results on several challenging benchmark
datasets even in comparison to recent supervised methods, and that our method
reaches previously unseen cross-dataset generalisation ability.",None,-1
b7ace227-9bbd-4a69-ba35-293c409328a6,PIVOINE: Instruction Tuning for Open-world Information Extraction,0.660573,8,"We consider the problem of Open-world Information Extraction (Open-world IE),
which extracts comprehensive entity profiles from unstructured texts. Different
from the conventional closed-world setting of Information Extraction (IE),
Open-world IE considers a more general situation where entities and relations
could be beyond a predefined ontology. More importantly, we seek to develop a
large language model (LLM) that is able to perform Open-world IE to extract
desirable entity profiles characterized by (possibly fine-grained) natural
language instructions. We achieve this by finetuning LLMs using instruction
tuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction
tuning dataset for Open-world IE enriched with a comprehensive corpus,
extensive annotations, and diverse instructions. We finetune the pretrained
BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE
with strong instruction-following capabilities. Our experiments demonstrate
that PIVOINE significantly outperforms traditional closed-world methods and
other LLM baselines, displaying impressive generalization capabilities on both
unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as
a promising solution to tackle the open-world challenge in IE effectively.",None,-1
85661fbd-4c4a-4a0b-92cd-0c465c4db4ae,On Context Distribution Shift in Task Representation Learning for Offline Meta RL,0.240305,2,"Offline Meta Reinforcement Learning (OMRL) aims to learn transferable
knowledge from offline datasets to enhance the learning process for new target
tasks. Context-based Reinforcement Learning (RL) adopts a context encoder to
expediently adapt the agent to new tasks by inferring the task representation,
and then adjusting the policy based on this inferred representation. In this
work, we focus on context-based OMRL, specifically on the challenge of learning
task representation for OMRL. We conduct experiments that demonstrate that the
context encoder trained on offline datasets might encounter distribution shift
between the contexts used for training and testing. To overcome this problem,
we present a hard-sampling-based strategy to train a robust task context
encoder. Our experimental findings on diverse continuous control tasks reveal
that utilizing our approach yields more robust task representations and better
testing performance in terms of accumulated returns compared to baseline
methods. Our code is available at https://github.com/ZJLAB-AMMI/HS-OMRL.",None,-1
0edaae5d-6bae-4cad-9fb7-d2a339505fe4,Task-Specific Context Decoupling for Object Detection,0.444875,17,"Classification and localization are two main sub-tasks in object detection.
Nonetheless, these two tasks have inconsistent preferences for feature context,
i.e., localization expects more boundary-aware features to accurately regress
the bounding box, while more semantic context is preferred for object
classification. Exsiting methods usually leverage disentangled heads to learn
different feature context for each task. However, the heads are still applied
on the same input features, which leads to an imperfect balance between
classifcation and localization. In this work, we propose a novel Task-Specific
COntext DEcoupling (TSCODE) head which further disentangles the feature
encoding for two tasks. For classification, we generate spatially-coarse but
semantically-strong feature encoding. For localization, we provide
high-resolution feature map containing more edge information to better regress
object boundaries. TSCODE is plug-and-play and can be easily incorperated into
existing detection pipelines. Extensive experiments demonstrate that our method
stably improves different detectors by over 1.0 AP with less computational
cost. Our code and models will be publicly released.",None,-1
3015c09e-e631-4e28-9779-a59d998d0310,Neural LiDAR Fields for Novel View Synthesis,0.750486,21,"We present Neural Fields for LiDAR (NFL), a method to optimise a neural field
scene representation from LiDAR measurements, with the goal of synthesizing
realistic LiDAR scans from novel viewpoints. NFL combines the rendering power
of neural fields with a detailed, physically motivated model of the LiDAR
sensing process, thus enabling it to accurately reproduce key sensor behaviors
like beam divergence, secondary returns, and ray dropping. We evaluate NFL on
synthetic and real LiDAR scans and show that it outperforms explicit
reconstruct-then-simulate methods as well as other NeRF-style methods on LiDAR
novel view synthesis task. Moreover, we show that the improved realism of the
synthesized views narrows the domain gap to real scans and translates to better
registration and semantic segmentation performance.",None,-1
e1d88613-9ad3-4d39-ae3c-1b51d0b506cd,NarrativePlay: Interactive Narrative Understanding,0.305824,2,"In this paper, we introduce NarrativePlay, a novel system that allows users
to role-play a fictional character and interact with other characters in
narratives such as novels in an immersive environment. We leverage Large
Language Models (LLMs) to generate human-like responses, guided by personality
traits extracted from narratives. The system incorporates auto-generated visual
display of narrative settings, character portraits, and character speech,
greatly enhancing user experience. Our approach eschews predefined sandboxes,
focusing instead on main storyline events extracted from narratives from the
perspective of a user-selected character. NarrativePlay has been evaluated on
two types of narratives, detective and adventure stories, where users can
either explore the world or improve their favorability with the narrative
characters through conversations.",None,-1
12ef3f0f-d03b-4c58-82be-43d5d557d501,Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature,0.234385,4,"We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large
language model to engage in meaningful interactions with Astronomy papers using
in-context prompting. To optimize for efficiency, we employ a distillation
technique that effectively reduces the size of the original input paper by
50\%, while maintaining the paragraph structure and overall semantic integrity.
We then explore the model's responses using a multi-document context (ten
distilled documents). Our findings indicate that GPT-4 excels in the
multi-document domain, providing detailed answers contextualized within the
framework of related research findings. Our results showcase the potential of
large language models for the astronomical community, offering a promising
avenue for further exploration, particularly the possibility of utilizing the
models for hypothesis generation.",None,-1
200feff4-7ed6-403e-980f-f964d62edae9,Causal Confirmation Measures: From Simpson's Paradox to COVID-19,0.537795,3,"When we compare the influences of two causes on an outcome, if the conclusion
from every group is against that from the conflation, we think there is
Simpson's Paradox. The Existing Causal Inference Theory (ECIT) can make the
overall conclusion consistent with the grouping conclusion by removing the
confounder's influence to eliminate the paradox. The ECIT uses relative risk
difference Pd = max(0, (R - 1)/R) (R denotes the risk ratio) as the probability
of causation. In contrast, Philosopher Fitelson uses confirmation measure D
(posterior probability minus prior probability) to measure the strength of
causation. Fitelson concludes that from the perspective of Bayesian
confirmation, we should directly accept the overall conclusion without
considering the paradox. The author proposed a Bayesian confirmation measure b*
similar to Pd before. To overcome the contradiction between the ECIT and
Bayesian confirmation, the author uses the semantic information method with the
minimum cross-entropy criterion to deduce causal confirmation measure Cc = (R
-1)/max(R, 1). Cc is like Pd but has normalizing property (between -1 and 1)
and cause symmetry. It especially fits cases where a cause restrains an
outcome, such as the COVID-19 vaccine controlling the infection. Some examples
(about kidney stone treatments and COVID-19) reveal that Pd and Cc are more
reasonable than D; Cc is more useful than Pd.",None,-1
6506c333-e09f-4b57-bf11-13085f58927b,VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders,0.249271,4,"Large-scale text-to-image diffusion models have shown impressive capabilities
for generative tasks by leveraging strong vision-language alignment from
pre-training. However, most vision-language discriminative tasks require
extensive fine-tuning on carefully-labeled datasets to acquire such alignment,
with great cost in time and computing resources. In this work, we explore
directly applying a pre-trained generative diffusion model to the challenging
discriminative task of visual grounding without any fine-tuning and additional
training dataset. Specifically, we propose VGDiffZero, a simple yet effective
zero-shot visual grounding framework based on text-to-image diffusion models.
We also design a comprehensive region-scoring method considering both global
and local contexts of each isolated proposal. Extensive experiments on RefCOCO,
RefCOCO+, and RefCOCOg show that VGDiffZero achieves strong performance on
zero-shot visual grounding. Our code is available at
https://github.com/xuyang-liu16/VGDiffZero.",None,-1
da16153c-8487-423b-8f83-de59e89d4223,Low-Light Image Enhancement via Structure Modeling and Guidance,0.999538,26,"This paper proposes a new framework for low-light image enhancement by
simultaneously conducting the appearance as well as structure modeling. It
employs the structural feature to guide the appearance enhancement, leading to
sharp and realistic results. The structure modeling in our framework is
implemented as the edge detection in low-light images. It is achieved with a
modified generative model via designing a structure-aware feature extractor and
generator. The detected edge maps can accurately emphasize the essential
structural information, and the edge prediction is robust towards the noises in
dark areas. Moreover, to improve the appearance modeling, which is implemented
with a simple U-Net, a novel structure-guided enhancement module is proposed
with structure-guided feature synthesis layers. The appearance modeling, edge
detector, and enhancement module can be trained end-to-end. The experiments are
conducted on representative datasets (sRGB and RAW domains), showing that our
model consistently achieves SOTA performance on all datasets with the same
architecture.",None,-1
3186ff07-ce4d-44a5-b404-15b35da421fe,GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models,0.847993,6,"Relation extraction (RE) is a crucial task in natural language processing
(NLP) that aims to identify and classify relationships between entities
mentioned in text. In the financial domain, relation extraction plays a vital
role in extracting valuable information from financial documents, such as news
articles, earnings reports, and company filings. This paper describes our
solution to relation extraction on one such dataset REFinD. The dataset was
released along with shared task as a part of the Fourth Workshop on Knowledge
Discovery from Unstructured Data in Financial Services, co-located with SIGIR
2023. In this paper, we employed OpenAI models under the framework of
in-context learning (ICL). We utilized two retrieval strategies to find top K
relevant in-context learning demonstrations / examples from training data for a
given test example. The first retrieval mechanism, we employed, is a
learning-free dense retriever and the other system is a learning-based
retriever. We were able to achieve 3rd rank overall. Our best F1-score is
0.718.",None,-1
e09eb647-2a83-4db2-9ee0-9a32dd6a451d,ChatGPT and a New Academic Reality: Artificial Intelligence-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing,0.998579,272,"This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,
which uses natural language processing to fulfill text-based user requests
(i.e., a chatbot). The history and principles behind ChatGPT and similar models
are discussed. This technology is then discussed in relation to its potential
impact on academia and scholarly research and publishing. ChatGPT is seen as a
potential model for the automated preparation of essays and other types of
scholarly manuscripts. Potential ethical issues that could arise with the
emergence of large language models like GPT-3, the underlying technology behind
ChatGPT, and its usage by academics and researchers, are discussed and situated
within the context of broader advancements in artificial intelligence, machine
learning, and natural language processing for research and scholarly
publishing.",None,-1
356c3497-9f7d-4c49-a800-05b59eee833d,MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds,0.3404,7,"3D semantic segmentation on multi-scan large-scale point clouds plays an
important role in autonomous systems. Unlike the single-scan-based semantic
segmentation task, this task requires distinguishing the motion states of
points in addition to their semantic categories. However, methods designed for
single-scan-based segmentation tasks perform poorly on the multi-scan task due
to the lacking of an effective way to integrate temporal information. We
propose MarS3D, a plug-and-play motion-aware module for semantic segmentation
on multi-scan 3D point clouds. This module can be flexibly combined with
single-scan models to allow them to have multi-scan perception abilities. The
model encompasses two key designs: the Cross-Frame Feature Embedding module for
enriching representation learning and the Motion-Aware Feature Learning module
for enhancing motion awareness. Extensive experiments show that MarS3D can
improve the performance of the baseline model by a large margin. The code is
available at https://github.com/CVMI-Lab/MarS3D.",None,-1
29e1cee8-4dd6-4c42-82f0-02120f4f4996,APR: Online Distant Point Cloud Registration Through Aggregated Point Cloud Reconstruction,0.202008,2,"For many driving safety applications, it is of great importance to accurately
register LiDAR point clouds generated on distant moving vehicles. However, such
point clouds have extremely different point density and sensor perspective on
the same object, making registration on such point clouds very hard. In this
paper, we propose a novel feature extraction framework, called APR, for online
distant point cloud registration. Specifically, APR leverages an autoencoder
design, where the autoencoder reconstructs a denser aggregated point cloud with
several frames instead of the original single input point cloud. Our design
forces the encoder to extract features with rich local geometry information
based on one single input point cloud. Such features are then used for online
distant point cloud registration. We conduct extensive experiments against
state-of-the-art (SOTA) feature extractors on KITTI and nuScenes datasets.
Results show that APR outperforms all other extractors by a large margin,
increasing average registration recall of SOTA extractors by 7.1% on LoKITTI
and 4.6% on LoNuScenes. Code is available at https://github.com/liuQuan98/APR.",None,-1
5d011456-10bc-4230-9aa8-389e14dec976,GATOR: Graph-Aware Transformer with Motion-Disentangled Regression for Human Mesh Recovery from a 2D Pose,0.119001,1,"3D human mesh recovery from a 2D pose plays an important role in various
applications. However, it is hard for existing methods to simultaneously
capture the multiple relations during the evolution from skeleton to mesh,
including joint-joint, joint-vertex and vertex-vertex relations, which often
leads to implausible results. To address this issue, we propose a novel
solution, called GATOR, that contains an encoder of Graph-Aware Transformer
(GAT) and a decoder with Motion-Disentangled Regression (MDR) to explore these
multiple relations. Specifically, GAT combines a GCN and a graph-aware
self-attention in parallel to capture physical and hidden joint-joint
relations. Furthermore, MDR models joint-vertex and vertex-vertex interactions
to explore joint and vertex relations. Based on the clustering characteristics
of vertex offset fields, MDR regresses the vertices by composing the predicted
base motions. Extensive experiments show that GATOR achieves state-of-the-art
performance on two challenging benchmarks.",None,-1
74240bd5-128d-4eba-b940-3eb27b7ebdb1,ScaleDet: A Scalable Multi-Dataset Object Detector,0.324725,9,"Multi-dataset training provides a viable solution for exploiting
heterogeneous large-scale datasets without extra annotation cost. In this work,
we propose a scalable multi-dataset detector (ScaleDet) that can scale up its
generalization across datasets when increasing the number of training datasets.
Unlike existing multi-dataset learners that mostly rely on manual relabelling
efforts or sophisticated optimizations to unify labels across datasets, we
introduce a simple yet scalable formulation to derive a unified semantic label
space for multi-dataset training. ScaleDet is trained by visual-textual
alignment to learn the label assignment with label semantic similarities across
datasets. Once trained, ScaleDet can generalize well on any given upstream and
downstream datasets with seen and unseen classes. We conduct extensive
experiments using LVIS, COCO, Objects365, OpenImages as upstream datasets, and
13 datasets from Object Detection in the Wild (ODinW) as downstream datasets.
Our results show that ScaleDet achieves compelling strong model performance
with an mAP of 50.7 on LVIS, 58.8 on COCO, 46.8 on Objects365, 76.2 on
OpenImages, and 71.8 on ODinW, surpassing state-of-the-art detectors with the
same backbone.",None,-1
3fc26c69-06d7-4b07-9bc3-395409ccb6c9,Predicting Motion Plans for Articulating Everyday Objects,0.6094,4,"Mobile manipulation tasks such as opening a door, pulling open a drawer, or
lifting a toilet lid require constrained motion of the end-effector under
environmental and task constraints. This, coupled with partial information in
novel environments, makes it challenging to employ classical motion planning
approaches at test time. Our key insight is to cast it as a learning problem to
leverage past experience of solving similar planning problems to directly
predict motion plans for mobile manipulation tasks in novel situations at test
time. To enable this, we develop a simulator, ArtObjSim, that simulates
articulated objects placed in real scenes. We then introduce SeqIK+$\theta_0$,
a fast and flexible representation for motion plans. Finally, we learn models
that use SeqIK+$\theta_0$ to quickly predict motion plans for articulating
novel objects at test time. Experimental evaluation shows improved speed and
accuracy at generating motion plans than pure search-based methods and pure
learning methods.",None,-1
0ca28bf2-ffa6-4f37-bb18-fa1fd59b411e,Augmenting Large Language Model Translators via Translation Memories,0.668645,12,"Using translation memories (TMs) as prompts is a promising approach to
in-context learning of machine translation models. In this work, we take a step
towards prompting large language models (LLMs) with TMs and making them better
translators. We find that the ability of LLMs to ``understand'' prompts is
indeed helpful for making better use of TMs. Experiments show that the results
of a pre-trained LLM translator can be greatly improved by using high-quality
TM-based prompts. These results are even comparable to those of the
state-of-the-art NMT systems which have access to large-scale in-domain
bilingual data and are well tuned on the downstream tasks.",None,-1
3a099514-ca9e-4278-bf36-d24f281beaa2,Prompting Large Language Model for Machine Translation: A Case Study,0.999956,135,"Research on prompting has shown excellent performance with little or even no
supervised training across many tasks. However, prompting for machine
translation is still under-explored in the literature. We fill this gap by
offering a systematic study on prompting strategies for translation, examining
various factors for prompt template and demonstration example selection. We
further explore the use of monolingual data and the feasibility of
cross-lingual, cross-domain, and sentence-to-document transfer learning in
prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the
testbed show that 1) the number and the quality of prompt examples matter,
where using suboptimal examples degenerates translation; 2) several features of
prompt examples, such as semantic similarity, show significant Spearman
correlation with their prompting performance; yet, none of the correlations are
strong enough; 3) using pseudo parallel prompt examples constructed from
monolingual data via zero-shot prompting could improve translation; and 4)
improved performance is achievable by transferring knowledge from prompt
examples selected in other settings. We finally provide an analysis on the
model outputs and discuss several problems that prompting still suffers from.",None,-1
d5a69772-f28f-4650-8c14-8ca96c04f46a,CrossSpeech: Speaker-independent Acoustic Representation for Cross-lingual Speech Synthesis,0.641148,5,"While recent text-to-speech (TTS) systems have made remarkable strides toward
human-level quality, the performance of cross-lingual TTS lags behind that of
intra-lingual TTS. This gap is mainly rooted from the speaker-language
entanglement problem in cross-lingual TTS. In this paper, we propose
CrossSpeech which improves the quality of cross-lingual speech by effectively
disentangling speaker and language information in the level of acoustic feature
space. Specifically, CrossSpeech decomposes the speech generation pipeline into
the speaker-independent generator (SIG) and speaker-dependent generator (SDG).
The SIG produces the speaker-independent acoustic representation which is not
biased to specific speaker distributions. On the other hand, the SDG models
speaker-dependent speech variation that characterizes speaker attributes. By
handling each information separately, CrossSpeech can obtain disentangled
speaker and language representations. From the experiments, we verify that
CrossSpeech achieves significant improvements in cross-lingual TTS, especially
in terms of speaker similarity to the target speaker.",None,-1
b868548d-aca9-482a-b30c-68cb5b3f288b,Real-Word Error Correction with Trigrams: Correcting Multiple Errors in a Sentence,0.339864,13,"Spelling correction is a fundamental task in Text Mining. In this study, we
assess the real-word error correction model proposed by Mays, Damerau and
Mercer and describe several drawbacks of the model. We propose a new variation
which focuses on detecting and correcting multiple real-word errors in a
sentence, by manipulating a Probabilistic Context-Free Grammar (PCFG) to
discriminate between items in the search space. We test our approach on the
Wall Street Journal corpus and show that it outperforms Hirst and Budanitsky's
WordNet-based method and Wilcox-O'Hearn, Hirst, and Budanitsky's fixed windows
size method.-O'Hearn, Hirst, and Budanitsky's fixed windows size method.",None,-1
ba6fa6a1-dd46-43a1-9ed6-b867b9665fee,Monocular Depth Estimation using Diffusion Models,0.99565,43,"We formulate monocular depth estimation using denoising diffusion models,
inspired by their recent successes in high fidelity image generation. To that
end, we introduce innovations to address problems arising due to noisy,
incomplete depth maps in training data, including step-unrolled denoising
diffusion, an $L_1$ loss, and depth infilling during training. To cope with the
limited availability of data for supervised training, we leverage pre-training
on self-supervised image-to-image translation tasks. Despite the simplicity of
the approach, with a generic loss and architecture, our DepthGen model achieves
SOTA performance on the indoor NYU dataset, and near SOTA results on the
outdoor KITTI dataset. Further, with a multimodal posterior, DepthGen naturally
represents depth ambiguity (e.g., from transparent surfaces), and its zero-shot
performance combined with depth imputation, enable a simple but effective
text-to-3D pipeline. Project page: https://depth-gen.github.io",None,-1
1f8287f1-3b11-4d07-b54a-bdf433f77ead,TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction,0.301368,8,"Deep saliency prediction algorithms complement the object recognition
features, they typically rely on additional information, such as scene context,
semantic relationships, gaze direction, and object dissimilarity. However, none
of these models consider the temporal nature of gaze shifts during image
observation. We introduce a novel saliency prediction model that learns to
output saliency maps in sequential time intervals by exploiting human temporal
attention patterns. Our approach locally modulates the saliency predictions by
combining the learned temporal maps. Our experiments show that our method
outperforms the state-of-the-art models, including a multi-duration saliency
model, on the SALICON benchmark. Our code will be publicly available on GitHub.",None,-1
1efa9d6b-dfce-42dd-a831-64534bf03479,Efficient Multimodal Fusion via Interactive Prompting,0.963658,25,"Large-scale pre-training has brought unimodal fields such as computer vision
and natural language processing to a new era. Following this trend, the size of
multi-modal learning models constantly increases, leading to an urgent need to
reduce the massive computational cost of finetuning these models for downstream
tasks. In this paper, we propose an efficient and flexible multimodal fusion
method, namely PMF, tailored for fusing unimodally pre-trained transformers.
Specifically, we first present a modular multimodal fusion framework that
exhibits high flexibility and facilitates mutual interactions among different
modalities. In addition, we disentangle vanilla prompts into three types in
order to learn different optimizing objectives for multimodal learning. It is
also worth noting that we propose to add prompt vectors only on the deep layers
of the unimodal transformers, thus significantly reducing the training memory
usage. Experiment results show that our proposed method achieves comparable
performance to several other multimodal finetuning methods with less than 3%
trainable parameters and up to 66% saving of training memory usage.",None,-1
49f87702-b72c-4737-9221-728d117d88a3,Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique,0.381496,1,"This paper presents a novel benchmarking framework Dyport for evaluating
biomedical hypothesis generation systems. Utilizing curated datasets, our
approach tests these systems under realistic conditions, enhancing the
relevance of our evaluations. We integrate knowledge from the curated databases
into a dynamic graph, accompanied by a method to quantify discovery importance.
This not only assesses hypothesis accuracy but also their potential impact in
biomedical research which significantly extends traditional link prediction
benchmarks. Applicability of our benchmarking process is demonstrated on
several link prediction systems applied on biomedical semantic knowledge
graphs. Being flexible, our benchmarking system is designed for broad
application in hypothesis generation quality verification, aiming to expand the
scope of scientific discovery within the biomedical research community.
Availability and implementation: Dyport framework is fully open-source. All
code and datasets are available at: https://github.com/IlyaTyagin/Dyport",None,-1
e253cafd-9731-4853-a567-88e40aadaed3,Zero-shot Object Counting,0.496458,15,"Class-agnostic object counting aims to count object instances of an arbitrary
class at test time. It is challenging but also enables many potential
applications. Current methods require human-annotated exemplars as inputs which
are often unavailable for novel categories, especially for autonomous systems.
Thus, we propose zero-shot object counting (ZSC), a new setting where only the
class name is available during test time. Such a counting system does not
require human annotators in the loop and can operate automatically. Starting
from a class name, we propose a method that can accurately identify the optimal
patches which can then be used as counting exemplars. Specifically, we first
construct a class prototype to select the patches that are likely to contain
the objects of interest, namely class-relevant patches. Furthermore, we
introduce a model that can quantitatively measure how suitable an arbitrary
patch is as a counting exemplar. By applying this model to all the candidate
patches, we can select the most suitable patches as exemplars for counting.
Experimental results on a recent class-agnostic counting dataset, FSC-147,
validate the effectiveness of our method. Code is available at
https://github.com/cvlab-stonybrook/zero-shot-counting",None,-1
4d04107b-2155-40b7-b1e6-983ea3b4a923,Performance of the Pre-Trained Large Language Model GPT-4 on Automated Short Answer Grading,0.94838,4,"Automated Short Answer Grading (ASAG) has been an active area of
machine-learning research for over a decade. It promises to let educators grade
and give feedback on free-form responses in large-enrollment courses in spite
of limited availability of human graders. Over the years, carefully trained
models have achieved increasingly higher levels of performance. More recently,
pre-trained Large Language Models (LLMs) emerged as a commodity, and an
intriguing question is how a general-purpose tool without additional training
compares to specialized models. We studied the performance of GPT-4 on the
standard benchmark 2-way and 3-way datasets SciEntsBank and Beetle, where in
addition to the standard task of grading the alignment of the student answer
with a reference answer, we also investigated withholding the reference answer.
We found that overall, the performance of the pre-trained general-purpose GPT-4
LLM is comparable to hand-engineered models, but worse than pre-trained LLMs
that had specialized training.",None,-1
4f269702-1795-4605-adb9-23829c8cde9d,Transfer-Ensemble Learning based Deep Convolutional Neural Networks for Diabetic Retinopathy Classification,0.565402,2,"This article aims to classify diabetic retinopathy (DR) disease into five
different classes using an ensemble approach based on two popular pre-trained
convolutional neural networks: VGG16 and Inception V3. The proposed model aims
to leverage the strengths of the two individual nets to enhance the
classification performance for diabetic retinopathy. The ensemble model
architecture involves freezing a portion of the layers in each pre-trained
model to utilize their learned representations effectively. Global average
pooling layers are added to transform the output feature maps into fixed-length
vectors. These vectors are then concatenated to form a consolidated
representation of the input image. The ensemble model is trained using a
dataset of diabetic retinopathy images (APTOS), divided into training and
validation sets. During the training process, the model learns to classify the
retinal images into the corresponding diabetic retinopathy classes.
Experimental results on the test set demonstrate the efficacy of the proposed
ensemble model for DR classification achieving an accuracy of 96.4%.",None,-1
308e4bf4-fe2c-4302-a6d7-c165ff7d85d5,Diffusion-based 3D Object Detection with Random Boxes,0.47813,3,"3D object detection is an essential task for achieving autonomous driving.
Existing anchor-based detection methods rely on empirical heuristics setting of
anchors, which makes the algorithms lack elegance. In recent years, we have
witnessed the rise of several generative models, among which diffusion models
show great potential for learning the transformation of two distributions. Our
proposed Diff3Det migrates the diffusion model to proposal generation for 3D
object detection by considering the detection boxes as generative targets.
During training, the object boxes diffuse from the ground truth boxes to the
Gaussian distribution, and the decoder learns to reverse this noise process. In
the inference stage, the model progressively refines a set of random boxes to
the prediction results. We provide detailed experiments on the KITTI benchmark
and achieve promising performance compared to classical anchor-based 3D
detection methods.",None,-1
43597e7b-b5ee-4a21-adef-1f6fd5192e85,GaitRef: Gait Recognition with Refined Sequential Skeletons,0.661168,7,"Identifying humans with their walking sequences, known as gait recognition,
is a useful biometric understanding task as it can be observed from a long
distance and does not require cooperation from the subject. Two common
modalities used for representing the walking sequence of a person are
silhouettes and joint skeletons. Silhouette sequences, which record the
boundary of the walking person in each frame, may suffer from the variant
appearances from carried-on objects and clothes of the person. Framewise joint
detections are noisy and introduce some jitters that are not consistent with
sequential detections. In this paper, we combine the silhouettes and skeletons
and refine the framewise joint predictions for gait recognition. With temporal
information from the silhouette sequences, we show that the refined skeletons
can improve gait recognition performance without extra annotations. We compare
our methods on four public datasets, CASIA-B, OUMVLP, Gait3D and GREW, and show
state-of-the-art performance.",None,-1
6f756058-8e17-47b6-9425-ed8ef1ab7436,EventCLIP: Adapting CLIP for Event-based Object Recognition,0.644146,6,"Recent advances in zero-shot and few-shot classification heavily rely on the
success of pre-trained vision-language models (VLMs) such as CLIP. Due to a
shortage of large-scale datasets, training such models for event camera data
remains infeasible. Thus, adapting existing VLMs across modalities to event
vision is an important research challenge. In this work, we introduce
EventCLIP, a novel approach that utilizes CLIP for zero-shot and few-shot
event-based object recognition. We first generalize CLIP's image encoder to
event data by converting raw events to 2D grid-based representations. To
further enhance performance, we propose a feature adapter to aggregate temporal
information over event frames and refine text embeddings to better align with
the visual inputs. We evaluate EventCLIP on N-Caltech, N-Cars, and N-ImageNet
datasets, achieving state-of-the-art few-shot performance. When fine-tuned on
the entire dataset, our method outperforms all existing event classifiers.
Moreover, we explore practical applications of EventCLIP including robust event
classification and label-free event recognition, where our approach surpasses
previous baselines designed specifically for these tasks.",None,-1
a45aa7cf-82f5-496b-9f8e-5156b01d1866,ECQED: Emotion-Cause Quadruple Extraction in Dialogs,0.573469,2,"The existing emotion-cause pair extraction (ECPE) task, unfortunately,
ignores extracting the emotion type and cause type, while these fine-grained
meta-information can be practically useful in real-world applications, i.e.,
chat robots and empathic dialog generation. Also the current ECPE is limited to
the scenario of single text piece, while neglecting the studies at dialog level
that should have more realistic values. In this paper, we extend the ECPE task
with a broader definition and scenario, presenting a new task, Emotion-Cause
Quadruple Extraction in Dialogs (ECQED), which requires detecting emotion-cause
utterance pairs and emotion and cause types. We present an ECQED model based on
a structural and semantic heterogeneous graph as well as a parallel grid
tagging scheme, which advances in effectively incorporating the dialog context
structure, meanwhile solving the challenging overlapped quadruple issue. Via
experiments we show that introducing the fine-grained emotion and cause
features evidently helps better dialog generation. Also our proposed ECQED
system shows exceptional superiority over baselines on both the emotion-cause
quadruple or pair extraction tasks, meanwhile being highly efficient.",None,-1
206b5990-66aa-4e47-86dc-eb6141e2f77f,Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative Multi-Agent Reinforcement Learning,0.256498,2,"Value decomposition methods have gained popularity in the field of
cooperative multi-agent reinforcement learning. However, almost all existing
methods follow the principle of Individual Global Max (IGM) or its variants,
which limits their problem-solving capabilities. To address this, we propose a
dual self-awareness value decomposition framework, inspired by the notion of
dual self-awareness in psychology, that entirely rejects the IGM premise. Each
agent consists of an ego policy for action selection and an alter ego value
function to solve the credit assignment problem. The value function
factorization can ignore the IGM assumption by utilizing an explicit search
procedure. On the basis of the above, we also suggest a novel anti-ego
exploration mechanism to avoid the algorithm becoming stuck in a local optimum.
As the first fully IGM-free value decomposition method, our proposed framework
achieves desirable performance in various cooperative tasks.",None,-1
9b1d1d88-a596-4ea6-89f8-b788423339ab,Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?,0.492632,5,"ASR systems are generally built for the spoken 'standard', and their
performance declines for non-standard dialects/varieties. This is a problem for
a language like Irish, where there is no single spoken standard, but rather
three major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As a
diagnostic to quantify the effect of the speaker's dialect on recognition
performance, 12 ASR systems were trained, firstly using baseline
dialect-balanced training corpora, and then using modified versions of the
baseline corpora, where dialect-specific materials were either subtracted or
added. Results indicate that dialect-balanced corpora do not yield a similar
performance across the dialects: the Ul dialect consistently underperforms,
whereas Mu yields lowest WERs. There is a close relationship between Co and Mu
dialects, but one that is not symmetrical. These results will guide future
corpus collection and system building strategies to optimise for cross-dialect
performance equity.",None,-1
850154e8-db21-4fe4-86b1-b37a564b9e22,Texture Generation on 3D Meshes with Point-UV Diffusion,0.478166,24,"In this work, we focus on synthesizing high-quality textures on 3D meshes. We
present Point-UV diffusion, a coarse-to-fine pipeline that marries the
denoising diffusion model with UV mapping to generate 3D consistent and
high-quality texture images in UV space. We start with introducing a point
diffusion model to synthesize low-frequency texture components with our
tailored style guidance to tackle the biased color distribution. The derived
coarse texture offers global consistency and serves as a condition for the
subsequent UV diffusion stage, aiding in regularizing the model to generate a
3D consistent UV texture image. Then, a UV diffusion model with hybrid
conditions is developed to enhance the texture fidelity in the 2D UV space. Our
method can process meshes of any genus, generating diversified,
geometry-compatible, and high-fidelity textures. Code is available at
https://cvmi-lab.github.io/Point-UV-Diffusion",None,-1
a8e00ac3-e46f-4568-a847-0ada85e09d62,DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks,0.0745631,8,"Large language models (LLMs) have achieved remarkable performance in various
evaluation benchmarks. However, concerns are raised about potential data
contamination in their considerable volume of training corpus. Moreover, the
static nature and fixed complexity of current benchmarks may inadequately gauge
the advancing capabilities of LLMs. In this paper, we introduce DyVal, a
general and flexible protocol for dynamic evaluation of LLMs. Based on our
framework, we build graph-informed DyVal by leveraging the structural advantage
of directed acyclic graphs to dynamically generate evaluation samples with
controllable complexities. DyVal generates challenging evaluation sets on
reasoning tasks including mathematics, logical reasoning, and algorithm
problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo
and GPT-4. Experiments show that LLMs perform worse in DyVal-generated
evaluation samples with different complexities, highlighting the significance
of dynamic evaluation. We also analyze the failure cases and results of
different prompting methods. Moreover, DyVal-generated samples are not only
evaluation sets, but also helpful data for fine-tuning to improve the
performance of LLMs on existing benchmarks. We hope that DyVal can shed light
on future evaluation research of LLMs. Code is available at:
https://github.com/microsoft/promptbench.",None,-1
7e46810d-9dac-452f-9c28-a959836c45e3,Towards Flow Graph Prediction of Open-Domain Procedural Texts,0.0453723,1,"Machine comprehension of procedural texts is essential for reasoning about
the steps and automating the procedures. However, this requires identifying
entities within a text and resolving the relationships between the entities.
Previous work focused on the cooking domain and proposed a framework to convert
a recipe text into a flow graph (FG) representation. In this work, we propose a
framework based on the recipe FG for flow graph prediction of open-domain
procedural texts. To investigate flow graph prediction performance in
non-cooking domains, we introduce the wikiHow-FG corpus from articles on
wikiHow, a website of how-to instruction articles. In experiments, we consider
using the existing recipe corpus and performing domain adaptation from the
cooking to the target domain. Experimental results show that the domain
adaptation models achieve higher performance than those trained only on the
cooking or target domain data.",None,-1
f3d3ca86-7637-490c-867e-bdfd0f955fc4,Learning the greatest common divisor: explaining transformer predictions,0.361206,6,"The predictions of small transformers, trained to calculate the greatest
common divisor (GCD) of two positive integers, can be fully characterized by
looking at model inputs and outputs. As training proceeds, the model learns a
list $\mathcal D$ of integers, products of divisors of the base used to
represent integers and small primes, and predicts the largest element of
$\mathcal D$ that divides both inputs. Training distributions impact
performance. Models trained from uniform operands only learn a handful of GCD
(up to $38$ GCD $\leq100$). Log-uniform operands boost performance to $73$ GCD
$\leq 100$, and a log-uniform distribution of outcomes (i.e. GCD) to $91$.
However, training from uniform (balanced) GCD breaks explainability.",None,-1
b61936a8-de49-4e2e-9563-e4f1c16e0494,Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors,0.604992,10,"Current popular backbones in computer vision, such as Vision Transformers
(ViT) and ResNets are trained to perceive the world from 2D images. However, to
more effectively understand 3D structural priors in 2D backbones, we propose
Mask3D to leverage existing large-scale RGB-D data in a self-supervised
pre-training to embed these 3D priors into 2D learned feature representations.
In contrast to traditional 3D contrastive learning paradigms requiring 3D
reconstructions or multi-view correspondences, our approach is simple: we
formulate a pre-text reconstruction task by masking RGB and depth patches in
individual RGB-D frames. We demonstrate the Mask3D is particularly effective in
embedding 3D priors into the powerful 2D ViT backbone, enabling improved
representation learning for various scene understanding tasks, such as semantic
segmentation, instance segmentation and object detection. Experiments show that
Mask3D notably outperforms existing self-supervised 3D pre-training approaches
on ScanNet, NYUv2, and Cityscapes image understanding tasks, with an
improvement of +6.5% mIoU against the state-of-the-art Pri3D on ScanNet image
semantic segmentation.",None,-1
96694b24-ca8e-444b-8487-b3dea1ad03f7,DDP: Diffusion Model for Dense Visual Prediction,0.987893,56,"We propose a simple, efficient, yet powerful framework for dense visual
predictions based on the conditional diffusion pipeline. Our approach follows a
""noise-to-map"" generative paradigm for prediction by progressively removing
noise from a random Gaussian distribution, guided by the image. The method,
called DDP, efficiently extends the denoising diffusion process into the modern
perception pipeline. Without task-specific design and architecture
customization, DDP is easy to generalize to most dense prediction tasks, e.g.,
semantic segmentation and depth estimation. In addition, DDP shows attractive
properties such as dynamic inference and uncertainty awareness, in contrast to
previous single-step discriminative methods. We show top results on three
representative tasks with six diverse benchmarks, without tricks, DDP achieves
state-of-the-art or competitive performance on each task compared to the
specialist counterparts. For example, semantic segmentation (83.9 mIoU on
Cityscapes), BEV map segmentation (70.6 mIoU on nuScenes), and depth estimation
(0.05 REL on KITTI). We hope that our approach will serve as a solid baseline
and facilitate future research",None,-1
e997a552-8238-4cb2-8bee-687910ae8586,Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data,0.188949,1,"Multivariate time series (MTS) data collected from multiple sensors provide
the potential for accurate abnormal activity detection in smart healthcare
scenarios. However, anomalies exhibit diverse patterns and become unnoticeable
in MTS data. Consequently, achieving accurate anomaly detection is challenging
since we have to capture both temporal dependencies of time series and
inter-relationships among variables. To address this problem, we propose a
Residual-based Anomaly Detection approach, Rs-AD, for effective representation
learning and abnormal activity detection. We evaluate our scheme on a
real-world gait dataset and the experimental results demonstrate an F1 score of
0.839.",None,-1
3616370b-6143-4a21-b483-b573586016c8,Towards Self-Explainability of Deep Neural Networks with Heatmap Captioning and Large-Language Models,0.506483,4,"Heatmaps are widely used to interpret deep neural networks, particularly for
computer vision tasks, and the heatmap-based explainable AI (XAI) techniques
are a well-researched topic. However, most studies concentrate on enhancing the
quality of the generated heatmap or discovering alternate heatmap generation
techniques, and little effort has been devoted to making heatmap-based XAI
automatic, interactive, scalable, and accessible. To address this gap, we
propose a framework that includes two modules: (1) context modelling and (2)
reasoning. We proposed a template-based image captioning approach for context
modelling to create text-based contextual information from the heatmap and
input data. The reasoning module leverages a large language model to provide
explanations in combination with specialised knowledge. Our qualitative
experiments demonstrate the effectiveness of our framework and heatmap
captioning approach. The code for the proposed template-based heatmap
captioning approach will be publicly available.",None,-1
c87bad5b-830a-4c68-998b-525b622302d5,Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding,0.286313,8,"Recently, large language models (LLMs) have made significant advancements in
natural language understanding and generation. However, their potential in
computer vision remains largely unexplored. In this paper, we introduce a new,
exploratory approach that enables LLMs to process images using the Scalable
Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions
of SVG representations instead of raster images, we aim to bridge the gap
between the visual and textual modalities, allowing LLMs to directly understand
and manipulate images without the need for parameterized visual components. Our
method facilitates simple image classification, generation, and in-context
learning using only LLM capabilities. We demonstrate the promise of our
approach across discriminative and generative tasks, highlighting its (i)
robustness against distribution shift, (ii) substantial improvements achieved
by tapping into the in-context learning abilities of LLMs, and (iii) image
understanding and generation capabilities with human guidance. Our code, data,
and models can be found here https://github.com/mu-cai/svg-llm.",None,-1
6a8cd841-2265-4491-a659-8d71afec93ff,Massively Multilingual Language Models for Cross Lingual Fact Extraction from Low Resource Indian Languages,0.230416,2,"Massive knowledge graphs like Wikidata attempt to capture world knowledge
about multiple entities. Recent approaches concentrate on automatically
enriching these KGs from text. However a lot of information present in the form
of natural text in low resource languages is often missed out. Cross Lingual
Information Extraction aims at extracting factual information in the form of
English triples from low resource Indian Language text. Despite its massive
potential, progress made on this task is lagging when compared to Monolingual
Information Extraction. In this paper, we propose the task of Cross Lingual
Fact Extraction(CLFE) from text and devise an end-to-end generative approach
for the same which achieves an overall F1 score of 77.46.",None,-1
9af385ac-64ec-445b-a6d8-7e385e59ca70,Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference,0.167455,2,"The warming of the Arctic, also known as Arctic amplification, is led by
several atmospheric and oceanic drivers. However, the details of its underlying
thermodynamic causes are still unknown. Inferring the causal effects of
atmospheric processes on sea ice melt using fixed treatment effect strategies
leads to unrealistic counterfactual estimations. Such models are also prone to
bias due to time-varying confoundedness. Further, the complex non-linearity in
Earth science data makes it infeasible to perform causal inference using
existing marginal structural techniques. In order to tackle these challenges,
we propose TCINet - time-series causal inference model to infer causation under
continuous treatment using recurrent neural networks and a novel probabilistic
balancing technique. Through experiments on synthetic and observational data,
we show how our research can substantially improve the ability to quantify
leading causes of Arctic sea ice melt, further paving paths for causal
inference in observational Earth science.",None,-1
83ffd7f0-c5e2-4514-98dc-40e81d9a1aef,A Formal Perspective on Byte-Pair Encoding,0.739265,11,"Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in
NLP, despite being devised initially as a compression method. BPE appears to be
a greedy algorithm at face value, but the underlying optimization problem that
BPE seeks to solve has not yet been laid down. We formalize BPE as a
combinatorial optimization problem. Via submodular functions, we prove that the
iterative greedy version is a
$\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-approximation
of an optimal merge sequence, where ${\sigma(\boldsymbol{\mu}^\star)}$ is the
total backward curvature with respect to the optimal merge sequence
$\boldsymbol{\mu}^\star$. Empirically the lower bound of the approximation is
$\approx 0.37$.
  We provide a faster implementation of BPE which improves the runtime
complexity from $\mathcal{O}\left(N M\right)$ to $\mathcal{O}\left(N \log
M\right)$, where $N$ is the sequence length and $M$ is the merge count.
Finally, we optimize the brute-force algorithm for optimal BPE using
memoization.",None,-1
5d370cfb-9f35-4391-af3e-af20730580ad,Boundary-weighted logit consistency improves calibration of segmentation networks,0.0743001,2,"Neural network prediction probabilities and accuracy are often only
weakly-correlated. Inherent label ambiguity in training data for image
segmentation aggravates such miscalibration. We show that logit consistency
across stochastic transformations acts as a spatially varying regularizer that
prevents overconfident predictions at pixels with ambiguous labels. Our
boundary-weighted extension of this regularizer provides state-of-the-art
calibration for prostate and heart MRI segmentation.",None,-1
7ec12ec1-147b-46a6-8a27-af763413a059,Cone: Unsupervised Contrastive Opinion Extraction,0.251943,1,"Contrastive opinion extraction aims to extract a structured summary or key
points organised as positive and negative viewpoints towards a common aspect or
topic. Most recent works for unsupervised key point extraction is largely built
on sentence clustering or opinion summarisation based on the popularity of
opinions expressed in text. However, these methods tend to generate aspect
clusters with incoherent sentences, conflicting viewpoints, redundant aspects.
To address these problems, we propose a novel unsupervised Contrastive OpinioN
Extraction model, called Cone, which learns disentangled latent aspect and
sentiment representations based on pseudo aspect and sentiment labels by
combining contrastive learning with iterative aspect/sentiment clustering
refinement. Apart from being able to extract contrastive opinions, it is also
able to quantify the relative popularity of aspects and their associated
sentiment distributions. The model has been evaluated on both a hotel review
dataset and a Twitter dataset about COVID vaccines. The results show that
despite using no label supervision or aspect-denoted seed words, Cone
outperforms a number of competitive baselines on contrastive opinion
extraction. The results of Cone can be used to offer a better recommendation of
products and services online.",None,-1
f841e596-33d7-4f9c-a59c-65380578fb80,Learning Domain-Independent Heuristics for Grounded and Lifted Planning,0.335351,5,"We present three novel graph representations of planning tasks suitable for
learning domain-independent heuristics using Graph Neural Networks (GNNs) to
guide search. In particular, to mitigate the issues caused by large grounded
GNNs we present the first method for learning domain-independent heuristics
with only the lifted representation of a planning task. We also provide a
theoretical analysis of the expressiveness of our models, showing that some are
more powerful than STRIPS-HGN, the only other existing model for learning
domain-independent heuristics. Our experiments show that our heuristics
generalise to much larger problems than those in the training set, vastly
surpassing STRIPS-HGN heuristics.",None,-1
cd9d6677-d1f5-4f4d-9ece-4c3ef004e529,Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models,0.893902,97,"We present Text2Room, a method for generating room-scale textured 3D meshes
from a given text prompt as input. To this end, we leverage pre-trained 2D
text-to-image models to synthesize a sequence of images from different poses.
In order to lift these outputs into a consistent 3D scene representation, we
combine monocular depth estimation with a text-conditioned inpainting model.
The core idea of our approach is a tailored viewpoint selection such that the
content of each image can be fused into a seamless, textured 3D mesh. More
specifically, we propose a continuous alignment strategy that iteratively fuses
scene frames with the existing geometry to create a seamless mesh. Unlike
existing works that focus on generating single objects or zoom-out trajectories
from text, our method generates complete 3D scenes with multiple objects and
explicit 3D geometry. We evaluate our approach using qualitative and
quantitative metrics, demonstrating it as the first method to generate
room-scale 3D geometry with compelling textures from only text as input.",None,-1
58395591-3795-494f-bebd-1dd8ee7b447b,The Promise and Peril of Artificial Intelligence -- Violet Teaming Offers a Balanced Path Forward,0.0738586,1,"Artificial intelligence (AI) promises immense benefits across sectors, yet
also poses risks from dual-use potentials, biases, and unintended behaviors.
This paper reviews emerging issues with opaque and uncontrollable AI systems
and proposes an integrative framework called violet teaming to develop reliable
and responsible AI. Violet teaming combines adversarial vulnerability probing
(red teaming) with solutions for safety and security (blue teaming) while
prioritizing ethics and social benefit. It emerged from AI safety research to
manage risks proactively by design. The paper traces the evolution of red,
blue, and purple teaming toward violet teaming, and then discusses applying
violet techniques to address biosecurity risks of AI in biotechnology.
Additional sections review key perspectives across law, ethics, cybersecurity,
macrostrategy, and industry best practices essential for operationalizing
responsible AI through holistic technical and social considerations. Violet
teaming provides both philosophy and method for steering AI trajectories toward
societal good. With conscience and wisdom, the extraordinary capabilities of AI
can enrich humanity. But without adequate precaution, the risks could prove
catastrophic. Violet teaming aims to empower moral technology for the common
welfare.",None,-1
6e41f41e-d741-430e-8346-30f1edc06798,HoloDiffusion: Training a 3D Diffusion Model using 2D Images,0.821449,78,"Diffusion models have emerged as the best approach for generative modeling of
2D images. Part of their success is due to the possibility of training them on
millions if not billions of images with a stable learning objective. However,
extending these models to 3D remains difficult for two reasons. First, finding
a large quantity of 3D training data is much more complex than for 2D images.
Second, while it is conceptually trivial to extend the models to operate on 3D
rather than 2D grids, the associated cubic growth in memory and compute
complexity makes this infeasible. We address the first challenge by introducing
a new diffusion setup that can be trained, end-to-end, with only posed 2D
images for supervision; and the second challenge by proposing an image
formation model that decouples model memory from spatial memory. We evaluate
our method on real-world data, using the CO3D dataset which has not been used
to train 3D generative models before. We show that our diffusion models are
scalable, train robustly, and are competitive in terms of sample quality and
fidelity to existing approaches for 3D generative modeling.",None,-1
ab028c50-52d1-477f-a4bc-9f29c53d68ea,DevelSet: Deep Neural Level Set for Instant Mask Optimization,0.917004,31,"With the feature size continuously shrinking in advanced technology nodes,
mask optimization is increasingly crucial in the conventional design flow,
accompanied by an explosive growth in prohibitive computational overhead in
optical proximity correction (OPC) methods. Recently, inverse lithography
technique (ILT) has drawn significant attention and is becoming prevalent in
emerging OPC solutions. However, ILT methods are either time-consuming or in
weak performance of mask printability and manufacturability. In this paper, we
present DevelSet, a GPU and deep neural network (DNN) accelerated level set OPC
framework for metal layer. We first improve the conventional level set-based
ILT algorithm by introducing the curvature term to reduce mask complexity and
applying GPU acceleration to overcome computational bottlenecks. To further
enhance printability and fast iterative convergence, we propose a novel deep
neural network delicately designed with level set intrinsic principles to
facilitate the joint optimization of DNN and GPU accelerated level set
optimizer. Experimental results show that DevelSet framework surpasses the
state-of-the-art methods in printability and boost the runtime performance
achieving instant level (around 1 second).",None,-1
9f1584d1-9482-4ed4-8a8e-298fa528ea92,Balanced Energy Regularization Loss for Out-of-distribution Detection,0.746105,4,"In the field of out-of-distribution (OOD) detection, a previous method that
use auxiliary data as OOD data has shown promising performance. However, the
method provides an equal loss to all auxiliary data to differentiate them from
inliers. However, based on our observation, in various tasks, there is a
general imbalance in the distribution of the auxiliary OOD data across classes.
We propose a balanced energy regularization loss that is simple but generally
effective for a variety of tasks. Our balanced energy regularization loss
utilizes class-wise different prior probabilities for auxiliary data to address
the class imbalance in OOD data. The main concept is to regularize auxiliary
samples from majority classes, more heavily than those from minority classes.
Our approach performs better for OOD detection in semantic segmentation,
long-tailed image classification, and image classification than the prior
energy regularization loss. Furthermore, our approach achieves state-of-the-art
performance in two tasks: OOD detection in semantic segmentation and
long-tailed image classification. Code is available at
https://github.com/hyunjunChhoi/Balanced_Energy.",None,-1
cb22b410-77c7-48ce-850c-a0783514e9cf,Emergent and Predictable Memorization in Large Language Models,0.383328,66,"Memorization, or the tendency of large language models (LLMs) to output
entire sequences from their training data verbatim, is a key concern for safely
deploying language models. In particular, it is vital to minimize a model's
memorization of sensitive datapoints such as those containing personal
identifiable information (PII). The prevalence of such undesirable memorization
can pose issues for model trainers, and may even require discarding an
otherwise functional model. We therefore seek to predict which sequences will
be memorized before a large model's full train-time by extrapolating the
memorization behavior of lower-compute trial runs. We measure memorization of
the Pythia model suite and plot scaling laws for forecasting memorization,
allowing us to provide equi-compute recommendations to maximize the reliability
(recall) of such predictions. We additionally provide further novel discoveries
on the distribution of memorization scores across models and data. We release
all code and data necessary to reproduce the results in this paper at
https://github.com/EleutherAI/pythia",None,-1
9f049fe7-17ab-49c3-b3a4-491be1b04b9f,Cross-lingual German Biomedical Information Extraction: from Zero-shot to Human-in-the-Loop,0.525956,3,"This paper presents our project proposal for extracting biomedical
information from German clinical narratives with limited amounts of
annotations. We first describe the applied strategies in transfer learning and
active learning for solving our problem. After that, we discuss the design of
the user interface for both supplying model inspection and obtaining user
annotations in the interactive environment.",None,-1
2bfc4078-18fa-438e-9385-e217b61c4205,Rethinking the Localization in Weakly Supervised Object Localization,0.466265,2,"Weakly supervised object localization (WSOL) is one of the most popular and
challenging tasks in computer vision. This task is to localize the objects in
the images given only the image-level supervision. Recently, dividing WSOL into
two parts (class-agnostic object localization and object classification) has
become the state-of-the-art pipeline for this task. However, existing solutions
under this pipeline usually suffer from the following drawbacks: 1) they are
not flexible since they can only localize one object for each image due to the
adopted single-class regression (SCR) for localization; 2) the generated pseudo
bounding boxes may be noisy, but the negative impact of such noise is not well
addressed. To remedy these drawbacks, we first propose to replace SCR with a
binary-class detector (BCD) for localizing multiple objects, where the detector
is trained by discriminating the foreground and background. Then we design a
weighted entropy (WE) loss using the unlabeled data to reduce the negative
impact of noisy bounding boxes. Extensive experiments on the popular
CUB-200-2011 and ImageNet-1K datasets demonstrate the effectiveness of our
method.",None,-1
1483988d-a2b6-422f-a1d2-eb1162cdbbaf,Toward Unsupervised Realistic Visual Question Answering,0.0722922,2,"The problem of realistic VQA (RVQA), where a model has to reject unanswerable
questions (UQs) and answer answerable ones (AQs), is studied. We first point
out 2 drawbacks in current RVQA research, where (1) datasets contain too many
unchallenging UQs and (2) a large number of annotated UQs are required for
training. To resolve the first drawback, we propose a new testing dataset,
RGQA, which combines AQs from an existing VQA dataset with around 29K
human-annotated UQs. These UQs consist of both fine-grained and coarse-grained
image-question pairs generated with 2 approaches: CLIP-based and
Perturbation-based. To address the second drawback, we introduce an
unsupervised training approach. This combines pseudo UQs obtained by randomly
pairing images and questions, with an RoI Mixup procedure to generate more
fine-grained pseudo UQs, and model ensembling to regularize model confidence.
Experiments show that using pseudo UQs significantly outperforms RVQA
baselines. RoI Mixup and model ensembling further increase the gain. Finally,
human evaluation reveals a performance gap between humans and models, showing
that more RVQA research is needed.",None,-1
3f488bda-df2c-40b7-8417-2fd0c062f5a2,Learning Efficient Representations for Image-Based Patent Retrieval,0.264303,1,"Patent retrieval has been attracting tremendous interest from researchers in
intellectual property and information retrieval communities in the past
decades. However, most existing approaches rely on textual and metadata
information of the patent, and content-based image-based patent retrieval is
rarely investigated. Based on traits of patent drawing images, we present a
simple and lightweight model for this task. Without bells and whistles, this
approach significantly outperforms other counterparts on a large-scale
benchmark and noticeably improves the state-of-the-art by 33.5% with the mean
average precision (mAP) score. Further experiments reveal that this model can
be elaborately scaled up to achieve a surprisingly high mAP of 93.5%. Our
method ranks first in the ECCV 2022 Patent Diagram Image Retrieval Challenge.",None,-1
092cf19e-3366-4d35-98e3-48ee86d2a131,GuidedMixup: An Efficient Mixup Strategy Guided by Saliency Maps,0.483634,6,"Data augmentation is now an essential part of the image training process, as
it effectively prevents overfitting and makes the model more robust against
noisy datasets. Recent mixing augmentation strategies have advanced to generate
the mixup mask that can enrich the saliency information, which is a supervisory
signal. However, these methods incur a significant computational burden to
optimize the mixup mask. From this motivation, we propose a novel
saliency-aware mixup method, GuidedMixup, which aims to retain the salient
regions in mixup images with low computational overhead. We develop an
efficient pairing algorithm that pursues to minimize the conflict of salient
regions of paired images and achieve rich saliency in mixup images. Moreover,
GuidedMixup controls the mixup ratio for each pixel to better preserve the
salient region by interpolating two paired images smoothly. The experiments on
several datasets demonstrate that GuidedMixup provides a good trade-off between
augmentation overhead and generalization performance on classification
datasets. In addition, our method shows good performance in experiments with
corrupted or reduced datasets.",None,-1
ea8683d6-eff5-491c-a89c-d07b369d9514,RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing,0.823442,6,"Reaction diagram parsing is the task of extracting reaction schemes from a
diagram in the chemistry literature. The reaction diagrams can be arbitrarily
complex, thus robustly parsing them into structured data is an open challenge.
In this paper, we present RxnScribe, a machine learning model for parsing
reaction diagrams of varying styles. We formulate this structured prediction
task with a sequence generation approach, which condenses the traditional
pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378
diagrams and evaluate it with cross validation, achieving an 80.0% soft match
F1 score, with significant improvements over previous models. Our code and data
are publicly available at https://github.com/thomas0809/RxnScribe.",None,-1
54fb37cc-602e-4149-84ca-b30019b40567,Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study,0.098361,1,"Text classification is an area of research which has been studied over the
years in Natural Language Processing (NLP). Adapting NLP to multiple domains
has introduced many new challenges for text classification and one of them is
long document classification. While state-of-the-art transformer models provide
excellent results in text classification, most of them have limitations in the
maximum sequence length of the input sequence. The majority of the transformer
models are limited to 512 tokens, and therefore, they struggle with long
document classification problems. In this research, we explore on employing
Model Fusing for long document classification while comparing the results with
well-known BERT and Longformer architectures.",None,-1
d3cde918-22f4-4b8d-b0d8-35096ff19834,RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting,0.190128,20,"Large Language Models (LLMs) have demonstrated impressive capabilities in
creative tasks such as storytelling and E-mail generation. However, as LLMs are
primarily trained on final text results rather than intermediate revisions, it
might be challenging for them to perform text rewriting tasks. Most studies in
the rewriting tasks focus on a particular transformation type within the
boundaries of single sentences. In this work, we develop new strategies for
instruction tuning and reinforcement learning to better align LLMs for
cross-sentence rewriting tasks using diverse wording and structures expressed
through natural languages including 1) generating rewriting instruction data
from Wiki edits and public corpus through instruction generation and
chain-of-thought prompting; 2) collecting comparison data for reward model
training through a new ranking function. To facilitate this research, we
introduce OpenRewriteEval, a novel benchmark covers a wide variety of rewriting
types expressed through natural language instructions. Our results show
significant improvements over a variety of baselines. The public repository is
available on GitHub under Google Research
(https://github.com/google-research/google-research/tree/master/rewritelm).",None,-1
c8160b38-653d-4d04-a711-653393d077d9,LOAF-M2L: Joint Learning of Wording and Formatting for Singable Melody-to-Lyric Generation,0.411049,1,"Despite previous efforts in melody-to-lyric generation research, there is
still a significant compatibility gap between generated lyrics and melodies,
negatively impacting the singability of the outputs. This paper bridges the
singability gap with a novel approach to generating singable lyrics by jointly
Learning wOrding And Formatting during Melody-to-Lyric training (LOAF-M2L).
After general-domain pretraining, our proposed model acquires length awareness
first from a large text-only lyric corpus. Then, we introduce a new objective
informed by musicological research on the relationship between melody and
lyrics during melody-to-lyric training, which enables the model to learn the
fine-grained format requirements of the melody. Our model achieves 3.75% and
21.44% absolute accuracy gains in the outputs' number-of-line and
syllable-per-line requirements compared to naive fine-tuning, without
sacrificing text fluency. Furthermore, our model demonstrates a 63.92% and
74.18% relative improvement of music-lyric compatibility and overall quality in
the subjective evaluation, compared to the state-of-the-art melody-to-lyric
generation model, highlighting the significance of formatting learning.",None,-1
8eac789b-5cd9-423e-b960-6867623c967c,Almanac: Retrieval-Augmented Language Models for Clinical Medicine,0.994767,51,"Large-language models have recently demonstrated impressive zero-shot
capabilities in a variety of natural language tasks such as summarization,
dialogue generation, and question-answering. Despite many promising
applications in clinical medicine, adoption of these models in real-world
settings has been largely limited by their tendency to generate incorrect and
sometimes even toxic statements. In this study, we develop Almanac, a large
language model framework augmented with retrieval capabilities for medical
guideline and treatment recommendations. Performance on a novel dataset of
clinical scenarios (n = 130) evaluated by a panel of 5 board-certified and
resident physicians demonstrates significant increases in factuality (mean of
18% at p-value < 0.05) across all specialties, with improvements in
completeness and safety. Our results demonstrate the potential for large
language models to be effective tools in the clinical decision-making process,
while also emphasizing the importance of careful testing and deployment to
mitigate their shortcomings.",None,-1
81a36d9c-7071-4354-b393-6f89d750b92e,MPCHAT: Towards Multimodal Persona-Grounded Conversation,0.719124,12,"In order to build self-consistent personalized dialogue agents, previous
research has mostly focused on textual persona that delivers personal facts or
personalities. However, to fully describe the multi-faceted nature of persona,
image modality can help better reveal the speaker's personal characteristics
and experiences in episodic memory (Rubin et al., 2003; Conway, 2009). In this
work, we extend persona-based dialogue to the multimodal domain and make two
main contributions. First, we present the first multimodal persona-based
dialogue dataset named MPCHAT, which extends persona with both text and images
to contain episodic memories. Second, we empirically show that incorporating
multimodal persona, as measured by three proposed multimodal persona-grounded
dialogue tasks (i.e., next response prediction, grounding persona prediction,
and speaker identification), leads to statistically significant performance
improvements across all tasks. Thus, our work highlights that multimodal
persona is crucial for improving multimodal dialogue comprehension, and our
MPCHAT serves as a high-quality resource for this research.",None,-1
09b1228c-842e-4f8e-8a63-4d2d22c91285,Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8,0.937686,7,"This paper introduces a software architecture for real-time object detection
using machine learning (ML) in an augmented reality (AR) environment. Our
approach uses the recent state-of-the-art YOLOv8 network that runs onboard on
the Microsoft HoloLens 2 head-mounted display (HMD). The primary motivation
behind this research is to enable the application of advanced ML models for
enhanced perception and situational awareness with a wearable, hands-free AR
platform. We show the image processing pipeline for the YOLOv8 model and the
techniques used to make it real-time on the resource-limited edge computing
platform of the headset. The experimental results demonstrate that our solution
achieves real-time processing without needing offloading tasks to the cloud or
any other external servers while retaining satisfactory accuracy regarding the
usual mAP metric and measured qualitative performance",None,-1
335415d7-1065-44eb-ae62-8c409f055602,StyLess: Boosting the Transferability of Adversarial Examples,0.448485,7,"Adversarial attacks can mislead deep neural networks (DNNs) by adding
imperceptible perturbations to benign examples. The attack transferability
enables adversarial examples to attack black-box DNNs with unknown
architectures or parameters, which poses threats to many real-world
applications. We find that existing transferable attacks do not distinguish
between style and content features during optimization, limiting their attack
transferability. To improve attack transferability, we propose a novel attack
method called style-less perturbation (StyLess). Specifically, instead of using
a vanilla network as the surrogate model, we advocate using stylized networks,
which encode different style features by perturbing an adaptive instance
normalization. Our method can prevent adversarial examples from using
non-robust style features and help generate transferable perturbations.
Comprehensive experiments show that our method can significantly improve the
transferability of adversarial examples. Furthermore, our approach is generic
and can outperform state-of-the-art transferable attacks when combined with
other attack techniques.",None,-1
a6e523ed-0a9d-4108-b925-17fca1b5b7cf,"ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design",0.997894,87,"This paper presents prompt design techniques for software engineering, in the
form of patterns, to solve common problems when using large language models
(LLMs), such as ChatGPT to automate common software engineering activities,
such as ensuring code is decoupled from third-party libraries and simulating a
web application API before it is implemented. This paper provides two
contributions to research on using LLMs for software engineering. First, it
provides a catalog of patterns for software engineering that classifies
patterns according to the types of problems they solve. Second, it explores
several prompt patterns that have been applied to improve requirements
elicitation, rapid prototyping, code quality, refactoring, and system design.",None,-1
f0630bd0-a54e-4556-8471-43a492acc3ba,Deep Metric Multi-View Hashing for Multimedia Retrieval,0.795111,5,"Learning the hash representation of multi-view heterogeneous data is an
important task in multimedia retrieval. However, existing methods fail to
effectively fuse the multi-view features and utilize the metric information
provided by the dissimilar samples, leading to limited retrieval precision.
Current methods utilize weighted sum or concatenation to fuse the multi-view
features. We argue that these fusion methods cannot capture the interaction
among different views. Furthermore, these methods ignored the information
provided by the dissimilar samples. We propose a novel deep metric multi-view
hashing (DMMVH) method to address the mentioned problems. Extensive empirical
evidence is presented to show that gate-based fusion is better than typical
methods. We introduce deep metric learning to the multi-view hashing problems,
which can utilize metric information of dissimilar samples. On the
MIR-Flickr25K, MS COCO, and NUS-WIDE, our method outperforms the current
state-of-the-art methods by a large margin (up to 15.28 mean Average Precision
(mAP) improvement).",None,-1
cdc3f85a-cd0e-477d-97ad-30aaefa8c283,BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering,0.164201,6,"Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as
the construction of commonsense reasoning datasets is expensive, and they are
inevitably limited in their scope. A popular approach to UCR is to fine-tune
language models with external knowledge (e.g., knowledge graphs), but this
usually requires a large number of training examples. In this paper, we propose
to transform the downstream multiple choice question answering task into a
simpler binary classification task by ranking all candidate answers according
to their reasonableness. To this end, for training the model, we convert the
knowledge graph triples into reasonable and unreasonable texts. Extensive
experimental results show the effectiveness of our approach on various multiple
choice question answering benchmarks. Furthermore, compared with existing UCR
approaches using KGs, ours is less data hungry. Our code is available at
https://github.com/probe2/BUCA.",None,-1
d96d3c2f-ae33-4b33-91bf-83e7533d1dd4,LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation,0.800991,33,"Layout-to-image generation refers to the task of synthesizing photo-realistic
images based on semantic layouts. In this paper, we propose LayoutDiffuse that
adapts a foundational diffusion model pretrained on large-scale image or
text-image datasets for layout-to-image generation. By adopting a novel neural
adaptor based on layout attention and task-aware prompts, our method trains
efficiently, generates images with both high perceptual quality and layout
alignment, and needs less data. Experiments on three datasets show that our
method significantly outperforms other 10 generative models based on GANs,
VQ-VAE, and diffusion models.",None,-1
33836917-d83b-40ed-8452-bdf67054693c,Enhancing Efficient Continual Learning with Dynamic Structure Development of Spiking Neural Networks,0.79931,6,"Children possess the ability to learn multiple cognitive tasks sequentially,
which is a major challenge toward the long-term goal of artificial general
intelligence. Existing continual learning frameworks are usually applicable to
Deep Neural Networks (DNNs) and lack the exploration on more brain-inspired,
energy-efficient Spiking Neural Networks (SNNs). Drawing on continual learning
mechanisms during child growth and development, we propose Dynamic Structure
Development of Spiking Neural Networks (DSD-SNN) for efficient and adaptive
continual learning. When learning a sequence of tasks, the DSD-SNN dynamically
assigns and grows new neurons to new tasks and prunes redundant neurons,
thereby increasing memory capacity and reducing computational overhead. In
addition, the overlapping shared structure helps to quickly leverage all
acquired knowledge to new tasks, empowering a single network capable of
supporting multiple incremental tasks (without the separate sub-network mask
for each task). We validate the effectiveness of the proposed model on multiple
class incremental learning and task incremental learning benchmarks. Extensive
experiments demonstrated that our model could significantly improve
performance, learning speed and memory capacity, and reduce computational
overhead. Besides, our DSD-SNN model achieves comparable performance with the
DNNs-based methods, and significantly outperforms the state-of-the-art (SOTA)
performance for existing SNNs-based continual learning methods.",None,-1
1709902f-a57f-478f-907d-01b6cfd7fd00,Learning Structure-Guided Diffusion Model for 2D Human Pose Estimation,0.797734,7,"One of the mainstream schemes for 2D human pose estimation (HPE) is learning
keypoints heatmaps by a neural network. Existing methods typically improve the
quality of heatmaps by customized architectures, such as high-resolution
representation and vision Transformers. In this paper, we propose
\textbf{DiffusionPose}, a new scheme that formulates 2D HPE as a keypoints
heatmaps generation problem from noised heatmaps. During training, the
keypoints are diffused to random distribution by adding noises and the
diffusion model learns to recover ground-truth heatmaps from noised heatmaps
with respect to conditions constructed by image feature. During inference, the
diffusion model generates heatmaps from initialized heatmaps in a progressive
denoising way. Moreover, we further explore improving the performance of
DiffusionPose with conditions from human structural information. Extensive
experiments show the prowess of our DiffusionPose, with improvements of 1.6,
1.2, and 1.2 mAP on widely-used COCO, CrowdPose, and AI Challenge datasets,
respectively.",None,-1
19164fef-ad1b-4a2a-a62f-f6d6f8513d1c,Urban Generative Intelligence (UGI): A Foundational Platform for Agents in Embodied City Environment,0.548843,8,"Urban environments, characterized by their complex, multi-layered networks
encompassing physical, social, economic, and environmental dimensions, face
significant challenges in the face of rapid urbanization. These challenges,
ranging from traffic congestion and pollution to social inequality, call for
advanced technological interventions. Recent developments in big data,
artificial intelligence, urban computing, and digital twins have laid the
groundwork for sophisticated city modeling and simulation. However, a gap
persists between these technological capabilities and their practical
implementation in addressing urban challenges in an systemic-intelligent way.
This paper proposes Urban Generative Intelligence (UGI), a novel foundational
platform integrating Large Language Models (LLMs) into urban systems to foster
a new paradigm of urban intelligence. UGI leverages CityGPT, a foundation model
trained on city-specific multi-source data, to create embodied agents for
various urban tasks. These agents, operating within a textual urban environment
emulated by city simulator and urban knowledge graph, interact through a
natural language interface, offering an open platform for diverse intelligent
and embodied agent development. This platform not only addresses specific urban
issues but also simulates complex urban systems, providing a multidisciplinary
approach to understand and manage urban complexity. This work signifies a
transformative step in city science and urban intelligence, harnessing the
power of LLMs to unravel and address the intricate dynamics of urban systems.
The code repository with demonstrations will soon be released here
https://github.com/tsinghua-fib-lab/UGI.",None,-1
5e7dcbe2-0098-4d1c-907d-b611188fefe4,Anomaly Detection of Command Shell Sessions based on DistilBERT: Unsupervised and Supervised Approaches,0.2822,2,"Anomaly detection in command shell sessions is a critical aspect of computer
security. Recent advances in deep learning and natural language processing,
particularly transformer-based models, have shown great promise for addressing
complex security challenges. In this paper, we implement a comprehensive
approach to detect anomalies in Unix shell sessions using a pretrained
DistilBERT model, leveraging both unsupervised and supervised learning
techniques to identify anomalous activity while minimizing data labeling. The
unsupervised method captures the underlying structure and syntax of Unix shell
commands, enabling the detection of session deviations from normal behavior.
Experiments on a large-scale enterprise dataset collected from production
systems demonstrate the effectiveness of our approach in detecting anomalous
behavior in Unix shell sessions. This work highlights the potential of
leveraging recent advances in transformers to address important computer
security challenges.",None,-1
dba3f44b-26ed-4dd0-bfa4-0b836a52a5ee,Predicting Spine Geometry and Scoliosis from DXA Scans,0.551582,2,"Our objective in this paper is to estimate spine curvature in DXA scans. To
this end we first train a neural network to predict the middle spine curve in
the scan, and then use an integral-based method to determine the curvature
along the spine curve. We use the curvature to compare to the standard angle
scoliosis measure obtained using the DXA Scoliosis Method (DSM). The
performance improves over the prior work of Jamaludin et al. 2018. We show that
the maximum curvature can be used as a scoring function for ordering the
severity of spinal deformation.",None,-1
f379f2a7-044d-4bef-a520-f8fc432df276,CLRerNet: Improving Confidence of Lane Detection with LaneIoU,0.478081,9,"Lane marker detection is a crucial component of the autonomous driving and
driver assistance systems. Modern deep lane detection methods with row-based
lane representation exhibit excellent performance on lane detection benchmarks.
Through preliminary oracle experiments, we firstly disentangle the lane
representation components to determine the direction of our approach. We show
that correct lane positions are already among the predictions of an existing
row-based detector, and the confidence scores that accurately represent
intersection-over-union (IoU) with ground truths are the most beneficial. Based
on the finding, we propose LaneIoU that better correlates with the metric, by
taking the local lane angles into consideration. We develop a novel detector
coined CLRerNet featuring LaneIoU for the target assignment cost and loss
functions aiming at the improved quality of confidence scores. Through careful
and fair benchmark including cross validation, we demonstrate that CLRerNet
outperforms the state-of-the-art by a large margin - enjoying F1 score of
81.43% compared with 80.47% of the existing method on CULane, and 86.47%
compared with 86.10% on CurveLanes.",None,-1
3a82c10b-2b30-40dc-8e42-35e141505084,Fast model inference and training on-board of Satellites,0.652435,4,"Artificial intelligence onboard satellites has the potential to reduce data
transmission requirements, enable real-time decision-making and collaboration
within constellations. This study deploys a lightweight foundational model
called RaVAEn on D-Orbit's ION SCV004 satellite. RaVAEn is a variational
auto-encoder (VAE) that generates compressed latent vectors from small image
tiles, enabling several downstream tasks. In this work we demonstrate the
reliable use of RaVAEn onboard a satellite, achieving an encoding time of
0.110s for tiles of a 4.8x4.8 km$^2$ area. In addition, we showcase fast
few-shot training onboard a satellite using the latent representation of data.
We compare the deployment of the model on the on-board CPU and on the available
Myriad vision processing unit (VPU) accelerator. To our knowledge, this work
shows for the first time the deployment of a multi-task model on-board a
CubeSat and the on-board training of a machine learning model.",None,-1
b55047e0-af06-4e45-ae40-708b4f07869e,GameGPT: Multi-agent Collaborative Framework for Game Development,0.990941,12,"The large language model (LLM) based agents have demonstrated their capacity
to automate and expedite software development processes. In this paper, we
focus on game development and propose a multi-agent collaborative framework,
dubbed GameGPT, to automate game development. While many studies have
pinpointed hallucination as a primary roadblock for deploying LLMs in
production, we identify another concern: redundancy. Our framework presents a
series of methods to mitigate both concerns. These methods include dual
collaboration and layered approaches with several in-house lexicons, to
mitigate the hallucination and redundancy in the planning, task identification,
and implementation phases. Furthermore, a decoupling approach is also
introduced to achieve code generation with better precision.",None,-1
f782d452-8f26-4fe0-95c0-a37b093f7925,"Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion",0.525978,31,"Producing quality segmentation masks for images is a fundamental problem in
computer vision. Recent research has explored large-scale supervised training
to enable zero-shot segmentation on virtually any image style and unsupervised
training to enable segmentation without dense annotations. However,
constructing a model capable of segmenting anything in a zero-shot manner
without any annotations is still challenging. In this paper, we propose to
utilize the self-attention layers in stable diffusion models to achieve this
goal because the pre-trained stable diffusion model has learned inherent
concepts of objects within its attention layers. Specifically, we introduce a
simple yet effective iterative merging process based on measuring KL divergence
among attention maps to merge them into valid segmentation masks. The proposed
method does not require any training or language dependency to extract quality
segmentation for any images. On COCO-Stuff-27, our method surpasses the prior
unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17%
in mean IoU. The project page is at
\url{https://sites.google.com/view/diffseg/home}.",None,-1
12e3faa6-06ce-4d2b-8701-54bd9476c8a5,Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting,0.772749,10,"Automatically generated reports from medical images promise to improve the
workflow of radiologists. Existing methods consider an image-to-report modeling
task by directly generating a fully-fledged report from an image. However, this
conflates the content of the report (e.g., findings and their attributes) with
its style (e.g., format and choice of words), which can lead to clinically
inaccurate reports. To address this, we propose a two-step approach for
radiology report generation. First, we extract the content from an image; then,
we verbalize the extracted content into a report that matches the style of a
specific radiologist. For this, we leverage RadGraph -- a graph representation
of reports -- together with large language models (LLMs). In our quantitative
evaluations, we find that our approach leads to beneficial performance. Our
human evaluation with clinical raters highlights that the AI-generated reports
are indistinguishably tailored to the style of individual radiologist despite
leveraging only a few examples as context.",None,-1
2950a3e6-b3f4-4e21-be49-6b4a09493b12,ParaLS: Lexical Substitution via Pretrained Paraphraser,0.564799,4,"Lexical substitution (LS) aims at finding appropriate substitutes for a
target word in a sentence. Recently, LS methods based on pretrained language
models have made remarkable progress, generating potential substitutes for a
target word through analysis of its contextual surroundings. However, these
methods tend to overlook the preservation of the sentence's meaning when
generating the substitutes. This study explores how to generate the substitute
candidates from a paraphraser, as the generated paraphrases from a paraphraser
contain variations in word choice and preserve the sentence's meaning. Since we
cannot directly generate the substitutes via commonly used decoding strategies,
we propose two simple decoding strategies that focus on the variations of the
target word during decoding. Experimental results show that our methods
outperform state-of-the-art LS methods based on pre-trained language models on
three benchmarks.",None,-1
1d9fe077-4ccd-4bbc-8b51-5113854ebdbe,Revisiting Supertagging for HPSG,0.864665,1,"We present new supertaggers trained on HPSG-based treebanks. These treebanks
feature high-quality annotation based on a well-developed linguistic theory and
include diverse and challenging test datasets, beyond the usual WSJ section 23
and Wikipedia data. HPSG supertagging has previously relied on MaxEnt-based
models. We use SVM and neural CRF- and BERT-based methods and show that both
SVM and neural supertaggers achieve considerably higher accuracy compared to
the baseline. Our fine-tuned BERT-based tagger achieves 97.26% accuracy on 1000
sentences from WSJ23 and 93.88% on the completely out-of-domain The Cathedral
and the Bazaar (cb)). We conclude that it therefore makes sense to integrate
these new supertaggers into modern HPSG parsers, and we also hope that the
diverse and difficult datasets we used here will gain more popularity in the
field. We contribute the complete dataset reformatted for token classification.",None,-1
e023bc59-5778-4c59-8770-bf324f7ea696,From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions,0.650209,2,"In this work, we show that contemporary language models have a previously
unknown skill -- the capacity for electronic circuit design from high-level
textual descriptions, akin to code generation. We introduce two benchmarks:
Pins100, assessing model knowledge of electrical components, and Micro25,
evaluating a model's capability to design common microcontroller circuits and
code in the Arduino ecosystem that involve input, output, sensors, motors,
protocols, and logic -- with models such as GPT-4 and Claude-V1 achieving
between 60% to 96% Pass@1 on generating full devices. We include six case
studies of using language models as a design assistant for moderately complex
devices, such as a radiation-powered random number generator, an emoji
keyboard, a visible spectrometer, and several assistive devices, while offering
a qualitative analysis performance, outlining evaluation challenges, and
suggesting areas of development to improve complex circuit design and practical
utility. With this work, we aim to spur research at the juncture of natural
language processing and electronic design.",None,-1
157535f1-9008-4056-aeb9-41f36c6e9c61,Examining Temporal Bias in Abusive Language Detection,0.196738,3,"The use of abusive language online has become an increasingly pervasive
problem that damages both individuals and society, with effects ranging from
psychological harm right through to escalation to real-life violence and even
death. Machine learning models have been developed to automatically detect
abusive language, but these models can suffer from temporal bias, the
phenomenon in which topics, language use or social norms change over time. This
study aims to investigate the nature and impact of temporal bias in abusive
language detection across various languages and explore mitigation methods. We
evaluate the performance of models on abusive data sets from different time
periods. Our results demonstrate that temporal bias is a significant challenge
for abusive language detection, with models trained on historical data showing
a significant drop in performance over time. We also present an extensive
linguistic analysis of these abusive data sets from a diachronic perspective,
aiming to explore the reasons for language evolution and performance decline.
This study sheds light on the pervasive issue of temporal bias in abusive
language detection across languages, offering crucial insights into language
evolution and temporal bias mitigation.",None,-1
e025883a-121f-405c-b941-939b20ffc754,Benchmarking of Cancelable Biometrics for Deep Templates,0.746949,7,"In this paper, we benchmark several cancelable biometrics (CB) schemes on
different biometric characteristics. We consider BioHashing, Multi-Layer
Perceptron (MLP) Hashing, Bloom Filters, and two schemes based on
Index-of-Maximum (IoM) Hashing (i.e., IoM-URP and IoM-GRP). In addition to the
mentioned CB schemes, we introduce a CB scheme (as a baseline) based on
user-specific random transformations followed by binarization. We evaluate the
unlinkability, irreversibility, and recognition performance (which are the
required criteria by the ISO/IEC 24745 standard) of these CB schemes on deep
learning based templates extracted from different physiological and behavioral
biometric characteristics including face, voice, finger vein, and iris. In
addition, we provide an open-source implementation of all the experiments
presented to facilitate the reproducibility of our results.",None,-1
f8f415dd-1dcf-4a8d-889c-45c2e6c139a3,Age-Friendly Route Planner: Calculating Comfortable Routes for Senior Citizens,0.535536,1,"The application of routing algorithms to real-world situations is a widely
studied research topic. Despite this, routing algorithms and applications are
usually developed for a general purpose, meaning that certain groups, such as
ageing people, are often marginalized due to the broad approach of the designed
algorithms. This situation may pose a problem in cities which are suffering a
slow but progressive ageing of their populations. With this motivation in mind,
this paper focuses on describing our implemented Age-Friendly Route Planner,
whose goal is to improve the experience in the city for senior citizens. In
order to measure the age-friendliness of a route, several variables have been
deemed, such as the number of amenities along the route, the amount of
comfortable elements found, or the avoidance of sloppy sections. In this paper,
we describe one of the main features of the Age-Friendly Route Planner: the
preference-based routes, and we also demonstrate how it can contribute to the
creation of adapted friendly routes.",None,-1
95c767e8-4e4e-4a63-a02c-1ba9d8e8df65,Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory,0.417567,4,"Human Object Interaction (HOI) detection aims to localize and infer the
relationships between a human and an object. Arguably, training supervised
models for this task from scratch presents challenges due to the performance
drop over rare classes and the high computational cost and time required to
handle long-tailed distributions of HOIs in complex HOI scenes in realistic
settings. This observation motivates us to design an HOI detector that can be
trained even with long-tailed labeled data and can leverage existing knowledge
from pre-trained models. Inspired by the powerful generalization ability of the
large Vision-Language Models (VLM) on classification and retrieval tasks, we
propose an efficient Adaptive HOI Detector with Concept-guided Memory (ADA-CM).
ADA-CM has two operating modes. The first mode makes it tunable without
learning new parameters in a training-free paradigm. Its second mode
incorporates an instance-aware adapter mechanism that can further efficiently
boost performance if updating a lightweight set of parameters can be afforded.
Our proposed method achieves competitive results with state-of-the-art on the
HICO-DET and V-COCO datasets with much less training time. Code can be found at
https://github.com/ltttpku/ADA-CM.",None,-1
671dea2b-3e32-4e17-a1c9-0c0d1f9c0f42,Overwriting Pretrained Bias with Finetuning Data,0.406082,15,"Transfer learning is beneficial by allowing the expressive features of models
pretrained on large-scale datasets to be finetuned for the target task of
smaller, more domain-specific datasets. However, there is a concern that these
pretrained models may come with their own biases which would propagate into the
finetuned model. In this work, we investigate bias when conceptualized as both
spurious correlations between the target task and a sensitive attribute as well
as underrepresentation of a particular group in the dataset. Under both notions
of bias, we find that (1) models finetuned on top of pretrained models can
indeed inherit their biases, but (2) this bias can be corrected for through
relatively minor interventions to the finetuning dataset, and often with a
negligible impact to performance. Our findings imply that careful curation of
the finetuning dataset is important for reducing biases on a downstream task,
and doing so can even compensate for bias in the pretrained model.",None,-1
b0c199f1-e7f5-46c1-8cbb-bf75c3566d77,Generative AI in Mafia-like Game Simulation,0.0433638,1,"In this research, we explore the efficacy and potential of Generative AI
models, specifically focusing on their application in role-playing simulations
exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's
advanced capabilities, the study aimed to showcase the model's potential in
understanding, decision-making, and interaction during game scenarios.
Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo,
demonstrated GPT-4's enhanced adaptability to the game environment, with
significant improvements in posing relevant questions and forming human-like
responses. However, challenges such as the model;s limitations in bluffing and
predicting opponent moves emerged. Reflections on game development, financial
constraints, and non-verbal limitations of the study were also discussed. The
findings suggest that while GPT-4 exhibits promising advancements over earlier
models, there remains potential for further development, especially in
instilling more human-like attributes in AI.",None,-1
c4cbb718-d44c-4b94-9665-7b4702aeff5e,SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool,0.174892,5,"Large Language Model (LLM) based Generative AI systems have seen significant
progress in recent years. Integrating a knowledge retrieval architecture allows
for seamless integration of private data into publicly available Generative AI
systems using pre-trained LLM without requiring additional model fine-tuning.
Moreover, Retrieval-Centric Generation (RCG) approach, a promising future
research direction that explicitly separates roles of LLMs and retrievers in
context interpretation and knowledge memorization, potentially leads to more
efficient implementation. SimplyRetrieve is an open-source tool with the goal
of providing a localized, lightweight, and user-friendly interface to these
sophisticated advancements to the machine learning community. SimplyRetrieve
features a GUI and API based RCG platform, assisted by a Private Knowledge Base
Constructor and a Retrieval Tuning Module. By leveraging these capabilities,
users can explore the potential of RCG for improving generative AI performance
while maintaining privacy standards. The tool is available at
https://github.com/RCGAI/SimplyRetrieve with an MIT license.",None,-1
8a253456-07ff-4896-8b56-d06a594bda53,Learning Optimal Policy for Simultaneous Machine Translation via Binary Search,0.699964,12,"Simultaneous machine translation (SiMT) starts to output translation while
reading the source sentence and needs a precise policy to decide when to output
the generated translation. Therefore, the policy determines the number of
source tokens read during the translation of each target token. However, it is
difficult to learn a precise translation policy to achieve good latency-quality
trade-offs, because there is no golden policy corresponding to parallel
sentences as explicit supervision. In this paper, we present a new method for
constructing the optimal policy online via binary search. By employing explicit
supervision, our approach enables the SiMT model to learn the optimal policy,
which can guide the model in completing the translation during inference.
Experiments on four translation tasks show that our method can exceed strong
baselines across all latency scenarios.",None,-1
8930d5a9-ee43-4b45-83bd-29a8de820564,Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data,0.699403,72,"Chain-of-thought (CoT) advances the reasoning abilities of large language
models (LLMs) and achieves superior performance in complex reasoning tasks.
However, most CoT studies rely on carefully designed human-annotated rational
chains to prompt LLMs, posing challenges for real-world applications where
labeled data is available without rational chains. This paper proposes a new
strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with
Chain-of-Thought), that can bypass human engineering of CoT by automatically
augmenting rational chains from a small labeled dataset, and then pruning
low-quality chains to construct a candidate pool of machine-generated rationale
chains based on the labels. Finally, it selects the optimal combination of
several rationale chains from the pool for CoT prompting by employing a
variance-reduced policy gradient strategy to estimate the significance of each
example. Automate-CoT enables a quick adaptation of the CoT technique to
different tasks. Experimental results demonstrate the effectiveness of our
method, where competitive results are achieved on arithmetic reasoning (+2.7%),
commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning
tasks (+2.5%). The code is available at
https://github.com/SHUMKASHUN/Automate-CoT.",None,-1
55c1a611-0e71-4b8d-ab0e-818f67b82586,MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes,0.97303,121,"Neural radiance fields enable state-of-the-art photorealistic view synthesis.
However, existing radiance field representations are either too
compute-intensive for real-time rendering or require too much memory to scale
to large scenes. We present a Memory-Efficient Radiance Field (MERF)
representation that achieves real-time rendering of large-scale scenes in a
browser. MERF reduces the memory consumption of prior sparse volumetric
radiance fields using a combination of a sparse feature grid and
high-resolution 2D feature planes. To support large-scale unbounded scenes, we
introduce a novel contraction function that maps scene coordinates into a
bounded volume while still allowing for efficient ray-box intersection. We
design a lossless procedure for baking the parameterization used during
training into a model that achieves real-time rendering while still preserving
the photorealistic view synthesis quality of a volumetric radiance field.",None,-1
06df866c-28cc-4ae2-81e1-4eada1a50162,Enhancing Large Language Model with Self-Controlled Memory Framework,0.0772392,11,"Large Language Models (LLMs) are constrained by their inability to process
lengthy inputs, resulting in the loss of critical historical information. To
address this limitation, in this paper, we propose the Self-Controlled Memory
(SCM) framework to enhance the ability of LLMs to maintain long-term memory and
recall relevant information. Our SCM framework comprises three key components:
an LLM-based agent serving as the backbone of the framework, a memory stream
storing agent memories, and a memory controller updating memories and
determining when and how to utilize memories from memory stream. Additionally,
the proposed SCM is able to process ultra-long texts without any modification
or fine-tuning, which can integrate with any instruction following LLMs in a
plug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the
effectiveness of SCM for handling lengthy inputs. The annotated dataset covers
three tasks: long-term dialogues, book summarization, and meeting
summarization. Experimental results demonstrate that our method achieves better
retrieval recall and generates more informative responses compared to
competitive baselines in long-term dialogues.
(https://github.com/wbbeyourself/SCM4LLMs)",None,-1
51b2cb42-a826-40dc-ad6b-417f1cb28b7a,"Natural Language Processing in Ethiopian Languages: Current State, Challenges, and Opportunities",0.0917977,4,"This survey delves into the current state of natural language processing
(NLP) for four Ethiopian languages: Amharic, Afaan Oromo, Tigrinya, and
Wolaytta. Through this paper, we identify key challenges and opportunities for
NLP research in Ethiopia. Furthermore, we provide a centralized repository on
GitHub that contains publicly available resources for various NLP tasks in
these languages. This repository can be updated periodically with contributions
from other researchers. Our objective is to identify research gaps and
disseminate the information to NLP researchers interested in Ethiopian
languages and encourage future research in this domain.",None,-1
c570768f-2ebe-42a9-977f-ddf75e6de197,MoSFPAD: An end-to-end Ensemble of MobileNet and Support Vector Classifier for Fingerprint Presentation Attack Detection,0.756462,4,"Automatic fingerprint recognition systems are the most extensively used
systems for person authentication although they are vulnerable to Presentation
attacks. Artificial artifacts created with the help of various materials are
used to deceive these systems causing a threat to the security of
fingerprint-based applications. This paper proposes a novel end-to-end model to
detect fingerprint Presentation attacks. The proposed model incorporates
MobileNet as a feature extractor and a Support Vector Classifier as a
classifier to detect presentation attacks in cross-material and cross-sensor
paradigms. The feature extractor's parameters are learned with the loss
generated by the support vector classifier. The proposed model eliminates the
need for intermediary data preparation procedures, unlike other static hybrid
architectures. The performance of the proposed model has been validated on
benchmark LivDet 2011, 2013, 2015, 2017, and 2019 databases, and overall
accuracy of 98.64%, 99.50%, 97.23%, 95.06%, and 95.20% is achieved on these
databases, respectively. The performance of the proposed model is compared with
state-of-the-art methods and the proposed method outperforms in cross-material
and cross-sensor paradigms in terms of average classification error.",None,-1
7ea1fbb7-64a7-4c19-8b0e-26650da8af26,Multi-view Self-supervised Disentanglement for General Image Denoising,0.431914,5,"With its significant performance improvements, the deep learning paradigm has
become a standard tool for modern image denoisers. While promising performance
has been shown on seen noise distributions, existing approaches often suffer
from generalisation to unseen noise types or general and real noise. It is
understandable as the model is designed to learn paired mapping (e.g. from a
noisy image to its clean version). In this paper, we instead propose to learn
to disentangle the noisy image, under the intuitive assumption that different
corrupted versions of the same clean image share a common latent space. A
self-supervised learning framework is proposed to achieve the goal, without
looking at the latent clean image. By taking two different corrupted versions
of the same image as input, the proposed Multi-view Self-supervised
Disentanglement (MeD) approach learns to disentangle the latent clean features
from the corruptions and recover the clean image consequently. Extensive
experimental analysis on both synthetic and real noise shows the superiority of
the proposed method over prior self-supervised approaches, especially on unseen
novel noise types. On real noise, the proposed method even outperforms its
supervised counterparts by over 3 dB.",None,-1
2db9584b-9209-4c7e-8eaf-7ce88076759a,MVFusion: Multi-View 3D Object Detection with Semantic-aligned Radar and Camera Fusion,0.811274,14,"Multi-view radar-camera fused 3D object detection provides a farther
detection range and more helpful features for autonomous driving, especially
under adverse weather. The current radar-camera fusion methods deliver kinds of
designs to fuse radar information with camera data. However, these fusion
approaches usually adopt the straightforward concatenation operation between
multi-modal features, which ignores the semantic alignment with radar features
and sufficient correlations across modals. In this paper, we present MVFusion,
a novel Multi-View radar-camera Fusion method to achieve semantic-aligned radar
features and enhance the cross-modal information interaction. To achieve so, we
inject the semantic alignment into the radar features via the semantic-aligned
radar encoder (SARE) to produce image-guided radar features. Then, we propose
the radar-guided fusion transformer (RGFT) to fuse our radar and image features
to strengthen the two modals' correlation from the global scope via the
cross-attention mechanism. Extensive experiments show that MVFusion achieves
state-of-the-art performance (51.7% NDS and 45.3% mAP) on the nuScenes dataset.
We shall release our code and trained networks upon publication.",None,-1
4b433c1f-22f8-46a7-a28d-dae7203d7e19,Slot-VAE: Object-Centric Scene Generation with Slot Attention,0.651369,9,"Slot attention has shown remarkable object-centric representation learning
performance in computer vision tasks without requiring any supervision. Despite
its object-centric binding ability brought by compositional modelling, as a
deterministic module, slot attention lacks the ability to generate novel
scenes. In this paper, we propose the Slot-VAE, a generative model that
integrates slot attention with the hierarchical VAE framework for
object-centric structured scene generation. For each image, the model
simultaneously infers a global scene representation to capture high-level scene
structure and object-centric slot representations to embed individual object
components. During generation, slot representations are generated from the
global scene representation to ensure coherent scene structures. Our extensive
evaluation of the scene generation ability indicates that Slot-VAE outperforms
slot representation-based generative baselines in terms of sample quality and
scene structure accuracy.",None,-1
bb997a6c-9de6-42c3-856e-357085ddd17b,Solving the Kidney-Exchange Problem via Graph Neural Networks with No Supervision,0.242535,1,"This paper introduces a new learning-based approach for approximately solving
the Kidney-Exchange Problem (KEP), an NP-hard problem on graphs. The problem
consists of, given a pool of kidney donors and patients waiting for kidney
donations, optimally selecting a set of donations to optimize the quantity and
quality of transplants performed while respecting a set of constraints about
the arrangement of these donations. The proposed technique consists of two main
steps: the first is a Graph Neural Network (GNN) trained without supervision;
the second is a deterministic non-learned search heuristic that uses the output
of the GNN to find paths and cycles. To allow for comparisons, we also
implemented and tested an exact solution method using integer programming, two
greedy search heuristics without the machine learning module, and the GNN alone
without a heuristic. We analyze and compare the methods and conclude that the
learning-based two-stage approach is the best solution quality, outputting
approximate solutions on average 1.1 times more valuable than the ones from the
deterministic heuristic alone.",None,-1
61a292b4-d198-4506-952a-ec727286a361,A Quantum Computing-based System for Portfolio Optimization using Future Asset Values and Automatic Reduction of the Investment Universe,0.431541,1,"One of the problems in quantitative finance that has received the most
attention is the portfolio optimization problem. Regarding its solving, this
problem has been approached using different techniques, with those related to
quantum computing being especially prolific in recent years. In this study, we
present a system called Quantum Computing-based System for Portfolio
Optimization with Future Asset Values and Automatic Universe Reduction
(Q4FuturePOP), which deals with the Portfolio Optimization Problem considering
the following innovations: i) the developed tool is modeled for working with
future prediction of assets, instead of historical values; and ii) Q4FuturePOP
includes an automatic universe reduction module, which is conceived to
intelligently reduce the complexity of the problem. We also introduce a brief
discussion about the preliminary performance of the different modules that
compose the prototypical version of Q4FuturePOP.",None,-1
eb98d280-a593-468f-9f66-65cefd7eac6c,Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules,0.27274,2,"Automated game design (AGD), the study of automatically generating game
rules, has a long history in technical games research. AGD approaches generally
rely on approximations of human play, either objective functions or AI agents.
Despite this, the majority of these approximators are static, meaning they do
not reflect human player's ability to learn and improve in a game. In this
paper, we investigate the application of Reinforcement Learning (RL) as an
approximator for human play for rule generation. We recreate the classic AGD
environment Mechanic Maker in Unity as a new, open-source rule generation
framework. Our results demonstrate that RL produces distinct sets of rules from
an A* agent baseline, which may be more usable by humans.",None,-1
1cfcd07d-9942-4999-b632-3dead73b399b,Stable and Causal Inference for Discriminative Self-supervised Deep Visual Representations,0.234687,1,"In recent years, discriminative self-supervised methods have made significant
strides in advancing various visual tasks. The central idea of learning a data
encoder that is robust to data distortions/augmentations is straightforward yet
highly effective. Although many studies have demonstrated the empirical success
of various learning methods, the resulting learned representations can exhibit
instability and hinder downstream performance. In this study, we analyze
discriminative self-supervised methods from a causal perspective to explain
these unstable behaviors and propose solutions to overcome them. Our approach
draws inspiration from prior works that empirically demonstrate the ability of
discriminative self-supervised methods to demix ground truth causal sources to
some extent. Unlike previous work on causality-empowered representation
learning, we do not apply our solutions during the training process but rather
during the inference process to improve time efficiency. Through experiments on
both controlled image datasets and realistic image datasets, we show that our
proposed solutions, which involve tempering a linear transformation with
controlled synthetic data, are effective in addressing these issues.",None,-1
89edab00-cba9-435a-8618-0948087c451f,Missing Modality Robustness in Semi-Supervised Multi-Modal Semantic Segmentation,0.436762,6,"Using multiple spatial modalities has been proven helpful in improving
semantic segmentation performance. However, there are several real-world
challenges that have yet to be addressed: (a) improving label efficiency and
(b) enhancing robustness in realistic scenarios where modalities are missing at
the test time. To address these challenges, we first propose a simple yet
efficient multi-modal fusion mechanism Linear Fusion, that performs better than
the state-of-the-art multi-modal models even with limited supervision. Second,
we propose M3L: Multi-modal Teacher for Masked Modality Learning, a
semi-supervised framework that not only improves the multi-modal performance
but also makes the model robust to the realistic missing modality scenario
using unlabeled data. We create the first benchmark for semi-supervised
multi-modal semantic segmentation and also report the robustness to missing
modalities. Our proposal shows an absolute improvement of up to 10% on robust
mIoU above the most competitive baselines. Our code is available at
https://github.com/harshm121/M3L",None,-1
bf11a7bc-9fff-4dab-bf38-1e765cc86efd,MedMine: Examining Pre-trained Language Models on Medication Mining,0.474115,3,"Automatic medication mining from clinical and biomedical text has become a
popular topic due to its real impact on healthcare applications and the recent
development of powerful language models (LMs). However, fully-automatic
extraction models still face obstacles to be overcome such that they can be
deployed directly into clinical practice for better impacts. Such obstacles
include their imbalanced performances on different entity types and clinical
events. In this work, we examine current state-of-the-art pre-trained language
models (PLMs) on such tasks, via fine-tuning including the monolingual model
Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their
advantages and drawbacks using historical medication mining shared task data
sets from n2c2-2018 challenges. We report the findings we get from these
fine-tuning experiments such that they can facilitate future research on
addressing them, for instance, how to combine their outputs, merge such models,
or improve their overall accuracy by ensemble learning and data augmentation.
MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}",None,-1
58b374a6-14d0-461c-9c22-9bc38f16accb,MetaGait: Learning to Learn an Omni Sample Adaptive Representation for Gait Recognition,0.975688,20,"Gait recognition, which aims at identifying individuals by their walking
patterns, has recently drawn increasing research attention. However, gait
recognition still suffers from the conflicts between the limited binary visual
clues of the silhouette and numerous covariates with diverse scales, which
brings challenges to the model's adaptiveness. In this paper, we address this
conflict by developing a novel MetaGait that learns to learn an omni sample
adaptive representation. Towards this goal, MetaGait injects meta-knowledge,
which could guide the model to perceive sample-specific properties, into the
calibration network of the attention mechanism to improve the adaptiveness from
the omni-scale, omni-dimension, and omni-process perspectives. Specifically, we
leverage the meta-knowledge across the entire process, where Meta Triple
Attention and Meta Temporal Pooling are presented respectively to adaptively
capture omni-scale dependency from spatial/channel/temporal dimensions
simultaneously and to adaptively aggregate temporal information through
integrating the merits of three complementary temporal aggregation methods.
Extensive experiments demonstrate the state-of-the-art performance of the
proposed MetaGait. On CASIA-B, we achieve rank-1 accuracy of 98.7%, 96.0%, and
89.3% under three conditions, respectively. On OU-MVLP, we achieve rank-1
accuracy of 92.4%.",None,-1
16559f4e-7854-45f8-b605-4dee585d2b4b,Interpretable Multi Labeled Bengali Toxic Comments Classification using Deep Learning,0.636325,4,"This paper presents a deep learning-based pipeline for categorizing Bengali
toxic comments, in which at first a binary classification model is used to
determine whether a comment is toxic or not, and then a multi-label classifier
is employed to determine which toxicity type the comment belongs to. For this
purpose, we have prepared a manually labeled dataset consisting of 16,073
instances among which 8,488 are Toxic and any toxic comment may correspond to
one or more of the six toxic categories - vulgar, hate, religious, threat,
troll, and insult simultaneously. Long Short Term Memory (LSTM) with BERT
Embedding achieved 89.42% accuracy for the binary classification task while as
a multi-label classifier, a combination of Convolutional Neural Network and
Bi-directional Long Short Term Memory (CNN-BiLSTM) with attention mechanism
achieved 78.92% accuracy and 0.86 as weighted F1-score. To explain the
predictions and interpret the word feature importance during classification by
the proposed models, we utilized Local Interpretable Model-Agnostic
Explanations (LIME) framework. We have made our dataset public and can be
accessed at -
https://github.com/deepu099cse/Multi-Labeled-Bengali-Toxic-Comments-Classification",None,-1
59bc0a5f-ec15-46d8-bd6a-69176e84ba90,Robust Evaluation of Diffusion-Based Adversarial Purification,0.62877,20,"We question the current evaluation practice on diffusion-based purification
methods. Diffusion-based purification methods aim to remove adversarial effects
from an input data point at test time. The approach gains increasing attention
as an alternative to adversarial training due to the disentangling between
training and testing. Well-known white-box attacks are often employed to
measure the robustness of the purification. However, it is unknown whether
these attacks are the most effective for the diffusion-based purification since
the attacks are often tailored for adversarial training. We analyze the current
practices and provide a new guideline for measuring the robustness of
purification methods against adversarial attacks. Based on our analysis, we
further propose a new purification strategy improving robustness compared to
the current diffusion-based purification methods.",None,-1
fc291533-f4ad-4a89-94be-843d0874a91a,Kanbun-LM: Reading and Translating Classical Chinese in Japanese Methods by Language Models,0.291648,1,"Recent studies in natural language processing (NLP) have focused on modern
languages and achieved state-of-the-art results in many tasks. Meanwhile,
little attention has been paid to ancient texts and related tasks. Classical
Chinese first came to Japan approximately 2,000 years ago. It was gradually
adapted to a Japanese form called Kanbun-Kundoku (Kanbun) in Japanese reading
and translating methods, which has significantly impacted Japanese literature.
However, compared to the rich resources for ancient texts in mainland China,
Kanbun resources remain scarce in Japan. To solve this problem, we construct
the first Classical-Chinese-to-Kanbun dataset in the world. Furthermore, we
introduce two tasks, character reordering and machine translation, both of
which play a significant role in Kanbun comprehension. We also test the current
language models on these tasks and discuss the best evaluation method by
comparing the results with human scores. We release our code and dataset on
GitHub.",None,-1
0429ee38-5af4-4de7-b30f-404b74cabb4e,CEN-HDR: Computationally Efficient neural Network for real-time High Dynamic Range imaging,0.149544,2,"High dynamic range (HDR) imaging is still a challenging task in modern
digital photography. Recent research proposes solutions that provide
high-quality acquisition but at the cost of a very large number of operations
and a slow inference time that prevent the implementation of these solutions on
lightweight real-time systems. In this paper, we propose CEN-HDR, a new
computationally efficient neural network by providing a novel architecture
based on a light attention mechanism and sub-pixel convolution operations for
real-time HDR imaging. We also provide an efficient training scheme by applying
network compression using knowledge distillation. We performed extensive
qualitative and quantitative comparisons to show that our approach produces
competitive results in image quality while being faster than state-of-the-art
solutions, allowing it to be practically deployed under real-time constraints.
Experimental results show our method obtains a score of 43.04 mu-PSNR on the
Kalantari2017 dataset with a framerate of 33 FPS using a Macbook M1 NPU.",None,-1
ee9d018a-0ba5-48ec-91f0-2b572aa7ff69,An Abstract Specification of VoxML as an Annotation Language,0.304672,2,"VoxML is a modeling language used to map natural language expressions into
real-time visualizations using commonsense semantic knowledge of objects and
events. Its utility has been demonstrated in embodied simulation environments
and in agent-object interactions in situated multimodal human-agent
collaboration and communication. It introduces the notion of object affordance
(both Gibsonian and Telic) from HRI and robotics, as well as the concept of
habitat (an object's context of use) for interactions between a rational agent
and an object. This paper aims to specify VoxML as an annotation language in
general abstract terms. It then shows how it works on annotating linguistic
data that express visually perceptible human-object interactions. The
annotation structures thus generated will be interpreted against the enriched
minimal model created by VoxML as a modeling language while supporting the
modeling purposes of VoxML linguistically.",None,-1
56058e89-d720-4d86-af36-f0278d742b99,Super-Resolution Information Enhancement For Crowd Counting,0.613305,8,"Crowd counting is a challenging task due to the heavy occlusions, scales, and
density variations. Existing methods handle these challenges effectively while
ignoring low-resolution (LR) circumstances. The LR circumstances weaken the
counting performance deeply for two crucial reasons: 1) limited detail
information; 2) overlapping head regions accumulate in density maps and result
in extreme ground-truth values. An intuitive solution is to employ
super-resolution (SR) pre-processes for the input LR images. However, it
complicates the inference steps and thus limits application potentials when
requiring real-time. We propose a more elegant method termed Multi-Scale
Super-Resolution Module (MSSRM). It guides the network to estimate the lost de
tails and enhances the detailed information in the feature space. Noteworthy
that the MSSRM is plug-in plug-out and deals with the LR problems with no
inference cost. As the proposed method requires SR labels, we further propose a
Super-Resolution Crowd Counting dataset (SR-Crowd). Extensive experiments on
three datasets demonstrate the superiority of our method. The code will be
available at https://github.com/PRIS-CV/MSSRM.git.",None,-1
9da959d3-196f-48c2-b64a-a612d0e12b88,Tree Prompting: Efficient Task Adaptation without Fine-Tuning,0.125345,8,"Prompting language models (LMs) is the main interface for applying them to
new tasks. However, for smaller LMs, prompting provides low accuracy compared
to gradient-based finetuning. Tree Prompting is an approach to prompting which
builds a decision tree of prompts, linking multiple LM calls together to solve
a task. At inference time, each call to the LM is determined by efficiently
routing the outcome of the previous call using the tree. Experiments on
classification datasets show that Tree Prompting improves accuracy over
competing methods and is competitive with fine-tuning. We also show that
variants of Tree Prompting allow inspection of a model's decision-making
process.",None,-1
bb983cd6-9cc6-4020-967e-e10af613328b,Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination,0.621822,4,"Enormous hand images with reliable annotations are collected through
marker-based MoCap. Unfortunately, degradations caused by markers limit their
application in hand appearance reconstruction. A clear appearance recovery
insight is an image-to-image translation trained with unpaired data. However,
most frameworks fail because there exists structure inconsistency from a
degraded hand to a bare one. The core of our approach is to first disentangle
the bare hand structure from those degraded images and then wrap the appearance
to this structure with a dual adversarial discrimination (DAD) scheme. Both
modules take full advantage of the semi-supervised learning paradigm: The
structure disentanglement benefits from the modeling ability of ViT, and the
translator is enhanced by the dual discrimination on both translation processes
and translation results. Comprehensive evaluations have been conducted to prove
that our framework can robustly recover photo-realistic hand appearance from
diverse marker-contained and even object-occluded datasets. It provides a novel
avenue to acquire bare hand appearance data for other downstream learning
problems.The codes will be publicly available at https://www.yangangwang.com",None,-1
cc8f2eb2-1745-4bac-8843-5cdea2f380be,Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents,0.795438,6,"Robustly cooperating with unseen agents and human partners presents
significant challenges due to the diverse cooperative conventions these
partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this
challenge by training an agent with a population of diverse teammate policies
obtained through maximizing specific diversity metrics. However, prior
heuristic-based diversity metrics do not always maximize the agent's robustness
in all cooperative problems. In this work, we first propose that maximizing an
AHT agent's robustness requires it to emulate policies in the minimum coverage
set (MCS), the set of best-response policies to any partner policies in the
environment. We then introduce the L-BRDiv algorithm that generates a set of
teammate policies that, when used for AHT training, encourage agents to emulate
policies from the MCS. L-BRDiv works by solving a constrained optimization
problem to jointly train teammate policies for AHT training and approximating
AHT agent policies that are members of the MCS. We empirically demonstrate that
L-BRDiv produces more robust AHT agents than state-of-the-art methods in a
broader range of two-player cooperative problems without the need for extensive
hyperparameter tuning for its objectives. Our study shows that L-BRDiv
outperforms the baseline methods by prioritizing discovering distinct members
of the MCS instead of repeatedly finding redundant policies.",None,-1
54c50691-52e2-4412-a41c-3ea663703df7,Behind the Scenes: Density Fields for Single View Reconstruction,0.79327,29,"Inferring a meaningful geometric scene representation from a single image is
a fundamental problem in computer vision. Approaches based on traditional depth
map prediction can only reason about areas that are visible in the image.
Currently, neural radiance fields (NeRFs) can capture true 3D including color,
but are too complex to be generated from a single image. As an alternative, we
propose to predict implicit density fields. A density field maps every location
in the frustum of the input image to volumetric density. By directly sampling
color from the available views instead of storing color in the density field,
our scene representation becomes significantly less complex compared to NeRFs,
and a neural network can predict it in a single forward pass. The prediction
network is trained through self-supervision from only video data. Our
formulation allows volume rendering to perform both depth prediction and novel
view synthesis. Through experiments, we show that our method is able to predict
meaningful geometry for regions that are occluded in the input image.
Additionally, we demonstrate the potential of our approach on three datasets
for depth prediction and novel-view synthesis.",None,-1
5f158910-9c99-47e1-bcf9-500ab58c2dcc,OptiMUS: Optimization Modeling Using MIP Solvers and large language models,0.25695,11,"Optimization problems are pervasive across various sectors, from
manufacturing and distribution to healthcare. However, most such problems are
still solved heuristically by hand rather than optimally by state-of-the-art
solvers, as the expertise required to formulate and solve these problems limits
the widespread adoption of optimization tools and techniques. We introduce
OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and
solve MILP problems from their natural language descriptions. OptiMUS is
capable of developing mathematical models, writing and debugging solver code,
developing tests, and checking the validity of generated solutions. To
benchmark our agent, we present NLP4LP, a novel dataset of linear programming
(LP) and mixed integer linear programming (MILP) problems. Our experiments
demonstrate that OptiMUS solves nearly twice as many problems as a basic LLM
prompting strategy. OptiMUS code and NLP4LP dataset are available at
\href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}",None,-1
cbad1145-c0fe-4636-b05f-64b4c922f79b,Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement,0.624692,5,"Mentions of new concepts appear regularly in texts and require automated
approaches to harvest and place them into Knowledge Bases (KB), e.g.,
ontologies and taxonomies. Existing datasets suffer from three issues, (i)
mostly assuming that a new concept is pre-discovered and cannot support
out-of-KB mention discovery; (ii) only using the concept label as the input
along with the KB and thus lacking the contexts of a concept label; and (iii)
mostly focusing on concept placement w.r.t a taxonomy of atomic concepts,
instead of complex concepts, i.e., with logical operators. To address these
issues, we propose a new benchmark, adapting MedMentions dataset (PubMed
abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases
sub-category and the broader categories of Clinical finding, Procedure, and
Pharmaceutical / biologic product. We provide usage on the evaluation with the
dataset for out-of-KB mention discovery and concept placement, adapting recent
Large Language Model based methods.",None,-1
1127bbc9-2b86-47ca-a014-ac0a7005dac6,Trade-Offs Between Fairness and Privacy in Language Modeling,0.0556893,1,"Protecting privacy in contemporary NLP models is gaining in importance. So
does the need to mitigate social biases of such models. But can we have both at
the same time? Existing research suggests that privacy preservation comes at
the price of worsening biases in classification tasks. In this paper, we
explore the extent to which this tradeoff really holds when we incorporate both
privacy preservation and de-biasing techniques into training text generation
models. How does improving the model along one dimension affect the other
dimension as well as the utility of the model? We conduct an extensive set of
experiments that include bias detection, privacy attacks, language modeling,
and performance on downstream tasks.",None,-1
5d52b646-d9eb-4092-b992-66f9f3e4b33e,Independent Component Alignment for Multi-Task Learning,0.591255,19,"In a multi-task learning (MTL) setting, a single model is trained to tackle a
diverse set of tasks jointly. Despite rapid progress in the field, MTL remains
challenging due to optimization issues such as conflicting and dominating
gradients. In this work, we propose using a condition number of a linear system
of gradients as a stability criterion of an MTL optimization. We theoretically
demonstrate that a condition number reflects the aforementioned optimization
issues. Accordingly, we present Aligned-MTL, a novel MTL optimization approach
based on the proposed criterion, that eliminates instability in the training
process by aligning the orthogonal components of the linear system of
gradients. While many recent MTL approaches guarantee convergence to a minimum,
task trade-offs cannot be specified in advance. In contrast, Aligned-MTL
provably converges to an optimal point with pre-defined task-specific weights,
which provides more control over the optimization result. Through experiments,
we show that the proposed approach consistently improves performance on a
diverse set of MTL benchmarks, including semantic and instance segmentation,
depth estimation, surface normal estimation, and reinforcement learning. The
source code is publicly available at https://github.com/SamsungLabs/MTL .",None,-1
335c656e-e484-4cf2-afe6-286a76d0c4d7,Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,0.520159,19,"The large number of ReLU non-linearity operations in existing deep neural
networks makes them ill-suited for latency-efficient private inference (PI).
Existing techniques to reduce ReLU operations often involve manual effort and
sacrifice significant accuracy. In this paper, we first present a novel measure
of non-linearity layers' ReLU sensitivity, enabling mitigation of the
time-consuming manual efforts in identifying the same. Based on this
sensitivity, we then present SENet, a three-stage training method that for a
given ReLU budget, automatically assigns per-layer ReLU counts, decides the
ReLU locations for each layer's activation map, and trains a model with
significantly fewer ReLUs to potentially yield latency and communication
efficient PI. Experimental evaluations with multiple models on various datasets
show SENet's superior performance both in terms of reduced ReLUs and improved
classification accuracy compared to existing alternatives. In particular, SENet
can yield models that require up to ~2x fewer ReLUs while yielding similar
accuracy. For a similar ReLU budget SENet can yield models with ~2.32% improved
classification accuracy, evaluated on CIFAR-100.",None,-1
7042cf7f-bcfa-4a9d-b8eb-4f09c560b92a,Machine Learning Recommendation System For Health Insurance Decision Making In Nigeria,0.506172,2,"The uptake of health insurance has been poor in Nigeria, a significant step
to improving this includes improved awareness, access to information and tools
to support decision making. Artificial intelligence (AI) based recommender
systems have gained popularity in helping individuals find movies, books,
music, and different types of products on the internet including diverse
applications in healthcare. The content-based methodology (item-based approach)
was employed in the recommender system. We applied both the K-Nearest Neighbor
(KNN) and Cosine similarity algorithm. We chose the Cosine similarity as our
chosen algorithm after several evaluations based of their outcomes in
comparison with domain knowledge. The recommender system takes into
consideration the choices entered by the user, filters the health management
organization (HMO) data by location and chosen prices. It then recommends the
top 3 HMOs with closest similarity in services offered. A recommendation tool
to help people find and select the best health insurance plan for them is
useful in reducing the barrier of accessing health insurance. Users are
empowered to easily find appropriate information on available plans, reduce
cognitive overload in dealing with over 100 options available in the market and
easily see what matches their financial capacity.",None,-1
660dbdab-864d-4005-ab79-04a540bd01fc,Cheap Talk Discovery and Utilization in Multi-Agent Reinforcement Learning,0.137056,1,"By enabling agents to communicate, recent cooperative multi-agent
reinforcement learning (MARL) methods have demonstrated better task performance
and more coordinated behavior. Most existing approaches facilitate inter-agent
communication by allowing agents to send messages to each other through free
communication channels, i.e., cheap talk channels. Current methods require
these channels to be constantly accessible and known to the agents a priori. In
this work, we lift these requirements such that the agents must discover the
cheap talk channels and learn how to use them. Hence, the problem has two main
parts: cheap talk discovery (CTD) and cheap talk utilization (CTU). We
introduce a novel conceptual framework for both parts and develop a new
algorithm based on mutual information maximization that outperforms existing
algorithms in CTD/CTU settings. We also release a novel benchmark suite to
stimulate future research in CTD/CTU.",None,-1
3ec9ecae-92e8-4954-9969-3d16149464a9,On the Effectiveness of LayerNorm Tuning for Continual Learning in Vision Transformers,0.607661,5,"State-of-the-art rehearsal-free continual learning methods exploit the
peculiarities of Vision Transformers to learn task-specific prompts,
drastically reducing catastrophic forgetting. However, there is a tradeoff
between the number of learned parameters and the performance, making such
models computationally expensive. In this work, we aim to reduce this cost
while maintaining competitive performance. We achieve this by revisiting and
extending a simple transfer learning idea: learning task-specific normalization
layers. Specifically, we tune the scale and bias parameters of LayerNorm for
each continual learning task, selecting them at inference time based on the
similarity between task-specific keys and the output of the pre-trained model.
To make the classifier robust to incorrect selection of parameters during
inference, we introduce a two-stage training procedure, where we first optimize
the task-specific parameters and then train the classifier with the same
selection procedure of the inference time. Experiments on ImageNet-R and
CIFAR-100 show that our method achieves results that are either superior or on
par with {the state of the art} while being computationally cheaper.",None,-1
cb734dac-5de1-482a-a277-095e362d66be,Forms of Understanding of XAI-Explanations,0.37126,1,"Explainability has become an important topic in computer science and
artificial intelligence, leading to a subfield called Explainable Artificial
Intelligence (XAI). The goal of providing or seeking explanations is to achieve
(better) 'understanding' on the part of the explainee. However, what it means
to 'understand' is still not clearly defined, and the concept itself is rarely
the subject of scientific investigation. This conceptual article aims to
present a model of forms of understanding in the context of XAI and beyond.
From an interdisciplinary perspective bringing together computer science,
linguistics, sociology, and psychology, a definition of understanding and its
forms, assessment, and dynamics during the process of giving everyday
explanations are explored. Two types of understanding are considered as
possible outcomes of explanations, namely enabledness, 'knowing how' to do or
decide something, and comprehension, 'knowing that' -- both in different
degrees (from shallow to deep). Explanations regularly start with shallow
understanding in a specific domain and can lead to deep comprehension and
enabledness of the explanandum, which we see as a prerequisite for human users
to gain agency. In this process, the increase of comprehension and enabledness
are highly interdependent. Against the background of this systematization,
special challenges of understanding in XAI are discussed.",None,-1
ddcf358e-abaf-4def-8095-0866631eccfe,Conformal Prediction for Time Series with Modern Hopfield Networks,0.751651,12,"To quantify uncertainty, conformal prediction methods are gaining
continuously more interest and have already been successfully applied to
various domains. However, they are difficult to apply to time series as the
autocorrelative structure of time series violates basic assumptions required by
conformal prediction. We propose HopCPT, a novel conformal prediction approach
for time series that not only copes with temporal structures but leverages
them. We show that our approach is theoretically well justified for time series
where temporal dependencies are present. In experiments, we demonstrate that
our new approach outperforms state-of-the-art conformal prediction methods on
multiple real-world time series datasets from four different domains.",None,-1
f26f6fea-c64c-4184-b8e3-ab97015664ec,Roll-Drop: accounting for observation noise with a single parameter,0.138803,1,"This paper proposes a simple strategy for sim-to-real in Deep-Reinforcement
Learning (DRL) -- called Roll-Drop -- that uses dropout during simulation to
account for observation noise during deployment without explicitly modelling
its distribution for each state. DRL is a promising approach to control robots
for highly dynamic and feedback-based manoeuvres, and accurate simulators are
crucial to providing cheap and abundant data to learn the desired behaviour.
Nevertheless, the simulated data are noiseless and generally show a
distributional shift that challenges the deployment on real machines where
sensor readings are affected by noise. The standard solution is modelling the
latter and injecting it during training; while this requires a thorough system
identification, Roll-Drop enhances the robustness to sensor noise by tuning
only a single parameter. We demonstrate an 80% success rate when up to 25%
noise is injected in the observations, with twice higher robustness than the
baselines. We deploy the controller trained in simulation on a Unitree A1
platform and assess this improved robustness on the physical system.",None,-1
746f969a-2704-4269-91d3-6e06987a2015,Geographical Erasure in Language Generation,0.0362095,1,"Large language models (LLMs) encode vast amounts of world knowledge. However,
since these models are trained on large swaths of internet data, they are at
risk of inordinately capturing information about dominant groups. This
imbalance can propagate into generated language. In this work, we study and
operationalise a form of geographical erasure, wherein language models
underpredict certain countries. We demonstrate consistent instances of erasure
across a range of LLMs. We discover that erasure strongly correlates with low
frequencies of country mentions in the training corpus. Lastly, we mitigate
erasure by finetuning using a custom objective.",None,-1
ecb1eae1-5610-474b-b80b-c607b23fb249,Disentangling Neuron Representations with Concept Vectors,0.417703,5,"Mechanistic interpretability aims to understand how models store
representations by breaking down neural networks into interpretable units.
However, the occurrence of polysemantic neurons, or neurons that respond to
multiple unrelated features, makes interpreting individual neurons challenging.
This has led to the search for meaningful vectors, known as concept vectors, in
activation space instead of individual neurons. The main contribution of this
paper is a method to disentangle polysemantic neurons into concept vectors
encapsulating distinct features. Our method can search for fine-grained
concepts according to the user's desired level of concept separation. The
analysis shows that polysemantic neurons can be disentangled into directions
consisting of linear combinations of neurons. Our evaluations show that the
concept vectors found encode coherent, human-understandable features.",None,-1
1ead0706-9708-4b9c-904c-85db78b51dd1,Test Time Adaptation for Blind Image Quality Assessment,0.86683,8,"While the design of blind image quality assessment (IQA) algorithms has
improved significantly, the distribution shift between the training and testing
scenarios often leads to a poor performance of these methods at inference time.
This motivates the study of test time adaptation (TTA) techniques to improve
their performance at inference time. Existing auxiliary tasks and loss
functions used for TTA may not be relevant for quality-aware adaptation of the
pre-trained model. In this work, we introduce two novel quality-relevant
auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In
particular, we introduce a group contrastive loss at the batch level and a
relative rank loss at the sample level to make the model quality aware and
adapt to the target data. Our experiments reveal that even using a small batch
of images from the test distribution helps achieve significant improvement in
performance by updating the batch normalization statistics of the source model.",None,-1
2bef8cf3-9145-4ac5-9628-a8c107431d71,Ladder Fine-tuning approach for SAM integrating complementary network,0.806241,15,"Recently, foundation models have been introduced demonstrating various tasks
in the field of computer vision. These models such as Segment Anything Model
(SAM) are generalized models trained using huge datasets. Currently, ongoing
research focuses on exploring the effective utilization of these generalized
models for specific domains, such as medical imaging. However, in medical
imaging, the lack of training samples due to privacy concerns and other factors
presents a major challenge for applying these generalized models to medical
image segmentation task. To address this issue, the effective fine tuning of
these models is crucial to ensure their optimal utilization. In this study, we
propose to combine a complementary Convolutional Neural Network (CNN) along
with the standard SAM network for medical image segmentation. To reduce the
burden of fine tuning large foundation model and implement cost-efficient
trainnig scheme, we focus only on fine-tuning the additional CNN network and
SAM decoder part. This strategy significantly reduces trainnig time and
achieves competitive results on publicly available dataset. The code is
available at https://github.com/11yxk/SAM-LST.",None,-1
8b3fba75-57f9-497e-9d44-e89d599815f0,The language of sounds unheard: Exploring musical timbre semantics of large language models,0.669247,1,"Semantic dimensions of sound have been playing a central role in
understanding the nature of auditory sensory experience as well as the broader
relation between perception, language, and meaning. Accordingly, and given the
recent proliferation of large language models (LLMs), here we asked whether
such models exhibit an organisation of perceptual semantics similar to those
observed in humans. Specifically, we prompted ChatGPT, a chatbot based on a
state-of-the-art LLM, to rate musical instrument sounds on a set of 20 semantic
scales. We elicited multiple responses in separate chats, analogous to having
multiple human raters. ChatGPT generated semantic profiles that only partially
correlated with human ratings, yet showed robust agreement along well-known
psychophysical dimensions of musical sounds such as brightness (bright-dark)
and pitch height (deep-high). Exploratory factor analysis suggested the same
dimensionality but different spatial configuration of a latent factor space
between the chatbot and human ratings. Unexpectedly, the chatbot showed degrees
of internal variability that were comparable in magnitude to that of human
ratings. Our work highlights the potential of LLMs to capture salient
dimensions of human sensory experience.",None,-1
076ff4eb-a896-468a-a582-4a553875f390,Predicting Hateful Discussions on Reddit using Graph Transformer Networks and Communal Context,0.655788,7,"We propose a system to predict harmful discussions on social media platforms.
Our solution uses contextual deep language models and proposes the novel idea
of integrating state-of-the-art Graph Transformer Networks to analyze all
conversations that follow an initial post. This framework also supports
adapting to future comments as the conversation unfolds. In addition, we study
whether a community-specific analysis of hate speech leads to more effective
detection of hateful discussions. We evaluate our approach on 333,487 Reddit
discussions from various communities. We find that community-specific modeling
improves performance two-fold and that models which capture wider-discussion
context improve accuracy by 28\% (35\% for the most hateful content) compared
to limited context models.",None,-1
b7cb28c8-e240-4233-a1ef-c3485fed2970,IUTEAM1 at MEDIQA-Chat 2023: Is simple fine tuning effective for multilayer summarization of clinical conversations?,0.164741,1,"Clinical conversation summarization has become an important application of
Natural language Processing. In this work, we intend to analyze summarization
model ensembling approaches, that can be utilized to improve the overall
accuracy of the generated medical report called chart note. The work starts
with a single summarization model creating the baseline. Then leads to an
ensemble of summarization models trained on a separate section of the chart
note. This leads to the final approach of passing the generated results to
another summarization model in a multi-layer/stage fashion for better coherency
of the generated text. Our results indicate that although an ensemble of models
specialized in each section produces better results, the multi-layer/stage
approach does not improve accuracy. The code for the above paper is available
at https://github.com/dhananjay-srivastava/MEDIQA-Chat-2023-iuteam1.git",None,-1
052f3848-176e-4c6a-8485-abf395884668,LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using XLM-RoBERTa,0.880884,9,"Named Entity Recognition(NER) is a task of recognizing entities at a token
level in a sentence. This paper focuses on solving NER tasks in a multilingual
setting for complex named entities. Our team, LLM-RM participated in the
recently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual
Complex Named Entity Recognition. We approach the problem by leveraging
cross-lingual representation provided by fine-tuning XLM-Roberta base model on
datasets of all of the 12 languages provided -- Bangla, Chinese, English,
Farsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and
Ukrainian",None,-1
c08bb8fc-a846-43c3-b901-96dc3ea71921,Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models,0.302666,2,"As the use of artificial intelligent (AI) models becomes more prevalent in
industries such as engineering and manufacturing, it is essential that these
models provide transparent reasoning behind their predictions. This paper
proposes the AI-Reasoner, which extracts the morphological characteristics of
defects (DefChars) from images and utilises decision trees to reason with the
DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e.
charts) and textual explanations to provide insights into outputs made by
masked-based defect detection and classification models. It also provides
effective mitigation strategies to enhance data pre-processing and overall
model performance. The AI-Reasoner was tested on explaining the outputs of an
IE Mask R-CNN model using a set of 366 images containing defects. The results
demonstrated its effectiveness in explaining the IE Mask R-CNN model's
predictions. Overall, the proposed AI-Reasoner provides a solution for
improving the performance of AI models in industrial applications that require
defect analysis.",None,-1
5d0d6e58-f2c1-4794-8629-9764abff79a9,FormalGeo: An Extensible Formalized Framework for Olympiad Geometric Problem Solving,0.535811,1,"This is the first paper in a series of work we have accomplished over the
past three years. In this paper, we have constructed a consistent formal plane
geometry system. This will serve as a crucial bridge between IMO-level plane
geometry challenges and readable AI automated reasoning. Within this formal
framework, we have been able to seamlessly integrate modern AI models with our
formal system. AI is now capable of providing deductive reasoning solutions to
IMO-level plane geometry problems, just like handling other natural languages,
and these proofs are readable, traceable, and verifiable. We propose the
geometry formalization theory (GFT) to guide the development of the geometry
formal system. Based on the GFT, we have established the FormalGeo, which
consists of 88 geometric predicates and 196 theorems. It can represent,
validate, and solve IMO-level geometry problems. we also have crafted the FGPS
(formal geometry problem solver) in Python. It serves as both an interactive
assistant for verifying problem-solving processes and an automated problem
solver. We've annotated the formalgeo7k and formalgeo-imo datasets. The former
contains 6,981 (expand to 133,818 through data augmentation) geometry problems,
while the latter includes 18 (expand to 2,627 and continuously increasing)
IMO-level challenging geometry problems. All annotated problems include
detailed formal language descriptions and solutions. Implementation of the
formal system and experiments validate the correctness and utility of the GFT.
The backward depth-first search method only yields a 2.42% problem-solving
failure rate, and we can incorporate deep learning techniques to achieve lower
one. The source code of FGPS and datasets are available at
https://github.com/BitSecret/FGPS.",None,-1
25217840-8f82-4d7a-941c-48b24409838f,CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification,0.657088,3,"Medical image classification poses unique challenges due to the long-tailed
distribution of diseases, the co-occurrence of diagnostic findings, and the
multiple views available for each study or patient. This paper introduces our
solution to the ICCV CVAMD 2023 Shared Task on CXR-LT: Multi-Label Long-Tailed
Classification on Chest X-Rays. Our approach introduces CheXFusion, a
transformer-based fusion module incorporating multi-view images. The fusion
module, guided by self-attention and cross-attention mechanisms, efficiently
aggregates multi-view features while considering label co-occurrence.
Furthermore, we explore data balancing and self-training methods to optimize
the model's performance. Our solution achieves state-of-the-art results with
0.372 mAP in the MIMIC-CXR test set, securing 1st place in the competition. Our
success in the task underscores the significance of considering multi-view
settings, class imbalance, and label co-occurrence in medical image
classification. Public code is available at
https://github.com/dongkyuk/CXR-LT-public-solution",None,-1
c11445c8-ed58-4d15-b182-698923f33175,Getting too personal(ized): The importance of feature choice in online adaptive algorithms,0.214496,4,"Digital educational technologies offer the potential to customize students'
experiences and learn what works for which students, enhancing the technology
as more students interact with it. We consider whether and when attempting to
discover how to personalize has a cost, such as if the adaptation to personal
information can delay the adoption of policies that benefit all students. We
explore these issues in the context of using multi-armed bandit (MAB)
algorithms to learn a policy for what version of an educational technology to
present to each student, varying the relation between student characteristics
and outcomes and also whether the algorithm is aware of these characteristics.
Through simulations, we demonstrate that the inclusion of student
characteristics for personalization can be beneficial when those
characteristics are needed to learn the optimal action. In other scenarios,
this inclusion decreases performance of the bandit algorithm. Moreover,
including unneeded student characteristics can systematically disadvantage
students with less common values for these characteristics. Our simulations do
however suggest that real-time personalization will be helpful in particular
real-world scenarios, and we illustrate this through case studies using
existing experimental results in ASSISTments. Overall, our simulations show
that adaptive personalization in educational technologies can be a double-edged
sword: real-time adaptation improves student experiences in some contexts, but
the slower adaptation and potentially discriminatory results mean that a more
personalized model is not always beneficial.",None,-1
cda470f7-dd54-4ac4-8132-ce1b25018c25,Motif: Intrinsic Motivation from Artificial Intelligence Feedback,0.548875,19,"Exploring rich environments and evaluating one's actions without prior
knowledge is immensely challenging. In this paper, we propose Motif, a general
method to interface such prior knowledge from a Large Language Model (LLM) with
an agent. Motif is based on the idea of grounding LLMs for decision-making
without requiring them to interact with the environment: it elicits preferences
from an LLM over pairs of captions to construct an intrinsic reward, which is
then used to train agents with reinforcement learning. We evaluate Motif's
performance and behavior on the challenging, open-ended and
procedurally-generated NetHack game. Surprisingly, by only learning to maximize
its intrinsic reward, Motif achieves a higher game score than an algorithm
directly trained to maximize the score itself. When combining Motif's intrinsic
reward with the environment reward, our method significantly outperforms
existing approaches and makes progress on tasks where no advancements have ever
been made without demonstrations. Finally, we show that Motif mostly generates
intuitive human-aligned behaviors which can be steered easily through prompt
modifications, while scaling well with the LLM size and the amount of
information given in the prompt.",None,-1
95693b8c-3cf0-4bb1-891a-9ddd16654a66,A Case-Based Persistent Memory for a Large Language Model,0.0109463,1,"Case-based reasoning (CBR) as a methodology for problem-solving can use any
appropriate computational technique. This position paper argues that CBR
researchers have somewhat overlooked recent developments in deep learning and
large language models (LLMs). The underlying technical developments that have
enabled the recent breakthroughs in AI have strong synergies with CBR and could
be used to provide a persistent memory for LLMs to make progress towards
Artificial General Intelligence.",None,-1
76516f46-14c2-4789-96a7-2f526b10c8ba,Overlap Bias Matching is Necessary for Point Cloud Registration,0.133406,1,"Point cloud registration is a fundamental problem in many domains.
Practically, the overlap between point clouds to be registered may be
relatively small. Most unsupervised methods lack effective initial evaluation
of overlap, leading to suboptimal registration accuracy. To address this issue,
we propose an unsupervised network Overlap Bias Matching Network (OBMNet) for
partial point cloud registration. Specifically, we propose a plug-and-play
Overlap Bias Matching Module (OBMM) comprising two integral components, overlap
sampling module and bias prediction module. These two components are utilized
to capture the distribution of overlapping regions and predict bias
coefficients of point cloud common structures, respectively. Then, we integrate
OBMM with the neighbor map matching module to robustly identify correspondences
by precisely merging matching scores of points within the neighborhood, which
addresses the ambiguities in single-point features. OBMNet can maintain
efficacy even in pair-wise registration scenarios with low overlap ratios.
Experimental results on extensive datasets demonstrate that our approach's
performance achieves a significant improvement compared to the state-of-the-art
registration approach.",None,-1
3683f89b-9243-4747-a229-efe30a79592b,Assessing the Effectiveness of GPT-3 in Detecting False Political Statements: A Case Study on the LIAR Dataset,0.844836,5,"The detection of political fake statements is crucial for maintaining
information integrity and preventing the spread of misinformation in society.
Historically, state-of-the-art machine learning models employed various methods
for detecting deceptive statements. These methods include the use of metadata
(W. Wang et al., 2018), n-grams analysis (Singh et al., 2021), and linguistic
(Wu et al., 2022) and stylometric (Islam et al., 2020) features. Recent
advancements in large language models, such as GPT-3 (Brown et al., 2020) have
achieved state-of-the-art performance on a wide range of tasks. In this study,
we conducted experiments with GPT-3 on the LIAR dataset (W. Wang et al., 2018)
and achieved higher accuracy than state-of-the-art models without using any
additional meta or linguistic features. Additionally, we experimented with
zero-shot learning using a carefully designed prompt and achieved near
state-of-the-art performance. An advantage of this approach is that the model
provided evidence for its decision, which adds transparency to the model's
decision-making and offers a chance for users to verify the validity of the
evidence provided.",None,-1
a328c93e-fd49-474f-9594-f05885fdf362,She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models,0.968864,10,"Implicit gender bias in software development is a well-documented issue, such
as the association of technical roles with men. To address this bias, it is
important to understand it in more detail. This study uses data mining
techniques to investigate the extent to which 56 tasks related to software
development, such as assigning GitHub issues and testing, are affected by
implicit gender bias embedded in large language models. We systematically
translated each task from English into a genderless language and back, and
investigated the pronouns associated with each task. Based on translating each
task 100 times in different permutations, we identify a significant disparity
in the gendered pronoun associations with different tasks. Specifically,
requirements elicitation was associated with the pronoun ""he"" in only 6% of
cases, while testing was associated with ""he"" in 100% of cases. Additionally,
tasks related to helping others had a 91% association with ""he"" while the same
association for tasks related to asking coworkers was only 52%. These findings
reveal a clear pattern of gender bias related to software development tasks and
have important implications for addressing this issue both in the training of
large language models and in broader society.",None,-1
6089f409-a4b9-43d2-9038-bed44cbf2470,Parallelizing Optical Flow Estimation on an Ultra-Low Power RISC-V Cluster for Nano-UAV Navigation,0.150126,2,"Optical flow estimation is crucial for autonomous navigation and localization
of unmanned aerial vehicles (UAV). On micro and nano UAVs, real-time
calculation of the optical flow is run on low power and resource-constrained
microcontroller units (MCUs). Thus, lightweight algorithms for optical flow
have been proposed targeting real-time execution on traditional single-core
MCUs. This paper introduces an efficient parallelization strategy for optical
flow computation targeting new-generation multicore low power RISC-V based
microcontroller units. Our approach enables higher frame rates at lower clock
speeds. It has been implemented and evaluated on the eight-core cluster of a
commercial octa-core MCU (GAP8) reaching a parallelization speedup factor of
7.21 allowing for a frame rate of 500 frames per second when running on a 50
MHz clock frequency. The proposed parallel algorithm significantly boosts the
camera frame rate on micro unmanned aerial vehicles, which enables higher
flight speeds: the maximum flight speed can be doubled, while using less than a
third of the clock frequency of previous single-core implementations.",None,-1
09fb1d19-8400-4056-b228-fdbcbd0b59ad,Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?,0.807754,2,"Large Language Models (LLMs) are advancing at a rapid pace, with significant
improvements at natural language processing and coding tasks. Yet, their
ability to work with formal languages representing data, specifically within
the realm of knowledge graph engineering, remains under-investigated. To
evaluate the proficiency of various LLMs, we created a set of five tasks that
probe their ability to parse, understand, analyze, and create knowledge graphs
serialized in Turtle syntax. These tasks, each embodying distinct degrees of
complexity and being able to scale with the size of the problem, have been
integrated into our automated evaluation system, the LLM-KG-Bench. The
evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4,
Claude 1.3, and Claude 2.0, as well as two freely accessible offline models,
GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth
understanding of the strengths and shortcomings of LLMs in relation to their
application within RDF knowledge graph engineering workflows utilizing Turtle
representation. While our findings show that the latest commercial models
outperform their forerunners in terms of proficiency with the Turtle language,
they also reveal an apparent weakness. These models fall short when it comes to
adhering strictly to the output formatting constraints, a crucial requirement
in this context.",None,-1
10886ace-3c1b-4fbb-8c16-3a615bea57d5,ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding,0.837592,55,"We introduce ZeroSCROLLS, a zero-shot benchmark for natural language
understanding over long texts, which contains only test and small validation
sets, without training data. We adapt six tasks from the SCROLLS benchmark, and
add four new datasets, including two novel information fusing tasks, such as
aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a
comprehensive evaluation of both open-source and closed large language models,
finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest
average score. However, there is still room for improvement on multiple open
challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to
pass the naive baseline. As the state of the art is a moving target, we invite
researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.",None,-1
5c346228-1af2-40d8-b7e7-3804c70c72a4,Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark,0.964962,18,"Artificial intelligence (AI) systems possess significant potential to drive
societal progress. However, their deployment often faces obstacles due to
substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a
solution to optimize policies while simultaneously adhering to multiple
constraints, thereby addressing the challenge of integrating reinforcement
learning in safety-critical scenarios. In this paper, we present an environment
suite called Safety-Gymnasium, which encompasses safety-critical tasks in both
single and multi-agent scenarios, accepting vector and vision-only input.
Additionally, we offer a library of algorithms named Safe Policy Optimization
(SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive
library can serve as a validation tool for the research community. By
introducing this benchmark, we aim to facilitate the evaluation and comparison
of safety performance, thus fostering the development of reinforcement learning
for safer, more reliable, and responsible real-world applications. The website
of this project can be accessed at
https://sites.google.com/view/safety-gymnasium.",None,-1
4e2090ab-2e70-4d84-9a40-9b8bc6e88787,Bridging the Transparency Gap: What Can Explainable AI Learn From the AI Act?,0.341543,6,"The European Union has proposed the Artificial Intelligence Act which
introduces detailed requirements of transparency for AI systems. Many of these
requirements can be addressed by the field of explainable AI (XAI), however,
there is a fundamental difference between XAI and the Act regarding what
transparency is. The Act views transparency as a means that supports wider
values, such as accountability, human rights, and sustainable innovation. In
contrast, XAI views transparency narrowly as an end in itself, focusing on
explaining complex algorithmic properties without considering the
socio-technical context. We call this difference the ``transparency gap''.
Failing to address the transparency gap, XAI risks leaving a range of
transparency issues unaddressed. To begin to bridge this gap, we overview and
clarify the terminology of how XAI and European regulation -- the Act and the
related General Data Protection Regulation (GDPR) -- view basic definitions of
transparency. By comparing the disparate views of XAI and regulation, we arrive
at four axes where practical work could bridge the transparency gap: defining
the scope of transparency, clarifying the legal status of XAI, addressing
issues with conformity assessment, and building explainability for datasets.",None,-1
5f9820a1-2f73-43a7-aee7-de8889c2fad7,Few-shot Semantic Image Synthesis with Class Affinity Transfer,0.154503,5,"Semantic image synthesis aims to generate photo realistic images given a
semantic segmentation map. Despite much recent progress, training them still
requires large datasets of images annotated with per-pixel label maps that are
extremely tedious to obtain. To alleviate the high annotation cost, we propose
a transfer method that leverages a model trained on a large source dataset to
improve the learning ability on small target datasets via estimated pairwise
relations between source and target classes. The class affinity matrix is
introduced as a first layer to the source model to make it compatible with the
target label maps, and the source model is then further finetuned for the
target domain. To estimate the class affinities we consider different
approaches to leverage prior knowledge: semantic segmentation on the source
domain, textual label embeddings, and self-supervised vision features. We apply
our approach to GAN-based and diffusion-based architectures for semantic
synthesis. Our experiments show that the different ways to estimate class
affinity can be effectively combined, and that our approach significantly
improves over existing state-of-the-art transfer approaches for generative
image models.",None,-1
46c57745-4a9b-49a7-a304-424741dd7db2,To be Robust and to be Fair: Aligning Fairness with Robustness,0.136645,2,"Adversarial training has been shown to be reliable in improving robustness
against adversarial samples. However, the problem of adversarial training in
terms of fairness has not yet been properly studied, and the relationship
between fairness and accuracy attack still remains unclear. Can we
simultaneously improve robustness w.r.t. both fairness and accuracy? To tackle
this topic, in this paper, we study the problem of adversarial training and
adversarial attack w.r.t. both metrics. We propose a unified structure for
fairness attack which brings together common notions in group fairness, and we
theoretically prove the equivalence of fairness attack against different
notions. Moreover, we show the alignment of fairness and accuracy attack, and
theoretically demonstrate that robustness w.r.t. one metric benefits from
robustness w.r.t. the other metric. Our study suggests a novel way to unify
adversarial training and attack w.r.t. fairness and accuracy, and experimental
results show that our proposed method achieves better performance in terms of
robustness w.r.t. both metrics.",None,-1
7261d0e8-0a4e-45b4-a0bc-4b7f2fbe34e4,Knowledge-aware Bayesian Co-attention for Multimodal Emotion Recognition,0.617307,10,"Multimodal emotion recognition is a challenging research area that aims to
fuse different modalities to predict human emotion. However, most existing
models that are based on attention mechanisms have difficulty in learning
emotionally relevant parts on their own. To solve this problem, we propose to
incorporate external emotion-related knowledge in the co-attention based fusion
of pre-trained models. To effectively incorporate this knowledge, we enhance
the co-attention model with a Bayesian attention module (BAM) where a prior
distribution is estimated using the emotion-related knowledge. Experimental
results on the IEMOCAP dataset show that the proposed approach can outperform
several state-of-the-art approaches by at least 0.7% unweighted accuracy (UA).",None,-1
df0fe517-6ce0-43c1-9b0e-6e74457fefd5,Brain subtle anomaly detection based on auto-encoders latent space analysis : application to de novo parkinson patients,0.289691,4,"Neural network-based anomaly detection remains challenging in clinical
applications with little or no supervised information and subtle anomalies such
as hardly visible brain lesions. Among unsupervised methods, patch-based
auto-encoders with their efficient representation power provided by their
latent space, have shown good results for visible lesion detection. However,
the commonly used reconstruction error criterion may limit their performance
when facing less obvious lesions. In this work, we design two alternative
detection criteria. They are derived from multivariate analysis and can more
directly capture information from latent space representations. Their
performance compares favorably with two additional supervised learning methods,
on a difficult de novo Parkinson Disease (PD) classification task.",None,-1
24b4fab7-84d2-4e22-9d2c-38d0eb377e6b,Text-to-Image Diffusion Models are Zero-Shot Classifiers,0.60156,47,"The excellent generative capabilities of text-to-image diffusion models
suggest they learn informative representations of image-text data. However,
what knowledge their representations capture is not fully understood, and they
have not been thoroughly explored on downstream tasks. We investigate diffusion
models by proposing a method for evaluating them as zero-shot classifiers. The
key idea is using a diffusion model's ability to denoise a noised image given a
text description of a label as a proxy for that label's likelihood. We apply
our method to Stable Diffusion and Imagen, using it to probe fine-grained
aspects of the models' knowledge and comparing them with CLIP's zero-shot
abilities. They perform competitively with CLIP on a wide range of zero-shot
image classification datasets. Additionally, they achieve state-of-the-art
results on shape/texture bias tests and can successfully perform attribute
binding while CLIP cannot. Although generative pre-training is prevalent in
NLP, visual foundation models often use other methods such as contrastive
learning. Based on our findings, we argue that generative pre-training should
be explored as a compelling alternative for vision-language tasks.",None,-1
8f5bc5f8-6eec-4eaf-a137-c4fd7dbab17f,Deep Clustering Survival Machines with Interpretable Expert Distributions,0.121201,1,"Conventional survival analysis methods are typically ineffective to
characterize heterogeneity in the population while such information can be used
to assist predictive modeling. In this study, we propose a hybrid survival
analysis method, referred to as deep clustering survival machines, that
combines the discriminative and generative mechanisms. Similar to the mixture
models, we assume that the timing information of survival data is generatively
described by a mixture of certain numbers of parametric distributions, i.e.,
expert distributions. We learn weights of the expert distributions for
individual instances according to their features discriminatively such that
each instance's survival information can be characterized by a weighted
combination of the learned constant expert distributions. This method also
facilitates interpretable subgrouping/clustering of all instances according to
their associated expert distributions. Extensive experiments on both real and
synthetic datasets have demonstrated that the method is capable of obtaining
promising clustering results and competitive time-to-event predicting
performance.",None,-1
7c50c7c0-a948-4bc4-84dd-3776c6836254,ControlVideo: Training-free Controllable Text-to-Video Generation,0.991939,111,"Text-driven diffusion models have unlocked unprecedented abilities in image
generation, whereas their video counterpart still lags behind due to the
excessive training cost of temporal modeling. Besides the training burden, the
generated videos also suffer from appearance inconsistency and structural
flickers, especially in long video synthesis. To address these challenges, we
design a \emph{training-free} framework called \textbf{ControlVideo} to enable
natural and efficient text-to-video generation. ControlVideo, adapted from
ControlNet, leverages coarsely structural consistency from input motion
sequences, and introduces three modules to improve video generation. Firstly,
to ensure appearance coherence between frames, ControlVideo adds fully
cross-frame interaction in self-attention modules. Secondly, to mitigate the
flicker effect, it introduces an interleaved-frame smoother that employs frame
interpolation on alternated frames. Finally, to produce long videos
efficiently, it utilizes a hierarchical sampler that separately synthesizes
each short clip with holistic coherency. Empowered with these modules,
ControlVideo outperforms the state-of-the-arts on extensive motion-prompt pairs
quantitatively and qualitatively. Notably, thanks to the efficient designs, it
generates both short and long videos within several minutes using one NVIDIA
2080Ti. Code is available at https://github.com/YBYBZhang/ControlVideo.",None,-1
2cf08edc-01e2-46ce-bda9-cf3a21da1a2d,Learning to Compress Prompts with Gist Tokens,0.774314,99,"Prompting is the primary way to utilize the multitask capabilities of
language models (LMs), but prompts occupy valuable space in the input context
window, and repeatedly encoding the same prompt is computationally inefficient.
Finetuning and distillation methods allow for specialization of LMs without
prompting, but require retraining the model for each task. To avoid this
trade-off entirely, we present gisting, which trains an LM to compress prompts
into smaller sets of ""gist"" tokens which can be cached and reused for compute
efficiency. Gist models can be trained with no additional cost over standard
instruction finetuning by simply modifying Transformer attention masks to
encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder
(FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting
in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings,
all with minimal loss in output quality.",None,-1
b80aaa30-1aaf-4769-ab31-d338c05f6162,SimCGNN: Simple Contrastive Graph Neural Network for Session-based Recommendation,0.154882,2,"Session-based recommendation (SBR) problem, which focuses on next-item
prediction for anonymous users, has received increasingly more attention from
researchers. Existing graph-based SBR methods all lack the ability to
differentiate between sessions with the same last item, and suffer from severe
popularity bias. Inspired by nowadays emerging contrastive learning methods,
this paper presents a Simple Contrastive Graph Neural Network for Session-based
Recommendation (SimCGNN). In SimCGNN, we first obtain normalized session
embeddings on constructed session graphs. We next construct positive and
negative samples of the sessions by two forward propagation and a novel
negative sample selection strategy, and then calculate the constructive loss.
Finally, session embeddings are used to give prediction. Extensive experiments
conducted on two real-word datasets show our SimCGNN achieves a significant
improvement over state-of-the-art methods.",None,-1
5d156219-2e5b-42ba-aacb-33bbd4fc3452,Asynchronous training of quantum reinforcement learning,0.612224,10,"The development of quantum machine learning (QML) has received a lot of
interest recently thanks to developments in both quantum computing (QC) and
machine learning (ML). One of the ML paradigms that can be utilized to address
challenging sequential decision-making issues is reinforcement learning (RL).
It has been demonstrated that classical RL can successfully complete many
difficult tasks. A leading method of building quantum RL agents relies on the
variational quantum circuits (VQC). However, training QRL algorithms with VQCs
requires significant amount of computational resources. This issue hurdles the
exploration of various QRL applications. In this paper, we approach this
challenge through asynchronous training QRL agents. Specifically, we choose the
asynchronous training of advantage actor-critic variational quantum policies.
We demonstrate the results via numerical simulations that within the tasks
considered, the asynchronous training of QRL agents can reach performance
comparable to or superior than classical agents with similar model sizes and
architectures.",None,-1
511158c8-d67e-415d-98d8-aa6124831ee7,Zero and Few-shot Semantic Parsing with Ambiguous Inputs,0.405125,7,"Despite the frequent challenges posed by ambiguity when representing meaning
via natural language, it is often ignored or deliberately removed in tasks
mapping language to formally-designed representations, which generally assume a
one-to-one mapping between linguistic and formal representations. We attempt to
address this shortcoming by introducing AmP, a framework, dataset, and
challenge for translating ambiguous natural language to formal representations
like logic and code. We define templates and generate data for five
well-documented linguistic ambiguities. Using AmP, we investigate how several
few-shot text-to-code systems handle ambiguity, introducing three new metrics.
We find that large pre-trained models perform poorly at capturing the
distribution of possible meanings without deliberate instruction. However,
models are able to capture the distribution well when ambiguity is attested in
their inputs. These results motivate a call for including ambiguity explicitly
in datasets and promote considering the distribution of possible outputs when
evaluating systems. Data and code: https://github.com/esteng/ambiguous_parsing",None,-1
f51b72b0-6c7a-4aef-a4d5-5ef58c7a69c5,Designerly Understanding: Information Needs for Model Transparency to Support Design Ideation for AI-Powered User Experience,0.999809,39,"Despite the widespread use of artificial intelligence (AI), designing user
experiences (UX) for AI-powered systems remains challenging. UX designers face
hurdles understanding AI technologies, such as pre-trained language models, as
design materials. This limits their ability to ideate and make decisions about
whether, where, and how to use AI. To address this problem, we bridge the
literature on AI design and AI transparency to explore whether and how
frameworks for transparent model reporting can support design ideation with
pre-trained models. By interviewing 23 UX practitioners, we find that
practitioners frequently work with pre-trained models, but lack support for
UX-led ideation. Through a scenario-based design task, we identify common goals
that designers seek model understanding for and pinpoint their model
transparency information needs. Our study highlights the pivotal role that UX
designers can play in Responsible AI and calls for supporting their
understanding of AI limitations through model transparency and interrogation.",None,-1
7699b192-c2f5-46e9-8528-634d66fd7e40,Inpainting borehole images using Generative Adversarial Networks,0.0755199,1,"In this paper, we propose a GAN-based approach for gap filling in borehole
images created by wireline microresistivity imaging tools. The proposed method
utilizes a generator, global discriminator, and local discriminator to inpaint
the missing regions of the image. The generator is based on an auto-encoder
architecture with skip-connections, and the loss function used is the
Wasserstein GAN loss. Our experiments on a dataset of borehole images
demonstrate that the proposed model can effectively deal with large-scale
missing pixels and generate realistic completion results. This approach can
improve the quantitative evaluation of reservoirs and provide an essential
basis for interpreting geological phenomena and reservoir parameters.",None,-1
626168a9-1afd-4793-9e3c-bf589286c537,Glancing Future for Simultaneous Machine Translation,0.541831,5,"Simultaneous machine translation (SiMT) outputs translation while reading the
source sentence. Unlike conventional sequence-to-sequence (seq2seq) training,
existing SiMT methods adopt the prefix-to-prefix (prefix2prefix) training,
where the model predicts target tokens based on partial source tokens. However,
the prefix2prefix training diminishes the ability of the model to capture
global information and introduces forced predictions due to the absence of
essential source information. Consequently, it is crucial to bridge the gap
between the prefix2prefix training and seq2seq training to enhance the
translation capability of the SiMT model. In this paper, we propose a novel
method that glances future in curriculum learning to achieve the transition
from the seq2seq training to prefix2prefix training. Specifically, we gradually
reduce the available source information from the whole sentence to the prefix
corresponding to that latency. Our method is applicable to a wide range of SiMT
methods and experiments demonstrate that our method outperforms strong
baselines.",None,-1
c51165c0-5e9d-489c-8109-e0ea88d34441,Fine-grained building roof instance segmentation based on domain adapted pretraining and composite dual-backbone,0.468793,2,"The diversity of building architecture styles of global cities situated on
various landforms, the degraded optical imagery affected by clouds and shadows,
and the significant inter-class imbalance of roof types pose challenges for
designing a robust and accurate building roof instance segmentor. To address
these issues, we propose an effective framework to fulfill semantic
interpretation of individual buildings with high-resolution optical satellite
imagery. Specifically, the leveraged domain adapted pretraining strategy and
composite dual-backbone greatly facilitates the discriminative feature
learning. Moreover, new data augmentation pipeline, stochastic weight averaging
(SWA) training and instance segmentation based model ensemble in testing are
utilized to acquire additional performance boost. Experiment results show that
our approach ranks in the first place of the 2023 IEEE GRSS Data Fusion Contest
(DFC) Track 1 test phase ($mAP_{50}$:50.6\%). Note-worthily, we have also
explored the potential of multimodal data fusion with both optical satellite
imagery and SAR data.",None,-1
267e96c6-8238-4140-a8af-9147abfaa1de,DISPEL: Domain Generalization via Domain-Specific Liberating,0.317435,4,"Domain generalization aims to learn a generalization model that can perform
well on unseen test domains by only training on limited source domains.
However, existing domain generalization approaches often bring in
prediction-irrelevant noise or require the collection of domain labels. To
address these challenges, we consider the domain generalization problem from a
different perspective by categorizing underlying feature groups into
domain-shared and domain-specific features. Nevertheless, the domain-specific
features are difficult to be identified and distinguished from the input data.
In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing
fine-grained masking approach that can filter out undefined and
indistinguishable domain-specific features in the embedding space.
Specifically, DISPEL utilizes a mask generator that produces a unique mask for
each input data to filter domain-specific features. The DISPEL framework is
highly flexible to be applied to any fine-tuned models. We derive a
generalization error bound to guarantee the generalization performance by
optimizing a designed objective loss. The experimental results on five
benchmarks demonstrate DISPEL outperforms existing methods and can further
generalize various algorithms.",None,-1
2cbdc5a8-fcb7-48ba-8064-618fd923cfc7,Scale Guided Hypernetwork for Blind Super-Resolution Image Quality Assessment,0.40159,2,"With the emergence of image super-resolution (SR) algorithm, how to blindly
evaluate the quality of super-resolution images has become an urgent task.
However, existing blind SR image quality assessment (IQA) metrics merely focus
on visual characteristics of super-resolution images, ignoring the available
scale information. In this paper, we reveal that the scale factor has a
statistically significant impact on subjective quality scores of SR images,
indicating that the scale information can be used to guide the task of blind SR
IQA. Motivated by this, we propose a scale guided hypernetwork framework that
evaluates SR image quality in a scale-adaptive manner. Specifically, the blind
SR IQA procedure is divided into three stages, i.e., content perception,
evaluation rule generation, and quality prediction. After content perception, a
hypernetwork generates the evaluation rule used in quality prediction based on
the scale factor of the SR image. We apply the proposed scale guided
hypernetwork framework to existing representative blind IQA metrics, and
experimental results show that the proposed framework not only boosts the
performance of these IQA metrics but also enhances their generalization
abilities. Source code will be available at https://github.com/JunFu1995/SGH.",None,-1
ea20b59e-64f5-4009-b6cf-5c73e86096fa,Optimization-Inspired Cross-Attention Transformer for Compressive Sensing,0.852979,10,"By integrating certain optimization solvers with deep neural networks, deep
unfolding network (DUN) with good interpretability and high performance has
attracted growing attention in compressive sensing (CS). However, existing DUNs
often improve the visual quality at the price of a large number of parameters
and have the problem of feature information loss during iteration. In this
paper, we propose an Optimization-inspired Cross-attention Transformer (OCT)
module as an iterative process, leading to a lightweight OCT-based Unfolding
Framework (OCTUF) for image CS. Specifically, we design a novel Dual Cross
Attention (Dual-CA) sub-module, which consists of an Inertia-Supplied Cross
Attention (ISCA) block and a Projection-Guided Cross Attention (PGCA) block.
ISCA block introduces multi-channel inertia forces and increases the memory
effect by a cross attention mechanism between adjacent iterations. And, PGCA
block achieves an enhanced information interaction, which introduces the
inertia force into the gradient descent step through a cross attention block.
Extensive CS experiments manifest that our OCTUF achieves superior performance
compared to state-of-the-art methods while training lower complexity. Codes are
available at https://github.com/songjiechong/OCTUF.",None,-1
fa0d2ec1-31fc-42d0-9764-6184c074e18a,L2T-DLN: Learning to Teach with Dynamic Loss Network,0.297761,1,"With the concept of teaching being introduced to the machine learning
community, a teacher model start using dynamic loss functions to teach the
training of a student model. The dynamic intends to set adaptive loss functions
to different phases of student model learning. In existing works, the teacher
model 1) merely determines the loss function based on the present states of the
student model, i.e., disregards the experience of the teacher; 2) only utilizes
the states of the student model, e.g., training iteration number and
loss/accuracy from training/validation sets, while ignoring the states of the
loss function. In this paper, we first formulate the loss adjustment as a
temporal task by designing a teacher model with memory units, and, therefore,
enables the student learning to be guided by the experience of the teacher
model. Then, with a dynamic loss network, we can additionally use the states of
the loss to assist the teacher learning in enhancing the interactions between
the teacher and the student model. Extensive experiments demonstrate our
approach can enhance student learning and improve the performance of various
deep models on real-world tasks, including classification, objective detection,
and semantic segmentation scenarios.",None,-1
fab0efc8-dced-4620-838f-4ce5faeeac31,Class Prior-Free Positive-Unlabeled Learning with Taylor Variational Loss for Hyperspectral Remote Sensing Imagery,0.496704,2,"Positive-unlabeled learning (PU learning) in hyperspectral remote sensing
imagery (HSI) is aimed at learning a binary classifier from positive and
unlabeled data, which has broad prospects in various earth vision applications.
However, when PU learning meets limited labeled HSI, the unlabeled data may
dominate the optimization process, which makes the neural networks overfit the
unlabeled data. In this paper, a Taylor variational loss is proposed for HSI PU
learning, which reduces the weight of the gradient of the unlabeled data by
Taylor series expansion to enable the network to find a balance between
overfitting and underfitting. In addition, the self-calibrated optimization
strategy is designed to stabilize the training process. Experiments on 7
benchmark datasets (21 tasks in total) validate the effectiveness of the
proposed method. Code is at: https://github.com/Hengwei-Zhao96/T-HOneCls.",None,-1
f5b31e46-b470-46c6-bd20-7e20a34e79e3,AMR Parsing with Causal Hierarchical Attention and Pointers,0.236382,1,"Translation-based AMR parsers have recently gained popularity due to their
simplicity and effectiveness. They predict linearized graphs as free texts,
avoiding explicit structure modeling. However, this simplicity neglects
structural locality in AMR graphs and introduces unnecessary tokens to
represent coreferences. In this paper, we introduce new target forms of AMR
parsing and a novel model, CHAP, which is equipped with causal hierarchical
attention and the pointer mechanism, enabling the integration of structures
into the Transformer decoder. We empirically explore various alternative
modeling options. Experiments show that our model outperforms baseline models
on four out of five benchmarks in the setting of no additional data.",None,-1
19ee08af-a612-4f07-ad0d-8ac3ea3748a9,PM-DETR: Domain Adaptive Prompt Memory for Object Detection with Transformers,0.0463178,1,"The Transformer-based detectors (i.e., DETR) have demonstrated impressive
performance on end-to-end object detection. However, transferring DETR to
different data distributions may lead to a significant performance degradation.
Existing adaptation techniques focus on model-based approaches, which aim to
leverage feature alignment to narrow the distribution shift between different
domains. In this study, we propose a hierarchical Prompt Domain Memory (PDM)
for adapting detection transformers to different distributions. PDM
comprehensively leverages the prompt memory to extract domain-specific
knowledge and explicitly constructs a long-term memory space for the data
distribution, which represents better domain diversity compared to existing
methods. Specifically, each prompt and its corresponding distribution value are
paired in the memory space, and we inject top M distribution-similar prompts
into the input and multi-level embeddings of DETR. Additionally, we introduce
the Prompt Memory Alignment (PMA) to reduce the discrepancy between the source
and target domains by fully leveraging the domain-specific knowledge extracted
from the prompt domain memory. Extensive experiments demonstrate that our
method outperforms state-of-the-art domain adaptive object detection methods on
three benchmarks, including scene, synthetic to real, and weather adaptation.
Codes will be released.",None,-1
9db5db8e-c1d6-4c8a-898c-0bbaecf1b721,Exploring the Law of Numbers: Evidence from China's Real Estate,0.119866,1,"The renowned proverb, Numbers do not lie, underscores the reliability and
insight that lie beneath numbers, a concept of undisputed importance,
especially in economics and finance etc. Despite the prosperity of Benford's
Law in the first digit analysis, its scope fails to remain comprehensiveness
when it comes to deciphering the laws of number. This paper delves into number
laws by taking the financial statements of China real estate as a
representative, quantitatively study not only the first digit, but also depict
the other two dimensions of numbers: frequency and length. The research
outcomes transcend mere reservations about data manipulation and open the door
to discussions surrounding number diversity and the delineation of the usage
insights. This study wields both economic significance and the capacity to
foster a deeper comprehension of numerical phenomena.",None,-1
4c5fd558-11be-4157-a42f-ffe4621bf24f,Extracting Multi-valued Relations from Language Models,0.508716,2,"The widespread usage of latent language representations via pre-trained
language models (LMs) suggests that they are a promising source of structured
knowledge. However, existing methods focus only on a single object per
subject-relation pair, even though often multiple objects are correct. To
overcome this limitation, we analyze these representations for their potential
to yield materialized multi-object relational knowledge. We formulate the
problem as a rank-then-select task. For ranking candidate objects, we evaluate
existing prompting techniques and propose new ones incorporating domain
knowledge. Among the selection methods, we find that choosing objects with a
likelihood above a learned relation-specific threshold gives a 49.5% F1 score.
Our results highlight the difficulty of employing LMs for the multi-valued
slot-filling task and pave the way for further research on extracting
relational knowledge from latent language representations.",None,-1
93412ab3-8512-445a-b218-f75ff4847c13,Crystal: Introspective Reasoners Reinforced with Self-Feedback,0.355776,13,"Extensive work has shown that the performance and interpretability of
commonsense reasoning can be improved via knowledge-augmented reasoning
methods, where the knowledge that underpins the reasoning process is explicitly
verbalized and utilized. However, existing implementations, including
""chain-of-thought"" and its variants, fall short in capturing the introspective
nature of knowledge required in commonsense reasoning, and in accounting for
the mutual adaptation between the generation and utilization of knowledge. We
propose a novel method to develop an introspective commonsense reasoner,
Crystal. To tackle commonsense problems, it first introspects for knowledge
statements related to the given question, and subsequently makes an informed
prediction that is grounded in the previously introspected knowledge. The
knowledge introspection and knowledge-grounded reasoning modes of the model are
tuned via reinforcement learning to mutually adapt, where the reward derives
from the feedback given by the model itself. Experiments show that Crystal
significantly outperforms both the standard supervised finetuning and
chain-of-thought distilled methods, and enhances the transparency of the
commonsense reasoning process. Our work ultimately validates the feasibility
and potential of reinforcing a neural model with self-feedback.",None,-1
fbb2feee-18dd-432c-855b-af24c486ee41,Towards Reliable Rare Category Analysis on Graphs via Individual Calibration,0.387298,3,"Rare categories abound in a number of real-world networks and play a pivotal
role in a variety of high-stakes applications, including financial fraud
detection, network intrusion detection, and rare disease diagnosis. Rare
category analysis (RCA) refers to the task of detecting, characterizing, and
comprehending the behaviors of minority classes in a highly-imbalanced data
distribution. While the vast majority of existing work on RCA has focused on
improving the prediction performance, a few fundamental research questions
heretofore have received little attention and are less explored: How confident
or uncertain is a prediction model in rare category analysis? How can we
quantify the uncertainty in the learning process and enable reliable rare
category analysis?
  To answer these questions, we start by investigating miscalibration in
existing RCA methods. Empirical results reveal that state-of-the-art RCA
methods are mainly over-confident in predicting minority classes and
under-confident in predicting majority classes. Motivated by the observation,
we propose a novel individual calibration framework, named CALIRARE, for
alleviating the unique challenges of RCA, thus enabling reliable rare category
analysis. In particular, to quantify the uncertainties in RCA, we develop a
node-level uncertainty quantification algorithm to model the overlapping
support regions with high uncertainty; to handle the rarity of minority classes
in miscalibration calculation, we generalize the distribution-based calibration
metric to the instance level and propose the first individual calibration
measurement on graphs named Expected Individual Calibration Error (EICE). We
perform extensive experimental evaluations on real-world datasets, including
rare category characterization and model calibration tasks, which demonstrate
the significance of our proposed framework.",None,-1
26c9c08b-0172-4383-a7d4-6456503252f9,From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models,0.181373,2,"Reasoning is a distinctive human capacity, enabling us to address complex
problems by breaking them down into a series of manageable cognitive steps.
Yet, complex logical reasoning is still cumbersome for language models. Based
on the dual process theory in cognitive science, we are the first to unravel
the cognitive reasoning abilities of language models. Our framework employs an
iterative methodology to construct a Cognitive Tree (CogTree). The root node of
this tree represents the initial query, while the leaf nodes consist of
straightforward questions that can be answered directly. This construction
involves two main components: the implicit extraction module (referred to as
the intuitive system) and the explicit reasoning module (referred to as the
reflective system). The intuitive system rapidly generates multiple responses
by utilizing in-context examples, while the reflective system scores these
responses using comparative learning. The scores guide the intuitive system in
its subsequent generation step. Our experimental results on two popular and
challenging reasoning tasks indicate that it is possible to achieve a
performance level comparable to that of GPT-3.5 (with 175B parameters), using a
significantly smaller language model that contains fewer parameters (<=7B) than
5% of GPT-3.5.",None,-1
b4839bb7-b64f-490f-add9-8ada86da4fa5,Learning in POMDPs is Sample-Efficient with Hindsight Observability,0.986986,15,"POMDPs capture a broad class of decision making problems, but hardness
results suggest that learning is intractable even in simple settings due to the
inherent partial observability. However, in many realistic problems, more
information is either revealed or can be computed during some point of the
learning process. Motivated by diverse applications ranging from robotics to
data center scheduling, we formulate a Hindsight Observable Markov Decision
Process (HOMDP) as a POMDP where the latent states are revealed to the learner
in hindsight and only during training. We introduce new algorithms for the
tabular and function approximation settings that are provably sample-efficient
with hindsight observability, even in POMDPs that would otherwise be
statistically intractable. We give a lower bound showing that the tabular
algorithm is optimal in its dependence on latent state and observation
cardinalities.",None,-1
3a8a3bf9-aed9-48cb-a016-dddcd239be57,FactKG: Fact Verification via Reasoning on Knowledge Graphs,0.875688,24,"In real world applications, knowledge graphs (KG) are widely used in various
domains (e.g. medical applications and dialogue agents). However, for fact
verification, KGs have not been adequately utilized as a knowledge source. KGs
can be a valuable knowledge source in fact verification due to their
reliability and broad applicability. A KG consists of nodes and edges which
makes it clear how concepts are linked together, allowing machines to reason
over chains of topics. However, there are many challenges in understanding how
these machine-readable concepts map to information in text. To enable the
community to better use KGs, we introduce a new dataset, FactKG: Fact
Verification via Reasoning on Knowledge Graphs. It consists of 108k natural
language claims with five types of reasoning: One-hop, Conjunction, Existence,
Multi-hop, and Negation. Furthermore, FactKG contains various linguistic
patterns, including colloquial style claims as well as written style claims to
increase practicality. Lastly, we develop a baseline approach and analyze
FactKG over these reasoning types. We believe FactKG can advance both
reliability and practicality in KG-based fact verification.",None,-1
6a14636b-68a3-4426-979b-195ceed239bd,FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration,0.987484,9,"We propose an end-to-end deep learning approach to rectify fisheye images and
simultaneously calibrate camera intrinsic and distortion parameters. Our method
consists of two parts: a Quick Image Rectification Module developed with a
Pix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with a
CNN architecture. Our Quick Rectification Network performs robust rectification
with good resolution, making it suitable for constant calibration in
camera-based surveillance equipment. To achieve high-quality calibration, we
use the straightened output from the Quick Rectification Module as a
guidance-like semantic feature map for the Calibration Module to learn the
geometric relationship between the straightened feature and the distorted
feature. We train and validate our method with a large synthesized dataset
labeled with well-simulated parameters applied to a perspective image dataset.
Our solution has achieved robust performance in high-resolution with a
significant PSNR value of 22.343.",None,-1
27655399-a8a2-447b-94fe-9c7527cffc01,Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages,0.495302,7,"One of the challenges with finetuning pretrained language models (PLMs) is
that their tokenizer is optimized for the language(s) it was pretrained on, but
brittle when it comes to previously unseen variations in the data. This can for
instance be observed when finetuning PLMs on one language and evaluating them
on data in a closely related language variety with no standardized orthography.
Despite the high linguistic similarity, tokenization no longer corresponds to
meaningful representations of the target data, leading to low performance in,
e.g., part-of-speech tagging.
  In this work, we finetune PLMs on seven languages from three different
families and analyze their zero-shot performance on closely related,
non-standardized varieties. We consider different measures for the divergence
in the tokenization of the source and target data, and the way they can be
adjusted by manipulating the tokenization during the finetuning step. Overall,
we find that the similarity between the percentage of words that get split into
subwords in the source and target data (the split word ratio difference) is the
strongest predictor for model performance on target data.",None,-1
e2128fc5-d39f-47c0-a711-a847986eecdb,Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations,0.91337,48,"Causal abstraction is a promising theoretical framework for explainable
artificial intelligence that defines when an interpretable high-level causal
model is a faithful simplification of a low-level deep learning system.
However, existing causal abstraction methods have two major limitations: they
require a brute-force search over alignments between the high-level model and
the low-level one, and they presuppose that variables in the high-level model
will align with disjoint sets of neurons in the low-level one. In this paper,
we present distributed alignment search (DAS), which overcomes these
limitations. In DAS, we find the alignment between high-level and low-level
models using gradient descent rather than conducting a brute-force search, and
we allow individual neurons to play multiple distinct roles by analyzing
representations in non-standard bases-distributed representations. Our
experiments show that DAS can discover internal structure that prior approaches
miss. Overall, DAS removes previous obstacles to conducting causal abstraction
analyses and allows us to find conceptual structure in trained neural nets.",None,-1
ba8f2c3c-4b8a-4f6a-bdf5-473f728934b0,CLIP the Gap: A Single Domain Generalization Approach for Object Detection,0.6474,36,"Single Domain Generalization (SDG) tackles the problem of training a model on
a single source domain so that it generalizes to any unseen target domain.
While this has been well studied for image classification, the literature on
SDG object detection remains almost non-existent. To address the challenges of
simultaneously learning robust object localization and representation, we
propose to leverage a pre-trained vision-language model to introduce semantic
domain concepts via textual prompts. We achieve this via a semantic
augmentation strategy acting on the features extracted by the detector
backbone, as well as a text-based classification loss. Our experiments evidence
the benefits of our approach, outperforming by 10% the only existing SDG object
detection method, Single-DGOD [49], on their own diverse weather-driving
benchmark.",None,-1
068285f9-b606-4251-841e-37d0b924b6f0,Towards Multi-Layered 3D Garments Animation,0.491815,3,"Mimicking realistic dynamics in 3D garment animations is a challenging task
due to the complex nature of multi-layered garments and the variety of outer
forces involved. Existing approaches mostly focus on single-layered garments
driven by only human bodies and struggle to handle general scenarios. In this
paper, we propose a novel data-driven method, called LayersNet, to model
garment-level animations as particle-wise interactions in a micro physics
system. We improve simulation efficiency by representing garments as
patch-level particles in a two-level structural hierarchy. Moreover, we
introduce a novel Rotation Equivalent Transformation that leverages the
rotation invariance and additivity of physics systems to better model outer
forces. To verify the effectiveness of our approach and bridge the gap between
experimental environments and real-world scenarios, we introduce a new
challenging dataset, D-LAYERS, containing 700K frames of dynamics of 4,900
different combinations of multi-layered garments driven by both human bodies
and randomly sampled wind. Our experiments show that LayersNet achieves
superior performance both quantitatively and qualitatively. We will make the
dataset and code publicly available at
https://mmlab-ntu.github.io/project/layersnet/index.html .",None,-1
76265de0-7188-43e4-bd65-03e994fc0b44,Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models,0.18365,4,"This report introduces the Attention Visualizer package, which is crafted to
visually illustrate the significance of individual words in encoder-only
transformer-based models. In contrast to other methods that center on tokens
and self-attention scores, our approach will examine the words and their impact
on the final embedding representation. Libraries like this play a crucial role
in enhancing the interpretability and explainability of neural networks. They
offer the opportunity to illuminate their internal mechanisms, providing a
better understanding of how they operate and can be enhanced. You can access
the code and review examples on the following GitHub repository:
https://github.com/AlaFalaki/AttentionVisualizer.",None,-1
1b43fda2-e235-43fe-bf1b-a8fb3680ac9d,SpeechAlign: a Framework for Speech Translation Alignment Evaluation,0.343819,2,"Speech-to-Speech and Speech-to-Text translation are currently dynamic areas
of research. In our commitment to advance these fields, we present SpeechAlign,
a framework designed to evaluate the underexplored field of source-target
alignment in speech models. The SpeechAlign framework has two core components.
First, to tackle the absence of suitable evaluation datasets, we introduce the
Speech Gold Alignment dataset, built upon a English-German text translation
gold alignment dataset. Secondly, we introduce two novel metrics, Speech
Alignment Error Rate (SAER) and Time-weighted Speech Alignment Error Rate
(TW-SAER), which enable the evaluation of alignment quality within speech
models. While the former gives equal importance to each word, the latter
assigns weights based on the length of the words in the speech signal. By
publishing SpeechAlign we provide an accessible evaluation framework for model
assessment, and we employ it to benchmark open-source Speech Translation
models. In doing so, we contribute to the ongoing research progress within the
fields of Speech-to-Speech and Speech-to-Text translation.",None,-1
1287d3c0-3979-4eee-9d25-56d8cc7e43d2,FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow,0.648888,14,"Reconstruction of 3D neural fields from posed images has emerged as a
promising method for self-supervised representation learning. The key challenge
preventing the deployment of these 3D scene learners on large-scale video data
is their dependence on precise camera poses from structure-from-motion, which
is prohibitively expensive to run at scale. We propose a method that jointly
reconstructs camera poses and 3D neural scene representations online and in a
single forward pass. We estimate poses by first lifting frame-to-frame optical
flow to 3D scene flow via differentiable rendering, preserving locality and
shift-equivariance of the image processing backbone. SE(3) camera pose
estimation is then performed via a weighted least-squares fit to the scene flow
field. This formulation enables us to jointly supervise pose estimation and a
generalizable neural scene representation via re-rendering the input video, and
thus, train end-to-end and fully self-supervised on real-world video datasets.
We demonstrate that our method performs robustly on diverse, real-world video,
notably on sequences traditionally challenging to optimization-based pose
estimation techniques.",None,-1
3af3d6cd-aa26-4085-9d1b-7879c68cde09,Distracting Downpour: Adversarial Weather Attacks for Motion Estimation,0.460798,3,"Current adversarial attacks on motion estimation, or optical flow, optimize
small per-pixel perturbations, which are unlikely to appear in the real world.
In contrast, adverse weather conditions constitute a much more realistic threat
scenario. Hence, in this work, we present a novel attack on motion estimation
that exploits adversarially optimized particles to mimic weather effects like
snowflakes, rain streaks or fog clouds. At the core of our attack framework is
a differentiable particle rendering system that integrates particles (i)
consistently over multiple time steps (ii) into the 3D space (iii) with a
photo-realistic appearance. Through optimization, we obtain adversarial weather
that significantly impacts the motion estimation. Surprisingly, methods that
previously showed good robustness towards small per-pixel perturbations are
particularly vulnerable to adversarial weather. At the same time, augmenting
the training with non-optimized weather increases a method's robustness towards
weather effects and improves generalizability at almost no additional cost. Our
code will be available at https://github.com/cv-stuttgart/DistractingDownpour.",None,-1
b972cd54-1815-4ecf-8db2-86393628fd6d,Mimicking the Thinking Process for Emotion Recognition in Conversation with Prompts and Paraphrasing,0.797573,5,"Emotion recognition in conversation, which aims to predict the emotion for
all utterances, has attracted considerable research attention in recent years.
It is a challenging task since the recognition of the emotion in one utterance
involves many complex factors, such as the conversational context, the
speaker's background, and the subtle difference between emotion labels. In this
paper, we propose a novel framework which mimics the thinking process when
modeling these factors. Specifically, we first comprehend the conversational
context with a history-oriented prompt to selectively gather information from
predecessors of the target utterance. We then model the speaker's background
with an experience-oriented prompt to retrieve the similar utterances from all
conversations. We finally differentiate the subtle label semantics with a
paraphrasing mechanism to elicit the intrinsic label related knowledge. We
conducted extensive experiments on three benchmarks. The empirical results
demonstrate the superiority of our proposed framework over the state-of-the-art
baselines.",None,-1
2d102a7a-9c16-4296-a8a8-024233a1762d,Instant Domain Augmentation for LiDAR Semantic Segmentation,0.592645,14,"Despite the increasing popularity of LiDAR sensors, perception algorithms
using 3D LiDAR data struggle with the 'sensor-bias problem'. Specifically, the
performance of perception algorithms significantly drops when an unseen
specification of LiDAR sensor is applied at test time due to the domain
discrepancy. This paper presents a fast and flexible LiDAR augmentation method
for the semantic segmentation task, called 'LiDomAug'. It aggregates raw LiDAR
scans and creates a LiDAR scan of any configurations with the consideration of
dynamic distortion and occlusion, resulting in instant domain augmentation. Our
on-demand augmentation module runs at 330 FPS, so it can be seamlessly
integrated into the data loader in the learning framework. In our experiments,
learning-based approaches aided with the proposed LiDomAug are less affected by
the sensor-bias issue and achieve new state-of-the-art domain adaptation
performances on SemanticKITTI and nuScenes dataset without the use of the
target domain data. We also present a sensor-agnostic model that faithfully
works on the various LiDAR configurations.",None,-1
6050d5a2-cbd9-46cd-b58e-1b20303e77d3,AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling,0.699167,2,"Business optimisation refers to the process of finding and implementing
efficient and cost-effective means of operation to bring a competitive
advantage for businesses. Synthesizing problem formulations is an integral part
of business optimisation, which relies on human expertise to construct problem
formulations using optimisation languages. Interestingly, with advancements in
Large Language Models (LLMs), the human expertise needed in problem formulation
can be minimized. However, developing an LLM for problem formulation is
challenging, due to training data, token limitations, and lack of appropriate
performance metrics. For the requirement of training data, recent attention has
been directed towards fine-tuning pre-trained LLMs for downstream tasks rather
than training an LLM from scratch for a specific task. In this paper, we adopt
an LLM fine-tuning approach and propose an AI-Copilot for business optimisation
problem formulation. For token limitations, we introduce modularization and
prompt engineering techniques to synthesize complex problem formulations as
modules that fit into the token limits of LLMs. Additionally, we design
performance evaluation metrics that are better suited for assessing the
accuracy and quality of problem formulations. The experiment results
demonstrate that with this approach we can synthesize complex and large problem
formulations for a typical business optimisation problem in production
scheduling.",None,-1
79c69dea-00b1-4bf3-9f73-ce602cbfa865,Solving Long-run Average Reward Robust MDPs via Stochastic Games,0.296625,1,"Markov decision processes (MDPs) provide a standard framework for sequential
decision making under uncertainty. However, MDPs do not take uncertainty in
transition probabilities into account. Robust Markov decision processes (RMDPs)
address this shortcoming of MDPs by assigning to each transition an uncertainty
set rather than a single probability value. In this work, we consider polytopic
RMDPs in which all uncertainty sets are polytopes and study the problem of
solving long-run average reward polytopic RMDPs. We present a novel perspective
on this problem and show that it can be reduced to solving long-run average
reward turn-based stochastic games with finite state and action spaces. This
reduction allows us to derive several important consequences that were hitherto
not known to hold for polytopic RMDPs. First, we derive new computational
complexity bounds for solving long-run average reward polytopic RMDPs, showing
for the first time that the threshold decision problem for them is in $NP \cap
coNP$ and that they admit a randomized algorithm with sub-exponential expected
runtime. Second, we present Robust Polytopic Policy Iteration (RPPI), a novel
policy iteration algorithm for solving long-run average reward polytopic RMDPs.
Our experimental evaluation shows that RPPI is much more efficient in solving
long-run average reward polytopic RMDPs compared to state-of-the-art methods
based on value iteration.",None,-1
891d5df1-8fbf-48ca-b435-bdd31f05b526,The Entoptic Field Camera as Metaphor-Driven Research-through-Design with AI Technologies,0.33143,13,"Artificial intelligence (AI) technologies are widely deployed in smartphone
photography; and prompt-based image synthesis models have rapidly become
commonplace. In this paper, we describe a Research-through-Design (RtD) project
which explores this shift in the means and modes of image production via the
creation and use of the Entoptic Field Camera. Entoptic phenomena usually refer
to perceptions of floaters or bright blue dots stemming from the physiological
interplay of the eye and brain. We use the term entoptic as a metaphor to
investigate how the material interplay of data and models in AI technologies
shapes human experiences of reality. Through our case study using first-person
design and a field study, we offer implications for critical, reflective,
more-than-human and ludic design to engage AI technologies; the
conceptualisation of an RtD research space which contributes to AI literacy
discourses; and outline a research trajectory concerning materiality and design
affordances of AI technologies.",None,-1
80ce33b3-ac87-4344-b8a4-aaf009e53442,Advancing Medical Imaging with Language Models: A Journey from N-grams to ChatGPT,0.883215,27,"In this paper, we aimed to provide a review and tutorial for researchers in
the field of medical imaging using language models to improve their tasks at
hand. We began by providing an overview of the history and concepts of language
models, with a special focus on large language models. We then reviewed the
current literature on how language models are being used to improve medical
imaging, emphasizing different applications such as image captioning, report
generation, report classification, finding extraction, visual question
answering, interpretable diagnosis, and more for various modalities and organs.
The ChatGPT was specially highlighted for researchers to explore more potential
applications. We covered the potential benefits of accurate and efficient
language models for medical imaging analysis, including improving clinical
workflow efficiency, reducing diagnostic errors, and assisting healthcare
professionals in providing timely and accurate diagnoses. Overall, our goal was
to bridge the gap between language models and medical imaging and inspire new
ideas and innovations in this exciting area of research. We hope that this
review paper will serve as a useful resource for researchers in this field and
encourage further exploration of the possibilities of language models in
medical imaging.",None,-1
00c58834-781a-472e-b090-11b197b9b4e7,Spectrum-guided Multi-granularity Referring Video Object Segmentation,0.790169,21,"Current referring video object segmentation (R-VOS) techniques extract
conditional kernels from encoded (low-resolution) vision-language features to
segment the decoded high-resolution features. We discovered that this causes
significant feature drift, which the segmentation kernels struggle to perceive
during the forward computation. This negatively affects the ability of
segmentation kernels. To address the drift problem, we propose a
Spectrum-guided Multi-granularity (SgMg) approach, which performs direct
segmentation on the encoded features and employs visual details to further
optimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion
(SCF) to perform intra-frame global interactions in the spectral domain for
effective multimodal representation. Finally, we extend SgMg to perform
multi-object R-VOS, a new paradigm that enables simultaneous segmentation of
multiple referred objects in a video. This not only makes R-VOS faster, but
also more practical. Extensive experiments show that SgMg achieves
state-of-the-art performance on four video benchmark datasets, outperforming
the nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMg
enables multi-object R-VOS, runs about 3 times faster while maintaining
satisfactory performance. Code is available at https://github.com/bo-miao/SgMg.",None,-1
c5eed6bf-87d6-4c42-aaf6-c100532869c0,SAOR: Single-View Articulated Object Reconstruction,0.457083,5,"We introduce SAOR, a novel approach for estimating the 3D shape, texture, and
viewpoint of an articulated object from a single image captured in the wild.
Unlike prior approaches that rely on pre-defined category-specific 3D templates
or tailored 3D skeletons, SAOR learns to articulate shapes from single-view
image collections with a skeleton-free part-based model without requiring any
3D object shape priors. To prevent ill-posed solutions, we propose a
cross-instance consistency loss that exploits disentangled object shape
deformation and articulation. This is helped by a new silhouette-based sampling
mechanism to enhance viewpoint diversity during training. Our method only
requires estimated object silhouettes and relative depth maps from
off-the-shelf pre-trained networks during training. At inference time, given a
single-view image, it efficiently outputs an explicit mesh representation. We
obtain improved qualitative and quantitative results on challenging quadruped
animals compared to relevant existing work.",None,-1
92066ce2-cd06-4266-988b-71e3febb1d62,SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene Reconstruction by Neural Radiance Field (NeRF),0.151706,1,"The accurate reconstruction of surgical scenes from surgical videos is
critical for various applications, including intraoperative navigation and
image-guided robotic surgery automation. However, previous approaches, mainly
relying on depth estimation, have limited effectiveness in reconstructing
surgical scenes with moving surgical tools. To address this limitation and
provide accurate 3D position prediction for surgical tools in all frames, we
propose a novel approach called SAMSNeRF that combines Segment Anything Model
(SAM) and Neural Radiance Field (NeRF) techniques. Our approach generates
accurate segmentation masks of surgical tools using SAM, which guides the
refinement of the dynamic surgical scene reconstruction by NeRF. Our
experimental results on public endoscopy surgical videos demonstrate that our
approach successfully reconstructs high-fidelity dynamic surgical scenes and
accurately reflects the spatial information of surgical tools. Our proposed
approach can significantly enhance surgical navigation and automation by
providing surgeons with accurate 3D position information of surgical tools
during surgery.The source code will be released soon.",None,-1
e79e8333-bc54-4c68-b140-26157bc2b616,Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions,0.737271,5,"We propose a method for synthesizing edited photo-realistic digital avatars
with text instructions. Given a short monocular RGB video and text
instructions, our method uses an image-conditioned diffusion model to edit one
head image and uses the video stylization method to accomplish the editing of
other head images. Through iterative training and update (three times or more),
our method synthesizes edited photo-realistic animatable 3D neural head avatars
with a deformable neural radiance field head synthesis method. In quantitative
and qualitative studies on various subjects, our method outperforms
state-of-the-art methods.",None,-1
56f3e730-bafc-404f-ad52-d1a85e7d681f,Patton: Language Model Pretraining on Text-Rich Networks,0.699437,25,"A real-world text corpus sometimes comprises not only text documents but also
semantic links between them (e.g., academic papers in a bibliographic network
are linked by citations and co-authorships). Text documents and semantic
connections form a text-rich network, which empowers a wide range of downstream
tasks such as classification and retrieval. However, pretraining methods for
such structures are still lacking, making it difficult to build one generic
model that can be adapted to various tasks on text-rich networks. Current
pretraining objectives, such as masked language modeling, purely model texts
and do not take inter-document structure information into consideration. To
this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.
Patton includes two pretraining strategies: network-contextualized masked
language modeling and masked node prediction, to capture the inherent
dependency between textual attributes and network structure. We conduct
experiments on four downstream tasks in five datasets from both academic and
e-commerce domains, where Patton outperforms baselines significantly and
consistently.",None,-1
30409f0e-c891-4d78-8d3a-a32712e44fb7,TherapyView: Visualizing Therapy Sessions with Temporal Topic Modeling and AI-Generated Arts,0.76477,4,"We present the TherapyView, a demonstration system to help therapists
visualize the dynamic contents of past treatment sessions, enabled by the
state-of-the-art neural topic modeling techniques to analyze the topical
tendencies of various psychiatric conditions and deep learning-based image
generation engine to provide a visual summary. The system incorporates temporal
modeling to provide a time-series representation of topic similarities at a
turn-level resolution and AI-generated artworks given the dialogue segments to
provide a concise representations of the contents covered in the session,
offering interpretable insights for therapists to optimize their strategies and
enhance the effectiveness of psychotherapy. This system provides a proof of
concept of AI-augmented therapy tools with e in-depth understanding of the
patient's mental state and enabling more effective treatment.",None,-1
15604f89-ad05-4739-b303-81050c6c32de,WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models,0.978655,13,"We present WinoQueer: a benchmark specifically designed to measure whether
large language models (LLMs) encode biases that are harmful to the LGBTQ+
community. The benchmark is community-sourced, via application of a novel
method that generates a bias benchmark from a community survey. We apply our
benchmark to several popular LLMs and find that off-the-shelf models generally
do exhibit considerable anti-queer bias. Finally, we show that LLM bias against
a marginalized community can be somewhat mitigated by finetuning on data
written about or by members of that community, and that social media text
written by community members is more effective than news text written about the
community by non-members. Our method for community-in-the-loop benchmark
development provides a blueprint for future researchers to develop
community-driven, harms-grounded LLM benchmarks for other marginalized
communities.",None,-1
d3d7bc16-b490-45b5-82d8-4dc1a5f3e312,Towards Understanding the Generalization of Medical Text-to-SQL Models and Datasets,0.994752,6,"Electronic medical records (EMRs) are stored in relational databases. It can
be challenging to access the required information if the user is unfamiliar
with the database schema or general database fundamentals. Hence, researchers
have explored text-to-SQL generation methods that provide healthcare
professionals direct access to EMR data without needing a database expert.
However, currently available datasets have been essentially ""solved"" with
state-of-the-art models achieving accuracy greater than or near 90%. In this
paper, we show that there is still a long way to go before solving text-to-SQL
generation in the medical domain. To show this, we create new splits of the
existing medical text-to-SQL dataset MIMICSQL that better measure the
generalizability of the resulting models. We evaluate state-of-the-art language
models on our new split showing substantial drops in performance with accuracy
dropping from up to 92% to 28%, thus showing substantial room for improvement.
Moreover, we introduce a novel data augmentation approach to improve the
generalizability of the language models. Overall, this paper is the first step
towards developing more robust text-to-SQL models in the medical
domain.\footnote{The dataset and code will be released upon acceptance.",None,-1
f1111510-f535-4268-adfe-94e89fc55f7a,Active Coarse-to-Fine Segmentation of Moveable Parts from Real Images,0.105271,1,"We introduce the first active learning (AL) framework for high-accuracy
instance segmentation of moveable parts from RGB images of real indoor scenes.
As with most human-in-the-loop approaches, the key criterion for success in AL
is to minimize human effort while still attaining high performance. To this
end, we employ a transformer that utilizes a masked-attention mechanism to
supervise the active segmentation. To enhance the network tailored to moveable
parts, we introduce a coarse-to-fine AL approach which first uses an
object-aware masked attention and then a pose-aware one, leveraging the
hierarchical nature of the problem and a correlation between moveable parts and
object poses and interaction directions. Our method achieves close to fully
accurate (96% and higher) segmentation results, with semantic labels, on real
images, with 82% time saving over manual effort, where the training data
consists of only 11.45% annotated real photographs. At last, we contribute a
dataset of 2,550 real photographs with annotated moveable parts, demonstrating
its superior quality and diversity over the current best alternatives.",None,-1
6851d828-4151-492a-a41f-c8d7e2515623,A store-and-forward cloud-based telemonitoring system for automatic assessing dysarthria evolution in neurological diseases from video-recording analysis,0.900509,5,"Background and objectives: Patients suffering from neurological diseases may
develop dysarthria, a motor speech disorder affecting the execution of speech.
Close and quantitative monitoring of dysarthria evolution is crucial for
enabling clinicians to promptly implement patient management strategies and
maximizing effectiveness and efficiency of communication functions in term of
restoring, compensating or adjusting. In the clinical assessment of orofacial
structures and functions, at rest condition or during speech and non-speech
movements, a qualitative evaluation is usually performed, throughout visual
observation. Methods: To overcome limitations posed by qualitative assessments,
this work presents a store-and-forward self-service telemonitoring system that
integrates, within its cloud architecture, a convolutional neural network (CNN)
for analyzing video recordings acquired by individuals with dysarthria. This
architecture, called facial landmark Mask RCNN, aims at locating facial
landmarks as a prior for assessing the orofacial functions related to speech
and examining dysarthria evolution in neurological diseases. Results: When
tested on the Toronto NeuroFace dataset, a publicly available annotated dataset
of video recordings from patients with amyotrophic lateral sclerosis (ALS) and
stroke, the proposed CNN achieved a normalized mean error equal to 1.79 on
localizing the facial landmarks. We also tested our system in a real-life
scenario on 11 bulbar-onset ALS subjects, obtaining promising outcomes in terms
of facial landmark position estimation. Discussion and conclusions: This
preliminary study represents a relevant step towards the use of remote tools to
support clinicians in monitoring the evolution of dysarthria.",None,-1
0ca697d4-d1a5-47cb-afc7-cf6bcf482d31,Fast Neural Scene Flow,0.495673,14,"Neural Scene Flow Prior (NSFP) is of significant interest to the vision
community due to its inherent robustness to out-of-distribution (OOD) effects
and its ability to deal with dense lidar points. The approach utilizes a
coordinate neural network to estimate scene flow at runtime, without any
training. However, it is up to 100 times slower than current state-of-the-art
learning methods. In other applications such as image, video, and radiance
function reconstruction innovations in speeding up the runtime performance of
coordinate networks have centered upon architectural changes. In this paper, we
demonstrate that scene flow is different -- with the dominant computational
bottleneck stemming from the loss function itself (i.e., Chamfer distance).
Further, we rediscover the distance transform (DT) as an efficient,
correspondence-free loss function that dramatically speeds up the runtime
optimization. Our fast neural scene flow (FNSF) approach reports for the first
time real-time performance comparable to learning methods, without any training
or OOD bias on two of the largest open autonomous driving (AV) lidar datasets
Waymo Open and Argoverse.",None,-1
b8d50bb1-73ba-4880-8bcd-148cebc498cd,SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition,0.477285,2,"Multilingual automatic speech recognition (ASR) systems have garnered
attention for their potential to extend language coverage globally. While
self-supervised learning (SSL) models, like MMS, have demonstrated their
effectiveness in multilingual ASR, it is worth noting that various layers'
representations potentially contain distinct information that has not been
fully leveraged. In this study, we propose a novel method that leverages
self-supervised hierarchical representations (SSHR) to fine-tune the MMS model.
We first analyze the different layers of MMS and show that the middle layers
capture language-related information, and the high layers encode
content-related information, which gradually decreases in the final layers.
Then, we extract a language-related frame from correlated middle layers and
guide specific language extraction through self-attention mechanisms.
Additionally, we steer the model toward acquiring more content-related
information in the final layers using our proposed Cross-CTC. We evaluate SSHR
on two multilingual datasets, Common Voice and ML-SUPERB, and the experimental
results demonstrate that our method achieves state-of-the-art performance.",None,-1
14f6795c-8263-4bf7-a066-1a3c9ac472c4,Ontology engineering with Large Language Models,0.927114,13,"We tackle the task of enriching ontologies by automatically translating
natural language sentences into Description Logic. Since Large Language Models
(LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to
convert Natural Language sentences into OWL Functional Syntax. We employ
objective and concise examples to fine-tune the model regarding: instances,
class subsumption, domain and range of relations, object properties
relationships, disjoint classes, complements, cardinality restrictions. The
resulted axioms are used to enrich an ontology, in a human supervised manner.
The developed tool is publicly provided as a Protge plugin.",None,-1
8c7ce85f-c271-4716-96f2-6ab8ecfea59b,Depth-Relative Self Attention for Monocular Depth Estimation,0.512263,5,"Monocular depth estimation is very challenging because clues to the exact
depth are incomplete in a single RGB image. To overcome the limitation, deep
neural networks rely on various visual hints such as size, shade, and texture
extracted from RGB information. However, we observe that if such hints are
overly exploited, the network can be biased on RGB information without
considering the comprehensive view. We propose a novel depth estimation model
named RElative Depth Transformer (RED-T) that uses relative depth as guidance
in self-attention. Specifically, the model assigns high attention weights to
pixels of close depth and low attention weights to pixels of distant depth. As
a result, the features of similar depth can become more likely to each other
and thus less prone to misused visual hints. We show that the proposed model
achieves competitive results in monocular depth estimation benchmarks and is
less biased to RGB information. In addition, we propose a novel monocular depth
estimation benchmark that limits the observable depth range during training in
order to evaluate the robustness of the model for unseen depths.",None,-1
5ad7d3b8-ceea-43a3-9b75-3a2a3909ea2e,Towards human-like spoken dialogue generation between AI agents from written dialogue,0.577001,5,"The advent of large language models (LLMs) has made it possible to generate
natural written dialogues between two agents. However, generating human-like
spoken dialogues from these written dialogues remains challenging. Spoken
dialogues have several unique characteristics: they frequently include
backchannels and laughter, and the smoothness of turn-taking significantly
influences the fluidity of conversation. This study proposes CHATS - CHatty
Agents Text-to-Speech - a discrete token-based system designed to generate
spoken dialogues based on written dialogues. Our system can generate speech for
both the speaker side and the listener side simultaneously, using only the
transcription from the speaker side, which eliminates the need for
transcriptions of backchannels or laughter. Moreover, CHATS facilitates natural
turn-taking; it determines the appropriate duration of silence after each
utterance in the absence of overlap, and it initiates the generation of
overlapping speech based on the phoneme sequence of the next utterance in case
of overlap. Experimental evaluations indicate that CHATS outperforms the
text-to-speech baseline, producing spoken dialogues that are more interactive
and fluid while retaining clarity and intelligibility.",None,-1
bc03dcd7-d75e-49cc-ab2a-ca6228a6c5db,Energy Efficiency of Training Neural Network Architectures: An Empirical Study,0.524001,11,"The evaluation of Deep Learning models has traditionally focused on criteria
such as accuracy, F1 score, and related measures. The increasing availability
of high computational power environments allows the creation of deeper and more
complex models. However, the computations needed to train such models entail a
large carbon footprint. In this work, we study the relations between DL model
architectures and their environmental impact in terms of energy consumed and
CO$_2$ emissions produced during training by means of an empirical study using
Deep Convolutional Neural Networks. Concretely, we study: (i) the impact of the
architecture and the location where the computations are hosted on the energy
consumption and emissions produced; (ii) the trade-off between accuracy and
energy efficiency; and (iii) the difference on the method of measurement of the
energy consumed using software-based and hardware-based tools.",None,-1
e788283b-285c-4112-aca6-d0095f90dd6c,FrameBERT: Conceptual Metaphor Detection with Frame Embedding Learning,0.891416,14,"In this paper, we propose FrameBERT, a RoBERTa-based model that can
explicitly learn and incorporate FrameNet Embeddings for concept-level metaphor
detection. FrameBERT not only achieves better or comparable performance to the
state-of-the-art, but also is more explainable and interpretable compared to
existing models, attributing to its ability of accounting for external
knowledge of FrameNet.",None,-1
20ad6c83-b0af-4067-b41d-5d93a005d57a,ClusterLLM: Large Language Models as a Guide for Text Clustering,0.820056,19,"We introduce ClusterLLM, a novel text clustering framework that leverages
feedback from an instruction-tuned large language model, such as ChatGPT.
Compared with traditional unsupervised methods that builds upon ""small""
embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the
emergent capability of LLM even if its embeddings are inaccessible; and (2) it
understands the user's preference on clustering through textual instruction
and/or a few annotated data. First, we prompt ChatGPT for insights on
clustering perspective by constructing hard triplet questions <does A better
correspond to B than C>, where A, B and C are similar data points that belong
to different clusters according to small embedder. We empirically show that
this strategy is both effective for fine-tuning small embedder and
cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on
clustering granularity by carefully designed pairwise questions <do A and B
belong to the same category>, and tune the granularity from cluster hierarchies
that is the most consistent with the ChatGPT answers. Extensive experiments on
14 datasets show that ClusterLLM consistently improves clustering quality, at
an average cost of ~$0.6 per dataset. The code will be available at
https://github.com/zhang-yu-wei/ClusterLLM.",None,-1
34eb7def-81a4-4c3a-a605-c57b13aeeba4,ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint,0.534718,2,"Large-scale online recommender system spreads all over the Internet being in
charge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion
Rate (CVR) estimations. However, traditional CVR estimators suffer from
well-known Sample Selection Bias and Data Sparsity issues. Entire space models
were proposed to address the two issues via tracing the decision-making path of
""exposure_click_purchase"". Further, some researchers observed that there are
purchase-related behaviors between click and purchase, which can better draw
the user's decision-making intention and improve the recommendation
performance. Thus, the decision-making path has been extended to
""exposure_click_in-shop action_purchase"" and can be modeled with conditional
probability approach. Nevertheless, we observe that the chain rule of
conditional probability does not always hold. We report Probability Space
Confusion (PSC) issue and give a derivation of difference between ground-truth
and estimation mathematically. We propose a novel Entire Space Multi-Task Model
for Post-Click Conversion Rate via Parameter Constraint (ESMC) and two
alternatives: Entire Space Multi-Task Model with Siamese Network (ESMS) and
Entire Space Multi-Task Model in Global Domain (ESMG) to address the PSC issue.
Specifically, we handle ""exposure_click_in-shop action"" and ""in-shop
action_purchase"" separately in the light of characteristics of in-shop action.
The first path is still treated with conditional probability while the second
one is treated with parameter constraint strategy. Experiments on both offline
and online environments in a large-scale recommendation system illustrate the
superiority of our proposed methods over state-of-the-art models. The
real-world datasets will be released.",None,-1
1add42ab-5ec1-4085-a0f7-a38fd559bf2b,Bridging the Gap between Model Explanations in Partially Annotated Multi-label Classification,0.451894,5,"Due to the expensive costs of collecting labels in multi-label classification
datasets, partially annotated multi-label classification has become an emerging
field in computer vision. One baseline approach to this task is to assume
unobserved labels as negative labels, but this assumption induces label noise
as a form of false negative. To understand the negative impact caused by false
negative labels, we study how these labels affect the model's explanation. We
observe that the explanation of two models, trained with full and partial
labels each, highlights similar regions but with different scaling, where the
latter tends to have lower attribution scores. Based on these findings, we
propose to boost the attribution scores of the model trained with partial
labels to make its explanation resemble that of the model trained with full
labels. Even with the conceptually simple approach, the multi-label
classification performance improves by a large margin in three different
datasets on a single positive label setting and one on a large-scale partial
label setting. Code is available at
https://github.com/youngwk/BridgeGapExplanationPAMC.",None,-1
25fc9d1f-aec3-4385-b2ec-5b56328418c2,Optimising Human-Machine Collaboration for Efficient High-Precision Information Extraction from Text Documents,0.127383,2,"While humans can extract information from unstructured text with high
precision and recall, this is often too time-consuming to be practical.
Automated approaches, on the other hand, produce nearly-immediate results, but
may not be reliable enough for high-stakes applications where precision is
essential. In this work, we consider the benefits and drawbacks of various
human-only, human-machine, and machine-only information extraction approaches.
We argue for the utility of a human-in-the-loop approach in applications where
high precision is required, but purely manual extraction is infeasible. We
present a framework and an accompanying tool for information extraction using
weak-supervision labelling with human validation. We demonstrate our approach
on three criminal justice datasets. We find that the combination of computer
speed and human understanding yields precision comparable to manual annotation
while requiring only a fraction of time, and significantly outperforms fully
automated baselines in terms of precision.",None,-1
d8fd03b9-252e-482f-9164-8b4f7e5ab00a,Towards Diverse and Coherent Augmentation for Time-Series Forecasting,0.206409,2,"Time-series data augmentation mitigates the issue of insufficient training
data for deep learning models. Yet, existing augmentation methods are mainly
designed for classification, where class labels can be preserved even if
augmentation alters the temporal dynamics. We note that augmentation designed
for forecasting requires diversity as well as coherence with the original
temporal dynamics. As time-series data generated by real-life physical
processes exhibit characteristics in both the time and frequency domains, we
propose to combine Spectral and Time Augmentation (STAug) for generating more
diverse and coherent samples. Specifically, in the frequency domain, we use the
Empirical Mode Decomposition to decompose a time series and reassemble the
subcomponents with random weights. This way, we generate diverse samples while
being coherent with the original temporal relationships as they contain the
same set of base components. In the time domain, we adapt a mix-up strategy
that generates diverse as well as linearly in-between coherent samples.
Experiments on five real-world time-series datasets demonstrate that STAug
outperforms the base models without data augmentation as well as
state-of-the-art augmentation methods.",None,-1
80b2b6e5-30ab-41b8-9663-a2999f8db9fa,Chain-of-Skills: A Configurable Model for Open-domain Question Answering,0.341764,8,"The retrieval model is an indispensable component for real-world
knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As
separate retrieval skills are annotated for different datasets, recent work
focuses on customized methods, limiting the model transferability and
scalability. In this work, we propose a modular retriever where individual
modules correspond to key skills that can be reused across datasets. Our
approach supports flexible skill configurations based on the target domain to
boost performance. To mitigate task interference, we design a novel
modularization parameterization inspired by sparse Transformer. We demonstrate
that our model can benefit from self-supervised pretraining on Wikipedia and
fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our
approach outperforms recent self-supervised retrievers in zero-shot evaluations
and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA
and OTT-QA.",None,-1
a10b6170-16e2-4510-a233-c9b93c5e850a,eTag: Class-Incremental Learning with Embedding Distillation and Task-Oriented Generation,0.159275,1,"Class-Incremental Learning (CIL) aims to solve the neural networks'
catastrophic forgetting problem, which refers to the fact that once the network
updates on a new task, its performance on previously-learned tasks drops
dramatically. Most successful CIL methods incrementally train a feature
extractor with the aid of stored exemplars, or estimate the feature
distribution with the stored prototypes. However, the stored exemplars would
violate the data privacy concerns, while the stored prototypes might not
reasonably be consistent with a proper feature distribution, hindering the
exploration of real-world CIL applications. In this paper, we propose a method
of \textit{e}mbedding distillation and \textit{Ta}sk-oriented
\textit{g}eneration (\textit{eTag}) for CIL, which requires neither the
exemplar nor the prototype. Instead, eTag achieves a data-free manner to train
the neural networks incrementally. To prevent the feature extractor from
forgetting, eTag distills the embeddings of the network's intermediate blocks.
Additionally, eTag enables a generative network to produce suitable features,
fitting the needs of the top incremental classifier. Experimental results
confirmed that our proposed eTag considerably outperforms the state-of-the-art
methods on CIFAR-100 and ImageNet-sub\footnote{Our code is available in the
Supplementary Materials.",None,-1
9f4853c4-6a4f-4917-be2f-6f5fda4f869b,CT-Net: Arbitrary-Shaped Text Detection via Contour Transformer,0.0650635,1,"Contour based scene text detection methods have rapidly developed recently,
but still suffer from inaccurate frontend contour initialization, multi-stage
error accumulation, or deficient local information aggregation. To tackle these
limitations, we propose a novel arbitrary-shaped scene text detection framework
named CT-Net by progressive contour regression with contour transformers.
Specifically, we first employ a contour initialization module that generates
coarse text contours without any post-processing. Then, we adopt contour
refinement modules to adaptively refine text contours in an iterative manner,
which are beneficial for context information capturing and progressive global
contour deformation. Besides, we propose an adaptive training strategy to
enable the contour transformers to learn more potential deformation paths, and
introduce a re-score mechanism that can effectively suppress false positives.
Extensive experiments are conducted on four challenging datasets, which
demonstrate the accuracy and efficiency of our CT-Net over state-of-the-art
methods. Particularly, CT-Net achieves F-measure of 86.1 at 11.2 frames per
second (FPS) and F-measure of 87.8 at 10.1 FPS for CTW1500 and Total-Text
datasets, respectively.",None,-1
de418764-c8f0-4659-b960-7c183fee7469,"Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting",0.0996637,6,"Language models can be prompted to reason through problems in a manner that
significantly improves performance. However, \textit{why} such prompting
improves performance is unclear. Recent work showed that using logically
\textit{invalid} Chain-of-Thought (CoT) prompting improves performance almost
as much as logically \textit{valid} CoT prompting, and that editing CoT prompts
to replace problem-specific information with abstract information or
out-of-distribution information typically doesn't harm performance. Critics
have responded that these findings are based on too few and too easily solved
tasks to draw meaningful conclusions. To resolve this dispute, we test whether
logically invalid CoT prompts offer the same level of performance gains as
logically valid prompts on the hardest tasks in the BIG-Bench benchmark, termed
BIG-Bench Hard (BBH). We find that the logically \textit{invalid} reasoning
prompts do indeed achieve similar performance gains on BBH tasks as logically
valid reasoning prompts. We also discover that some CoT prompts used by
previous works contain logical errors. This suggests that covariates beyond
logically valid reasoning are responsible for performance improvements.",None,-1
c39f1264-97fd-4eb1-a301-e39b35f75a87,Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation,0.588244,18,"Zero-shot instance segmentation aims to detect and precisely segment objects
of unseen categories without any training samples. Since the model is trained
on seen categories, there is a strong bias that the model tends to classify all
the objects into seen categories. Besides, there is a natural confusion between
background and novel objects that have never shown up in training. These two
challenges make novel objects hard to be raised in the final instance
segmentation results. It is desired to rescue novel objects from background and
dominated seen categories. To this end, we propose D$^2$Zero with
Semantic-Promoted Debiasing and Background Disambiguation to enhance the
performance of Zero-shot instance segmentation. Semantic-promoted debiasing
utilizes inter-class semantic relationships to involve unseen categories in
visual feature training and learns an input-conditional classifier to conduct
dynamical classification based on the input image. Background disambiguation
produces image-adaptive background representation to avoid mistaking novel
objects for background. Extensive experiments show that we significantly
outperform previous state-of-the-art methods by a large margin, e.g., 16.86%
improvement on COCO. Project page: https://henghuiding.github.io/D2Zero/",None,-1
42c1ce47-abf4-4146-a6ba-f8d5089ba1fd,Does Collaborative Human-LM Dialogue Generation Help Information Extraction from Human Dialogues?,0.0798407,1,"The capabilities of pretrained language models have opened opportunities to
explore new application areas, but applications involving human-human
interaction are limited by the fact that most data is protected from public
release for privacy reasons. Problem-solving human dialogues in real
applications can be much more complex than existing Wizard-of-Oz collections,
preventing successful domain transfer. To support information extraction (IE)
for a private call center dataset, we introduce a human-in-the-loop dialogue
generation framework capable of synthesizing realistic dialogues. In IE
experiments with auto insurance call center dialogues, we observe 25\% relative
improvement in $F_1$ after augmenting a small set of real human conversations
with synthetic data. We release code and our synthetic dataset to illustrate
the complexity of real-world call center conversations and encourage
development of complex dialogue datasets that are more representative of
natural data.",None,-1
d5bb9cf2-4cc8-403f-9f30-454e5a1cf28f,Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures,0.841617,10,"Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.",None,-1
da9a6454-041d-418f-9fd9-89f17187d382,A Recipe for Watermarking Diffusion Models,0.999294,61,"Diffusion models (DMs) have demonstrated advantageous potential on generative
tasks. Widespread interest exists in incorporating DMs into downstream
applications, such as producing or editing photorealistic images. However,
practical deployment and unprecedented power of DMs raise legal issues,
including copyright protection and monitoring of generated content. In this
regard, watermarking has been a proven solution for copyright protection and
content monitoring, but it is underexplored in the DMs literature.
Specifically, DMs generate samples from longer tracks and may have newly
designed multimodal structures, necessitating the modification of conventional
watermarking pipelines. To this end, we conduct comprehensive analyses and
derive a recipe for efficiently watermarking state-of-the-art DMs (e.g., Stable
Diffusion), via training from scratch or finetuning. Our recipe is
straightforward but involves empirically ablated implementation details,
providing a foundation for future research on watermarking DMs. The code is
available at https://github.com/yunqing-me/WatermarkDM.",None,-1
da13e15a-a23c-4cbb-b0f3-5d750d4cf939,Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint,0.627672,6,"Clinical trials are indispensable in developing new treatments, but they face
obstacles in patient recruitment and retention, hindering the enrollment of
necessary participants. To tackle these challenges, deep learning frameworks
have been created to match patients to trials. These frameworks calculate the
similarity between patients and clinical trial eligibility criteria,
considering the discrepancy between inclusion and exclusion criteria. Recent
studies have shown that these frameworks outperform earlier approaches.
However, deep learning models may raise fairness issues in patient-trial
matching when certain sensitive groups of individuals are underrepresented in
clinical trials, leading to incomplete or inaccurate data and potential harm.
To tackle the issue of fairness, this work proposes a fair patient-trial
matching framework by generating a patient-criterion level fairness constraint.
The proposed framework considers the inconsistency between the embedding of
inclusion and exclusion criteria among patients of different sensitive groups.
The experimental results on real-world patient-trial and patient-criterion
matching tasks demonstrate that the proposed framework can successfully
alleviate the predictions that tend to be biased.",None,-1
2a814aec-8664-4aee-96e6-072efc2670ed,Attention Schema in Neural Agents,0.0833177,3,"Attention has become a common ingredient in deep learning architectures. It
adds a dynamical selection of information on top of the static selection of
information supported by weights. In the same way, we can imagine a
higher-order informational filter built on top of attention: an Attention
Schema (AS), namely, a descriptive and predictive model of attention. In
cognitive neuroscience, Attention Schema Theory (AST) supports this idea of
distinguishing attention from AS. A strong prediction of this theory is that an
agent can use its own AS to also infer the states of other agents' attention
and consequently enhance coordination with other agents. As such, multi-agent
reinforcement learning would be an ideal setting to experimentally test the
validity of AST. We explore different ways in which attention and AS interact
with each other. Our preliminary results indicate that agents that implement
the AS as a recurrent internal control achieve the best performance. In
general, these exploratory experiments suggest that equipping artificial agents
with a model of attention can enhance their social intelligence.",None,-1
f1d418de-b10d-43c1-b7f5-cebda32a90ef,Exploring Speaker-Related Information in Spoken Language Understanding for Better Speaker Diarization,0.452244,3,"Speaker diarization(SD) is a classic task in speech processing and is crucial
in multi-party scenarios such as meetings and conversations. Current mainstream
speaker diarization approaches consider acoustic information only, which result
in performance degradation when encountering adverse acoustic conditions. In
this paper, we propose methods to extract speaker-related information from
semantic content in multi-party meetings, which, as we will show, can further
benefit speaker diarization. We introduce two sub-tasks, Dialogue Detection and
Speaker-Turn Detection, in which we effectively extract speaker information
from conversational semantics. We also propose a simple yet effective algorithm
to jointly model acoustic and semantic information and obtain
speaker-identified texts. Experiments on both AISHELL-4 and AliMeeting datasets
show that our method achieves consistent improvements over acoustic-only
speaker diarization systems.",None,-1
9b4351a1-14ff-46ec-b5e9-d65900b9bd0d,Modular Visual Question Answering via Code Generation,0.743643,26,"We present a framework that formulates visual question answering as modular
code generation. In contrast to prior work on modular approaches to VQA, our
approach requires no additional training and relies on pre-trained language
models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA
examples used for in-context learning. The generated Python programs invoke and
compose the outputs of the visual models using arithmetic and conditional
logic. Our approach improves accuracy on the COVR dataset by at least 3% and on
the GQA dataset by roughly 2% compared to the few-shot baseline that does not
employ code generation.",None,-1
c6dcad62-821b-4a72-9cad-7e5d29132145,Pulling Target to Source: A New Perspective on Domain Adaptive Semantic Segmentation,0.421473,4,"Domain adaptive semantic segmentation aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. However, existing methods
primarily focus on directly learning qualified target features, making it
challenging to guarantee their discrimination in the absence of target labels.
This work provides a new perspective. We observe that the features learned with
source data manage to keep categorically discriminative during training,
thereby enabling us to implicitly learn adequate target representations by
simply \textbf{pulling target features close to source features for each
category}. To this end, we propose T2S-DA, which we interpret as a form of
pulling Target to Source for Domain Adaptation, encouraging the model in
learning similar cross-domain features. Also, considering the pixel categories
are heavily imbalanced for segmentation datasets, we come up with a dynamic
re-weighting strategy to help the model concentrate on those underperforming
classes. Extensive experiments confirm that T2S-DA learns a more discriminative
and generalizable representation, significantly surpassing the
state-of-the-art. We further show that our method is quite qualified for the
domain generalization task, verifying its domain-invariant property.",None,-1
71a65d6b-22c5-42bf-a180-d62e919b62b1,Toward Unsupervised 3D Point Cloud Anomaly Detection using Variational Autoencoder,0.515316,14,"In this paper, we present an end-to-end unsupervised anomaly detection
framework for 3D point clouds. To the best of our knowledge, this is the first
work to tackle the anomaly detection task on a general object represented by a
3D point cloud. We propose a deep variational autoencoder-based unsupervised
anomaly detection network adapted to the 3D point cloud and an anomaly score
specifically for 3D point clouds. To verify the effectiveness of the model, we
conducted extensive experiments on the ShapeNet dataset. Through quantitative
and qualitative evaluation, we demonstrate that the proposed method outperforms
the baseline method. Our code is available at
https://github.com/llien30/point_cloud_anomaly_detection.",None,-1
f9b3fc5d-7444-47c2-b213-95641ffc8429,Level Generation Through Large Language Models,0.779704,30,"Large Language Models (LLMs) are powerful tools, capable of leveraging their
training on natural language to write stories, generate code, and answer
questions. But can they generate functional video game levels? Game levels,
with their complex functional constraints and spatial relationships in more
than one dimension, are very different from the kinds of data an LLM typically
sees during training. Datasets of game levels are also hard to come by,
potentially taxing the abilities of these data-hungry models. We investigate
the use of LLMs to generate levels for the game Sokoban, finding that LLMs are
indeed capable of doing so, and that their performance scales dramatically with
dataset size. We also perform preliminary experiments on controlling LLM level
generators and discuss promising areas for future work.",None,-1
31e81c8f-86b1-4d6b-8956-74986a688e7b,A Neural Span-Based Continual Named Entity Recognition Model,0.442146,3,"Named Entity Recognition (NER) models capable of Continual Learning (CL) are
realistically valuable in areas where entity types continuously increase (e.g.,
personal assistants). Meanwhile the learning paradigm of NER advances to new
patterns such as the span-based methods. However, its potential to CL has not
been fully explored. In this paper, we propose SpanKL, a simple yet effective
Span-based model with Knowledge distillation (KD) to preserve memories and
multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence
labeling approaches, the inherently independent modeling in span and entity
level with the designed coherent optimization on SpanKL promotes its learning
at each incremental step and mitigates the forgetting. Experiments on synthetic
CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly
outperforms previous SoTA in many aspects, and obtains the smallest gap from CL
to the upper bound revealing its high practiced value. The code is available at
https://github.com/Qznan/SpanKL.",None,-1
186913b0-c9e7-46ca-962b-3564f287b8dc,Towards Designing a ChatGPT Conversational Companion for Elderly People,0.783826,17,"Loneliness and social isolation are serious and widespread problems among
older people, affecting their physical and mental health, quality of life, and
longevity. In this paper, we propose a ChatGPT-based conversational companion
system for elderly people. The system is designed to provide companionship and
help reduce feelings of loneliness and social isolation. The system was
evaluated with a preliminary study. The results showed that the system was able
to generate responses that were relevant to the created elderly personas.
However, it is essential to acknowledge the limitations of ChatGPT, such as
potential biases and misinformation, and to consider the ethical implications
of using AI-based companionship for the elderly, including privacy concerns.",None,-1
2eb8945d-c0e5-43c2-b1da-be1bc315e135,Enhancing Model Performance in Multilingual Information Retrieval with Comprehensive Data Engineering Techniques,0.0873773,1,"In this paper, we present our solution to the Multilingual Information
Retrieval Across a Continuum of Languages (MIRACL) challenge of WSDM CUP
2023\footnote{https://project-miracl.github.io/}. Our solution focuses on
enhancing the ranking stage, where we fine-tune pre-trained multilingual
transformer-based models with MIRACL dataset. Our model improvement is mainly
achieved through diverse data engineering techniques, including the collection
of additional relevant training data, data augmentation, and negative sampling.
Our fine-tuned model effectively determines the semantic relevance between
queries and documents, resulting in a significant improvement in the efficiency
of the multilingual information retrieval process. Finally, Our team is pleased
to achieve remarkable results in this challenging competition, securing 2nd
place in the Surprise-Languages track with a score of 0.835 and 3rd place in
the Known-Languages track with an average nDCG@10 score of 0.716 across the 16
known languages on the final leaderboard.",None,-1
2865e255-f22c-4ddb-9171-a77f9c038645,How Many Demonstrations Do You Need for In-context Learning?,0.468882,20,"Large language models (LLMs) are capable to perform complex reasoning by
in-context learning (ICL) when provided with a few input-output demonstrations
(demos) and more powerful when intermediate reasoning steps (""chain of thoughts
(CoT)"") of the demos are given. Is it necessary to use multi-demo in ICL? In
this paper, we study ICL using fewer demos for each test query on the tasks
in~\cite{wei2022chain}. Surprisingly, we do not observe significant degradation
when using only one randomly chosen demo. To study this phenomenon, for each
test query, we categorize demos into ""correct demos"" leading to the correct
answer, and ""wrong demos"" resulting in wrong answers. Our analysis reveals an
inherent bias in those widely studied datasets: most demos are correct for a
majority of test queries, which explains the good performance of using one
random demo. Moreover, ICL (with and w/o CoT) using only one correct demo
significantly outperforms all-demo ICL adopted by most previous works,
indicating the weakness of LLMs in finding correct demo(s) for input queries,
which is difficult to evaluate on the biased datasets. Furthermore, we observe
a counterintuitive behavior of ICL using multi-demo, i.e., its accuracy
degrades(improves) when given more correct(wrong) demos. This implies that ICL
can be easily misguided by interference among demos and their spurious
correlations. Our analyses highlight several fundamental challenges that need
to be addressed in LLMs training, ICL, and benchmark design.",None,-1
53bf3d47-8d26-4f52-bff0-f7444c4bf783,HyperAttack: Multi-Gradient-Guided White-box Adversarial Structure Attack of Hypergraph Neural Networks,0.204705,3,"Hypergraph neural networks (HGNN) have shown superior performance in various
deep learning tasks, leveraging the high-order representation ability to
formulate complex correlations among data by connecting two or more nodes
through hyperedge modeling. Despite the well-studied adversarial attacks on
Graph Neural Networks (GNN), there is few study on adversarial attacks against
HGNN, which leads to a threat to the safety of HGNN applications. In this
paper, we introduce HyperAttack, the first white-box adversarial attack
framework against hypergraph neural networks. HyperAttack conducts a white-box
structure attack by perturbing hyperedge link status towards the target node
with the guidance of both gradients and integrated gradients. We evaluate
HyperAttack on the widely-used Cora and PubMed datasets and three hypergraph
neural networks with typical hypergraph modeling techniques. Compared to
state-of-the-art white-box structural attack methods for GNN, HyperAttack
achieves a 10-20X improvement in time efficiency while also increasing attack
success rates by 1.3%-3.7%. The results show that HyperAttack can achieve
efficient adversarial attacks that balance effectiveness and time costs.",None,-1
0ad9c322-b2fb-41f5-8e29-6fe85e70e7ab,Implicit neural representations for joint decomposition and registration of gene expression images in the marmoset brain,0.605263,2,"We propose a novel image registration method based on implicit neural
representations that addresses the challenging problem of registering a pair of
brain images with similar anatomical structures, but where one image contains
additional features or artifacts that are not present in the other image. To
demonstrate its effectiveness, we use 2D microscopy $\textit{in situ}$
hybridization gene expression images of the marmoset brain. Accurately
quantifying gene expression requires image registration to a brain template,
which is difficult due to the diversity of patterns causing variations in
visible anatomical brain structures. Our approach uses implicit networks in
combination with an image exclusion loss to jointly perform the registration
and decompose the image into a support and residual image. The support image
aligns well with the template, while the residual image captures individual
image characteristics that diverge from the template. In experiments, our
method provided excellent results and outperformed other registration
techniques.",None,-1
e212b3b9-d746-4c8f-8187-ab4fa40548ca,Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection,0.678621,3,"The spread of disinformation and propagandistic content poses a threat to
societal harmony, undermining informed decision-making and trust in reliable
sources. Online platforms often serve as breeding grounds for such content, and
malicious actors exploit the vulnerabilities of audiences to shape public
opinion. Although there have been research efforts aimed at the automatic
identification of disinformation and propaganda in social media content, there
remain challenges in terms of performance. The ArAIEval shared task aims to
further research on these particular issues within the context of the Arabic
language. In this paper, we discuss our participation in these shared tasks. We
competed in subtasks 1A and 2A, where our submitted system secured positions
9th and 10th, respectively. Our experiments consist of fine-tuning transformer
models and using zero- and few-shot learning with GPT-4.",None,-1
3a5a15ec-1833-46d9-a6e1-f384d933218b,CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation,0.502305,7,"Dialogue structure discovery is essential in dialogue generation.
Well-structured topic flow can leverage background information and predict
future topics to help generate controllable and explainable responses. However,
most previous work focused on dialogue structure learning in task-oriented
dialogue other than open-domain dialogue which is more complicated and
challenging. In this paper, we present a new framework CTRLStruct for dialogue
structure learning to effectively explore topic-level dialogue clusters as well
as their transitions with unlabelled information. Precisely, dialogue
utterances encoded by bi-directional Transformer are further trained through a
special designed contrastive learning task to improve representation. Then we
perform clustering to utterance-level representations and form topic-level
clusters that can be considered as vertices in dialogue structure graph. The
edges in the graph indicating transition probability between vertices are
calculated by mimicking expert behavior in datasets. Finally, dialogue
structure graph is integrated into dialogue model to perform controlled
response generation. Experiments on two popular open-domain dialogue datasets
show our model can generate more coherent responses compared to some excellent
dialogue models, as well as outperform some typical sentence embedding methods
in dialogue utterance representation. Code is available in GitHub.",None,-1
e230fdbd-c5d1-4249-8950-4627e352e304,Progressive Prompts: Continual Learning for Language Models,0.987635,64,"We introduce Progressive Prompts - a simple and efficient approach for
continual learning in language models. Our method allows forward transfer and
resists catastrophic forgetting, without relying on data replay or a large
number of task-specific parameters. Progressive Prompts learns a new soft
prompt for each task and sequentially concatenates it with the previously
learned prompts, while keeping the base model frozen. Experiments on standard
continual learning benchmarks show that our approach outperforms
state-of-the-art methods, with an improvement >20% in average test accuracy
over the previous best-preforming method on T5 model. We also explore a more
challenging continual learning setup with longer sequences of tasks and show
that Progressive Prompts significantly outperforms prior methods.",None,-1
7869f598-0ce5-4cc6-8d0f-c0cea9473c49,Can We Revitalize Interventional Healthcare with AI-XR Surgical Metaverses?,0.19339,3,"Recent advancements in technology, particularly in machine learning (ML),
deep learning (DL), and the metaverse, offer great potential for
revolutionizing surgical science. The combination of artificial intelligence
and extended reality (AI-XR) technologies has the potential to create a
surgical metaverse, a virtual environment where surgeries can be planned and
performed. This paper aims to provide insight into the various potential
applications of an AI-XR surgical metaverse and the challenges that must be
addressed to bring its full potential to fruition. It is important for the
community to focus on these challenges to fully realize the potential of the
AI-XR surgical metaverses. Furthermore, to emphasize the need for secure and
robust AI-XR surgical metaverses and to demonstrate the real-world implications
of security threats to the AI-XR surgical metaverses, we present a case study
in which the ``an immersive surgical attack'' on incision point localization is
performed in the context of preoperative planning in a surgical metaverse.",None,-1
40ba5653-8614-427a-a123-05016f6f33c1,Consciousness as a logically consistent and prognostic model of reality,0.254528,5,"The work demonstrates that brain might reflect the external world causal
relationships in the form of a logically consistent and prognostic model of
reality, which shows up as consciousness. The paper analyses and solves the
problem of statistical ambiguity and provides a formal model of causal
relationships as probabilistic maximally specific rules. We suppose that brain
makes all possible inferences from causal relationships. We prove that the
suggested formal model has a property of an unambiguous inference: from
consistent premises we infer a consistent conclusion. It enables a set of all
inferences to form a consistent model of the perceived world. Causal
relationships may create fixed points of cyclic inter-predictable properties.
We consider the ""natural"" classification introduced by John St. Mill and
demonstrate that a variety of fixed points of the objects' attributes forms a
""natural"" classification of the external world. Then we consider notions of
""natural"" categories and causal models of categories, introduced by Eleanor
Rosch and Bob Rehder and demonstrate that fixed points of causal relationships
between objects attributes, which we perceive, formalize these notions. If the
""natural"" classification describes the objects of the external world, and
""natural"" concepts the perception of these objects, then the theory of
integrated information, introduced by G. Tononi, describes the information
processes of the brain for ""natural"" concepts formation that reflects the
""natural"" classification. We argue that integrated information provides high
accuracy of the objects identification. A computer-based experiment is provided
that illustrates fixed points formation for coded digits.",None,-1
a0736e28-8a1d-4710-b570-c4ed49958147,Geometrically Consistent Partial Shape Matching,0.790775,3,"Finding correspondences between 3D shapes is a crucial problem in computer
vision and graphics, which is for example relevant for tasks like shape
interpolation, pose transfer, or texture transfer. An often neglected but
essential property of matchings is geometric consistency, which means that
neighboring triangles in one shape are consistently matched to neighboring
triangles in the other shape. Moreover, while in practice one often has only
access to partial observations of a 3D shape (e.g. due to occlusion, or
scanning artifacts), there do not exist any methods that directly address
geometrically consistent partial shape matching. In this work we fill this gap
by proposing to integrate state-of-the-art deep shape features into a novel
integer linear programming partial shape matching formulation. Our optimization
yields a globally optimal solution on low resolution shapes, which we then
refine using a coarse-to-fine scheme. We show that our method can find more
reliable results on partial shapes in comparison to existing geometrically
consistent algorithms (for which one first has to fill missing parts with a
dummy geometry). Moreover, our matchings are substantially smoother than
learning-based state-of-the-art shape matching methods.",None,-1
9f69b4fb-e496-4e06-ba02-4672d4899903,Learning Human-Compatible Representations for Case-Based Decision Support,0.514484,3,"Algorithmic case-based decision support provides examples to help human make
sense of predicted labels and aid human in decision-making tasks. Despite the
promising performance of supervised learning, representations learned by
supervised models may not align well with human intuitions: what models
consider as similar examples can be perceived as distinct by humans. As a
result, they have limited effectiveness in case-based decision support. In this
work, we incorporate ideas from metric learning with supervised learning to
examine the importance of alignment for effective decision support. In addition
to instance-level labels, we use human-provided triplet judgments to learn
human-compatible decision-focused representations. Using both synthetic data
and human subject experiments in multiple classification tasks, we demonstrate
that such representation is better aligned with human perception than
representation solely optimized for classification. Human-compatible
representations identify nearest neighbors that are perceived as more similar
by humans and allow humans to make more accurate predictions, leading to
substantial improvements in human decision accuracies (17.8% in butterfly vs.
moth classification and 13.2% in pneumonia classification).",None,-1
83816abd-4a22-477f-ad6d-562b61e8ae9b,Diffusion Models for Interferometric Satellite Aperture Radar,0.834891,2,"Probabilistic Diffusion Models (PDMs) have recently emerged as a very
promising class of generative models, achieving high performance in natural
image generation. However, their performance relative to non-natural images,
like radar-based satellite data, remains largely unknown. Generating large
amounts of synthetic (and especially labelled) satellite data is crucial to
implement deep-learning approaches for the processing and analysis of
(interferometric) satellite aperture radar data. Here, we leverage PDMs to
generate several radar-based satellite image datasets. We show that PDMs
succeed in generating images with complex and realistic structures, but that
sampling time remains an issue. Indeed, accelerated sampling strategies, which
work well on simple image datasets like MNIST, fail on our radar datasets. We
provide a simple and versatile open-source
https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample and
evaluate PDMs using any dataset on a single GPU.",None,-1
7ecc0782-66f7-497a-bd57-892102b24267,Follow-on Question Suggestion via Voice Hints for Voice Assistants,0.600456,2,"The adoption of voice assistants like Alexa or Siri has grown rapidly,
allowing users to instantly access information via voice search. Query
suggestion is a standard feature of screen-based search experiences, allowing
users to explore additional topics. However, this is not trivial to implement
in voice-based settings. To enable this, we tackle the novel task of suggesting
questions with compact and natural voice hints to allow users to ask follow-up
questions.
  We define the task, ground it in syntactic theory and outline linguistic
desiderata for spoken hints. We propose baselines and an approach using
sequence-to-sequence Transformers to generate spoken hints from a list of
questions. Using a new dataset of 6681 input questions and human written hints,
we evaluated the models with automatic metrics and human evaluation. Results
show that a naive approach of concatenating suggested questions creates poor
voice hints. Our approach, which applies a linguistically-motivated pretraining
task was strongly preferred by humans for producing the most natural hints.",None,-1
d8944a8e-59d8-4dbd-84cc-8b0d0d3867a8,How Many Answers Should I Give? An Empirical Study of Multi-Answer Reading Comprehension,0.410117,2,"The multi-answer phenomenon, where a question may have multiple answers
scattered in the document, can be well handled by humans but is challenging
enough for machine reading comprehension (MRC) systems. Despite recent progress
in multi-answer MRC, there lacks a systematic analysis of how this phenomenon
arises and how to better address it. In this work, we design a taxonomy to
categorize commonly-seen multi-answer MRC instances, with which we inspect
three multi-answer datasets and analyze where the multi-answer challenge comes
from. We further analyze how well different paradigms of current multi-answer
MRC models deal with different types of multi-answer instances. We find that
some paradigms capture well the key information in the questions while others
better model the relationship between questions and contexts. We thus explore
strategies to make the best of the strengths of different paradigms.
Experiments show that generation models can be a promising platform to
incorporate different paradigms. Our annotations and code are released for
further research.",None,-1
807a1fa7-a109-4cf7-9847-739c80319e1d,Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment,0.789691,10,"The egocentric and exocentric viewpoints of a human activity look
dramatically different, yet invariant representations to link them are
essential for many potential applications in robotics and augmented reality.
Prior work is limited to learning view-invariant features from paired
synchronized viewpoints. We relax that strong data assumption and propose to
learn fine-grained action features that are invariant to the viewpoints by
aligning egocentric and exocentric videos in time, even when not captured
simultaneously or in the same environment. To this end, we propose AE2, a
self-supervised embedding approach with two key designs: (1) an object-centric
encoder that explicitly focuses on regions corresponding to hands and active
objects; and (2) a contrastive-based alignment objective that leverages
temporally reversed frames as negative samples. For evaluation, we establish a
benchmark for fine-grained video understanding in the ego-exo context,
comprising four datasets -- including an ego tennis forehand dataset we
collected, along with dense per-frame labels we annotated for each dataset. On
the four datasets, our AE2 method strongly outperforms prior work in a variety
of fine-grained downstream tasks, both in regular and cross-view settings.",None,-1
5f6045bf-b108-4cf6-a182-0de13f7a037a,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,0.572134,15,"Current dataset collection methods typically scrape large amounts of data
from the web. While this technique is extremely scalable, data collected in
this way tends to reinforce stereotypical biases, can contain personally
identifiable information, and typically originates from Europe and North
America. In this work, we rethink the dataset collection paradigm and introduce
GeoDE, a geographically diverse dataset with 61,940 images from 40 classes and
6 world regions, and no personally identifiable information, collected through
crowd-sourcing. We analyse GeoDE to understand differences in images collected
in this manner compared to web-scraping. Despite the smaller size of this
dataset, we demonstrate its use as both an evaluation and training dataset,
highlight shortcomings in current models, as well as show improved performances
when even small amounts of GeoDE (1000 - 2000 images per region) are added to a
training dataset. We release the full dataset and code at
https://geodiverse-data-collection.cs.princeton.edu/",None,-1
83984d46-232b-4856-81d6-5387c9079106,Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models,0.559234,14,"Diffusion-based generative models have recently emerged as powerful solutions
for high-quality synthesis in multiple domains. Leveraging the bidirectional
Markov chains, diffusion probabilistic models generate samples by inferring the
reversed Markov chain based on the learned distribution mapping at the forward
diffusion process. In this work, we propose Modiff, a conditional paradigm that
benefits from the denoising diffusion probabilistic model (DDPM) to tackle the
problem of realistic and diverse action-conditioned 3D skeleton-based motion
generation. We are a pioneering attempt that uses DDPM to synthesize a variable
number of motion sequences conditioned on a categorical action. We evaluate our
approach on the large-scale NTU RGB+D dataset and show improvements over
state-of-the-art motion generation methods.",None,-1
723ee770-0d85-4168-b419-9fad1960e430,Attentive Graph Enhanced Region Representation Learning,0.523219,2,"Representing urban regions accurately and comprehensively is essential for
various urban planning and analysis tasks. Recently, with the expansion of the
city, modeling long-range spatial dependencies with multiple data sources plays
an important role in urban region representation. In this paper, we propose the
Attentive Graph Enhanced Region Representation Learning (ATGRL) model, which
aims to capture comprehensive dependencies from multiple graphs and learn rich
semantic representations of urban regions. Specifically, we propose a
graph-enhanced learning module to construct regional graphs by incorporating
mobility flow patterns, point of interests (POIs) functions, and check-in
semantics with noise filtering. Then, we present a multi-graph aggregation
module to capture both local and global spatial dependencies between regions by
integrating information from multiple graphs. In addition, we design a
dual-stage fusion module to facilitate information sharing between different
views and efficiently fuse multi-view representations for urban region
embedding using an improved linear attention mechanism. Finally, extensive
experiments on real-world datasets for three downstream tasks demonstrate the
superior performance of our model compared to state-of-the-art methods.",None,-1
62a929c9-5738-4a44-b113-efec88c27ff9,SPRING: Studying the Paper and Reasoning to Play Games,0.922154,11,"Open-world survival games pose significant challenges for AI algorithms due
to their multi-tasking, deep exploration, and goal prioritization requirements.
Despite reinforcement learning (RL) being popular for solving games, its high
sample complexity limits its effectiveness in complex open-world games like
Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's
original academic paper and use the knowledge learned to reason and play the
game through a large language model (LLM). Prompted with the LaTeX source as
game context and a description of the agent's current observation, our SPRING
framework employs a directed acyclic graph (DAG) with game-related questions as
nodes and dependencies as edges. We identify the optimal action to take in the
environment by traversing the DAG and calculating LLM responses for each node
in topological order, with the LLM's answer to final node directly translating
to environment actions. In our experiments, we study the quality of in-context
""reasoning"" induced by different forms of prompts under the setting of the
Crafter open-world environment. Our experiments suggest that LLMs, when
prompted with consistent chain-of-thought, have great potential in completing
sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4
outperforms all state-of-the-art RL baselines, trained for 1M steps, without
any training. Finally, we show the potential of games as a test bed for LLMs.",None,-1
8b2979a2-d946-4a24-811d-2e3d39909764,Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses,0.917285,7,"In the field of Japanese-Chinese translation linguistics, the issue of
correctly translating attributive clauses has persistently proven to be
challenging. Present-day machine translation tools often fail to accurately
translate attributive clauses from Japanese to Chinese. In light of this, this
paper investigates the linguistic problem underlying such difficulties, namely
how does the semantic role of the modified noun affect the selection of
translation patterns for attributive clauses, from a linguistic perspective. To
ad-dress these difficulties, a pre-edit scheme is proposed, which aims to
enhance the accuracy of translation. Furthermore, we propose a novel two-step
prompt strategy, which combines this pre-edit scheme with ChatGPT, currently
the most widely used large language model. This prompt strategy is capable of
optimizing translation input in zero-shot scenarios and has been demonstrated
to improve the average translation accuracy score by over 35%.",None,-1
1e65779c-4cab-4014-8dcc-28ae427c084b,Collaborative Blind Image Deblurring,0.127887,1,"Blurry images usually exhibit similar blur at various locations across the
image domain, a property barely captured in nowadays blind deblurring neural
networks. We show that when extracting patches of similar underlying blur is
possible, jointly processing the stack of patches yields superior accuracy than
handling them separately. Our collaborative scheme is implemented in a neural
architecture with a pooling layer on the stack dimension. We present three
practical patch extraction strategies for image sharpening, camera shake
removal and optical aberration correction, and validate the proposed approach
on both synthetic and real-world benchmarks. For each blur instance, the
proposed collaborative strategy yields significant quantitative and qualitative
improvements.",None,-1
476d7bf2-b3b3-4313-870e-9e5148a66066,Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources,0.195585,4,"To address the data scarcity issue in Conversational question answering
(ConvQA), a dialog inpainting method, which utilizes documents to generate
ConvQA datasets, has been proposed. However, the original dialog inpainting
model is trained solely on the dialog reconstruction task, resulting in the
generation of questions with low contextual relevance due to insufficient
learning of question-answer alignment. To overcome this limitation, we propose
a novel framework called Dialogizer, which has the capability to automatically
generate ConvQA datasets with high contextual relevance from textual sources.
The framework incorporates two training tasks: question-answer matching (QAM)
and topic-aware dialog generation (TDG). Moreover, re-ranking is conducted
during the inference phase based on the contextual relevance of the generated
questions. Using our framework, we produce four ConvQA datasets by utilizing
documents from multiple domains as the primary source. Through automatic
evaluation using diverse metrics, as well as human evaluation, we validate that
our proposed framework exhibits the ability to generate datasets of higher
quality compared to the baseline dialog inpainting model.",None,-1
6e2947dc-efaf-4041-b39d-e620749ce2f1,Improving Seq2Seq Grammatical Error Correction via Decoding Interventions,0.745858,4,"The sequence-to-sequence (Seq2Seq) approach has recently been widely used in
grammatical error correction (GEC) and shows promising performance. However,
the Seq2Seq GEC approach still suffers from two issues. First, a Seq2Seq GEC
model can only be trained on parallel data, which, in GEC task, is often noisy
and limited in quantity. Second, the decoder of a Seq2Seq GEC model lacks an
explicit awareness of the correctness of the token being generated. In this
paper, we propose a unified decoding intervention framework that employs an
external critic to assess the appropriateness of the token to be generated
incrementally, and then dynamically influence the choice of the next token. We
discover and investigate two types of critics: a pre-trained left-to-right
language model critic and an incremental target-side grammatical error detector
critic. Through extensive experiments on English and Chinese datasets, our
framework consistently outperforms strong baselines and achieves results
competitive with state-of-the-art methods.",None,-1
e497f8c6-91cb-499f-ae85-1411e3d4f75e,Let's Reinforce Step by Step,0.232998,5,"While recent advances have boosted LM proficiency in linguistic benchmarks,
LMs consistently struggle to reason correctly on complex tasks like
mathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as a
method with which to shape model reasoning processes. In particular, we explore
two reward schemes, outcome-supervised reward models (ORMs) and
process-supervised reward models (PRMs), to optimize for logical reasoning. Our
results show that the fine-grained reward provided by PRM-based methods
enhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly,
reducing performance in complex tasks (MATH). Furthermore, we show the critical
role reward aggregation functions play in model performance. Providing
promising avenues for future research, our study underscores the need for
further exploration into fine-grained reward modeling for more reliable
language models.",None,-1
6a437abc-4311-470f-b9d7-131d0f19ebc5,Counting Crowds in Bad Weather,0.683705,8,"Crowd counting has recently attracted significant attention in the field of
computer vision due to its wide applications to image understanding. Numerous
methods have been proposed and achieved state-of-the-art performance for
real-world tasks. However, existing approaches do not perform well under
adverse weather such as haze, rain, and snow since the visual appearances of
crowds in such scenes are drastically different from those images in clear
weather of typical datasets. In this paper, we propose a method for robust
crowd counting in adverse weather scenarios. Instead of using a two-stage
approach that involves image restoration and crowd counting modules, our model
learns effective features and adaptive queries to account for large appearance
variations. With these weather queries, the proposed model can learn the
weather information according to the degradation of the input image and
optimize with the crowd counting module simultaneously. Experimental results
show that the proposed algorithm is effective in counting crowds under
different weather types on benchmark datasets. The source code and trained
models will be made available to the public.",None,-1
d0d0a108-29b7-4224-bac5-b90bb9a92866,Why Does ChatGPT Fall Short in Providing Truthful Answers?,0.598812,40,"Recent advancements in large language models, such as ChatGPT, have
demonstrated significant potential to impact various aspects of human life.
However, ChatGPT still faces challenges in providing reliable and accurate
answers to user questions. To better understand the model's particular
weaknesses in providing truthful answers, we embark an in-depth exploration of
open-domain question answering. Specifically, we undertake a detailed
examination of ChatGPT's failures, categorized into: comprehension, factuality,
specificity, and inference. We further pinpoint factuality as the most
contributing failure and identify two critical abilities associated with
factuality: knowledge memorization and knowledge recall. Through experiments
focusing on factuality, we propose several potential enhancement strategies.
Our findings suggest that augmenting the model with granular external knowledge
and cues for knowledge recall can enhance the model's factuality in answering
questions.",None,-1
11bb9573-fac6-41c5-9c4f-99014ac61653,Argumentation Element Annotation Modeling using XLNet,0.773344,2,"This study demonstrates the effectiveness of XLNet, a transformer-based
language model, for annotating argumentative elements in persuasive essays.
XLNet's architecture incorporates a recurrent mechanism that allows it to model
long-term dependencies in lengthy texts. Fine-tuned XLNet models were applied
to three datasets annotated with different schemes - a proprietary dataset
using the Annotations for Revisions and Reflections on Writing (ARROW) scheme,
the PERSUADE corpus, and the Argument Annotated Essays (AAE) dataset. The XLNet
models achieved strong performance across all datasets, even surpassing human
agreement levels in some cases. This shows XLNet capably handles diverse
annotation schemes and lengthy essays. Comparisons between the model outputs on
different datasets also revealed insights into the relationships between the
annotation tags. Overall, XLNet's strong performance on modeling argumentative
structures across diverse datasets highlights its suitability for providing
automated feedback on essay organization.",None,-1
7663c54c-d45a-4ca4-b784-d9a121349cb3,Joint Skeletal and Semantic Embedding Loss for Micro-gesture Classification,0.649246,2,"In this paper, we briefly introduce the solution of our team HFUT-VUT for the
Micros-gesture Classification in the MiGA challenge at IJCAI 2023. The
micro-gesture classification task aims at recognizing the action category of a
given video based on the skeleton data. For this task, we propose a
3D-CNNs-based micro-gesture recognition network, which incorporates a skeletal
and semantic embedding loss to improve action classification performance.
Finally, we rank 1st in the Micro-gesture Classification Challenge, surpassing
the second-place team in terms of Top-1 accuracy by 1.10%.",None,-1
3da4c02a-9cd6-433d-a610-4d040e900c6f,Diff-Privacy: Diffusion-based Face Privacy Protection,0.554069,4,"Privacy protection has become a top priority as the proliferation of AI
techniques has led to widespread collection and misuse of personal data.
Anonymization and visual identity information hiding are two important facial
privacy protection tasks that aim to remove identification characteristics from
facial images at the human perception level. However, they have a significant
difference in that the former aims to prevent the machine from recognizing
correctly, while the latter needs to ensure the accuracy of machine
recognition. Therefore, it is difficult to train a model to complete these two
tasks simultaneously. In this paper, we unify the task of anonymization and
visual identity information hiding and propose a novel face privacy protection
method based on diffusion models, dubbed Diff-Privacy. Specifically, we train
our proposed multi-scale image inversion module (MSI) to obtain a set of SDM
format conditional embeddings of the original image. Based on the conditional
embeddings, we design corresponding embedding scheduling strategies and
construct different energy functions during the denoising process to achieve
anonymization and visual identity information hiding. Extensive experiments
have been conducted to validate the effectiveness of our proposed framework in
protecting facial privacy.",None,-1
fa50b24d-eda2-4ec2-b202-102f360f256c,A Unified Conditional Framework for Diffusion-based Image Restoration,0.726608,10,"Diffusion Probabilistic Models (DPMs) have recently shown remarkable
performance in image generation tasks, which are capable of generating highly
realistic images. When adopting DPMs for image restoration tasks, the crucial
aspect lies in how to integrate the conditional information to guide the DPMs
to generate accurate and natural output, which has been largely overlooked in
existing works. In this paper, we present a unified conditional framework based
on diffusion models for image restoration. We leverage a lightweight UNet to
predict initial guidance and the diffusion model to learn the residual of the
guidance. By carefully designing the basic module and integration module for
the diffusion model block, we integrate the guidance and other auxiliary
conditional information into every block of the diffusion model to achieve
spatially-adaptive generation conditioning. To handle high-resolution images,
we propose a simple yet effective inter-step patch-splitting strategy to
produce arbitrary-resolution images without grid artifacts. We evaluate our
conditional framework on three challenging tasks: extreme low-light denoising,
deblurring, and JPEG restoration, demonstrating its significant improvements in
perceptual quality and the generalization to restoration tasks.",None,-1
89926a8e-d9b1-46f4-9eec-875aa72fe099,Principal-Agent Reward Shaping in MDPs,0.342994,5,"Principal-agent problems arise when one party acts on behalf of another,
leading to conflicts of interest. The economic literature has extensively
studied principal-agent problems, and recent work has extended this to more
complex scenarios such as Markov Decision Processes (MDPs). In this paper, we
further explore this line of research by investigating how reward shaping under
budget constraints can improve the principal's utility. We study a two-player
Stackelberg game where the principal and the agent have different reward
functions, and the agent chooses an MDP policy for both players. The principal
offers an additional reward to the agent, and the agent picks their policy
selfishly to maximize their reward, which is the sum of the original and the
offered reward. Our results establish the NP-hardness of the problem and offer
polynomial approximation algorithms for two classes of instances: Stochastic
trees and deterministic decision processes with a finite horizon.",None,-1
5f2ab749-e778-4ad8-99df-cba8d163a145,Assessing the Impact of Noise on Quantum Neural Networks: An Experimental Analysis,0.401511,2,"In the race towards quantum computing, the potential benefits of quantum
neural networks (QNNs) have become increasingly apparent. However, Noisy
Intermediate-Scale Quantum (NISQ) processors are prone to errors, which poses a
significant challenge for the execution of complex algorithms or quantum
machine learning. To ensure the quality and security of QNNs, it is crucial to
explore the impact of noise on their performance. This paper provides a
comprehensive analysis of the impact of noise on QNNs, examining the Mottonen
state preparation algorithm under various noise models and studying the
degradation of quantum states as they pass through multiple layers of QNNs.
Additionally, the paper evaluates the effect of noise on the performance of
pre-trained QNNs and highlights the challenges posed by noise models in quantum
computing. The findings of this study have significant implications for the
development of quantum software, emphasizing the importance of prioritizing
stability and noise-correction measures when developing QNNs to ensure reliable
and trustworthy results. This paper contributes to the growing body of
literature on quantum computing and quantum machine learning, providing new
insights into the impact of noise on QNNs and paving the way towards the
development of more robust and efficient quantum algorithms.",None,-1
3dddbbf5-dc1a-4b91-b7ff-2aeb6cd2ea8e,ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base,0.154417,8,"Analogical reasoning is a fundamental cognitive ability of humans. However,
current language models (LMs) still struggle to achieve human-like performance
in analogical reasoning tasks due to a lack of resources for model training. In
this work, we address this gap by proposing ANALOGYKB, a million-scale analogy
knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB
identifies two types of analogies from the KGs: 1) analogies of the same
relations, which can be directly extracted from the KGs, and 2) analogies of
analogous relations, which are identified with a selection and filtering
pipeline enabled by large language models (LLMs), followed by minor human
efforts for data quality control. Evaluations on a series of datasets of two
analogical reasoning tasks (analogy recognition and generation) demonstrate
that ANALOGYKB successfully enables both smaller LMs and LLMs to gain better
analogical reasoning capabilities.",None,-1
b01e3b60-9044-46ce-b570-717d82d9102c,Application-Agnostic Language Modeling for On-Device ASR,0.639628,2,"On-device automatic speech recognition systems face several challenges
compared to server-based systems. They have to meet stricter constraints in
terms of speed, disk size and memory while maintaining the same accuracy. Often
they have to serve several applications with different distributions at once,
such as communicating with a virtual assistant and speech-to-text. The simplest
solution to serve multiple applications is to build application-specific
(language) models, but this leads to an increase in memory. Therefore, we
explore different data- and architecture-driven language modeling approaches to
build a single application-agnostic model. We propose two novel feed-forward
architectures that find an optimal trade off between different on-device
constraints. In comparison to the application-specific solution, one of our
novel approaches reduces the disk size by half, while maintaining speed and
accuracy of the original model.",None,-1
e4b78444-9f90-4fe6-9ed0-f4d7f3a886cc,Privacy Aware Question-Answering System for Online Mental Health Risk Assessment,0.303483,1,"Social media platforms have enabled individuals suffering from mental
illnesses to share their lived experiences and find the online support
necessary to cope. However, many users fail to receive genuine clinical
support, thus exacerbating their symptoms. Screening users based on what they
post online can aid providers in administering targeted healthcare and minimize
false positives. Pre-trained Language Models (LMs) can assess users' social
media data and classify them in terms of their mental health risk. We propose a
Question-Answering (QA) approach to assess mental health risk using the
Unified-QA model on two large mental health datasets. To protect user data, we
extend Unified-QA by anonymizing the model training process using differential
privacy. Our results demonstrate the effectiveness of modeling risk assessment
as a QA task, specifically for mental health use cases. Furthermore, the
model's performance decreases by less than 1% with the inclusion of
differential privacy. The proposed system's performance is indicative of a
promising research direction that will lead to the development of privacy-aware
diagnostic systems.",None,-1
07bfcb2e-bfc5-436e-a525-8b693b8f28b4,Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction,0.975747,24,"We present Mask-then-Fill, a flexible and effective data augmentation
framework for event extraction. Our approach allows for more flexible
manipulation of text and thus can generate more diverse data while keeping the
original event structure unchanged as much as possible. Specifically, it first
randomly masks out an adjunct sentence fragment and then infills a
variable-length text span with a fine-tuned infilling model. The main advantage
lies in that it can replace a fragment of arbitrary length in the text with
another fragment of variable length, compared to the existing methods which can
only replace a single word or a fixed-length fragment. On trigger and argument
extraction tasks, the proposed framework is more effective than baseline
methods and it demonstrates particularly strong results in the low-resource
setting. Our further analysis shows that it achieves a good balance between
diversity and distributional similarity.",None,-1
a0c43641-5ca8-4f58-887f-858e0a13a0c2,Unsupervised Inference of Signed Distance Functions from Single Sparse Point Clouds without Learning Priors,0.66934,13,"It is vital to infer signed distance functions (SDFs) from 3D point clouds.
The latest methods rely on generalizing the priors learned from large scale
supervision. However, the learned priors do not generalize well to various
geometric variations that are unseen during training, especially for extremely
sparse point clouds. To resolve this issue, we present a neural network to
directly infer SDFs from single sparse point clouds without using signed
distance supervision, learned priors or even normals. Our insight here is to
learn surface parameterization and SDFs inference in an end-to-end manner. To
make up the sparsity, we leverage parameterized surfaces as a coarse surface
sampler to provide many coarse surface estimations in training iterations,
according to which we mine supervision and our thin plate splines (TPS) based
network infers SDFs as smooth functions in a statistical way. Our method
significantly improves the generalization ability and accuracy in unseen point
clouds. Our experimental results show our advantages over the state-of-the-art
methods in surface reconstruction for sparse point clouds under synthetic
datasets and real scans.The code is available at
\url{https://github.com/chenchao15/NeuralTPS}.",None,-1
8c3c328e-b233-4e4f-aa6c-5e9dbc33a17b,Refined Pseudo labeling for Source-free Domain Adaptive Object Detection,0.245094,3,"Domain adaptive object detection (DAOD) assumes that both labeled source data
and unlabeled target data are available for training, but this assumption does
not always hold in real-world scenarios. Thus, source-free DAOD is proposed to
adapt the source-trained detectors to target domains with only unlabeled target
data. Existing source-free DAOD methods typically utilize pseudo labeling,
where the performance heavily relies on the selection of confidence threshold.
However, most prior works adopt a single fixed threshold for all classes to
generate pseudo labels, which ignore the imbalanced class distribution,
resulting in biased pseudo labels. In this work, we propose a refined pseudo
labeling framework for source-free DAOD. First, to generate unbiased pseudo
labels, we present a category-aware adaptive threshold estimation module, which
adaptively provides the appropriate threshold for each category. Second, to
alleviate incorrect box regression, a localization-aware pseudo label
assignment strategy is introduced to divide labels into certain and uncertain
ones and optimize them separately. Finally, extensive experiments on four
adaptation tasks demonstrate the effectiveness of our method.",None,-1
0c1a7a55-fe93-4a37-9ff6-414529c35c7f,Understanding Counterspeech for Online Harm Mitigation,0.684579,6,"Counterspeech offers direct rebuttals to hateful speech by challenging
perpetrators of hate and showing support to targets of abuse. It provides a
promising alternative to more contentious measures, such as content moderation
and deplatforming, by contributing a greater amount of positive online speech
rather than attempting to mitigate harmful content through removal. Advances in
the development of large language models mean that the process of producing
counterspeech could be made more efficient by automating its generation, which
would enable large-scale online campaigns. However, we currently lack a
systematic understanding of several important factors relating to the efficacy
of counterspeech for hate mitigation, such as which types of counterspeech are
most effective, what are the optimal conditions for implementation, and which
specific effects of hate it can best ameliorate. This paper aims to fill this
gap by systematically reviewing counterspeech research in the social sciences
and comparing methodologies and findings with computer science efforts in
automatic counterspeech generation. By taking this multi-disciplinary view, we
identify promising future directions in both fields.",None,-1
f04bc4b9-4655-4f87-b712-9d9339c30276,Stopping Criteria for Value Iteration on Stochastic Games with Quantitative Objectives,0.247332,2,"A classic solution technique for Markov decision processes (MDP) and
stochastic games (SG) is value iteration (VI). Due to its good practical
performance, this approximative approach is typically preferred over exact
techniques, even though no practical bounds on the imprecision of the result
could be given until recently. As a consequence, even the most used model
checkers could return arbitrarily wrong results. Over the past decade,
different works derived stopping criteria, indicating when the precision
reaches the desired level, for various settings, in particular MDP with
reachability, total reward, and mean payoff, and SG with reachability.
  In this paper, we provide the first stopping criteria for VI on SG with total
reward and mean payoff, yielding the first anytime algorithms in these
settings. To this end, we provide the solution in two flavours: First through a
reduction to the MDP case and second directly on SG. The former is simpler and
automatically utilizes any advances on MDP. The latter allows for more local
computations, heading towards better practical efficiency.
  Our solution unifies the previously mentioned approaches for MDP and SG and
their underlying ideas. To achieve this, we isolate objective-specific
subroutines as well as identify objective-independent concepts. These
structural concepts, while surprisingly simple, form the very essence of the
unified solution.",None,-1
3ad61236-4360-4b7b-8056-ed9c48b7464e,WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models,0.703456,9,"This paper describes our submission to the MEDIQA-Chat 2023 shared task for
automatic clinical note generation from doctor-patient conversations. We report
results for two approaches: the first fine-tunes a pre-trained language model
(PLM) on the shared task data, and the second uses few-shot in-context learning
(ICL) with a large language model (LLM). Both achieve high performance as
measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and
first, respectively, of all submissions to the shared task. Expert human
scrutiny indicates that notes generated via the ICL-based approach with GPT-4
are preferred about as often as human-written notes, making it a promising path
toward automated note generation from doctor-patient conversations.",None,-1
c7b9985f-ae8c-4fa3-a2f8-40257a98d3ec,Training-Free Neural Matte Extraction for Visual Effects,0.0765659,1,"Alpha matting is widely used in video conferencing as well as in movies,
television, and social media sites. Deep learning approaches to the matte
extraction problem are well suited to video conferencing due to the consistent
subject matter (front-facing humans), however training-based approaches are
somewhat pointless for entertainment videos where varied subjects (spaceships,
monsters, etc.) may appear only a few times in a single movie -- if a method of
creating ground truth for training exists, just use that method to produce the
desired mattes. We introduce a training-free high quality neural matte
extraction approach that specifically targets the assumptions of visual effects
production. Our approach is based on the deep image prior, which optimizes a
deep neural network to fit a single image, thereby providing a deep encoding of
the particular image. We make use of the representations in the penultimate
layer to interpolate coarse and incomplete ""trimap"" constraints. Videos
processed with this approach are temporally consistent. The algorithm is both
very simple and surprisingly effective.",None,-1
9fb70fdf-4d92-4252-8a88-737a5d21ac9c,BodySLAM++: Fast and Tightly-Coupled Visual-Inertial Camera and Human Motion Tracking,0.395527,2,"Robust, fast, and accurate human state - 6D pose and posture - estimation
remains a challenging problem. For real-world applications, the ability to
estimate the human state in real-time is highly desirable. In this paper, we
present BodySLAM++, a fast, efficient, and accurate human and camera state
estimation framework relying on visual-inertial data. BodySLAM++ extends an
existing visual-inertial state estimation framework, OKVIS2, to solve the dual
task of estimating camera and human states simultaneously. Our system improves
the accuracy of both human and camera state estimation with respect to baseline
methods by 26% and 12%, respectively, and achieves real-time performance at 15+
frames per second on an Intel i7-model CPU. Experiments were conducted on a
custom dataset containing both ground truth human and camera poses collected
with an indoor motion tracking system.",None,-1
51af7c13-d1fd-4e74-b92a-d4c198e813b6,Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling,0.417397,3,"Convolutional neural networks encode images through a sequence of
convolutions, normalizations and non-linearities as well as downsampling
operations into potentially strong semantic embeddings. Yet, previous work
showed that even slight mistakes during sampling, leading to aliasing, can be
directly attributed to the networks' lack in robustness. To address such issues
and facilitate simpler and faster adversarial training, [12] recently proposed
FLC pooling, a method for provably alias-free downsampling - in theory. In this
work, we conduct a further analysis through the lens of signal processing and
find that such current pooling methods, which address aliasing in the frequency
domain, are still prone to spectral leakage artifacts. Hence, we propose
aliasing and spectral artifact-free pooling, short ASAP. While only introducing
a few modifications to FLC pooling, networks using ASAP as downsampling method
exhibit higher native robustness against common corruptions, a property that
FLC pooling was missing. ASAP also increases native robustness against
adversarial attacks on high and low resolution data while maintaining similar
clean accuracy or even outperforming the baseline.",None,-1
0ae98e27-03ad-471e-b27d-2bc5030d4b22,A Schedule of Duties in the Cloud Space Using a Modified Salp Swarm Algorithm,0.223362,2,"Cloud computing is a concept introduced in the information technology era,
with the main components being the grid, distributed, and valuable computing.
The cloud is being developed continuously and, naturally, comes up with many
challenges, one of which is scheduling. A schedule or timeline is a mechanism
used to optimize the time for performing a duty or set of duties. A scheduling
process is accountable for choosing the best resources for performing a duty.
The main goal of a scheduling algorithm is to improve the efficiency and
quality of the service while at the same time ensuring the acceptability and
effectiveness of the targets. The task scheduling problem is one of the most
important NP-hard issues in the cloud domain and, so far, many techniques have
been proposed as solutions, including using genetic algorithms (GAs), particle
swarm optimization, (PSO), and ant colony optimization (ACO). To address this
problem, in this paper, one of the collective intelligence algorithms, called
the Salp Swarm Algorithm (SSA), has been expanded, improved, and applied. The
performance of the proposed algorithm has been compared with that of GAs, PSO,
continuous ACO, and the basic SSA. The results show that our algorithm has
generally higher performance than the other algorithms. For example, compared
to the basic SSA, the proposed method has an average reduction of approximately
21% in makespan.",None,-1
5c9b235a-166a-475f-b618-91bc305cf382,Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment,0.913031,15,"The multi-modal entity alignment (MMEA) aims to find all equivalent entity
pairs between multi-modal knowledge graphs (MMKGs). Rich attributes and
neighboring entities are valuable for the alignment task, but existing works
ignore contextual gap problems that the aligned entities have different numbers
of attributes on specific modality when learning entity representations. In
this paper, we propose a novel attribute-consistent knowledge graph
representation learning framework for MMEA (ACK-MMEA) to compensate the
contextual gaps through incorporating consistent alignment knowledge.
Attribute-consistent KGs (ACKGs) are first constructed via multi-modal
attribute uniformization with merge and generate operators so that each entity
has one and only one uniform feature in each modality. The ACKGs are then fed
into a relation-aware graph neural network with random dropouts, to obtain
aggregated relation representations and robust entity representations. In order
to evaluate the ACK-MMEA facilitated for entity alignment, we specially design
a joint alignment loss for both entity and attribute evaluation. Extensive
experiments conducted on two benchmark datasets show that our approach achieves
excellent performance compared to its competitors.",None,-1
9ff2aafb-4bbd-4f07-8481-372654a272e4,Interpretable Stereotype Identification through Reasoning,0.0782608,1,"Given that language models are trained on vast datasets that may contain
inherent biases, there is a potential danger of inadvertently perpetuating
systemic discrimination. Consequently, it becomes essential to examine and
address biases in language models, integrating fairness into their development
to ensure these models are equitable and free from bias. In this work, we
demonstrate the importance of reasoning in zero-shot stereotype identification
based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from
13B to 33B, we show that the performance gain from reasoning significantly
exceeds the gain from scaling up. Our findings suggest that reasoning could be
a key factor that enables LLMs to trescend the scaling law on out-of-domain
tasks such as stereotype identification. Additionally, through a qualitative
analysis of select reasoning traces, we highlight how reasoning enhances not
just accuracy but also the interpretability of the decision.",None,-1
ffa194c8-7c3b-4962-b820-e8ef4ba2f662,Synthcity: facilitating innovative use cases of synthetic data in different data modalities,0.796039,34,"Synthcity is an open-source software package for innovative use cases of
synthetic data in ML fairness, privacy and augmentation across diverse tabular
data modalities, including static data, regular and irregular time series, data
with censoring, multi-source data, composite data, and more. Synthcity provides
the practitioners with a single access point to cutting edge research and tools
in synthetic data. It also offers the community a playground for rapid
experimentation and prototyping, a one-stop-shop for SOTA benchmarks, and an
opportunity for extending research impact. The library can be accessed on
GitHub (https://github.com/vanderschaarlab/synthcity) and pip
(https://pypi.org/project/synthcity/). We warmly invite the community to join
the development effort by providing feedback, reporting bugs, and contributing
code.",None,-1
4f8a90f6-5e2e-4b60-aa0b-d31d8e7e7751,Nonverbal Cues in Human-Robot Interaction: A Communication Studies Perspective,0.905412,21,"Communication between people is characterized by a broad range of nonverbal
cues. Transferring these cues into the design of robots and other artificial
agents that interact with people may foster more natural, inviting, and
accessible experiences. In this position paper, we offer a series of definitive
nonverbal codes for human-robot interaction (HRI) that address the five human
sensory systems (visual, auditory, haptic, olfactory, gustatory) drawn from the
field of communication studies. We discuss how these codes can be translated
into design patterns for HRI using a curated sample of the communication
studies and HRI literatures. As nonverbal codes are an essential mode in human
communication, we argue that integrating robotic nonverbal codes in HRI will
afford robots a feeling of ""aliveness"" or ""social agency"" that would otherwise
be missing. We end with suggestions for research directions to stimulate work
on nonverbal communication within the field of HRI and improve communication
between human and robots.",None,-1
f4e412cd-1012-46bf-a667-c989d1a2235a,From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning,0.0332029,1,"Pre-trained language models (PLMs) have shown impressive performance in
various language tasks. However, they are prone to spurious correlations, and
often generate illusory information. In real-world applications, PLMs should
justify decisions with formalized, coherent reasoning chains, but this
challenge remains under-explored. Cognitive psychology theorizes that humans
are capable of utilizing fast and intuitive heuristic thinking to make
decisions based on past experience, then rationalizing the decisions through
slower and deliberative analytic reasoning. We incorporate these interlinked
dual processes in fine-tuning and in-context learning with PLMs, applying them
to two language understanding tasks that require coherent physical commonsense
reasoning. We show that our proposed Heuristic-Analytic Reasoning (HAR)
strategies drastically improve the coherence of rationalizations for model
decisions, yielding state-of-the-art results on Tiered Reasoning for Intuitive
Physics (TRIP). We also find that this improved coherence is a direct result of
more faithful attention to relevant language context in each step of reasoning.
Our findings suggest that human-like reasoning strategies can effectively
improve the coherence and reliability of PLM reasoning.",None,-1
38872345-e49c-4b49-a387-395a6107d255,HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness,0.130356,3,"Convolutional Neural Networks (CNN) are known to exhibit poor generalization
performance under distribution shifts. Their generalization have been studied
extensively, and one line of work approaches the problem from a
frequency-centric perspective. These studies highlight the fact that humans and
CNNs might focus on different frequency components of an image. First, inspired
by these observations, we propose a simple yet effective data augmentation
method HybridAugment that reduces the reliance of CNNs on high-frequency
components, and thus improves their robustness while keeping their clean
accuracy high. Second, we propose HybridAugment++, which is a hierarchical
augmentation method that attempts to unify various frequency-spectrum
augmentations. HybridAugment++ builds on HybridAugment, and also reduces the
reliance of CNNs on the amplitude component of images, and promotes phase
information instead. This unification results in competitive to or better than
state-of-the-art results on clean accuracy (CIFAR-10/100 and ImageNet),
corruption benchmarks (ImageNet-C, CIFAR-10-C and CIFAR-100-C), adversarial
robustness on CIFAR-10 and out-of-distribution detection on various datasets.
HybridAugment and HybridAugment++ are implemented in a few lines of code, does
not require extra data, ensemble models or additional networks.",None,-1
c6b930e8-186f-4698-bb87-4030c23e2a33,CGMI: Configurable General Multi-Agent Interaction Framework,0.94061,13,"Benefiting from the powerful capabilities of large language models (LLMs),
agents based on LLMs have shown the potential to address domain-specific tasks
and emulate human behaviors. However, the content generated by these agents
remains somewhat superficial, owing to their limited domain expertise and the
absence of an effective cognitive architecture. To address this, we present the
Configurable General Multi-Agent Interaction (CGMI) framework, designed to
replicate human interactions in real-world scenarios. Specifically, we propose
a tree-structured methodology for the assignment, detection, and maintenance of
agent personality. Additionally, we designed a cognitive architecture equipped
with a skill library based on the ACT* model, which contains memory,
reflection, and planning modules. We have also integrated general agents to
augment the virtual environment's realism. Using the CGMI framework, we
simulated numerous classroom interactions between teacher and students. The
experiments indicate that aspects such as the teaching methodology, curriculum,
and student performance closely mirror real classroom settings. We will open
source our work.",None,-1
128ffa68-9a33-47e2-bb20-3da3a4b89061,Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task,0.160954,3,"This paper describes and analyzes our participation in the 2023 Eval4NLP
shared task, which focuses on assessing the effectiveness of prompt-based
techniques to empower Large Language Models to handle the task of quality
estimation, particularly in the context of evaluating machine translations and
summaries. We conducted systematic experiments with various prompting
techniques, including standard prompting, prompts informed by annotator
instructions, and innovative chain-of-thought prompting. In addition, we
integrated these approaches with zero-shot and one-shot learning methods to
maximize the efficacy of our evaluation procedures. Our work reveals that
combining these approaches using a ""small"", open source model (orca_mini_v3_7B)
yields competitive results.",None,-1
094c0ea8-8cb3-4cca-937d-2b870d5d2815,Continual Learning with Scaled Gradient Projection,0.428807,8,"In neural networks, continual learning results in gradient interference among
sequential tasks, leading to catastrophic forgetting of old tasks while
learning new ones. This issue is addressed in recent methods by storing the
important gradient spaces for old tasks and updating the model orthogonally
during new tasks. However, such restrictive orthogonal gradient updates hamper
the learning capability of the new tasks resulting in sub-optimal performance.
To improve new learning while minimizing forgetting, in this paper we propose a
Scaled Gradient Projection (SGP) method, where we combine the orthogonal
gradient projections with scaled gradient steps along the important gradient
spaces for the past tasks. The degree of gradient scaling along these spaces
depends on the importance of the bases spanning them. We propose an efficient
method for computing and accumulating importance of these bases using the
singular value decomposition of the input representations for each task. We
conduct extensive experiments ranging from continual image classification to
reinforcement learning tasks and report better performance with less training
overhead than the state-of-the-art approaches.",None,-1
3663ef41-b4a6-4fed-8059-813c512433d0,Traj-MAE: Masked Autoencoders for Trajectory Prediction,0.864259,16,"Trajectory prediction has been a crucial task in building a reliable
autonomous driving system by anticipating possible dangers. One key issue is to
generate consistent trajectory predictions without colliding. To overcome the
challenge, we propose an efficient masked autoencoder for trajectory prediction
(Traj-MAE) that better represents the complicated behaviors of agents in the
driving environment. Specifically, our Traj-MAE employs diverse masking
strategies to pre-train the trajectory encoder and map encoder, allowing for
the capture of social and temporal information among agents while leveraging
the effect of environment from multiple granularities. To address the
catastrophic forgetting problem that arises when pre-training the network with
multiple masking strategies, we introduce a continual pre-training framework,
which can help Traj-MAE learn valuable and diverse information from various
strategies efficiently. Our experimental results in both multi-agent and
single-agent settings demonstrate that Traj-MAE achieves competitive results
with state-of-the-art methods and significantly outperforms our baseline model.",None,-1
1a5c1c12-28b1-4d4d-9827-1fe7bf527aa0,"Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)",0.999358,38,"Large language models, particularly GPT-3, are able to produce high quality
summaries of general domain news articles in few- and zero-shot settings.
However, it is unclear if such models are similarly capable in more
specialized, high-stakes domains such as biomedicine. In this paper, we enlist
domain experts (individuals with medical training) to evaluate summaries of
biomedical articles generated by GPT-3, given zero supervision. We consider
both single- and multi-document settings. In the former, GPT-3 is tasked with
generating regular and plain-language summaries of articles describing
randomized controlled trials; in the latter, we assess the degree to which
GPT-3 is able to \emph{synthesize} evidence reported across a collection of
articles. We design an annotation scheme for evaluating model outputs, with an
emphasis on assessing the factual accuracy of generated summaries. We find that
while GPT-3 is able to summarize and simplify single biomedical articles
faithfully, it struggles to provide accurate aggregations of findings over
multiple documents. We release all data and annotations used in this work.",None,-1
3a64e7b4-422f-4d74-ac2a-1a483caadb18,Tile Networks: Learning Optimal Geometric Layout for Whole-page Recommendation,0.644287,3,"Finding optimal configurations in a geometric space is a key challenge in
many technological disciplines. Current approaches either rely heavily on human
domain expertise and are difficult to scale. In this paper we show it is
possible to solve configuration optimization problems for whole-page
recommendation using reinforcement learning. The proposed \textit{Tile
Networks} is a neural architecture that optimizes 2D geometric configurations
by arranging items on proper positions. Empirical results on real dataset
demonstrate its superior performance compared to traditional learning to rank
approaches and recent deep models.",None,-1
4cf6662a-206b-4c43-ac36-b15544133119,Pinpointing Why Object Recognition Performance Degrades Across Income Levels and Geographies,0.381122,6,"Despite impressive advances in object-recognition, deep learning systems'
performance degrades significantly across geographies and lower income levels
raising pressing concerns of inequity. Addressing such performance gaps remains
a challenge, as little is understood about why performance degrades across
incomes or geographies. We take a step in this direction by annotating images
from Dollar Street, a popular benchmark of geographically and economically
diverse images, labeling each image with factors such as color, shape, and
background. These annotations unlock a new granular view into how objects
differ across incomes and regions. We then use these object differences to
pinpoint model vulnerabilities across incomes and regions. We study a range of
modern vision models, finding that performance disparities are most associated
with differences in texture, occlusion, and images with darker lighting. We
illustrate how insights from our factor labels can surface mitigations to
improve models' performance disparities. As an example, we show that mitigating
a model's vulnerability to texture can improve performance on the lower income
level. We release all the factor annotations along with an interactive
dashboard to facilitate research into more equitable vision systems.",None,-1
57d8c2f5-16da-4cfe-b90e-925a092c8b32,Predicting Privacy Preferences for Smart Devices as Norms,0.463227,4,"Smart devices, such as smart speakers, are becoming ubiquitous, and users
expect these devices to act in accordance with their preferences. In
particular, since these devices gather and manage personal data, users expect
them to adhere to their privacy preferences. However, the current approach of
gathering these preferences consists in asking the users directly, which
usually triggers automatic responses failing to capture their true preferences.
In response, in this paper we present a collaborative filtering approach to
predict user preferences as norms. These preference predictions can be readily
adopted or can serve to assist users in determining their own preferences.
Using a dataset of privacy preferences of smart assistant users, we test the
accuracy of our predictions.",None,-1
81d4d4bd-8037-4708-9df9-da9c0d9829af,SpellMapper: A non-autoregressive neural spellchecker for ASR customization with candidate retrieval based on n-gram mappings,0.798104,4,"Contextual spelling correction models are an alternative to shallow fusion to
improve automatic speech recognition (ASR) quality given user vocabulary. To
deal with large user vocabularies, most of these models include candidate
retrieval mechanisms, usually based on minimum edit distance between fragments
of ASR hypothesis and user phrases. However, the edit-distance approach is
slow, non-trainable, and may have low recall as it relies only on common
letters. We propose: 1) a novel algorithm for candidate retrieval, based on
misspelled n-gram mappings, which gives up to 90% recall with just the top 10
candidates on Spoken Wikipedia; 2) a non-autoregressive neural model based on
BERT architecture, where the initial transcript and ten candidates are combined
into one input. The experiments on Spoken Wikipedia show 21.4% word error rate
improvement compared to a baseline ASR system.",None,-1
26689db1-53da-4594-ae6a-8dda951cbd8a,Shape-Erased Feature Learning for Visible-Infrared Person Re-Identification,0.983116,21,"Due to the modality gap between visible and infrared images with high visual
ambiguity, learning \textbf{diverse} modality-shared semantic concepts for
visible-infrared person re-identification (VI-ReID) remains a challenging
problem. Body shape is one of the significant modality-shared cues for VI-ReID.
To dig more diverse modality-shared cues, we expect that erasing
body-shape-related semantic concepts in the learned features can force the ReID
model to extract more and other modality-shared features for identification. To
this end, we propose shape-erased feature learning paradigm that decorrelates
modality-shared features in two orthogonal subspaces. Jointly learning
shape-related feature in one subspace and shape-erased features in the
orthogonal complement achieves a conditional mutual information maximization
between shape-erased feature and identity discarding body shape information,
thus enhancing the diversity of the learned representation explicitly.
Extensive experiments on SYSU-MM01, RegDB, and HITSZ-VCM datasets demonstrate
the effectiveness of our method.",None,-1
0dcecf75-c132-4b4f-b296-7a4d4e33c595,Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation,0.483877,13,"The objective of this paper is self-supervised learning of video object
segmentation. We develop a unified framework which simultaneously models
cross-frame dense correspondence for locally discriminative feature learning
and embeds object-level context for target-mask decoding. As a result, it is
able to directly learn to perform mask-guided sequential segmentation from
unlabeled videos, in contrast to previous efforts usually relying on an oblique
solution - cheaply ""copying"" labels according to pixel-wise correlations.
Concretely, our algorithm alternates between i) clustering video pixels for
creating pseudo segmentation labels ex nihilo; and ii) utilizing the pseudo
labels to learn mask encoding and decoding for VOS. Unsupervised correspondence
learning is further incorporated into this self-taught, mask embedding scheme,
so as to ensure the generic nature of the learnt representation and avoid
cluster degeneracy. Our algorithm sets state-of-the-arts on two standard
benchmarks (i.e., DAVIS17 and YouTube-VOS), narrowing the gap between self- and
fully-supervised VOS, in terms of both performance and network architecture
design.",None,-1
d8d6daf3-3f14-4bd8-ad28-80b0f752d962,Explainable Goal Recognition: A Framework Based on Weight of Evidence,0.231949,3,"We introduce and evaluate an eXplainable Goal Recognition (XGR) model that
uses the Weight of Evidence (WoE) framework to explain goal recognition
problems. Our model provides human-centered explanations that answer why? and
why not? questions. We computationally evaluate the performance of our system
over eight different domains. Using a human behavioral study to obtain the
ground truth from human annotators, we further show that the XGR model can
successfully generate human-like explanations. We then report on a study with
60 participants who observe agents playing Sokoban game and then receive
explanations of the goal recognition output. We investigate participants'
understanding obtained by explanations through task prediction, explanation
satisfaction, and trust.",None,-1
b857d092-6c6e-445e-b24d-69f9eb1b2c87,RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations,0.299882,12,"Despite significant progress having been made in question answering on
tabular data (Table QA), it's unclear whether, and to what extent existing
Table QA models are robust to task-specific perturbations, e.g., replacing key
question entities or shuffling table columns. To systematically study the
robustness of Table QA models, we propose a benchmark called RobuT, which
builds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and
includes human-annotated adversarial perturbations in terms of table header,
table content, and question. Our results indicate that both state-of-the-art
Table QA models and large language models (e.g., GPT-3) with few-shot learning
falter in these adversarial sets. We propose to address this problem by using
large language models to generate adversarial examples to enhance training,
which significantly improves the robustness of Table QA models. Our data and
code is publicly available at https://github.com/yilunzhao/RobuT.",None,-1
db3a7f3e-7a4f-429f-abe4-6333c7b5d170,Symbolic Metamodels for Interpreting Black-boxes Using Primitive Functions,0.179518,3,"One approach for interpreting black-box machine learning models is to find a
global approximation of the model using simple interpretable functions, which
is called a metamodel (a model of the model). Approximating the black-box with
a metamodel can be used to 1) estimate instance-wise feature importance; 2)
understand the functional form of the model; 3) analyze feature interactions.
In this work, we propose a new method for finding interpretable metamodels. Our
approach utilizes Kolmogorov superposition theorem, which expresses
multivariate functions as a composition of univariate functions (our primitive
parameterized functions). This composition can be represented in the form of a
tree. Inspired by symbolic regression, we use a modified form of genetic
programming to search over different tree configurations. Gradient descent (GD)
is used to optimize the parameters of a given configuration. Our method is a
novel memetic algorithm that uses GD not only for training numerical constants
but also for the training of building blocks. Using several experiments, we
show that our method outperforms recent metamodeling approaches suggested for
interpreting black-boxes.",None,-1
5484dd23-d77d-425f-b78c-f7ea38ae1794,MMANet: Margin-aware Distillation and Modality-aware Regularization for Incomplete Multimodal Learning,0.33423,7,"Multimodal learning has shown great potentials in numerous scenes and
attracts increasing interest recently. However, it often encounters the problem
of missing modality data and thus suffers severe performance degradation in
practice. To this end, we propose a general framework called MMANet to assist
incomplete multimodal learning. It consists of three components: the deployment
network used for inference, the teacher network transferring comprehensive
multimodal information to the deployment network, and the regularization
network guiding the deployment network to balance weak modality combinations.
Specifically, we propose a novel margin-aware distillation (MAD) to assist the
information transfer by weighing the sample contribution with the
classification uncertainty. This encourages the deployment network to focus on
the samples near decision boundaries and acquire the refined inter-class
margin. Besides, we design a modality-aware regularization (MAR) algorithm to
mine the weak modality combinations and guide the regularization network to
calculate prediction loss for them. This forces the deployment network to
improve its representation ability for the weak modality combinations
adaptively. Finally, extensive experiments on multimodal classification and
segmentation tasks demonstrate that our MMANet outperforms the state-of-the-art
significantly. Code is available at: https://github.com/shicaiwei123/MMANet",None,-1
5dbba6cd-76a0-4e36-bb6f-aaeef793a62c,Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits,0.586906,25,"We present Second Thought, a new learning paradigm that enables language
models (LMs) to re-align with human values. By modeling the chain-of-edits
between value-unaligned and value-aligned text, with LM fine-tuning and
additional refinement through reinforcement learning, Second Thought not only
achieves superior performance in three value alignment benchmark datasets but
also shows strong human-value transfer learning ability in few-shot scenarios.
The generated editing steps also offer better interpretability and ease for
interactive error correction. Extensive human evaluations further confirm its
effectiveness.",None,-1
784409bc-6740-4589-a889-55ac93dddee8,Compositional 3D Scene Generation using Locally Conditioned Diffusion,0.808259,49,"Designing complex 3D scenes has been a tedious, manual process requiring
domain expertise. Emerging text-to-3D generative models show great promise for
making this task more intuitive, but existing approaches are limited to
object-level generation. We introduce \textbf{locally conditioned diffusion} as
an approach to compositional scene diffusion, providing control over semantic
parts using text prompts and bounding boxes while ensuring seamless transitions
between these parts. We demonstrate a score distillation sampling--based
text-to-3D synthesis pipeline that enables compositional 3D scene generation at
a higher fidelity than relevant baselines.",None,-1
c95eed74-3bc9-4a57-855d-a198a626ae39,POEM: Polarization of Embeddings for Domain-Invariant Representations,0.244915,3,"Handling out-of-distribution samples is a long-lasting challenge for deep
visual models. In particular, domain generalization (DG) is one of the most
relevant tasks that aims to train a model with a generalization capability on
novel domains. Most existing DG approaches share the same philosophy to
minimize the discrepancy between domains by finding the domain-invariant
representations. On the contrary, our proposed method called POEM acquires a
strong DG capability by learning domain-invariant and domain-specific
representations and polarizing them. Specifically, POEM cotrains
category-classifying and domain-classifying embeddings while regularizing them
to be orthogonal via minimizing the cosine-similarity between their features,
i.e., the polarization of embeddings. The clear separation of embeddings
suppresses domain-specific features in the domain-invariant embeddings. The
concept of POEM shows a unique direction to enhance the domain robustness of
representations that brings considerable and consistent performance gains when
combined with existing DG methods. Extensive simulation results in popular DG
benchmarks with the PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet
datasets show that POEM indeed facilitates the category-classifying embedding
to be more domain-invariant.",None,-1
71a16f0c-a258-4c67-9b4a-8a97e02d7e57,Overinformative Question Answering by Humans and Machines,0.0481635,2,"When faced with a polar question, speakers often provide overinformative
answers going beyond a simple ""yes"" or ""no"". But what principles guide the
selection of additional information? In this paper, we provide experimental
evidence from two studies suggesting that overinformativeness in human
answering is driven by considerations of relevance to the questioner's goals
which they flexibly adjust given the functional context in which the question
is uttered. We take these human results as a strong benchmark for investigating
question-answering performance in state-of-the-art neural language models,
conducting an extensive evaluation on items from human experiments. We find
that most models fail to adjust their answering behavior in a human-like way
and tend to include irrelevant information. We show that GPT-3 is highly
sensitive to the form of the prompt and only achieves human-like answer
patterns when guided by an example and cognitively-motivated explanation.",None,-1
65dd9d99-51b4-4e01-b92f-6b18fcb47bad,Arukikata Travelogue Dataset,0.0425294,2,"We have constructed Arukikata Travelogue Dataset and released it free of
charge for academic research. This dataset is a Japanese text dataset with a
total of over 31 million words, comprising 4,672 Japanese domestic travelogues
and 9,607 overseas travelogues. Before providing our dataset, there was a
scarcity of widely available travelogue data for research purposes, and each
researcher had to prepare their own data. This hinders the replication of
existing studies and fair comparative analysis of experimental results. Our
dataset enables any researchers to conduct investigation on the same data and
to ensure transparency and reproducibility in research. In this paper, we
describe the academic significance, characteristics, and prospects of our
dataset.",None,-1
0a67bb3b-4679-42e8-ba8d-8cbcad62c7aa,Hierarchical Neural Memory Network for Low Latency Event Processing,0.85635,11,"This paper proposes a low latency neural network architecture for event-based
dense prediction tasks. Conventional architectures encode entire scene contents
at a fixed rate regardless of their temporal characteristics. Instead, the
proposed network encodes contents at a proper temporal scale depending on its
movement speed. We achieve this by constructing temporal hierarchy using
stacked latent memories that operate at different rates. Given low latency
event steams, the multi-level memories gradually extract dynamic to static
scene contents by propagating information from the fast to the slow memory
modules. The architecture not only reduces the redundancy of conventional
architectures but also exploits long-term dependencies. Furthermore, an
attention-based event representation efficiently encodes sparse event streams
into the memory cells. We conduct extensive evaluations on three event-based
dense prediction tasks, where the proposed approach outperforms the existing
methods on accuracy and latency, while demonstrating effective event and image
fusion capabilities. The code is available at https://hamarh.github.io/hmnet/",None,-1
b511e8c1-c184-47bf-b5a5-13ae49084721,Knowledge-based Reasoning and Learning under Partial Observability in Ad Hoc Teamwork,0.364152,2,"Ad hoc teamwork refers to the problem of enabling an agent to collaborate
with teammates without prior coordination. Data-driven methods represent the
state of the art in ad hoc teamwork. They use a large labeled dataset of prior
observations to model the behavior of other agent types and to determine the ad
hoc agent's behavior. These methods are computationally expensive, lack
transparency, and make it difficult to adapt to previously unseen changes,
e.g., in team composition. Our recent work introduced an architecture that
determined an ad hoc agent's behavior based on non-monotonic logical reasoning
with prior commonsense domain knowledge and predictive models of other agents'
behavior that were learned from limited examples. In this paper, we
substantially expand the architecture's capabilities to support: (a) online
selection, adaptation, and learning of the models that predict the other
agents' behavior; and (b) collaboration with teammates in the presence of
partial observability and limited communication. We illustrate and
experimentally evaluate the capabilities of our architecture in two simulated
multiagent benchmark domains for ad hoc teamwork: Fort Attack and Half Field
Offense. We show that the performance of our architecture is comparable or
better than state of the art data-driven baselines in both simple and complex
scenarios, particularly in the presence of limited training data, partial
observability, and changes in team composition.",None,-1
746c8cbc-af61-47ae-9835-329b65433380,Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models,0.73487,2,"This work introduces approaches to assessing phrase breaks in ESL learners'
speech using pre-trained language models (PLMs) and large language models
(LLMs). There are two tasks: overall assessment of phrase break for a speech
clip and fine-grained assessment of every possible phrase break position. To
leverage NLP models, speech input is first force-aligned with texts, and then
pre-processed into a token sequence, including words and phrase break
information. To utilize PLMs, we propose a pre-training and fine-tuning
pipeline with the processed tokens. This process includes pre-training with a
replaced break token detection module and fine-tuning with text classification
and sequence labeling. To employ LLMs, we design prompts for ChatGPT. The
experiments show that with the PLMs, the dependence on labeled training data
has been greatly reduced, and the performance has improved. Meanwhile, we
verify that ChatGPT, a renowned LLM, has potential for further advancement in
this area.",None,-1
59f8121d-5d4d-4ff5-8c76-0ed31bf41ee6,Adversarial Contrastive Distillation with Adaptive Denoising,0.843592,20,"Adversarial Robustness Distillation (ARD) is a novel method to boost the
robustness of small models. Unlike general adversarial training, its robust
knowledge transfer can be less easily restricted by the model capacity.
However, the teacher model that provides the robustness of knowledge does not
always make correct predictions, interfering with the student's robust
performances. Besides, in the previous ARD methods, the robustness comes
entirely from one-to-one imitation, ignoring the relationship between examples.
To this end, we propose a novel structured ARD method called Contrastive
Relationship DeNoise Distillation (CRDND). We design an adaptive compensation
module to model the instability of the teacher. Moreover, we utilize the
contrastive relationship to explore implicit robustness knowledge among
multiple examples. Experimental results on multiple attack benchmarks show
CRDND can transfer robust knowledge efficiently and achieves state-of-the-art
performances.",None,-1
43d42b9c-65a6-4a44-aa45-122eb7ec42ff,Automated multilingual detection of Pro-Kremlin propaganda in newspapers and Telegram posts,0.536058,11,"The full-scale conflict between the Russian Federation and Ukraine generated
an unprecedented amount of news articles and social media data reflecting
opposing ideologies and narratives. These polarized campaigns have led to
mutual accusations of misinformation and fake news, shaping an atmosphere of
confusion and mistrust for readers worldwide. This study analyses how the media
affected and mirrored public opinion during the first month of the war using
news articles and Telegram news channels in Ukrainian, Russian, Romanian and
English. We propose and compare two methods of multilingual automated
pro-Kremlin propaganda identification, based on Transformers and linguistic
features. We analyse the advantages and disadvantages of both methods, their
adaptability to new genres and languages, and ethical considerations of their
usage for content moderation. With this work, we aim to lay the foundation for
further development of moderation tools tailored to the current conflict.",None,-1
d7a5bfa2-ff23-4033-a6af-b49df8b32032,BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations,0.453804,6,"Real-world planning problems, including autonomous driving and sustainable
energy applications like carbon storage and resource exploration, have recently
been modeled as partially observable Markov decision processes (POMDPs) and
solved using approximate methods. To solve high-dimensional POMDPs in practice,
state-of-the-art methods use online planning with problem-specific heuristics
to reduce planning horizons and make the problems tractable. Algorithms that
learn approximations to replace heuristics have recently found success in
large-scale fully observable domains. The key insight is the combination of
online Monte Carlo tree search with offline neural network approximations of
the optimal policy and value function. In this work, we bring this insight to
partially observed domains and propose BetaZero, a belief-state planning
algorithm for high-dimensional POMDPs. BetaZero learns offline approximations
that replace heuristics to enable online decision making in long-horizon
problems. We address several challenges inherent in large-scale partially
observable domains; namely challenges of transitioning in stochastic
environments, prioritizing action branching with a limited search budget, and
representing beliefs as input to the network. To formalize the use of all
limited search information we train against a novel Q-weighted policy vector
target. We test BetaZero on various well-established benchmark POMDPs found in
the literature and a real-world, high-dimensional problem of critical mineral
exploration. Experiments show that BetaZero outperforms state-of-the-art POMDP
solvers on a variety of tasks.",None,-1
507fc34a-ebbe-489b-a8ba-41b3a888e998,Algorithmic Transparency and Manipulation,0.773198,3,"A series of recent papers raises worries about the manipulative potential of
algorithmic transparency. But while the concern is apt and relevant, it is
based on a fraught understanding of manipulation. Therefore, this paper draws
attention to the indifference view of manipulation, which explains better than
the vulnerability view why algorithmic transparency has manipulative potential.
The paper also raises pertinent research questions for future studies of
manipulation in the context of algorithmic transparency.",None,-1
f63bbea8-7baa-4f98-b051-c3633eb68804,AutoTrial: Prompting Language Models for Clinical Trial Design,0.627051,8,"Clinical trials are critical for drug development. Constructing the
appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for
patient recruitment) is essential for the trial's success. Proper design of
clinical trial protocols should consider similar precedent trials and their
eligibility criteria to ensure sufficient patient coverage. In this paper, we
present a method named AutoTrial to aid the design of clinical eligibility
criteria using language models. It allows (1) controllable generation under
instructions via a hybrid of discrete and neural prompting, (2) scalable
knowledge incorporation via in-context learning, and (3) explicit reasoning
chains to provide rationales for understanding the outputs. Experiments on over
70K clinical trials verify that AutoTrial generates high-quality criteria texts
that are fluent and coherent and with high accuracy in capturing the relevant
clinical concepts to the target trial. It is noteworthy that our method, with a
much smaller parameter size, gains around 60% winning rate against the GPT-3.5
baselines via human evaluations.",None,-1
edaeabe8-64a9-4d59-aa3f-6929a8cfcbd6,Graph Laplacian for Semi-Supervised Learning,0.19118,2,"Semi-supervised learning is highly useful in common scenarios where labeled
data is scarce but unlabeled data is abundant. The graph (or nonlocal)
Laplacian is a fundamental smoothing operator for solving various learning
tasks. For unsupervised clustering, a spectral embedding is often used, based
on graph-Laplacian eigenvectors. For semi-supervised problems, the common
approach is to solve a constrained optimization problem, regularized by a
Dirichlet energy, based on the graph-Laplacian. However, as supervision
decreases, Dirichlet optimization becomes suboptimal. We therefore would like
to obtain a smooth transition between unsupervised clustering and
low-supervised graph-based classification. In this paper, we propose a new type
of graph-Laplacian which is adapted for Semi-Supervised Learning (SSL)
problems. It is based on both density and contrastive measures and allows the
encoding of the labeled data directly in the operator. Thus, we can perform
successfully semi-supervised learning using spectral clustering. The benefits
of our approach are illustrated for several SSL problems.",None,-1
ff55ae36-5cca-4e0c-b250-cd4319cce72f,Fast Inference and Update of Probabilistic Density Estimation on Trajectory Prediction,0.7577,7,"Safety-critical applications such as autonomous vehicles and social robots
require fast computation and accurate probability density estimation on
trajectory prediction. To address both requirements, this paper presents a new
normalizing flow-based trajectory prediction model named FlowChain. FlowChain
is a stack of conditional continuously-indexed flows (CIFs) that are expressive
and allow analytical probability density computation. This analytical
computation is faster than the generative models that need additional
approximations such as kernel density estimation. Moreover, FlowChain is more
accurate than the Gaussian mixture-based models due to fewer assumptions on the
estimated density. FlowChain also allows a rapid update of estimated
probability densities. This update is achieved by adopting the \textit{newest
observed position} and reusing the flow transformations and its
log-det-jacobians that represent the \textit{motion trend}. This update is
completed in less than one millisecond because this reuse greatly omits the
computational cost. Experimental results showed our FlowChain achieved
state-of-the-art trajectory prediction accuracy compared to previous methods.
Furthermore, our FlowChain demonstrated superiority in the accuracy and speed
of density estimation. Our code is available at
\url{https://github.com/meaten/FlowChain-ICCV2023}",None,-1
771966d7-269c-4603-9091-05ef4fa734fd,Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue,0.699011,3,"Open-domain dialogue system usually requires different sources of knowledge
to generate more informative and evidential responses. However, existing
knowledge-grounded dialogue systems either focus on a single knowledge source
or overlook the dependency between multiple sources of knowledge, which may
result in generating inconsistent or even paradoxical responses. To incorporate
multiple knowledge sources and dependencies between them, we propose SAFARI, a
novel framework that leverages the exceptional capabilities of large language
models (LLMs) in planning, understanding, and incorporating under both
supervised and unsupervised settings. Specifically, SAFARI decouples the
knowledge grounding into multiple sources and response generation, which allows
easy extension to various knowledge sources including the possibility of not
using any sources. To study the problem, we construct a personalized
knowledge-grounded dialogue dataset \textit{\textbf{K}nowledge \textbf{B}ehind
\textbf{P}ersona}~(\textbf{KBP}), which is the first to consider the dependency
between persona and implicit knowledge. Experimental results on the KBP dataset
demonstrate that the SAFARI framework can effectively produce
persona-consistent and knowledge-enhanced responses.",None,-1
31f3e3e1-81b4-4be3-a2e2-33e7862bc9af,Integrated Conflict Management for UAM with Strategic Demand Capacity Balancing and Learning-based Tactical Deconfliction,0.84909,5,"Urban air mobility (UAM) has the potential to revolutionize our daily
transportation, offering rapid and efficient deliveries of passengers and cargo
between dedicated locations within and around the urban environment. Before the
commercialization and adoption of this emerging transportation mode, however,
aviation safety must be guaranteed, i.e., all the aircraft have to be safely
separated by strategic and tactical deconfliction. Reinforcement learning has
demonstrated effectiveness in the tactical deconfliction of en route commercial
air traffic in simulation. However, its performance is found to be dependent on
the traffic density. In this project, we propose a novel framework that
combines demand capacity balancing (DCB) for strategic conflict management and
reinforcement learning for tactical separation. By using DCB to precondition
traffic to proper density levels, we show that reinforcement learning can
achieve much better performance for tactical safety separation. Our results
also indicate that this DCB preconditioning can allow target levels of safety
to be met that are otherwise impossible. In addition, combining strategic DCB
with reinforcement learning for tactical separation can meet these safety
levels while achieving greater operational efficiency than alternative
solutions.",None,-1
5b86dc0a-8411-4ebd-8f40-d627b479dc41,Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles,0.866629,43,"Modern hierarchical vision transformers have added several vision-specific
components in the pursuit of supervised classification performance. While these
components lead to effective accuracies and attractive FLOP counts, the added
complexity actually makes these transformers slower than their vanilla ViT
counterparts. In this paper, we argue that this additional bulk is unnecessary.
By pretraining with a strong visual pretext task (MAE), we can strip out all
the bells-and-whistles from a state-of-the-art multi-stage vision transformer
without losing accuracy. In the process, we create Hiera, an extremely simple
hierarchical vision transformer that is more accurate than previous models
while being significantly faster both at inference and during training. We
evaluate Hiera on a variety of tasks for image and video recognition. Our code
and models are available at https://github.com/facebookresearch/hiera.",None,-1
cfff49ae-59ab-4d9c-a02d-385c67c7daa2,Incorporating Transformer Designs into Convolutions for Lightweight Image Super-Resolution,0.32162,4,"In recent years, the use of large convolutional kernels has become popular in
designing convolutional neural networks due to their ability to capture
long-range dependencies and provide large receptive fields. However, the
increase in kernel size also leads to a quadratic growth in the number of
parameters, resulting in heavy computation and memory requirements. To address
this challenge, we propose a neighborhood attention (NA) module that upgrades
the standard convolution with a self-attention mechanism. The NA module
efficiently extracts long-range dependencies in a sliding window pattern,
thereby achieving similar performance to large convolutional kernels but with
fewer parameters.
  Building upon the NA module, we propose a lightweight single image
super-resolution (SISR) network named TCSR. Additionally, we introduce an
enhanced feed-forward network (EFFN) in TCSR to improve the SISR performance.
EFFN employs a parameter-free spatial-shift operation for efficient feature
aggregation. Our extensive experiments and ablation studies demonstrate that
TCSR outperforms existing lightweight SISR methods and achieves
state-of-the-art performance. Our codes are available at
\url{https://github.com/Aitical/TCSR}.",None,-1
1e59fd34-0097-4ed5-baf0-9481709fc2c6,Using Z3 for Formal Modeling and Verification of FNN Global Robustness,0.853753,6,"While Feedforward Neural Networks (FNNs) have achieved remarkable success in
various tasks, they are vulnerable to adversarial examples. Several techniques
have been developed to verify the adversarial robustness of FNNs, but most of
them focus on robustness verification against the local perturbation
neighborhood of a single data point. There is still a large research gap in
global robustness analysis. The global-robustness verifiable framework
DeepGlobal has been proposed to identify \textit{all} possible Adversarial
Dangerous Regions (ADRs) of FNNs, not limited to data samples in a test set. In
this paper, we propose a complete specification and implementation of
DeepGlobal utilizing the SMT solver Z3 for more explicit definition, and
propose several improvements to DeepGlobal for more efficient verification. To
evaluate the effectiveness of our implementation and improvements, we conduct
extensive experiments on a set of benchmark datasets. Visualization of our
experiment results shows the validity and effectiveness of the approach.",None,-1
f51339e5-4982-4f09-8b79-dc18409cc94d,Efficient Computation of Shap Explanation Scores for Neural Network Classifiers via Knowledge Compilation,0.0382654,2,"The use of Shap scores has become widespread in Explainable AI. However,
their computation is in general intractable, in particular when done with a
black-box classifier, such as neural network. Recent research has unveiled
classes of open-box Boolean Circuit classifiers for which Shap can be computed
efficiently. We show how to transform binary neural networks into those
circuits for efficient Shap computation.We use logic-based knowledge
compilation techniques. The performance gain is huge, as we show in the light
of our experiments.",None,-1
cf9e5bc2-bb9c-4ccc-ae4f-4a4df22a51d6,Vision Transformer for Action Units Detection,0.963505,11,"Facial Action Units detection (FAUs) represents a fine-grained classification
problem that involves identifying different units on the human face, as defined
by the Facial Action Coding System. In this paper, we present a simple yet
efficient Vision Transformer-based approach for addressing the task of Action
Units (AU) detection in the context of Affective Behavior Analysis in-the-wild
(ABAW) competition. We employ the Video Vision Transformer(ViViT) Network to
capture the temporal facial change in the video. Besides, to reduce massive
size of the Vision Transformers model, we replace the ViViT feature extraction
layers with the CNN backbone (Regnet). Our model outperform the baseline model
of ABAW 2023 challenge, with a notable 14% difference in result. Furthermore,
the achieved results are comparable to those of the top three teams in the
previous ABAW 2022 challenge.",None,-1
58ac0ef3-f143-4519-be60-c61e3646558e,Leveraging Few-Shot Data Augmentation and Waterfall Prompting for Response Generation,0.133682,2,"This paper discusses our approaches for task-oriented conversational
modelling using subjective knowledge, with a particular emphasis on response
generation. Our methodology was shaped by an extensive data analysis that
evaluated key factors such as response length, sentiment, and dialogue acts
present in the provided dataset. We used few-shot learning to augment the data
with newly generated subjective knowledge items and present three approaches
for DSTC11: (1) task-specific model exploration, (2) incorporation of the most
frequent question into all generated responses, and (3) a waterfall prompting
technique using a combination of both GPT-3 and ChatGPT.",None,-1
1d4b515e-e5c5-4d01-ad74-5688b8edecf2,Learning Image-Adaptive Codebooks for Class-Agnostic Image Restoration,0.624476,7,"Recent work on discrete generative priors, in the form of codebooks, has
shown exciting performance for image reconstruction and restoration, as the
discrete prior space spanned by the codebooks increases the robustness against
diverse image degradations. Nevertheless, these methods require separate
training of codebooks for different image categories, which limits their use to
specific image categories only (e.g. face, architecture, etc.), and fail to
handle arbitrary natural images. In this paper, we propose AdaCode for learning
image-adaptive codebooks for class-agnostic image restoration. Instead of
learning a single codebook for each image category, we learn a set of basis
codebooks. For a given input image, AdaCode learns a weight map with which we
compute a weighted combination of these basis codebooks for adaptive image
restoration. Intuitively, AdaCode is a more flexible and expressive discrete
generative prior than previous work. Experimental results demonstrate that
AdaCode achieves state-of-the-art performance on image reconstruction and
restoration tasks, including image super-resolution and inpainting.",None,-1
19cd5548-d873-4972-a711-c0274bf4ea4f,"SAM Struggles in Concealed Scenes -- Empirical Study on ""Segment Anything""",0.883141,64,"Segmenting anything is a ground-breaking step toward artificial general
intelligence, and the Segment Anything Model (SAM) greatly fosters the
foundation models for computer vision. We could not be more excited to probe
the performance traits of SAM. In particular, exploring situations in which SAM
does not perform well is interesting. In this report, we choose three concealed
scenes, i.e., camouflaged animals, industrial defects, and medical lesions, to
evaluate SAM under unprompted settings. Our main observation is that SAM looks
unskilled in concealed scenes.",None,-1
e36124c6-7f24-49b5-a8bb-c460a8393322,Clickbait Detection via Large Language Models,0.0572167,2,"Clickbait, which aims to induce users with some surprising and even thrilling
headlines for increasing click-through rates, permeates almost all online
content publishers, such as news portals and social media. Recently, Large
Language Models (LLMs) have emerged as a powerful instrument and achieved
tremendous success in a series of NLP downstream tasks. However, it is not yet
known whether LLMs can be served as a high-quality clickbait detection system.
In this paper, we analyze the performance of LLMs in the few-shot and zero-shot
scenarios on several English and Chinese benchmark datasets. Experimental
results show that LLMs cannot achieve the best results compared to the
state-of-the-art deep and fine-tuning PLMs methods. Different from human
intuition, the experiments demonstrated that LLMs cannot make satisfied
clickbait detection just by the headlines.",None,-1
bf02a055-53d0-42a2-9f20-876fb9a39395,Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation,0.762063,8,"Translating images from a source domain to a target domain for learning
target models is one of the most common strategies in domain adaptive semantic
segmentation (DASS). However, existing methods still struggle to preserve
semantically-consistent local details between the original and translated
images. In this work, we present an innovative approach that addresses this
challenge by using source-domain labels as explicit guidance during image
translation. Concretely, we formulate cross-domain image translation as a
denoising diffusion process and utilize a novel Semantic Gradient Guidance
(SGG) method to constrain the translation process, conditioning it on the
pixel-wise source labels. Additionally, a Progressive Translation Learning
(PTL) strategy is devised to enable the SGG method to work reliably across
domains with large gaps. Extensive experiments demonstrate the superiority of
our approach over state-of-the-art methods.",None,-1
b017153a-3e96-42d7-a382-2508901274ca,A Psycholinguistic Analysis of BERT's Representations of Compounds,0.497774,3,"This work studies the semantic representations learned by BERT for compounds,
that is, expressions such as sunlight or bodyguard. We build on recent studies
that explore semantic information in Transformers at the word level and test
whether BERT aligns with human semantic intuitions when dealing with
expressions (e.g., sunlight) whose overall meaning depends -- to a various
extent -- on the semantics of the constituent words (sun, light). We leverage a
dataset that includes human judgments on two psycholinguistic measures of
compound semantic analysis: lexeme meaning dominance (LMD; quantifying the
weight of each constituent toward the compound meaning) and semantic
transparency (ST; evaluating the extent to which the compound meaning is
recoverable from the constituents' semantics). We show that BERT-based measures
moderately align with human intuitions, especially when using contextualized
representations, and that LMD is overall more predictable than ST. Contrary to
the results reported for 'standard' words, higher, more contextualized layers
are the best at representing compound meaning. These findings shed new light on
the abilities of BERT in dealing with fine-grained semantic phenomena.
Moreover, they can provide insights into how speakers represent compounds.",None,-1
481d6fe9-64f5-4189-afe7-97bdcda40a4b,Multimodality and Attention Increase Alignment in Natural Language Prediction Between Humans and Computational Models,0.056846,2,"The potential of multimodal generative artificial intelligence (mAI) to
replicate human grounded language understanding, including the pragmatic,
context-rich aspects of communication, remains to be clarified. Humans are
known to use salient multimodal features, such as visual cues, to facilitate
the processing of upcoming words. Correspondingly, multimodal computational
models can integrate visual and linguistic data using a visual attention
mechanism to assign next-word probabilities. To test whether these processes
align, we tasked both human participants (N = 200) as well as several
state-of-the-art computational models with evaluating the predictability of
forthcoming words after viewing short audio-only or audio-visual clips with
speech. During the task, the model's attention weights were recorded and human
attention was indexed via eye tracking. Results show that predictability
estimates from humans aligned more closely with scores generated from
multimodal models vs. their unimodal counterparts. Furthermore, including an
attention mechanism doubled alignment with human judgments when visual and
linguistic context facilitated predictions. In these cases, the model's
attention patches and human eye tracking significantly overlapped. Our results
indicate that improved modeling of naturalistic language processing in mAI does
not merely depend on training diet but can be driven by multimodality in
combination with attention-based architectures. Humans and computational models
alike can leverage the predictive constraints of multimodal information by
attending to relevant features in the input.",None,-1
93ae4af2-3986-40fa-9868-c7970789b969,Multiscale Video Pretraining for Long-Term Activity Forecasting,0.462933,3,"Long-term activity forecasting is an especially challenging research problem
because it requires understanding the temporal relationships between observed
actions, as well as the variability and complexity of human activities. Despite
relying on strong supervision via expensive human annotations, state-of-the-art
forecasting approaches often generalize poorly to unseen data. To alleviate
this issue, we propose Multiscale Video Pretraining (MVP), a novel
self-supervised pretraining approach that learns robust representations for
forecasting by learning to predict contextualized representations of future
video clips over multiple timescales. MVP is based on our observation that
actions in videos have a multiscale nature, where atomic actions typically
occur at a short timescale and more complex actions may span longer timescales.
We compare MVP to state-of-the-art self-supervised video learning approaches on
downstream long-term forecasting tasks including long-term action anticipation
and video summary prediction. Our comprehensive experiments across the Ego4D
and Epic-Kitchens-55/100 datasets demonstrate that MVP out-performs
state-of-the-art methods by significant margins. Notably, MVP obtains a
relative performance gain of over 20% accuracy in video summary forecasting
over existing methods.",None,-1
7601a6df-85a9-4339-a24d-ec98d7bde41b,Emotion-Aware Prosodic Phrasing for Expressive Text-to-Speech,0.290818,2,"Prosodic phrasing is crucial to the naturalness and intelligibility of
end-to-end Text-to-Speech (TTS). There exist both linguistic and emotional
prosody in natural speech. As the study of prosodic phrasing has been
linguistically motivated, prosodic phrasing for expressive emotion rendering
has not been well studied. In this paper, we propose an emotion-aware prosodic
phrasing model, termed \textit{EmoPP}, to mine the emotional cues of utterance
accurately and predict appropriate phrase breaks. We first conduct objective
observations on the ESD dataset to validate the strong correlation between
emotion and prosodic phrasing. Then the objective and subjective evaluations
show that the EmoPP outperforms all baselines and achieves remarkable
performance in terms of emotion expressiveness. The audio samples and the code
are available at \url{https://github.com/AI-S2-Lab/EmoPP}.",None,-1
4cc01980-201d-4a53-b90a-630b13cfdca7,Reliability Scores from Saliency Map Clusters for Improved Image-based Harvest-Readiness Prediction in Cauliflower,0.748747,4,"Cauliflower is a hand-harvested crop that must fulfill high-quality standards
in sales making the timing of harvest important. However, accurately
determining harvest-readiness can be challenging due to the cauliflower head
being covered by its canopy. While deep learning enables automated
harvest-readiness estimation, errors can occur due to field-variability and
limited training data. In this paper, we analyze the reliability of a
harvest-readiness classifier with interpretable machine learning. By
identifying clusters of saliency maps, we derive reliability scores for each
classification result using knowledge about the domain and the image
properties. For unseen data, the reliability can be used to (i) inform farmers
to improve their decision-making and (ii) increase the model prediction
accuracy. Using RGB images of single cauliflower plants at different
developmental stages from the GrowliFlower dataset, we investigate various
saliency mapping approaches and find that they result in different quality of
reliability scores. With the most suitable interpretation tool, we adjust the
classification result and achieve a 15.72% improvement of the overall accuracy
to 88.14% and a 15.44% improvement of the average class accuracy to 88.52% for
the GrowliFlower dataset.",None,-1
f29018c2-05a5-4940-83ce-f5bba204f254,DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers,0.444631,1,"We propose a novel talking head synthesis pipeline called ""DiT-Head"", which
is based on diffusion transformers and uses audio as a condition to drive the
denoising process of a diffusion model. Our method is scalable and can
generalise to multiple identities while producing high-quality results. We
train and evaluate our proposed approach and compare it against existing
methods of talking head synthesis. We show that our model can compete with
these methods in terms of visual quality and lip-sync accuracy. Our results
highlight the potential of our proposed approach to be used for a wide range of
applications, including virtual assistants, entertainment, and education. For a
video demonstration of the results and our user study, please refer to our
supplementary material.",None,-1
3c57d930-f327-4326-adcc-9bbd593e45d4,Reference-guided Controllable Inpainting of Neural Radiance Fields,0.730219,22,"The popularity of Neural Radiance Fields (NeRFs) for view synthesis has led
to a desire for NeRF editing tools. Here, we focus on inpainting regions in a
view-consistent and controllable manner. In addition to the typical NeRF inputs
and masks delineating the unwanted region in each view, we require only a
single inpainted view of the scene, i.e., a reference view. We use monocular
depth estimators to back-project the inpainted view to the correct 3D
positions. Then, via a novel rendering technique, a bilateral solver can
construct view-dependent effects in non-reference views, making the inpainted
region appear consistent from any view. For non-reference disoccluded regions,
which cannot be supervised by the single reference view, we devise a method
based on image inpainters to guide both the geometry and appearance. Our
approach shows superior performance to NeRF inpainting baselines, with the
additional advantage that a user can control the generated scene via a single
inpainted image. Project page: https://ashmrz.github.io/reference-guided-3d",None,-1
41588a50-ea3a-48bd-9f0c-eca8d2a759dc,Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods,0.998907,60,"Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Due to rapid
technological advances and their extreme versatility, LLMs nowadays have
millions of users and are at the cusp of being the main go-to technology for
information retrieval, content generation, problem-solving, etc. Therefore, it
is of great importance to thoroughly assess and scrutinize their capabilities.
Due to increasingly complex and novel behavioral patterns in current LLMs, this
can be done by treating them as participants in psychology experiments that
were originally designed to test humans. For this purpose, the paper introduces
a new field of research called ""machine psychology"". The paper outlines how
different subfields of psychology can inform behavioral tests for LLMs. It
defines methodological standards for machine psychology research, especially by
focusing on policies for prompt designs. Additionally, it describes how
behavioral patterns discovered in LLMs are to be interpreted. In sum, machine
psychology aims to discover emergent abilities in LLMs that cannot be detected
by most traditional natural language processing benchmarks.",None,-1
2c8d64b9-5aa6-4f2b-b6cf-619f61e6b2e9,Hi4D: 4D Instance Segmentation of Close Human Interaction,0.893827,19,"We propose Hi4D, a method and dataset for the automatic analysis of
physically close human-human interaction under prolonged contact. Robustly
disentangling several in-contact subjects is a challenging task due to
occlusions and complex shapes. Hence, existing multi-view systems typically
fuse 3D surfaces of close subjects into a single, connected mesh. To address
this issue we leverage i) individually fitted neural implicit avatars; ii) an
alternating optimization scheme that refines pose and surface through periods
of close proximity; and iii) thus segment the fused raw scans into individual
instances. From these instances we compile Hi4D dataset of 4D textured scans of
20 subject pairs, 100 sequences, and a total of more than 11K frames. Hi4D
contains rich interaction-centric annotations in 2D and 3D alongside accurately
registered parametric body models. We define varied human pose and shape
estimation tasks on this dataset and provide results from state-of-the-art
methods on these benchmarks.",None,-1
facacc20-3d79-439a-ae5b-bbe98c7bd786,Spatial-Temporal Alignment Network for Action Recognition,0.14201,1,"This paper studies introducing viewpoint invariant feature representations in
existing action recognition architecture. Despite significant progress in
action recognition, efficiently handling geometric variations in large-scale
datasets remains challenging. To tackle this problem, we propose a novel
Spatial-Temporal Alignment Network (STAN), which explicitly learns geometric
invariant representations for action recognition. Notably, the STAN model is
light-weighted and generic, which could be plugged into existing action
recognition models (e.g., MViTv2) with a low extra computational cost. We test
our STAN model on widely-used datasets like UCF101 and HMDB51. The experimental
results show that the STAN model can consistently improve the state-of-the-art
models in action recognition tasks in trained-from-scratch settings.",None,-1
0accc8e8-2229-4c43-b6a6-86833a584ffb,LiT Tuned Models for Efficient Species Detection,0.0170185,1,"Recent advances in training vision-language models have demonstrated
unprecedented robustness and transfer learning effectiveness; however, standard
computer vision datasets are image-only, and therefore not well adapted to such
training methods. Our paper introduces a simple methodology for adapting any
fine-grained image classification dataset for distributed vision-language
pretraining. We implement this methodology on the challenging iNaturalist-2021
dataset, comprised of approximately 2.7 million images of macro-organisms
across 10,000 classes, and achieve a new state-of-the art model in terms of
zero-shot classification accuracy. Somewhat surprisingly, our model (trained
using a new method called locked-image text tuning) uses a pre-trained, frozen
vision representation, proving that language alignment alone can attain strong
transfer learning performance, even on fractious, long-tailed datasets. Our
approach opens the door for utilizing high quality vision-language pretrained
models in agriculturally relevant applications involving species detection.",None,-1
ca21baed-476c-4772-ace2-ded003616139,Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization,0.698806,3,"Recent computational approaches for combating online hate speech involve the
automatic generation of counter narratives by adapting Pretrained
Transformer-based Language Models (PLMs) with human-curated data. This process,
however, can produce in-domain overfitting, resulting in models generating
acceptable narratives only for hatred similar to training data, with little
portability to other targets or to real-world toxic language. This paper
introduces novel attention regularization methodologies to improve the
generalization capabilities of PLMs for counter narratives generation.
Overfitting to training-specific terms is then discouraged, resulting in more
diverse and richer narratives. We experiment with two attention-based
regularization techniques on a benchmark English dataset. Regularized models
produce better counter narratives than state-of-the-art approaches in most
cases, both in terms of automatic metrics and human evaluation, especially when
hateful targets are not present in the training data. This work paves the way
for better and more flexible counter-speech generation models, a task for which
datasets are highly challenging to produce.",None,-1
46e92ca6-dd64-4a32-8bcf-72625dd618b1,Towards Example-Based NMT with Multi-Levenshtein Transformers,0.145368,1,"Retrieval-Augmented Machine Translation (RAMT) is attracting growing
attention. This is because RAMT not only improves translation metrics, but is
also assumed to implement some form of domain adaptation. In this contribution,
we study another salient trait of RAMT, its ability to make translation
decisions more transparent by allowing users to go back to examples that
contributed to these decisions.
  For this, we propose a novel architecture aiming to increase this
transparency. This model adapts a retrieval-augmented version of the
Levenshtein Transformer and makes it amenable to simultaneously edit multiple
fuzzy matches found in memory. We discuss how to perform training and inference
in this model, based on multi-way alignment algorithms and imitation learning.
Our experiments show that editing several examples positively impacts
translation scores, notably increasing the number of target spans that are
copied from existing instances.",None,-1
5bbd9e6e-cf64-4107-baa2-b9d5c5ad6e16,Visual Instruction Tuning,1.0,1596,"Instruction tuning large language models (LLMs) using machine-generated
instruction-following data has improved zero-shot capabilities on new tasks,
but the idea is less explored in the multimodal field. In this paper, we
present the first attempt to use language-only GPT-4 to generate multimodal
language-image instruction-following data. By instruction tuning on such
generated data, we introduce LLaVA: Large Language and Vision Assistant, an
end-to-end trained large multimodal model that connects a vision encoder and
LLM for general-purpose visual and language understanding.Our early experiments
show that LLaVA demonstrates impressive multimodel chat abilities, sometimes
exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and
yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal
instruction-following dataset. When fine-tuned on Science QA, the synergy of
LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make
GPT-4 generated visual instruction tuning data, our model and code base
publicly available.",None,-1
f17370f8-f101-4efd-8446-22f02a3f4c72,An Expression Tree Decoding Strategy for Mathematical Equation Generation,0.502986,3,"Generating mathematical equations from natural language requires an accurate
understanding of the relations among math expressions. Existing approaches can
be broadly categorized into token-level and expression-level generation. The
former treats equations as a mathematical language, sequentially generating
math tokens. Expression-level methods generate each expression one by one.
However, each expression represents a solving step, and there naturally exist
parallel or dependent relations between these steps, which are ignored by
current sequential methods. Therefore, we integrate tree structure into the
expression-level generation and advocate an expression tree decoding strategy.
To generate a tree with expression as its node, we employ a layer-wise parallel
decoding strategy: we decode multiple independent expressions (leaf nodes) in
parallel at each layer and repeat parallel decoding layer by layer to
sequentially generate these parent node expressions that depend on others.
Besides, a bipartite matching algorithm is adopted to align multiple
predictions with annotations for each layer. Experiments show our method
outperforms other baselines, especially for these equations with complex
structures.",None,-1
7cb061ed-0012-4c41-abb9-925dad666b9e,Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization,0.593385,4,"End-to-end speech summarization (E2E SSum) directly summarizes input speech
into easy-to-read short sentences with a single model. This approach is
promising because it, in contrast to the conventional cascade approach, can
utilize full acoustical information and mitigate to the propagation of
transcription errors. However, due to the high cost of collecting
speech-summary pairs, an E2E SSum model tends to suffer from training data
scarcity and output unnatural sentences. To overcome this drawback, we propose
for the first time to integrate a pre-trained language model (LM), which is
highly capable of generating natural sentences, into the E2E SSum decoder via
transfer learning. In addition, to reduce the gap between the independently
pre-trained encoder and decoder, we also propose to transfer the baseline E2E
SSum encoder instead of the commonly used automatic speech recognition encoder.
Experimental results show that the proposed model outperforms baseline and data
augmented models.",None,-1
33669ebb-7206-4650-a3fd-1d8ba6b55867,Dialog act guided contextual adapter for personalized speech recognition,0.178878,2,"Personalization in multi-turn dialogs has been a long standing challenge for
end-to-end automatic speech recognition (E2E ASR) models. Recent work on
contextual adapters has tackled rare word recognition using user catalogs. This
adaptation, however, does not incorporate an important cue, the dialog act,
which is available in a multi-turn dialog scenario. In this work, we propose a
dialog act guided contextual adapter network. Specifically, it leverages dialog
acts to select the most relevant user catalogs and creates queries based on
both -- the audio as well as the semantic relationship between the carrier
phrase and user catalogs to better guide the contextual biasing. On industrial
voice assistant datasets, our model outperforms both the baselines - dialog act
encoder-only model, and the contextual adaptation, leading to the most
improvement over the no-context model: 58% average relative word error rate
reduction (WERR) in the multi-turn dialog scenario, in comparison to the
prior-art contextual adapter, which has achieved 39% WERR over the no-context
model.",None,-1
dc50b9a9-15cc-4551-bb5a-e817d71d8b6d,AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder,0.954397,28,"The recently introduced Segment Anything Model (SAM) combines a clever
architecture and large quantities of training data to obtain remarkable image
segmentation capabilities. However, it fails to reproduce such results for
Out-Of-Distribution (OOD) domains such as medical images. Moreover, while SAM
is conditioned on either a mask or a set of points, it may be desirable to have
a fully automatic solution. In this work, we replace SAM's conditioning with an
encoder that operates on the same input image. By adding this encoder and
without further fine-tuning SAM, we obtain state-of-the-art results on multiple
medical images and video benchmarks. This new encoder is trained via gradients
provided by a frozen SAM. For inspecting the knowledge within it, and providing
a lightweight segmentation solution, we also learn to decode it into a mask by
a shallow deconvolution network.",None,-1
0d5964dc-9952-442e-a271-6bc944c09fc9,ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation,0.981731,50,"We introduce ""ImageDream,"" an innovative image-prompt, multi-view diffusion
model for 3D object generation. ImageDream stands out for its ability to
produce 3D models of higher quality compared to existing state-of-the-art,
image-conditioned methods. Our approach utilizes a canonical camera
coordination for the objects in images, improving visual geometry accuracy. The
model is designed with various levels of control at each block inside the
diffusion model based on the input image, where global control shapes the
overall object layout and local control fine-tunes the image details. The
effectiveness of ImageDream is demonstrated through extensive evaluations using
a standard prompt list. For more information, visit our project page at
https://Image-Dream.github.io.",None,-1
257a4de4-2de3-45b0-8453-6dfcbf58e086,MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models,0.98973,17,"Large language models (LLMs) have achieved remarkable performance in natural
language understanding and generation tasks. However, they often suffer from
limitations such as difficulty in incorporating new knowledge, generating
hallucinations, and explaining their reasoning process. To address these
challenges, we propose a novel prompting pipeline, named \method, that
leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency.
Our method enables LLMs to comprehend KG inputs and infer with a combination of
implicit and external knowledge. Moreover, our method elicits the mind map of
LLMs, which reveals their reasoning pathways based on the ontology of
knowledge. We evaluate our method on diverse question \& answering tasks,
especially in medical domains, and show significant improvements over
baselines. We also introduce a new hallucination evaluation benchmark and
analyze the effects of different components of our method. Our results
demonstrate the effectiveness and robustness of our method in merging knowledge
from LLMs and KGs for combined inference. To reproduce our results and extend
the framework further, we make our codebase available at
https://github.com/wyl-willing/MindMap.",None,-1
51317756-f1b8-4113-8e2a-e1b0c02f4e2c,Probabilistic Dataset Reconstruction from Interpretable Models,0.334835,3,"Interpretability is often pointed out as a key requirement for trustworthy
machine learning. However, learning and releasing models that are inherently
interpretable leaks information regarding the underlying training data. As such
disclosure may directly conflict with privacy, a precise quantification of the
privacy impact of such breach is a fundamental problem. For instance, previous
work have shown that the structure of a decision tree can be leveraged to build
a probabilistic reconstruction of its training dataset, with the uncertainty of
the reconstruction being a relevant metric for the information leak. In this
paper, we propose of a novel framework generalizing these probabilistic
reconstructions in the sense that it can handle other forms of interpretable
models and more generic types of knowledge. In addition, we demonstrate that
under realistic assumptions regarding the interpretable models' structure, the
uncertainty of the reconstruction can be computed efficiently. Finally, we
illustrate the applicability of our approach on both decision trees and rule
lists, by comparing the theoretical information leak associated to either exact
or heuristic learning algorithms. Our results suggest that optimal
interpretable models are often more compact and leak less information regarding
their training data than greedily-built ones, for a given accuracy level.",None,-1
02316f20-69fe-49cf-8958-6935edef092e,APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses,0.434316,4,"The efficacy of availability poisoning, a method of poisoning data by
injecting imperceptible perturbations to prevent its use in model training, has
been a hot subject of investigation. Previous research suggested that it was
difficult to effectively counteract such poisoning attacks. However, the
introduction of various defense methods has challenged this notion. Due to the
rapid progress in this field, the performance of different novel methods cannot
be accurately validated due to variations in experimental setups. To further
evaluate the attack and defense capabilities of these poisoning methods, we
have developed a benchmark -- APBench for assessing the efficacy of adversarial
poisoning. APBench consists of 9 state-of-the-art availability poisoning
attacks, 8 defense algorithms, and 4 conventional data augmentation techniques.
We also have set up experiments with varying different poisoning ratios, and
evaluated the attacks on multiple datasets and their transferability across
model architectures. We further conducted a comprehensive evaluation of 2
additional attacks specifically targeting unsupervised models. Our results
reveal the glaring inadequacy of existing attacks in safeguarding individual
privacy. APBench is open source and available to the deep learning community:
https://github.com/lafeat/apbench.",None,-1
0309415a-c550-4b8f-98e0-d39dd3462427,Alignment with human representations supports robust few-shot learning,0.482883,15,"Should we care whether AI systems have representations of the world that are
similar to those of humans? We provide an information-theoretic analysis that
suggests that there should be a U-shaped relationship between the degree of
representational alignment with humans and performance on few-shot learning
tasks. We confirm this prediction empirically, finding such a relationship in
an analysis of the performance of 491 computer vision models. We also show that
highly-aligned models are more robust to both natural adversarial attacks and
domain shifts. Our results suggest that human-alignment is often a sufficient,
but not necessary, condition for models to make effective use of limited data,
be robust, and generalize well.",None,-1
c0ef4644-350b-4e69-b2fc-8916169a4732,Aligning Language Models with Offline Learning from Human Feedback,0.0226404,3,"Learning from human preferences is crucial for language models (LMs) to
effectively cater to human needs and societal values. Previous research has
made notable progress by leveraging human feedback to follow instructions.
However, these approaches rely primarily on online learning techniques like
Proximal Policy Optimization (PPO), which have been proven unstable and
challenging to tune for language models. Moreover, PPO requires complex
distributed system implementation, hindering the efficiency of large-scale
distributed training. In this study, we propose an offline learning from human
feedback framework to align LMs without interacting with environments.
Specifically, we explore filtering alignment (FA), reward-weighted regression
(RWR), and conditional alignment (CA) to align language models to human
preferences. By employing a loss function similar to supervised fine-tuning,
our methods ensure more stable model training than PPO with a simple machine
learning system~(MLSys) and much fewer (around 9\%) computing resources.
Experimental results demonstrate that conditional alignment outperforms other
offline alignment methods and is comparable to PPO.",None,-1
29b81d20-b726-4200-91e7-576e3a0da88d,CLEVA: Chinese Language Models EVAluation Platform,0.252819,7,"With the continuous emergence of Chinese Large Language Models (LLMs), how to
evaluate a model's capabilities has become an increasingly significant issue.
The absence of a comprehensive Chinese benchmark that thoroughly assesses a
model's performance, the unstandardized and incomparable prompting procedure,
and the prevalent risk of contamination pose major challenges in the current
evaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted
to holistically evaluate Chinese LLMs. Our platform employs a standardized
workflow to assess LLMs' performance across various dimensions, regularly
updating a competitive leaderboard. To alleviate contamination, CLEVA curates a
significant proportion of new data and develops a sampling strategy that
guarantees a unique subset for each leaderboard round. Empowered by an
easy-to-use interface that requires just a few mouse clicks and a model API,
users can conduct a thorough evaluation with minimal coding. Large-scale
experiments featuring 23 Chinese LLMs have validated CLEVA's efficacy.",None,-1
ac64e5b2-d4ec-4d6b-b02c-6cee7cc5b7e3,Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models,0.0741534,7,"This paper identifies a cultural dominance issue within large language models
(LLMs) due to the predominant use of English data in model training (e.g.,
ChatGPT). LLMs often provide inappropriate English-culture-related answers that
are not relevant to the expected culture when users ask in non-English
languages. To systematically evaluate the cultural dominance issue, we build a
benchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and
opinions) cultural objects. Empirical results show that the representative GPT
models suffer from the culture dominance problem, where GPT-4 is the most
affected while text-davinci-003 suffers the least from this problem. Our study
emphasizes the need to critically examine cultural dominance and ethical
consideration in their development and deployment. We show that two
straightforward methods in model development (i.e., pretraining on more diverse
data) and deployment (e.g., culture-aware prompting) can significantly mitigate
the cultural dominance issue in LLMs.",None,-1
3a3ad082-34e6-4de3-b747-d4b53a93056a,TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments,0.701866,27,"Although the estimation of 3D human pose and shape (HPS) is rapidly
progressing, current methods still cannot reliably estimate moving humans in
global coordinates, which is critical for many applications. This is
particularly challenging when the camera is also moving, entangling human and
camera motion. To address these issues, we adopt a novel 5D representation
(space, time, and identity) that enables end-to-end reasoning about people in
scenes. Our method, called TRACE, introduces several novel architectural
components. Most importantly, it uses two new ""maps"" to reason about the 3D
trajectory of people over time in camera, and world, coordinates. An additional
memory unit enables persistent tracking of people even during long occlusions.
TRACE is the first one-stage method to jointly recover and track 3D humans in
global coordinates from dynamic cameras. By training it end-to-end, and using
full image information, TRACE achieves state-of-the-art performance on tracking
and HPS benchmarks. The code and dataset are released for research purposes.",None,-1
39239bb3-6269-49c3-90d1-23cede34f3b6,IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction,0.845477,10,"Reliable multi-agent trajectory prediction is crucial for the safe planning
and control of autonomous systems. Compared with single-agent cases, the major
challenge in simultaneously processing multiple agents lies in modeling complex
social interactions caused by various driving intentions and road conditions.
Previous methods typically leverage graph-based message propagation or
attention mechanism to encapsulate such interactions in the format of marginal
probabilistic distributions. However, it is inherently sub-optimal. In this
paper, we propose IPCC-TP, a novel relevance-aware module based on Incremental
Pearson Correlation Coefficient to improve multi-agent interaction modeling.
IPCC-TP learns pairwise joint Gaussian Distributions through the
tightly-coupled estimation of the means and covariances according to
interactive incremental movements. Our module can be conveniently embedded into
existing multi-agent prediction methods to extend original motion distribution
decoders. Extensive experiments on nuScenes and Argoverse 2 datasets
demonstrate that IPCC-TP improves the performance of baselines by a large
margin.",None,-1
564f48b0-3e03-4046-bf4d-435942227500,Gloss-Free End-to-End Sign Language Translation,0.501553,8,"In this paper, we tackle the problem of sign language translation (SLT)
without gloss annotations. Although intermediate representation like gloss has
been proven effective, gloss annotations are hard to acquire, especially in
large quantities. This limits the domain coverage of translation datasets, thus
handicapping real-world applications. To mitigate this problem, we design the
Gloss-Free End-to-end sign language translation framework (GloFE). Our method
improves the performance of SLT in the gloss-free setting by exploiting the
shared underlying semantics of signs and the corresponding spoken translation.
Common concepts are extracted from the text and used as a weak form of
intermediate representation. The global embedding of these concepts is used as
a query for cross-attention to find the corresponding information within the
learned visual features. In a contrastive manner, we encourage the similarity
of query results between samples containing such concepts and decrease those
that do not. We obtained state-of-the-art results on large-scale datasets,
including OpenASL and How2Sign. The code and model will be available at
https://github.com/HenryLittle/GloFE.",None,-1
926247f7-30a1-48bc-97a9-446aa8471384,Normalizing Flow based Feature Synthesis for Outlier-Aware Object Detection,0.791661,8,"Real-world deployment of reliable object detectors is crucial for
applications such as autonomous driving. However, general-purpose object
detectors like Faster R-CNN are prone to providing overconfident predictions
for outlier objects. Recent outlier-aware object detection approaches estimate
the density of instance-wide features with class-conditional Gaussians and
train on synthesized outlier features from their low-likelihood regions.
However, this strategy does not guarantee that the synthesized outlier features
will have a low likelihood according to the other class-conditional Gaussians.
We propose a novel outlier-aware object detection framework that distinguishes
outliers from inlier objects by learning the joint data distribution of all
inlier classes with an invertible normalizing flow. The appropriate sampling of
the flow model ensures that the synthesized outliers have a lower likelihood
than inliers of all object classes, thereby modeling a better decision boundary
between inlier and outlier objects. Our approach significantly outperforms the
state-of-the-art for outlier-aware object detection on both image and video
datasets. Code available at https://github.com/nish03/FFS",None,-1
0e84dae5-857a-4313-bc5f-ba614e77b0a3,MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data,0.99889,154,"As large language models (LLMs) like OpenAI's GPT series continue to make
strides, we witness the emergence of artificial intelligence applications in an
ever-expanding range of fields. In medicine, these LLMs hold considerable
promise for improving medical workflows, diagnostics, patient care, and
education. Yet, there is an urgent need for open-source models that can be
deployed on-premises to safeguard patient privacy. In our work, we present an
innovative dataset consisting of over 160,000 entries, specifically crafted to
fine-tune LLMs for effective medical applications. We investigate the impact of
fine-tuning these datasets on publicly accessible pre-trained LLMs, and
subsequently, we juxtapose the performance of pre-trained-only models against
the fine-tuned models concerning the examinations that future medical doctors
must pass to achieve certification.",None,-1
7fd4de29-e6f4-498a-a357-494f1568020b,"Sea Ice Extraction via Remote Sensed Imagery: Algorithms, Datasets, Applications and Challenges",0.777271,3,"The deep learning, which is a dominating technique in artificial
intelligence, has completely changed the image understanding over the past
decade. As a consequence, the sea ice extraction (SIE) problem has reached a
new era. We present a comprehensive review of four important aspects of SIE,
including algorithms, datasets, applications, and the future trends. Our review
focuses on researches published from 2016 to the present, with a specific focus
on deep learning-based approaches in the last five years. We divided all
relegated algorithms into 3 categories, including classical image segmentation
approach, machine learning-based approach and deep learning-based methods. We
reviewed the accessible ice datasets including SAR-based datasets, the
optical-based datasets and others. The applications are presented in 4 aspects
including climate research, navigation, geographic information systems (GIS)
production and others. It also provides insightful observations and inspiring
future research directions.",None,-1
42c6c95a-ce32-4900-ad27-5247dc4b9c96,InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning,0.651931,20,"Inferring traffic object such as lane information is of foremost importance
for deployment of autonomous driving. Previous approaches focus on offline
construction of HD map inferred with GPS localization, which is insufficient
for globally scalable autonomous driving. To alleviate these issues, we propose
online HD map learning framework that detects HD map elements from onboard
sensor observations. We represent the map elements as a graph; we propose
InstaGraM, instance-level graph modeling of HD map that brings accurate and
fast end-to-end vectorized HD map learning. Along with the graph modeling
strategy, we propose end-to-end neural network composed of three stages: a
unified BEV feature extraction, map graph component detection, and association
via graph neural networks. Comprehensive experiments on public open dataset
show that our proposed network outperforms previous models by up to 13.7 mAP
with up to 33.8X faster computation time.",None,-1
ac43debe-c020-4f63-97f5-ae9b27dfd0ba,An Examination of the Compositionality of Large Generative Vision-Language Models,0.0502871,2,"With the success of Large Language Models (LLMs), many Generative
Vision-Language Models (GVLMs) have been constructed via multimodal instruction
tuning. However, the performance of GVLMs in multimodal compositional reasoning
remains under-explored. In this paper, we examine both the evaluation metrics
(VisualGPTScore, etc.) and current benchmarks for evaluating the
compositionality of GVLMs. We identify the syntactical bias in current
benchmarks, which is exploited by the linguistic capability of GVLMs. The bias
renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat
this, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such
bias for mitigation. A challenging new task is subsequently added to evaluate
the robustness of GVLMs against inherent inclination toward syntactical
correctness. Using the bias-mitigated datasets and the new task, we propose a
novel benchmark, namely SyntActically DE-biased benchmark (SADE). Our study
provides an unbiased benchmark for the compositionality of GVLMs, facilitating
future research in this direction (Code and dataset are available at
https://github.com/TeleeMa/SADE).",None,-1
cfe03d8b-5651-49a0-b173-8ed0a146aa8b,Class Attention Transfer Based Knowledge Distillation,0.846371,19,"Previous knowledge distillation methods have shown their impressive
performance on model compression tasks, however, it is hard to explain how the
knowledge they transferred helps to improve the performance of the student
network. In this work, we focus on proposing a knowledge distillation method
that has both high interpretability and competitive performance. We first
revisit the structure of mainstream CNN models and reveal that possessing the
capacity of identifying class discriminative regions of input is critical for
CNN to perform classification. Furthermore, we demonstrate that this capacity
can be obtained and enhanced by transferring class activation maps. Based on
our findings, we propose class attention transfer based knowledge distillation
(CAT-KD). Different from previous KD methods, we explore and present several
properties of the knowledge transferred by our method, which not only improve
the interpretability of CAT-KD but also contribute to a better understanding of
CNN. While having high interpretability, CAT-KD achieves state-of-the-art
performance on multiple benchmarks. Code is available at:
https://github.com/GzyAftermath/CAT-KD.",None,-1
87656744-a699-44b2-9085-aaa56fab126b,HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention,0.797609,31,"The success of large-scale contrastive vision-language pretraining (CLIP) has
benefited both visual recognition and multimodal content understanding. The
concise design brings CLIP the advantage in inference efficiency against other
vision-language models with heavier cross-attention fusion layers, making it a
popular choice for a wide spectrum of downstream tasks. However, CLIP does not
explicitly capture the hierarchical nature of high-level and fine-grained
semantics conveyed in images and texts, which is arguably critical to
vision-language understanding and reasoning. To this end, we equip both the
visual and language branches in CLIP with hierarchy-aware attentions, namely
Hierarchy-aware CLIP (HiCLIP), to progressively discover semantic hierarchies
layer-by-layer from both images and texts in an unsupervised manner. As a
result, such hierarchical aggregation significantly improves the cross-modal
alignment. To demonstrate the advantages of HiCLIP, we conduct qualitative
analysis on its unsupervised hierarchy induction during inference, as well as
extensive quantitative experiments on both visual recognition and
vision-language downstream tasks.",None,-1
0a7f5d7a-203b-48ca-81c9-eda30dae52a3,Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images,0.872206,46,"While multi-modal foundation models pre-trained on large-scale data have been
successful in natural language understanding and vision recognition, their use
in medical domains is still limited due to the fine-grained nature of medical
tasks and the high demand for domain knowledge. To address this challenge, we
propose a novel approach called Knowledge-enhanced Auto Diagnosis (KAD) which
leverages existing medical domain knowledge to guide vision-language
pre-training using paired chest X-rays and radiology reports. We evaluate KAD
on {four} external X-ray datasets and demonstrate that its zero-shot
performance is not only comparable to that of fully-supervised models, but also
superior to the average of three expert radiologists for three (out of five)
pathologies with statistical significance. Moreover, when few-shot annotation
is available, KAD outperforms all existing approaches in fine-tuning settings,
demonstrating its potential for application in different clinical scenarios.",None,-1
2d87d52d-73e9-4f5b-b3ac-48ce0c9290d2,Dynamic Token-Pass Transformers for Semantic Segmentation,0.150262,2,"Vision transformers (ViT) usually extract features via forwarding all the
tokens in the self-attention layers from top to toe. In this paper, we
introduce dynamic token-pass vision transformers (DoViT) for semantic
segmentation, which can adaptively reduce the inference cost for images with
different complexity. DoViT gradually stops partial easy tokens from
self-attention calculation and keeps the hard tokens forwarding until meeting
the stopping criteria. We employ lightweight auxiliary heads to make the
token-pass decision and divide the tokens into keeping/stopping parts. With a
token separate calculation, the self-attention layers are speeded up with
sparse tokens and still work friendly with hardware. A token reconstruction
module is built to collect and reset the grouped tokens to their original
position in the sequence, which is necessary to predict correct semantic masks.
We conduct extensive experiments on two common semantic segmentation tasks, and
demonstrate that our method greatly reduces about 40% $\sim$ 60% FLOPs and the
drop of mIoU is within 0.8% for various segmentation transformers. The
throughput and inference speed of ViT-L/B are increased to more than 2$\times$
on Cityscapes.",None,-1
c3645b92-f85a-4c04-9a76-d5a825bcf4ff,Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements,0.973528,26,"Despite the much discussed capabilities of today's language models, they are
still prone to silly and unexpected commonsense failures. We consider a
retrospective verification approach that reflects on the correctness of LM
outputs, and introduce Vera, a general-purpose model that estimates the
plausibility of declarative statements based on commonsense knowledge. Trained
on ~7M commonsense statements created from 19 QA datasets and two large-scale
knowledge bases, and with a combination of three training objectives, Vera is a
versatile model that effectively separates correct from incorrect statements
across diverse commonsense domains. When applied to solving commonsense
problems in the verification format, Vera substantially outperforms existing
models that can be repurposed for commonsense verification, and it further
exhibits generalization capabilities to unseen tasks and provides
well-calibrated outputs. We find that Vera excels at filtering LM-generated
commonsense knowledge and is useful in detecting erroneous commonsense
statements generated by models like ChatGPT in real-world settings.",None,-1
b5f88d49-94a6-460b-b85e-e05396dceb82,Score-Based Generative Models for Medical Image Segmentation using Signed Distance Functions,0.292948,4,"Medical image segmentation is a crucial task that relies on the ability to
accurately identify and isolate regions of interest in medical images. Thereby,
generative approaches allow to capture the statistical properties of
segmentation masks that are dependent on the respective structures. In this
work we propose a conditional score-based generative modeling framework to
represent the signed distance function (SDF) leading to an implicit
distribution of segmentation masks. The advantage of leveraging the SDF is a
more natural distortion when compared to that of binary masks. By learning the
score function of the conditional distribution of SDFs we can accurately sample
from the distribution of segmentation masks, allowing for the evaluation of
statistical quantities. Thus, this probabilistic representation allows for the
generation of uncertainty maps represented by the variance, which can aid in
further analysis and enhance the predictive robustness. We qualitatively and
quantitatively illustrate competitive performance of the proposed method on a
public nuclei and gland segmentation data set, highlighting its potential
utility in medical image segmentation applications.",None,-1
37700bb1-6916-43ac-8fad-ef602adf08c4,SikuGPT: A Generative Pre-trained Model for Intelligent Information Processing of Ancient Texts from the Perspective of Digital Humanities,0.88527,7,"The rapid advance in artificial intelligence technology has facilitated the
prosperity of digital humanities research. Against such backdrop, research
methods need to be transformed in the intelligent processing of ancient texts,
which is a crucial component of digital humanities research, so as to adapt to
new development trends in the wave of AIGC. In this study, we propose a GPT
model called SikuGPT based on the corpus of Siku Quanshu. The model's
performance in tasks such as intralingual translation and text classification
exceeds that of other GPT-type models aimed at processing ancient texts.
SikuGPT's ability to process traditional Chinese ancient texts can help promote
the organization of ancient information and knowledge services, as well as the
international dissemination of Chinese ancient culture.",None,-1
99d37568-c6fb-4c1d-b164-cce50cc18865,Physics-Preserving AI-Accelerated Simulations of Plasma Turbulence,0.730899,2,"Turbulence in fluids, gases, and plasmas remains an open problem of both
practical and fundamental importance. Its irreducible complexity usually cannot
be tackled computationally in a brute-force style. Here, we combine Large Eddy
Simulation (LES) techniques with Machine Learning (ML) to retain only the
largest dynamics explicitly, while small-scale dynamics are described by an
ML-based sub-grid-scale model. Applying this novel approach to self-driven
plasma turbulence allows us to remove large parts of the inertial range,
reducing the computational effort by about three orders of magnitude, while
retaining the statistical physical properties of the turbulent system.",None,-1
ba48f096-ae45-449f-81ea-deff0884cdeb,Chain-of-Verification Reduces Hallucination in Large Language Models,0.502733,72,"Generation of plausible yet incorrect factual information, termed
hallucination, is an unsolved issue in large language models. We study the
ability of language models to deliberate on the responses they give in order to
correct their mistakes. We develop the Chain-of-Verification (CoVe) method
whereby the model first (i) drafts an initial response; then (ii) plans
verification questions to fact-check its draft; (iii) answers those questions
independently so the answers are not biased by other responses; and (iv)
generates its final verified response. In experiments, we show CoVe decreases
hallucinations across a variety of tasks, from list-based questions from
Wikidata, closed book MultiSpanQA and longform text generation.",None,-1
7a871583-561a-4d8a-a5c6-0f76b9c5cb5a,InfoStyler: Disentanglement Information Bottleneck for Artistic Style Transfer,0.456517,2,"Artistic style transfer aims to transfer the style of an artwork to a
photograph while maintaining its original overall content. Many prior works
focus on designing various transfer modules to transfer the style statistics to
the content image. Although effective, ignoring the clear disentanglement of
the content features and the style features from the first beginning, they have
difficulty in balancing between content preservation and style transferring. To
tackle this problem, we propose a novel information disentanglement method,
named InfoStyler, to capture the minimal sufficient information for both
content and style representations from the pre-trained encoding network.
InfoStyler formulates the disentanglement representation learning as an
information compression problem by eliminating style statistics from the
content image and removing the content structure from the style image. Besides,
to further facilitate disentanglement learning, a cross-domain Information
Bottleneck (IB) learning strategy is proposed by reconstructing the content and
style domains. Extensive experiments demonstrate that our InfoStyler can
synthesize high-quality stylized images while balancing content structure
preservation and style pattern richness.",None,-1
8f6f0688-ac48-4784-9810-1fcf2226fa7b,ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters,0.580708,4,"We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via
the use of language adapters (LAs). Most of the earlier works have explored
training with adapter of a single source (often English), and testing either
using the target LA or LA of another related language. Training target LA
requires unlabeled data, which may not be readily available for low resource
unseen languages: those that are neither seen by the underlying multilingual
language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data
for them. We posit that for more effective cross-lingual transfer, instead of
just one source LA, we need to leverage LAs of multiple (linguistically or
geographically related) source languages, both at train and test-time - which
we investigate via our novel neural architecture, ZGUL. Extensive
experimentation across four language groups, covering 15 unseen target
languages, demonstrates improvements of up to 3.2 average F1 points over
standard fine-tuning and other strong baselines on POS tagging and NER tasks.
We also extend ZGUL to settings where either (1) some unlabeled data or (2)
few-shot training examples are available for the target language. We find that
ZGUL continues to outperform baselines in these settings too.",None,-1
8e4ca00d-d8e5-4e8c-84ba-c67c5eacd7a0,Bridging Physics-Informed Neural Networks with Reinforcement Learning: Hamilton-Jacobi-Bellman Proximal Policy Optimization (HJBPPO),0.592809,5,"This paper introduces the Hamilton-Jacobi-Bellman Proximal Policy
Optimization (HJBPPO) algorithm into reinforcement learning. The
Hamilton-Jacobi-Bellman (HJB) equation is used in control theory to evaluate
the optimality of the value function. Our work combines the HJB equation with
reinforcement learning in continuous state and action spaces to improve the
training of the value network. We treat the value network as a Physics-Informed
Neural Network (PINN) to solve for the HJB equation by computing its
derivatives with respect to its inputs exactly. The Proximal Policy
Optimization (PPO)-Clipped algorithm is improvised with this implementation as
it uses a value network to compute the objective function for its policy
network. The HJBPPO algorithm shows an improved performance compared to PPO on
the MuJoCo environments.",None,-1
536e9ac2-0bb3-456d-9d0c-08cdf1dea6c8,Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models,0.98266,24,"Current dialogue research primarily studies pairwise (two-party)
conversations, and does not address the everyday setting where more than two
speakers converse together. In this work, we both collect and evaluate
multi-party conversations to study this more general case. We use the LIGHT
environment to construct grounded conversations, where each participant has an
assigned character to role-play. We thus evaluate the ability of language
models to act as one or more characters in such conversations. Models require
two skills that pairwise-trained models appear to lack: (1) being able to
decide when to talk; (2) producing coherent utterances grounded on multiple
characters. We compare models trained on our new dataset to existing
pairwise-trained dialogue models, as well as large language models with
few-shot prompting. We find that our new dataset, MultiLIGHT, which we will
publicly release, can help bring significant improvements in the group setting.",None,-1
1c003e55-88cb-4eb5-ad8f-3641187ed5ae,ImageBind: One Embedding Space To Bind Them All,1.0,436,"We present ImageBind, an approach to learn a joint embedding across six
different modalities - images, text, audio, depth, thermal, and IMU data. We
show that all combinations of paired data are not necessary to train such a
joint embedding, and only image-paired data is sufficient to bind the
modalities together. ImageBind can leverage recent large scale vision-language
models, and extends their zero-shot capabilities to new modalities just by
using their natural pairing with images. It enables novel emergent applications
'out-of-the-box' including cross-modal retrieval, composing modalities with
arithmetic, cross-modal detection and generation. The emergent capabilities
improve with the strength of the image encoder and we set a new
state-of-the-art on emergent zero-shot recognition tasks across modalities,
outperforming specialist supervised models. Finally, we show strong few-shot
recognition results outperforming prior work, and that ImageBind serves as a
new way to evaluate vision models for visual and non-visual tasks.",None,-1
154a5be7-4dc6-40de-8956-868baa5a2f75,ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-based Consistency,0.69659,11,"We present ShapeClipper, a novel method that reconstructs 3D object shapes
from real-world single-view RGB images. Instead of relying on laborious 3D,
multi-view or camera pose annotation, ShapeClipper learns shape reconstruction
from a set of single-view segmented images. The key idea is to facilitate shape
learning via CLIP-based shape consistency, where we encourage objects with
similar CLIP encodings to share similar shapes. We also leverage off-the-shelf
normals as an additional geometric constraint so the model can learn better
bottom-up reasoning of detailed surface geometry. These two novel consistency
constraints, when used to regularize our model, improve its ability to learn
both global shape structure and local geometric details. We evaluate our method
over three challenging real-world datasets, Pix3D, Pascal3D+, and OpenImages,
where we achieve superior performance over state-of-the-art methods.",None,-1
4d7d5702-8ab8-42e7-8923-2753a4b10ed2,Gloss Attention for Gloss-free Sign Language Translation,0.705029,12,"Most sign language translation (SLT) methods to date require the use of gloss
annotations to provide additional supervision information, however, the
acquisition of gloss is not easy. To solve this problem, we first perform an
analysis of existing models to confirm how gloss annotations make SLT easier.
We find that it can provide two aspects of information for the model, 1) it can
help the model implicitly learn the location of semantic boundaries in
continuous sign language videos, 2) it can help the model understand the sign
language video globally. We then propose \emph{gloss attention}, which enables
the model to keep its attention within video segments that have the same
semantics locally, just as gloss helps existing models do. Furthermore, we
transfer the knowledge of sentence-to-sentence similarity from the natural
language model to our gloss attention SLT network (GASLT) to help it understand
sign language videos at the sentence level. Experimental results on multiple
large-scale sign language datasets show that our proposed GASLT model
significantly outperforms existing methods. Our code is provided in
\url{https://github.com/YinAoXiong/GASLT}.",None,-1
2d2f371b-25ca-4d1d-ba6c-abf164c99b9c,Rotation and Translation Invariant Representation Learning with Implicit Neural Representations,0.291554,1,"In many computer vision applications, images are acquired with arbitrary or
random rotations and translations, and in such setups, it is desirable to
obtain semantic representations disentangled from the image orientation.
Examples of such applications include semiconductor wafer defect inspection,
plankton microscope images, and inference on single-particle cryo-electron
microscopy (cryo-EM) micro-graphs. In this work, we propose Invariant
Representation Learning with Implicit Neural Representation (IRL-INR), which
uses an implicit neural representation (INR) with a hypernetwork to obtain
semantic representations disentangled from the orientation of the image. We
show that IRL-INR can effectively learn disentangled semantic representations
on more complex images compared to those considered in prior works and show
that these semantic representations synergize well with SCAN to produce
state-of-the-art unsupervised clustering results.",None,-1
75eb3021-7641-49d5-aa78-41843ccc11fe,History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling,0.185961,3,"Context information modeling is an important task in conversational KBQA.
However, existing methods usually assume the independence of utterances and
model them in isolation. In this paper, we propose a History Semantic Graph
Enhanced KBQA model (HSGE) that is able to effectively model long-range
semantic dependencies in conversation history while maintaining low
computational cost. The framework incorporates a context-aware encoder, which
employs a dynamic memory decay mechanism and models context at different levels
of granularity. We evaluate HSGE on a widely used benchmark dataset for complex
sequential question answering. Experimental results demonstrate that it
outperforms existing baselines averaged on all question types.",None,-1
0889e5b3-68aa-4c1c-8d16-5e540f60ee99,Passive Radio Frequency-based 3D Indoor Positioning System via Ensemble Learning,0.832189,4,"Passive radio frequency (PRF)-based indoor positioning systems (IPS) have
attracted researchers' attention due to their low price, easy and customizable
configuration, and non-invasive design. This paper proposes a PRF-based
three-dimensional (3D) indoor positioning system (PIPS), which is able to use
signals of opportunity (SoOP) for positioning and also capture a scenario
signature. PIPS passively monitors SoOPs containing scenario signatures through
a single receiver. Moreover, PIPS leverages the Dynamic Data Driven
Applications System (DDDAS) framework to devise and customize the sampling
frequency, enabling the system to use the most impacted frequency band as the
rated frequency band. Various regression methods within three ensemble learning
strategies are used to train and predict the receiver position. The PRF
spectrum of 60 positions is collected in the experimental scenario, and three
criteria are applied to evaluate the performance of PIPS. Experimental results
show that the proposed PIPS possesses the advantages of high accuracy,
configurability, and robustness.",None,-1
acc72f12-3b82-4148-9943-6b2450be3040,Measuring Classification Decision Certainty and Doubt,0.388971,2,"Quantitative characterizations and estimations of uncertainty are of
fundamental importance in optimization and decision-making processes. Herein,
we propose intuitive scores, which we call certainty and doubt, that can be
used in both a Bayesian and frequentist framework to assess and compare the
quality and uncertainty of predictions in (multi-)classification decision
machine learning problems.",None,-1
c280c954-afbb-4f67-865d-a498736cded4,Measuring axiomatic soundness of counterfactual image models,0.907701,16,"We present a general framework for evaluating image counterfactuals. The
power and flexibility of deep generative models make them valuable tools for
learning mechanisms in structural causal models. However, their flexibility
makes counterfactual identifiability impossible in the general case. Motivated
by these issues, we revisit Pearl's axiomatic definition of counterfactuals to
determine the necessary constraints of any counterfactual inference model:
composition, reversibility, and effectiveness. We frame counterfactuals as
functions of an input variable, its parents, and counterfactual parents and use
the axiomatic constraints to restrict the set of functions that could represent
the counterfactual, thus deriving distance metrics between the approximate and
ideal functions. We demonstrate how these metrics can be used to compare and
choose between different approximate counterfactual inference models and to
provide insight into a model's shortcomings and trade-offs.",None,-1
58b8df8e-e81f-4687-a5eb-d597066cf750,Abstract Visual Reasoning Enabled by Language,0.190403,7,"While artificial intelligence (AI) models have achieved human or even
superhuman performance in many well-defined applications, they still struggle
to show signs of broad and flexible intelligence. The Abstraction and Reasoning
Corpus (ARC), a visual intelligence benchmark introduced by Fran\c{c}ois
Chollet, aims to assess how close AI systems are to human-like cognitive
abilities. Most current approaches rely on carefully handcrafted
domain-specific program searches to brute-force solutions for the tasks present
in ARC. In this work, we propose a general learning-based framework for solving
ARC. It is centered on transforming tasks from the vision to the language
domain. This composition of language and vision allows for pre-trained models
to be leveraged at each stage, enabling a shift from handcrafted priors towards
the learned priors of the models. While not yet beating state-of-the-art models
on ARC, we demonstrate the potential of our approach, for instance, by solving
some ARC tasks that have not been solved previously.",None,-1
597c84ec-21b9-4a65-8f8c-a6db029d9ad6,Inferring Capabilities from Task Performance with Bayesian Triangulation,0.905794,4,"As machine learning models become more general, we need to characterise them
in richer, more meaningful ways. We describe a method to infer the cognitive
profile of a system from diverse experimental data. To do so, we introduce
measurement layouts that model how task-instance features interact with system
capabilities to affect performance. These features must be triangulated in
complex ways to be able to infer capabilities from non-populational data -- a
challenge for traditional psychometric and inferential tools. Using the
Bayesian probabilistic programming library PyMC, we infer different cognitive
profiles for agents in two scenarios: 68 actual contestants in the AnimalAI
Olympics and 30 synthetic agents for O-PIAAGETS, an object permanence battery.
We showcase the potential for capability-oriented evaluation.",None,-1
2967aa29-43d2-4c45-8cde-bbe068af0da9,Automatic Truss Design with Reinforcement Learning,0.318084,1,"Truss layout design, namely finding a lightweight truss layout satisfying all
the physical constraints, is a fundamental problem in the building industry.
Generating the optimal layout is a challenging combinatorial optimization
problem, which can be extremely expensive to solve by exhaustive search.
Directly applying end-to-end reinforcement learning (RL) methods to truss
layout design is infeasible either, since only a tiny portion of the entire
layout space is valid under the physical constraints, leading to particularly
sparse rewards for RL training. In this paper, we develop AutoTruss, a
two-stage framework to efficiently generate both lightweight and valid truss
layouts. AutoTruss first adopts Monte Carlo tree search to discover a diverse
collection of valid layouts. Then RL is applied to iteratively refine the valid
solutions. We conduct experiments and ablation studies in popular truss layout
design test cases in both 2D and 3D settings. AutoTruss outperforms the
best-reported layouts by 25.1% in the most challenging 3D test cases, resulting
in the first effective deep-RL-based approach in the truss layout design
literature.",None,-1
c93a8b67-735f-443c-8ed6-d6e3b33902b5,A Universal Question-Answering Platform for Knowledge Graphs,0.546348,14,"Knowledge from diverse application domains is organized as knowledge graphs
(KGs) that are stored in RDF engines accessible in the web via SPARQL
endpoints. Expressing a well-formed SPARQL query requires information about the
graph structure and the exact URIs of its components, which is impractical for
the average user. Question answering (QA) systems assist by translating natural
language questions to SPARQL. Existing QA systems are typically based on
application-specific human-curated rules, or require prior information,
expensive pre-processing and model adaptation for each targeted KG. Therefore,
they are hard to generalize to a broad set of applications and KGs.
  In this paper, we propose KGQAn, a universal QA system that does not need to
be tailored to each target KG. Instead of curated rules, KGQAn introduces a
novel formalization of question understanding as a text generation problem to
convert a question into an intermediate abstract representation via a neural
sequence-to-sequence model. We also develop a just-in-time linker that maps at
query time the abstract representation to a SPARQL query for a specific KG,
using only the publicly accessible APIs and the existing indices of the RDF
store, without requiring any pre-processing. Our experiments with several real
KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin
the state-of-the-art in terms of quality of answers and processing time,
especially for arbitrary KGs, unseen during the training.",None,-1
38bbfc99-f1c4-4722-a93f-82bd31e512ef,XFEVER: Exploring Fact Verification across Languages,0.355132,1,"This paper introduces the Cross-lingual Fact Extraction and VERification
(XFEVER) dataset designed for benchmarking the fact verification models across
different languages. We constructed it by translating the claim and evidence
texts of the Fact Extraction and VERification (FEVER) dataset into six
languages. The training and development sets were translated using machine
translation, whereas the test set includes texts translated by professional
translators and machine-translated texts. Using the XFEVER dataset, two
cross-lingual fact verification scenarios, zero-shot learning and
translate-train learning, are defined, and baseline models for each scenario
are also proposed in this paper. Experimental results show that the
multilingual language model can be used to build fact verification models in
different languages efficiently. However, the performance varies by language
and is somewhat inferior to the English case. We also found that we can
effectively mitigate model miscalibration by considering the prediction
similarity between the English and target languages. The XFEVER dataset, code,
and model checkpoints are available at
https://github.com/nii-yamagishilab/xfever.",None,-1
54574fe7-ca60-4ebd-9792-0f397ad3f1d5,Few-Shot 3D Point Cloud Semantic Segmentation via Stratified Class-Specific Attention Based Transformer Network,0.597376,5,"3D point cloud semantic segmentation aims to group all points into different
semantic categories, which benefits important applications such as point cloud
scene reconstruction and understanding. Existing supervised point cloud
semantic segmentation methods usually require large-scale annotated point
clouds for training and cannot handle new categories. While a few-shot learning
method was proposed recently to address these two problems, it suffers from
high computational complexity caused by graph construction and inability to
learn fine-grained relationships among points due to the use of pooling
operations. In this paper, we further address these problems by developing a
new multi-layer transformer network for few-shot point cloud semantic
segmentation. In the proposed network, the query point cloud features are
aggregated based on the class-specific support features in different scales.
Without using pooling operations, our method makes full use of all pixel-level
features from the support samples. By better leveraging the support features
for few-shot learning, the proposed method achieves the new state-of-the-art
performance, with 15\% less inference time, over existing few-shot 3D point
cloud segmentation models on the S3DIS dataset and the ScanNet dataset.",None,-1
f639aac4-3dc7-49a2-a1dd-984a16ea7443,Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?,0.732865,11,"Much discussion about large language models and language-and-vision models
has focused on whether these models are intelligent agents. We present an
alternative perspective. We argue that these artificial intelligence models are
cultural technologies that enhance cultural transmission in the modern world,
and are efficient imitation engines. We explore what AI models can tell us
about imitation and innovation by evaluating their capacity to design new tools
and discover novel causal structures, and contrast their responses with those
of human children. Our work serves as a first step in determining which
particular representations and competences, as well as which kinds of knowledge
or skill, can be derived from particular learning techniques and data.
Critically, our findings suggest that machines may need more than large scale
language and images to achieve what a child can do.",None,-1
9dffd89c-21f9-4daa-b076-99892537fa44,Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation,0.998716,36,"Accurate segmentation of topological tubular structures, such as blood
vessels and roads, is crucial in various fields, ensuring accuracy and
efficiency in downstream tasks. However, many factors complicate the task,
including thin local structures and variable global morphologies. In this work,
we note the specificity of tubular structures and use this knowledge to guide
our DSCNet to simultaneously enhance perception in three stages: feature
extraction, feature fusion, and loss constraint. First, we propose a dynamic
snake convolution to accurately capture the features of tubular structures by
adaptively focusing on slender and tortuous local structures. Subsequently, we
propose a multi-view feature fusion strategy to complement the attention to
features from multiple perspectives during feature fusion, ensuring the
retention of important information from different global morphologies. Finally,
a continuity constraint loss function, based on persistent homology, is
proposed to constrain the topological continuity of the segmentation better.
Experiments on 2D and 3D datasets show that our DSCNet provides better accuracy
and continuity on the tubular structure segmentation task compared with several
methods. Our codes will be publicly available.",None,-1
a8b1ea0e-3407-4dc4-848c-1272e3811fc0,LLaMA: Open and Efficient Foundation Language Models,1.0,6582,"We introduce LLaMA, a collection of foundation language models ranging from
7B to 65B parameters. We train our models on trillions of tokens, and show that
it is possible to train state-of-the-art models using publicly available
datasets exclusively, without resorting to proprietary and inaccessible
datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,
and LLaMA-65B is competitive with the best models, Chinchilla-70B and
PaLM-540B. We release all our models to the research community.",None,-1
3e293699-ff48-49d4-8aaf-5025f72554f0,POPGym: Benchmarking Partially Observable Reinforcement Learning,0.998967,23,"Real world applications of Reinforcement Learning (RL) are often partially
observable, thus requiring memory. Despite this, partial observability is still
largely ignored by contemporary RL benchmarks and libraries. We introduce
Partially Observable Process Gym (POPGym), a two-part library containing (1) a
diverse collection of 15 partially observable environments, each with multiple
difficulties and (2) implementations of 13 memory model baselines -- the most
in a single RL library. Existing partially observable benchmarks tend to fixate
on 3D visual navigation, which is computationally expensive and only one type
of POMDP. In contrast, POPGym environments are diverse, produce smaller
observations, use less memory, and often converge within two hours of training
on a consumer-grade GPU. We implement our high-level memory API and memory
baselines on top of the popular RLlib framework, providing plug-and-play
compatibility with various training algorithms, exploration strategies, and
distributed training paradigms. Using POPGym, we execute the largest comparison
across RL memory models to date. POPGym is available at
https://github.com/proroklab/popgym.",None,-1
d81c8979-fa7c-4e82-b2a9-f66da55baca3,Robustness of Segment Anything Model (SAM) for Autonomous Driving in Adverse Weather Conditions,0.289185,5,"Segment Anything Model (SAM) has gained considerable interest in recent times
for its remarkable performance and has emerged as a foundational model in
computer vision. It has been integrated in diverse downstream tasks, showcasing
its strong zero-shot transfer capabilities. Given its impressive performance,
there is a strong desire to apply SAM in autonomous driving to improve the
performance of vision tasks, particularly in challenging scenarios such as
driving under adverse weather conditions. However, its robustness under adverse
weather conditions remains uncertain. In this work, we investigate the
application of SAM in autonomous driving and specifically explore its
robustness under adverse weather conditions. Overall, this work aims to enhance
understanding of SAM's robustness in challenging scenarios before integrating
it into autonomous driving vision tasks, providing valuable insights for future
applications.",None,-1
496acd16-bf33-402d-ba3c-477802a8963d,Denoising Diffusion Autoencoders are Unified Self-supervised Learners,0.643802,23,"Inspired by recent advances in diffusion models, which are reminiscent of
denoising autoencoders, we investigate whether they can acquire discriminative
representations for classification via generative pre-training. This paper
shows that the networks in diffusion models, namely denoising diffusion
autoencoders (DDAE), are unified self-supervised learners: by pre-training on
unconditional image generation, DDAE has already learned strongly
linear-separable representations within its intermediate layers without
auxiliary encoders, thus making diffusion pre-training emerge as a general
approach for generative-and-discriminative dual learning. To validate this, we
conduct linear probe and fine-tuning evaluations. Our diffusion-based approach
achieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and
Tiny-ImageNet, respectively, and is comparable to contrastive learning and
masked autoencoders for the first time. Transfer learning from ImageNet also
confirms the suitability of DDAE for Vision Transformers, suggesting the
potential to scale DDAEs as unified foundation models. Code is available at
github.com/FutureXiang/ddae.",None,-1
9246689d-0f04-4f97-a2fd-28ebd7357e0a,The 1st-place Solution for CVPR 2023 OpenLane Topology in Autonomous Driving Challenge,0.238964,4,"We present the 1st-place solution of OpenLane Topology in Autonomous Driving
Challenge. Considering that topology reasoning is based on centerline detection
and traffic element detection, we develop a multi-stage framework for high
performance. Specifically, the centerline is detected by the powerful PETRv2
detector and the popular YOLOv8 is employed to detect the traffic elements.
Further, we design a simple yet effective MLP-based head for topology
prediction. Our method achieves 55\% OLS on the OpenLaneV2 test set, surpassing
the 2nd solution by 8 points.",None,-1
8d0cf620-d723-49d2-b491-847027a6f26c,eXplainable Artificial Intelligence (XAI) in aging clock models,0.50448,5,"eXplainable Artificial Intelligence (XAI) is a rapidly progressing field of
machine learning, aiming to unravel the predictions of complex models. XAI is
especially required in sensitive applications, e.g. in health care, when
diagnosis, recommendations and treatment choices might rely on the decisions
made by artificial intelligence systems. AI approaches have become widely used
in aging research as well, in particular, in developing biological clock models
and identifying biomarkers of aging and age-related diseases. However, the
potential of XAI here awaits to be fully appreciated. We discuss the
application of XAI for developing the ""aging clocks"" and present a
comprehensive analysis of the literature categorized by the focus on particular
physiological systems.",None,-1
0e5f884a-4a43-4d35-ba1c-ba3878d58781,Syntax-semantics interface: an algebraic model,0.449537,1,"We extend our formulation of Merge and Minimalism in terms of Hopf algebras
to an algebraic model of a syntactic-semantic interface. We show that methods
adopted in the formulation of renormalization (extraction of meaningful
physical values) in theoretical physics are relevant to describe the extraction
of meaning from syntactic expressions. We show how this formulation relates to
computational models of semantics and we answer some recent controversies about
implications for generative linguistics of the current functioning of large
language models.",None,-1
2f4ce541-bc44-4c7b-b450-e60c170c5090,"If our aim is to build morality into an artificial agent, how might we begin to go about doing so?",0.584772,4,"As Artificial Intelligence (AI) becomes pervasive in most fields, from
healthcare to autonomous driving, it is essential that we find successful ways
of building morality into our machines, especially for decision-making.
However, the question of what it means to be moral is still debated,
particularly in the context of AI. In this paper, we highlight the different
aspects that should be considered when building moral agents, including the
most relevant moral paradigms and challenges. We also discuss the top-down and
bottom-up approaches to design and the role of emotion and sentience in
morality. We then propose solutions including a hybrid approach to design and a
hierarchical approach to combining moral paradigms. We emphasize how governance
and policy are becoming ever more critical in AI Ethics and in ensuring that
the tasks we set for moral agents are attainable, that ethical behavior is
achieved, and that we obtain good AI.",None,-1
56d735c1-d95a-48ed-9060-17725e72ccf3,CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data Generation,0.605424,3,"Conversational search provides a natural interface for information retrieval
(IR). Recent approaches have demonstrated promising results in applying dense
retrieval to conversational IR. However, training dense retrievers requires
large amounts of in-domain paired data. This hinders the development of
conversational dense retrievers, as abundant in-domain conversations are
expensive to collect. In this paper, we propose CONVERSER, a framework for
training conversational dense retrievers with at most 6 examples of in-domain
dialogues. Specifically, we utilize the in-context learning capability of large
language models to generate conversational queries given a passage in the
retrieval corpus. Experimental results on conversational retrieval benchmarks
OR-QuAC and TREC CAsT 19 show that the proposed CONVERSER achieves comparable
performance to fully-supervised models, demonstrating the effectiveness of our
proposed framework in few-shot conversational dense retrieval. All source code
and generated datasets are available at https://github.com/MiuLab/CONVERSER",None,-1
7fbf29a2-82a5-4c6a-afa2-e6475db99168,From Chaos to Clarity: Claim Normalization to Empower Fact-Checking,0.673253,5,"With the rise of social media, users are exposed to many misleading claims.
However, the pervasive noise inherent in these posts presents a challenge in
identifying precise and prominent claims that require verification. Extracting
the important claims from such posts is arduous and time-consuming, yet it is
an underexplored problem. Here, we aim to bridge this gap. We introduce a novel
task, Claim Normalization (aka ClaimNorm), which aims to decompose complex and
noisy social media posts into more straightforward and understandable forms,
termed normalized claims. We propose CACN, a pioneering approach that leverages
chain-of-thought and claim check-worthiness estimation, mimicking human
reasoning processes, to comprehend intricate claims. Moreover, we capitalize on
the in-context learning capabilities of large language models to provide
guidance and to improve claim normalization. To evaluate the effectiveness of
our proposed model, we meticulously compile a comprehensive real-world dataset,
CLAN, comprising more than 6k instances of social media posts alongside their
respective normalized claims. Our experiments demonstrate that CACN outperforms
several baselines across various evaluation measures. Finally, our rigorous
error analysis validates CACN's capabilities and pitfalls.",None,-1
2c013846-3960-485f-9614-95c8dfdf6609,BandRe: Rethinking Band-Pass Filters for Scale-Wise Object Detection Evaluation,0.0469814,1,"Scale-wise evaluation of object detectors is important for real-world
applications. However, existing metrics are either coarse or not sufficiently
reliable. In this paper, we propose novel scale-wise metrics that strike a
balance between fineness and reliability, using a filter bank consisting of
triangular and trapezoidal band-pass filters. We conduct experiments with two
methods on two datasets and show that the proposed metrics can highlight the
differences between the methods and between the datasets. Code is available at
https://github.com/shinya7y/UniverseNet .",None,-1
387a6210-d4f4-494d-b9e9-215ca9917b0b,How Good is the Model in Model-in-the-loop Event Coreference Resolution Annotation?,0.743904,4,"Annotating cross-document event coreference links is a time-consuming and
cognitively demanding task that can compromise annotation quality and
efficiency. To address this, we propose a model-in-the-loop annotation approach
for event coreference resolution, where a machine learning model suggests
likely corefering event pairs only. We evaluate the effectiveness of this
approach by first simulating the annotation process and then, using a novel
annotator-centric Recall-Annotation effort trade-off metric, we compare the
results of various underlying models and datasets. We finally present a method
for obtaining 97\% recall while substantially reducing the workload required by
a fully manual annotation process. Code and data can be found at
https://github.com/ahmeshaf/model_in_coref",None,-1
7405b93c-e79a-44ca-8047-04d31b419377,SparseVSR: Lightweight and Noise Robust Visual Speech Recognition,0.333421,2,"Recent advances in deep neural networks have achieved unprecedented success
in visual speech recognition. However, there remains substantial disparity
between current methods and their deployment in resource-constrained devices.
In this work, we explore different magnitude-based pruning techniques to
generate a lightweight model that achieves higher performance than its dense
model equivalent, especially under the presence of visual noise. Our sparse
models achieve state-of-the-art results at 10% sparsity on the LRS3 dataset and
outperform the dense equivalent up to 70% sparsity. We evaluate our 50% sparse
model on 7 different visual noise types and achieve an overall absolute
improvement of more than 2% WER compared to the dense equivalent. Our results
confirm that sparse networks are more resistant to noise than dense networks.",None,-1
955a359c-7cbc-4c41-bea9-fbb413ad62ad,RelPose++: Recovering 6D Poses from Sparse-view Observations,0.982951,25,"We address the task of estimating 6D camera poses from sparse-view image sets
(2-8 images). This task is a vital pre-processing stage for nearly all
contemporary (neural) reconstruction algorithms but remains challenging given
sparse views, especially for objects with visual symmetries and texture-less
surfaces. We build on the recent RelPose framework which learns a network that
infers distributions over relative rotations over image pairs. We extend this
approach in two key ways; first, we use attentional transformer layers to
process multiple images jointly, since additional views of an object may
resolve ambiguous symmetries in any given image pair (such as the handle of a
mug that becomes visible in a third view). Second, we augment this network to
also report camera translations by defining an appropriate coordinate system
that decouples the ambiguity in rotation estimation from translation
prediction. Our final system results in large improvements in 6D pose
prediction over prior art on both seen and unseen object categories and also
enables pose estimation and 3D reconstruction for in-the-wild objects.",None,-1
4bdae3bc-b59b-413f-a530-3e4c3f9aa515,AutoDroid: LLM-powered Task Automation in Android,0.87099,16,"Mobile task automation is an attractive technique that aims to enable
voice-based hands-free user interaction with smartphones. However, existing
approaches suffer from poor scalability due to the limited language
understanding ability and the non-trivial manual efforts required from
developers or end-users. The recent advance of large language models (LLMs) in
language understanding and reasoning inspires us to rethink the problem from a
model-centric perspective, where task preparation, comprehension, and execution
are handled by a unified language model. In this work, we introduce AutoDroid,
a mobile task automation system capable of handling arbitrary tasks on any
Android application without manual efforts. The key insight is to combine the
commonsense knowledge of LLMs and domain-specific knowledge of apps through
automated dynamic analysis. The main components include a functionality-aware
UI representation method that bridges the UI with the LLM, exploration-based
memory injection techniques that augment the app-specific domain knowledge of
LLM, and a multi-granularity query optimization module that reduces the cost of
model inference. We integrate AutoDroid with off-the-shelf LLMs including
online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a
new benchmark for memory-augmented Android task automation with 158 common
tasks. The results demonstrated that AutoDroid is able to precisely generate
actions with an accuracy of 90.9%, and complete tasks with a success rate of
71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo,
benchmark suites, and source code of AutoDroid will be released at
url{https://autodroid-sys.github.io/}.",None,-1
43d2a314-a54b-4d7d-99e2-0b0842dd59e4,An Empirical Evaluation of Federated Contextual Bandit Algorithms,0.470348,2,"As the adoption of federated learning increases for learning from sensitive
data local to user devices, it is natural to ask if the learning can be done
using implicit signals generated as users interact with the applications of
interest, rather than requiring access to explicit labels which can be
difficult to acquire in many tasks. We approach such problems with the
framework of federated contextual bandits, and develop variants of prominent
contextual bandit algorithms from the centralized seting for the federated
setting. We carefully evaluate these algorithms in a range of scenarios
simulated using publicly available datasets. Our simulations model typical
setups encountered in the real-world, such as various misalignments between an
initial pre-trained model and the subsequent user interactions due to
non-stationarity in the data and/or heterogeneity across clients. Our
experiments reveal the surprising effectiveness of the simple and commonly used
softmax heuristic in balancing the well-know exploration-exploitation tradeoff
across the breadth of our settings.",None,-1
404d2233-ed1b-4a2e-96ec-81f8280a4d62,GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text Summarization,0.0153406,1,"Human experts write summaries using different techniques, including
extracting a sentence from the document and rewriting it, or fusing various
information from the document to abstract it. These techniques are flexible and
thus difficult to be imitated by any single method. To address this issue, we
propose an adaptive model, GEMINI, that integrates a rewriter and a generator
to mimic the sentence rewriting and abstracting techniques, respectively.
GEMINI adaptively chooses to rewrite a specific document sentence or generate a
summary sentence from scratch. Experiments demonstrate that our adaptive
approach outperforms the pure abstractive and rewriting baselines on three
benchmark datasets, achieving the best results on WikiHow. Interestingly,
empirical results show that the human summary styles of summary sentences are
consistently predictable given their context. We release our code and model at
\url{https://github.com/baoguangsheng/gemini}.",None,-1
add603cf-6c6e-43ce-8ff1-35a4d1c1a589,Dynamic fairness-aware recommendation through multi-agent social choice,0.276668,2,"Algorithmic fairness in the context of personalized recommendation presents
significantly different challenges to those commonly encountered in
classification tasks. Researchers studying classification have generally
considered fairness to be a matter of achieving equality of outcomes between a
protected and unprotected group, and built algorithmic interventions on this
basis. We argue that fairness in real-world application settings in general,
and especially in the context of personalized recommendation, is much more
complex and multi-faceted, requiring a more general approach. We propose a
model to formalize multistakeholder fairness in recommender systems as a two
stage social choice problem. In particular, we express recommendation fairness
as a novel combination of an allocation and an aggregation problem, which
integrate both fairness concerns and personalized recommendation provisions,
and derive new recommendation techniques based on this formulation. Simulations
demonstrate the ability of the framework to integrate multiple fairness
concerns in a dynamic way.",None,-1
ca0ecef1-0307-4543-a73b-8bd5e7e98355,Modality-invariant Visual Odometry for Embodied Vision,0.071517,2,"Effectively localizing an agent in a realistic, noisy setting is crucial for
many embodied vision tasks. Visual Odometry (VO) is a practical substitute for
unreliable GPS and compass sensors, especially in indoor environments. While
SLAM-based methods show a solid performance without large data requirements,
they are less flexible and robust w.r.t. to noise and changes in the sensor
suite compared to learning-based approaches. Recent deep VO models, however,
limit themselves to a fixed set of input modalities, e.g., RGB and depth, while
training on millions of samples. When sensors fail, sensor suites change, or
modalities are intentionally looped out due to available resources, e.g., power
consumption, the models fail catastrophically. Furthermore, training these
models from scratch is even more expensive without simulator access or suitable
existing models that can be fine-tuned. While such scenarios get mostly ignored
in simulation, they commonly hinder a model's reusability in real-world
applications. We propose a Transformer-based modality-invariant VO approach
that can deal with diverse or changing sensor suites of navigation agents. Our
model outperforms previous methods while training on only a fraction of the
data. We hope this method opens the door to a broader range of real-world
applications that can benefit from flexible and learned VO models.",None,-1
36ab12a9-abd5-445e-9059-0dd95ace1107,Prompt Engineering a Prompt Engineer,0.637058,9,"Prompt engineering is a challenging yet crucial task for optimizing the
performance of large language models on customized tasks. It requires complex
reasoning to examine the model's errors, hypothesize what is missing or
misleading in the current prompt, and communicate the task with clarity. While
recent works indicate that large language models can be meta-prompted to
perform automatic prompt engineering, we argue that their potential is limited
due to insufficient guidance for complex reasoning in the meta-prompt. We fill
this gap by infusing into the meta-prompt three key components: detailed
descriptions, context specification, and a step-by-step reasoning template. The
resulting method, named PE2, showcases remarkable versatility across diverse
language tasks. It finds prompts that outperform ""let's think step by step"" by
6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on
counterfactual tasks by 6.9%. Further, we show that PE2 can make targeted
prompt edits, rectify erroneous prompts, and induce multi-step plans for
complex tasks.",None,-1
567cbff5-d614-41cb-bbf3-eaa1a9a53b6c,Octopus: A Multitask Model and Toolkit for Arabic Natural Language Generation,0.0760653,1,"Understanding Arabic text and generating human-like responses is a
challenging endeavor. While many researchers have proposed models and solutions
for individual problems, there is an acute shortage of a comprehensive Arabic
natural language generation toolkit that is capable of handling a wide range of
tasks. In this work, we present a novel Arabic text-to-text Transformer model,
namely AraT5v2. Our new model is methodically trained on extensive and diverse
data, utilizing an extended sequence length of 2,048 tokens. We explore various
pretraining strategies including unsupervised, supervised, and joint
pertaining, under both single and multitask settings. Our models outperform
competitive baselines with large margins. We take our work one step further by
developing and publicly releasing Octopus, a Python-based package and
command-line toolkit tailored for eight Arabic generation tasks all exploiting
a single model. We release the models and the toolkit on our public repository.",None,-1
c8ba3421-433d-4d15-a0e0-88f33aee54d9,Distilled Reverse Attention Network for Open-world Compositional Zero-Shot Learning,0.0999963,3,"Open-World Compositional Zero-Shot Learning (OW-CZSL) aims to recognize new
compositions of seen attributes and objects. In OW-CZSL, methods built on the
conventional closed-world setting degrade severely due to the unconstrained OW
test space. While previous works alleviate the issue by pruning compositions
according to external knowledge or correlations in seen pairs, they introduce
biases that harm the generalization. Some methods thus predict state and object
with independently constructed and trained classifiers, ignoring that
attributes are highly context-dependent and visually entangled with objects. In
this paper, we propose a novel Distilled Reverse Attention Network to address
the challenges. We also model attributes and objects separately but with
different motivations, capturing contextuality and locality, respectively. We
further design a reverse-and-distill strategy that learns disentangled
representations of elementary components in training data supervised by reverse
attention and knowledge distillation. We conduct experiments on three datasets
and consistently achieve state-of-the-art (SOTA) performance.",None,-1
8378eb07-b7fa-4b4a-b9d4-dbb34d4a8850,On the Importance of Signer Overlap for Sign Language Detection,0.109127,2,"Sign language detection, identifying if someone is signing or not, is
becoming crucially important for its applications in remote conferencing
software and for selecting useful sign data for training sign language
recognition or translation tasks. We argue that the current benchmark data sets
for sign language detection estimate overly positive results that do not
generalize well due to signer overlap between train and test partitions. We
quantify this with a detailed analysis of the effect of signer overlap on
current sign detection benchmark data sets. Comparing accuracy with and without
overlap on the DGS corpus and Signing in the Wild, we observed a relative
decrease in accuracy of 4.17% and 6.27%, respectively. Furthermore, we propose
new data set partitions that are free of overlap and allow for more realistic
performance assessment. We hope this work will contribute to improving the
accuracy and generalization of sign language detection systems.",None,-1
f4052ae8-3c25-4c97-8da8-7f7931b6303a,A Blackbox Approach to Best of Both Worlds in Bandits and Beyond,0.631179,16,"Best-of-both-worlds algorithms for online learning which achieve near-optimal
regret in both the adversarial and the stochastic regimes have received growing
attention recently. Existing techniques often require careful adaptation to
every new problem setup, including specialised potentials and careful tuning of
algorithm parameters. Yet, in domains such as linear bandits, it is still
unknown if there exists an algorithm that can simultaneously obtain
$O(\log(T))$ regret in the stochastic regime and $\tilde{O}(\sqrt{T})$ regret
in the adversarial regime. In this work, we resolve this question positively
and present a general reduction from best of both worlds to a wide family of
follow-the-regularized-leader (FTRL) and online-mirror-descent (OMD)
algorithms. We showcase the capability of this reduction by transforming
existing algorithms that are only known to achieve worst-case guarantees into
new algorithms with best-of-both-worlds guarantees in contextual bandits, graph
bandits and tabular Markov decision processes.",None,-1
3382d2e5-fad7-43da-965a-61898bec5ec3,Human Feedback is not Gold Standard,0.573444,26,"Human feedback has become the de facto standard for evaluating the
performance of Large Language Models, and is increasingly being used as a
training objective. However, it is not clear which properties of a generated
output this single `preference' score captures. We hypothesise that preference
scores are subjective and open to undesirable biases. We critically analyse the
use of human feedback for both training and evaluation, to verify whether it
fully captures a range of crucial error criteria. We find that while preference
scores have fairly good coverage, they under-represent important aspects like
factuality. We further hypothesise that both preference scores and error
annotation may be affected by confounders, and leverage instruction-tuned
models to generate outputs that vary along two possible confounding dimensions:
assertiveness and complexity. We find that the assertiveness of an output skews
the perceived rate of factuality errors, indicating that human annotations are
not a fully reliable evaluation metric or training objective. Finally, we offer
preliminary evidence that using human feedback as a training objective
disproportionately increases the assertiveness of model outputs. We encourage
future work to carefully consider whether preference scores are well aligned
with the desired objective.",None,-1
b456ed17-e4a4-4f77-bfe0-9361e76147f9,ViT2EEG: Leveraging Hybrid Pretrained Vision Transformers for EEG Data,0.53499,3,"In this study, we demonstrate the application of a hybrid Vision Transformer
(ViT) model, pretrained on ImageNet, on an electroencephalogram (EEG)
regression task. Despite being originally trained for image classification
tasks, when fine-tuned on EEG data, this model shows a notable increase in
performance compared to other models, including an identical architecture ViT
trained without the ImageNet weights. This discovery challenges the traditional
understanding of model generalization, suggesting that Transformer models
pretrained on seemingly unrelated image data can provide valuable priors for
EEG regression tasks with an appropriate fine-tuning pipeline.
  The success of this approach suggests that the features extracted by ViT
models in the context of visual tasks can be readily transformed for the
purpose of EEG predictive modeling. We recommend utilizing this methodology not
only in neuroscience and related fields, but generally for any task where data
collection is limited by practical, financial, or ethical constraints. Our
results illuminate the potential of pretrained models on tasks that are clearly
distinct from their original purpose.",None,-1
6559fc97-e3eb-4b47-b1dc-12616404fb8f,Optimized Custom Dataset for Efficient Detection of Underwater Trash,0.580041,2,"Accurately quantifying and removing submerged underwater waste plays a
crucial role in safeguarding marine life and preserving the environment. While
detecting floating and surface debris is relatively straightforward,
quantifying submerged waste presents significant challenges due to factors like
light refraction, absorption, suspended particles, and color distortion. This
paper addresses these challenges by proposing the development of a custom
dataset and an efficient detection approach for submerged marine debris. The
dataset encompasses diverse underwater environments and incorporates
annotations for precise labeling of debris instances. Ultimately, the primary
objective of this custom dataset is to enhance the diversity of litter
instances and improve their detection accuracy in deep submerged environments
by leveraging state-of-the-art deep learning architectures.",None,-1
1e15785b-df4c-4240-bf0d-fcbb49bb4490,Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training,0.130355,4,"Parallel training of neural networks at scale is challenging due to
significant overheads arising from communication. Recently, deep learning
researchers have developed a variety of pruning algorithms that are capable of
pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to
yield sparse subnetworks that equal the accuracy of the unpruned parent
network. In this work, we propose a novel approach that exploits these sparse
subnetworks to optimize the memory utilization and communication in two popular
algorithms for parallel deep learning namely -- data and inter-layer
parallelism. We integrate our approach into AxoNN, a highly scalable framework
for parallel deep learning that relies on data and inter-layer parallelism, and
demonstrate the reduction in communication time and memory utilization. On 512
NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7
billion parameter model by 74%, and the total communication time by 40%, thus
providing an overall speedup of 34% over AxoNN, 32% over DeepSpeed-3D and 46%
over Sputnik, a sparse matrix computation baseline.",None,-1
5328878b-6ef5-4d66-8ae1-0acab17d92d1,Faithfulness Tests for Natural Language Explanations,0.669655,31,"Explanations of neural models aim to reveal a model's decision-making process
for its predictions. However, recent work shows that current methods giving
explanations such as saliency maps or counterfactuals can be misleading, as
they are prone to present reasons that are unfaithful to the model's inner
workings. This work explores the challenging question of evaluating the
faithfulness of natural language explanations (NLEs). To this end, we present
two tests. First, we propose a counterfactual input editor for inserting
reasons that lead to counterfactual predictions but are not reflected by the
NLEs. Second, we reconstruct inputs from the reasons stated in the generated
NLEs and check how often they lead to the same predictions. Our tests can
evaluate emerging NLE models, proving a fundamental tool in the development of
faithful NLEs.",None,-1
26439dfb-8504-42ab-9c07-90112efb6670,Continual Event Extraction with Semantic Confusion Rectification,0.395144,1,"We study continual event extraction, which aims to extract incessantly
emerging event information while avoiding forgetting. We observe that the
semantic confusion on event types stems from the annotations of the same text
being updated over time. The imbalance between event types even aggravates this
issue. This paper proposes a novel continual event extraction model with
semantic confusion rectification. We mark pseudo labels for each sentence to
alleviate semantic confusion. We transfer pivotal knowledge between current and
previous models to enhance the understanding of event types. Moreover, we
encourage the model to focus on the semantics of long-tailed event types by
leveraging other associated types. Experimental results show that our model
outperforms state-of-the-art baselines and is proficient in imbalanced
datasets.",None,-1
acc923e0-6f74-4222-ac8a-4824cd51ec9f,Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation,0.618403,3,"Simultaneous speech translation is an essential communication task difficult
for humans whereby a translation is generated concurrently with oncoming speech
inputs. For such a streaming task, transformers using block processing to break
an input sequence into segments have achieved state-of-the-art performance at a
reduced cost. Current methods to allow information to propagate across
segments, including left context and memory banks, have faltered as they are
both insufficient representations and unnecessarily expensive to compute. In
this paper, we propose an Implicit Memory Transformer that implicitly retains
memory through a new left context method, removing the need to explicitly
represent memory with memory banks. We generate the left context from the
attention output of the previous segment and include it in the keys and values
of the current segment's attention calculation. Experiments on the MuST-C
dataset show that the Implicit Memory Transformer provides a substantial
speedup on the encoder forward pass with nearly identical translation quality
when compared with the state-of-the-art approach that employs both left context
and memory banks.",None,-1
1ba26987-7af6-48df-9339-36c743e114b5,Heterogeneous Multi-Robot Reinforcement Learning,0.829906,21,"Cooperative multi-robot tasks can benefit from heterogeneity in the robots'
physical and behavioral traits. In spite of this, traditional Multi-Agent
Reinforcement Learning (MARL) frameworks lack the ability to explicitly
accommodate policy heterogeneity, and typically constrain agents to share
neural network parameters. This enforced homogeneity limits application in
cases where the tasks benefit from heterogeneous behaviors. In this paper, we
crystallize the role of heterogeneity in MARL policies. Towards this end, we
introduce Heterogeneous Graph Neural Network Proximal Policy Optimization
(HetGPPO), a paradigm for training heterogeneous MARL policies that leverages a
Graph Neural Network for differentiable inter-agent communication. HetGPPO
allows communicating agents to learn heterogeneous behaviors while enabling
fully decentralized training in partially observable environments. We
complement this with a taxonomical overview that exposes more heterogeneity
classes than previously identified. To motivate the need for our model, we
present a characterization of techniques that homogeneous models can leverage
to emulate heterogeneous behavior, and show how this ""apparent heterogeneity""
is brittle in real-world conditions. Through simulations and real-world
experiments, we show that: (i) when homogeneous methods fail due to strong
heterogeneous requirements, HetGPPO succeeds, and, (ii) when homogeneous
methods are able to learn apparently heterogeneous behaviors, HetGPPO achieves
higher resilience to both training and deployment noise.",None,-1
8202bfcd-c5f4-434c-9f9f-76f6d094e4ec,Masked Contrastive Graph Representation Learning for Age Estimation,0.883153,19,"Age estimation of face images is a crucial task with various practical
applications in areas such as video surveillance and Internet access control.
While deep learning-based age estimation frameworks, e.g., convolutional neural
network (CNN), multi-layer perceptrons (MLP), and transformers have shown
remarkable performance, they have limitations when modelling complex or
irregular objects in an image that contains a large amount of redundant
information. To address this issue, this paper utilizes the robustness property
of graph representation learning in dealing with image redundancy information
and proposes a novel Masked Contrastive Graph Representation Learning (MCGRL)
method for age estimation. Specifically, our approach first leverages CNN to
extract semantic features of the image, which are then partitioned into patches
that serve as nodes in the graph. Then, we use a masked graph convolutional
network (GCN) to derive image-based node representations that capture rich
structural information. Finally, we incorporate multiple losses to explore the
complementary relationship between structural information and semantic
features, which improves the feature representation capability of GCN.
Experimental results on real-world face image datasets demonstrate the
superiority of our proposed method over other state-of-the-art age estimation
approaches.",None,-1
ef2911b4-4886-4c1e-bca9-9f3918f467d2,An Information Extraction Study: Take In Mind the Tokenization!,0.13458,2,"Current research on the advantages and trade-offs of using characters,
instead of tokenized text, as input for deep learning models, has evolved
substantially. New token-free models remove the traditional tokenization step;
however, their efficiency remains unclear. Moreover, the effect of tokenization
is relatively unexplored in sequence tagging tasks. To this end, we investigate
the impact of tokenization when extracting information from documents and
present a comparative study and analysis of subword-based and character-based
models. Specifically, we study Information Extraction (IE) from biomedical
texts. The main outcome is twofold: tokenization patterns can introduce
inductive bias that results in state-of-the-art performance, and the
character-based models produce promising results; thus, transitioning to
token-free IE models is feasible.",None,-1
29a7b071-638b-4d5a-a74f-6bf48afddbec,3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V,0.476662,6,"In this work, we present a new visual prompting method called 3DAxiesPrompts
(3DAP) to unleash the capabilities of GPT-4V in performing 3D spatial tasks.
Our investigation reveals that while GPT-4V exhibits proficiency in discerning
the position and interrelations of 2D entities through current visual prompting
techniques, its abilities in handling 3D spatial tasks have yet to be explored.
In our approach, we create a 3D coordinate system tailored to 3D imagery,
complete with annotated scale information. By presenting images infused with
the 3DAP visual prompt as inputs, we empower GPT-4V to ascertain the spatial
positioning information of the given 3D target image with a high degree of
precision. Through experiments, We identified three tasks that could be stably
completed using the 3DAP method, namely, 2D to 3D Point Reconstruction, 2D to
3D point matching, and 3D Object Detection. We perform experiments on our
proposed dataset 3DAP-Data, the results from these experiments validate the
efficacy of 3DAP-enhanced GPT-4V inputs, marking a significant stride in 3D
spatial task execution.",None,-1
496e7de2-f2dd-434d-b295-f7c383ef00af,Free Lunch for Efficient Textual Commonsense Integration in Language Models,0.118557,2,"Recent years have witnessed the emergence of textual commonsense knowledge
bases, aimed at providing more nuanced and context-rich knowledge. The
integration of external commonsense into language models has been shown to be a
key enabler in advancing the state-of-the-art for a wide range of NLP tasks.
However, incorporating textual commonsense descriptions is computationally
expensive, as compared to encoding conventional symbolic knowledge. In this
paper, we propose a method to improve its efficiency without modifying the
model. We group training samples with similar commonsense descriptions into a
single batch, thus reusing the encoded description across multiple samples. One
key observation is that the upper bound of batch partitioning can be reduced to
the classic {\it graph k-cut problem}. Consequently, we propose a spectral
clustering-based algorithm to solve this problem. Extensive experiments
illustrate that the proposed batch partitioning approach effectively reduces
the computational cost while preserving performance. The efficiency improvement
is more pronounced on larger datasets and on devices with more memory capacity,
attesting to its practical utility for large-scale applications.",None,-1
75545d22-28ef-4734-9964-a061bed93585,Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification,0.974722,6,"Autism spectrum disorder (ASD) is a prevalent psychiatric condition
characterized by atypical cognitive, emotional, and social patterns. Timely and
accurate diagnosis is crucial for effective interventions and improved outcomes
in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced
Transformer framework, METAFormer, ASD classification. Our framework utilizes
resting-state functional magnetic resonance imaging data from the ABIDE I
dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer
employs a multi-atlas approach, where flattened connectivity matrices from the
AAL, CC200, and DOS160 atlases serve as input to the transformer encoder.
Notably, we demonstrate that self-supervised pretraining, involving the
reconstruction of masked values from the input, significantly enhances
classification performance without the need for additional or separate training
data. Through stratified cross-validation, we evaluate the proposed framework
and show that it surpasses state-of-the-art performance on the ABIDE I dataset,
with an average accuracy of 83.7% and an AUC-score of 0.832. The code for our
framework is available at https://github.com/Lugges991/METAFormer",None,-1
038ef31b-9af1-495a-b3a6-5ee1184b43fc,FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction,0.851952,4,"Universal Information Extraction (UIE) has been introduced as a unified
framework for various Information Extraction (IE) tasks and has achieved
widespread success. Despite this, UIE models have limitations. For example,
they rely heavily on span boundaries in the data during training, which does
not reflect the reality of span annotation challenges. Slight adjustments to
positions can also meet requirements. Additionally, UIE models lack attention
to the limited span length feature in IE. To address these deficiencies, we
propose the Fuzzy Span Universal Information Extraction (FSUIE) framework.
Specifically, our contribution consists of two concepts: fuzzy span loss and
fuzzy span attention. Our experimental results on a series of main IE tasks
show significant improvement compared to the baseline, especially in terms of
fast convergence and strong performance with small amounts of data and training
epochs. These results demonstrate the effectiveness and generalization of FSUIE
in different tasks, settings, and scenarios.",None,-1
924c78cb-1847-4463-adac-de9bc5323e34,DiffVoice: Text-to-Speech with Latent Diffusion,0.768762,12,"In this work, we present DiffVoice, a novel text-to-speech model based on
latent diffusion. We propose to first encode speech signals into a phoneme-rate
latent representation with a variational autoencoder enhanced by adversarial
training, and then jointly model the duration and the latent representation
with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS
datasets demonstrate that our method beats the best publicly available systems
in naturalness. By adopting recent generative inverse problem solving
algorithms for diffusion models, DiffVoice achieves the state-of-the-art
performance in text-based speech editing, and zero-shot adaptation.",None,-1
144ff934-d72d-4b09-a88d-06627a492c49,Scaling Vision-Language Models with Sparse Mixture of Experts,0.474456,31,"The field of natural language processing (NLP) has made significant strides
in recent years, particularly in the development of large-scale vision-language
models (VLMs). These models aim to bridge the gap between text and visual
information, enabling a more comprehensive understanding of multimedia data.
However, as these models become larger and more complex, they also become more
challenging to train and deploy. One approach to addressing this challenge is
the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the
model into smaller, specialized sub-models that can jointly solve a task. In
this paper, we explore the effectiveness of MoE in scaling vision-language
models, demonstrating its potential to achieve state-of-the-art performance on
a range of benchmarks over dense models of equivalent computational cost. Our
research offers valuable insights into stabilizing the training of MoE models,
understanding the impact of MoE on model interpretability, and balancing the
trade-offs between compute performance when scaling VLMs. We hope our work will
inspire further research into the use of MoE for scaling large-scale
vision-language models and other multimodal machine learning applications.",None,-1
25bbe71d-94d7-4c23-99c9-060c062d8191,PANACEA: An Automated Misinformation Detection System on COVID-19,0.153003,5,"In this demo, we introduce a web-based misinformation detection system
PANACEA on COVID-19 related claims, which has two modules, fact-checking and
rumour detection. Our fact-checking module, which is supported by novel natural
language inference methods with a self-attention network, outperforms
state-of-the-art approaches. It is also able to give automated veracity
assessment and ranked supporting evidence with the stance towards the claim to
be checked. In addition, PANACEA adapts the bi-directional graph convolutional
networks model, which is able to detect rumours based on comment networks of
related tweets, instead of relying on the knowledge base. This rumour detection
module assists by warning the users in the early stages when a knowledge base
may not be available.",None,-1
54e6f948-12ad-4b70-b4cc-a7131e75c05b,Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation,0.711215,10,"Lifelong sequence generation (LSG), a problem in continual learning, aims to
continually train a model on a sequence of generation tasks to learn constantly
emerging new generation patterns while avoiding the forgetting of previous
knowledge. Existing LSG methods mainly focus on maintaining old knowledge while
paying little attention to knowledge transfer across tasks. In contrast, humans
can better learn new tasks by leveraging previously acquired knowledge from
similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic
Module Expansion and Adaptation (DMEA), which enables the model to dynamically
determine the architecture for acquiring new knowledge based on task
correlation and select the most similar previous tasks to facilitate adaptation
to new tasks. In addition, as the learning process can easily be biased towards
the current task which might cause more severe forgetting of previously learned
knowledge, we propose dynamic gradient scaling to balance the learning of the
current task and replayed tasks. With extensive experiments, we demonstrate
that DMEA can consistently outperform existing methods in different LSG
settings.",None,-1
9119f873-da72-41b7-899e-d39ad978c62a,An Empirical Study and Improvement for Speech Emotion Recognition,0.210408,1,"Multimodal speech emotion recognition aims to detect speakers' emotions from
audio and text. Prior works mainly focus on exploiting advanced networks to
model and fuse different modality information to facilitate performance, while
neglecting the effect of different fusion strategies on emotion recognition. In
this work, we consider a simple yet important problem: how to fuse audio and
text modality information is more helpful for this multimodal task. Further, we
propose a multimodal emotion recognition model improved by perspective loss.
Empirical results show our method obtained new state-of-the-art results on the
IEMOCAP dataset. The in-depth analysis explains why the improved model can
achieve improvements and outperforms baselines.",None,-1
bbea2e83-ac8f-4d6f-a752-d7b18806ec17,Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications,0.789293,7,"Large-scale pre-trained language models have shown outstanding performance in
a variety of NLP tasks. However, they are also known to be significantly
brittle against specifically crafted adversarial examples, leading to
increasing interest in probing the adversarial robustness of NLP systems. We
introduce RSMI, a novel two-stage framework that combines randomized smoothing
(RS) with masked inference (MI) to improve the adversarial robustness of NLP
systems. RS transforms a classifier into a smoothed classifier to obtain robust
representations, whereas MI forces a model to exploit the surrounding context
of a masked token in an input sequence. RSMI improves adversarial robustness by
2 to 3 times over existing state-of-the-art methods on benchmark datasets. We
also perform in-depth qualitative analysis to validate the effectiveness of the
different stages of RSMI and probe the impact of its components through
extensive ablations. By empirically proving the stability of RSMI, we put it
forward as a practical method to robustly train large-scale NLP models. Our
code and datasets are available at https://github.com/Han8931/rsmi_nlp",None,-1
5a4f87ad-8e3a-4bf3-b7d9-ece9a33ba00c,On Explicit Curvature Regularization in Deep Generative Models,0.48949,6,"We propose a family of curvature-based regularization terms for deep
generative model learning. Explicit coordinate-invariant formulas for both
intrinsic and extrinsic curvature measures are derived for the case of
arbitrary data manifolds embedded in higher-dimensional Euclidean space.
Because computing the curvature is a highly computation-intensive process
involving the evaluation of second-order derivatives, efficient formulas are
derived for approximately evaluating intrinsic and extrinsic curvatures.
Comparative studies are conducted that compare the relative efficacy of
intrinsic versus extrinsic curvature-based regularization measures, as well as
performance comparisons against existing autoencoder training methods.
Experiments involving noisy motion capture data confirm that curvature-based
methods outperform existing autoencoder regularization methods, with intrinsic
curvature measures slightly more effective than extrinsic curvature measures.",None,-1
c2661a27-4339-4539-86df-e07c9068c6d8,Efficient Real Time Recurrent Learning through combined activity and parameter sparsity,0.0846928,1,"Backpropagation through time (BPTT) is the standard algorithm for training
recurrent neural networks (RNNs), which requires separate simulation phases for
the forward and backward passes for inference and learning, respectively.
Moreover, BPTT requires storing the complete history of network states between
phases, with memory consumption growing proportional to the input sequence
length. This makes BPTT unsuited for online learning and presents a challenge
for implementation on low-resource real-time systems. Real-Time Recurrent
Learning (RTRL) allows online learning, and the growth of required memory is
independent of sequence length. However, RTRL suffers from exceptionally high
computational costs that grow proportional to the fourth power of the state
size, making RTRL computationally intractable for all but the smallest of
networks. In this work, we show that recurrent networks exhibiting high
activity sparsity can reduce the computational cost of RTRL. Moreover,
combining activity and parameter sparsity can lead to significant enough
savings in computational and memory costs to make RTRL practical. Unlike
previous work, this improvement in the efficiency of RTRL can be achieved
without using any approximations for the learning process.",None,-1
543b9f0a-1f02-4a9f-9bc5-ed7348b29940,An Analysis of Dialogue Repair in Voice Assistants,0.271346,1,"Spoken dialogue systems have transformed human-machine interaction by
providing real-time responses to queries. However, misunderstandings between
the user and system persist. This study explores the significance of
interactional language in dialogue repair between virtual assistants and users
by analyzing interactions with Google Assistant and Siri, focusing on their
utilization and response to the other-initiated repair strategy ""huh?""
prevalent in human-human interaction. Findings reveal several
assistant-generated strategies but an inability to replicate human-like repair
strategies such as ""huh?"". English and Spanish user acceptability surveys show
differences in users' repair strategy preferences and assistant usage, with
both similarities and disparities among the two surveyed languages. These
results shed light on inequalities between interactional language in
human-human interaction and human-machine interaction, underscoring the need
for further research on the impact of interactional language in human-machine
interaction in English and beyond.",None,-1
a11fae81-57f2-4fb2-a20a-8aa338c33daf,Gradient-Based Automated Iterative Recovery for Parameter-Efficient Tuning,0.620888,3,"Pretrained large language models (LLMs) are able to solve a wide variety of
tasks through transfer learning. Various explainability methods have been
developed to investigate their decision making process. TracIn (Pruthi et al.,
2020) is one such gradient-based method which explains model inferences based
on the influence of training examples. In this paper, we explore the use of
TracIn to improve model performance in the parameter-efficient tuning (PET)
setting. We develop conversational safety classifiers via the prompt-tuning PET
method and show how the unique characteristics of the PET regime enable TracIn
to identify the cause for certain misclassifications by LLMs. We develop a new
methodology for using gradient-based explainability techniques to improve model
performance, G-BAIR: gradient-based automated iterative recovery. We show that
G-BAIR can recover LLM performance on benchmarks after manually corrupting
training labels. This suggests that influence methods like TracIn can be used
to automatically perform data cleaning, and introduces the potential for
interactive debugging and relabeling for PET-based transfer learning methods.",None,-1
7a78e528-bd7b-4e6e-aded-838fd808adb0,Orientation-Guided Contrastive Learning for UAV-View Geo-Localisation,0.640985,2,"Retrieving relevant multimedia content is one of the main problems in a world
that is increasingly data-driven. With the proliferation of drones, high
quality aerial footage is now available to a wide audience for the first time.
Integrating this footage into applications can enable GPS-less geo-localisation
or location correction.
  In this paper, we present an orientation-guided training framework for
UAV-view geo-localisation. Through hierarchical localisation orientations of
the UAV images are estimated in relation to the satellite imagery. We propose a
lightweight prediction module for these pseudo labels which predicts the
orientation between the different views based on the contrastive learned
embeddings. We experimentally demonstrate that this prediction supports the
training and outperforms previous approaches. The extracted pseudo-labels also
enable aligned rotation of the satellite image as augmentation to further
strengthen the generalisation. During inference, we no longer need this
orientation module, which means that no additional computations are required.
We achieve state-of-the-art results on both the University-1652 and
University-160k datasets.",None,-1
a5564152-b1d4-412d-91f0-5910b2084e44,Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4,0.999682,19,"Unlike perfect information games, where all elements are known to every
player, imperfect information games emulate the real-world complexities of
decision-making under uncertain or incomplete information. GPT-4, the recent
breakthrough in large language models (LLMs) trained on massive passive data,
is notable for its knowledge retrieval and reasoning abilities. This paper
delves into the applicability of GPT-4's learned knowledge for imperfect
information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an
innovative agent that leverages GPT-4's capabilities for performing in
imperfect information games. With proper prompt engineering to achieve
different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable
adaptability across a range of imperfect information card games. Importantly,
GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it
can understand others and intentionally impact others' behavior. Leveraging
this, we design a planning strategy that enables GPT-4 to competently play
against different opponents, adapting its gameplay style as needed, while
requiring only the game rules and descriptions of observations as input. In the
experiments, we qualitatively showcase the capabilities of Suspicion-Agent
across three different imperfect information games and then quantitatively
evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can
potentially outperform traditional algorithms designed for imperfect
information games, without any specialized training or examples. In order to
encourage and foster deeper insights within the community, we make our
game-related data publicly available.",None,-1
6ca7942e-bc94-4ff6-805d-431b47d8c22e,Causality Analysis for Evaluating the Security of Large Language Models,0.0470595,3,"Large Language Models (LLMs) such as GPT and Llama2 are increasingly adopted
in many safety-critical applications. Their security is thus essential. Even
with considerable efforts spent on reinforcement learning from human feedback
(RLHF), recent studies have shown that LLMs are still subject to attacks such
as adversarial perturbation and Trojan attacks. Further research is thus needed
to evaluate their security and/or understand the lack of it. In this work, we
propose a framework for conducting light-weight causality-analysis of LLMs at
the token, layer, and neuron level. We applied our framework to open-source
LLMs such as Llama2 and Vicuna and had multiple interesting discoveries. Based
on a layer-level causality analysis, we show that RLHF has the effect of
overfitting a model to harmful prompts. It implies that such security can be
easily overcome by `unusual' harmful prompts. As evidence, we propose an
adversarial perturbation method that achieves 100\% attack success rate on the
red-teaming tasks of the Trojan Detection Competition 2023. Furthermore, we
show the existence of one mysterious neuron in both Llama2 and Vicuna that has
an unreasonably high causal effect on the output. While we are uncertain on why
such a neuron exists, we show that it is possible to conduct a ``Trojan''
attack targeting that particular neuron to completely cripple the LLM, i.e., we
can generate transferable suffixes to prompts that frequently make the LLM
produce meaningless responses.",None,-1
bb4116bd-8559-4f7d-91c4-48798414a0ac,Dynamic Transformers Provide a False Sense of Efficiency,0.146577,6,"Despite much success in natural language processing (NLP), pre-trained
language models typically lead to a high computational cost during inference.
Multi-exit is a mainstream approach to address this issue by making a trade-off
between efficiency and accuracy, where the saving of computation comes from an
early exit. However, whether such saving from early-exiting is robust remains
unknown. Motivated by this, we first show that directly adapting existing
adversarial attack approaches targeting model accuracy cannot significantly
reduce inference efficiency. To this end, we propose a simple yet effective
attacking framework, SAME, a novel slowdown attack framework on multi-exit
models, which is specially tailored to reduce the efficiency of the multi-exit
models. By leveraging the multi-exit models' design characteristics, we utilize
all internal predictions to guide the adversarial sample generation instead of
merely considering the final prediction. Experiments on the GLUE benchmark show
that SAME can effectively diminish the efficiency gain of various multi-exit
models by 80% on average, convincingly validating its effectiveness and
generalization ability.",None,-1
841fb77b-de73-4b61-aaec-a8dba768e057,Explaining How Transformers Use Context to Build Predictions,0.367639,16,"Language Generation Models produce words based on the previous context.
Although existing methods offer input attributions as explanations for a
model's prediction, it is still unclear how prior words affect the model's
decision throughout the layers. In this work, we leverage recent advances in
explainability of the Transformer and present a procedure to analyze models for
language generation. Using contrastive examples, we compare the alignment of
our explanations with evidence of the linguistic phenomena, and show that our
method consistently aligns better than gradient-based and perturbation-based
baselines. Then, we investigate the role of MLPs inside the Transformer and
show that they learn features that help the model predict words that are
grammatically acceptable. Lastly, we apply our method to Neural Machine
Translation models, and demonstrate that they generate human-like source-target
alignments for building predictions.",None,-1
084ad6f0-9215-457e-8680-8bb9f594fca9,Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification,0.768949,21,"Recent advances in large language models (LLMs) have shown impressive ability
in biomedical question-answering, but have not been adequately investigated for
more specific biomedical applications. This study investigates the performance
of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical
tasks beyond question-answering. Because no patient data can be passed to the
OpenAI API public interface, we evaluated model performance with over 10000
samples as proxies for two fundamental tasks in the clinical domain -
classification and reasoning. The first task is classifying whether statements
of clinical and policy recommendations in scientific literature constitute
health advice. The second task is causal relation detection from the biomedical
literature. We compared LLMs with simpler models, such as bag-of-words (BoW)
with logistic regression, and fine-tuned BioBERT models. Despite the excitement
around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks
remained the best strategy. The simple BoW model performed on par with the most
complex LLM prompting. Prompt engineering required significant investment.",None,-1
2b6228c4-98f1-47a4-89ea-f163f600aec4,AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics,0.748095,5,"The quality of the video stream is key to neural network-based video
analytics. However, low-quality video is inevitably collected by existing
surveillance systems because of poor quality cameras or over-compressed/pruned
video streaming protocols, e.g., as a result of upstream bandwidth limit. To
address this issue, existing studies use quality enhancers (e.g., neural
super-resolution) to improve the quality of videos (e.g., resolution) and
eventually ensure inference accuracy. Nevertheless, directly applying quality
enhancers does not work in practice because it will introduce unacceptable
latency. In this paper, we present AccDecoder, a novel accelerated decoder for
real-time and neural-enhanced video analytics. AccDecoder can select a few
frames adaptively via Deep Reinforcement Learning (DRL) to enhance the quality
by neural super-resolution and then up-scale the unselected frames that
reference them, which leads to 6-21% accuracy improvement. AccDecoder provides
efficient inference capability via filtering important frames using DRL for
DNN-based inference and reusing the results for the other frames via extracting
the reference relationship among frames and blocks, which results in a latency
reduction of 20-80% than baselines.",None,-1
9c1159ef-fb4f-4e56-b878-3e6d8a5f3896,Computer Vision Estimation of Emotion Reaction Intensity in the Wild,0.518931,2,"Emotions play an essential role in human communication. Developing computer
vision models for automatic recognition of emotion expression can aid in a
variety of domains, including robotics, digital behavioral healthcare, and
media analytics. There are three types of emotional representations which are
traditionally modeled in affective computing research: Action Units, Valence
Arousal (VA), and Categorical Emotions. As part of an effort to move beyond
these representations towards more fine-grained labels, we describe our
submission to the newly introduced Emotional Reaction Intensity (ERI)
Estimation challenge in the 5th competition for Affective Behavior Analysis
in-the-Wild (ABAW). We developed four deep neural networks trained in the
visual domain and a multimodal model trained with both visual and audio
features to predict emotion reaction intensity. Our best performing model on
the Hume-Reaction dataset achieved an average Pearson correlation coefficient
of 0.4080 on the test set using a pre-trained ResNet50 model. This work
provides a first step towards the development of production-grade models which
predict emotion reaction intensities rather than discrete emotion categories.",None,-1
9d1b0f6a-11f3-4014-9bab-179db6e75ccc,"Lightweight, Pre-trained Transformers for Remote Sensing Timeseries",0.98204,32,"Machine learning methods for satellite data have a range of societally
relevant applications, but labels used to train models can be difficult or
impossible to acquire. Self-supervision is a natural solution in settings with
limited labeled data, but current self-supervised models for satellite data
fail to take advantage of the characteristics of that data, including the
temporal dimension (which is critical for many applications, such as monitoring
crop growth) and availability of data from many complementary sensors (which
can significantly improve a model's predictive performance). We present Presto
(the Pretrained Remote Sensing Transformer), a model pre-trained on remote
sensing pixel-timeseries data. By designing Presto specifically for remote
sensing data, we can create a significantly smaller but performant model.
Presto excels at a wide variety of globally distributed remote sensing tasks
and performs competitively with much larger models while requiring far less
compute. Presto can be used for transfer learning or as a feature extractor for
simple models, enabling efficient deployment at scale.",None,-1
4652bebf-71ee-4aab-a355-cf60f3b9c808,Mapping and Cleaning Open Commonsense Knowledge Bases with Generative Translation,0.204956,1,"Structured knowledge bases (KBs) are the backbone of many
know\-ledge-intensive applications, and their automated construction has
received considerable attention. In particular, open information extraction
(OpenIE) is often used to induce structure from a text. However, although it
allows high recall, the extracted knowledge tends to inherit noise from the
sources and the OpenIE algorithm. Besides, OpenIE tuples contain an open-ended,
non-canonicalized set of relations, making the extracted knowledge's downstream
exploitation harder. In this paper, we study the problem of mapping an open KB
into the fixed schema of an existing KB, specifically for the case of
commonsense knowledge. We propose approaching the problem by generative
translation, i.e., by training a language model to generate fixed-schema
assertions from open ones. Experiments show that this approach occupies a sweet
spot between traditional manual, rule-based, or classification-based
canonicalization and purely generative KB construction like COMET. Moreover, it
produces higher mapping accuracy than the former while avoiding the
association-based noise of the latter.",None,-1
82306b70-ff5c-45a0-b0ab-04cd5cbc3c6d,Learning Interpretable Style Embeddings via Prompting LLMs,0.999127,9,"Style representation learning builds content-independent representations of
author style in text. Stylometry, the analysis of style in text, is often
performed by expert forensic linguists and no large dataset of stylometric
annotations exists for training. Current style representation learning uses
neural methods to disentangle style from content to create style vectors,
however, these approaches result in uninterpretable representations,
complicating their usage in downstream applications like authorship attribution
where auditing and explainability is critical. In this work, we use prompting
to perform stylometry on a large number of texts to create a synthetic dataset
and train human-interpretable style representations we call LISA embeddings. We
release our synthetic stylometry dataset and our interpretable style models as
resources.",None,-1
e16acb9b-875f-4252-8336-1ace1553b6dd,CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation,0.587348,24,"Annotated data plays a critical role in Natural Language Processing (NLP) in
training models and evaluating their performance. Given recent developments in
Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot
capability on many text-annotation tasks, comparable with or even exceeding
human annotators. Such LLMs can serve as alternatives for manual annotation,
due to lower costs and higher scalability. However, limited work has leveraged
LLMs as complementary annotators, nor explored how annotation work is best
allocated among humans and LLMs to achieve both quality and cost objectives. We
propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of
unstructured texts at scale. Under this framework, we utilize uncertainty to
estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to
be an effective means to allocate work from results on different datasets, with
up to 21% performance improvement over random baseline. For code
implementation, see https://github.com/SALT-NLP/CoAnnotating.",None,-1
062eccfa-bcac-462f-ab6a-cf5ff0eebff1,Tab2KG: Semantic Table Interpretation with Lightweight Semantic Profiles,0.447764,5,"Tabular data plays an essential role in many data analytics and machine
learning tasks. Typically, tabular data does not possess any machine-readable
semantics. In this context, semantic table interpretation is crucial for making
data analytics workflows more robust and explainable. This article proposes
Tab2KG - a novel method that targets at the interpretation of tables with
previously unseen data and automatically infers their semantics to transform
them into semantic data graphs. We introduce original lightweight semantic
profiles that enrich a domain ontology's concepts and relations and represent
domain and table characteristics. We propose a one-shot learning approach that
relies on these profiles to map a tabular dataset containing previously unseen
instances to a domain ontology. In contrast to the existing semantic table
interpretation approaches, Tab2KG relies on the semantic profiles only and does
not require any instance lookup. This property makes Tab2KG particularly
suitable in the data analytics context, in which data tables typically contain
new instances. Our experimental evaluation on several real-world datasets from
different application domains demonstrates that Tab2KG outperforms
state-of-the-art semantic table interpretation baselines.",None,-1
941ef0c2-258d-4122-8bb6-437f14b9fdf3,Adaptive Reconvergence-driven AIG Rewriting via Strategy Learning,0.102072,1,"Rewriting is a common procedure in logic synthesis aimed at improving the
performance, power, and area (PPA) of circuits. The traditional
reconvergence-driven And-Inverter Graph (AIG) rewriting method focuses solely
on optimizing the reconvergence cone through Boolean algebra minimization.
However, there exist opportunities to incorporate other node-rewriting
algorithms that are better suited for specific cones. In this paper, we propose
an adaptive reconvergence-driven AIG rewriting algorithm that combines two key
techniques: multi-strategy-based AIG rewriting and strategy learning-based
algorithm selection. The multi-strategy-based rewriting method expands upon the
traditional approach by incorporating support for multi-node-rewriting
algorithms, thus expanding the optimization space. Additionally, the strategy
learning-based algorithm selection method determines the most suitable
node-rewriting algorithm for a given cone. Experimental results demonstrate
that our proposed method yields a significant average improvement of 5.567\% in
size and 5.327\% in depth.",None,-1
9e0bb62e-d71f-417b-b1db-d930cb165578,MixedTeacher : Knowledge Distillation for fast inference textural anomaly detection,0.304856,3,"For a very long time, unsupervised learning for anomaly detection has been at
the heart of image processing research and a stepping stone for high
performance industrial automation process. With the emergence of CNN, several
methods have been proposed such as Autoencoders, GAN, deep feature extraction,
etc. In this paper, we propose a new method based on the promising concept of
knowledge distillation which consists of training a network (the student) on
normal samples while considering the output of a larger pretrained network (the
teacher). The main contributions of this paper are twofold: First, a reduced
student architecture with optimal layer selection is proposed, then a new
Student-Teacher architecture with network bias reduction combining two teachers
is proposed in order to jointly enhance the performance of anomaly detection
and its localization accuracy. The proposed texture anomaly detector has an
outstanding capability to detect defects in any texture and a fast inference
time compared to the SOTA methods.",None,-1
92406a0e-3294-42fb-a2fe-4de23bfb7e1b,"Deep Learning Method for Cell-Wise Object Tracking, Velocity Estimation and Projection of Sensor Data over Time",0.180967,1,"Current Deep Learning methods for environment segmentation and velocity
estimation rely on Convolutional Recurrent Neural Networks to exploit
spatio-temporal relationships within obtained sensor data. These approaches
derive scene dynamics implicitly by correlating novel input and memorized data
utilizing ConvNets. We show how ConvNets suffer from architectural restrictions
for this task. Based on these findings, we then provide solutions to various
issues on exploiting spatio-temporal correlations in a sequence of sensor
recordings by presenting a novel Recurrent Neural Network unit utilizing
Transformer mechanisms. Within this unit, object encodings are tracked across
consecutive frames by correlating key-query pairs derived from sensor inputs
and memory states, respectively. We then use resulting tracking patterns to
obtain scene dynamics and regress velocities. In a last step, the memory state
of the Recurrent Neural Network is projected based on extracted velocity
estimates to resolve aforementioned spatio-temporal misalignment.",None,-1
97f5074b-41c8-4086-8f7f-1fe809d801f7,Automated Visual Monitoring of Nocturnal Insects with Light-based Camera Traps,0.190998,1,"Automatic camera-assisted monitoring of insects for abundance estimations is
crucial to understand and counteract ongoing insect decline. In this paper, we
present two datasets of nocturnal insects, especially moths as a subset of
Lepidoptera, photographed in Central Europe. One of the datasets, the EU-Moths
dataset, was captured manually by citizen scientists and contains species
annotations for 200 different species and bounding box annotations for those.
We used this dataset to develop and evaluate a two-stage pipeline for insect
detection and moth species classification in previous work. We further
introduce a prototype for an automated visual monitoring system. This prototype
produced the second dataset consisting of more than 27,000 images captured on
95 nights. For evaluation and bootstrapping purposes, we annotated a subset of
the images with bounding boxes enframing nocturnal insects. Finally, we present
first detection and classification baselines for these datasets and encourage
other scientists to use this publicly available data.",None,-1
549a7d24-ede5-4cdf-944d-3c2b60d5c1ad,Dynamic Coarse-to-Fine Learning for Oriented Tiny Object Detection,0.624324,19,"Detecting arbitrarily oriented tiny objects poses intense challenges to
existing detectors, especially for label assignment. Despite the exploration of
adaptive label assignment in recent oriented object detectors, the extreme
geometry shape and limited feature of oriented tiny objects still induce severe
mismatch and imbalance issues. Specifically, the position prior, positive
sample feature, and instance are mismatched, and the learning of extreme-shaped
objects is biased and unbalanced due to little proper feature supervision. To
tackle these issues, we propose a dynamic prior along with the coarse-to-fine
assigner, dubbed DCFL. For one thing, we model the prior, label assignment, and
object representation all in a dynamic manner to alleviate the mismatch issue.
For another, we leverage the coarse prior matching and finer posterior
constraint to dynamically assign labels, providing appropriate and relatively
balanced supervision for diverse instances. Extensive experiments on six
datasets show substantial improvements to the baseline. Notably, we obtain the
state-of-the-art performance for one-stage detectors on the DOTA-v1.5,
DOTA-v2.0, and DIOR-R datasets under single-scale training and testing. Codes
are available at https://github.com/Chasel-Tsui/mmrotate-dcfl.",None,-1
b684bd6d-51d4-434b-9665-4ce415b6febc,The sample complexity of multi-distribution learning,0.550959,3,"Multi-distribution learning generalizes the classic PAC learning to handle
data coming from multiple distributions. Given a set of $k$ data distributions
and a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis
that minimizes the maximum population loss over $k$ distributions, up to
$\epsilon$ additive error. In this paper, we settle the sample complexity of
multi-distribution learning by giving an algorithm of sample complexity
$\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$. This matches the
lower bound up to sub-polynomial factor and resolves the COLT 2023 open problem
of Awasthi, Haghtalab and Zhao [AHZ23].",None,-1
31223757-72c9-47b8-a002-2885176d16d7,Bayesian Program Learning by Decompiling Amortized Knowledge,0.253976,1,"DreamCoder is an inductive program synthesis system that, whilst solving
problems, learns to simplify search in an iterative wake-sleep procedure. The
cost of search is amortized by training a neural search policy, reducing search
breadth and effectively ""compiling"" useful information to compose program
solutions across tasks. Additionally, a library of program components is learnt
to compress and express discovered solutions in fewer components, reducing
search depth. We present a novel approach for library learning that directly
leverages the neural search policy, effectively ""decompiling"" its amortized
knowledge to extract relevant program components. This provides stronger
amortized inference: the amortized knowledge learnt to reduce search breadth is
now also used to reduce search depth. We integrate our approach with DreamCoder
and demonstrate faster domain proficiency with improved generalization on a
range of domains, particularly when fewer example solutions are available.",None,-1
ccdc0af6-b5fd-472b-8f16-841968f897cc,Artificial Intelligence Impact On The Labour Force -- Searching For The Analytical Skills Of The Future Software Engineers,0.236324,1,"This systematic literature review aims to investigate the impact of
artificial intelligence (AI) on the labour force in software engineering, with
a particular focus on the skills needed for future software engineers, the
impact of AI on the demand for software engineering skills, and the future of
work for software engineers. The review identified 42 relevant publications
through a comprehensive search strategy and analysed their findings. The
results indicate that future software engineers will need to be competent in
programming and have soft skills such as problem-solving and interpersonal
communication. AI will have a significant impact on the software engineering
workforce, with the potential to automate many jobs currently done by software
engineers. The role of a software engineer is changing and will continue to
change in the future, with AI-assisted software development posing challenges
for the software engineering profession. The review suggests that the software
engineering profession must adapt to the changing landscape to remain relevant
and effective in the future.",None,-1
79b164a3-4b8f-4116-848b-5e6456917862,Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,0.742348,13,"Predicting the pose of objects from a single image is an important but
difficult computer vision problem. Methods that predict a single point estimate
do not predict the pose of objects with symmetries well and cannot represent
uncertainty. Alternatively, some works predict a distribution over orientations
in $\mathrm{SO}(3)$. However, training such models can be computation- and
sample-inefficient. Instead, we propose a novel mapping of features from the
image domain to the 3D rotation manifold. Our method then leverages
$\mathrm{SO}(3)$ equivariant layers, which are more sample efficient, and
outputs a distribution over rotations that can be sampled at arbitrary
resolution. We demonstrate the effectiveness of our method at object
orientation prediction, and achieve state-of-the-art performance on the popular
PASCAL3D+ dataset. Moreover, we show that our method can model complex object
symmetries, without any modifications to the parameters or loss function. Code
is available at https://dmklee.github.io/image2sphere.",None,-1
43364561-f169-49fb-9d3c-73b134274f82,Self-supervised representations in speech-based depression detection,0.972758,11,"This paper proposes handling training data sparsity in speech-based automatic
depression detection (SDD) using foundation models pre-trained with
self-supervised learning (SSL). An analysis of SSL representations derived from
different layers of pre-trained foundation models is first presented for SDD,
which provides insight to suitable indicator for depression detection.
Knowledge transfer is then performed from automatic speech recognition (ASR)
and emotion recognition to SDD by fine-tuning the foundation models. Results
show that the uses of oracle and ASR transcriptions yield similar SDD
performance when the hidden representations of the ASR model is incorporated
along with the ASR textual information. By integrating representations from
multiple foundation models, state-of-the-art SDD results based on real ASR were
achieved on the DAIC-WOZ dataset.",None,-1
e5bdb0df-46a3-469d-b455-50ecdf4e6801,Transparency at the Source: Evaluating and Interpreting Language Models With Access to the True Distribution,0.0726254,2,"We present a setup for training, evaluating and interpreting neural language
models, that uses artificial, language-like data. The data is generated using a
massive probabilistic grammar (based on state-split PCFGs), that is itself
derived from a large natural language corpus, but also provides us complete
control over the generative process. We describe and release both grammar and
corpus, and test for the naturalness of our generated data. This approach
allows us to define closed-form expressions to efficiently compute exact lower
bounds on obtainable perplexity using both causal and masked language
modelling. Our results show striking differences between neural language
modelling architectures and training objectives in how closely they allow
approximating the lower bound on perplexity. Our approach also allows us to
directly compare learned representations to symbolic rules in the underlying
source. We experiment with various techniques for interpreting model behaviour
and learning dynamics. With access to the underlying true source, our results
show striking differences and outcomes in learning dynamics between different
classes of words.",None,-1
a298e444-5b8f-4204-9490-d54908fc4eb0,StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized Tokenizer of a Large-Scale Generative Model,0.614412,6,"Despite the progress made in the style transfer task, most previous work
focus on transferring only relatively simple features like color or texture,
while missing more abstract concepts such as overall art expression or
painter-specific traits. However, these abstract semantics can be captured by
models like DALL-E or CLIP, which have been trained using huge datasets of
images and textual documents. In this paper, we propose StylerDALLE, a style
transfer method that exploits both of these models and uses natural language to
describe abstract art styles. Specifically, we formulate the language-guided
style transfer task as a non-autoregressive token sequence translation, i.e.,
from input content image to output stylized image, in the discrete latent space
of a large-scale pretrained vector-quantized tokenizer, e.g., the discrete
variational auto-encoder (dVAE) of DALL-E. To incorporate style information, we
propose a Reinforcement Learning strategy with CLIP-based language supervision
that ensures stylization and content preservation simultaneously. Experimental
results demonstrate the superiority of our method, which can effectively
transfer art styles using language instructions at different granularities.
Code is available at https://github.com/zipengxuc/StylerDALLE.",None,-1
54e2f563-6667-417e-9a45-dae62f46834a,Privacy-Preserving Prompt Tuning for Large Language Model Services,0.491618,41,"Prompt tuning provides an efficient way for users to customize Large Language
Models (LLMs) with their private data in the emerging LLM service scenario.
However, the sensitive nature of private data brings the need for privacy
preservation in LLM service customization. Based on prompt tuning, we propose
Privacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy
guarantees for LLM services. \textsc{rapt} adopts a local privacy setting,
allowing users to privatize their data locally with local differential privacy.
As prompt tuning performs poorly when directly trained on privatized data, we
introduce a novel privatized token reconstruction task that is trained jointly
with the downstream task, allowing LLMs to learn better task-dependent
representations. Despite the simplicity of our framework, experiments show that
RAPT achieves competitive performance across tasks while providing privacy
guarantees against adversaries.",None,-1
b52d15f8-6a56-47c7-a0f6-cbc9b876e660,Can Language Models Employ the Socratic Method? Experiments with Code Debugging,0.802069,4,"When employing the Socratic method of teaching, instructors guide students
toward solving a problem on their own rather than providing the solution
directly. While this strategy can substantially improve learning outcomes, it
is usually time-consuming and cognitively demanding. Automated Socratic
conversational agents can augment human instruction and provide the necessary
scale, however their development is hampered by the lack of suitable data for
training and evaluation. In this paper, we introduce a manually created dataset
of multi-turn Socratic advice that is aimed at helping a novice programmer fix
buggy solutions to simple computational problems. The dataset is then used for
benchmarking the Socratic debugging abilities of a number of language models,
ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5
to zero-shot and chain of thought prompting of the much larger GPT-4. The code
and datasets are made freely available for research at the link below.
https://github.com/taisazero/socratic-debugging-benchmark",None,-1
6e830458-6170-4d39-98fc-818a1f91f471,Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis,0.702697,5,"Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various
fine-grained sentiment analysis tasks on a target domain by transferring
knowledge from a source domain. Since labeled data only exists in the source
domain, a model is expected to bridge the domain gap for tackling cross-domain
ABSA. Though domain adaptation methods have proven to be effective, most of
them are based on a discriminative model, which needs to be specifically
designed for different ABSA tasks. To offer a more general solution, we propose
a unified bidirectional generative framework to tackle various cross-domain
ABSA tasks. Specifically, our framework trains a generative model in both
text-to-label and label-to-text directions. The former transforms each task
into a unified format to learn domain-agnostic features, and the latter
generates natural sentences from noisy labels for data augmentation, with which
a more accurate model can be trained. To investigate the effectiveness and
generality of our framework, we conduct extensive experiments on four
cross-domain ABSA tasks and present new state-of-the-art results on all tasks.
Our data and code are publicly available at
\url{https://github.com/DAMO-NLP-SG/BGCA}.",None,-1
2391bc66-374b-461a-a20d-c3e4aa705297,Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In,0.722883,26,"Retrieval augmentation can aid language models (LMs) in knowledge-intensive
tasks by supplying them with external information. Prior works on retrieval
augmentation usually jointly fine-tune the retriever and the LM, making them
closely coupled. In this paper, we explore the scheme of generic retrieval
plug-in: the retriever is to assist target LMs that may not be known beforehand
or are unable to be fine-tuned together. To retrieve useful documents for
unseen target LMs, we propose augmentation-adapted retriever (AAR), which
learns LM's preferences obtained from a known source LM. Experiments on the
MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM
is able to significantly improve the zero-shot generalization of larger target
LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates
that the preferences of different LMs overlap, enabling AAR trained with a
single source LM to serve as a generic plug-in for various target LMs. Our code
is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.",None,-1
0ef1d2de-2c85-4f5c-afb7-f5dbab947aa0,Semi-Supervised SAR ATR Framework with Transductive Auxiliary Segmentation,0.999866,12,"Convolutional neural networks (CNNs) have achieved high performance in
synthetic aperture radar (SAR) automatic target recognition (ATR). However, the
performance of CNNs depends heavily on a large amount of training data. The
insufficiency of labeled training SAR images limits the recognition performance
and even invalidates some ATR methods. Furthermore, under few labeled training
data, many existing CNNs are even ineffective. To address these challenges, we
propose a Semi-supervised SAR ATR Framework with transductive Auxiliary
Segmentation (SFAS). The proposed framework focuses on exploiting the
transductive generalization on available unlabeled samples with an auxiliary
loss serving as a regularizer. Through auxiliary segmentation of unlabeled SAR
samples and information residue loss (IRL) in training, the framework can
employ the proposed training loop process and gradually exploit the information
compilation of recognition and segmentation to construct a helpful inductive
bias and achieve high performance. Experiments conducted on the MSTAR dataset
have shown the effectiveness of our proposed SFAS for few-shot learning. The
recognition performance of 94.18\% can be achieved under 20 training samples in
each class with simultaneous accurate segmentation results. Facing variances of
EOCs, the recognition ratios are higher than 88.00\% when 10 training samples
each class.",None,-1
2a5b5612-035b-49c3-817b-a1cb2e360947,Autonomous GIS: the next-generation AI-powered GIS,0.746296,24,"Large Language Models (LLMs), such as ChatGPT, demonstrate a strong
understanding of human natural language and have been explored and applied in
various fields, including reasoning, creative writing, code generation,
translation, and information retrieval. By adopting LLM as the reasoning core,
we introduce Autonomous GIS as an AI-powered geographic information system
(GIS) that leverages the LLM's general abilities in natural language
understanding, reasoning, and coding for addressing spatial problems with
automatic spatial data collection, analysis, and visualization. We envision
that autonomous GIS will need to achieve five autonomous goals:
self-generating, self-organizing, self-verifying, self-executing, and
self-growing. We developed a prototype system called LLM-Geo using the GPT-4
API in a Python environment, demonstrating what an autonomous GIS looks like
and how it delivers expected results without human intervention using three
case studies. For all case studies, LLM-Geo was able to return accurate
results, including aggregated numbers, graphs, and maps, significantly reducing
manual operation time. Although still in its infancy and lacking several
important modules such as logging and code testing, LLM-Geo demonstrates a
potential path toward the next-generation AI-powered GIS. We advocate for the
GIScience community to dedicate more effort to the research and development of
autonomous GIS, making spatial analysis easier, faster, and more accessible to
a broader audience.",None,-1
83748743-fcaa-4a58-b73c-feed0083c73a,Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora,0.695636,6,"Grammatical error correction (GEC) is the task of correcting typos, spelling,
punctuation and grammatical issues in text. Approaching the problem as a
sequence-to-sequence task, we compare the use of a common subword unit
vocabulary and byte-level encoding. Initial synthetic training data is created
using an error-generating pipeline, and used for finetuning two subword-level
models and one byte-level model. Models are then finetuned further on
hand-corrected error corpora, including texts written by children, university
students, dyslexic and second-language writers, and evaluated over different
error types and origins. We show that a byte-level model enables higher
correction quality than a subword approach, not only for simple spelling
errors, but also for more complex semantic, stylistic and grammatical issues.
In particular, initial training on synthetic corpora followed by finetuning on
a relatively small parallel corpus of real-world errors helps the byte-level
model correct a wide range of commonly occurring errors. Our experiments are
run for the Icelandic language but should hold for other similar languages,
particularly morphologically rich ones.",None,-1
4d12366e-01ff-4116-a083-e4b5f9723e74,Pointerformer: Deep Reinforced Multi-Pointer Transformer for the Traveling Salesman Problem,0.995723,15,"Traveling Salesman Problem (TSP), as a classic routing optimization problem
originally arising in the domain of transportation and logistics, has become a
critical task in broader domains, such as manufacturing and biology. Recently,
Deep Reinforcement Learning (DRL) has been increasingly employed to solve TSP
due to its high inference efficiency. Nevertheless, most of existing end-to-end
DRL algorithms only perform well on small TSP instances and can hardly
generalize to large scale because of the drastically soaring memory consumption
and computation time along with the enlarging problem scale. In this paper, we
propose a novel end-to-end DRL approach, referred to as Pointerformer, based on
multi-pointer Transformer. Particularly, Pointerformer adopts both reversible
residual network in the encoder and multi-pointer network in the decoder to
effectively contain memory consumption of the encoder-decoder architecture. To
further improve the performance of TSP solutions, Pointerformer employs both a
feature augmentation method to explore the symmetries of TSP at both training
and inference stages as well as an enhanced context embedding approach to
include more comprehensive context information in the query. Extensive
experiments on a randomly generated benchmark and a public benchmark have shown
that, while achieving comparative results on most small-scale TSP instances as
SOTA DRL approaches do, Pointerformer can also well generalize to large-scale
TSPs.",None,-1
150bb673-4bfc-4b46-9783-58d693aee3c6,RGB-T Tracking via Multi-Modal Mutual Prompt Learning,0.310338,1,"Object tracking based on the fusion of visible and thermal im-ages, known as
RGB-T tracking, has gained increasing atten-tion from researchers in recent
years. How to achieve a more comprehensive fusion of information from the two
modalities with fewer computational costs has been a problem that re-searchers
have been exploring. Recently, with the rise of prompt learning in computer
vision, we can better transfer knowledge from visual large models to downstream
tasks. Considering the strong complementarity between visible and thermal
modalities, we propose a tracking architecture based on mutual prompt learning
between the two modalities. We also design a lightweight prompter that
incorporates attention mechanisms in two dimensions to transfer information
from one modality to the other with lower computational costs, embedding it
into each layer of the backbone. Extensive ex-periments have demonstrated that
our proposed tracking ar-chitecture is effective and efficient, achieving
state-of-the-art performance while maintaining high running speeds.",None,-1
c32d2497-8f46-4baa-9a06-dd2a3109b69b,TG-VQA: Ternary Game of Video Question Answering,0.215858,7,"Video question answering aims at answering a question about the video content
by reasoning the alignment semantics within them. However, since relying
heavily on human instructions, i.e., annotations or priors, current contrastive
learning-based VideoQA methods remains challenging to perform fine-grained
visual-linguistic alignments. In this work, we innovatively resort to game
theory, which can simulate complicated relationships among multiple players
with specific interaction strategies, e.g., video, question, and answer as
ternary players, to achieve fine-grained alignment for VideoQA task.
Specifically, we carefully design a VideoQA-specific interaction strategy to
tailor the characteristics of VideoQA, which can mathematically generate the
fine-grained visual-linguistic alignment label without label-intensive efforts.
Our TG-VQA outperforms existing state-of-the-art by a large margin (more than
5%) on long-term and short-term VideoQA datasets, verifying its effectiveness
and generalization ability. Thanks to the guidance of game-theoretic
interaction, our model impressively convergences well on limited data (${10}^4
~videos$), surpassing most of those pre-trained on large-scale data
($10^7~videos$).",None,-1
e3ca8fef-9243-462b-ab84-3f5a91bad962,Structured State Space Models for Multiple Instance Learning in Digital Pathology,0.859252,8,"Multiple instance learning is an ideal mode of analysis for histopathology
data, where vast whole slide images are typically annotated with a single
global label. In such cases, a whole slide image is modelled as a collection of
tissue patches to be aggregated and classified. Common models for performing
this classification include recurrent neural networks and transformers.
Although powerful compression algorithms, such as deep pre-trained neural
networks, are used to reduce the dimensionality of each patch, the sequences
arising from whole slide images remain excessively long, routinely containing
tens of thousands of patches. Structured state space models are an emerging
alternative for sequence modelling, specifically designed for the efficient
modelling of long sequences. These models invoke an optimal projection of an
input sequence into memory units that compress the entire sequence. In this
paper, we propose the use of state space models as a multiple instance learner
to a variety of problems in digital pathology. Across experiments in metastasis
detection, cancer subtyping, mutation classification, and multitask learning,
we demonstrate the competitiveness of this new class of models with existing
state of the art approaches. Our code is available at
https://github.com/MICS-Lab/s4_digital_pathology.",None,-1
3d74e05b-382c-4a6c-8469-94bb86b4ec5c,TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models,0.65292,7,"Data augmentation has been established as an efficacious approach to
supplement useful information for low-resource datasets. Traditional
augmentation techniques such as noise injection and image transformations have
been widely used. In addition, generative data augmentation (GDA) has been
shown to produce more diverse and flexible data. While generative adversarial
networks (GANs) have been frequently used for GDA, they lack diversity and
controllability compared to text-to-image diffusion models. In this paper, we
propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the
capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image
(T2I) generative models for data augmentation. By conditioning the T2I model on
detailed descriptions produced by T2T models, we are able to generate
photo-realistic labeled images in a flexible and controllable manner.
Experiments on in-domain classification, cross-domain classification, and image
captioning tasks show consistent improvements over other data augmentation
baselines. Analytical studies in varied settings, including few-shot,
long-tail, and adversarial, further reinforce the effectiveness of TTIDA in
enhancing performance and increasing robustness.",None,-1
f830e094-7c7d-443d-a4b6-d3fd73d07353,Methodology for generating synthetic labeled datasets for visual container inspection,0.235579,2,"Nowadays, containerized freight transport is one of the most important
transportation systems that is undergoing an automation process due to the Deep
Learning success. However, it suffers from a lack of annotated data in order to
incorporate state-of-the-art neural network models to its systems. In this
paper we present an innovative methodology to generate a realistic, varied,
balanced, and labelled dataset for visual inspection task of containers in a
dock environment. In addition, we validate this methodology with multiple
visual tasks recurrently found in the state of the art. We prove that the
generated synthetic labelled dataset allows to train a deep neural network that
can be used in a real world scenario. On the other side, using this methodology
we provide the first open synthetic labelled dataset called SeaFront available
in: https://datasets.vicomtech.org/di21-seafront/readme.txt.",None,-1
77d6d65a-6ed2-4224-b071-c55e54eebc11,Teaching Probabilistic Logical Reasoning to Transformers,0.214351,2,"In this paper, we evaluate the capability of transformer-based language
models in making inferences over uncertain text that includes uncertain rules
of reasoning. We cover both Pre-trained Language Models (PLMs) and generative
Large Language Models (LLMs). Our evaluation results show that both generations
of language models struggle with reasoning over uncertain text. We propose a
novel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT),
that utilizes probabilistic logical rules as constraints in the fine-tuning
phase without relying on these rules in the inference stage. To assess the
effectiveness of PCT, we utilize the related corpora and, additionally, create
a new and more challenging benchmark that, unlike the previous ones, uses
instance-specific rules. Our study demonstrates that PCT improves the
transformer-based language model's intrinsic reasoning and makes their
probabilistic logical reasoning process more explicit and explainable.
Furthermore, PCT equips these models to effectively handle novel situations,
including higher reasoning depth, new domains, and complex probabilistic
structures.",None,-1
c2fab99e-09ea-4ef2-974d-fa529a75b538,Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on Summarizing Patients' Active Diagnoses and Problems from Electronic Health Record Progress Notes,0.611931,9,"The BioNLP Workshop 2023 initiated the launch of a shared task on Problem
List Summarization (ProbSum) in January 2023. The aim of this shared task is to
attract future research efforts in building NLP models for real-world
diagnostic decision support applications, where a system generating relevant
and accurate diagnoses will augment the healthcare providers decision-making
process and improve the quality of care for patients. The goal for participants
is to develop models that generated a list of diagnoses and problems using
input from the daily care notes collected from the hospitalization of
critically ill patients. Eight teams submitted their final systems to the
shared task leaderboard. In this paper, we describe the tasks, datasets,
evaluation metrics, and baseline systems. Additionally, the techniques and
results of the evaluation of the different approaches tried by the
participating teams are summarized.",None,-1
06981561-be9e-44ff-a135-7d5454d77187,Beyond Prompts: Exploring the Design Space of Mixed-Initiative Co-Creativity Systems,0.917915,12,"Generative Artificial Intelligence systems have been developed for image,
code, story, and game generation with the goal of facilitating human
creativity. Recent work on neural generative systems has emphasized one
particular means of interacting with AI systems: the user provides a
specification, usually in the form of prompts, and the AI system generates the
content. However, there are other configurations of human and AI coordination,
such as co-creativity (CC) in which both human and AI systems can contribute to
content creation, and mixed-initiative (MI) in which both human and AI systems
can initiate content changes. In this paper, we define a hypothetical human-AI
configuration design space consisting of different means for humans and AI
systems to communicate creative intent to each other. We conduct a human
participant study with 185 participants to understand how users want to
interact with differently configured MI-CC systems. We find out that MI-CC
systems with more extensive coverage of the design space are rated higher or on
par on a variety of creative and goal-completion metrics, demonstrating that
wider coverage of the design space can improve user experience and achievement
when using the system; Preference varies greatly between expertise groups,
suggesting the development of adaptive, personalized MI-CC systems;
Participants identified new design space dimensions including scrutability --
the ability to poke and prod at models -- and explainability.",None,-1
1628b028-99ab-4d61-8307-209d04fba2f1,Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?,0.999652,56,"We evaluated the capability of generative pre-trained transformers (GPT), to
pass assessments in introductory and intermediate Python programming courses at
the postsecondary level. Discussions of potential uses (e.g., exercise
generation, code explanation) and misuses (e.g., cheating) of this emerging
technology in programming education have intensified, but to date there has not
been a rigorous analysis of the models' capabilities in the realistic context
of a full-fledged programming course with diverse set of assessment
instruments. We evaluated GPT on three Python courses that employ assessments
ranging from simple multiple-choice questions (no code involved) to complex
programming projects with code bases distributed into multiple files (599
exercises overall). Further, we studied if and how successfully GPT models
leverage feedback provided by an auto-grader. We found that the current models
are not capable of passing the full spectrum of assessments typically involved
in a Python programming course (<70% on even entry-level modules). Yet, it is
clear that a straightforward application of these easily accessible models
could enable a learner to obtain a non-trivial portion of the overall available
score (>55%) in introductory and intermediate courses alike. While the models
exhibit remarkable capabilities, including correcting solutions based on
auto-grader's feedback, some limitations exist (e.g., poor handling of
exercises requiring complex chains of reasoning steps). These findings can be
leveraged by instructors wishing to adapt their assessments so that GPT becomes
a valuable assistant for a learner as opposed to an end-to-end solution.",None,-1
846d8c3d-5485-4ef2-b016-5de35a779f0b,CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models,0.00840268,1,"Parameter-efficient tuning (PET) has been widely explored in recent years
because it tunes much fewer parameters (PET modules) than full-parameter
fine-tuning (FT) while still stimulating sufficient knowledge from large
language models (LLMs) for downstream tasks. Moreover, when PET is employed to
serve multiple tasks, different task-specific PET modules can be built on a
frozen LLM, avoiding redundant LLM deployments. Although PET significantly
reduces the cost of tuning and deploying LLMs, its inference still suffers from
the computational bottleneck of LLMs. To address the above issue, we propose an
effective PET framework based on compressed LLMs, named ""CPET"". In CPET, we
evaluate the impact of mainstream LLM compression techniques on PET performance
and then introduce knowledge inheritance and recovery strategies to restore the
knowledge loss caused by these compression techniques. Our experimental results
demonstrate that, owing to the restoring strategies of CPET, collaborating
task-specific PET modules with a compressed LLM can achieve comparable
performance to collaborating PET modules with the original version of the
compressed LLM and outperform directly applying vanilla PET methods to the
compressed LLM.",None,-1
112d7ef6-2a62-4919-9bd7-870256c92489,SIO: Synthetic In-Distribution Data Benefits Out-of-Distribution Detection,0.147791,1,"Building up reliable Out-of-Distribution (OOD) detectors is challenging,
often requiring the use of OOD data during training. In this work, we develop a
data-driven approach which is distinct and complementary to existing works:
Instead of using external OOD data, we fully exploit the internal
in-distribution (ID) training set by utilizing generative models to produce
additional synthetic ID images. The classifier is then trained using a novel
objective that computes weighted loss on real and synthetic ID samples
together. Our training framework, which is termed SIO, serves as a
""plug-and-play"" technique that is designed to be compatible with existing and
future OOD detection algorithms, including the ones that leverage available OOD
training data. Our experiments on CIFAR-10, CIFAR-100, and ImageNet variants
demonstrate that SIO consistently improves the performance of nearly all
state-of-the-art (SOTA) OOD detection algorithms. For instance, on the
challenging CIFAR-10 v.s. CIFAR-100 detection problem, SIO improves the average
OOD detection AUROC of 18 existing methods from 86.25\% to 89.04\% and achieves
a new SOTA of 92.94\% according to the OpenOOD benchmark. Code is available at
https://github.com/zjysteven/SIO.",None,-1
f42fbe1c-5477-4d3c-bc52-cd194a612f59,Multilingual Word Error Rate Estimation: e-WER3,0.595366,4,"The success of the multilingual automatic speech recognition systems
empowered many voice-driven applications. However, measuring the performance of
such systems remains a major challenge, due to its dependency on manually
transcribed speech data in both mono- and multilingual scenarios. In this
paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on
acoustic and lexical representation to estimate word error rate. We demonstrate
the effectiveness of eWER3 to (i) predict WER without using any internal states
from the ASR and (ii) use the multilingual shared latent space to push the
performance of the close-related languages. We show our proposed multilingual
model outperforms the previous monolingual word error rate estimation method
(eWER2) by an absolute 9\% increase in Pearson correlation coefficient (PCC),
with better overall estimation between the predicted and reference WER.",None,-1
72ddd4dd-1171-495d-b4d8-3040ee0319ea,Multi Modal Facial Expression Recognition with Transformer-Based Fusion Networks and Dynamic Sampling,0.770541,6,"Facial expression recognition is an essential task for various applications,
including emotion detection, mental health analysis, and human-machine
interactions. In this paper, we propose a multi-modal facial expression
recognition method that exploits audio information along with facial images to
provide a crucial clue to differentiate some ambiguous facial expressions.
Specifically, we introduce a Modal Fusion Module (MFM) to fuse audio-visual
information, where image and audio features are extracted from Swin
Transformer. Additionally, we tackle the imbalance problem in the dataset by
employing dynamic data resampling. Our model has been evaluated in the
Affective Behavior in-the-wild (ABAW) challenge of CVPR 2023.",None,-1
d5ae9127-f96d-4e47-9cda-fd8dfee2b0ba,Exploring the Potential of Large Language Models to Generate Formative Programming Feedback,0.998081,27,"Ever since the emergence of large language models (LLMs) and related
applications, such as ChatGPT, its performance and error analysis for
programming tasks have been subject to research. In this work-in-progress
paper, we explore the potential of such LLMs for computing educators and
learners, as we analyze the feedback it generates to a given input containing
program code. In particular, we aim at (1) exploring how an LLM like ChatGPT
responds to students seeking help with their introductory programming tasks,
and (2) identifying feedback types in its responses. To achieve these goals, we
used students' programming sequences from a dataset gathered within a CS1
course as input for ChatGPT along with questions required to elicit feedback
and correct solutions. The results show that ChatGPT performs reasonably well
for some of the introductory programming tasks and student errors, which means
that students can potentially benefit. However, educators should provide
guidance on how to use the provided feedback, as it can contain misleading
information for novices.",None,-1
48feefd7-b99b-4502-91ab-6416a13e8809,How Much Context Does My Attention-Based ASR System Need?,0.184029,1,"For the task of speech recognition, the use of more than 30 seconds of
acoustic context during training is uncommon and under-investigated in
literature. In this work, we conduct an empirical study on the effect of
scaling the sequence length used to train/evaluate (dense-attention-based)
acoustic models on speech recognition performance. For these experiments, a
dataset of roughly 100,000 pseudo-labelled Spotify podcasts is used, with
context lengths of 5 seconds to 1 hour being explored. Zero-shot evaluations
are presented on the long-format datasets: Earnings-22, Tedlium and Rev16.
Results demonstrate a benefit from training with up to 21.8 minutes of acoustic
context, showing up to a 14.5\% relative improvement from a baseline trained
with 10 seconds of context. We find that the model's width/depth, positional
encoding scheme and number of attention heads impact its ability to use longer
contexts.",None,-1
38c0a05a-021e-483f-a52a-0d79962788fe,Fast Matrix Multiplication Without Tears: A Constraint Programming Approach,0.178464,1,"It is known that the multiplication of an $N \times M$ matrix with an $M
\times P$ matrix can be performed using fewer multiplications than what the
naive $NMP$ approach suggests. The most famous instance of this is Strassen's
algorithm for multiplying two $2\times 2$ matrices in 7 instead of 8
multiplications. This gives rise to the constraint satisfaction problem of fast
matrix multiplication, where a set of $R < NMP$ multiplication terms must be
chosen and combined such that they satisfy correctness constraints on the
output matrix. Despite its highly combinatorial nature, this problem has not
been exhaustively examined from that perspective, as evidenced for example by
the recent deep reinforcement learning approach of AlphaTensor. In this work,
we propose a simple yet novel Constraint Programming approach to find
non-commutative algorithms for fast matrix multiplication or provide proof of
infeasibility otherwise. We propose a set of symmetry-breaking constraints and
valid inequalities that are particularly helpful in proving infeasibility. On
the feasible side, we find that exploiting solver performance variability in
conjunction with a sparsity-based problem decomposition enables finding
solutions for larger (feasible) instances of fast matrix multiplication. Our
experimental results using CP Optimizer demonstrate that we can find fast
matrix multiplication algorithms for matrices up to $3\times 3$ in a short
amount of time.",None,-1
742b345a-a218-46ba-902e-26e60b42c236,From Knowledge Representation to Knowledge Organization and Back,0.291615,2,"Knowledge Representation (KR) and facet-analytical Knowledge Organization
(KO) have been the two most prominent methodologies of data and knowledge
modelling in the Artificial Intelligence community and the Information Science
community, respectively. KR boasts of a robust and scalable ecosystem of
technologies to support knowledge modelling while, often, underemphasizing the
quality of its models (and model-based data). KO, on the other hand, is less
technology-driven but has developed a robust framework of guiding principles
(canons) for ensuring modelling (and model-based data) quality. This paper
elucidates both the KR and facet-analytical KO methodologies in detail and
provides a functional mapping between them. Out of the mapping, the paper
proposes an integrated KO-enriched KR methodology with all the standard
components of a KR methodology plus the guiding canons of modelling quality
provided by KO. The practical benefits of the methodological integration has
been exemplified through a prominent case study of KR-based image annotation
exercise.",None,-1
41d14c9a-deda-4b85-a2f0-5419cd033063,BiLMa: Bidirectional Local-Matching for Text-based Person Re-identification,0.826398,3,"Text-based person re-identification (TBPReID) aims to retrieve person images
represented by a given textual query. In this task, how to effectively align
images and texts globally and locally is a crucial challenge. Recent works have
obtained high performances by solving Masked Language Modeling (MLM) to align
image/text parts. However, they only performed uni-directional (i.e., from
image to text) local-matching, leaving room for improvement by introducing
opposite-directional (i.e., from text to image) local-matching. In this work,
we introduce Bidirectional Local-Matching (BiLMa) framework that jointly
optimize MLM and Masked Image Modeling (MIM) in TBPReID model training. With
this framework, our model is trained so as the labels of randomly masked both
image and text tokens are predicted by unmasked tokens. In addition, to narrow
the semantic gap between image and text in MIM, we propose Semantic MIM
(SemMIM), in which the labels of masked image tokens are automatically given by
a state-of-the-art human parser. Experimental results demonstrate that our
BiLMa framework with SemMIM achieves state-of-the-art Rank@1 and mAP scores on
three benchmarks.",None,-1
39c71498-6b51-403c-af0d-6f1f358ab7be,Byzantine-Resilient Federated Learning at Edge,0.435448,10,"Both Byzantine resilience and communication efficiency have attracted
tremendous attention recently for their significance in edge federated
learning. However, most existing algorithms may fail when dealing with
real-world irregular data that behaves in a heavy-tailed manner. To address
this issue, we study the stochastic convex and non-convex optimization problem
for federated learning at edge and show how to handle heavy-tailed data while
retaining the Byzantine resilience, communication efficiency and the optimal
statistical error rates simultaneously. Specifically, we first present a
Byzantine-resilient distributed gradient descent algorithm that can handle the
heavy-tailed data and meanwhile converge under the standard assumptions. To
reduce the communication overhead, we further propose another algorithm that
incorporates gradient compression techniques to save communication costs during
the learning process. Theoretical analysis shows that our algorithms achieve
order-optimal statistical error rate in presence of Byzantine devices. Finally,
we conduct extensive experiments on both synthetic and real-world datasets to
verify the efficacy of our algorithms.",None,-1
28946420-28bd-4864-8279-e4d32748e367,Robust Deep Reinforcement Learning Scheduling via Weight Anchoring,0.14753,3,"Questions remain on the robustness of data-driven learning methods when
crossing the gap from simulation to reality. We utilize weight anchoring, a
method known from continual learning, to cultivate and fixate desired behavior
in Neural Networks. Weight anchoring may be used to find a solution to a
learning problem that is nearby the solution of another learning problem.
Thereby, learning can be carried out in optimal environments without neglecting
or unlearning desired behavior. We demonstrate this approach on the example of
learning mixed QoS-efficient discrete resource scheduling with infrequent
priority messages. Results show that this method provides performance
comparable to the state of the art of augmenting a simulation environment,
alongside significantly increased robustness and steerability.",None,-1
8c62d505-f8db-488b-8587-241f89bd7276,SwinFSR: Stereo Image Super-Resolution using SwinIR and Frequency Domain Knowledge,0.542926,7,"Stereo Image Super-Resolution (stereoSR) has attracted significant attention
in recent years due to the extensive deployment of dual cameras in mobile
phones, autonomous vehicles and robots. In this work, we propose a new StereoSR
method, named SwinFSR, based on an extension of SwinIR, originally designed for
single image restoration, and the frequency domain knowledge obtained by the
Fast Fourier Convolution (FFC). Specifically, to effectively gather global
information, we modify the Residual Swin Transformer blocks (RSTBs) in SwinIR
by explicitly incorporating the frequency domain knowledge using the FFC and
employing the resulting residual Swin Fourier Transformer blocks (RSFTBs) for
feature extraction. Besides, for the efficient and accurate fusion of stereo
views, we propose a new cross-attention module referred to as RCAM, which
achieves highly competitive performance while requiring less computational cost
than the state-of-the-art cross-attention modules. Extensive experimental
results and ablation studies demonstrate the effectiveness and efficiency of
our proposed SwinFSR.",None,-1
ac77efec-ec19-468b-a945-e0a21d811588,The MiniPile Challenge for Data-Efficient Language Models,0.476068,23,"The ever-growing diversity of pre-training text corpora has equipped language
models with generalization capabilities across various downstream tasks.
However, such diverse datasets are often too large for academic budgets; hence,
most research on Transformer architectures, training procedures, optimizers,
etc. gets conducted on smaller, homogeneous datasets. To this end, we present
The MiniPile Challenge, where one pre-trains a language model on a diverse text
corpus containing at most 1M documents. MiniPile is a 6GB subset of the
deduplicated 825GB The Pile corpus. To curate MiniPile, we perform a simple,
three-step data filtering process: we (1) infer embeddings for all documents of
the Pile, (2) cluster the embedding space using $k$-means, and (3) filter out
low-quality clusters. To verify MiniPile's suitability for language model
pre-training, we use it to pre-train a BERT and T5 model, yielding a
performance drop of only $1.9\%$/$2.5\%$ on the GLUE and SNI benchmarks
compared to the original pre-trained checkpoints trained on $2.6$x/$745$x the
amount of data. MiniPile is available at
https://huggingface.co/datasets/JeanKaddour/minipile.",None,-1
704eed5e-907d-48ec-b5b7-19b65e33dbf9,Does progress on ImageNet transfer to real-world datasets?,0.459414,20,"Does progress on ImageNet transfer to real-world datasets? We investigate
this question by evaluating ImageNet pre-trained models with varying accuracy
(57% - 83%) on six practical image classification datasets. In particular, we
study datasets collected with the goal of solving real-world tasks (e.g.,
classifying images from camera traps or satellites), as opposed to web-scraped
benchmarks collected for comparing models. On multiple datasets, models with
higher ImageNet accuracy do not consistently yield performance improvements.
For certain tasks, interventions such as data augmentation improve performance
even when architectures do not. We hope that future benchmarks will include
more diverse datasets to encourage a more comprehensive approach to improving
learning algorithms.",None,-1
3fe9a8c2-7df5-4e59-a3a2-5c9a63503f82,Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task,0.850962,5,"Large-scale language models (LLMs) has shown remarkable capability in various
of Natural Language Processing (NLP) tasks and attracted lots of attention
recently. However, some studies indicated that large language models fail to
achieve promising result beyond the state-of-the-art models in English
grammatical error correction (GEC) tasks. In this report, we aim to explore the
how large language models perform on Chinese grammatical error correction tasks
and provide guidance for future work. We conduct experiments with 3 different
LLMs of different model scale on 4 Chinese GEC dataset. Our experimental
results indicate that the performances of LLMs on automatic evaluation metrics
falls short of the previous sota models because of the problem of
over-correction. Furthermore, we also discover notable variations in the
performance of LLMs when evaluated on different data distributions. Our
findings demonstrates that further investigation is required for the
application of LLMs on Chinese GEC task.",None,-1
85c06942-18b1-4d5b-99bb-22cd468db51c,TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation,0.855161,16,"Test-time adaptation methods have been gaining attention recently as a
practical solution for addressing source-to-target domain gaps by gradually
updating the model without requiring labels on the target data. In this paper,
we propose a method of test-time adaptation for category-level object pose
estimation called TTA-COPE. We design a pose ensemble approach with a
self-training loss using pose-aware confidence. Unlike previous unsupervised
domain adaptation methods for category-level object pose estimation, our
approach processes the test data in a sequential, online manner, and it does
not require access to the source domain at runtime. Extensive experimental
results demonstrate that the proposed pose ensemble and the self-training loss
improve category-level object pose performance during test time under both
semi-supervised and unsupervised settings. Project page:
https://taeyeop.com/ttacope",None,-1
79c24eb3-d114-4eb3-965e-6ae25b503b33,HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling,0.996578,59,"Volumetric scene representations enable photorealistic view synthesis for
static scenes and form the basis of several existing 6-DoF video techniques.
However, the volume rendering procedures that drive these representations
necessitate careful trade-offs in terms of quality, rendering speed, and memory
efficiency. In particular, existing methods fail to simultaneously achieve
real-time performance, small memory footprint, and high-quality rendering for
challenging real-world scenes. To address these issues, we present HyperReel --
a novel 6-DoF video representation. The two core components of HyperReel are:
(1) a ray-conditioned sample prediction network that enables high-fidelity,
high frame rate rendering at high resolutions and (2) a compact and
memory-efficient dynamic volume representation. Our 6-DoF video pipeline
achieves the best performance compared to prior and contemporary approaches in
terms of visual quality with small memory requirements, while also rendering at
up to 18 frames-per-second at megapixel resolution without any custom CUDA
code.",None,-1
0c7434f7-9c7b-4362-b6b7-283208fa2f36,X-ReID: Cross-Instance Transformer for Identity-Level Person Re-Identification,0.358756,2,"Currently, most existing person re-identification methods use Instance-Level
features, which are extracted only from a single image. However, these
Instance-Level features can easily ignore the discriminative information due to
the appearance of each identity varies greatly in different images. Thus, it is
necessary to exploit Identity-Level features, which can be shared across
different images of each identity. In this paper, we propose to promote
Instance-Level features to Identity-Level features by employing cross-attention
to incorporate information from one image to another of the same identity, thus
more unified and discriminative pedestrian information can be obtained. We
propose a novel training framework named X-ReID. Specifically, a Cross
Intra-Identity Instances module (IntraX) fuses different intra-identity
instances to transfer Identity-Level knowledge and make Instance-Level features
more compact. A Cross Inter-Identity Instances module (InterX) involves hard
positive and hard negative instances to improve the attention response to the
same identity instead of different identity, which minimizes intra-identity
variation and maximizes inter-identity variation. Extensive experiments on
benchmark datasets show the superiority of our method over existing works.
Particularly, on the challenging MSMT17, our proposed method gains 1.1% mAP
improvements when compared to the second place.",None,-1
14305269-c706-489f-a83f-27c1d551e616,DinoSR: Self-Distillation and Online Clustering for Self-supervised Speech Representation Learning,0.666813,8,"In this paper, we introduce self-distillation and online clustering for
self-supervised speech representation learning (DinoSR) which combines masked
language modeling, self-distillation, and online clustering. We show that these
concepts complement each other and result in a strong representation learning
model for speech. DinoSR first extracts contextualized embeddings from the
input audio with a teacher network, then runs an online clustering system on
the embeddings to yield a machine-discovered phone inventory, and finally uses
the discretized tokens to guide a student network. We show that DinoSR
surpasses previous state-of-the-art performance in several downstream tasks,
and provide a detailed analysis of the model and the learned discrete units.",None,-1
92ebdc31-7c0a-4dac-9dac-e9fd4cb945eb,TUVF: Learning Generalizable Texture UV Radiance Fields,0.0632253,2,"Textures are a vital aspect of creating visually appealing and realistic 3D
models. In this paper, we study the problem of generating high-fidelity texture
given shapes of 3D assets, which has been relatively less explored compared
with generic 3D shape modeling. Our goal is to facilitate a controllable
texture generation process, such that one texture code can correspond to a
particular appearance style independent of any input shapes from a category. We
introduce Texture UV Radiance Fields (TUVF) that generate textures in a
learnable UV sphere space rather than directly on the 3D shape. This allows the
texture to be disentangled from the underlying shape and transferable to other
shapes that share the same UV space, i.e., from the same category. We integrate
the UV sphere space with the radiance field, which provides a more efficient
and accurate representation of textures than traditional texture maps. We
perform our experiments on synthetic and real-world object datasets where we
achieve not only realistic synthesis but also substantial improvements over
state-of-the-arts on texture controlling and editing. Project Page:
https://www.anjiecheng.me/TUVF",None,-1
fd3297ef-bf60-4373-a0e1-2d2129c6f6df,The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models,0.272042,1,"Partially Observable Markov Decision Processes (POMDPs) are used to model
environments where the full state cannot be perceived by an agent. As such the
agent needs to reason taking into account the past observations and actions.
However, simply remembering the full history is generally intractable due to
the exponential growth in the history space. Maintaining a probability
distribution that models the belief over what the true state is can be used as
a sufficient statistic of the history, but its computation requires access to
the model of the environment and is often intractable. While SOTA algorithms
use Recurrent Neural Networks to compress the observation-action history aiming
to learn a sufficient statistic, they lack guarantees of success and can lead
to sub-optimal policies. To overcome this, we propose the Wasserstein Belief
Updater, an RL algorithm that learns a latent model of the POMDP and an
approximation of the belief update. Our approach comes with theoretical
guarantees on the quality of our approximation ensuring that our outputted
beliefs allow for learning the optimal value function.",None,-1
0ea639c0-1103-4a81-a202-ccafa0de6363,Ontology Revision based on Pre-trained Language Models,0.42607,1,"Ontology revision aims to seamlessly incorporate a new ontology into an
existing ontology and plays a crucial role in tasks such as ontology evolution,
ontology maintenance, and ontology alignment. Similar to repair single
ontologies, resolving logical incoherence in the task of ontology revision is
also important and meaningful, because incoherence is a main potential factor
to cause inconsistency and reasoning with an inconsistent ontology will obtain
meaningless answers.To deal with this problem, various ontology revision
approaches have been proposed to define revision operators and design ranking
strategies for axioms in an ontology. However, they rarely consider axiom
semantics which provides important information to differentiate axioms. In
addition, pre-trained models can be utilized to encode axiom semantics, and
have been widely applied in many natural language processing tasks and
ontology-related ones in recent years.Therefore, in this paper, we study how to
apply pre-trained models to revise ontologies. We first define four scoring
functions to rank axioms based on a pre-trained model by considering various
information from an ontology. Based on the functions, an ontology revision
algorithm is then proposed to deal with unsatisfiable concepts at once. To
improve efficiency, an adapted revision algorithm is designed to deal with
unsatisfiable concepts group by group. We conduct experiments over 19 ontology
pairs and compare our algorithms and scoring functions with existing ones.
According to the experiments, our algorithms could achieve promising
performance.",None,-1
dbedd9c6-40dc-413b-8200-693cdfae159c,An Alternative to WSSS? An Empirical Study of the Segment Anything Model (SAM) on Weakly-Supervised Semantic Segmentation Problems,0.771619,12,"The Segment Anything Model (SAM) has demonstrated exceptional performance and
versatility, making it a promising tool for various related tasks. In this
report, we explore the application of SAM in Weakly-Supervised Semantic
Segmentation (WSSS). Particularly, we adapt SAM as the pseudo-label generation
pipeline given only the image-level class labels. While we observed impressive
results in most cases, we also identify certain limitations. Our study includes
performance evaluations on PASCAL VOC and MS-COCO, where we achieved remarkable
improvements over the latest state-of-the-art methods on both datasets. We
anticipate that this report encourages further explorations of adopting SAM in
WSSS, as well as wider real-world applications.",None,-1
4874b6b4-6e03-4178-9427-8c0cd872cc5d,Parmesan: mathematical concept extraction for education,0.507478,2,"Mathematics is a highly specialized domain with its own unique set of
challenges that has seen limited study in natural language processing. However,
mathematics is used in a wide variety of fields and multidisciplinary research
in many different domains often relies on an understanding of mathematical
concepts. To aid researchers coming from other fields, we develop a prototype
system for searching for and defining mathematical concepts in context,
focusing on the field of category theory. This system, Parmesan, depends on
natural language processing components including concept extraction, relation
extraction, definition extraction, and entity linking. In developing this
system, we show that existing techniques cannot be applied directly to the
category theory domain, and suggest hybrid techniques that do perform well,
though we expect the system to evolve over time. We also provide two cleaned
mathematical corpora that power the prototype system, which are based on
journal articles and wiki pages, respectively. The corpora have been annotated
with dependency trees, lemmas, and part-of-speech tags.",None,-1
59471178-437a-4386-8b03-ec9ff2258a31,Voxel or Pillar: Exploring Efficient Point Cloud Representation for 3D Object Detection,0.129441,1,"Efficient representation of point clouds is fundamental for LiDAR-based 3D
object detection. While recent grid-based detectors often encode point clouds
into either voxels or pillars, the distinctions between these approaches remain
underexplored. In this paper, we quantify the differences between the current
encoding paradigms and highlight the limited vertical learning within. To
tackle these limitations, we introduce a hybrid Voxel-Pillar Fusion network
(VPF), which synergistically combines the unique strengths of both voxels and
pillars. Specifically, we first develop a sparse voxel-pillar encoder that
encodes point clouds into voxel and pillar features through 3D and 2D sparse
convolutions respectively, and then introduce the Sparse Fusion Layer (SFL),
facilitating bidirectional interaction between sparse voxel and pillar
features. Our efficient, fully sparse method can be seamlessly integrated into
both dense and sparse detectors. Leveraging this powerful yet straightforward
framework, VPF delivers competitive performance, achieving real-time inference
speeds on the nuScenes and Waymo Open Dataset. The code will be available.",None,-1
7f6a27ac-c5b8-4831-86ea-c45fbda19a3c,Aligning Bag of Regions for Open-Vocabulary Object Detection,0.856991,56,"Pre-trained vision-language models (VLMs) learn to align vision and language
representations on large-scale datasets, where each image-text pair usually
contains a bag of semantic concepts. However, existing open-vocabulary object
detectors only align region embeddings individually with the corresponding
features extracted from the VLMs. Such a design leaves the compositional
structure of semantic concepts in a scene under-exploited, although the
structure may be implicitly learned by the VLMs. In this work, we propose to
align the embedding of bag of regions beyond individual regions. The proposed
method groups contextually interrelated regions as a bag. The embeddings of
regions in a bag are treated as embeddings of words in a sentence, and they are
sent to the text encoder of a VLM to obtain the bag-of-regions embedding, which
is learned to be aligned to the corresponding features extracted by a frozen
VLM. Applied to the commonly used Faster R-CNN, our approach surpasses the
previous best results by 4.6 box AP50 and 2.8 mask AP on novel categories of
open-vocabulary COCO and LVIS benchmarks, respectively. Code and models are
available at https://github.com/wusize/ovdet.",None,-1
4f29243a-71a8-4f3e-995e-658031feaf2d,"Point-Query Quadtree for Crowd Counting, Localization, and More",0.823747,11,"We show that crowd counting can be viewed as a decomposable point querying
process. This formulation enables arbitrary points as input and jointly reasons
whether the points are crowd and where they locate. The querying processing,
however, raises an underlying problem on the number of necessary querying
points. Too few imply underestimation; too many increase computational
overhead. To address this dilemma, we introduce a decomposable structure, i.e.,
the point-query quadtree, and propose a new counting model, termed Point quEry
Transformer (PET). PET implements decomposable point querying via
data-dependent quadtree splitting, where each querying point could split into
four new points when necessary, thus enabling dynamic processing of sparse and
dense regions. Such a querying process yields an intuitive, universal modeling
of crowd as both the input and output are interpretable and steerable. We
demonstrate the applications of PET on a number of crowd-related tasks,
including fully-supervised crowd counting and localization, partial annotation
learning, and point annotation refinement, and also report state-of-the-art
performance. For the first time, we show that a single counting model can
address multiple crowd-related tasks across different learning paradigms. Code
is available at https://github.com/cxliu0/PET.",None,-1
1a667b77-157b-420b-bcb3-87ed5276fd1c,"3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement",0.729421,6,"Disentangling uncorrelated information in speech utterances is a crucial
research topic within speech community. Different speech-related tasks focus on
extracting distinct speech representations while minimizing the affects of
other uncorrelated information. We present a large-scale speech corpus to
facilitate the research of speech representation disentanglement. 3D-Speaker
contains over 10,000 speakers, each of whom are simultaneously recorded by
multiple Devices, locating at different Distances, and some speakers are
speaking multiple Dialects. The controlled combinations of multi-dimensional
audio data yield a matrix of a diverse blend of speech representation
entanglement, thereby motivating intriguing methods to untangle them. The
multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate
large universal speech models and experiment methods of out-of-domain learning
and self-supervised learning. https://3dspeaker.github.io/",None,-1
022a5efa-ac5c-47d1-9513-4ded5197a5bd,Dense Text-to-Image Generation with Attention Modulation,0.843161,50,"Existing text-to-image diffusion models struggle to synthesize realistic
images given dense captions, where each text prompt provides a detailed
description for a specific image region. To address this, we propose
DenseDiffusion, a training-free method that adapts a pre-trained text-to-image
model to handle such dense captions while offering control over the scene
layout. We first analyze the relationship between generated images' layouts and
the pre-trained model's intermediate attention maps. Next, we develop an
attention modulation method that guides objects to appear in specific regions
according to layout guidance. Without requiring additional fine-tuning or
datasets, we improve image generation performance given dense captions
regarding both automatic and human evaluation scores. In addition, we achieve
similar-quality visual results with models specifically trained with layout
conditions.",None,-1
cd6b1a7a-7bfc-4cc8-a31f-d5cf3a1c2ca3,MoDAR: Using Motion Forecasting for 3D Object Detection in Point Cloud Sequences,0.531145,5,"Occluded and long-range objects are ubiquitous and challenging for 3D object
detection. Point cloud sequence data provide unique opportunities to improve
such cases, as an occluded or distant object can be observed from different
viewpoints or gets better visibility over time. However, the efficiency and
effectiveness in encoding long-term sequence data can still be improved. In
this work, we propose MoDAR, using motion forecasting outputs as a type of
virtual modality, to augment LiDAR point clouds. The MoDAR modality propagates
object information from temporal contexts to a target frame, represented as a
set of virtual points, one for each object from a waypoint on a forecasted
trajectory. A fused point cloud of both raw sensor points and the virtual
points can then be fed to any off-the-shelf point-cloud based 3D object
detector. Evaluated on the Waymo Open Dataset, our method significantly
improves prior art detectors by using motion forecasting from extra-long
sequences (e.g. 18 seconds), achieving new state of the arts, while not adding
much computation overhead.",None,-1
ed522fd6-9513-49a3-a864-78abcbeab90c,Enhancing Low-resolution Face Recognition with Feature Similarity Knowledge Distillation,0.233339,2,"In this study, we introduce a feature knowledge distillation framework to
improve low-resolution (LR) face recognition performance using knowledge
obtained from high-resolution (HR) images. The proposed framework transfers
informative features from an HR-trained network to an LR-trained network by
reducing the distance between them. A cosine similarity measure was employed as
a distance metric to effectively align the HR and LR features. This approach
differs from conventional knowledge distillation frameworks, which use the L_p
distance metrics and offer the advantage of converging well when reducing the
distance between features of different resolutions. Our framework achieved a 3%
improvement over the previous state-of-the-art method on the AgeDB-30 benchmark
without bells and whistles, while maintaining a strong performance on HR
images. The effectiveness of cosine similarity as a distance metric was
validated through statistical analysis, making our approach a promising
solution for real-world applications in which LR images are frequently
encountered. The code and pretrained models are publicly available on
https://github.com/gist-ailab/feature-similarity-KD.",None,-1
44b66100-f44f-43e4-8fe1-dcf8d35ca594,Learning from Children: Improving Image-Caption Pretraining via Curriculum,0.0330592,1,"Image-caption pretraining has been quite successfully used for downstream
vision tasks like zero-shot image classification and object detection. However,
image-caption pretraining is still a hard problem -- it requires multiple
concepts (nouns) from captions to be aligned to several objects in images. To
tackle this problem, we go to the roots -- the best learner, children. We take
inspiration from cognitive science studies dealing with children's language
learning to propose a curriculum learning framework. The learning begins with
easy-to-align image caption pairs containing one concept per caption. The
difficulty is progressively increased with each new phase by adding one more
concept per caption. Correspondingly, the knowledge acquired in each learning
phase is utilized in subsequent phases to effectively constrain the learning
problem to aligning one new concept-object pair in each phase. We show that
this learning strategy improves over vanilla image-caption training in various
settings -- pretraining from scratch, using a pretrained image or/and
pretrained text encoder, low data regime etc.",None,-1
24af0096-2348-4ebd-8e82-afaceab6bf23,ACI-BENCH: a Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation,0.947131,28,"Recent immense breakthroughs in generative models such as in GPT4 have
precipitated re-imagined ubiquitous usage of these models in all applications.
One area that can benefit by improvements in artificial intelligence (AI) is
healthcare. The note generation task from doctor-patient encounters, and its
associated electronic medical record documentation, is one of the most arduous
time-consuming tasks for physicians. It is also a natural prime potential
beneficiary to advances in generative models. However with such advances,
benchmarking is more critical than ever. Whether studying model weaknesses or
developing new evaluation metrics, shared open datasets are an imperative part
of understanding the current state-of-the-art. Unfortunately as clinic
encounter conversations are not routinely recorded and are difficult to
ethically share due to patient confidentiality, there are no sufficiently large
clinic dialogue-note datasets to benchmark this task. Here we present the
Ambient Clinical Intelligence Benchmark (ACI-BENCH) corpus, the largest dataset
to date tackling the problem of AI-assisted note generation from visit
dialogue. We also present the benchmark performances of several common
state-of-the-art approaches.",None,-1
398ad47a-9af1-4ab4-ad8e-eec143cdcb13,Multi-View Keypoints for Reliable 6D Object Pose Estimation,0.572846,3,"6D Object pose estimation is a fundamental component in robotics enabling
efficient interaction with the environment. It is particularly challenging in
bin-picking applications, where many objects are low-feature and reflective,
and self-occlusion between objects of the same type is common. We propose a
novel multi-view approach leveraging known camera transformations from an
eye-in-hand setup to combine heatmap and keypoint estimates into a probability
density map over 3D space. The result is a robust approach that is scalable in
the number of views. It relies on a confidence score composed of keypoint
probabilities and point-cloud alignment error, which allows reliable rejection
of false positives. We demonstrate an average pose estimation error of
approximately 0.5mm and 2 degrees across a variety of difficult low-feature and
reflective objects in the ROBI dataset, while also surpassing the state-of-art
correct detection rate, measured using the 10% object diameter threshold on ADD
error.",None,-1
cd6c3015-af40-478c-9fbb-9faa33d4881b,Do LLM Agents Exhibit Social Behavior?,0.91663,10,"The advances of Large Language Models (LLMs) are expanding their utility in
both academic research and practical applications. Recent social science
research has explored the use of these ``black-box'' LLM agents for simulating
complex social systems and potentially substituting human subjects in
experiments. Our study delves into this emerging domain, investigating the
extent to which LLMs exhibit key social interaction principles, such as social
learning, social preference, and cooperative behavior (indirect reciprocity),
in their interactions with humans and other agents. We develop a framework for
our study, wherein classical laboratory experiments involving human subjects
are adapted to use LLM agents. This approach involves step-by-step reasoning
that mirrors human cognitive processes and zero-shot learning to assess the
innate preferences of LLMs. Our analysis of LLM agents' behavior includes both
the primary effects and an in-depth examination of the underlying mechanisms.
Focusing on GPT-4, our analyses suggest that LLM agents appear to exhibit a
range of human-like social behaviors such as distributional and reciprocity
preferences, responsiveness to group identity cues, engagement in indirect
reciprocity, and social learning capabilities. However, our analysis also
reveals notable differences: LLMs demonstrate a pronounced fairness preference,
weaker positive reciprocity, and a more calculating approach in social learning
compared to humans. These insights indicate that while LLMs hold great promise
for applications in social science research, such as in laboratory experiments
and agent-based modeling, the subtle behavioral differences between LLM agents
and humans warrant further investigation. Careful examination and development
of protocols in evaluating the social behaviors of LLMs are necessary before
directly applying these models to emulate human behavior.",None,-1
ccb13440-515d-4eac-bcd3-e39c8b4d3667,DeGPR: Deep Guided Posterior Regularization for Multi-Class Cell Detection and Counting,0.523711,5,"Multi-class cell detection and counting is an essential task for many
pathological diagnoses. Manual counting is tedious and often leads to
inter-observer variations among pathologists. While there exist multiple,
general-purpose, deep learning-based object detection and counting methods,
they may not readily transfer to detecting and counting cells in medical
images, due to the limited data, presence of tiny overlapping objects, multiple
cell types, severe class-imbalance, minute differences in size/shape of cells,
etc. In response, we propose guided posterior regularization (DeGPR), which
assists an object detector by guiding it to exploit discriminative features
among cells. The features may be pathologist-provided or inferred directly from
visual data. We validate our model on two publicly available datasets (CoNSeP
and MoNuSAC), and on MuCeD, a novel dataset that we contribute. MuCeD consists
of 55 biopsy images of the human duodenum for predicting celiac disease. We
perform extensive experimentation with three object detection baselines on
three datasets to show that DeGPR is model-agnostic, and consistently improves
baselines obtaining up to 9% (absolute) mAP gains.",None,-1
4bcf7992-9fe6-46a7-b6ab-679aea86b3ee,Maximal Ordinal Two-Factorizations,0.197092,2,"Given a formal context, an ordinal factor is a subset of its incidence
relation that forms a chain in the concept lattice, i.e., a part of the dataset
that corresponds to a linear order. To visualize the data in a formal context,
Ganter and Glodeanu proposed a biplot based on two ordinal factors. For the
biplot to be useful, it is important that these factors comprise as much data
points as possible, i.e., that they cover a large part of the incidence
relation. In this work, we investigate such ordinal two-factorizations. First,
we investigate for formal contexts that omit ordinal two-factorizations the
disjointness of the two factors. Then, we show that deciding on the existence
of two-factorizations of a given size is an NP-complete problem which makes
computing maximal factorizations computationally expensive. Finally, we provide
the algorithm Ord2Factor that allows us to compute large ordinal
two-factorizations.",None,-1
6642ba50-051b-4210-8056-2b4dd38ff868,Language Models can be Logical Solvers,0.435382,11,"Logical reasoning is a fundamental aspect of human intelligence and a key
component of tasks like problem-solving and decision-making. Recent
advancements have enabled Large Language Models (LLMs) to potentially exhibit
reasoning capabilities, but complex logical reasoning remains a challenge. The
state-of-the-art, solver-augmented language models, use LLMs to parse natural
language logical questions into symbolic representations first and then adopt
external logical solvers to take in the symbolic representations and output the
answers. Despite their impressive performance, any parsing errors will
inevitably result in the failure of the execution of the external logical
solver and no answer to the logical questions. In this paper, we introduce
LoGiPT, a novel language model that directly emulates the reasoning processes
of logical solvers and bypasses the parsing errors by learning to strict
adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly
constructed instruction-tuning dataset derived from revealing and refining the
invisible reasoning process of deductive solvers. Experimental results on two
public deductive reasoning datasets demonstrate that LoGiPT outperforms
state-of-the-art solver-augmented LMs and few-shot prompting methods on
competitive LLMs like ChatGPT or GPT-4.",None,-1
6260d486-cf22-4565-8a57-8b3bfed1256b,The Effect of Information Type on Human Cognitive Augmentation,0.145252,2,"When performing a task alone, humans achieve a certain level of performance.
When humans are assisted by a tool or automation to perform the same task,
performance is enhanced (augmented). Recently developed cognitive systems are
able to perform cognitive processing at or above the level of a human in some
domains. When humans work collaboratively with such cogs in a human/cog
ensemble, we expect augmentation of cognitive processing to be evident and
measurable. This paper shows the degree of cognitive augmentation depends on
the nature of the information the cog contributes to the ensemble. Results of
an experiment are reported showing conceptual information is the most effective
type of information resulting in increases in cognitive accuracy, cognitive
precision, and cognitive power.",None,-1
94922363-1c0f-4c63-aa81-d70a531fff61,Cognitive Architecture Toward Common Ground Sharing Among Humans and Generative AIs: Trial on Model-Model Interactions in Tangram Naming Task,0.441829,1,"For generative AIs to be trustworthy, establishing transparent common
grounding with humans is essential. As a preparation toward human-model common
grounding, this study examines the process of model-model common grounding. In
this context, common ground is defined as a cognitive framework shared among
agents in communication, enabling the connection of symbols exchanged between
agents to the meanings inherent in each agent. This connection is facilitated
by a shared cognitive framework among the agents involved. In this research, we
focus on the tangram naming task (TNT) as a testbed to examine the
common-ground-building process. Unlike previous models designed for this task,
our approach employs generative AIs to visualize the internal processes of the
model. In this task, the sender constructs a metaphorical image of an abstract
figure within the model and generates a detailed description based on this
image. The receiver interprets the generated description from the partner by
constructing another image and reconstructing the original abstract figure.
Preliminary results from the study show an improvement in task performance
beyond the chance level, indicating the effect of the common cognitive
framework implemented in the models. Additionally, we observed that incremental
backpropagations leveraging successful communication cases for a component of
the model led to a statistically significant increase in performance. These
results provide valuable insights into the mechanisms of common grounding made
by generative AIs, improving human communication with the evolving intelligent
machines in our future society.",None,-1
0a7d7339-e379-4fff-b9bb-498543dd1b42,Better Diffusion Models Further Improve Adversarial Training,0.999596,108,"It has been recognized that the data generated by the denoising diffusion
probabilistic model (DDPM) improves adversarial training. After two years of
rapid development in diffusion models, a question naturally arises: can better
diffusion models further improve adversarial training? This paper gives an
affirmative answer by employing the most recent diffusion model which has
higher efficiency ($\sim 20$ sampling steps) and image quality (lower FID
score) compared with DDPM. Our adversarially trained models achieve
state-of-the-art performance on RobustBench using only generated data (no
external datasets). Under the $\ell_\infty$-norm threat model with
$\epsilon=8/255$, our models achieve $70.69\%$ and $42.67\%$ robust accuracy on
CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous
state-of-the-art models by $+4.58\%$ and $+8.03\%$. Under the $\ell_2$-norm
threat model with $\epsilon=128/255$, our models achieve $84.86\%$ on CIFAR-10
($+4.44\%$). These results also beat previous works that use external data. We
also provide compelling results on the SVHN and TinyImageNet datasets. Our code
is available at https://github.com/wzekai99/DM-Improves-AT.",None,-1
470490d7-6624-4b44-b39e-bd25fd288c71,An optimization method for out-of-distribution anomaly detection models,0.0740389,1,"Frequent false alarms impede the promotion of unsupervised anomaly detection
algorithms in industrial applications. Potential characteristics of false
alarms depending on the trained detector are revealed by investigating density
probability distributions of prediction scores in the out-of-distribution
anomaly detection tasks. An SVM-based classifier is exploited as a
post-processing module to identify false alarms from the anomaly map at the
object level. Besides, a sample synthesis strategy is devised to incorporate
fuzzy prior knowledge on the specific application in the anomaly-free training
dataset. Experimental results illustrate that the proposed method
comprehensively improves the performances of two segmentation models at both
image and pixel levels on two industrial applications.",None,-1
1df89941-bac9-44e3-aaec-d748d0e0cc32,Language Model Analysis for Ontology Subsumption Inference,0.987982,13,"Investigating whether pre-trained language models (LMs) can function as
knowledge bases (KBs) has raised wide research interests recently. However,
existing works focus on simple, triple-based, relational KBs, but omit more
sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To
investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of
inference-based probing tasks and datasets from ontology subsumption axioms
involving both atomic and complex concepts. We conduct extensive experiments on
ontologies of different domains and scales, and our results demonstrate that
LMs encode relatively less background knowledge of Subsumption Inference (SI)
than traditional Natural Language Inference (NLI) but can improve on SI
significantly when a small number of samples are given. We will open-source our
code and datasets.",None,-1
3a805517-e757-4d81-a536-27c329debdb2,Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning,0.570882,3,"Knowledge graph (KG) embedding has been used to benefit the diagnosis of
animal diseases by analyzing electronic medical records (EMRs), such as notes
and veterinary records. However, learning representations to capture entities
and relations with literal information in KGs is challenging as the KGs show
heterogeneous properties and various types of literal information. Meanwhile,
the existing methods mostly aim to preserve graph structures surrounding target
nodes without considering different types of literals, which could also carry
significant information. In this paper, we propose a knowledge graph embedding
model for the efficient diagnosis of animal diseases, which could learn various
types of literal information and graph structure and fuse them into unified
representations, namely LiteralKG. Specifically, we construct a knowledge graph
that is built from EMRs along with literal information collected from various
animal hospitals. We then fuse different types of entities and node feature
information into unified vector representations through gate networks. Finally,
we propose a self-supervised learning task to learn graph structure in pretext
tasks and then towards various downstream tasks. Experimental results on link
prediction tasks demonstrate that our model outperforms the baselines that
consist of state-of-the-art models. The source code is available at
https://github.com/NSLab-CUK/LiteralKG.",None,-1
737ef357-e6b3-4632-843b-0a5a38a40daa,Understanding Natural Language Understanding Systems. A Critical Analysis,0.275561,7,"The development of machines that {\guillemotleft}talk like
us{\guillemotright}, also known as Natural Language Understanding (NLU)
systems, is the Holy Grail of Artificial Intelligence (AI), since language is
the quintessence of human intelligence. The brief but intense life of NLU
research in AI and Natural Language Processing (NLP) is full of ups and downs,
with periods of high hopes that the Grail is finally within reach, typically
followed by phases of equally deep despair and disillusion. But never has the
trust that we can build {\guillemotleft}talking machines{\guillemotright} been
stronger than the one engendered by the last generation of NLU systems. But is
it gold all that glitters in AI? do state-of-the-art systems possess something
comparable to the human knowledge of language? Are we at the dawn of a new era,
in which the Grail is finally closer to us? In fact, the latest achievements of
AI systems have sparkled, or better renewed, an intense scientific debate on
their true language understanding capabilities. Some defend the idea that, yes,
we are on the right track, despite the limits that computational models still
show. Others are instead radically skeptic and even dismissal: The present
limits are not just contingent and temporary problems of NLU systems, but the
sign of the intrinsic inadequacy of the epistemological and technological
paradigm grounding them. This paper aims at contributing to such debate by
carrying out a critical analysis of the linguistic abilities of the most recent
NLU systems. I contend that they incorporate important aspects of the way
language is learnt and processed by humans, but at the same time they lack key
interpretive and inferential skills that it is unlikely they can attain unless
they are integrated with structured knowledge and the ability to exploit it for
language use.",None,-1
e38a3229-fb58-49cc-963c-12c904d46b24,Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions,0.410679,15,"Societal biases present in pre-trained large language models are a critical
issue as these models have been shown to propagate biases in countless
downstream applications, rendering them unfair towards specific groups of
people. Since large-scale retraining of these models from scratch is both time
and compute-expensive, a variety of approaches have been previously proposed
that de-bias a pre-trained model. While the majority of current
state-of-the-art debiasing methods focus on changes to the training regime, in
this paper, we propose data intervention strategies as a powerful yet simple
technique to reduce gender bias in pre-trained models. Specifically, we
empirically show that by fine-tuning a pre-trained model on only 10 de-biased
(intervened) training examples, the tendency to favor any gender is
significantly reduced. Since our proposed method only needs a few training
examples, our few-shot debiasing approach is highly feasible and practical.
Through extensive experimentation, we show that our debiasing technique
performs better than competitive state-of-the-art baselines with minimal loss
in language modeling ability.",None,-1
b8408ea9-776a-4d77-976c-3c603db7c06a,3-Objective Pareto Optimization for Problems with Chance Constraints,0.200416,2,"Evolutionary multi-objective algorithms have successfully been used in the
context of Pareto optimization where a given constraint is relaxed into an
additional objective. In this paper, we explore the use of 3-objective
formulations for problems with chance constraints. Our formulation trades off
the expected cost and variance of the stochastic component as well as the given
deterministic constraint. We point out benefits that this 3-objective
formulation has compared to a bi-objective one recently investigated for chance
constraints with Normally distributed stochastic components. Our analysis shows
that the 3-objective formulation allows to compute all required trade-offs
using 1-bit flips only, when dealing with a deterministic cardinality
constraint. Furthermore, we carry out experimental investigations for the
chance constrained dominating set problem and show the benefit for this
classical NP-hard problem.",None,-1
5ab1cf87-b8fb-4a17-a346-b4858b022b5b,Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation,0.591151,6,"Multilingualism is widespread around the world and code-switching (CSW) is a
common practice among different language pairs/tuples across locations and
regions. However, there is still not much progress in building successful CSW
systems, despite the recent advances in Massive Multilingual Language Models
(MMLMs). We investigate the reasons behind this setback through a critical
study about the existing CSW data sets (68) across language pairs in terms of
the collection and preparation (e.g. transcription and annotation) stages. This
in-depth analysis reveals that \textbf{a)} most CSW data involves English
ignoring other language pairs/tuples \textbf{b)} there are flaws in terms of
representativeness in data collection and preparation stages due to ignoring
the location based, socio-demographic and register variation in CSW. In
addition, lack of clarity on the data selection and filtering stages shadow the
representativeness of CSW data sets. We conclude by providing a short
check-list to improve the representativeness for forthcoming studies involving
CSW data collection and preparation.",None,-1
2fa2bd7a-0355-4fea-a2d3-1cd162783cf5,Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning,0.569656,1,"Clinical note classification is a common clinical NLP task. However,
annotated data-sets are scarse. Prompt-based learning has recently emerged as
an effective method to adapt pre-trained models for text classification using
only few training examples. A critical component of prompt design is the
definition of the template (i.e. prompt text). The effect of template position,
however, has been insufficiently investigated. This seems particularly
important in the clinical setting, where task-relevant information is usually
sparse in clinical notes. In this study we develop a keyword-optimized template
insertion method (KOTI) and show how optimizing position can improve
performance on several clinical tasks in a zero-shot and few-shot training
setting.",None,-1
0a4ed4a4-f3c7-4ae6-b412-3a85a4f3e5d2,Empowering Wildlife Guardians: An Equitable Digital Stewardship and Reward System for Biodiversity Conservation using Deep Learning and 3/4G Camera Traps,0.744144,5,"The biodiversity of our planet is under threat, with approximately one
million species expected to become extinct within decades. The reason; negative
human actions, which include hunting, overfishing, pollution, and the
conversion of land for urbanisation and agricultural purposes. Despite
significant investment from charities and governments for activities that
benefit nature, global wildlife populations continue to decline. Local wildlife
guardians have historically played a critical role in global conservation
efforts and have shown their ability to achieve sustainability at various
levels. In 2021, COP26 recognised their contributions and pledged US$1.7
billion per year; however, this is a fraction of the global biodiversity budget
available (between US$124 billion and US$143 billion annually) given they
protect 80% of the planets biodiversity. This paper proposes a radical new
solution based on ""Interspecies Money,"" where animals own their own money.
Creating a digital twin for each species allows animals to dispense funds to
their guardians for the services they provide. For example, a rhinoceros may
release a payment to its guardian each time it is detected in a camera trap as
long as it remains alive and well. To test the efficacy of this approach 27
camera traps were deployed over a 400km2 area in Welgevonden Game Reserve in
Limpopo Province in South Africa. The motion-triggered camera traps were
operational for ten months and, using deep learning, we managed to capture
images of 12 distinct animal species. For each species, a makeshift bank
account was set up and credited with {\pounds}100. Each time an animal was
captured in a camera and successfully classified, 1 penny (an arbitrary amount
- mechanisms still need to be developed to determine the real value of species)
was transferred from the animal account to its associated guardian.",None,-1
5f480d75-d0b0-49c0-b0f8-8e202590dc0d,Composing Task Knowledge with Modular Successor Feature Approximators,0.532457,3,"Recently, the Successor Features and Generalized Policy Improvement (SF&GPI)
framework has been proposed as a method for learning, composing, and
transferring predictive knowledge and behavior. SF&GPI works by having an agent
learn predictive representations (SFs) that can be combined for transfer to new
tasks with GPI. However, to be effective this approach requires state features
that are useful to predict, and these state-features are typically
hand-designed. In this work, we present a novel neural network architecture,
""Modular Successor Feature Approximators"" (MSFA), where modules both discover
what is useful to predict, and learn their own predictive representations. We
show that MSFA is able to better generalize compared to baseline architectures
for learning SFs and modular architectures",None,-1
0ff270e6-390b-413f-9816-896190b424cc,ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior Constraints,0.0363728,1,"Recent text-to-image generative models have enabled us to transform our words
into vibrant, captivating imagery. The surge of personalization techniques that
has followed has also allowed us to imagine unique concepts in new scenes.
However, an intriguing question remains: How can we generate a new, imaginary
concept that has never been seen before? In this paper, we present the task of
creative text-to-image generation, where we seek to generate new members of a
broad category (e.g., generating a pet that differs from all existing pets). We
leverage the under-studied Diffusion Prior models and show that the creative
generation problem can be formulated as an optimization process over the output
space of the diffusion prior, resulting in a set of ""prior constraints"". To
keep our generated concept from converging into existing members, we
incorporate a question-answering Vision-Language Model (VLM) that adaptively
adds new constraints to the optimization problem, encouraging the model to
discover increasingly more unique creations. Finally, we show that our prior
constraints can also serve as a strong mixing mechanism allowing us to create
hybrids between generated concepts, introducing even more flexibility into the
creative process.",None,-1
8e7ff3a9-fdc1-4d1c-a428-da7c4438fa14,"TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models",0.999753,17,"With the promotion of chatgpt to the public, Large language models indeed
showcase remarkable common sense, reasoning, and planning skills, frequently
providing insightful guidance. These capabilities hold significant promise for
their application in urban traffic management and control. However, LLMs
struggle with addressing traffic issues, especially processing numerical data
and interacting with simulations, limiting their potential in solving
traffic-related challenges. In parallel, specialized traffic foundation models
exist but are typically designed for specific tasks with limited input-output
interactions. Combining these models with LLMs presents an opportunity to
enhance their capacity for tackling complex traffic-related problems and
providing insightful suggestions. To bridge this gap, we present TrafficGPT, a
fusion of ChatGPT and traffic foundation models. This integration yields the
following key enhancements: 1) empowering ChatGPT with the capacity to view,
analyze, process traffic data, and provide insightful decision support for
urban transportation system management; 2) facilitating the intelligent
deconstruction of broad and complex tasks and sequential utilization of traffic
foundation models for their gradual completion; 3) aiding human decision-making
in traffic control through natural language dialogues; and 4) enabling
interactive feedback and solicitation of revised outcomes. By seamlessly
intertwining large language model and traffic expertise, TrafficGPT not only
advances traffic management but also offers a novel approach to leveraging AI
capabilities in this domain. The TrafficGPT demo can be found in
https://github.com/lijlansg/TrafficGPT.git.",None,-1
d70442cf-a4aa-4c3e-9698-8d7d23dc87f7,A Simple and Effective Pruning Approach for Large Language Models,0.981832,125,"As their size increases, Large Languages Models (LLMs) are natural candidates
for network pruning methods: approaches that drop a subset of network weights
while striving to preserve performance. Existing methods, however, require
either retraining, which is rarely affordable for billion-scale LLMs, or
solving a weight reconstruction problem reliant on second-order information,
which may also be computationally expensive. In this paper, we introduce a
novel, straightforward yet effective pruning method, termed Wanda (Pruning by
Weights and activations), designed to induce sparsity in pretrained LLMs.
Motivated by the recent observation of emergent large magnitude features in
LLMs, our approach prunes weights with the smallest magnitudes multiplied by
the corresponding input activations, on a per-output basis. Notably, Wanda
requires no retraining or weight update, and the pruned LLM can be used as is.
We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2
across various language benchmarks. Wanda significantly outperforms the
established baseline of magnitude pruning and performs competitively against
recent method involving intensive weight update. Code is available at
https://github.com/locuslab/wanda.",None,-1
8ae098b1-0afd-4f67-8d80-65a5fb983fc3,Fast Dust Sand Image Enhancement Based on Color Correction and New Membership Function,0.0431599,1,"Images captured in dusty environments suffering from poor visibility and
quality. Enhancement of these images such as sand dust images plays a critical
role in various atmospheric optics applications. In this work, proposed a new
model based on Color Correction and new membership function to enhance san dust
images. The proposed model consists of three phases: correction of color shift,
removal of haze, and enhancement of contrast and brightness. The color shift is
corrected using a new membership function to adjust the values of U and V in
the YUV color space. The Adaptive Dark Channel Prior (A-DCP) is used for haze
removal. The stretching contrast and improving image brightness are based on
Contrast Limited Adaptive Histogram Equalization (CLAHE). The proposed model
tests and evaluates through many real sand dust images. The experimental
results show that the proposed solution is outperformed the current studies in
terms of effectively removing the red and yellow cast and provides high quality
and quantity dust images.",None,-1
1d18d8a1-f40a-472f-86f2-d3b1cf79d48e,Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs,0.251534,2,"Recent work in Natural Language Processing and Computer Vision has been using
textual information -- e.g., entity names and descriptions -- available in
knowledge graphs to ground neural models to high-quality structured data.
However, when it comes to non-English languages, the quantity and quality of
textual information are comparatively scarce. To address this issue, we
introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and
perform a thorough investigation on bridging the gap in both the quantity and
quality of textual information between English and non-English languages. More
specifically, we: i) bring to light the problem of increasing multilingual
coverage and precision of entity names and descriptions in Wikidata; ii)
demonstrate that state-of-the-art methods, namely, Machine Translation (MT),
Web Search (WS), and Large Language Models (LLMs), struggle with this task;
iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and
LLMs to generate high-quality textual information; and, iv) study the impact of
increasing multilingual coverage and precision of non-English textual
information in Entity Linking, Knowledge Graph Completion, and Question
Answering. As part of our effort towards better multilingual knowledge graphs,
we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE
approaches in 10 languages across 7 language families.",None,-1
24cdbee8-464b-482a-b506-86c431056849,Semi-supervised learning made simple with self-supervised clustering,0.812127,11,"Self-supervised learning models have been shown to learn rich visual
representations without requiring human annotations. However, in many
real-world scenarios, labels are partially available, motivating a recent line
of work on semi-supervised methods inspired by self-supervised principles. In
this paper, we propose a conceptually simple yet empirically powerful approach
to turn clustering-based self-supervised methods such as SwAV or DINO into
semi-supervised learners. More precisely, we introduce a multi-task framework
merging a supervised objective using ground-truth labels and a self-supervised
objective relying on clustering assignments with a single cross-entropy loss.
This approach may be interpreted as imposing the cluster centroids to be class
prototypes. Despite its simplicity, we provide empirical evidence that our
approach is highly effective and achieves state-of-the-art performance on
CIFAR100 and ImageNet.",None,-1
a5a9ed73-51a4-4925-b862-80dc341e2747,Knowledge Transfer via Multi-Head Feature Adaptation for Whole Slide Image Classification,0.181138,1,"Transferring prior knowledge from a source domain to the same or similar
target domain can greatly enhance the performance of models on the target
domain. However, it is challenging to directly leverage the knowledge from the
source domain due to task discrepancy and domain shift. To bridge the gaps
between different tasks and domains, we propose a Multi-Head Feature Adaptation
module, which projects features in the source feature space to a new space that
is more similar to the target space. Knowledge transfer is particularly
important in Whole Slide Image (WSI) classification since the number of WSIs in
one dataset might be too small to achieve satisfactory performance. Therefore,
WSI classification is an ideal testbed for our method, and we adapt multiple
knowledge transfer methods for WSI classification. The experimental results
show that models with knowledge transfer outperform models that are trained
from scratch by a large margin regardless of the number of WSIs in the
datasets, and our method achieves state-of-the-art performances among other
knowledge transfer methods on multiple datasets, including TCGA-RCC,
TCGA-NSCLC, and Camelyon16 datasets.",None,-1
b54c864d-b7a1-4d8f-a7e8-e6fcc4e4db26,AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models,0.627369,5,"Recent large language models (LLMs) are promising for making decisions in
grounded environments. However, LLMs frequently fail in complex decision-making
tasks due to the misalignment between the pre-trained knowledge in LLMs and the
actual rules in the environment. Existing methods require either costly
gradient computation or lengthy in-context demonstrations. In this paper, we
propose AutoPlan, an approach to guide LLM-based agents to accomplish
interactive decision-making tasks. AutoPlan augments the LLM prompt with a
task-solving plan and optimizes it through iterative experience collection and
reflection. Our experiments show that AutoPlan, though using no in-context
demonstrations, achieves success rates on par with the baselines using
human-written demonstrations on ALFWorld and even outperforms them by 8% on
HotpotQA. The code is available at https://github.com/owaski/AutoPlan.",None,-1
e1cd1960-b5cc-407d-ad8a-d44cb8d8feb3,TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications,0.67874,3,"We introduce TabRepo, a new dataset of tabular model evaluations and
predictions. TabRepo contains the predictions and metrics of 1310 models
evaluated on 200 classification and regression datasets. We illustrate the
benefit of our dataset in multiple ways. First, we show that it allows to
perform analysis such as comparing Hyperparameter Optimization against current
AutoML systems while also considering ensembling at marginal cost by using
precomputed model predictions. Second, we show that our dataset can be readily
leveraged to perform transfer-learning. In particular, we show that applying
standard transfer-learning techniques allows to outperform current
state-of-the-art tabular systems in accuracy, runtime and latency.",None,-1
c89f16d1-be26-42c1-aae6-25fbe0a0a404,Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs,0.864471,14,"The performance of large language models (LLMs) has recently improved to the
point where the models can perform well on many language tasks. We show here
that for the first time, the models can also generate coherent and valid formal
analyses of linguistic data and illustrate the vast potential of large language
models for analyses of their metalinguistic abilities. LLMs are primarily
trained on language data in the form of text; analyzing and evaluating their
metalinguistic abilities improves our understanding of their general
capabilities and sheds new light on theoretical models in linguistics. In this
paper, we probe into GPT-4's metalinguistic capabilities by focusing on three
subfields of formal linguistics: syntax, phonology, and semantics. We outline a
research program for metalinguistic analyses of large language models, propose
experimental designs, provide general guidelines, discuss limitations, and
offer future directions for this line of research. This line of inquiry also
exemplifies behavioral interpretability of deep learning, where models'
representations are accessed by explicit prompting rather than internal
representations.",None,-1
eab788b9-a438-4af8-b51f-add9d3b97b17,CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields,0.480495,8,"Neural Radiance Fields (NeRF) have the potential to be a major representation
of media. Since training a NeRF has never been an easy task, the protection of
its model copyright should be a priority. In this paper, by analyzing the pros
and cons of possible copyright protection solutions, we propose to protect the
copyright of NeRF models by replacing the original color representation in NeRF
with a watermarked color representation. Then, a distortion-resistant rendering
scheme is designed to guarantee robust message extraction in 2D renderings of
NeRF. Our proposed method can directly protect the copyright of NeRF models
while maintaining high rendering quality and bit accuracy when compared among
optional solutions.",None,-1
dea81070-bfeb-4ce7-87f1-79d794a51c15,Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health,0.949664,18,"Pretrained language models have been used in various natural language
processing applications. In the mental health domain, domain-specific language
models are pretrained and released, which facilitates the early detection of
mental health conditions. Social posts, e.g., on Reddit, are usually long
documents. However, there are no domain-specific pretrained models for
long-sequence modeling in the mental health domain. This paper conducts
domain-specific continued pretraining to capture the long context for mental
health. Specifically, we train and release MentalXLNet and MentalLongformer
based on XLNet and Longformer. We evaluate the mental health classification
performance and the long-range ability of these two domain-specific pretrained
models. Our models are released in HuggingFace.",None,-1
d3e17c6f-d1ab-42f7-b203-b646bd4f8bba,CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models,0.764718,10,"In this paper, we consider the challenge of summarizing patients' medical
progress notes in a limited data setting. For the Problem List Summarization
(shared task 1A) at the BioNLP Workshop 2023, we demonstrate that Clinical-T5
fine-tuned to 765 medical clinic notes outperforms other extractive,
abstractive and zero-shot baselines, yielding reasonable baseline systems for
medical note summarization. Further, we introduce Hierarchical Ensemble of
Summarization Models (HESM), consisting of token-level ensembles of diverse
fine-tuned Clinical-T5 models, followed by Minimum Bayes Risk (MBR) decoding.
Our HESM approach lead to a considerable summarization performance boost, and
when evaluated on held-out challenge data achieved a ROUGE-L of 32.77, which
was the best-performing system at the top of the shared task leaderboard.",None,-1
4b8fd880-4f04-4017-8b7d-cbf9c38b660c,MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic,0.975648,17,"Theory of Mind (ToM) is a critical component of intelligence but its
assessment remains the subject of heated debates. Prior research applied human
ToM assessments to natural language processing models using either
human-created standardized tests or rule-based templates. However, these
methods primarily focus on simplistic reasoning and require further validation.
Here, we leverage dynamic epistemic logic to isolate a particular component of
ToM and to generate controlled problems. We also introduce new verbalization
techniques to express these problems in English natural language. Our findings
indicate that some language model scaling (from 70M to 6B and 350M to 174B)
does not consistently yield results better than random chance. While GPT-4
demonstrates superior epistemic reasoning capabilities, there is still room for
improvement. Our code and datasets are publicly available
(https://huggingface.co/datasets/sileod/mindgames ,
https://github.com/sileod/llm-theory-of-mind )",None,-1
4ac9b071-03bb-4974-a85c-c9ee75b7da35,Chain-Of-Thought Prompting Under Streaming Batch: A Case Study,0.0153679,2,"Recently, Large Language Models (LLMs) have demonstrated remarkable
capabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting
LLMs in performing complex reasoning. However, developing effective prompts can
be a challenging and labor-intensive task. Many studies come out of some way to
automatically construct CoT from test data. Most of them assume that all test
data is visible before testing and only select a small subset to generate
rationales, which is an unrealistic assumption. In this paper, we present a
case study on how to construct and optimize chain-of-thought prompting using
batch data in streaming settings.",None,-1
4a537d76-3f2e-4b02-8f3e-f356ea6c18ee,Understanding the Distillation Process from Deep Generative Models to Tractable Probabilistic Circuits,0.806473,9,"Probabilistic Circuits (PCs) are a general and unified computational
framework for tractable probabilistic models that support efficient computation
of various inference tasks (e.g., computing marginal probabilities). Towards
enabling such reasoning capabilities in complex real-world tasks, Liu et al.
(2022) propose to distill knowledge (through latent variable assignments) from
less tractable but more expressive deep generative models. However, it is still
unclear what factors make this distillation work well. In this paper, we
theoretically and empirically discover that the performance of a PC can exceed
that of its teacher model. Therefore, instead of performing distillation from
the most expressive deep generative model, we study what properties the teacher
model and the PC should have in order to achieve good distillation performance.
This leads to a generic algorithmic improvement as well as other
data-type-specific ones over the existing latent variable distillation
pipeline. Empirically, we outperform SoTA TPMs by a large margin on challenging
image modeling benchmarks. In particular, on ImageNet32, PCs achieve 4.06
bits-per-dimension, which is only 0.34 behind variational diffusion models
(Kingma et al., 2021).",None,-1
d5104f78-3eab-4f66-8c20-1d34c3221a66,Rule-based Out-Of-Distribution Detection,0.273337,2,"Out-of-distribution detection is one of the most critical issue in the
deployment of machine learning. The data analyst must assure that data in
operation should be compliant with the training phase as well as understand if
the environment has changed in a way that autonomous decisions would not be
safe anymore. The method of the paper is based on eXplainable Artificial
Intelligence (XAI); it takes into account different metrics to identify any
resemblance between in-distribution and out of, as seen by the XAI model. The
approach is non-parametric and distributional assumption free. The validation
over complex scenarios (predictive maintenance, vehicle platooning, covert
channels in cybersecurity) corroborates both precision in detection and
evaluation of training-operation conditions proximity. Results are available
via open source and open data at the following link:
https://github.com/giacomo97cnr/Rule-based-ODD.",None,-1
13f4d823-91d2-4ae9-a0f1-6c33d7decc66,Document-Level Language Models for Machine Translation,0.610523,6,"Despite the known limitations, most machine translation systems today still
operate on the sentence-level. One reason for this is, that most parallel
training data is only sentence-level aligned, without document-level meta
information available. In this work, we set out to build context-aware
translation systems utilizing document-level monolingual data instead. This can
be achieved by combining any existing sentence-level translation model with a
document-level language model. We improve existing approaches by leveraging
recent advancements in model combination. Additionally, we propose novel
weighting techniques that make the system combination more flexible and
significantly reduce computational overhead. In a comprehensive evaluation on
four diverse translation tasks, we show that our extensions improve
document-targeted scores substantially and are also computationally more
efficient. However, we also find that in most scenarios, back-translation gives
even better results, at the cost of having to re-train the translation system.
Finally, we explore language model fusion in the light of recent advancements
in large language models. Our findings suggest that there might be strong
potential in utilizing large language models via model combination.",None,-1
36f29abf-d816-4a27-a80e-d205d8eb7781,Real-time SLAM Pipeline in Dynamics Environment,0.199937,1,"Inspired by the recent success of application of dense data approach by using
ORB-SLAM and RGB-D SLAM, we propose a better pipeline of real-time SLAM in
dynamics environment. Different from previous SLAM which can only handle static
scenes, we are presenting a solution which use RGB-D SLAM as well as YOLO
real-time object detection to segment and remove dynamic scene and then
construct static scene 3D. We gathered a dataset which allows us to jointly
consider semantics, geometry, and physics and thus enables us to reconstruct
the static scene while filtering out all dynamic objects.",None,-1
a5536ade-70d2-42e4-9554-5d2afe7e217b,Characterizing Financial Market Coverage using Artificial Intelligence,0.377661,2,"This paper scrutinizes a database of over 4900 YouTube videos to characterize
financial market coverage. Financial market coverage generates a large number
of videos. Therefore, watching these videos to derive actionable insights could
be challenging and complex. In this paper, we leverage Whisper, a
speech-to-text model from OpenAI, to generate a text corpus of market coverage
videos from Bloomberg and Yahoo Finance. We employ natural language processing
to extract insights regarding language use from the market coverage. Moreover,
we examine the prominent presence of trending topics and their evolution over
time, and the impacts that some individuals and organizations have on the
financial market. Our characterization highlights the dynamics of the financial
market coverage and provides valuable insights reflecting broad discussions
regarding recent financial events and the world economy.",None,-1
890d503c-d843-4ab2-b9ca-86e85010d9f0,TUTORING: Instruction-Grounded Conversational Agent for Language Learners,0.0620104,1,"In this paper, we propose Tutoring bot, a generative chatbot trained on a
large scale of tutor-student conversations for English-language learning. To
mimic a human tutor's behavior in language education, the tutor bot leverages
diverse educational instructions and grounds to each instruction as additional
input context for the tutor response generation. As a single instruction
generally involves multiple dialogue turns to give the student sufficient
speaking practice, the tutor bot is required to monitor and capture when the
current instruction should be kept or switched to the next instruction. For
that, the tutor bot is learned to not only generate responses but also infer
its teaching action and progress on the current conversation simultaneously by
a multi-task learning scheme. Our Tutoring bot is deployed under a
non-commercial use license at https://tutoringai.com.",None,-1
c26cb1fe-4f95-4932-a431-093cb851d7ed,Marching-Primitives: Shape Abstraction from Signed Distance Function,0.103066,2,"Representing complex objects with basic geometric primitives has long been a
topic in computer vision. Primitive-based representations have the merits of
compactness and computational efficiency in higher-level tasks such as physics
simulation, collision checking, and robotic manipulation. Unlike previous works
which extract polygonal meshes from a signed distance function (SDF), in this
paper, we present a novel method, named Marching-Primitives, to obtain a
primitive-based abstraction directly from an SDF. Our method grows geometric
primitives (such as superquadrics) iteratively by analyzing the connectivity of
voxels while marching at different levels of signed distance. For each valid
connected volume of interest, we march on the scope of voxels from which a
primitive is able to be extracted in a probabilistic sense and simultaneously
solve for the parameters of the primitive to capture the underlying local
geometry. We evaluate the performance of our method on both synthetic and
real-world datasets. The results show that the proposed method outperforms the
state-of-the-art in terms of accuracy, and is directly generalizable among
different categories and scales. The code is open-sourced at
https://github.com/ChirikjianLab/Marching-Primitives.git.",None,-1
1abbd963-e043-4012-9bf4-b7ab317d3c0c,HistRED: A Historical Document-Level Relation Extraction Dataset,0.740519,4,"Despite the extensive applications of relation extraction (RE) tasks in
various domains, little has been explored in the historical context, which
contains promising data across hundreds and thousands of years. To promote the
historical RE research, we present HistRED constructed from Yeonhaengnok.
Yeonhaengnok is a collection of records originally written in Hanja, the
classical Chinese writing, which has later been translated into Korean. HistRED
provides bilingual annotations such that RE can be performed on Korean and
Hanja texts. In addition, HistRED supports various self-contained subtexts with
different lengths, from a sentence level to a document level, supporting
diverse context settings for researchers to evaluate the robustness of their RE
models. To demonstrate the usefulness of our dataset, we propose a bilingual RE
model that leverages both Korean and Hanja contexts to predict relations
between entities. Our model outperforms monolingual baselines on HistRED,
showing that employing multiple language contexts supplements the RE
predictions. The dataset is publicly available at:
https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.",None,-1
568d9a74-1415-42c0-ad92-491b2d41e935,Enhancing Continual Relation Extraction via Classifier Decomposition,0.815964,6,"Continual relation extraction (CRE) models aim at handling emerging new
relations while avoiding catastrophically forgetting old ones in the streaming
data. Though improvements have been shown by previous CRE studies, most of them
only adopt a vanilla strategy when models first learn representations of new
relations. In this work, we point out that there exist two typical biases after
training of this vanilla strategy: classifier bias and representation bias,
which causes the previous knowledge that the model learned to be shaded. To
alleviate those biases, we propose a simple yet effective classifier
decomposition framework that splits the last FFN layer into separated previous
and current classifiers, so as to maintain previous knowledge and encourage the
model to learn more robust representations at this training stage. Experimental
results on two standard benchmarks show that our proposed framework
consistently outperforms the state-of-the-art CRE models, which indicates that
the importance of the first training stage to CRE models may be underestimated.
Our code is available at https://github.com/hemingkx/CDec.",None,-1
8b319810-b250-4e9f-a9d5-81909f55f52d,Parachute: Evaluating Interactive Human-LM Co-writing Systems,0.913348,13,"A surge of advances in language models (LMs) has led to significant interest
in using LMs to build co-writing systems, in which humans and LMs interactively
contribute to a shared writing artifact. However, there is a lack of studies
assessing co-writing systems in interactive settings. We propose a
human-centered evaluation framework, Parachute, for interactive co-writing
systems. Parachute showcases an integrative view of interaction evaluation,
where each evaluation aspect consists of categorized practical metrics.
Furthermore, we present Parachute with a use case to demonstrate how to
evaluate and compare co-writing systems using Parachute.",None,-1
9c3c476a-aa6c-4066-a6ae-bb0f0c424b97,On Private and Robust Bandits,0.326396,1,"We study private and robust multi-armed bandits (MABs), where the agent
receives Huber's contaminated heavy-tailed rewards and meanwhile needs to
ensure differential privacy. We first present its minimax lower bound,
characterizing the information-theoretic limit of regret with respect to
privacy budget, contamination level and heavy-tailedness. Then, we propose a
meta-algorithm that builds on a private and robust mean estimation sub-routine
\texttt{PRM} that essentially relies on reward truncation and the Laplace
mechanism only. For two different heavy-tailed settings, we give specific
schemes of \texttt{PRM}, which enable us to achieve nearly-optimal regret. As
by-products of our main results, we also give the first minimax lower bound for
private heavy-tailed MABs (i.e., without contamination). Moreover, our two
proposed truncation-based \texttt{PRM} achieve the optimal trade-off between
estimation accuracy, privacy and robustness. Finally, we support our
theoretical results with experimental studies.",None,-1
0065d4f8-1b25-461e-8ea1-174671f2dbc0,Contestable Camera Cars: A Speculative Design Exploration of Public AI That Is Open and Responsive to Dispute,0.458317,8,"Local governments increasingly use artificial intelligence (AI) for automated
decision-making. Contestability, making systems responsive to dispute, is a way
to ensure they respect human rights to autonomy and dignity. We investigate the
design of public urban AI systems for contestability through the example of
camera cars: human-driven vehicles equipped with image sensors. Applying a
provisional framework for contestable AI, we use speculative design to create a
concept video of a contestable camera car. Using this concept video, we then
conduct semi-structured interviews with 17 civil servants who work with AI
employed by a large northwestern European city. The resulting data is analyzed
using reflexive thematic analysis to identify the main challenges facing the
implementation of contestability in public AI. We describe how civic
participation faces issues of representation, public AI systems should
integrate with existing democratic practices, and cities must expand capacities
for responsible AI development and operation.",None,-1
b9fa2297-a439-41f1-8f12-a2f356c099bf,Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization,0.442505,3,"This work investigates the effectiveness of different pseudonymization
techniques, ranging from rule-based substitutions to using pre-trained Large
Language Models (LLMs), on a variety of datasets and models used for two widely
used NLP tasks: text classification and summarization. Our work provides
crucial insights into the gaps between original and anonymized data (focusing
on the pseudonymization technique) and model quality and fosters future
research into higher-quality anonymization techniques to better balance the
trade-offs between data protection and utility preservation. We make our code,
pseudonymized datasets, and downstream models publicly available",None,-1
91732c0a-4000-45a4-bd73-7f20987f1d1a,COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers,0.202137,3,"We present COMEDIAN, a novel pipeline to initialize spatiotemporal
transformers for action spotting, which involves self-supervised learning and
knowledge distillation. Action spotting is a timestamp-level temporal action
detection task. Our pipeline consists of three steps, with two initialization
stages. First, we perform self-supervised initialization of a spatial
transformer using short videos as input. Additionally, we initialize a temporal
transformer that enhances the spatial transformer's outputs with global context
through knowledge distillation from a pre-computed feature bank aligned with
each short video segment. In the final step, we fine-tune the transformers to
the action spotting task. The experiments, conducted on the SoccerNet-v2
dataset, demonstrate state-of-the-art performance and validate the
effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several
advantages of our pretraining pipeline, including improved performance and
faster convergence compared to non-pretrained models.",None,-1
8460f0ae-5491-445c-9515-1726fb4e3be7,FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models,0.492267,11,"Large language models (LLMs) have demonstrated exceptional performance in
various natural language processing tasks, yet their efficacy in more
challenging and domain-specific tasks remains largely unexplored. This paper
presents FinEval, a benchmark specifically designed for the financial domain
knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice
questions covering Finance, Economy, Accounting, and Certificate. It includes
4,661 questions spanning 34 different academic subjects. To ensure a
comprehensive model performance evaluation, FinEval employs a range of prompt
types, including zero-shot and few-shot prompts, as well as answer-only and
chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs
on FinEval, the results show that only GPT-4 achieved an accuracy close to 70%
in different prompt settings, indicating significant growth potential for LLMs
in the financial domain knowledge. Our work offers a more comprehensive
financial knowledge evaluation benchmark, utilizing data of mock exams and
covering a wide range of evaluated LLMs.",None,-1
b66eeb41-77d4-45f4-bad6-3358251d8829,Foundational theories of hesitant fuzzy sets and hesitant fuzzy information systems and their applications for multi-strength intelligent classifiers,0.324298,1,"Hesitant fuzzy sets are widely used in certain instances of uncertainty and
hesitation. In sets, the inclusion relationship is an important and
foundational definition. Thus, as a kind of set, hesitant fuzzy sets require an
explicit definition of inclusion relationship. Based on the hesitant fuzzy
membership degree of discrete form, several kinds of inclusion relationships
for hesitant fuzzy sets are proposed in this work. Then, some foundational
propositions of hesitant fuzzy sets are presented, along with propositions of
families of hesitant fuzzy sets. Some foundational propositions of hesitant
fuzzy information systems are proposed with respect to parameter reductions and
an example and an algorithm are given to illustrate the processes of parameter
reduction. Finally, a multi-strength intelligent classifier is proposed to make
health state diagnoses for complex systems.",None,-1
6e92c0c8-5029-487d-bbc6-3a3b00803cae,Manga109Dialog: A Large-scale Dialogue Dataset for Comics Speaker Detection,0.725526,2,"The expanding market for e-comics has spurred interest in the development of
automated methods to analyze comics. For further understanding of comics, an
automated approach is needed to link text in comics to characters speaking the
words. Comics speaker detection research has practical applications, such as
automatic character assignment for audiobooks, automatic translation according
to characters' personalities, and inference of character relationships and
stories.
  To deal with the problem of insufficient speaker-to-text annotations, we
created a new annotation dataset Manga109Dialog based on Manga109.
Manga109Dialog is the world's largest comics speaker annotation dataset,
containing 132,692 speaker-to-text pairs. We further divided our dataset into
different levels by prediction difficulties to evaluate speaker detection
methods more appropriately. Unlike existing methods mainly based on distances,
we propose a deep learning-based method using scene graph generation models.
Due to the unique features of comics, we enhance the performance of our
proposed model by considering the frame reading order. We conducted experiments
using Manga109Dialog and other datasets. Experimental results demonstrate that
our scene-graph-based approach outperforms existing methods, achieving a
prediction accuracy of over 75%.",None,-1
62fe10ff-5833-431f-bae3-0d11f4612b82,Stackelberg Games for Learning Emergent Behaviors During Competitive Autocurricula,0.291658,2,"Autocurricular training is an important sub-area of multi-agent reinforcement
learning~(MARL) that allows multiple agents to learn emergent skills in an
unsupervised co-evolving scheme. The robotics community has experimented
autocurricular training with physically grounded problems, such as robust
control and interactive manipulation tasks. However, the asymmetric nature of
these tasks makes the generation of sophisticated policies challenging. Indeed,
the asymmetry in the environment may implicitly or explicitly provide an
advantage to a subset of agents which could, in turn, lead to a low-quality
equilibrium. This paper proposes a novel game-theoretic algorithm, Stackelberg
Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a
two-player MARL problem as a Stackelberg game with one player as the `leader'
and the other as the `follower' in a hierarchical interaction structure wherein
the leader has an advantage. We first demonstrate that the leader's advantage
from ST-MADDPG can be used to alleviate the inherent asymmetry in the
environment. By exploiting the leader's advantage, ST-MADDPG improves the
quality of a co-evolution process and results in more sophisticated and complex
strategies that work well even against an unseen strong opponent.",None,-1
628284ee-2c7c-4d24-9b28-db800f2aa648,Learning Logic Specifications for Soft Policy Guidance in POMCP,0.851188,6,"Partially Observable Monte Carlo Planning (POMCP) is an efficient solver for
Partially Observable Markov Decision Processes (POMDPs). It allows scaling to
large state spaces by computing an approximation of the optimal policy locally
and online, using a Monte Carlo Tree Search based strategy. However, POMCP
suffers from sparse reward function, namely, rewards achieved only when the
final goal is reached, particularly in environments with large state spaces and
long horizons. Recently, logic specifications have been integrated into POMCP
to guide exploration and to satisfy safety requirements. However, such
policy-related rules require manual definition by domain experts, especially in
real-world scenarios. In this paper, we use inductive logic programming to
learn logic specifications from traces of POMCP executions, i.e., sets of
belief-action pairs generated by the planner. Specifically, we learn rules
expressed in the paradigm of answer set programming. We then integrate them
inside POMCP to provide soft policy bias toward promising actions. In the
context of two benchmark scenarios, rocksample and battery, we show that the
integration of learned rules from small task instances can improve performance
with fewer Monte Carlo simulations and in larger task instances. We make our
modified version of POMCP publicly available at
https://github.com/GiuMaz/pomcp_clingo.git.",None,-1
41608637-07e6-42df-9483-8b32f9eb8957,Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration,0.230302,6,"The current approach for testing the robustness of object detectors suffers
from serious deficiencies such as improper methods of performing
out-of-distribution detection and using calibration metrics which do not
consider both localisation and classification quality. In this work, we address
these issues, and introduce the Self-Aware Object Detection (SAOD) task, a
unified testing framework which respects and adheres to the challenges that
object detectors face in safety-critical environments such as autonomous
driving. Specifically, the SAOD task requires an object detector to be: robust
to domain shift; obtain reliable uncertainty estimates for the entire scene;
and provide calibrated confidence scores for the detections. We extensively use
our framework, which introduces novel metrics and large scale test datasets, to
test numerous object detectors in two different use-cases, allowing us to
highlight critical insights into their robustness performance. Finally, we
introduce a simple baseline for the SAOD task, enabling researchers to
benchmark future proposed methods and move towards robust object detectors
which are fit for purpose. Code is available at https://github.com/fiveai/saod",None,-1
2e281e2d-c737-422d-b48b-a5dc4d1d78ad,ContrastNet: A Contrastive Learning Framework for Few-Shot Text Classification,0.781883,46,"Few-shot text classification has recently been promoted by the meta-learning
paradigm which aims to identify target classes with knowledge transferred from
source classes with sets of small tasks named episodes. Despite their success,
existing works building their meta-learner based on Prototypical Networks are
unsatisfactory in learning discriminative text representations between similar
classes, which may lead to contradictions during label prediction. In addition,
the tasklevel and instance-level overfitting problems in few-shot text
classification caused by a few training examples are not sufficiently tackled.
In this work, we propose a contrastive learning framework named ContrastNet to
tackle both discriminative representation and overfitting problems in few-shot
text classification. ContrastNet learns to pull closer text representations
belonging to the same class and push away text representations belonging to
different classes, while simultaneously introducing unsupervised contrastive
regularization at both task-level and instance-level to prevent overfitting.
Experiments on 8 few-shot text classification datasets show that ContrastNet
outperforms the current state-of-the-art models.",None,-1
05394c16-7376-4211-a6b2-01130f58b5ae,How much can ChatGPT really help Computational Biologists in Programming?,0.308766,2,"ChatGPT, a recently developed product by openAI, is successfully leaving its
mark as a multi-purpose natural language based chatbot. In this paper, we are
more interested in analyzing its potential in the field of computational
biology. A major share of work done by computational biologists these days
involve coding up bioinformatics algorithms, analyzing data, creating
pipelining scripts and even machine learning modeling and feature extraction.
This paper focuses on the potential influence (both positive and negative) of
ChatGPT in the mentioned aspects with illustrative examples from different
perspectives. Compared to other fields of computer science, computational
biology has - (1) less coding resources, (2) more sensitivity and bias issues
(deals with medical data) and (3) more necessity of coding assistance (people
from diverse background come to this field). Keeping such issues in mind, we
cover use cases such as code writing, reviewing, debugging, converting,
refactoring and pipelining using ChatGPT from the perspective of computational
biologists in this paper.",None,-1
6396e70b-7484-409a-9729-fc86b34a4a63,Unsupervised Domain Adaption with Pixel-level Discriminator for Image-aware Layout Generation,0.259061,4,"Layout is essential for graphic design and poster generation. Recently,
applying deep learning models to generate layouts has attracted increasing
attention. This paper focuses on using the GAN-based model conditioned on image
contents to generate advertising poster graphic layouts, which requires an
advertising poster layout dataset with paired product images and graphic
layouts. However, the paired images and layouts in the existing dataset are
collected by inpainting and annotating posters, respectively. There exists a
domain gap between inpainted posters (source domain data) and clean product
images (target domain data). Therefore, this paper combines unsupervised domain
adaption techniques to design a GAN with a novel pixel-level discriminator
(PD), called PDA-GAN, to generate graphic layouts according to image contents.
The PD is connected to the shallow level feature map and computes the GAN loss
for each input-image pixel. Both quantitative and qualitative evaluations
demonstrate that PDA-GAN can achieve state-of-the-art performances and generate
high-quality image-aware graphic layouts for advertising posters.",None,-1
2e8b928d-6f49-4831-b22f-b964756d5439,Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,0.764274,26,"Medical Visual Question Answering (VQA) is an important challenge, as it
would lead to faster and more accurate diagnoses and treatment decisions. Most
existing methods approach it as a multi-class classification problem, which
restricts the outcome to a predefined closed-set of curated answers. We focus
on open-ended VQA and motivated by the recent advances in language models
consider it as a generative task. Leveraging pre-trained language models, we
introduce a novel method particularly suited for small, domain-specific,
medical datasets. To properly communicate the medical images to the language
model, we develop a network that maps the extracted visual features to a set of
learnable tokens. Then, alongside the question, these learnable tokens directly
prompt the language model. We explore recent parameter-efficient fine-tuning
strategies for language models, which allow for resource- and data-efficient
fine-tuning. We evaluate our approach on the prime medical VQA benchmarks,
namely, Slake, OVQA and PathVQA. The results demonstrate that our approach
outperforms existing methods across various training settings while also being
computationally efficient.",None,-1
52e36a62-1b9f-4c92-b766-2b92111ce5e3,How to Design Translation Prompts for ChatGPT: An Empirical Study,0.886096,25,"The recently released ChatGPT has demonstrated surprising abilities in
natural language understanding and natural language generation. Machine
translation relies heavily on the abilities of language understanding and
generation. Thus, in this paper, we explore how to assist machine translation
with ChatGPT. We adopt several translation prompts on a wide range of
translations. Our experimental results show that ChatGPT with designed
translation prompts can achieve comparable or better performance over
commercial translation systems for high-resource language translations. We
further evaluate the translation quality using multiple references, and ChatGPT
achieves superior performance compared to commercial systems. We also conduct
experiments on domain-specific translations, the final results show that
ChatGPT is able to comprehend the provided domain keyword and adjust
accordingly to output proper translations. At last, we perform few-shot prompts
that show consistent improvement across different base prompts. Our work
provides empirical evidence that ChatGPT still has great potential in
translations.",None,-1
53382d2d-e15a-4ce7-9c29-8f6a9d2bf5cb,Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching,0.513523,4,"Class prototype construction and matching are core aspects of few-shot action
recognition. Previous methods mainly focus on designing spatiotemporal relation
modeling modules or complex temporal alignment algorithms. Despite the
promising results, they ignored the value of class prototype construction and
matching, leading to unsatisfactory performance in recognizing similar
categories in every task. In this paper, we propose GgHM, a new framework with
Graph-guided Hybrid Matching. Concretely, we learn task-oriented features by
the guidance of a graph neural network during class prototype construction,
optimizing the intra- and inter-class feature correlation explicitly. Next, we
design a hybrid matching strategy, combining frame-level and tuple-level
matching to classify videos with multivariate styles. We additionally propose a
learnable dense temporal modeling module to enhance the video feature temporal
representation to build a more solid foundation for the matching process. GgHM
shows consistent improvements over other challenging baselines on several
few-shot datasets, demonstrating the effectiveness of our method. The code will
be publicly available at https://github.com/jiazheng-xing/GgHM.",None,-1
9804930a-5f5f-4821-90a8-97c856fd6b39,Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT,0.613742,4,"Foundation models exhibit significant capabilities in decision-making and
logical deductions. Nonetheless, a continuing discourse persists regarding
their genuine understanding of the world as opposed to mere stochastic mimicry.
This paper meticulously examines a simple transformer trained for Othello,
extending prior research to enhance comprehension of the emergent world model
of Othello-GPT. The investigation reveals that Othello-GPT encapsulates a
linear representation of opposing pieces, a factor that causally steers its
decision-making process. This paper further elucidates the interplay between
the linear world representation and causal decision-making, and their
dependence on layer depth and model complexity. We have made the code public.",None,-1
503268d1-b72c-41e8-8219-eab26d197383,Language Prompt for Autonomous Driving,0.975004,41,"A new trend in the computer vision community is to capture objects of
interest following flexible human command represented by a natural language
prompt. However, the progress of using language prompts in driving scenarios is
stuck in a bottleneck due to the scarcity of paired prompt-instance data. To
address this challenge, we propose the first object-centric language prompt set
for driving scenes within 3D, multi-view, and multi-frame space, named
NuPrompt. It expands Nuscenes dataset by constructing a total of 35,367
language descriptions, each referring to an average of 5.3 object tracks. Based
on the object-text pairs from the new benchmark, we formulate a new
prompt-based driving task, \ie, employing a language prompt to predict the
described object trajectory across views and frames. Furthermore, we provide a
simple end-to-end baseline model based on Transformer, named PromptTrack.
Experiments show that our PromptTrack achieves impressive performance on
NuPrompt. We hope this work can provide more new insights for the autonomous
driving community. Dataset and Code will be made public at
\href{https://github.com/wudongming97/Prompt4Driving}{https://github.com/wudongming97/Prompt4Driving}.",None,-1
8dddaed4-eb43-4201-b2d5-fc1bc48f86e9,MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models,0.386338,2,"Despite tremendous advancements in bird's-eye view (BEV) perception, existing
models fall short in generating realistic and coherent semantic map layouts,
and they fail to account for uncertainties arising from partial sensor
information (such as occlusion or limited coverage). In this work, we introduce
MapPrior, a novel BEV perception framework that combines a traditional
discriminative BEV perception model with a learned generative model for
semantic map layouts. Our MapPrior delivers predictions with better accuracy,
realism, and uncertainty awareness. We evaluate our model on the large-scale
nuScenes benchmark. At the time of submission, MapPrior outperforms the
strongest competing method, with significantly improved MMD and ECE scores in
camera- and LiDAR-based BEV perception.",None,-1
b8af34ef-3619-4e15-8c76-443f98068aa9,Prototype Knowledge Distillation for Medical Segmentation with Missing Modality,0.646528,12,"Multi-modality medical imaging is crucial in clinical treatment as it can
provide complementary information for medical image segmentation. However,
collecting multi-modal data in clinical is difficult due to the limitation of
the scan time and other clinical situations. As such, it is clinically
meaningful to develop an image segmentation paradigm to handle this missing
modality problem. In this paper, we propose a prototype knowledge distillation
(ProtoKD) method to tackle the challenging problem, especially for the toughest
scenario when only single modal data can be accessed. Specifically, our ProtoKD
can not only distillate the pixel-wise knowledge of multi-modality data to
single-modality data but also transfer intra-class and inter-class feature
variations, such that the student model could learn more robust feature
representation from the teacher model and inference with only one single
modality data. Our method achieves state-of-the-art performance on BraTS
benchmark. The code is available at
\url{https://github.com/SakurajimaMaiii/ProtoKD}.",None,-1
7dcb517c-122b-427c-877e-c15c9af6fd7c,Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection,0.200003,3,"Foundation models, such as OpenAI's GPT-3 and GPT-4, Meta's LLaMA, and
Google's PaLM2, have revolutionized the field of artificial intelligence. A
notable paradigm shift has been the advent of the Segment Anything Model (SAM),
which has exhibited a remarkable capability to segment real-world objects,
trained on 1 billion masks and 11 million images. Although SAM excels in
general object segmentation, it lacks the intrinsic ability to detect salient
objects, resulting in suboptimal performance in this domain. To address this
challenge, we present the Segment Salient Object Model (SSOM), an innovative
approach that adaptively fine-tunes SAM for salient object detection by
harnessing the low-rank structure inherent in deep learning. Comprehensive
qualitative and quantitative evaluations across five challenging RGB benchmark
datasets demonstrate the superior performance of our approach, surpassing
state-of-the-art methods.",None,-1
80039060-cf16-425a-b610-564eb977ea9d,Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge,0.629679,47,"Pre-trained language models (LMs) are used for knowledge intensive tasks like
question answering, but their knowledge gets continuously outdated as the world
changes. Prior work has studied targeted updates to LMs, injecting individual
facts and evaluating whether the model learns these facts while not changing
predictions on other contexts. We take a step forward and study LMs' abilities
to make inferences based on injected facts (or propagate those facts): for
example, after learning that something is a TV show, does an LM predict that
you can watch it? We study this with two cloze-style tasks: an existing dataset
of real-world sentences about novel entities (ECBD) as well as a new controlled
benchmark with manually designed templates requiring varying levels of
inference about injected knowledge. Surprisingly, we find that existing methods
for updating knowledge (gradient-based fine-tuning and modifications of this
approach) show little propagation of injected knowledge. These methods improve
performance on cloze instances only when there is lexical overlap between
injected facts and target inferences. Yet, prepending entity definitions in an
LM's context improves performance across all settings, suggesting that there is
substantial headroom for parameter-updating approaches for knowledge injection.",None,-1
6961b17a-b50f-4350-aaca-d612ec7280d3,Contrasting Linguistic Patterns in Human and LLM-Generated Text,0.465616,21,"We conduct a quantitative analysis contrasting human-written English news
text with comparable large language model (LLM) output from 4 LLMs from the
LLaMa family. Our analysis spans several measurable linguistic dimensions,
including morphological, syntactic, psychometric and sociolinguistic aspects.
The results reveal various measurable differences between human and
AI-generated texts. Among others, human texts exhibit more scattered sentence
length distributions, a distinct use of dependency and constituent types,
shorter constituents, and more aggressive emotions (fear, disgust) than
LLM-generated texts. LLM outputs use more numbers, symbols and auxiliaries
(suggesting objective language) than human texts, as well as more pronouns. The
sexist bias prevalent in human text is also expressed by LLMs.",None,-1
a2096b88-b62e-4ba3-91c3-8949065e6fde,"Mathematics, word problems, common sense, and artificial intelligence",0.878225,14,"The paper discusses the capacities and limitations of current artificial
intelligence (AI) technology to solve word problems that combine elementary
knowledge with commonsense reasoning. No existing AI systems can solve these
reliably. We review three approaches that have been developed, using AI natural
language technology: outputting the answer directly, outputting a computer
program that solves the problem, and outputting a formalized representation
that can be input to an automated theorem verifier. We review some benchmarks
that have been developed to evaluate these systems and some experimental
studies. We discuss the limitations of the existing technology at solving these
kinds of problems. We argue that it is not clear whether these kinds of
limitations will be important in developing AI technology for pure mathematical
research, but that they will be important in applications of mathematics, and
may well be important in developing programs capable of reading and
understanding mathematical content written by humans.",None,-1
892599d1-6209-4589-aa10-db51c23a6c05,Mutually Guided Few-shot Learning for Relational Triple Extraction,0.373273,1,"Knowledge graphs (KGs), containing many entity-relation-entity triples,
provide rich information for downstream applications. Although extracting
triples from unstructured texts has been widely explored, most of them require
a large number of labeled instances. The performance will drop dramatically
when only few labeled data are available. To tackle this problem, we propose
the Mutually Guided Few-shot learning framework for Relational Triple
Extraction (MG-FTE). Specifically, our method consists of an entity-guided
relation proto-decoder to classify the relations firstly and a relation-guided
entity proto-decoder to extract entities based on the classified relations. To
draw the connection between entity and relation, we design a proto-level fusion
module to boost the performance of both entity extraction and relation
classification. Moreover, a new cross-domain few-shot triple extraction task is
introduced. Extensive experiments show that our method outperforms many
state-of-the-art methods by 12.6 F1 score on FewRel 1.0 (single-domain) and
20.5 F1 score on FewRel 2.0 (cross-domain).",None,-1
3569c34c-daa3-4f96-b6b0-0660028eeaac,Adversarial Capsule Networks for Romanian Satire Detection and Sentiment Analysis,0.0515573,2,"Satire detection and sentiment analysis are intensively explored natural
language processing (NLP) tasks that study the identification of the satirical
tone from texts and extracting sentiments in relationship with their targets.
In languages with fewer research resources, an alternative is to produce
artificial examples based on character-level adversarial processes to overcome
dataset size limitations. Such samples are proven to act as a regularization
method, thus improving the robustness of models. In this work, we improve the
well-known NLP models (i.e., Convolutional Neural Networks, Long Short-Term
Memory (LSTM), Bidirectional LSTM, Gated Recurrent Units (GRUs), and
Bidirectional GRUs) with adversarial training and capsule networks. The
fine-tuned models are used for satire detection and sentiment analysis tasks in
the Romanian language. The proposed framework outperforms the existing methods
for the two tasks, achieving up to 99.08% accuracy, thus confirming the
improvements added by the capsule layers and the adversarial training in NLP
approaches.",None,-1
fbc00ae9-03f0-43ea-82fa-e73b171d8f50,MDDial: A Multi-turn Differential Diagnosis Dialogue Dataset with Reliability Evaluation,0.392034,2,"Dialogue systems for Automatic Differential Diagnosis (ADD) have a wide range
of real-life applications. These dialogue systems are promising for providing
easy access and reducing medical costs. Building end-to-end ADD dialogue
systems requires dialogue training datasets. However, to the best of our
knowledge, there is no publicly available ADD dialogue dataset in English
(although non-English datasets exist). Driven by this, we introduce MDDial, the
first differential diagnosis dialogue dataset in English which can aid to build
and evaluate end-to-end ADD dialogue systems. Additionally, earlier studies
present the accuracy of diagnosis and symptoms either individually or as a
combined weighted score. This method overlooks the connection between the
symptoms and the diagnosis. We introduce a unified score for the ADD system
that takes into account the interplay between symptoms and diagnosis. This
score also indicates the system's reliability. To the end, we train two
moderate-size of language models on MDDial. Our experiments suggest that while
these language models can perform well on many natural language understanding
tasks, including dialogue tasks in the general domain, they struggle to relate
relevant symptoms and disease and thus have poor performance on MDDial. MDDial
will be released publicly to aid the study of ADD dialogue research.",None,-1
60865049-61c6-4af9-9ec2-6ec934d09221,LLMSTEP: LLM proofstep suggestions in Lean,0.996014,9,"We present LLMSTEP, a tool for integrating a language model into the Lean
proof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to
a server hosting a language model. The language model generates suggestions,
which are checked in Lean and displayed to a user in their development
environment. We provide a baseline language model, along with code for
fine-tuning and evaluation to support further development. We provide server
implementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a
step towards fast, effective language model suggestions for any user.",None,-1
f86627ce-a46c-4721-b666-2e95ba23227e,Distributionally Robust Optimization and Invariant Representation Learning for Addressing Subgroup Underrepresentation: Mechanisms and Limitations,0.137489,1,"Spurious correlation caused by subgroup underrepresentation has received
increasing attention as a source of bias that can be perpetuated by deep neural
networks (DNNs). Distributionally robust optimization has shown success in
addressing this bias, although the underlying working mechanism mostly relies
on upweighting under-performing samples as surrogates for those
underrepresented in data. At the same time, while invariant representation
learning has been a powerful choice for removing nuisance-sensitive features,
it has been little considered in settings where spurious correlations are
caused by significant underrepresentation of subgroups. In this paper, we take
the first step to better understand and improve the mechanisms for debiasing
spurious correlation due to subgroup underrepresentation in medical image
classification. Through a comprehensive evaluation study, we first show that 1)
generalized reweighting of under-performing samples can be problematic when
bias is not the only cause for poor performance, while 2) naive invariant
representation learning suffers from spurious correlations itself. We then
present a novel approach that leverages robust optimization to facilitate the
learning of invariant representations at the presence of spurious correlations.
Finetuned classifiers utilizing such representation demonstrated improved
abilities to reduce subgroup performance disparity, while maintaining high
average and worst-group performance.",None,-1
353b7ed9-5bfb-4615-81a5-2fba737b93e3,TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition,0.770044,9,"Emotion Support Conversation (ESC) is an emerging and challenging task with
the goal of reducing the emotional distress of people. Previous attempts fail
to maintain smooth transitions between utterances in ESC because they ignore to
grasp the fine-grained transition information at each dialogue turn. To solve
this problem, we propose to take into account turn-level state
\textbf{Trans}itions of \textbf{ESC} (\textbf{TransESC}) from three
perspectives, including semantics transition, strategy transition and emotion
transition, to drive the conversation in a smooth and natural way.
Specifically, we construct the state transition graph with a two-step way,
named transit-then-interact, to grasp such three types of turn-level transition
information. Finally, they are injected into the transition-aware decoder to
generate more engaging responses. Both automatic and human evaluations on the
benchmark dataset demonstrate the superiority of TransESC to generate more
smooth and effective supportive responses. Our source code is available at
\url{https://github.com/circle-hit/TransESC}.",None,-1
c65b6ec9-7cf1-4fed-94f0-152582fb2b8d,3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining,0.871314,7,"Masked autoencoders (MAE) have recently been introduced to 3D self-supervised
pretraining for point clouds due to their great success in NLP and computer
vision. Unlike MAEs used in the image domain, where the pretext task is to
restore features at the masked pixels, such as colors, the existing 3D MAE
works reconstruct the missing geometry only, i.e, the location of the masked
points. In contrast to previous studies, we advocate that point location
recovery is inessential and restoring intrinsic point features is much
superior. To this end, we propose to ignore point position reconstruction and
recover high-order features at masked points including surface normals and
surface variations, through a novel attention-based decoder which is
independent of the encoder design. We validate the effectiveness of our pretext
task and decoder design using different encoder structures for 3D training and
demonstrate the advantages of our pretrained networks on various point cloud
analysis tasks.",None,-1
43d2c366-347b-4992-aae6-1ce46d8eedbb,C3: Zero-shot Text-to-SQL with ChatGPT,0.935833,47,"This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3,
which achieves 82.3\% in terms of execution accuracy on the holdout test set of
Spider and becomes the state-of-the-art zero-shot Text-to-SQL method on the
Spider Challenge. C3 consists of three key components: Clear Prompting (CP),
Calibration with Hints (CH), and Consistent Output (CO), which are
corresponding to the model input, model bias and model output respectively. It
provides a systematic treatment for zero-shot Text-to-SQL. Extensive
experiments have been conducted to verify the effectiveness and efficiency of
our proposed method.",None,-1
afe46827-c928-4015-a666-2eff23330e4b,GarmentTracking: Category-Level Garment Pose Tracking,0.741504,6,"Garments are important to humans. A visual system that can estimate and track
the complete garment pose can be useful for many downstream tasks and
real-world applications. In this work, we present a complete package to address
the category-level garment pose tracking task: (1) A recording system
VR-Garment, with which users can manipulate virtual garment models in
simulation through a VR interface. (2) A large-scale dataset VR-Folding, with
complex garment pose configurations in manipulation like flattening and
folding. (3) An end-to-end online tracking framework GarmentTracking, which
predicts complete garment pose both in canonical space and task space given a
point cloud sequence. Extensive experiments demonstrate that the proposed
GarmentTracking achieves great performance even when the garment has large
non-rigid deformation. It outperforms the baseline approach on both speed and
accuracy. We hope our proposed solution can serve as a platform for future
research. Codes and datasets are available in
https://garment-tracking.robotflow.ai.",None,-1
fa04b7b9-b09b-4400-bfa8-f66f422c7990,Algorithm-assisted discovery of an intrinsic order among mathematical constants,0.425472,1,"In recent decades, a growing number of discoveries in fields of mathematics
have been assisted by computer algorithms, primarily for exploring large
parameter spaces that humans would take too long to investigate. As computers
and algorithms become more powerful, an intriguing possibility arises - the
interplay between human intuition and computer algorithms can lead to
discoveries of novel mathematical concepts that would otherwise remain elusive.
To realize this perspective, we have developed a massively parallel computer
algorithm that discovers an unprecedented number of continued fraction formulas
for fundamental mathematical constants. The sheer number of formulas discovered
by the algorithm unveils a novel mathematical structure that we call the
conservative matrix field. Such matrix fields (1) unify thousands of existing
formulas, (2) generate infinitely many new formulas, and most importantly, (3)
lead to unexpected relations between different mathematical constants,
including multiple integer values of the Riemann zeta function. Conservative
matrix fields also enable new mathematical proofs of irrationality. In
particular, we can use them to generalize the celebrated proof by Ap\'ery for
the irrationality of $\zeta(3)$. Utilizing thousands of personal computers
worldwide, our computer-supported research strategy demonstrates the power of
experimental mathematics, highlighting the prospects of large-scale
computational approaches to tackle longstanding open problems and discover
unexpected connections across diverse fields of science.",None,-1
990b5096-87f4-4b37-ae0b-30ea7aa28375,Token Imbalance Adaptation for Radiology Report Generation,0.185047,2,"Imbalanced token distributions naturally exist in text documents, leading
neural language models to overfit on frequent tokens. The token imbalance may
dampen the robustness of radiology report generators, as complex medical terms
appear less frequently but reflect more medical information. In this study, we
demonstrate how current state-of-the-art models fail to generate infrequent
tokens on two standard benchmark datasets (IU X-RAY and MIMIC-CXR) of radiology
report generation. % However, no prior study has proposed methods to adapt
infrequent tokens for text generators feeding with medical images. To solve the
challenge, we propose the \textbf{T}oken \textbf{Im}balance Adapt\textbf{er}
(\textit{TIMER}), aiming to improve generation robustness on infrequent tokens.
The model automatically leverages token imbalance by an unlikelihood loss and
dynamically optimizes generation processes to augment infrequent tokens. We
compare our approach with multiple state-of-the-art methods on the two
benchmarks. Experiments demonstrate the effectiveness of our approach in
enhancing model robustness overall and infrequent tokens. Our ablation analysis
shows that our reinforcement learning method has a major effect in adapting
token imbalance for radiology report generation.",None,-1
7d0c9fcd-64ac-413b-8e53-acbfa3596455,Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction,0.453106,1,"The task of multimodal relation extraction has attracted significant research
attention, but progress is constrained by the scarcity of available training
data. One natural thought is to extend existing datasets with cross-modal
generative models. In this paper, we consider a novel problem setting, where
only unimodal data, either text or image, are available during training. We aim
to train a multimodal classifier from synthetic data that perform well on real
multimodal test data. However, training with synthetic data suffers from two
obstacles: lack of data diversity and label information loss. To alleviate the
issues, we propose Mutual Information-aware Multimodal Iterated Relational dAta
GEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to
promote diversity in the generated data and exploits a teacher network to
select valuable training samples with high mutual information with the
ground-truth labels. Comparing our method to direct training on synthetic data,
we observed a significant improvement of 24.06% F1 with synthetic text and
26.42% F1 with synthetic images. Notably, our best model trained on completely
synthetic images outperforms prior state-of-the-art models trained on real
multimodal data by a margin of 3.76% in F1. Our codebase will be made available
upon acceptance.",None,-1
6be28591-b8db-4b32-b78c-823aeb99a4fc,Classification of retail products: From probabilistic ranking to neural networks,0.364721,7,"Food retailing is now on an accelerated path to a success penetration into
the digital market by new ways of value creation at all stages of the consumer
decision process. One of the most important imperatives in this path is the
availability of quality data to feed all the process in digital transformation.
But the quality of data is not so obvious if we consider the variety of
products and suppliers in the grocery market. Within this context of digital
transformation of grocery industry, \textit{Midiadia} is Spanish data provider
company that works on converting data from the retailers' products into
knowledge with attributes and insights from the product labels, that is,
maintaining quality data in a dynamic market with a high dispersion of
products. Currently, they manually categorize products (groceries) according to
the information extracted directly (text processing) from the product labelling
and packaging. This paper introduces a solution to automatically categorize the
constantly changing product catalogue into a 3-level food taxonomy. Our
proposal studies three different approaches: a score-based ranking method,
traditional machine learning algorithms, and deep neural networks. Thus, we
provide four different classifiers that support a more efficient and less
error-prone maintenance of groceries catalogues, the main asset of the company.
Finally, we have compared the performance of these three alternatives,
concluding that traditional machine learning algorithms perform better, but
closely followed by the score-based approach.",None,-1
7cb90907-8e73-48c5-8036-b9692960b7cd,Interventional Bag Multi-Instance Learning On Whole-Slide Pathological Images,0.998602,26,"Multi-instance learning (MIL) is an effective paradigm for whole-slide
pathological images (WSIs) classification to handle the gigapixel resolution
and slide-level label. Prevailing MIL methods primarily focus on improving the
feature extractor and aggregator. However, one deficiency of these methods is
that the bag contextual prior may trick the model into capturing spurious
correlations between bags and labels. This deficiency is a confounder that
limits the performance of existing MIL methods. In this paper, we propose a
novel scheme, Interventional Bag Multi-Instance Learning (IBMIL), to achieve
deconfounded bag-level prediction. Unlike traditional likelihood-based
strategies, the proposed scheme is based on the backdoor adjustment to achieve
the interventional training, thus is capable of suppressing the bias caused by
the bag contextual prior. Note that the principle of IBMIL is orthogonal to
existing bag MIL methods. Therefore, IBMIL is able to bring consistent
performance boosting to existing schemes, achieving new state-of-the-art
performance. Code is available at https://github.com/HHHedo/IBMIL.",None,-1
400e8c9a-c1dd-4957-8be9-a0ffe921b4ce,"A Study on Accuracy, Miscalibration, and Popularity Bias in Recommendations",0.205179,3,"Recent research has suggested different metrics to measure the inconsistency
of recommendation performance, including the accuracy difference between user
groups, miscalibration, and popularity lift. However, a study that relates
miscalibration and popularity lift to recommendation accuracy across different
user groups is still missing. Additionally, it is unclear if particular genres
contribute to the emergence of inconsistency in recommendation performance
across user groups. In this paper, we present an analysis of these three
aspects of five well-known recommendation algorithms for user groups that
differ in their preference for popular content. Additionally, we study how
different genres affect the inconsistency of recommendation performance, and
how this is aligned with the popularity of the genres. Using data from LastFm,
MovieLens, and MyAnimeList, we present two key findings. First, we find that
users with little interest in popular content receive the worst recommendation
accuracy, and that this is aligned with miscalibration and popularity lift.
Second, our experiments show that particular genres contribute to a different
extent to the inconsistency of recommendation performance, especially in terms
of miscalibration in the case of the MyAnimeList dataset.",None,-1
c4c75770-cc1a-4258-a1c4-4a7a4bf7211f,Word sense extension,0.756963,3,"Humans often make creative use of words to express novel senses. A
long-standing effort in natural language processing has been focusing on word
sense disambiguation (WSD), but little has been explored about how the sense
inventory of a word may be extended toward novel meanings. We present a
paradigm of word sense extension (WSE) that enables words to spawn new senses
toward novel context. We develop a framework that simulates novel word sense
extension by first partitioning a polysemous word type into two pseudo-tokens
that mark its different senses, and then inferring whether the meaning of a
pseudo-token can be extended to convey the sense denoted by the token
partitioned from the same word type. Our framework combines cognitive models of
chaining with a learning scheme that transforms a language model embedding
space to support various types of word sense extension. We evaluate our
framework against several competitive baselines and show that it is superior in
predicting plausible novel senses for over 7,500 English words. Furthermore, we
show that our WSE framework improves performance over a range of
transformer-based WSD models in predicting rare word senses with few or zero
mentions in the training data.",None,-1
0fdbe588-4d24-4cab-9a57-5936d2d2b923,Video Frame Interpolation with Densely Queried Bilateral Correlation,0.180763,2,"Video Frame Interpolation (VFI) aims to synthesize non-existent intermediate
frames between existent frames. Flow-based VFI algorithms estimate intermediate
motion fields to warp the existent frames. Real-world motions' complexity and
the reference frame's absence make motion estimation challenging. Many
state-of-the-art approaches explicitly model the correlations between two
neighboring frames for more accurate motion estimation. In common approaches,
the receptive field of correlation modeling at higher resolution depends on the
motion fields estimated beforehand. Such receptive field dependency makes
common motion estimation approaches poor at coping with small and fast-moving
objects. To better model correlations and to produce more accurate motion
fields, we propose the Densely Queried Bilateral Correlation (DQBC) that gets
rid of the receptive field dependency problem and thus is more friendly to
small and fast-moving objects. The motion fields generated with the help of
DQBC are further refined and up-sampled with context features. After the motion
fields are fixed, a CNN-based SynthNet synthesizes the final interpolated
frame. Experiments show that our approach enjoys higher accuracy and less
inference time than the state-of-the-art. Source code is available at
https://github.com/kinoud/DQBC.",None,-1
42e62ceb-4699-4f55-b061-e482c40a2523,SensePOLAR: Word sense aware interpretability for pre-trained contextual word embeddings,0.221102,2,"Adding interpretability to word embeddings represents an area of active
research in text representation. Recent work has explored thepotential of
embedding words via so-called polar dimensions (e.g. good vs. bad, correct vs.
wrong). Examples of such recent approaches include SemAxis, POLAR, FrameAxis,
and BiImp. Although these approaches provide interpretable dimensions for
words, they have not been designed to deal with polysemy, i.e. they can not
easily distinguish between different senses of words. To address this
limitation, we present SensePOLAR, an extension of the original POLAR framework
that enables word-sense aware interpretability for pre-trained contextual word
embeddings. The resulting interpretable word embeddings achieve a level of
performance that is comparable to original contextual word embeddings across a
variety of natural language processing tasks including the GLUE and SQuAD
benchmarks. Our work removes a fundamental limitation of existing approaches by
offering users sense aware interpretations for contextual word embeddings.",None,-1
55e03400-8f05-4a8e-8160-3dadd154fcae,FineDeb: A Debiasing Framework for Language Models,0.122652,4,"As language models are increasingly included in human-facing machine learning
tools, bias against demographic subgroups has gained attention. We propose
FineDeb, a two-phase debiasing framework for language models that starts with
contextual debiasing of embeddings learned by pretrained language models. The
model is then fine-tuned on a language modeling objective. Our results show
that FineDeb offers stronger debiasing in comparison to other methods which
often result in models as biased as the original language model. Our framework
is generalizable for demographics with multiple classes, and we demonstrate its
effectiveness through extensive experiments and comparisons with state of the
art techniques. We release our code and data on GitHub.",None,-1
dbc7764b-8b8a-4c16-a5bd-38590d1f8495,CCLAP: Controllable Chinese Landscape Painting Generation via Latent Diffusion Model,0.750882,2,"With the development of deep generative models, recent years have seen great
success of Chinese landscape painting generation. However, few works focus on
controllable Chinese landscape painting generation due to the lack of data and
limited modeling capabilities. In this work, we propose a controllable Chinese
landscape painting generation method named CCLAP, which can generate painting
with specific content and style based on Latent Diffusion Model. Specifically,
it consists of two cascaded modules, i.e., content generator and style
aggregator. The content generator module guarantees the content of generated
paintings specific to the input text. While the style aggregator module is to
generate paintings of a style corresponding to a reference image. Moreover, a
new dataset of Chinese landscape paintings named CLAP is collected for
comprehensive evaluation. Both the qualitative and quantitative results
demonstrate that our method achieves state-of-the-art performance, especially
in artfully-composed and artistic conception. Codes are available at
https://github.com/Robin-WZQ/CCLAP.",None,-1
e92d41a9-4aa2-4ad3-8fc9-23ae5290b3ad,Ambiguous Medical Image Segmentation using Diffusion Models,0.976718,41,"Collective insights from a group of experts have always proven to outperform
an individual's best diagnostic for clinical tasks. For the task of medical
image segmentation, existing research on AI-based alternatives focuses more on
developing models that can imitate the best individual rather than harnessing
the power of expert groups. In this paper, we introduce a single diffusion
model-based approach that produces multiple plausible outputs by learning a
distribution over group insights. Our proposed model generates a distribution
of segmentation masks by leveraging the inherent stochastic sampling process of
diffusion using only minimal additional learning. We demonstrate on three
different medical image modalities- CT, ultrasound, and MRI that our model is
capable of producing several possible variants while capturing the frequencies
of their occurrences. Comprehensive results show that our proposed approach
outperforms existing state-of-the-art ambiguous segmentation networks in terms
of accuracy while preserving naturally occurring variation. We also propose a
new metric to evaluate the diversity as well as the accuracy of segmentation
predictions that aligns with the interest of clinical practice of collective
insights.",None,-1
bd3d74d6-532c-4fc0-83ef-3af04924c141,Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI,0.867297,29,"Explainable AI (XAI) systems are sociotechnical in nature; thus, they are
subject to the sociotechnical gap--divide between the technical affordances and
the social needs. However, charting this gap is challenging. In the context of
XAI, we argue that charting the gap improves our problem understanding, which
can reflexively provide actionable insights to improve explainability.
Utilizing two case studies in distinct domains, we empirically derive a
framework that facilitates systematic charting of the sociotechnical gap by
connecting AI guidelines in the context of XAI and elucidating how to use them
to address the gap. We apply the framework to a third case in a new domain,
showcasing its affordances. Finally, we discuss conceptual implications of the
framework, share practical considerations in its operationalization, and offer
guidance on transferring it to new contexts. By making conceptual and practical
contributions to understanding the sociotechnical gap in XAI, the framework
expands the XAI design space.",None,-1
2e186ecd-7c12-4f88-a6a7-165245e0aec6,Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages,0.109318,1,"Neural text-to-speech (TTS) models can synthesize natural human speech when
trained on large amounts of transcribed speech. However, collecting such
large-scale transcribed data is expensive. This paper proposes an unsupervised
pre-training method for a sequence-to-sequence TTS model by leveraging large
untranscribed speech data. With our pre-training, we can remarkably reduce the
amount of paired transcribed data required to train the model for the target
downstream TTS task. The main idea is to pre-train the model to reconstruct
de-warped mel-spectrograms from warped ones, which may allow the model to learn
proper temporal assignment relation between input and output sequences. In
addition, we propose a data augmentation method that further improves the data
efficiency in fine-tuning. We empirically demonstrate the effectiveness of our
proposed method in low-resource language scenarios, achieving outstanding
performance compared to competing methods. The code and audio samples are
available at: https://github.com/cnaigithub/SpeechDewarping",None,-1
be866603-f4e1-469b-baae-e8b68b9bf8d3,NOPE: Novel Object Pose Estimation from a Single Image,0.633687,8,"The practicality of 3D object pose estimation remains limited for many
applications due to the need for prior knowledge of a 3D model and a training
period for new objects. To address this limitation, we propose an approach that
takes a single image of a new object as input and predicts the relative pose of
this object in new images without prior knowledge of the object's 3D model and
without requiring training time for new objects and categories. We achieve this
by training a model to directly predict discriminative embeddings for
viewpoints surrounding the object. This prediction is done using a simple U-Net
architecture with attention and conditioned on the desired pose, which yields
extremely fast inference. We compare our approach to state-of-the-art methods
and show it outperforms them both in terms of accuracy and robustness. Our
source code is publicly available at https://github.com/nv-nguyen/nope",None,-1
6b93b512-5d4c-4420-9260-6ee12f5f92e2,STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos,0.594313,3,"We address the problem of extracting key steps from unlabeled procedural
videos, motivated by the potential of Augmented Reality (AR) headsets to
revolutionize job training and performance. We decompose the problem into two
steps: representation learning and key steps extraction. We propose a training
objective, Bootstrapped Multi-Cue Contrastive (BMC2) loss to learn
discriminative representations for various steps without any labels. Different
from prior works, we develop techniques to train a light-weight temporal module
which uses off-the-shelf features for self supervision. Our approach can
seamlessly leverage information from multiple cues like optical flow, depth or
gaze to learn discriminative features for key-steps, making it amenable for AR
applications. We finally extract key steps via a tunable algorithm that
clusters the representations and samples. We show significant improvements over
prior works for the task of key step localization and phase classification.
Qualitative results demonstrate that the extracted key steps are meaningful and
succinctly represent various steps of the procedural tasks.",None,-1
9be5680e-ed10-440b-9113-51aab79cc7d6,Neuromorphic Event-based Facial Expression Recognition,0.949129,7,"Recently, event cameras have shown large applicability in several computer
vision fields especially concerning tasks that require high temporal
resolution. In this work, we investigate the usage of such kind of data for
emotion recognition by presenting NEFER, a dataset for Neuromorphic Event-based
Facial Expression Recognition. NEFER is composed of paired RGB and event videos
representing human faces labeled with the respective emotions and also
annotated with face bounding boxes and facial landmarks. We detail the data
acquisition process as well as providing a baseline method for RGB and event
data. The collected data captures subtle micro-expressions, which are hard to
spot with RGB data, yet emerge in the event domain. We report a double
recognition accuracy for the event-based approach, proving the effectiveness of
a neuromorphic approach for analyzing fast and hardly detectable expressions
and the emotions they conceal.",None,-1
a8590d8f-8a3c-4486-88fa-e8fb840e1cd5,Human Body Digital Twin: A Master Plan,0.176704,2,"A human body digital twin (DT) is a virtual representation of an individual's
physiological state, created using real-time data from sensors and medical test
devices, with the purpose of simulating, predicting, and optimizing health
outcomes through advanced analytics and simulations. The human body DT has the
potential to revolutionize healthcare and wellness, but its responsible and
effective implementation requires consideration of various factors. This
article presents a comprehensive overview of the current status and future
prospects of the human body DT and proposes a five-level roadmap for its
development. The roadmap covers the development of various components, such as
wearable devices, data collection, data analysis, and decision-making systems.
The article also highlights the necessary support, security, cost, and ethical
considerations that must be addressed in order to ensure responsible and
effective implementation of the human body DT. The proposed roadmap provides a
framework for guiding future development and offers a unique perspective on the
future of the human body DT, facilitating new interdisciplinary research and
innovative solutions in this rapidly evolving field.",None,-1
ba9086b6-63d2-40b0-a812-6f086806415d,"The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation",0.999719,119,"Conversational artificial intelligence (AI) disrupts how humans interact with
technology. Recently, OpenAI introduced ChatGPT, a state-of-the-art dialogue
model that can converse with its human counterparts with unprecedented
capabilities. ChatGPT has witnessed tremendous attention from the media,
academia, industry, and the general public, attracting more than a million
users within days of its release. However, its explosive adoption for
information search and as an automated decision aid underscores the importance
to understand its limitations and biases. This paper focuses on one of
democratic society's most important decision-making processes: political
elections. Prompting ChatGPT with 630 political statements from two leading
voting advice applications and the nation-agnostic political compass test in
three pre-registered experiments, we uncover ChatGPT's pro-environmental,
left-libertarian ideology. For example, ChatGPT would impose taxes on flights,
restrict rent increases, and legalize abortion. In the 2021 elections, it would
have voted most likely for the Greens both in Germany (B\""undnis 90/Die
Gr\""unen) and in the Netherlands (GroenLinks). Our findings are robust when
negating the prompts, reversing the order of the statements, varying prompt
formality, and across languages (English, German, Dutch, and Spanish). We
conclude by discussing the implications of politically biased conversational AI
on society.",None,-1
8b4cb05d-8018-4149-940c-b28c7d260160,Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models,0.770145,2,"Document-level Relation Extraction (DocRE), which aims to extract relations
from a long context, is a critical challenge in achieving fine-grained
structural comprehension and generating interpretable document representations.
Inspired by recent advances in in-context learning capabilities emergent from
large language models (LLMs), such as ChatGPT, we aim to design an automated
annotation method for DocRE with minimum human effort. Unfortunately, vanilla
in-context learning is infeasible for document-level relation extraction due to
the plenty of predefined fine-grained relation types and the uncontrolled
generations of LLMs. To tackle this issue, we propose a method integrating a
large language model (LLM) and a natural language inference (NLI) module to
generate relation triples, thereby augmenting document-level relation datasets.
We demonstrate the effectiveness of our approach by introducing an enhanced
dataset known as DocGNRE, which excels in re-annotating numerous long-tail
relation types. We are confident that our method holds the potential for
broader applications in domain-specific relation type definitions and offers
tangible benefits in advancing generalized language semantic comprehension.",None,-1
399410e2-95a7-47d9-aab7-905781e3d1be,"On the Planning, Search, and Memorization Capabilities of Large Language Models",0.00963941,1,"The rapid advancement of large language models, such as the Generative
Pre-trained Transformer (GPT) series, has had significant implications across
various disciplines. In this study, we investigate the potential of the
state-of-the-art large language model (GPT-4) for planning tasks. We explore
its effectiveness in multiple planning subfields, highlighting both its
strengths and limitations. Through a comprehensive examination, we identify
areas where large language models excel in solving planning problems and reveal
the constraints that limit their applicability. Our empirical analysis focuses
on GPT-4's performance in planning domain extraction, graph search path
planning, and adversarial planning. We then propose a way of fine-tuning a
domain-specific large language model to improve its Chain of Thought (CoT)
capabilities for the above-mentioned tasks. The results provide valuable
insights into the potential applications of large language models in the
planning domain and pave the way for future research to overcome their
limitations and expand their capabilities.",None,-1
d3b2dde2-8cd0-42f0-a693-827945afe3b4,Contrast with Major Classifier Vectors for Federated Medical Relation Extraction with Heterogeneous Label Distribution,0.125436,1,"Federated medical relation extraction enables multiple clients to train a
deep network collaboratively without sharing their raw medical data. In order
to handle the heterogeneous label distribution across clients, most of the
existing works only involve enforcing regularization between local and global
models during optimization. In this paper, we fully utilize the models of all
clients and propose a novel concept of \textit{major classifier vectors}, where
a group of class vectors is obtained in an ensemble rather than the weighted
average method on the server. The major classifier vectors are then distributed
to all clients and the local training of each client is Contrasted with Major
Classifier vectors (FedCMC), so the local model is not prone to overfitting to
the local label distribution. FedCMC requires only a small amount of additional
transfer of classifier parameters without any leakage of raw data, extracted
representations, and label distributions. Our extensive experiments show that
FedCMC outperforms the other state-of-the-art FL algorithms on three medical
relation extraction datasets.",None,-1
56917941-094a-4cdc-a5aa-7d05ce1e8021,On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion,0.747131,6,"Generalizing deep learning models to unknown target domain distribution with
low latency has motivated research into test-time training/adaptation
(TTT/TTA). Existing approaches often focus on improving test-time training
performance under well-curated target domain data. As figured out in this work,
many state-of-the-art methods fail to maintain the performance when the target
domain is contaminated with strong out-of-distribution (OOD) data, a.k.a.
open-world test-time training (OWTTT). The failure is mainly due to the
inability to distinguish strong OOD samples from regular weak OOD samples. To
improve the robustness of OWTTT we first develop an adaptive strong OOD pruning
which improves the efficacy of the self-training TTT method. We further propose
a way to dynamically expand the prototypes to represent strong OOD samples for
an improved weak/strong OOD data separation. Finally, we regularize
self-training with distribution alignment and the combination yields the
state-of-the-art performance on 5 OWTTT benchmarks. The code is available at
https://github.com/Yushu-Li/OWTTT.",None,-1
7164a418-afde-4db7-9b7c-b042dd4d6b66,Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning,0.660441,6,"The offline reinforcement learning (RL) paradigm provides a general recipe to
convert static behavior datasets into policies that can perform better than the
policy that collected the data. While policy constraints, conservatism, and
other methods for mitigating distributional shifts have made offline
reinforcement learning more effective, the continuous action setting often
necessitates various approximations for applying these techniques. Many of
these challenges are greatly alleviated in discrete action settings, where
offline RL constraints and regularizers can often be computed more precisely or
even exactly. In this paper, we propose an adaptive scheme for action
quantization. We use a VQ-VAE to learn state-conditioned action quantization,
avoiding the exponential blowup that comes with na\""ive discretization of the
action space. We show that several state-of-the-art offline RL methods such as
IQL, CQL, and BRAC improve in performance on benchmarks when combined with our
proposed discretization scheme. We further validate our approach on a set of
challenging long-horizon complex robotic manipulation tasks in the Robomimic
environment, where our discretized offline RL algorithms are able to improve
upon their continuous counterparts by 2-3x. Our project page is at
https://saqrl.github.io/",None,-1
5afdd44b-acf4-45e6-a61c-1ea36effbc08,Extractive Summarization via ChatGPT for Faithful Summary Generation,0.999996,49,"Extractive summarization is a crucial task in natural language processing
that aims to condense long documents into shorter versions by directly
extracting sentences. The recent introduction of large language models has
attracted significant interest in the NLP community due to its remarkable
performance on a wide range of downstream tasks. This paper first presents a
thorough evaluation of ChatGPT's performance on extractive summarization and
compares it with traditional fine-tuning methods on various benchmark datasets.
Our experimental analysis reveals that ChatGPT exhibits inferior extractive
summarization performance in terms of ROUGE scores compared to existing
supervised systems, while achieving higher performance based on LLM-based
evaluation metrics. In addition, we explore the effectiveness of in-context
learning and chain-of-thought reasoning for enhancing its performance.
Furthermore, we find that applying an extract-then-generate pipeline with
ChatGPT yields significant performance improvements over abstractive baselines
in terms of summary faithfulness. These observations highlight potential
directions for enhancing ChatGPT's capabilities in faithful summarization using
two-stage approaches.",None,-1
c0b5a234-071f-4710-b8f6-66d29f475ecf,RLLTE: Long-Term Evolution Project of Reinforcement Learning,0.0407192,1,"We present RLLTE: a long-term evolution, extremely modular, and open-source
framework for reinforcement learning (RL) research and application. Beyond
delivering top-notch algorithm implementations, RLLTE also serves as a toolkit
for developing algorithms. More specifically, RLLTE decouples the RL algorithms
completely from the exploitation-exploration perspective, providing a large
number of components to accelerate algorithm development and evolution. In
particular, RLLTE is the first RL framework to build a complete and luxuriant
ecosystem, which includes model training, evaluation, deployment, benchmark
hub, and large language model (LLM)-empowered copilot. RLLTE is expected to set
standards for RL engineering practice and be highly stimulative for industry
and academia.",None,-1
ae9f07af-3e08-4693-9e9e-6e98396bbe5e,Fair Decision-making Under Uncertainty,0.756516,29,"There has been concern within the artificial intelligence (AI) community and
the broader society regarding the potential lack of fairness of AI-based
decision-making systems. Surprisingly, there is little work quantifying and
guaranteeing fairness in the presence of uncertainty which is prevalent in many
socially sensitive applications, ranging from marketing analytics to actuarial
analysis and recidivism prediction instruments. To this end, we study a
longitudinal censored learning problem subject to fairness constraints, where
we require that algorithmic decisions made do not affect certain individuals or
social groups negatively in the presence of uncertainty on class label due to
censorship. We argue that this formulation has a broader applicability to
practical scenarios concerning fairness. We show how the newly devised fairness
notions involving censored information and the general framework for fair
predictions in the presence of censorship allow us to measure and mitigate
discrimination under uncertainty that bridges the gap with real-world
applications. Empirical evaluations on real-world discriminated datasets with
censorship demonstrate the practicality of our approach.",None,-1
4873741b-618e-496d-939c-9be51eafd938,Semantic Strengthening of Neuro-Symbolic Learning,0.735066,7,"Numerous neuro-symbolic approaches have recently been proposed typically with
the goal of adding symbolic knowledge to the output layer of a neural network.
Ideally, such losses maximize the probability that the neural network's
predictions satisfy the underlying domain. Unfortunately, this type of
probabilistic inference is often computationally infeasible. Neuro-symbolic
approaches therefore commonly resort to fuzzy approximations of this
probabilistic objective, sacrificing sound probabilistic semantics, or to
sampling which is very seldom feasible. We approach the problem by first
assuming the constraint decomposes conditioned on the features learned by the
network. We iteratively strengthen our approximation, restoring the dependence
between the constraints most responsible for degrading the quality of the
approximation. This corresponds to computing the mutual information between
pairs of constraints conditioned on the network's learned features, and may be
construed as a measure of how well aligned the gradients of two distributions
are. We show how to compute this efficiently for tractable circuits. We test
our approach on three tasks: predicting a minimum-cost path in Warcraft,
predicting a minimum-cost perfect matching, and solving Sudoku puzzles,
observing that it improves upon the baselines while sidestepping
intractability.",None,-1
6e9c1424-4e69-4758-9888-59d620a1f16f,GREC: Generalized Referring Expression Comprehension,0.481225,3,"The objective of Classic Referring Expression Comprehension (REC) is to
produce a bounding box corresponding to the object mentioned in a given textual
description. Commonly, existing datasets and techniques in classic REC are
tailored for expressions that pertain to a single target, meaning a sole
expression is linked to one specific object. Expressions that refer to multiple
targets or involve no specific target have not been taken into account. This
constraint hinders the practical applicability of REC. This study introduces a
new benchmark termed as Generalized Referring Expression Comprehension (GREC).
This benchmark extends the classic REC by permitting expressions to describe
any number of target objects. To achieve this goal, we have built the first
large-scale GREC dataset named gRefCOCO. This dataset encompasses a range of
expressions: those referring to multiple targets, expressions with no specific
target, and the single-target expressions. The design of GREC and gRefCOCO
ensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, a
GREC method implementation code, and GREC evaluation code are available at
https://github.com/henghuiding/gRefCOCO.",None,-1
c58509fc-5508-47c5-b89e-dfc08f3a81a9,STEERER: Resolving Scale Variations for Counting and Localization via Selective Inheritance Learning,0.445063,13,"Scale variation is a deep-rooted problem in object counting, which has not
been effectively addressed by existing scale-aware algorithms. An important
factor is that they typically involve cooperative learning across
multi-resolutions, which could be suboptimal for learning the most
discriminative features from each scale. In this paper, we propose a novel
method termed STEERER (\textbf{S}elec\textbf{T}iv\textbf{E}
inh\textbf{ER}itance l\textbf{E}a\textbf{R}ning) that addresses the issue of
scale variations in object counting. STEERER selects the most suitable scale
for patch objects to boost feature extraction and only inherits discriminative
features from lower to higher resolution progressively. The main insights of
STEERER are a dedicated Feature Selection and Inheritance Adaptor (FSIA), which
selectively forwards scale-customized features at each scale, and a Masked
Selection and Inheritance Loss (MSIL) that helps to achieve high-quality
density maps across all scales. Our experimental results on nine datasets with
counting and localization tasks demonstrate the unprecedented scale
generalization ability of STEERER. Code is available at
\url{https://github.com/taohan10200/STEERER}.",None,-1
fe5398b9-dc14-4915-88ae-0f7dd58ef1cb,SynMotor: A Benchmark Suite for Object Attribute Regression and Multi-task Learning,0.106787,1,"In this paper, we develop a novel benchmark suite including both a 2D
synthetic image dataset and a 3D synthetic point cloud dataset. Our work is a
sub-task in the framework of a remanufacturing project, in which small electric
motors are used as fundamental objects. Apart from the given detection,
classification, and segmentation annotations, the key objects also have
multiple learnable attributes with ground truth provided. This benchmark can be
used for computer vision tasks including 2D/3D detection, classification,
segmentation, and multi-attribute learning. It is worth mentioning that most
attributes of the motors are quantified as continuously variable rather than
binary, which makes our benchmark well-suited for the less explored regression
tasks. In addition, appropriate evaluation metrics are adopted or developed for
each task and promising baseline results are provided. We hope this benchmark
can stimulate more research efforts on the sub-domain of object attribute
learning and multi-task learning in the future.",None,-1
bb1d1f56-9997-46d5-921b-c7fd1121345b,A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases,0.332776,10,"Enterprise applications of Large Language Models (LLMs) hold promise for
question answering on enterprise SQL databases. However, the extent to which
LLMs can accurately respond to enterprise questions in such databases remains
unclear, given the absence of suitable Text-to-SQL benchmarks tailored to
enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to
enhance LLM-based question answering by providing business context is not well
understood. This study aims to evaluate the accuracy of LLM-powered question
answering systems in the context of enterprise questions and SQL databases,
while also exploring the role of knowledge graphs in improving accuracy. To
achieve this, we introduce a benchmark comprising an enterprise SQL schema in
the insurance domain, a range of enterprise queries encompassing reporting to
metrics, and a contextual layer incorporating an ontology and mappings that
define a knowledge graph. Our primary finding reveals that question answering
using GPT-4, with zero-shot prompts directly on SQL databases, achieves an
accuracy of 16%. Notably, this accuracy increases to 54% when questions are
posed over a Knowledge Graph representation of the enterprise SQL database.
Therefore, investing in Knowledge Graph provides higher accuracy for LLM
powered question answering systems.",None,-1
48a8f26d-8f81-42c9-af90-c77afaf88b18,Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction,0.423737,4,"Given their flexibility and encouraging performance, deep-learning models are
becoming standard for motion prediction in autonomous driving. However, with
great flexibility comes a lack of interpretability and possible violations of
physical constraints. Accompanying these data-driven methods with
differentially-constrained motion models to provide physically feasible
trajectories is a promising future direction. The foundation for this work is a
previously introduced graph-neural-network-based model, MTP-GO. The neural
network learns to compute the inputs to an underlying motion model to provide
physically feasible trajectories. This research investigates the performance of
various motion models in combination with numerical solvers for the prediction
task. The study shows that simpler models, such as low-order integrator models,
are preferred over more complex, e.g., kinematic models, to achieve accurate
predictions. Further, the numerical solver can have a substantial impact on
performance, advising against commonly used first-order methods like Euler
forward. Instead, a second-order method like Heun's can greatly improve
predictions.",None,-1
ddd159be-3a40-462b-8307-c9c1a68f07c7,Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks,0.99914,76,"The release of ChatGPT has uncovered a range of possibilities whereby large
language models (LLMs) can substitute human intelligence. In this paper, we
seek to understand whether ChatGPT has the potential to reproduce
human-generated label annotations in social computing tasks. Such an
achievement could significantly reduce the cost and complexity of social
computing research. As such, we use ChatGPT to relabel five seminal datasets
covering stance detection (2x), sentiment analysis, hate speech, and bot
detection. Our results highlight that ChatGPT does have the potential to handle
these data annotation tasks, although a number of challenges remain. ChatGPT
obtains an average accuracy 0.609. Performance is highest for the sentiment
analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we
show that performance varies substantially across individual labels. We believe
this work can open up new lines of analysis and act as a basis for future
research into the exploitation of ChatGPT for human annotation tasks.",None,-1
5c68fe2a-b315-4234-9e8f-d6019555a45f,An Error-Guided Correction Model for Chinese Spelling Error Correction,0.837056,5,"Although existing neural network approaches have achieved great success on
Chinese spelling correction, there is still room to improve. The model is
required to avoid over-correction and to distinguish a correct token from its
phonological and visually similar ones. In this paper, we propose an
error-guided correction model (EGCM) to improve Chinese spelling correction. By
borrowing the powerful ability of BERT, we propose a novel zero-shot error
detection method to do a preliminary detection, which guides our model to
attend more on the probably wrong tokens in encoding and to avoid modifying the
correct tokens in generating. Furthermore, we introduce a new loss function to
integrate the error confusion set, which enables our model to distinguish
easily misused tokens. Moreover, our model supports highly parallel decoding to
meet real application requirements. Experiments are conducted on widely used
benchmarks. Our model achieves superior performance against state-of-the-art
approaches by a remarkable margin, on both the correction quality and
computation speed.",None,-1
45b89870-f813-4cff-90bc-5a76265809d3,Empower Nested Boolean Logic via Self-Supervised Curriculum Learning,0.211346,5,"Beyond the great cognitive powers showcased by language models, it is crucial
to scrutinize whether their reasoning capabilities stem from strong
generalization or merely exposure to relevant data. As opposed to constructing
increasingly complex logic, this paper probes into the boolean logic, the root
capability of a logical reasoner. We find that any pre-trained language models
even including large language models only behave like a random selector in the
face of multi-nested boolean logic, a task that humans can handle with ease. To
empower language models with this fundamental capability, this paper proposes a
new self-supervised learning method \textit{Curriculum Logical Reasoning}
(\textsc{Clr}), where we augment the training data with nested boolean logic
chain step-by-step, and program the training from simpler logical patterns
gradually to harder ones. This new training paradigm allows language models to
effectively generalize to much harder and longer-hop logic, which can hardly be
learned through naive training. Furthermore, we show that boolean logic is a
great foundation for improving the subsequent general logical tasks.",None,-1
a0d1276e-ba07-43e7-b98b-3928a7c423de,A negation detection assessment of GPTs: analysis with the xNot360 dataset,0.16387,6,"Negation is a fundamental aspect of natural language, playing a critical role
in communication and comprehension. Our study assesses the negation detection
performance of Generative Pre-trained Transformer (GPT) models, specifically
GPT-2, GPT-3, GPT-3.5, and GPT-4. We focus on the identification of negation in
natural language using a zero-shot prediction approach applied to our custom
xNot360 dataset. Our approach examines sentence pairs labeled to indicate
whether the second sentence negates the first. Our findings expose a
considerable performance disparity among the GPT models, with GPT-4 surpassing
its counterparts and GPT-3.5 displaying a marked performance reduction. The
overall proficiency of the GPT models in negation detection remains relatively
modest, indicating that this task pushes the boundaries of their natural
language understanding capabilities. We not only highlight the constraints of
GPT models in handling negation but also emphasize the importance of logical
reliability in high-stakes domains such as healthcare, science, and law.",None,-1
54f7f99e-7e4e-4008-a7ab-8949e38c806c,Neural Machine Translation Models Can Learn to be Few-shot Learners,0.301717,4,"The emergent ability of Large Language Models to use a small number of
examples to learn to perform in novel domains and tasks, also called in-context
learning (ICL). In this work, we show that a much smaller model can be trained
to perform ICL by fine-tuning towards a specialized training objective,
exemplified on the task of domain adaptation for neural machine translation.
With this capacity for ICL, the model can take advantage of relevant few-shot
examples to adapt its output towards the domain. We compare the quality of this
domain adaptation to traditional supervised techniques and ICL with a
40B-parameter Large Language Model. Our approach allows efficient batch
inference on a mix of domains and outperforms state-of-the-art baselines in
terms of both translation quality and immediate adaptation rate, i.e. the
ability to reproduce a specific term after being shown a single example.",None,-1
d31f071b-a10f-433a-b141-6f0aacf9e52b,Efficient Computation of Counterfactual Bounds,0.27069,1,"We assume to be given structural equations over discrete variables inducing a
directed acyclic graph, namely, a structural causal model, together with data
about its internal nodes. The question we want to answer is how we can compute
bounds for partially identifiable counterfactual queries from such an input. We
start by giving a map from structural casual models to credal networks. This
allows us to compute exact counterfactual bounds via algorithms for credal nets
on a subclass of structural causal models. Exact computation is going to be
inefficient in general given that, as we show, causal inference is NP-hard even
on polytrees. We target then approximate bounds via a causal EM scheme. We
evaluate their accuracy by providing credible intervals on the quality of the
approximation; we show through a synthetic benchmark that the EM scheme
delivers accurate results in a fair number of runs. In the course of the
discussion, we also point out what seems to be a neglected limitation to the
trending idea that counterfactual bounds can be computed without knowledge of
the structural equations. We also present a real case study on palliative care
to show how our algorithms can readily be used for practical purposes.",None,-1
ab19b95c-14e2-4a73-8580-5715ca94122e,Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten,0.47037,8,"The Right to Explanation and the Right to be Forgotten are two important
principles outlined to regulate algorithmic decision making and data usage in
real-world applications. While the right to explanation allows individuals to
request an actionable explanation for an algorithmic decision, the right to be
forgotten grants them the right to ask for their data to be deleted from all
the databases and models of an organization. Intuitively, enforcing the right
to be forgotten may trigger model updates which in turn invalidate previously
provided explanations, thus violating the right to explanation. In this work,
we investigate the technical implications arising due to the interference
between the two aforementioned regulatory principles, and propose the first
algorithmic framework to resolve the tension between them. To this end, we
formulate a novel optimization problem to generate explanations that are robust
to model updates due to the removal of training data instances by data deletion
requests. We then derive an efficient approximation algorithm to handle the
combinatorial complexity of this optimization problem. We theoretically
demonstrate that our method generates explanations that are provably robust to
worst-case data deletion requests with bounded costs in case of linear models
and certain classes of non-linear models. Extensive experimentation with
real-world datasets demonstrates the efficacy of the proposed framework.",None,-1
686e9fa3-55ef-4890-84a0-d82b519c8e28,Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets,0.0390133,1,"A crucial aspect of a rumor detection model is its ability to generalize,
particularly its ability to detect emerging, previously unknown rumors. Past
research has indicated that content-based (i.e., using solely source posts as
input) rumor detection models tend to perform less effectively on unseen
rumors. At the same time, the potential of context-based models remains largely
untapped. The main contribution of this paper is in the in-depth evaluation of
the performance gap between content and context-based models specifically on
detecting new, unseen rumors. Our empirical findings demonstrate that
context-based models are still overly dependent on the information derived from
the rumors' source post and tend to overlook the significant role that
contextual information can play. We also study the effect of data split
strategies on classifier performance. Based on our experimental results, the
paper also offers practical suggestions on how to minimize the effects of
temporal concept drift in static datasets during the training of rumor
detection methods.",None,-1
83907ef1-2fa0-41da-a391-10746d3e7f5d,GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems,0.78489,51,"There has been considerable divergence of opinion on the reasoning abilities
of Large Language Models (LLMs). While the initial optimism that reasoning
might emerge automatically with scale has been tempered thanks to a slew of
counterexamples, a wide spread belief in their iterative self-critique
capabilities persists. In this paper, we set out to systematically investigate
the effectiveness of iterative prompting of LLMs in the context of Graph
Coloring, a canonical NP-complete reasoning problem that is related to
propositional satisfiability as well as practical problems like scheduling and
allocation. We present a principled empirical study of the performance of GPT4
in solving graph coloring instances or verifying the correctness of candidate
colorings. In iterative modes, we experiment with the model critiquing its own
answers and an external correct reasoner verifying proposed solutions. In both
cases, we analyze whether the content of the criticisms actually affects bottom
line performance. The study seems to indicate that (i) LLMs are bad at solving
graph coloring instances (ii) they are no better at verifying a solution--and
thus are not effective in iterative modes with LLMs critiquing LLM-generated
solutions (iii) the correctness and content of the criticisms--whether by LLMs
or external solvers--seems largely irrelevant to the performance of iterative
prompting. We show that the observed increase in effectiveness is largely due
to the correct solution being fortuitously present in the top-k completions of
the prompt (and being recognized as such by an external verifier). Our results
thus call into question claims about the self-critiquing capabilities of state
of the art LLMs.",None,-1
b674f653-2060-4516-aab0-63129d2087be,SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting,0.996609,20,"Building end-to-end task bots and maintaining their integration with new
functionalities using minimal human efforts is a long-standing challenge in
dialog research. Recently large language models (LLMs) have demonstrated
exceptional proficiency in conversational engagement and adherence to
instructions across various downstream tasks. In this work, we introduce
SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems
effortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we
instruct fixed LLMs to generate appropriate responses on novel tasks,
circumventing the need for training data. Specifically, SGP-TOD comprises three
components: a LLM for engaging with users, a DST Prompter to aid the LLM with
dialog state tracking, which is then used to retrieve database items, and a
Policy Prompter to elicit proper responses adhering to the provided dialog
policy. Experimental results on Multiwoz, RADDLE and STAR datasets show that
our training-free strategy SGP-TOD, without any task-specific data, yields
state-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot
approaches. In a domain-extension setting, SGP-TOD aptly adapts to new
functionalities by merely adding supplementary schema rules. We make our code
and data publicly available.",None,-1
f6f38d7e-37ad-4136-ab70-75b4d407cbf5,EVOLIN Benchmark: Evaluation of Line Detection and Association,0.32487,2,"Lines are interesting geometrical features commonly seen in indoor and urban
environments. There is missing a complete benchmark where one can evaluate
lines from a sequential stream of images in all its stages: Line detection,
Line Association and Pose error. To do so, we present a complete and exhaustive
benchmark for visual lines in a SLAM front-end, both for RGB and RGBD, by
providing a plethora of complementary metrics. We have also labelled data from
well-known SLAM datasets in order to have all in one poses and accurately
annotated lines. In particular, we have evaluated 17 line detection algorithms,
5 line associations methods and the resultant pose error for aligning a pair of
frames with several combinations of detector-association. We have packaged all
methods and evaluations metrics and made them publicly available on web-page
https://prime-slam.github.io/evolin/.",None,-1
ed525ae0-63d8-40c7-8266-1835657ce18e,SE-ORNet: Self-Ensembling Orientation-aware Network for Unsupervised Point Cloud Shape Correspondence,0.662247,5,"Unsupervised point cloud shape correspondence aims to obtain dense
point-to-point correspondences between point clouds without manually annotated
pairs. However, humans and some animals have bilateral symmetry and various
orientations, which lead to severe mispredictions of symmetrical parts.
Besides, point cloud noise disrupts consistent representations for point cloud
and thus degrades the shape correspondence accuracy. To address the above
issues, we propose a Self-Ensembling ORientation-aware Network termed SE-ORNet.
The key of our approach is to exploit an orientation estimation module with a
domain adaptive discriminator to align the orientations of point cloud pairs,
which significantly alleviates the mispredictions of symmetrical parts.
Additionally, we design a selfensembling framework for unsupervised point cloud
shape correspondence. In this framework, the disturbances of point cloud noise
are overcome by perturbing the inputs of the student and teacher networks with
different data augmentations and constraining the consistency of predictions.
Extensive experiments on both human and animal datasets show that our SE-ORNet
can surpass state-of-the-art unsupervised point cloud shape correspondence
methods.",None,-1
87023de7-6dfa-499a-8b89-ea4f0506f4d0,CGI-Stereo: Accurate and Real-Time Stereo Matching via Context and Geometry Interaction,0.701527,13,"In this paper, we propose CGI-Stereo, a novel neural network architecture
that can concurrently achieve real-time performance, competitive accuracy, and
strong generalization ability. The core of our CGI-Stereo is a Context and
Geometry Fusion (CGF) block which adaptively fuses context and geometry
information for more effective cost aggregation and meanwhile provides feedback
to feature learning to guide more effective contextual feature extraction. The
proposed CGF can be easily embedded into many existing stereo matching
networks, such as PSMNet, GwcNet and ACVNet. The resulting networks show a
significant improvement in accuracy. Specially, the model which incorporates
our CGF with ACVNet ranks $1^{st}$ on the KITTI 2012 and 2015 leaderboards
among all the published methods. We further propose an informative and concise
cost volume, named Attention Feature Volume (AFV), which exploits a correlation
volume as attention weights to filter a feature volume. Based on CGF and AFV,
the proposed CGI-Stereo outperforms all other published real-time methods on
KITTI benchmarks and shows better generalization ability than other real-time
methods. Code is available at https://github.com/gangweiX/CGI-Stereo.",None,-1
6bec8411-6f54-4b27-aa48-4d6ce178be8c,On the Challenges of Building Datasets for Hate Speech Detection,0.290066,1,"Detection of hate speech has been formulated as a standalone application of
NLP and different approaches have been adopted for identifying the target
groups, obtaining raw data, defining the labeling process, choosing the
detection algorithm, and evaluating the performance in the desired setting.
However, unlike other downstream tasks, hate speech suffers from the lack of
large-sized, carefully curated, generalizable datasets owing to the highly
subjective nature of the task. In this paper, we first analyze the issues
surrounding hate speech detection through a data-centric lens. We then outline
a holistic framework to encapsulate the data creation pipeline across seven
broad dimensions by taking the specific example of hate speech towards sexual
minorities. We posit that practitioners would benefit from following this
framework as a form of best practice when creating hate speech datasets in the
future.",None,-1
c04ad999-4ceb-4969-a88f-07e206ed477b,Mimic-IV-ICD: A new benchmark for eXtreme MultiLabel Classification,0.893704,8,"Clinical notes are assigned ICD codes - sets of codes for diagnoses and
procedures. In the recent years, predictive machine learning models have been
built for automatic ICD coding. However, there is a lack of widely accepted
benchmarks for automated ICD coding models based on large-scale public EHR
data.
  This paper proposes a public benchmark suite for ICD-10 coding using a large
EHR dataset derived from MIMIC-IV, the most recent public EHR dataset. We
implement and compare several popular methods for ICD coding prediction tasks
to standardize data preprocessing and establish a comprehensive ICD coding
benchmark dataset. This approach fosters reproducibility and model comparison,
accelerating progress toward employing automated ICD coding in future studies.
Furthermore, we create a new ICD-9 benchmark using MIMIC-IV data, providing
more data points and a higher number of ICD codes than MIMIC-III. Our
open-source code offers easy access to data processing steps, benchmark
creation, and experiment replication for those with MIMIC-IV access, providing
insights, guidance, and protocols to efficiently develop ICD coding models.",None,-1
1f325a82-7255-48ee-8c11-5a9b8f67630e,"Data-driven intelligent computational design for products: Method, techniques, and applications",0.326283,2,"Data-driven intelligent computational design (DICD) is a research hotspot
emerged under the context of fast-developing artificial intelligence. It
emphasizes on utilizing deep learning algorithms to extract and represent the
design features hidden in historical or fabricated design process data, and
then learn the combination and mapping patterns of these design features for
the purposes of design solution retrieval, generation, optimization,
evaluation, etc. Due to its capability of automatically and efficiently
generating design solutions and thus supporting human-in-the-loop intelligent
and innovative design activities, DICD has drawn the attentions from both
academic and industrial fields. However, as an emerging research subject, there
are still many unexplored issues that limit the development and application of
DICD, such as specific dataset building, engineering design related feature
engineering, systematic methods and techniques for DICD implementation in the
entire product design process, etc. In this regard, a systematic and operable
road map for DICD implementation from full-process perspective is established,
including a general workflow for DICD project planning, an overall framework
for DICD project implementation, the computing mechanisms for DICD
implementation, key enabling technologies for detailed DICD implementation, and
three application scenarios of DICD. The road map reveals the common mechanisms
and calculation principles of existing DICD researches, and thus it can provide
systematic guidance for the possible DICD applications that have not been
explored.",None,-1
4299e831-a6ac-4f27-ba58-5dcdfd21408e,Scalable 3D Captioning with Pretrained Models,0.902648,79,"We introduce Cap3D, an automatic approach for generating descriptive text for
3D objects. This approach utilizes pretrained models from image captioning,
image-text alignment, and LLM to consolidate captions from multiple views of a
3D asset, completely side-stepping the time-consuming and costly process of
manual annotation. We apply Cap3D to the recently introduced large-scale 3D
dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted
using 41k human annotations from the same dataset, demonstrates that Cap3D
surpasses human-authored descriptions in terms of quality, cost, and speed.
Through effective prompt engineering, Cap3D rivals human performance in
generating geometric descriptions on 17k collected annotations from the ABO
dataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,
and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,
and DreamFusion.",None,-1
53cea965-3fa4-4940-b824-83d133463aab,Embedding-based Retrieval with LLM for Effective Agriculture Information Extracting from Unstructured Data,0.99998,11,"Pest identification is a crucial aspect of pest control in agriculture.
However, most farmers are not capable of accurately identifying pests in the
field, and there is a limited number of structured data sources available for
rapid querying. In this work, we explored using domain-agnostic general
pre-trained large language model(LLM) to extract structured data from
agricultural documents with minimal or no human intervention. We propose a
methodology that involves text retrieval and filtering using embedding-based
retrieval, followed by LLM question-answering to automatically extract entities
and attributes from the documents, and transform them into structured data. In
comparison to existing methods, our approach achieves consistently better
accuracy in the benchmark while maintaining efficiency.",None,-1
b40537e0-4d82-4473-8dde-96a6a52bce25,Robust Unsupervised StyleGAN Image Restoration,0.683638,11,"GAN-based image restoration inverts the generative process to repair images
corrupted by known degradations. Existing unsupervised methods must be
carefully tuned for each task and degradation level. In this work, we make
StyleGAN image restoration robust: a single set of hyperparameters works across
a wide range of degradation levels. This makes it possible to handle
combinations of several degradations, without the need to retune. Our proposed
approach relies on a 3-phase progressive latent space extension and a
conservative optimizer, which avoids the need for any additional regularization
terms. Extensive experiments demonstrate robustness on inpainting, upsampling,
denoising, and deartifacting at varying degradations levels, outperforming
other StyleGAN-based inversion techniques. Our approach also favorably compares
to diffusion-based restoration by yielding much more realistic inversion
results. Code is available at https://lvsn.github.io/RobustUnsupervised/.",None,-1
4c5c48ab-aad2-4d80-8c22-60280633b8d8,Future Lens: Anticipating Subsequent Tokens from a Single Hidden State,0.901025,25,"We conjecture that hidden state vectors corresponding to individual input
tokens encode information sufficient to accurately predict several tokens
ahead. More concretely, in this paper we ask: Given a hidden (internal)
representation of a single token at position $t$ in an input, can we reliably
anticipate the tokens that will appear at positions $\geq t + 2$? To test this,
we measure linear approximation and causal intervention methods in GPT-J-6B to
evaluate the degree to which individual hidden states in the network contain
signal rich enough to predict future hidden states and, ultimately, token
outputs. We find that, at some layers, we can approximate a model's output with
more than 48% accuracy with respect to its prediction of subsequent tokens
through a single hidden state. Finally we present a ""Future Lens"" visualization
that uses these methods to create a new view of transformer states.",None,-1
f7850965-402e-4ce9-8250-2d505c337918,EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding,0.379022,1,"With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI),
large-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed.
However, most current research is built on resources derived from third-person
video action recognition. This inherent domain gap between first- and
third-person action videos, which have not been adequately addressed before,
makes current Ego-HOI suboptimal. This paper rethinks and proposes a new
framework as an infrastructure to advance Ego-HOI recognition by Probing,
Curation and Adaption (EgoPCA). We contribute comprehensive pre-train sets,
balanced test sets and a new baseline, which are complete with a
training-finetuning strategy. With our new framework, we not only achieve
state-of-the-art performance on Ego-HOI benchmarks but also build several new
and effective mechanisms and settings to advance further research. We believe
our data and the findings will pave a new way for Ego-HOI understanding. Code
and data are available at https://mvig-rhos.com/ego_pca",None,-1
e30921b6-a296-4115-952f-a02804e7059b,OO-dMVMT: A Deep Multi-view Multi-task Classification Framework for Real-time 3D Hand Gesture Classification and Segmentation,0.561454,2,"Continuous mid-air hand gesture recognition based on captured hand pose
streams is fundamental for human-computer interaction, particularly in AR / VR.
However, many of the methods proposed to recognize heterogeneous hand gestures
are tested only on the classification task, and the real-time low-latency
gesture segmentation in a continuous stream is not well addressed in the
literature. For this task, we propose the On-Off deep Multi-View Multi-Task
paradigm (OO-dMVMT). The idea is to exploit multiple time-local views related
to hand pose and movement to generate rich gesture descriptions, along with
using heterogeneous tasks to achieve high accuracy. OO-dMVMT extends the
classical MVMT paradigm, where all of the multiple tasks have to be active at
each time, by allowing specific tasks to switch on/off depending on whether
they can apply to the input. We show that OO-dMVMT defines the new SotA on
continuous/online 3D skeleton-based gesture recognition in terms of gesture
classification accuracy, segmentation accuracy, false positives, and decision
latency while maintaining real-time operation.",None,-1
4aad606d-3923-41ec-898e-8e6c5c2a36aa,ChiroDiff: Modelling chirographic data with Diffusion Models,0.343674,4,"Generative modelling over continuous-time geometric constructs, a.k.a such as
handwriting, sketches, drawings etc., have been accomplished through
autoregressive distributions. Such strictly-ordered discrete factorization
however falls short of capturing key properties of chirographic data -- it
fails to build holistic understanding of the temporal concept due to one-way
visibility (causality). Consequently, temporal data has been modelled as
discrete token sequences of fixed sampling rate instead of capturing the true
underlying concept. In this paper, we introduce a powerful model-class namely
""Denoising Diffusion Probabilistic Models"" or DDPMs for chirographic data that
specifically addresses these flaws. Our model named ""ChiroDiff"", being
non-autoregressive, learns to capture holistic concepts and therefore remains
resilient to higher temporal sampling rate up to a good extent. Moreover, we
show that many important downstream utilities (e.g. conditional sampling,
creative mixing) can be flexibly implemented using ChiroDiff. We further show
some unique use-cases like stochastic vectorization, de-noising/healing,
abstraction are also possible with this model-class. We perform quantitative
and qualitative evaluation of our framework on relevant datasets and found it
to be better or on par with competing approaches.",None,-1
10f1783b-3664-42c0-a865-921eaa5e1e26,Diversity-Measurable Anomaly Detection,0.802379,20,"Reconstruction-based anomaly detection models achieve their purpose by
suppressing the generalization ability for anomaly. However, diverse normal
patterns are consequently not well reconstructed as well. Although some efforts
have been made to alleviate this problem by modeling sample diversity, they
suffer from shortcut learning due to undesired transmission of abnormal
information. In this paper, to better handle the tradeoff problem, we propose
Diversity-Measurable Anomaly Detection (DMAD) framework to enhance
reconstruction diversity while avoid the undesired generalization on anomalies.
To this end, we design Pyramid Deformation Module (PDM), which models diverse
normals and measures the severity of anomaly by estimating multi-scale
deformation fields from reconstructed reference to original input. Integrated
with an information compression module, PDM essentially decouples deformation
from prototypical embedding and makes the final anomaly score more reliable.
Experimental results on both surveillance videos and industrial images
demonstrate the effectiveness of our method. In addition, DMAD works equally
well in front of contaminated data and anomaly-like normal samples.",None,-1
3cc9358c-be8c-46fa-9ce0-fed3e151f358,DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation,0.482733,8,"Empathy is a crucial factor in open-domain conversations, which naturally
shows one's caring and understanding to others. Though several methods have
been proposed to generate empathetic responses, existing works often lead to
monotonous empathy that refers to generic and safe expressions. In this paper,
we propose to use explicit control to guide the empathy expression and design a
framework DiffusEmp based on conditional diffusion language model to unify the
utilization of dialogue context and attribute-oriented control signals.
Specifically, communication mechanism, intent, and semantic frame are imported
as multi-grained signals that control the empathy realization from coarse to
fine levels. We then design a specific masking strategy to reflect the
relationship between multi-grained signals and response tokens, and integrate
it into the diffusion model to influence the generative process. Experimental
results on a benchmark dataset EmpatheticDialogue show that our framework
outperforms competitive baselines in terms of controllability, informativeness,
and diversity without the loss of context-relatedness.",None,-1
0484ac22-57d4-448a-afe1-6764b88178a9,DocILE 2023 Teaser: Document Information Localization and Extraction,0.38421,2,"The lack of data for information extraction (IE) from semi-structured
business documents is a real problem for the IE community. Publications relying
on large-scale datasets use only proprietary, unpublished data due to the
sensitive nature of such documents. Publicly available datasets are mostly
small and domain-specific. The absence of a large-scale public dataset or
benchmark hinders the reproducibility and cross-evaluation of published
methods. The DocILE 2023 competition, hosted as a lab at the CLEF 2023
conference and as an ICDAR 2023 competition, will run the first major benchmark
for the tasks of Key Information Localization and Extraction (KILE) and Line
Item Recognition (LIR) from business documents. With thousands of annotated
real documents from open sources, a hundred thousand of generated synthetic
documents, and nearly a million unlabeled documents, the DocILE lab comes with
the largest publicly available dataset for KILE and LIR. We are looking forward
to contributions from the Computer Vision, Natural Language Processing,
Information Retrieval, and other communities. The data, baselines, code and
up-to-date information about the lab and competition are available at
https://docile.rossum.ai/.",None,-1
8ac80a80-605a-45a5-bb95-1a236cbba150,A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models,0.996524,69,"We provide a theoretical justification for sample recovery using diffusion
based image inpainting in a linear model setting. While most inpainting
algorithms require retraining with each new mask, we prove that diffusion based
inpainting generalizes well to unseen masks without retraining. We analyze a
recently proposed popular diffusion based inpainting algorithm called RePaint
(Lugmayr et al., 2022), and show that it has a bias due to misalignment that
hampers sample recovery even in a two-state diffusion process. Motivated by our
analysis, we propose a modified RePaint algorithm we call RePaint$^+$ that
provably recovers the underlying true sample and enjoys a linear rate of
convergence. It achieves this by rectifying the misalignment error present in
drift and dispersion of the reverse process. To the best of our knowledge, this
is the first linear convergence result for a diffusion based image inpainting
algorithm.",None,-1
9d94a2c8-390b-4298-8a15-64d83dd87225,RLTF: Reinforcement Learning from Unit Test Feedback,0.96901,13,"The goal of program synthesis, or code generation, is to generate executable
code based on given descriptions. Recently, there has been an increasing number
of studies employing reinforcement learning (RL) to improve the performance of
large language models (LLMs) for code. However, current representative works
either rely solely on offline frameworks, limiting the exploration of new
sample spaces, or fall short in the utilization of unit test signals, not
accounting for specific error locations within the code. To address these
issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,
a novel online RL framework with unit test feedback of multi-granularity for
refining code LLMs. Our approach generates data in real-time during training
and simultaneously utilizes fine-grained feedback signals to guide the model
towards producing higher-quality code. Extensive experiments show that RLTF
achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our
code is available at: https://github.com/Zyq-scut/RLTF.",None,-1
cb37aa8c-293e-42dd-bc3d-843e1965ed3f,Emotionally Enhanced Talking Face Generation,0.609572,8,"Several works have developed end-to-end pipelines for generating lip-synced
talking faces with various real-world applications, such as teaching and
language translation in videos. However, these prior works fail to create
realistic-looking videos since they focus little on people's expressions and
emotions. Moreover, these methods' effectiveness largely depends on the faces
in the training dataset, which means they may not perform well on unseen faces.
To mitigate this, we build a talking face generation framework conditioned on a
categorical emotion to generate videos with appropriate expressions, making
them more realistic and convincing. With a broad range of six emotions, i.e.,
\emph{happiness}, \emph{sadness}, \emph{fear}, \emph{anger}, \emph{disgust},
and \emph{neutral}, we show that our model can adapt to arbitrary identities,
emotions, and languages. Our proposed framework is equipped with a
user-friendly web interface with a real-time experience for talking face
generation with emotions. We also conduct a user study for subjective
evaluation of our interface's usability, design, and functionality. Project
page: https://midas.iiitd.edu.in/emo/",None,-1
2a02d31a-32a4-4aac-ae1f-d3cb28569ab0,Detection of Mild Cognitive Impairment Using Facial Features in Video Conversations,0.131514,1,"Early detection of Mild Cognitive Impairment (MCI) leads to early
interventions to slow the progression from MCI into dementia. Deep Learning
(DL) algorithms could help achieve early non-invasive, low-cost detection of
MCI. This paper presents the detection of MCI in older adults using DL models
based only on facial features extracted from video-recorded conversations at
home. We used the data collected from the I-CONECT behavioral intervention
study (NCT02871921), where several sessions of semi-structured interviews
between socially isolated older individuals and interviewers were video
recorded. We develop a framework that extracts spatial holistic facial features
using a convolutional autoencoder and temporal information using transformers.
Our proposed DL model was able to detect the I-CONECT study participants'
cognitive conditions (MCI vs. those with normal cognition (NC)) using facial
features. The segments and sequence information of the facial features improved
the prediction performance compared with the non-temporal features. The
detection accuracy using this combined method reached 88% whereas 84% is the
accuracy without applying the segments and sequences information of the facial
features within a video on a certain theme.",None,-1
a594f957-112a-4dd4-87fd-cf9135c0023e,TransMatting: Tri-token Equipped Transformer Model for Image Matting,0.120544,1,"Image matting aims to predict alpha values of elaborate uncertainty areas of
natural images, like hairs, smoke, and spider web. However, existing methods
perform poorly when faced with highly transparent foreground objects due to the
large area of uncertainty to predict and the small receptive field of
convolutional networks. To address this issue, we propose a Transformer-based
network (TransMatting) to model transparent objects with long-range features
and collect a high-resolution matting dataset of transparent objects
(Transparent-460) for performance evaluation. Specifically, to utilize semantic
information in the trimap flexibly and effectively, we also redesign the trimap
as three learnable tokens, named tri-token. Both Transformer and convolution
matting models could benefit from our proposed tri-token design. By replacing
the traditional trimap concatenation strategy with our tri-token, existing
matting methods could achieve about 10% improvement in SAD and 20% in MSE.
Equipped with the new tri-token design, our proposed TransMatting outperforms
current state-of-the-art methods on several popular matting benchmarks and our
newly collected Transparent-460.",None,-1
27a4cdbe-96f3-4b43-8720-923edbbeb946,"DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering",0.649429,4,"Uncalibrated photometric stereo (UPS) is challenging due to the inherent
ambiguity brought by the unknown light. Although the ambiguity is alleviated on
non-Lambertian objects, the problem is still difficult to solve for more
general objects with complex shapes introducing irregular shadows and general
materials with complex reflectance like anisotropic reflectance. To exploit
cues from shadow and reflectance to solve UPS and improve performance on
general materials, we propose DANI-Net, an inverse rendering framework with
differentiable shadow handling and anisotropic reflectance modeling. Unlike
most previous methods that use non-differentiable shadow maps and assume
isotropic material, our network benefits from cues of shadow and anisotropic
reflectance through two differentiable paths. Experiments on multiple
real-world datasets demonstrate our superior and robust performance.",None,-1
d624f23d-7deb-4c45-a244-3626ef53bdc7,InstructDiffusion: A Generalist Modeling Interface for Vision Tasks,0.859959,36,"We present InstructDiffusion, a unifying and generic framework for aligning
computer vision tasks with human instructions. Unlike existing approaches that
integrate prior knowledge and pre-define the output space (e.g., categories and
coordinates) for each vision task, we cast diverse vision tasks into a
human-intuitive image-manipulating process whose output space is a flexible and
interactive pixel space. Concretely, the model is built upon the diffusion
process and is trained to predict pixels according to user instructions, such
as encircling the man's left shoulder in red or applying a blue mask to the
left car. InstructDiffusion could handle a variety of vision tasks, including
understanding tasks (such as segmentation and keypoint detection) and
generative tasks (such as editing and enhancement). It even exhibits the
ability to handle unseen tasks and outperforms prior methods on novel datasets.
This represents a significant step towards a generalist modeling interface for
vision tasks, advancing artificial general intelligence in the field of
computer vision.",None,-1
a5bbbb29-267a-4a30-a087-88f1efd4e927,TempT: Temporal consistency for Test-time adaptation,0.5716,3,"We introduce Temporal consistency for Test-time adaptation (TempT) a novel
method for test-time adaptation on videos through the use of temporal coherence
of predictions across sequential frames as a self-supervision signal. TempT is
an approach with broad potential applications in computer vision tasks
including facial expression recognition (FER) in videos. We evaluate TempT
performance on the AffWild2 dataset. Our approach focuses solely on the
unimodal visual aspect of the data and utilizes a popular 2D CNN backbone in
contrast to larger sequential or attention-based models used in other
approaches. Our preliminary experimental results demonstrate that TempT has
competitive performance compared to the previous years reported performances
and its efficacy provides a compelling proof-of-concept for its use in various
real-world applications.",None,-1
109a2cc1-7125-4382-97a3-7222ef867a71,PK-Chat: Pointer Network Guided Knowledge Driven Generative Dialogue Model,0.0826161,2,"In the research of end-to-end dialogue systems, using real-world knowledge to
generate natural, fluent, and human-like utterances with correct answers is
crucial. However, domain-specific conversational dialogue systems may be
incoherent and introduce erroneous external information to answer questions due
to the out-of-vocabulary issue or the wrong knowledge from the parameters of
the neural network. In this work, we propose PK-Chat, a Pointer network guided
Knowledge-driven generative dialogue model, incorporating a unified pretrained
language model and a pointer network over knowledge graphs. The words generated
by PK-Chat in the dialogue are derived from the prediction of word lists and
the direct prediction of the external knowledge graph knowledge. Moreover,
based on the PK-Chat, a dialogue system is built for academic scenarios in the
case of geosciences. Finally, an academic dialogue benchmark is constructed to
evaluate the quality of dialogue systems in academic scenarios and the source
code is available online.",None,-1
b88f11c9-094e-466a-a4cc-889727b053e9,Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature,0.824431,46,"Large language models (LLMs) have shown the ability to produce fluent and
cogent content, presenting both productivity opportunities and societal risks.
To build trustworthy AI systems, it is imperative to distinguish between
machine-generated and human-authored content. The leading zero-shot detector,
DetectGPT, showcases commendable performance but is marred by its intensive
computational costs. In this paper, we introduce the concept of conditional
probability curvature to elucidate discrepancies in word choices between LLMs
and humans within a given context. Utilizing this curvature as a foundational
metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which
substitutes DetectGPT's perturbation step with a more efficient sampling step.
Our evaluations on various datasets, source models, and test conditions
indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around
75% in both the white-box and black-box settings but also accelerates the
detection process by a factor of 340, as detailed in Table 1. See
\url{https://github.com/baoguangsheng/fast-detect-gpt} for code, data, and
results.",None,-1
1795a5fc-9759-4917-af40-c20968d164a6,Heterogeneous Forgetting Compensation for Class-Incremental Learning,0.833928,6,"Class-incremental learning (CIL) has achieved remarkable successes in
learning new classes consecutively while overcoming catastrophic forgetting on
old categories. However, most existing CIL methods unreasonably assume that all
old categories have the same forgetting pace, and neglect negative influence of
forgetting heterogeneity among different old classes on forgetting
compensation. To surmount the above challenges, we develop a novel
Heterogeneous Forgetting Compensation (HFC) model, which can resolve
heterogeneous forgetting of easy-to-forget and hard-to-forget old categories
from both representation and gradient aspects. Specifically, we design a
task-semantic aggregation block to alleviate heterogeneous forgetting from
representation aspect. It aggregates local category information within each
task to learn task-shared global representations. Moreover, we develop two
novel plug-and-play losses: a gradient-balanced forgetting compensation loss
and a gradient-balanced relation distillation loss to alleviate forgetting from
gradient aspect. They consider gradient-balanced compensation to rectify
forgetting heterogeneity of old categories and heterogeneous relation
consistency. Experiments on several representative datasets illustrate
effectiveness of our HFC model. The code is available at
https://github.com/JiahuaDong/HFC.",None,-1
b13f93a2-0598-4809-8854-a115d54e8d71,LaRS: A Diverse Panoptic Maritime Obstacle Detection Dataset and Benchmark,0.927786,6,"The progress in maritime obstacle detection is hindered by the lack of a
diverse dataset that adequately captures the complexity of general maritime
environments. We present the first maritime panoptic obstacle detection
benchmark LaRS, featuring scenes from Lakes, Rivers and Seas. Our major
contribution is the new dataset, which boasts the largest diversity in
recording locations, scene types, obstacle classes, and acquisition conditions
among the related datasets. LaRS is composed of over 4000 per-pixel labeled key
frames with nine preceding frames to allow utilization of the temporal texture,
amounting to over 40k frames. Each key frame is annotated with 8 thing, 3 stuff
classes and 19 global scene attributes. We report the results of 27 semantic
and panoptic segmentation methods, along with several performance insights and
future research directions. To enable objective evaluation, we have implemented
an online evaluation server. The LaRS dataset, evaluation toolkit and benchmark
are publicly available at: https://lojzezust.github.io/lars-dataset",None,-1
58246cc4-a523-4a6d-8bd6-5b84a386cc89,Chain of Thought Prompting Elicits Knowledge Augmentation,0.754477,15,"The knowledge-augmented deep learning paradigm refers to a paradigm in which
domain knowledge is identified and integrated into deep models. Conventional
methods typically employ task-specific approaches to gather external knowledge
from various sources. In contrast, large language models are extensively
pre-trained and can serve as a comprehensive source of external knowledge. In
this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments
knowledge for deep learning. CoT-KA avoids the need for additional knowledge
retrieval or knowledge reasoning models, as required in conventional
augmentation methods. Our results demonstrate that CoT-KA outperforms both pure
CoT-based methods and the non-augmented method across the majority of eleven
publicly available benchmarks for various reasoning tasks.",None,-1
08f552d8-438f-4654-8d8f-b43fcca3a834,SMPConv: Self-moving Point Representations for Continuous Convolution,0.758071,6,"Continuous convolution has recently gained prominence due to its ability to
handle irregularly sampled data and model long-term dependency. Also, the
promising experimental results of using large convolutional kernels have
catalyzed the development of continuous convolution since they can construct
large kernels very efficiently. Leveraging neural networks, more specifically
multilayer perceptrons (MLPs), is by far the most prevalent approach to
implementing continuous convolution. However, there are a few drawbacks, such
as high computational costs, complex hyperparameter tuning, and limited
descriptive power of filters. This paper suggests an alternative approach to
building a continuous convolution without neural networks, resulting in more
computationally efficient and improved performance. We present self-moving
point representations where weight parameters freely move, and interpolation
schemes are used to implement continuous functions. When applied to construct
convolutional kernels, the experimental results have shown improved performance
with drop-in replacement in the existing frameworks. Due to its lightweight
structure, we are first to demonstrate the effectiveness of continuous
convolution in a large-scale setting, e.g., ImageNet, presenting the
improvements over the prior arts. Our code is available on
https://github.com/sangnekim/SMPConv",None,-1
3c437a6e-b005-49d5-a640-c96bc97f882b,A Solution to Co-occurrence Bias: Attributes Disentanglement via Mutual Information Minimization for Pedestrian Attribute Recognition,0.354811,1,"Recent studies on pedestrian attribute recognition progress with either
explicit or implicit modeling of the co-occurrence among attributes.
Considering that this known a prior is highly variable and unforeseeable
regarding the specific scenarios, we show that current methods can actually
suffer in generalizing such fitted attributes interdependencies onto scenes or
identities off the dataset distribution, resulting in the underlined bias of
attributes co-occurrence. To render models robust in realistic scenes, we
propose the attributes-disentangled feature learning to ensure the recognition
of an attribute not inferring on the existence of others, and which is
sequentially formulated as a problem of mutual information minimization.
Rooting from it, practical strategies are devised to efficiently decouple
attributes, which substantially improve the baseline and establish
state-of-the-art performance on realistic datasets like PETAzs and RAPzs. Code
is released on
https://github.com/SDret/A-Solution-to-Co-occurence-Bias-in-Pedestrian-Attribute-Recognition.",None,-1
4b7d1b2b-f5b1-4760-a28b-1c7490d31fc7,Achieving Long-term Fairness in Submodular Maximization through Randomization,0.290776,4,"Submodular function optimization has numerous applications in machine
learning and data analysis, including data summarization which aims to identify
a concise and diverse set of data points from a large dataset. It is important
to implement fairness-aware algorithms when dealing with data items that may
contain sensitive attributes like race or gender, to prevent biases that could
lead to unequal representation of different groups. With this in mind, we
investigate the problem of maximizing a monotone submodular function while
meeting group fairness constraints. Unlike previous studies in this area, we
allow for randomized solutions, with the objective being to calculate a
distribution over feasible sets such that the expected number of items selected
from each group is subject to constraints in the form of upper and lower
thresholds, ensuring that the representation of each group remains balanced in
the long term. Here a set is considered feasible if its size does not exceed a
constant value of $b$. Our research includes the development of a series of
approximation algorithms for this problem.",None,-1
1c21cb8b-53fc-44ae-979d-4cc0f07c5f50,Sem-CS: Semantic CLIPStyler for Text-Based Image Style Transfer,0.0920407,1,"CLIPStyler demonstrated image style transfer with realistic textures using
only a style text description (instead of requiring a reference style image).
However, the ground semantics of objects in the style transfer output is lost
due to style spill-over on salient and background objects (content mismatch) or
over-stylization. To solve this, we propose Semantic CLIPStyler (Sem-CS), that
performs semantic style transfer. Sem-CS first segments the content image into
salient and non-salient objects and then transfers artistic style based on a
given style text description. The semantic style transfer is achieved using
global foreground loss (for salient objects) and global background loss (for
non-salient objects). Our empirical results, including DISTS, NIMA and user
study scores, show that our proposed framework yields superior qualitative and
quantitative performance. Our code is available at
github.com/chandagrover/sem-cs.",None,-1
8940aa1c-0984-4d76-8304-8bf6c2a9d2e4,Large Language Models Can Be Good Privacy Protection Learners,0.810937,14,"The proliferation of Large Language Models (LLMs) has driven considerable
interest in fine-tuning them with domain-specific data to create specialized
language models. Nevertheless, such domain-specific fine-tuning data often
contains sensitive personally identifiable information (PII). Direct
fine-tuning LLMs on this data without privacy protection poses a risk of
leakage. To address this challenge, we introduce Privacy Protection Language
Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects
domain-specific knowledge while safeguarding data privacy. Our work offers a
theoretical analysis for model design and delves into various techniques such
as corpus curation, penalty-based unlikelihood in training loss, and
instruction-based tuning, etc. Extensive experiments across diverse datasets
and scenarios demonstrate the effectiveness of our approaches. In particular,
instruction tuning with both positive and negative examples, stands out as a
promising method, effectively protecting private data while enhancing the
model's knowledge. Our work underscores the potential for Large Language Models
as robust privacy protection learners.",None,-1
0947a237-adaa-4756-bad1-52080051c7c5,Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning,0.322491,2,"We present a novel Diffusion Offline Multi-agent Model (DOM2) for offline
Multi-Agent Reinforcement Learning (MARL). Different from existing algorithms
that rely mainly on conservatism in policy design, DOM2 enhances policy
expressiveness and diversity based on diffusion. Specifically, we incorporate a
diffusion model into the policy network and propose a trajectory-based
data-augmentation scheme in training. These key ingredients make our algorithm
more robust to environment changes and achieve significant improvements in
performance, generalization and data-efficiency. Our extensive experimental
results demonstrate that DOM2 outperforms existing state-of-the-art methods in
multi-agent particle and multi-agent MuJoCo environments, and generalizes
significantly better in shifted environments thanks to its high expressiveness
and diversity. Furthermore, DOM2 shows superior data efficiency and can achieve
state-of-the-art performance with $20+$ times less data compared to existing
algorithms.",None,-1
df759dd6-1aa5-4f18-9dd6-b86e2feccc0d,DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles,0.434178,3,"We propose a new object-centric video prediction algorithm based on the deep
latent particle (DLP) representation. In comparison to existing slot- or
patch-based representations, DLPs model the scene using a set of keypoints with
learned parameters for properties such as position and size, and are both
efficient and interpretable. Our method, deep dynamic latent particles (DDLP),
yields state-of-the-art object-centric video prediction results on several
challenging datasets. The interpretable nature of DDLP allows us to perform
``what-if'' generation -- predict the consequence of changing properties of
objects in the initial frames, and DLP's compact structure enables efficient
diffusion-based unconditional video generation. Videos, code and pre-trained
models are available: https://taldatech.github.io/ddlp-web",None,-1
31c7b081-2191-4157-8872-8c6015e46bd3,Predictive Heterogeneity: Measures and Applications,0.0716784,1,"As an intrinsic and fundamental property of big data, data heterogeneity
exists in a variety of real-world applications, such as precision medicine,
autonomous driving, financial applications, etc. For machine learning
algorithms, the ignorance of data heterogeneity will greatly hurt the
generalization performance and the algorithmic fairness, since the prediction
mechanisms among different sub-populations are likely to differ from each
other. In this work, we focus on the data heterogeneity that affects the
prediction of machine learning models, and firstly propose the \emph{usable
predictive heterogeneity}, which takes into account the model capacity and
computational constraints. We prove that it can be reliably estimated from
finite data with probably approximately correct (PAC) bounds. Additionally, we
design a bi-level optimization algorithm to explore the usable predictive
heterogeneity from data. Empirically, the explored heterogeneity provides
insights for sub-population divisions in income prediction, crop yield
prediction and image classification tasks, and leveraging such heterogeneity
benefits the out-of-distribution generalization performance.",None,-1
4b6c4797-b381-4612-bab2-9252dac46be7,Cross-lingual Knowledge Transfer and Iterative Pseudo-labeling for Low-Resource Speech Recognition with Transducers,0.0938948,1,"Voice technology has become ubiquitous recently. However, the accuracy, and
hence experience, in different languages varies significantly, which makes the
technology not equally inclusive. The availability of data for different
languages is one of the key factors affecting accuracy, especially in training
of all-neural end-to-end automatic speech recognition systems.
  Cross-lingual knowledge transfer and iterative pseudo-labeling are two
techniques that have been shown to be successful for improving the accuracy of
ASR systems, in particular for low-resource languages, like Ukrainian.
  Our goal is to train an all-neural Transducer-based ASR system to replace a
DNN-HMM hybrid system with no manually annotated training data. We show that
the Transducer system trained using transcripts produced by the hybrid system
achieves 18% reduction in terms of word error rate. However, using a
combination of cross-lingual knowledge transfer from related languages and
iterative pseudo-labeling, we are able to achieve 35% reduction of the error
rate.",None,-1
cdf4ca53-e8ec-4da4-bc25-0217dc24a71e,Visualizing Semiotics in Generative Adversarial Networks,0.0699641,1,"We perform a set of experiments to demonstrate that images generated using a
Generative Adversarial Network can be modified using 'semiotics.' We show that
just as physical attributes such as the hue and saturation of an image can be
modified, so too can its non-physical, abstract properties using our method.
For example, the design of a flight attendant's uniform may be modified to look
more 'alert,' less 'austere,' or more 'practical.' The form of a house can be
modified to appear more 'futuristic,' a car more 'friendly' a pair of sneakers,
'evil.' Our method uncovers latent visual iconography associated with the
semiotic property of interest, enabling a process of visual form-finding using
abstract concepts. Our approach is iterative and allows control over the degree
of attribute presence and can be used to aid the design process to yield
emergent visual concepts.",None,-1
9314641c-3813-4e46-b569-dbf444b4185d,Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism,0.323201,5,"Transformer-based Large Language Models (LLMs) are the state-of-the-art for
natural language tasks. Recent work has attempted to decode, by reverse
engineering the role of linear layers, the internal mechanisms by which LLMs
arrive at their final predictions for text completion tasks. Yet little is
known about the specific role of attention heads in producing the final token
prediction. We propose Attention Lens, a tool that enables researchers to
translate the outputs of attention heads into vocabulary tokens via learned
attention-head-specific transformations called lenses. Preliminary findings
from our trained lenses indicate that attention heads play highly specialized
roles in language models. The code for Attention Lens is available at
github.com/msakarvadia/AttentionLens.",None,-1
62ecd736-7cef-43dd-acb0-e5ae8956fb12,Content-based Unrestricted Adversarial Attack,0.859301,22,"Unrestricted adversarial attacks typically manipulate the semantic content of
an image (e.g., color or texture) to create adversarial examples that are both
effective and photorealistic, demonstrating their ability to deceive human
perception and deep neural networks with stealth and success. However, current
works usually sacrifice unrestricted degrees and subjectively select some image
content to guarantee the photorealism of unrestricted adversarial examples,
which limits its attack performance. To ensure the photorealism of adversarial
examples and boost attack performance, we propose a novel unrestricted attack
framework called Content-based Unrestricted Adversarial Attack. By leveraging a
low-dimensional manifold that represents natural images, we map the images onto
the manifold and optimize them along its adversarial direction. Therefore,
within this framework, we implement Adversarial Content Attack based on Stable
Diffusion and can generate high transferable unrestricted adversarial examples
with various adversarial contents. Extensive experimentation and visualization
demonstrate the efficacy of ACA, particularly in surpassing state-of-the-art
attacks by an average of 13.3-50.4% and 16.8-48.0% in normally trained models
and defense methods, respectively.",None,-1
baee610c-6fcd-43cf-ba97-582e75d0a995,Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games,0.663078,5,"We introduce a class of networked Markov potential games in which agents are
associated with nodes in a network. Each agent has its own local potential
function, and the reward of each agent depends only on the states and actions
of the agents within a neighborhood. In this context, we propose a localized
actor-critic algorithm. The algorithm is scalable since each agent uses only
local information and does not need access to the global state. Further, the
algorithm overcomes the curse of dimensionality through the use of function
approximation. Our main results provide finite-sample guarantees up to a
localization error and a function approximation error. Specifically, we achieve
an $\tilde{\mathcal{O}}(\tilde{\epsilon}^{-4})$ sample complexity measured by
the averaged Nash regret. This is the first finite-sample bound for multi-agent
competitive games that does not depend on the number of agents.",None,-1
076a2a00-e95d-4b46-b482-36ea803283a2,Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models,0.0858629,2,"Large Language Models (LLMs) have significantly advanced the field of Natural
Language Processing (NLP), but their lack of interpretability has been a major
concern. Current methods for interpreting LLMs are post hoc, applied after
inference time, and have limitations such as their focus on low-level features
and lack of explainability at higher level text units. In this work, we
introduce proto-lm, a prototypical network-based white-box framework that
allows LLMs to learn immediately interpretable embeddings during the
fine-tuning stage while maintaining competitive performance. Our method's
applicability and interpretability are demonstrated through experiments on a
wide range of NLP tasks, and our results indicate a new possibility of creating
interpretable models without sacrificing performance. This novel approach to
interpretability in LLMs can pave the way for more interpretable models without
the need to sacrifice performance.",None,-1
796b66dc-65b9-44c9-b328-d4a455eb1617,Knowledge Engineering using Large Language Models,0.684334,8,"Knowledge engineering is a discipline that focuses on the creation and
maintenance of processes that generate and apply knowledge. Traditionally,
knowledge engineering approaches have focused on knowledge expressed in formal
languages. The emergence of large language models and their capabilities to
effectively work with natural language, in its broadest sense, raises questions
about the foundations and practice of knowledge engineering. Here, we outline
the potential role of LLMs in knowledge engineering, identifying two central
directions: 1) creating hybrid neuro-symbolic knowledge systems; and 2)
enabling knowledge engineering in natural language. Additionally, we formulate
key open research questions to tackle these directions.",None,-1
0a13adbe-f72e-448e-885b-ea25c3e439f9,Enhancing Keyphrase Generation by BART Finetuning with Splitting and Shuffling,0.0350804,2,"Keyphrase generation is a task of identifying a set of phrases that best
repre-sent the main topics or themes of a given text. Keyphrases are dividend
int pre-sent and absent keyphrases. Recent approaches utilizing
sequence-to-sequence models show effectiveness on absent keyphrase generation.
However, the per-formance is still limited due to the hardness of finding
absent keyphrases. In this paper, we propose Keyphrase-Focused BART, which
exploits the differ-ences between present and absent keyphrase generations, and
performs fine-tuning of two separate BART models for present and absent
keyphrases. We further show effective approaches of shuffling keyphrases and
candidate keyphrase ranking. For absent keyphrases, our Keyphrase-Focused BART
achieved new state-of-the-art score on F1@5 in two out of five keyphrase
gen-eration benchmark datasets.",None,-1
80c17c1e-7cbf-41a9-bfb0-9de4f9e148d6,NeuralKG-ind: A Python Library for Inductive Knowledge Graph Representation Learning,0.224397,1,"Since the dynamic characteristics of knowledge graphs, many inductive
knowledge graph representation learning (KGRL) works have been proposed in
recent years, focusing on enabling prediction over new entities. NeuralKG-ind
is the first library of inductive KGRL as an important update of NeuralKG
library. It includes standardized processes, rich existing methods, decoupled
modules, and comprehensive evaluation metrics. With NeuralKG-ind, it is easy
for researchers and engineers to reproduce, redevelop, and compare inductive
KGRL methods. The library, experimental methodologies, and model
re-implementing results of NeuralKG-ind are all publicly released at
https://github.com/zjukg/NeuralKG/tree/ind .",None,-1
02a5471a-de7c-4d25-8364-7ce57bdc0d5f,Shadow Generation with Decomposed Mask Prediction and Attentive Shadow Filling,0.0319283,2,"Image composition refers to inserting a foreground object into a background
image to obtain a composite image. In this work, we focus on generating
plausible shadows for the inserted foreground object to make the composite
image more realistic. To supplement the existing small-scale dataset, we create
a large-scale dataset called RdSOBA with rendering techniques. Moreover, we
design a two-stage network named DMASNet with decomposed mask prediction and
attentive shadow filling. Specifically, in the first stage, we decompose shadow
mask prediction into box prediction and shape prediction. In the second stage,
we attend to reference background shadow pixels to fill the foreground shadow.
Abundant experiments prove that our DMASNet achieves better visual effects and
generalizes well to real composite images.",None,-1
0e78f99a-b2ff-4677-be06-8909b7bc14a7,How Generalizable are Deepfake Detectors? An Empirical Study,0.113636,2,"Deepfake videos and images are becoming increasingly credible, posing a
significant threat given their potential to facilitate fraud or bypass access
control systems. This has motivated the development of deepfake detection
methods, in which deep learning models are trained to distinguish between real
and synthesized footage. Unfortunately, existing detection models struggle to
generalize to deepfakes from datasets they were not trained on, but little work
has been done to examine why or how this limitation can be addressed. In this
paper, we present the first empirical study on the generalizability of deepfake
detectors, an essential goal for detectors to stay one step ahead of attackers.
Our study utilizes six deepfake datasets, five deepfake detection methods, and
two model augmentation approaches, confirming that detectors do not generalize
in zero-shot settings. Additionally, we find that detectors are learning
unwanted properties specific to synthesis methods and struggling to extract
discriminative features, limiting their ability to generalize. Finally, we find
that there are neurons universally contributing to detection across seen and
unseen datasets, illuminating a possible path forward to zero-shot
generalizability.",None,-1
674fd692-d933-47da-897b-35d099193013,Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models,0.706474,10,"The mission of open knowledge graph (KG) completion is to draw new findings
from known facts. Existing works that augment KG completion require either (1)
factual triples to enlarge the graph reasoning space or (2) manually designed
prompts to extract knowledge from a pre-trained language model (PLM),
exhibiting limited performance and requiring expensive efforts from experts. To
this end, we propose TAGREAL that automatically generates quality query prompts
and retrieves support information from large text corpora to probe knowledge
from PLM for KG completion. The results show that TAGREAL achieves
state-of-the-art performance on two benchmark datasets. We find that TAGREAL
has superb performance even with limited training data, outperforming existing
embedding-based, graph-based, and PLM-based methods.",None,-1
237a20f9-c9dc-401c-b659-10937e87ef5a,When 3D Bounding-Box Meets SAM: Point Cloud Instance Segmentation with Weak-and-Noisy Supervision,0.628217,3,"Learning from bounding-boxes annotations has shown great potential in
weakly-supervised 3D point cloud instance segmentation. However, we observed
that existing methods would suffer severe performance degradation with
perturbed bounding box annotations. To tackle this issue, we propose a
complementary image prompt-induced weakly-supervised point cloud instance
segmentation (CIP-WPIS) method. CIP-WPIS leverages pretrained knowledge
embedded in the 2D foundation model SAM and 3D geometric prior to achieve
accurate point-wise instance labels from the bounding box annotations.
Specifically, CP-WPIS first selects image views in which 3D candidate points of
an instance are fully visible. Then, we generate complementary background and
foreground prompts from projections to obtain SAM 2D instance mask predictions.
According to these, we assign the confidence values to points indicating the
likelihood of points belonging to the instance. Furthermore, we utilize 3D
geometric homogeneity provided by superpoints to decide the final instance
label assignments. In this fashion, we achieve high-quality 3D point-wise
instance labels. Extensive experiments on both Scannet-v2 and S3DIS benchmarks
demonstrate that our method is robust against noisy 3D bounding-box annotations
and achieves state-of-the-art performance.",None,-1
38a96662-49eb-40b8-87f9-a560d46fec06,Referral Augmentation for Zero-Shot Information Retrieval,0.0697349,1,"We propose Referral-Augmented Retrieval (RAR), a simple technique that
concatenates document indices with referrals, i.e. text from other documents
that cite or link to the given document, to provide significant performance
gains for zero-shot information retrieval. The key insight behind our method is
that referrals provide a more complete, multi-view representation of a
document, much like incoming page links in algorithms like PageRank provide a
comprehensive idea of a webpage's importance. RAR works with both sparse and
dense retrievers, and outperforms generative text expansion techniques such as
DocT5Query and Query2Doc a 37% and 21% absolute improvement on ACL paper
retrieval Recall@10 -- while also eliminating expensive model training and
inference. We also analyze different methods for multi-referral aggregation and
show that RAR enables up-to-date information retrieval without re-training.",None,-1
040142c5-11ed-4c8f-9aaf-ba6219d5a9af,Chinese Spelling Correction as Rephrasing Language Model,0.75813,2,"This paper studies Chinese Spelling Correction (CSC), which aims to detect
and correct the potential spelling errors in a given sentence. Current
state-of-the-art methods regard CSC as a sequence tagging task and fine-tune
BERT-based models on sentence pairs. However, we note a critical flaw in the
process of tagging one character to another, that the correction is excessively
conditioned on the error. This is opposite from human mindset, where
individuals rephrase the complete sentence based on its semantics, rather than
solely on the error patterns memorized before. Such a counter-intuitive
learning process results in the bottleneck of generalizability and
transferability of machine spelling correction. To address this, we propose
Rephrasing Language Model (ReLM), where the model is trained to rephrase the
entire sentence by infilling additional slots, instead of
character-to-character tagging. This novel training paradigm achieves the new
state-of-the-art results across fine-tuned and zero-shot CSC benchmarks,
outperforming previous counterparts by a large margin. Our method also learns
transferable language representation when CSC is jointly trained with other
tasks.",None,-1
d1092b3c-bd69-4383-bcbd-87493effe708,BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text,0.930517,2,"Real-world NLP applications often deal with nonstandard text (e.g.,
dialectal, informal, or misspelled text). However, language models like BERT
deteriorate in the face of dialect variation or noise. How do we push BERT's
modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it
is designed for specializing a model to a task and does not seem to bring about
the deeper, more pervasive changes needed to adapt a model to nonstandard
language. In this paper, we introduce the novel idea of sandwiching BERT's
encoder stack between additional encoder layers trained to perform masked
language modeling on noisy text. We find that our approach, paired with recent
work on including character-level noise in fine-tuning data, can promote
zero-shot transfer to dialectal text, as well as reduce the distance in the
embedding space between words and their noisy counterparts.",None,-1
dffef0e1-cf71-4837-81c8-4c93be3b6de8,A Computational Approach to Style in American Poetry,0.29714,54,"We develop a quantitative method to assess the style of American poems and to
visualize a collection of poems in relation to one another. Qualitative poetry
criticism helped guide our development of metrics that analyze various
orthographic, syntactic, and phonemic features. These features are used to
discover comprehensive stylistic information from a poem's multi-layered latent
structure, and to compute distances between poems in this space. Visualizations
provide ready access to the analytical components. We demonstrate our method on
several collections of poetry, showing that it better delineates poetry style
than the traditional word-occurrence features that are used in typical text
analysis algorithms. Our method has potential applications to academic research
of texts, to research of the intuitive personal response to poetry, and to
making recommendations to readers based on their favorite poems.",None,-1
3ed0176e-ba29-4a80-abee-c37ec018ee9d,Learning Roles with Emergent Social Value Orientations,0.34892,3,"Social dilemmas can be considered situations where individual rationality
leads to collective irrationality. The multi-agent reinforcement learning
community has leveraged ideas from social science, such as social value
orientations (SVO), to solve social dilemmas in complex cooperative tasks. In
this paper, by first introducing the typical ""division of labor or roles""
mechanism in human society, we provide a promising solution for intertemporal
social dilemmas (ISD) with SVOs. A novel learning framework, called Learning
Roles with Emergent SVOs (RESVO), is proposed to transform the learning of
roles into the social value orientation emergence, which is symmetrically
solved by endowing agents with altruism to share rewards with other agents. An
SVO-based role embedding space is then constructed by individual conditioning
policies on roles with a novel rank regularizer and mutual information
maximizer. Experiments show that RESVO achieves a stable division of labor and
cooperation in ISDs with different complexity.",None,-1
79a86421-e679-4886-84c5-7f751e454219,Path To Gain Functional Transparency In Artificial Intelligence With Meaningful Explainability,0.130656,1,"Artificial Intelligence (AI) is rapidly integrating into various aspects of
our daily lives, influencing decision-making processes in areas such as
targeted advertising and matchmaking algorithms. As AI systems become
increasingly sophisticated, ensuring their transparency and explainability
becomes crucial. Functional transparency is a fundamental aspect of algorithmic
decision-making systems, allowing stakeholders to comprehend the inner workings
of these systems and enabling them to evaluate their fairness and accuracy.
However, achieving functional transparency poses significant challenges that
need to be addressed. In this paper, we propose a design for user-centered
compliant-by-design transparency in transparent systems. We emphasize that the
development of transparent and explainable AI systems is a complex and
multidisciplinary endeavor, necessitating collaboration among researchers from
diverse fields such as computer science, artificial intelligence, ethics, law,
and social science. By providing a comprehensive understanding of the
challenges associated with transparency in AI systems and proposing a
user-centered design framework, we aim to facilitate the development of AI
systems that are accountable, trustworthy, and aligned with societal values.",None,-1
612ce342-7007-46b3-93d2-fddd113b3e9c,PanoDiffusion: 360-degree Panorama Outpainting via Diffusion,0.633742,4,"Generating complete 360-degree panoramas from narrow field of view images is
ongoing research as omnidirectional RGB data is not readily available. Existing
GAN-based approaches face some barriers to achieving higher quality output, and
have poor generalization performance over different mask types. In this paper,
we present our 360-degree indoor RGB-D panorama outpainting model using latent
diffusion models (LDM), called PanoDiffusion. We introduce a new bi-modal
latent diffusion structure that utilizes both RGB and depth panoramic data
during training, which works surprisingly well to outpaint depth-free RGB
images during inference. We further propose a novel technique of introducing
progressive camera rotations during each diffusion denoising step, which leads
to substantial improvement in achieving panorama wraparound consistency.
Results show that our PanoDiffusion not only significantly outperforms
state-of-the-art methods on RGB-D panorama outpainting by producing diverse
well-structured results for different types of masks, but can also synthesize
high-quality depth panoramas to provide realistic 3D indoor models.",None,-1
a10757f9-02ce-4e97-bf02-1f30c4e52108,Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression,0.122281,1,"This paper proposes a novel framework for accelerating support vector
clustering. The proposed method first computes much smaller compressed data
sets while preserving the key cluster properties of the original data sets
based on a novel spectral data compression approach. Then, the resultant
spectrally-compressed data sets are leveraged for the development of fast and
high quality algorithm for support vector clustering. We conducted extensive
experiments using real-world data sets and obtained very promising results. The
proposed method allows us to achieve 100X and 115X speedups over the state of
the art SVC method on the Pendigits and USPS data sets, respectively, while
achieving even better clustering quality. To the best of our knowledge, this
represents the first practical method for high-quality and fast SVC on
large-scale real-world data sets",None,-1
72f43b3b-fbd4-4144-a468-7d84886aa42f,Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution,0.916413,26,"Although numerous solutions have been proposed for image super-resolution,
they are usually incompatible with low-power devices with many computational
and memory constraints. In this paper, we address this problem by proposing a
simple yet effective deep network to solve image super-resolution efficiently.
In detail, we develop a spatially-adaptive feature modulation (SAFM) mechanism
upon a vision transformer (ViT)-like block. Within it, we first apply the SAFM
block over input features to dynamically select representative feature
representations. As the SAFM block processes the input features from a
long-range perspective, we further introduce a convolutional channel mixer
(CCM) to simultaneously extract local contextual information and perform
channel mixing. Extensive experimental results show that the proposed method is
$3\times$ smaller than state-of-the-art efficient SR methods, e.g., IMDN, in
terms of the network parameters and requires less computational cost while
achieving comparable performance. The code is available at
https://github.com/sunny2109/SAFMN.",None,-1
bb7eb96e-0d40-4bd5-b391-eecee81c4d79,The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and POS,1.0,7,"Automatic analysis for modern Chinese has greatly improved the accuracy of
text mining in related fields, but the study of ancient Chinese is still
relatively rare. Ancient text division and lexical annotation are important
parts of classical literature comprehension, and previous studies have tried to
construct auxiliary dictionary and other fused knowledge to improve the
performance. In this paper, we propose a framework for ancient Chinese Word
Segmentation and Part-of-Speech Tagging that makes a twofold effort: on the one
hand, we try to capture the wordhood semantics; on the other hand, we
re-predict the uncertain samples of baseline model by introducing external
knowledge. The performance of our architecture outperforms pre-trained BERT
with CRF and existing tools such as Jiayan.",None,-1
a3692e91-5a60-40d2-8622-31294fe1cb0c,Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?,0.339179,8,"Pretraining a neural network on a large dataset is becoming a cornerstone in
machine learning that is within the reach of only a few communities with
large-resources. We aim at an ambitious goal of democratizing pretraining.
Towards that goal, we train and release a single neural network that can
predict high quality ImageNet parameters of other neural networks. By using
predicted parameters for initialization we are able to boost training of
diverse ImageNet models available in PyTorch. When transferred to other
datasets, models initialized with predicted parameters also converge faster and
reach competitive final performance.",None,-1
5b6ccee2-b881-423f-80b1-b7e109e63c57,Learning To Teach Large Language Models Logical Reasoning,0.247565,6,"Large language models (LLMs) have gained enormous attention from both
academia and industry, due to their exceptional ability in language generation
and extremely powerful generalization. However, current LLMs still output
unreliable content in practical reasoning tasks due to their inherent issues
(e.g., hallucination). To better disentangle this problem, in this paper, we
conduct an in-depth investigation to systematically explore the capability of
LLMs in logical reasoning. More in detail, we first investigate the deficiency
of LLMs in logical reasoning on different tasks, including event relation
extraction and deductive reasoning. Our study demonstrates that LLMs are not
good reasoners in solving tasks with rigorous reasoning and will produce
counterfactual answers, which require us to iteratively refine. Therefore, we
comprehensively explore different strategies to endow LLMs with logical
reasoning ability, and thus enable them to generate more logically consistent
answers across different scenarios. Based on our approach, we also contribute a
synthesized dataset (LLM-LR) involving multi-hop reasoning for evaluation and
pre-training. Extensive quantitative and qualitative analyses on different
tasks also validate the effectiveness and necessity of teaching LLMs with logic
and provide insights for solving practical tasks with LLMs in future work.",None,-1
ff502610-eaf8-43a4-bc39-33c8a0dab485,Cooperative Open-ended Learning Framework for Zero-shot Coordination,0.735513,10,"Zero-shot coordination in cooperative artificial intelligence (AI) remains a
significant challenge, which means effectively coordinating with a wide range
of unseen partners. Previous algorithms have attempted to address this
challenge by optimizing fixed objectives within a population to improve
strategy or behaviour diversity. However, these approaches can result in a loss
of learning and an inability to cooperate with certain strategies within the
population, known as cooperative incompatibility. To address this issue, we
propose the Cooperative Open-ended LEarning (COLE) framework, which constructs
open-ended objectives in cooperative games with two players from the
perspective of graph theory to assess and identify the cooperative ability of
each strategy. We further specify the framework and propose a practical
algorithm that leverages knowledge from game theory and graph theory.
Furthermore, an analysis of the learning process of the algorithm shows that it
can efficiently overcome cooperative incompatibility. The experimental results
in the Overcooked game environment demonstrate that our method outperforms
current state-of-the-art methods when coordinating with different-level
partners. Our demo is available at https://sites.google.com/view/cole-2023.",None,-1
946fdc52-58b6-4156-8cd1-5e0ab3c4f809,Event Blob Tracking: An Asynchronous Real-Time Algorithm,0.439,2,"Event-based cameras have become increasingly popular for tracking fast-moving
objects due to their high temporal resolution, low latency, and high dynamic
range. In this paper, we propose a novel algorithm for tracking event blobs
using raw events asynchronously in real time. We introduce the concept of an
event blob as a spatio-temporal likelihood of event occurrence where the
conditional spatial likelihood is blob-like. Many real-world objects generate
event blob data, for example, flickering LEDs such as car headlights or any
small foreground object moving against a static or slowly varying background.
The proposed algorithm uses a nearest neighbour classifier with a dynamic
threshold criteria for data association coupled with a Kalman filter to track
the event blob state. Our algorithm achieves highly accurate tracking and event
blob shape estimation even under challenging lighting conditions and high-speed
motions. The microsecond time resolution achieved means that the filter output
can be used to derive secondary information such as time-to-contact or range
estimation, that will enable applications to real-world problems such as
collision avoidance in autonomous driving.",None,-1
cdbc7141-66c2-449d-ad32-74c31a289f55,Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations,0.709483,15,"The abundance of instructional videos and their narrations over the Internet
offers an exciting avenue for understanding procedural activities. In this
work, we propose to learn video representation that encodes both action steps
and their temporal ordering, based on a large-scale dataset of web
instructional videos and their narrations, without using human annotations. Our
method jointly learns a video representation to encode individual step
concepts, and a deep probabilistic model to capture both temporal dependencies
and immense individual variations in the step ordering. We empirically
demonstrate that learning temporal ordering not only enables new capabilities
for procedure reasoning, but also reinforces the recognition of individual
steps. Our model significantly advances the state-of-the-art results on step
classification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting
(+7.4% on COIN). Moreover, our model attains promising results in zero-shot
inference for step classification and forecasting, as well as in predicting
diverse and plausible steps for incomplete procedures. Our code is available at
https://github.com/facebookresearch/ProcedureVRL.",None,-1
e816942a-6c8a-41e4-a66c-c8d6e614e95f,A Dynamic Multi-Scale Voxel Flow Network for Video Prediction,0.99586,36,"The performance of video prediction has been greatly boosted by advanced deep
neural networks. However, most of the current methods suffer from large model
sizes and require extra inputs, e.g., semantic/depth maps, for promising
performance. For efficiency consideration, in this paper, we propose a Dynamic
Multi-scale Voxel Flow Network (DMVFN) to achieve better video prediction
performance at lower computational costs with only RGB images, than previous
methods. The core of our DMVFN is a differentiable routing module that can
effectively perceive the motion scales of video frames. Once trained, our DMVFN
selects adaptive sub-networks for different inputs at the inference stage.
Experiments on several benchmarks demonstrate that our DMVFN is an order of
magnitude faster than Deep Voxel Flow and surpasses the state-of-the-art
iterative-based OPT on generated image quality. Our code and demo are available
at https://huxiaotaostasy.github.io/DMVFN/.",None,-1
0590cfde-f171-4c71-9264-bd6d87a06258,Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations,0.941993,9,"Large pre-trained language models have exhibited unprecedented capabilities
in producing high-quality text via prompting techniques. This fact introduces
new possibilities for data collection and annotation, particularly in
situations where such data is scarce, complex to gather, expensive, or even
sensitive. In this paper, we explore the potential of these models to generate
and annotate goal-oriented dialogues, and conduct an in-depth analysis to
evaluate their quality. Our experiments employ ChatGPT, and encompass three
categories of goal-oriented dialogues (task-oriented, collaborative, and
explanatory), two generation modes (interactive and one-shot), and two
languages (English and Italian). Based on extensive human-based evaluations, we
demonstrate that the quality of generated dialogues and annotations is on par
with those generated by humans.",None,-1
0c77cd12-375b-4b57-8ba5-1c0802cd1498,Category Query Learning for Human-Object Interaction Classification,0.603884,7,"Unlike most previous HOI methods that focus on learning better human-object
features, we propose a novel and complementary approach called category query
learning. Such queries are explicitly associated to interaction categories,
converted to image specific category representation via a transformer decoder,
and learnt via an auxiliary image-level classification task. This idea is
motivated by an earlier multi-label image classification method, but is for the
first time applied for the challenging human-object interaction classification
task. Our method is simple, general and effective. It is validated on three
representative HOI baselines and achieves new state-of-the-art results on two
benchmarks.",None,-1
26d0a1e2-becb-4a0e-8918-d9aa22bef836,PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model,0.586783,2,"Despite the remarkable progress in natural language understanding with
pretrained Transformers, neural language models often do not handle commonsense
knowledge well. Toward commonsense-aware models, there have been attempts to
obtain knowledge, ranging from automatic acquisition to crowdsourcing. However,
it is difficult to obtain a high-quality knowledge base at a low cost,
especially from scratch. In this paper, we propose PHALM, a method of building
a knowledge graph from scratch, by prompting both crowdworkers and a large
language model (LLM). We used this method to build a Japanese event knowledge
graph and trained Japanese commonsense generation models. Experimental results
revealed the acceptability of the built graph and inferences generated by the
trained models. We also report the difference in prompting humans and an LLM.
Our code, data, and models are available at
github.com/nlp-waseda/comet-atomic-ja.",None,-1
3049f080-f3bb-4aa6-833d-a7cd76854853,XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models,0.906285,44,"Large multilingual language models typically rely on a single vocabulary
shared across 100+ languages. As these models have increased in parameter count
and depth, vocabulary size has remained largely unchanged. This
\textit{vocabulary bottleneck} limits the representational capabilities of
multilingual models like XLM-R. In this paper, we introduce a new approach for
scaling to very large multilingual vocabularies by de-emphasizing token sharing
between languages with little lexical overlap and assigning vocabulary capacity
to achieve sufficient coverage for each individual language. Tokenizations
using our vocabulary are typically more semantically meaningful and shorter
compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a
multilingual language model with a one million token vocabulary. XLM-V
outperforms XLM-R on every task we tested on ranging from natural language
inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity
recognition (WikiAnn). XLM-V is particularly effective on low-resource language
tasks and outperforms XLM-R by 11.2% and 5.8% absolute on MasakhaNER and
Americas NLI, respectively.",None,-1
96155284-1232-4e1b-828e-a4823c284314,Deformation measurement of a soil mixing retaining wall using terrestrial laser scanning,0.49269,2,"Retaining walls are often built to prevent excessive lateral movements of the
ground surrounding an excavation site. During an excavation, failure of
retaining walls could cause catastrophic accidents and hence their lateral
deformations are monitored regularly. Laser scanning can rapidly acquire the
spatial data of a relatively large area at fine spatial resolutions, which is
ideal for monitoring retaining walls' deformations. This paper attempts to
apply laser scanning to measurements of the lateral deformations of a soil
mixing retaining wall at an ongoing excavation site. Reference measurements by
total station and inclinometer were also conducted to verify those from the
laser scanning. The deformations derived using laser scanning data were
consistent with the reference measurements at the top part of the retaining
wall (i.e., mainly the ring beam of the wall). This research also shows that
the multi-scale-model-to-model method was the most accurate deformation
estimation method on the research data.",None,-1
7b732d49-c53e-4030-889a-bc8980afec09,Acquiring Qualitative Explainable Graphs for Automated Driving Scene Interpretation,0.282159,1,"The future of automated driving (AD) is rooted in the development of robust,
fair and explainable artificial intelligence methods. Upon request, automated
vehicles must be able to explain their decisions to the driver and the car
passengers, to the pedestrians and other vulnerable road users and potentially
to external auditors in case of accidents. However, nowadays, most explainable
methods still rely on quantitative analysis of the AD scene representations
captured by multiple sensors. This paper proposes a novel representation of AD
scenes, called Qualitative eXplainable Graph (QXG), dedicated to qualitative
spatiotemporal reasoning of long-term scenes. The construction of this graph
exploits the recent Qualitative Constraint Acquisition paradigm. Our
experimental results on NuScenes, an open real-world multi-modal dataset, show
that the qualitative eXplainable graph of an AD scene composed of 40 frames can
be computed in real-time and light in space storage which makes it a
potentially interesting tool for improved and more trustworthy perception and
control processes in AD.",None,-1
642e1d84-e557-4f8a-93b5-d1031e68d0d5,Strivec: Sparse Tri-Vector Radiance Fields,0.83269,16,"We propose Strivec, a novel neural representation that models a 3D scene as a
radiance field with sparsely distributed and compactly factorized local tensor
feature grids. Our approach leverages tensor decomposition, following the
recent work TensoRF, to model the tensor grids. In contrast to TensoRF which
uses a global tensor and focuses on their vector-matrix decomposition, we
propose to utilize a cloud of local tensors and apply the classic
CANDECOMP/PARAFAC (CP) decomposition to factorize each tensor into triple
vectors that express local feature distributions along spatial axes and
compactly encode a local neural field. We also apply multi-scale tensor grids
to discover the geometry and appearance commonalities and exploit spatial
coherence with the tri-vector factorization at multiple local scales. The final
radiance field properties are regressed by aggregating neural features from
multiple local tensors across all scales. Our tri-vector tensors are sparsely
distributed around the actual scene surface, discovered by a fast coarse
reconstruction, leveraging the sparsity of a 3D scene. We demonstrate that our
model can achieve better rendering quality while using significantly fewer
parameters than previous methods, including TensoRF and Instant-NGP.",None,-1
851f6105-273a-4cce-a2f3-c64bcac67bae,A Re-Parameterized Vision Transformer (ReVT) for Domain-Generalized Semantic Segmentation,0.611043,4,"The task of semantic segmentation requires a model to assign semantic labels
to each pixel of an image. However, the performance of such models degrades
when deployed in an unseen domain with different data distributions compared to
the training domain. We present a new augmentation-driven approach to domain
generalization for semantic segmentation using a re-parameterized vision
transformer (ReVT) with weight averaging of multiple models after training. We
evaluate our approach on several benchmark datasets and achieve
state-of-the-art mIoU performance of 47.3% (prior art: 46.3%) for small models
and of 50.1% (prior art: 47.8%) for midsized models on commonly used benchmark
datasets. At the same time, our method requires fewer parameters and reaches a
higher frame rate than the best prior art. It is also easy to implement and,
unlike network ensembles, does not add any computational complexity during
inference.",None,-1
94ef1726-638a-4d24-b8b7-e8cd2539c0f0,Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models,0.488097,26,"Text-to-image generative models have demonstrated remarkable capabilities in
generating high-quality images based on textual prompts. However, crafting
prompts that accurately capture the user's creative intent remains challenging.
It often involves laborious trial-and-error procedures to ensure that the model
interprets the prompts in alignment with the user's intention. To address the
challenges, we present Promptify, an interactive system that supports prompt
exploration and refinement for text-to-image generative models. Promptify
utilizes a suggestion engine powered by large language models to help users
quickly explore and craft diverse prompts. Our interface allows users to
organize the generated images flexibly, and based on their preferences,
Promptify suggests potential changes to the original prompt. This feedback loop
enables users to iteratively refine their prompts and enhance desired features
while avoiding unwanted ones. Our user study shows that Promptify effectively
facilitates the text-to-image workflow and outperforms an existing baseline
tool widely used for text-to-image generation.",None,-1
eb942532-9a5f-4141-933b-578bd609500b,Few-Shot Medical Image Segmentation via a Region-enhanced Prototypical Transformer,0.488875,5,"Automated segmentation of large volumes of medical images is often plagued by
the limited availability of fully annotated data and the diversity of organ
surface properties resulting from the use of different acquisition protocols
for different patients. In this paper, we introduce a more promising few-shot
learning-based method named Region-enhanced Prototypical Transformer (RPT) to
mitigate the effects of large intra-class diversity/bias. First, a subdivision
strategy is introduced to produce a collection of regional prototypes from the
foreground of the support prototype. Second, a self-selection mechanism is
proposed to incorporate into the Bias-alleviated Transformer (BaT) block to
suppress or remove interferences present in the query prototype and regional
support prototypes. By stacking BaT blocks, the proposed RPT can iteratively
optimize the generated regional prototypes and finally produce rectified and
more accurate global prototypes for Few-Shot Medical Image Segmentation (FSMS).
Extensive experiments are conducted on three publicly available medical image
datasets, and the obtained results show consistent improvements compared to
state-of-the-art FSMS methods. The source code is available at:
https://github.com/YazhouZhu19/RPT.",None,-1
98108413-9e5f-4eac-a0ff-c264bad1866e,I2I: Initializing Adapters with Improvised Knowledge,0.343971,5,"Adapters present a promising solution to the catastrophic forgetting problem
in continual learning. However, training independent Adapter modules for every
new task misses an opportunity for cross-task knowledge transfer. We propose
Improvise to Initialize (I2I), a continual learning algorithm that initializes
Adapters for incoming tasks by distilling knowledge from previously-learned
tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning
benchmark, by conducting experiments on sequences of visual question answering
tasks. Adapters trained with I2I consistently achieve better task accuracy than
independently-trained Adapters, demonstrating that our algorithm facilitates
knowledge transfer between task Adapters. I2I also results in better cross-task
knowledge transfer than the state-of-the-art AdapterFusion without incurring
the associated parametric cost.",None,-1
1eab4c2d-d453-4b00-849d-f48d23d33d22,From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction Tuning,0.144381,13,"Fine-tuning language models on tasks with instructions has demonstrated
potential in facilitating zero-shot generalization to unseen tasks. In this
paper, we introduce a straightforward yet effective method for enhancing
instruction tuning by employing symbolic tasks. Compared to crowdsourced human
tasks or model-generated tasks, symbolic tasks present a unique advantage as
they can be easily generated in vast quantities, theoretically providing an
infinite supply of high-quality training instances. To explore the potential of
symbolic tasks, we carry out an extensive case study on the representative
symbolic task of SQL execution. Empirical results on various benchmarks
validate that the integration of SQL execution leads to significant
improvements in zero-shot scenarios, particularly in table reasoning. Notably,
our 3B model surpasses both the 175B GPT-3 and ChatGPT in zero-shot table
reasoning across four benchmarks. Furthermore, experimental results on BBH (27
tasks) and MMLU (57 tasks) reveal that language models can be enhanced through
symbolic tasks without compromising their generality. We hope that our paper
serves as a catalyst, inspiring increased efforts to incorporate symbolic tasks
in instruction tuning.",None,-1
e239e4ec-a2d3-4d21-b7f3-1d419ac9ce0d,Stanford MLab at SemEval-2023 Task 10: Exploring GloVe- and Transformer-Based Methods for the Explainable Detection of Online Sexism,0.164044,1,"In this paper, we discuss the methods we applied at SemEval-2023 Task 10:
Towards the Explainable Detection of Online Sexism. Given an input text, we
perform three classification tasks to predict whether the text is sexist and
classify the sexist text into subcategories in order to provide an additional
explanation as to why the text is sexist. We explored many different types of
models, including GloVe embeddings as the baseline approach, transformer-based
deep learning models like BERT, RoBERTa, and DeBERTa, ensemble models, and
model blending. We explored various data cleaning and augmentation methods to
improve model performance. Pre-training transformer models yielded significant
improvements in performance, and ensembles and blending slightly improved
robustness in the F1 score.",None,-1
1c0b2616-5ec0-4368-b687-de6578c06c0e,Unsupervised Optical Flow Estimation with Dynamic Timing Representation for Spike Camera,0.0719983,1,"Efficiently selecting an appropriate spike stream data length to extract
precise information is the key to the spike vision tasks. To address this
issue, we propose a dynamic timing representation for spike streams. Based on
multi-layers architecture, it applies dilated convolutions on temporal
dimension to extract features on multi-temporal scales with few parameters. And
we design layer attention to dynamically fuse these features. Moreover, we
propose an unsupervised learning method for optical flow estimation in a
spike-based manner to break the dependence on labeled data. In addition, to
verify the robustness, we also build a spike-based synthetic validation dataset
for extreme scenarios in autonomous driving, denoted as SSES dataset. It
consists of various corner cases. Experiments show that our method can predict
optical flow from spike streams in different high-speed scenes, including real
scenes. For instance, our method gets $15\%$ and $19\%$ error reduction from
the best spike-based work, SCFlow, in $\Delta t=10$ and $\Delta t=20$
respectively which are the same settings as the previous works.",None,-1
4d8d4de7-ca23-423f-9bcc-cfa729d5977a,Instance-based Max-margin for Practical Few-shot Recognition,0.0410332,1,"In order to mimic the human few-shot learning (FSL) ability better and to
make FSL closer to real-world applications, this paper proposes a practical FSL
(pFSL) setting. pFSL is based on unsupervised pretrained models (analogous to
human prior knowledge) and recognizes many novel classes simultaneously.
Compared to traditional FSL, pFSL is simpler in its formulation, easier to
evaluate, more challenging and more practical. To cope with the rarity of
training examples, this paper proposes IbM2, an instance-based max-margin
method not only for the new pFSL setting, but also works well in traditional
FSL scenarios. Based on the Gaussian Annulus Theorem, IbM2 converts random
noise applied to the instances into a mechanism to achieve maximum margin in
the many-way pFSL (or traditional FSL) recognition task. Experiments with
various self-supervised pretraining methods and diverse many- or few-way FSL
tasks show that IbM2 almost always leads to improvements compared to its
respective baseline methods, and in most cases the improvements are
significant. With both the new pFSL setting and novel IbM2 method, this paper
shows that practical few-shot learning is both viable and promising.",None,-1
34d46cb0-c79d-49eb-a2f6-1d3abf36257c,MWaste: A Deep Learning Approach to Manage Household Waste,0.0366138,1,"Computer vision methods have shown to be effective in classifying garbage
into recycling categories for waste processing, existing methods are costly,
imprecise, and unclear. To tackle this issue, we introduce MWaste, a mobile
application that uses computer vision and deep learning techniques to classify
waste materials as trash, plastic, paper, metal, glass or cardboard. Its
effectiveness was tested on various neural network architectures and real-world
images, achieving an average precision of 92\% on the test set. This app can
help combat climate change by enabling efficient waste processing and reducing
the generation of greenhouse gases caused by incorrect waste disposal.",None,-1
308e64f8-1231-41c4-afa8-d5094edae33c,T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,0.628843,5,"Cross-lingual text classification leverages text classifiers trained in a
high-resource language to perform text classification in other languages with
no or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays,
cross-lingual text classifiers are typically built on large-scale, multilingual
language models (LMs) pretrained on a variety of languages of interest.
However, the performance of these models vary significantly across languages
and classification tasks, suggesting that the superposition of the language
modelling and classification tasks is not always effective. For this reason, in
this paper we propose revisiting the classic ""translate-and-test"" pipeline to
neatly separate the translation and classification stages. The proposed
approach couples 1) a neural machine translator translating from the targeted
language to a high-resource language, with 2) a text classifier trained in the
high-resource language, but the neural machine translator generates ""soft""
translations to permit end-to-end backpropagation during fine-tuning of the
pipeline. Extensive experiments have been carried out over three cross-lingual
text classification datasets (XNLI, MLDoc and MultiEURLEX), with the results
showing that the proposed approach has significantly improved performance over
a competitive baseline.",None,-1
3eeb749d-4137-43c4-8ec7-1c529637e9b3,Template-free Articulated Neural Point Clouds for Reposable View Synthesis,0.147617,4,"Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when
synthesizing novel views of time-evolving 3D scenes. However, the common
reliance on backward deformation fields makes reanimation of the captured
object poses challenging. Moreover, the state of the art dynamic models are
often limited by low visual fidelity, long reconstruction time or specificity
to narrow application domains. In this paper, we present a novel method
utilizing a point-based representation and Linear Blend Skinning (LBS) to
jointly learn a Dynamic NeRF and an associated skeletal model from even sparse
multi-view video. Our forward-warping approach achieves state-of-the-art visual
fidelity when synthesizing novel views and poses while significantly reducing
the necessary learning time when compared to existing work. We demonstrate the
versatility of our representation on a variety of articulated objects from
common datasets and obtain reposable 3D reconstructions without the need of
object-specific skeletal templates. Code will be made available at
https://github.com/lukasuz/Articulated-Point-NeRF.",None,-1
ae05bf0a-2c42-4c81-9377-1e165a77a9c8,Cultivated Wildness: Technodiversity and Wildness in Machines,0.329466,2,"This paper investigates the idea of cultivated wildness at the intersection
of landscape design and artificial intelligence. The paper posits that
contemporary landscape practices should overcome the potentially single
understanding on wilderness, and instead explore landscape strategies to
cultivate new forms of wild places via ideas and concerns in contemporary
Environmental Humanities, Science and Technology Studies, Ecological Sciences,
and Landscape Architecture. Drawing cases in environmental engineering,
computer science, and landscape architecture research, this paper explores a
framework to construct wild places with intelligent machines. In this
framework, machines are not understood as a layer of ""digital infrastructure""
that is used to extend localized human intelligence and agency. Rather machines
are conceptualized as active agents who can participate in the intelligence of
co-production. Recent developments in cybernetic technologies such as sensing
networks, artificial intelligence, and cyberphysical systems can also
contribute to establishing the framework. At the heart of this framework is
""technodiversity,"" in parallel with biodiversity, since a singular vision on
technological development driven by optimization and efficiency reinforces a
monocultural approach that eliminates other possible relationships to construct
with the environment. Thus, cultivated wildness is also about recognizing
""wildness"" in machines.",None,-1
26b53903-15fb-4394-8816-a13002eca2ca,Towards Effective Disambiguation for Machine Translation with Large Language Models,0.334908,3,"Resolving semantic ambiguity has long been recognised as a central challenge
in the field of Machine Translation. Recent work on benchmarking translation
performance on ambiguous sentences has exposed the limitations of conventional
Neural Machine Translation (NMT) systems, which fail to handle many such cases.
Large language models (LLMs) have emerged as a promising alternative,
demonstrating comparable performance to traditional NMT models while
introducing new paradigms for controlling the target outputs. In this paper, we
study the capabilities of LLMs to translate ""ambiguous sentences"" - i.e. those
containing highly polysemous words and/or rare word senses. We also propose two
ways to improve their disambiguation capabilities, through a) in-context
learning and b) fine-tuning on carefully curated ambiguous datasets.
Experiments show that our methods can match or outperform state-of-the-art
systems such as DeepL and NLLB in four out of five language directions. Our
research provides valuable insights into effectively adapting LLMs to become
better disambiguators during Machine Translation. We release our curated
disambiguation corpora and resources at
https://data.statmt.org/ambiguous-europarl.",None,-1
8e244d4a-a103-437d-93d4-55ba599e3ef5,Position Interpolation Improves ALiBi Extrapolation,0.105028,3,"Linear position interpolation helps pre-trained models using rotary position
embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using
linear position interpolation to extend the extrapolation range of models using
Attention with Linear Biases (ALiBi). We find position interpolation
significantly improves extrapolation capability on upstream language modelling
and downstream summarization and retrieval tasks.",None,-1
88d8384c-95f0-4fc8-bde8-bf160ad31bdb,Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints,0.355432,4,"Learning robust local image feature matching is a fundamental low-level
vision task, which has been widely explored in the past few years. Recently,
detector-free local feature matchers based on transformers have shown promising
results, which largely outperform pure Convolutional Neural Network (CNN) based
ones. But correlations produced by transformer-based methods are spatially
limited to the center of source views' coarse patches, because of the costly
attention learning. In this work, we rethink this issue and find that such
matching formulation degrades pose estimation, especially for low-resolution
images. So we propose a transformer-based cascade matching model -- Cascade
feature Matching TRansformer (CasMTR), to efficiently learn dense feature
correlations, which allows us to choose more reliable matching pairs for the
relative pose estimation. Instead of re-training a new detector, we use a
simple yet effective Non-Maximum Suppression (NMS) post-process to filter
keypoints through the confidence map, and largely improve the matching
precision. CasMTR achieves state-of-the-art performance in indoor and outdoor
pose estimation as well as visual localization. Moreover, thorough ablations
show the efficacy of the proposed components and techniques.",None,-1
bb12bb61-da9b-4417-8852-8f52d45a3e88,Building Emotional Support Chatbots in the Era of LLMs,0.984269,25,"The integration of emotional support into various conversational scenarios
presents profound societal benefits, such as social interactions, mental health
counseling, and customer service. However, there are unsolved challenges that
hinder real-world applications in this field, including limited data
availability and the absence of well-accepted model training paradigms. This
work endeavors to navigate these challenges by harnessing the capabilities of
Large Language Models (LLMs). We introduce an innovative methodology that
synthesizes human insights with the computational prowess of LLMs to curate an
extensive emotional support dialogue dataset. Our approach is initiated with a
meticulously designed set of dialogues spanning diverse scenarios as generative
seeds. By utilizing the in-context learning potential of ChatGPT, we
recursively generate an ExTensible Emotional Support dialogue dataset, named
ExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,
examining the impact of diverse training strategies, ultimately yielding an LLM
meticulously optimized for emotional support interactions. An exhaustive
assessment of the resultant model showcases its proficiency in offering
emotional support, marking a pivotal step in the realm of emotional support
bots and paving the way for subsequent research and implementations.",None,-1
8e2fd36f-c9ad-463a-9f78-2d30decca4b9,Visual Hindsight Self-Imitation Learning for Interactive Navigation,0.235551,1,"Interactive visual navigation tasks, which involve following instructions to
reach and interact with specific targets, are challenging not only because
successful experiences are very rare but also because the complex visual inputs
require a substantial number of samples. Previous methods for these tasks often
rely on intricately designed dense rewards or the use of expensive expert data
for imitation learning. To tackle these challenges, we propose a novel
approach, Visual Hindsight Self-Imitation Learning (VHS) for enhancing sample
efficiency through hindsight goal re-labeling and self-imitation. We also
introduce a prototypical goal embedding method derived from experienced goal
observations, that is particularly effective in vision-based and partially
observable environments. This embedding technique allows the agent to visually
reinterpret its unsuccessful attempts, enabling vision-based goal re-labeling
and self-imitation from enhanced successful experiences. Experimental results
show that VHS outperforms existing techniques in interactive visual navigation
tasks, confirming its superior performance and sample efficiency.",None,-1
0ea612a6-4cd4-4700-84d0-620d7f5fbec5,Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues,0.67689,7,"Recently, deep learning enabled the accurate segmentation of various diseases
in medical imaging. These performances, however, typically demand large amounts
of manual voxel annotations. This tedious process for volumetric data becomes
more complex when not all required information is available in a single imaging
domain as is the case for PET/CT data. We propose a multimodal interactive
segmentation framework that mitigates these issues by combining anatomical and
physiological cues from PET/CT data. Our framework utilizes the geodesic
distance transform to represent the user annotations and we implement a novel
ellipsoid-based user simulation scheme during training. We further propose two
annotation interfaces and conduct a user study to estimate their usability. We
evaluated our model on the in-domain validation dataset and an unseen PET/CT
dataset. We make our code publicly available:
https://github.com/verena-hallitschke/pet-ct-annotate.",None,-1
36bcd6d9-7fab-46de-a142-c1b73cdf0bf3,Remind of the Past: Incremental Learning with Analogical Prompts,0.107997,1,"Although data-free incremental learning methods are memory-friendly,
accurately estimating and counteracting representation shifts is challenging in
the absence of historical data. This paper addresses this thorny problem by
proposing a novel incremental learning method inspired by human analogy
capabilities. Specifically, we design an analogy-making mechanism to remap the
new data into the old class by prompt tuning. It mimics the feature
distribution of the target old class on the old model using only samples of new
classes. The learnt prompts are further used to estimate and counteract the
representation shift caused by fine-tuning for the historical prototypes. The
proposed method sets up new state-of-the-art performance on four incremental
learning benchmarks under both the class and domain incremental learning
settings. It consistently outperforms data-replay methods by only saving
feature prototypes for each class. It has almost hit the empirical upper bound
by joint training on the Core50 benchmark. The code will be released at
\url{https://github.com/ZhihengCV/A-Prompts}.",None,-1
16f62a5a-c044-47da-82cf-3eff36cbbaf9,CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained Language Models,0.991522,18,"Large pre-trained language models (LLMs) have been shown to have significant
potential in few-shot learning across various fields, even with minimal
training data. However, their ability to generalize to unseen tasks in more
complex fields, such as biology, has yet to be fully evaluated. LLMs can offer
a promising alternative approach for biological inference, particularly in
cases where structured data and sample size are limited, by extracting prior
knowledge from text corpora. Our proposed few-shot learning approach uses LLMs
to predict the synergy of drug pairs in rare tissues that lack structured data
and features. Our experiments, which involved seven rare tissues from different
cancer types, demonstrated that the LLM-based prediction model achieved
significant accuracy with very few or zero samples. Our proposed model, the
CancerGPT (with $\sim$ 124M parameters), was even comparable to the larger
fine-tuned GPT-3 model (with $\sim$ 175B parameters). Our research is the first
to tackle drug pair synergy prediction in rare tissues with limited data. We
are also the first to utilize an LLM-based prediction model for biological
reaction prediction tasks.",None,-1
2bb70616-8c5b-41f6-b270-5c3ce6cc3b7f,An Open Dataset and Model for Language Identification,0.574042,17,"Language identification (LID) is a fundamental step in many natural language
processing pipelines. However, current LID systems are far from perfect,
particularly on lower-resource languages. We present a LID model which achieves
a macro-average F1 score of 0.93 and a false positive rate of 0.033 across 201
languages, outperforming previous work. We achieve this by training on a
curated dataset of monolingual data, the reliability of which we ensure by
auditing a sample from each source and each language manually. We make both the
model and the dataset available to the research community. Finally, we carry
out detailed analysis into our model's performance, both in comparison to
existing open models and by language class.",None,-1
98eb63f4-6aa9-4f90-b3b6-621850c84e26,Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers,0.970653,8,"Abstracts derived from biomedical literature possess distinct domain-specific
characteristics, including specialised writing styles and biomedical
terminologies, which necessitate a deep understanding of the related
literature. As a result, existing language models struggle to generate
technical summaries that are on par with those produced by biomedical experts,
given the absence of domain-specific background knowledge. This paper aims to
enhance the performance of language models in biomedical abstractive
summarisation by aggregating knowledge from external papers cited within the
source article. We propose a novel attention-based citation aggregation model
that integrates domain-specific knowledge from citation papers, allowing neural
networks to generate summaries by leveraging both the paper content and
relevant knowledge from citation papers. Furthermore, we construct and release
a large-scale biomedical summarisation dataset that serves as a foundation for
our research. Extensive experiments demonstrate that our model outperforms
state-of-the-art approaches and achieves substantial improvements in
abstractive biomedical text summarisation.",None,-1
de15c469-c310-438c-b4b0-3c349518e8db,A Multi-Grained Self-Interpretable Symbolic-Neural Model For Single/Multi-Labeled Text Classification,0.217297,5,"Deep neural networks based on layer-stacking architectures have historically
suffered from poor inherent interpretability. Meanwhile, symbolic probabilistic
models function with clear interpretability, but how to combine them with
neural networks to enhance their performance remains to be explored. In this
paper, we try to marry these two systems for text classification via a
structured language model. We propose a Symbolic-Neural model that can learn to
explicitly predict class labels of text spans from a constituency tree without
requiring any access to span-level gold labels. As the structured language
model learns to predict constituency trees in a self-supervised manner, only
raw texts and sentence-level labels are required as training data, which makes
it essentially a general constituent-level self-interpretable classification
model. Our experiments demonstrate that our approach could achieve good
prediction accuracy in downstream tasks. Meanwhile, the predicted span labels
are consistent with human rationales to a certain degree.",None,-1
c01175ef-b3f4-4d16-ab7d-8325abd430c3,MetaTKG: Learning Evolutionary Meta-Knowledge for Temporal Knowledge Graph Reasoning,0.665309,6,"Reasoning over Temporal Knowledge Graphs (TKGs) aims to predict future facts
based on given history. One of the key challenges for prediction is to learn
the evolution of facts. Most existing works focus on exploring evolutionary
information in history to obtain effective temporal embeddings for entities and
relations, but they ignore the variation in evolution patterns of facts, which
makes them struggle to adapt to future data with different evolution patterns.
Moreover, new entities continue to emerge along with the evolution of facts
over time. Since existing models highly rely on historical information to learn
embeddings for entities, they perform poorly on such entities with little
historical information. To tackle these issues, we propose a novel Temporal
Meta-learning framework for TKG reasoning, MetaTKG for brevity. Specifically,
our method regards TKG prediction as many temporal meta-tasks, and utilizes the
designed Temporal Meta-learner to learn evolutionary meta-knowledge from these
meta-tasks. The proposed method aims to guide the backbones to learn to adapt
quickly to future data and deal with entities with little historical
information by the learned meta-knowledge. Specially, in temporal meta-learner,
we design a Gating Integration module to adaptively establish temporal
correlations between meta-tasks. Extensive experiments on four widely-used
datasets and three backbones demonstrate that our method can greatly improve
the performance.",None,-1
0dd4a5b2-d5a9-4d95-8b59-bc104b6ab009,A Reference-less Quality Metric for Automatic Speech Recognition via Contrastive-Learning of a Multi-Language Model with Self-Supervision,0.43102,3,"The common standard for quality evaluation of automatic speech recognition
(ASR) systems is reference-based metrics such as the Word Error Rate (WER),
computed using manual ground-truth transcriptions that are time-consuming and
expensive to obtain. This work proposes a multi-language referenceless quality
metric, which allows comparing the performance of different ASR models on a
speech dataset without ground truth transcriptions. To estimate the quality of
ASR hypotheses, a pre-trained language model (LM) is fine-tuned with
contrastive learning in a self-supervised learning manner. In experiments
conducted on several unseen test datasets consisting of outputs from top
commercial ASR engines in various languages, the proposed referenceless metric
obtains a much higher correlation with WER scores and their ranks than the
perplexity metric from the state-of-art multi-lingual LM in all experiments,
and also reduces WER by more than $7\%$ when used for ensembling hypotheses.
The fine-tuned model and experiments are made available for the
reproducibility: https://github.com/aixplain/NoRefER",None,-1
a26eae8d-9632-4646-82f9-8a6bcd88d533,Text Encoders Lack Knowledge: Leveraging Generative LLMs for Domain-Specific Semantic Textual Similarity,0.586106,5,"Amidst the sharp rise in the evaluation of large language models (LLMs) on
various tasks, we find that semantic textual similarity (STS) has been
under-explored. In this study, we show that STS can be cast as a text
generation problem while maintaining strong performance on multiple STS
benchmarks. Additionally, we show generative LLMs significantly outperform
existing encoder-based STS models when characterizing the semantic similarity
between two texts with complex semantic relationships dependent on world
knowledge. We validate this claim by evaluating both generative LLMs and
existing encoder-based STS models on three newly collected STS challenge sets
which require world knowledge in the domains of Health, Politics, and Sports.
All newly collected data is sourced from social media content posted after May
2023 to ensure the performance of closed-source models like ChatGPT cannot be
credited to memorization. Our results show that, on average, generative LLMs
outperform the best encoder-only baselines by an average of 22.3% on STS tasks
requiring world knowledge. Our results suggest generative language models with
STS-specific prompting strategies achieve state-of-the-art performance in
complex, domain-specific STS tasks.",None,-1
144bd576-42c6-4f9e-a528-003836aa6e9d,Prompt-based Context- and Domain-aware Pretraining for Vision and Language Navigation,0.156818,1,"Pretrained visual-language models have extensive world knowledge and are
widely used in visual and language navigation (VLN). However, they are not
sensitive to indoor scenarios for VLN tasks. Another challenge for VLN is how
the agent understands the contextual relations between actions on a path and
performs cross-modal alignment sequentially. In this paper, we propose a novel
Prompt-bAsed coNtext- and inDoor-Aware (PANDA) pretraining framework to address
these problems. It performs prompting in two stages. In the indoor-aware stage,
we apply an efficient tuning paradigm to learn deep visual prompts from an
indoor dataset, in order to augment pretrained models with inductive biases
towards indoor environments. This can enable more sample-efficient adaptation
for VLN agents. Furthermore, in the context-aware stage, we design a set of
hard context prompts to capture the sequence-level semantics in the
instruction. They enable further tuning of the pretrained models via
contrastive learning. Experimental results on both R2R and REVERIE show the
superiority of PANDA compared to existing state-of-the-art methods.",None,-1
082241f9-4b40-410b-88bf-1aeb936f2b57,Self-supervised dense representation learning for live-cell microscopy with time arrow prediction,0.794942,3,"State-of-the-art object detection and segmentation methods for microscopy
images rely on supervised machine learning, which requires laborious manual
annotation of training data. Here we present a self-supervised method based on
time arrow prediction pre-training that learns dense image representations from
raw, unlabeled live-cell microscopy videos. Our method builds upon the task of
predicting the correct order of time-flipped image regions via a single-image
feature extractor followed by a time arrow prediction head that operates on the
fused features. We show that the resulting dense representations capture
inherently time-asymmetric biological processes such as cell divisions on a
pixel-level. We furthermore demonstrate the utility of these representations on
several live-cell microscopy datasets for detection and segmentation of
dividing cells, as well as for cell state classification. Our method
outperforms supervised methods, particularly when only limited ground truth
annotations are available as is commonly the case in practice. We provide code
at https://github.com/weigertlab/tarrow.",None,-1
b050a7dc-28de-4d98-a85d-5f90b6108d6a,On the Planning Abilities of Large Language Models : A Critical Investigation,0.984913,100,"Intrigued by the claims of emergent reasoning capabilities in LLMs trained on
general web corpora, in this paper, we set out to investigate their planning
capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating
plans autonomously in commonsense planning tasks and (2) the potential of LLMs
in LLM-Modulo settings where they act as a source of heuristic guidance for
external planners and verifiers. We conduct a systematic study by generating a
suite of instances on domains similar to the ones employed in the International
Planning Competition and evaluate LLMs in two distinct modes: autonomous and
heuristic. Our findings reveal that LLMs' ability to generate executable plans
autonomously is rather limited, with the best model (GPT-4) having an average
success rate of ~12% across the domains. However, the results in the LLM-Modulo
setting show more promise. In the LLM-Modulo setting, we demonstrate that
LLM-generated plans can improve the search process for underlying sound
planners and additionally show that external verifiers can help provide
feedback on the generated plans and back-prompt the LLM for better plan
generation.",None,-1
ceb7c16b-a54f-43a7-bebd-01cd02bfca0f,An AMR-based Link Prediction Approach for Document-level Event Argument Extraction,0.999127,9,"Recent works have introduced Abstract Meaning Representation (AMR) for
Document-level Event Argument Extraction (Doc-level EAE), since AMR provides a
useful interpretation of complex semantic structures and helps to capture
long-distance dependency. However, in these works AMR is used only implicitly,
for instance, as additional features or training signals. Motivated by the fact
that all event structures can be inferred from AMR, this work reformulates EAE
as a link prediction problem on AMR graphs. Since AMR is a generic structure
and does not perfectly suit EAE, we propose a novel graph structure, Tailored
AMR Graph (TAG), which compresses less informative subgraphs and edge types,
integrates span information, and highlights surrounding events in the same
document. With TAG, we further propose a novel method using graph neural
networks as a link prediction model to find event arguments. Our extensive
experiments on WikiEvents and RAMS show that this simpler approach outperforms
the state-of-the-art models by 3.63pt and 2.33pt F1, respectively, and do so
with reduced 56% inference time. The code is availabel at
https://github.com/ayyyq/TARA.",None,-1
a2f65590-2a80-431b-b1ad-5ea9afaade90,A Unified Framework of Policy Learning for Contextual Bandit with Confounding Bias and Missing Observations,0.22748,2,"We study the offline contextual bandit problem, where we aim to acquire an
optimal policy using observational data. However, this data usually contains
two deficiencies: (i) some variables that confound actions are not observed,
and (ii) missing observations exist in the collected data. Unobserved
confounders lead to a confounding bias and missing observations cause bias and
inefficiency problems. To overcome these challenges and learn the optimal
policy from the observed dataset, we present a new algorithm called
Causal-Adjusted Pessimistic (CAP) policy learning, which forms the reward
function as the solution of an integral equation system, builds a confidence
set, and greedily takes action with pessimism. With mild assumptions on the
data, we develop an upper bound to the suboptimality of CAP for the offline
contextual bandit problem.",None,-1
efb4aee2-2d22-408b-811b-96398d069ef6,Comparing Intrinsic Gender Bias Evaluation Measures without using Human Annotated Examples,0.394347,9,"Numerous types of social biases have been identified in pre-trained language
models (PLMs), and various intrinsic bias evaluation measures have been
proposed for quantifying those social biases. Prior works have relied on human
annotated examples to compare existing intrinsic bias evaluation measures.
However, this approach is not easily adaptable to different languages nor
amenable to large scale evaluations due to the costs and difficulties when
recruiting human annotators. To overcome this limitation, we propose a method
to compare intrinsic gender bias evaluation measures without relying on
human-annotated examples. Specifically, we create multiple bias-controlled
versions of PLMs using varying amounts of male vs. female gendered sentences,
mined automatically from an unannotated corpus using gender-related word lists.
Next, each bias-controlled PLM is evaluated using an intrinsic bias evaluation
measure, and the rank correlation between the computed bias scores and the
gender proportions used to fine-tune the PLMs is computed. Experiments on
multiple corpora and PLMs repeatedly show that the correlations reported by our
proposed method that does not require human annotated examples are comparable
to those computed using human annotated examples in prior work.",None,-1
2e596461-0b09-44b0-bfb0-0af32cb61d74,An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8,0.513535,4,"Qualitative Spatial Reasoning (QSR) is well explored area of Commonsense
Reasoning and has multiple applications ranging from Geographical Information
Systems to Robotics and Computer Vision. Recently many claims have been made
for the capabilities of Large Language Models (LLMs). In this paper we
investigate the extent to which one particular LLM can perform classical
qualitative spatial reasoning tasks on the mereotopological calculus, RCC-8.",None,-1
bff4edf7-35f1-4601-9516-7137963ae1d3,BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives,0.457191,3,"Implicit neural representations have become pivotal in robotic perception,
enabling robots to comprehend 3D environments from 2D images. Given a set of
camera poses and associated images, the models can be trained to synthesize
novel, unseen views. To successfully navigate and interact in dynamic settings,
robots require the understanding of their spatial surroundings driven by
unassisted reconstruction of 3D scenes and camera poses from real-time video
footage. Existing approaches like COLMAP and bundle-adjusting neural radiance
field methods take hours to days to process due to the high computational
demands of feature matching, dense point sampling, and training of a
multi-layer perceptron structure with a large number of parameters. To address
these challenges, we propose a framework called bundle-adjusting accelerated
neural graphics primitives (BAA-NGP) which leverages accelerated sampling and
hash encoding to expedite automatic pose refinement/estimation and 3D scene
reconstruction. Experimental results demonstrate 10 to 20 x speed improvement
compared to other bundle-adjusting neural radiance field methods without
sacrificing the quality of pose estimation. The github repository can be found
here https://github.com/IntelLabs/baa-ngp.",None,-1
95cb555c-2f06-40f8-a68a-ca311a7bb50b,Average Is Not Enough: Caveats of Multilingual Evaluation,0.054256,2,"This position paper discusses the problem of multilingual evaluation. Using
simple statistics, such as average language performance, might inject
linguistic biases in favor of dominant language families into evaluation
methodology. We argue that a qualitative analysis informed by comparative
linguistics is needed for multilingual results to detect this kind of bias. We
show in our case study that results in published works can indeed be
linguistically biased and we demonstrate that visualization based on URIEL
typological database can detect it.",None,-1
3e2c6986-adb7-48ca-b48d-bf284fd23f27,RenewNAT: Renewing Potential Translation for Non-Autoregressive Transformer,0.60886,3,"Non-autoregressive neural machine translation (NAT) models are proposed to
accelerate the inference process while maintaining relatively high performance.
However, existing NAT models are difficult to achieve the desired
efficiency-quality trade-off. For one thing, fully NAT models with efficient
inference perform inferior to their autoregressive counterparts. For another,
iterative NAT models can, though, achieve comparable performance while
diminishing the advantage of speed. In this paper, we propose RenewNAT, a
flexible framework with high efficiency and effectiveness, to incorporate the
merits of fully and iterative NAT models. RenewNAT first generates the
potential translation results and then renews them in a single pass. It can
achieve significant performance improvements at the same expense as traditional
NAT models (without introducing additional model parameters and decoding
latency). Experimental results on various translation benchmarks (e.g.,
\textbf{4} WMT) show that our framework consistently improves the performance
of strong fully NAT methods (e.g., GLAT and DSLP) without additional speed
overhead.",None,-1
77d6ed53-e768-4dca-a146-ef7d48387aa6,The Role of Interactive Visualization in Explaining (Large) NLP Models: from Data to Inference,0.656826,7,"With a constant increase of learned parameters, modern neural language models
become increasingly more powerful. Yet, explaining these complex model's
behavior remains a widely unsolved problem. In this paper, we discuss the role
interactive visualization can play in explaining NLP models (XNLP). We motivate
the use of visualization in relation to target users and common NLP pipelines.
We also present several use cases to provide concrete examples on XNLP with
visualization. Finally, we point out an extensive list of research
opportunities in this field.",None,-1
793d1495-72b8-4a22-94eb-13b25f1baf49,Memory-Augmented Theory of Mind Network,0.587321,5,"Social reasoning necessitates the capacity of theory of mind (ToM), the
ability to contextualise and attribute mental states to others without having
access to their internal cognitive structure. Recent machine learning
approaches to ToM have demonstrated that we can train the observer to read the
past and present behaviours of other agents and infer their beliefs (including
false beliefs about things that no longer exist), goals, intentions and future
actions. The challenges arise when the behavioural space is complex, demanding
skilful space navigation for rapidly changing contexts for an extended period.
We tackle the challenges by equipping the observer with novel neural memory
mechanisms to encode, and hierarchical attention to selectively retrieve
information about others. The memories allow rapid, selective querying of
distal related past behaviours of others to deliberatively reason about their
current mental state, beliefs and future behaviours. This results in ToMMY, a
theory of mind model that learns to reason while making little assumptions
about the underlying mental processes. We also construct a new suite of
experiments to demonstrate that memories facilitate the learning process and
achieve better theory of mind performance, especially for high-demand
false-belief tasks that require inferring through multiple steps of changes.",None,-1
07ca5dec-3cb6-444e-9f06-326dcf926d53,Leveraging TCN and Transformer for effective visual-audio fusion in continuous emotion recognition,0.780015,12,"Human emotion recognition plays an important role in human-computer
interaction. In this paper, we present our approach to the Valence-Arousal (VA)
Estimation Challenge, Expression (Expr) Classification Challenge, and Action
Unit (AU) Detection Challenge of the 5th Workshop and Competition on Affective
Behavior Analysis in-the-wild (ABAW). Specifically, we propose a novel
multi-modal fusion model that leverages Temporal Convolutional Networks (TCN)
and Transformer to enhance the performance of continuous emotion recognition.
Our model aims to effectively integrate visual and audio information for
improved accuracy in recognizing emotions. Our model outperforms the baseline
and ranks 3 in the Expression Classification challenge.",None,-1
7c389077-f227-495d-808a-a23806806060,NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis,0.605777,10,"This paper describes our system developed for the SemEval-2023 Task 12
""Sentiment Analysis for Low-resource African Languages using Twitter Dataset"".
Sentiment analysis is one of the most widely studied applications in natural
language processing. However, most prior work still focuses on a small number
of high-resource languages. Building reliable sentiment analysis systems for
low-resource languages remains challenging, due to the limited training data in
this task. In this work, we propose to leverage language-adaptive and
task-adaptive pretraining on African texts and study transfer learning with
source language selection on top of an African language-centric pretrained
language model. Our key findings are: (1) Adapting the pretrained model to the
target language and task using a small yet relevant corpus improves performance
remarkably by more than 10 F1 score points. (2) Selecting source languages with
positive transfer gains during training can avoid harmful interference from
dissimilar languages, leading to better results in multilingual and
cross-lingual settings. In the shared task, our system wins 8 out of 15 tracks
and, in particular, performs best in the multilingual evaluation.",None,-1
791a0c05-f79d-46f0-867b-efa4c0e6c445,RGB-T Multi-Modal Crowd Counting Based on Transformer,0.568092,8,"Crowd counting aims to estimate the number of persons in a scene. Most
state-of-the-art crowd counting methods based on color images can't work well
in poor illumination conditions due to invisible objects. With the widespread
use of infrared cameras, crowd counting based on color and thermal images is
studied. Existing methods only achieve multi-modal fusion without count
objective constraint. To better excavate multi-modal information, we use
count-guided multi-modal fusion and modal-guided count enhancement to achieve
the impressive performance. The proposed count-guided multi-modal fusion module
utilizes a multi-scale token transformer to interact two-modal information
under the guidance of count information and perceive different scales from the
token perspective. The proposed modal-guided count enhancement module employs
multi-scale deformable transformer decoder structure to enhance one modality
feature and count information by the other modality. Experiment in public
RGBT-CC dataset shows that our method refreshes the state-of-the-art results.
https://github.com/liuzywen/RGBTCC",None,-1
8e9eea42-9ac1-4a9c-9518-941e259b521d,AutoCost: Evolving Intrinsic Cost for Zero-violation Reinforcement Learning,0.558006,10,"Safety is a critical hurdle that limits the application of deep reinforcement
learning (RL) to real-world control tasks. To this end, constrained
reinforcement learning leverages cost functions to improve safety in
constrained Markov decision processes. However, such constrained RL methods
fail to achieve zero violation even when the cost limit is zero. This paper
analyzes the reason for such failure, which suggests that a proper cost
function plays an important role in constrained RL. Inspired by the analysis,
we propose AutoCost, a simple yet effective framework that automatically
searches for cost functions that help constrained RL to achieve zero-violation
performance. We validate the proposed method and the searched cost function on
the safe RL benchmark Safety Gym. We compare the performance of augmented
agents that use our cost function to provide additive intrinsic costs with
baseline agents that use the same policy learners but with only extrinsic
costs. Results show that the converged policies with intrinsic costs in all
environments achieve zero constraint violation and comparable performance with
baselines.",None,-1
e99073e4-5802-49b2-ba86-0d95108fb611,PyReason: Software for Open World Temporal Logic,0.520276,7,"The growing popularity of neuro symbolic reasoning has led to the adoption of
various forms of differentiable (i.e., fuzzy) first order logic. We introduce
PyReason, a software framework based on generalized annotated logic that both
captures the current cohort of differentiable logics and temporal extensions to
support inference over finite periods of time with capabilities for open world
reasoning. Further, PyReason is implemented to directly support reasoning over
graphical structures (e.g., knowledge graphs, social networks, biological
networks, etc.), produces fully explainable traces of inference, and includes
various practical features such as type checking and a memory-efficient
implementation. This paper reviews various extensions of generalized annotated
logic integrated into our implementation, our modern, efficient Python-based
implementation that conducts exact yet scalable deductive inference, and a
suite of experiments. PyReason is available at: github.com/lab-v2/pyreason.",None,-1
91d10da5-9f5f-4ea0-9cb0-33fb99be5a63,$\varepsilon$ KÚ <MASK>: Integrating Yorùbá cultural greetings into machine translation,0.338169,5,"This paper investigates the performance of massively multilingual neural
machine translation (NMT) systems in translating Yor\`ub\'a greetings
($\varepsilon$ k\'u [MASK]), which are a big part of Yor\`ub\'a language and
culture, into English. To evaluate these models, we present IkiniYor\`ub\'a, a
Yor\`ub\'a-English translation dataset containing some Yor\`ub\'a greetings,
and sample use cases. We analysed the performance of different multilingual NMT
systems including Google and NLLB and show that these models struggle to
accurately translate Yor\`ub\'a greetings into English. In addition, we trained
a Yor\`ub\'a-English model by finetuning an existing NMT model on the training
split of IkiniYor\`ub\'a and this achieved better performance when compared to
the pre-trained multilingual NMT models, although they were trained on a large
volume of data.",None,-1
92574386-a11b-48cf-9fef-62d64a0139e9,Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach,0.310662,1,"Named Entity Recognition (NER) aims to extract and classify entity mentions
in the text into pre-defined types (e.g., organization or person name).
Recently, many works have been proposed to shape the NER as a machine reading
comprehension problem (also termed MRC-based NER), in which entity recognition
is achieved by answering the formulated questions related to pre-defined entity
types through MRC, based on the contexts. However, these works ignore the label
dependencies among entity types, which are critical for precisely recognizing
named entities. In this paper, we propose to incorporate the label dependencies
among entity types into a multi-task learning framework for better MRC-based
NER. We decompose MRC-based NER into multiple tasks and use a self-attention
module to capture label dependencies. Comprehensive experiments on both nested
NER and flat NER datasets are conducted to validate the effectiveness of the
proposed Multi-NER. Experimental results show that Multi-NER can achieve better
performance on all datasets.",None,-1
c77768d7-68f2-4b88-8be6-1c77f4baa9a0,Erasure of Unaligned Attributes from Neural Representations,0.63754,8,"We present the Assignment-Maximization Spectral Attribute removaL (AMSAL)
algorithm, which erases information from neural representations when the
information to be erased is implicit rather than directly being aligned to each
input example. Our algorithm works by alternating between two steps. In one, it
finds an assignment of the input representations to the information to be
erased, and in the other, it creates projections of both the input
representations and the information to be erased into a joint latent space. We
test our algorithm on an extensive array of datasets, including a Twitter
dataset with multiple guarded attributes, the BiasBios dataset and the
BiasBench benchmark. The last benchmark includes four datasets with various
types of protected attributes. Our results demonstrate that bias can often be
removed in our setup. We also discuss the limitations of our approach when
there is a strong entanglement between the main task and the information to be
erased.",None,-1
8c272c41-eb8f-4703-aec4-8bc6df52fe0a,Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs,0.380221,3,"Despite the recent progress in text summarization made by large language
models (LLMs), they often generate summaries that are factually inconsistent
with original articles, known as ""hallucinations"" in text generation. Unlike
previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes
but more sophisticated ones, such as imposing cause and effect, adding false
details, overgeneralizing, etc. These hallucinations are challenging to detect
through traditional methods, which poses great challenges for improving the
factual consistency of text summarization. In this paper, we propose an
adversarially DEcoupling method to disentangle the Comprehension and
EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based
efficient training to cover the shortage of sensitivity for true and false in
the training process of LLMs. In this way, LLMs are less confused about
embellishing and understanding; thus, they can execute the instructions more
accurately and have enhanced abilities to distinguish hallucinations.
Experimental results show that DECENT significantly improves the reliability of
text summarization based on LLMs.",None,-1
e89da94b-99fb-4038-a2f3-ec1213dc19d8,Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty,0.819467,12,"We present our submission to the BabyLM challenge, whose goal was to improve
the sample efficiency of language models. We trained an ensemble consisting of
a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word
BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model,
which exceeds in performance both of its teachers as well as a similar model
trained without distillation. This suggests that distillation can not only
retain the full performance of the teacher model when the latter is trained on
a sufficiently small dataset; it can exceed it, and lead to significantly
better performance than direct training.",None,-1
b32dd8c4-d3a3-440d-b217-75230148c0f1,Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews,0.244491,5,"We propose a simple approach for weighting self-connecting edges in a Graph
Convolutional Network (GCN) and show its impact on depression detection from
transcribed clinical interviews. To this end, we use a GCN for modeling
non-consecutive and long-distance semantics to classify the transcriptions into
depressed or control subjects. The proposed method aims to mitigate the
limiting assumptions of locality and the equal importance of self-connections
vs. edges to neighboring nodes in GCNs, while preserving attractive features
such as low computational cost, data agnostic, and interpretability
capabilities. We perform an exhaustive evaluation in two benchmark datasets.
Results show that our approach consistently outperforms the vanilla GCN model
as well as previously reported results, achieving an F1=0.84 on both datasets.
Finally, a qualitative analysis illustrates the interpretability capabilities
of the proposed approach and its alignment with previous findings in
psychology.",None,-1
b3687265-4e96-45af-92d3-49fd6f0f04ac,Beyond Unimodal: Generalising Neural Processes for Multimodal Uncertainty Estimation,0.443066,4,"Uncertainty estimation is an important research area to make deep neural
networks (DNNs) more trustworthy. While extensive research on uncertainty
estimation has been conducted with unimodal data, uncertainty estimation for
multimodal data remains a challenge. Neural processes (NPs) have been
demonstrated to be an effective uncertainty estimation method for unimodal data
by providing the reliability of Gaussian processes with efficient and powerful
DNNs. While NPs hold significant potential for multimodal uncertainty
estimation, the adaptation of NPs for multimodal data has not been carefully
studied. To bridge this gap, we propose Multimodal Neural Processes (MNPs) by
generalising NPs for multimodal uncertainty estimation. Based on the framework
of NPs, MNPs consist of several novel and principled mechanisms tailored to the
characteristics of multimodal data. In extensive empirical evaluation, our
method achieves state-of-the-art multimodal uncertainty estimation performance,
showing its appealing robustness against noisy samples and reliability in
out-of-distribution detection with faster computation time compared to the
current state-of-the-art multimodal uncertainty estimation method.",None,-1
826c8b22-0d54-49f5-be16-86bce570b016,Building Manufacturing Deep Learning Models with Minimal and Imbalanced Training Data Using Domain Adaptation and Data Augmentation,0.144822,1,"Deep learning (DL) techniques are highly effective for defect detection from
images. Training DL classification models, however, requires vast amounts of
labeled data which is often expensive to collect. In many cases, not only the
available training data is limited but may also imbalanced. In this paper, we
propose a novel domain adaptation (DA) approach to address the problem of
labeled training data scarcity for a target learning task by transferring
knowledge gained from an existing source dataset used for a similar learning
task. Our approach works for scenarios where the source dataset and the dataset
available for the target learning task have same or different feature spaces.
We combine our DA approach with an autoencoder-based data augmentation approach
to address the problem of imbalanced target datasets. We evaluate our combined
approach using image data for wafer defect prediction. The experiments show its
superior performance against other algorithms when the number of labeled
samples in the target dataset is significantly small and the target dataset is
imbalanced.",None,-1
8b90975d-d146-4602-8aeb-c4210ba2f12f,OPHAvatars: One-shot Photo-realistic Head Avatars,0.084743,1,"We propose a method for synthesizing photo-realistic digital avatars from
only one portrait as the reference. Given a portrait, our method synthesizes a
coarse talking head video using driving keypoints features. And with the coarse
video, our method synthesizes a coarse talking head avatar with a deforming
neural radiance field. With rendered images of the coarse avatar, our method
updates the low-quality images with a blind face restoration model. With
updated images, we retrain the avatar for higher quality. After several
iterations, our method can synthesize a photo-realistic animatable 3D neural
head avatar. The motivation of our method is deformable neural radiance field
can eliminate the unnatural distortion caused by the image2video method. Our
method outperforms state-of-the-art methods in quantitative and qualitative
studies on various subjects.",None,-1
1d51748e-0434-4ca8-a69e-c5c461584ea8,Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost,0.437157,8,"Medical artificial general intelligence (AGI) is an emerging field that aims
to develop systems specifically designed for medical applications that possess
the ability to understand, learn, and apply knowledge across a wide range of
tasks and domains. Large language models (LLMs) represent a significant step
towards AGI. However, training cross-domain LLMs in the medical field poses
significant challenges primarily attributed to the requirement of collecting
data from diverse domains. This task becomes particularly difficult due to
privacy restrictions and the scarcity of publicly available medical datasets.
Here, we propose Medical AGI (MedAGI), a paradigm to unify domain-specific
medical LLMs with the lowest cost, and suggest a possible path to achieve
medical AGI. With an increasing number of domain-specific professional
multimodal LLMs in the medical field being developed, MedAGI is designed to
automatically select appropriate medical models by analyzing users' questions
with our novel adaptive expert selection algorithm. It offers a unified
approach to existing LLMs in the medical field, eliminating the need for
retraining regardless of the introduction of new models. This characteristic
renders it a future-proof solution in the dynamically advancing medical domain.
To showcase the resilience of MedAGI, we conducted an evaluation across three
distinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysis
of pathology pictures. The results demonstrated that MedAGI exhibited
remarkable versatility and scalability, delivering exceptional performance
across diverse domains. Our code is publicly available to facilitate further
research at https://github.com/JoshuaChou2018/MedAGI.",None,-1
76e49333-1132-409a-8b45-9d3af5f7f8c9,Cross-Cultural Transfer Learning for Chinese Offensive Language Detection,0.4725,13,"Detecting offensive language is a challenging task. Generalizing across
different cultures and languages becomes even more challenging: besides
lexical, syntactic and semantic differences, pragmatic aspects such as cultural
norms and sensitivities, which are particularly relevant in this context, vary
greatly. In this paper, we target Chinese offensive language detection and aim
to investigate the impact of transfer learning using offensive language
detection data from different cultural backgrounds, specifically Korean and
English. We find that culture-specific biases in what is considered offensive
negatively impact the transferability of language models (LMs) and that LMs
trained on diverse cultural data are sensitive to different features in Chinese
offensive language detection. In a few-shot learning scenario, however, our
study shows promising prospects for non-English offensive language detection
with limited resources. Our findings highlight the importance of cross-cultural
transfer learning in improving offensive language detection and promoting
inclusive digital spaces.",None,-1
6c12fd68-a44f-44c1-9b84-764f5e93496b,Self-supervised Pre-training with Masked Shape Prediction for 3D Scene Understanding,0.522717,8,"Masked signal modeling has greatly advanced self-supervised pre-training for
language and 2D images. However, it is still not fully explored in 3D scene
understanding. Thus, this paper introduces Masked Shape Prediction (MSP), a new
framework to conduct masked signal modeling in 3D scenes. MSP uses the
essential 3D semantic cue, i.e., geometric shape, as the prediction target for
masked points. The context-enhanced shape target consisting of explicit shape
context and implicit deep shape feature is proposed to facilitate exploiting
contextual cues in shape prediction. Meanwhile, the pre-training architecture
in MSP is carefully designed to alleviate the masked shape leakage from point
coordinates. Experiments on multiple 3D understanding tasks on both indoor and
outdoor datasets demonstrate the effectiveness of MSP in learning good feature
representations to consistently boost downstream performance.",None,-1
e8e82fd9-847d-4432-a769-7f4348125d82,Unsupervised Meta-Learning via Few-shot Pseudo-supervised Contrastive Learning,0.527867,11,"Unsupervised meta-learning aims to learn generalizable knowledge across a
distribution of tasks constructed from unlabeled data. Here, the main challenge
is how to construct diverse tasks for meta-learning without label information;
recent works have proposed to create, e.g., pseudo-labeling via pretrained
representations or creating synthetic samples via generative models. However,
such a task construction strategy is fundamentally limited due to heavy
reliance on the immutable pseudo-labels during meta-learning and the quality of
the representations or the generated samples. To overcome the limitations, we
propose a simple yet effective unsupervised meta-learning framework, coined
Pseudo-supervised Contrast (PsCo), for few-shot classification. We are inspired
by the recent self-supervised learning literature; PsCo utilizes a momentum
network and a queue of previous batches to improve pseudo-labeling and
construct diverse tasks in a progressive manner. Our extensive experiments
demonstrate that PsCo outperforms existing unsupervised meta-learning methods
under various in-domain and cross-domain few-shot classification benchmarks. We
also validate that PsCo is easily scalable to a large-scale benchmark, while
recent prior-art meta-schemes are not.",None,-1
02ddfce5-7c2d-48a5-b53d-cf1cb07a3608,Graph-based Asynchronous Event Processing for Rapid Object Recognition,0.976846,64,"Different from traditional video cameras, event cameras capture asynchronous
events stream in which each event encodes pixel location, trigger time, and the
polarity of the brightness changes. In this paper, we introduce a novel
graph-based framework for event cameras, namely SlideGCN. Unlike some recent
graph-based methods that use groups of events as input, our approach can
efficiently process data event-by-event, unlock the low latency nature of
events data while still maintaining the graph's structure internally. For fast
graph construction, we develop a radius search algorithm, which better exploits
the partial regular structure of event cloud against k-d tree based generic
methods. Experiments show that our method reduces the computational complexity
up to 100 times with respect to current graph-based methods while keeping
state-of-the-art performance on object recognition. Moreover, we verify the
superiority of event-wise processing with our method. When the state becomes
stable, we can give a prediction with high confidence, thus making an early
recognition. Project page: \url{https://zju3dv.github.io/slide_gcn/}.",None,-1
fbe5531f-4378-4c03-83c3-4200ed2fe6f4,Holy Grail 2.0: From Natural Language to Constraint Models,0.5314,4,"Twenty-seven years ago, E. Freuder highlighted that ""Constraint programming
represents one of the closest approaches computer science has yet made to the
Holy Grail of programming: the user states the problem, the computer solves
it"". Nowadays, CP users have great modeling tools available (like Minizinc and
CPMpy), allowing them to formulate the problem and then let a solver do the
rest of the job, getting closer to the stated goal. However, this still
requires the CP user to know the formalism and respect it. Another significant
challenge lies in the expertise required to effectively model combinatorial
problems. All this limits the wider adoption of CP. In this position paper, we
investigate a possible approach to leverage pre-trained Large Language Models
to extract models from textual problem descriptions. More specifically, we take
inspiration from the Natural Language Processing for Optimization (NL4OPT)
challenge and present early results with a decomposition-based prompting
approach to GPT Models.",None,-1
a0e11955-ab0c-4e80-8d1f-6785fd434117,Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning,0.881903,64,"Prompt tuning, in which a base pretrained model is adapted to each task via
conditioning on learned prompt vectors, has emerged as a promising approach for
efficiently adapting large language models to multiple downstream tasks.
However, existing methods typically learn soft prompt vectors from scratch, and
it has not been clear how to exploit the rich cross-task knowledge with prompt
vectors in a multitask learning setting. We propose multitask prompt tuning
(MPT), which first learns a single transferable prompt by distilling knowledge
from multiple task-specific source prompts. We then learn multiplicative low
rank updates to this shared prompt to efficiently adapt it to each downstream
target task. Extensive experiments on 23 NLP datasets demonstrate that our
proposed approach outperforms the state-of-the-art methods, including the full
finetuning baseline in some cases, despite only tuning 0.035% as many
task-specific parameters.",None,-1
7b37e3ed-3c7e-4afb-8bb1-f748b93839dc,Personalized Abstractive Summarization by Tri-agent Generation Pipeline,0.272962,3,"Tailoring outputs from large language models, like ChatGPT, to implicit user
preferences remains a challenge despite their impressive generative
capabilities. In this paper, we propose a tri-agent generation pipeline
comprising a generator, an instructor, and an editor to enhance output
personalization. The generator produces an initial output, the instructor
automatically generates editing instructions based on user preferences, and the
editor refines the output to align with those preferences. The inference-only
large language model (ChatGPT) serves as both the generator and editor, with a
smaller model acting as the instructor to guide output generation. We train the
instructor using editor-steered reinforcement learning, leveraging feedback
from a large-scale editor model to optimize instruction generation.
Experimental results on two abstractive summarization datasets demonstrate the
effectiveness of our approach in generating outputs that better meet user
expectations. Code is available at
\url{https://github.com/Wendy-Xiao/chatgpt_editing_summ}",None,-1
1938fdc3-1dd4-4add-93ef-786368060978,Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models,0.441321,44,"We investigate security concerns of the emergent instruction tuning paradigm,
that models are trained on crowdsourced datasets with task instructions to
achieve superior performance. Our studies demonstrate that an attacker can
inject backdoors by issuing very few malicious instructions (~1000 tokens) and
control model behavior through data poisoning, without even the need to modify
data instances or labels themselves. Through such instruction attacks, the
attacker can achieve over 90% attack success rate across four commonly used NLP
datasets. As an empirical study on instruction attacks, we systematically
evaluated unique perspectives of instruction attacks, such as poison transfer
where poisoned models can transfer to 15 diverse generative datasets in a
zero-shot manner; instruction transfer where attackers can directly apply
poisoned instruction on many other datasets; and poison resistance to continual
finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate
such backdoors to some degree. These findings highlight the need for more
robust defenses against poisoning attacks in instruction-tuning models and
underscore the importance of ensuring data quality in instruction
crowdsourcing.",None,-1
a93237ba-155a-47d1-8243-9467cabacb5c,BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations,0.0357884,1,"Coreference Resolution is a well studied problem in NLP. While widely studied
for English and other resource-rich languages, research on coreference
resolution in Bengali largely remains unexplored due to the absence of relevant
datasets. Bengali, being a low-resource language, exhibits greater
morphological richness compared to English. In this article, we introduce a new
dataset, BenCoref, comprising coreference annotations for Bengali texts
gathered from four distinct domains. This relatively small dataset contains
5200 mention annotations forming 502 mention clusters within 48,569 tokens. We
describe the process of creating this dataset and report performance of
multiple models trained using BenCoref. We expect that our work provides some
valuable insights on the variations in coreference phenomena across several
domains in Bengali and encourages the development of additional resources for
Bengali. Furthermore, we found poor crosslingual performance at zero-shot
setting from English, highlighting the need for more language-specific
resources for this task.",None,-1
a8b8079c-f940-4aab-b189-5765b6d55579,Running cognitive evaluations on large language models: The do's and the don'ts,0.911715,7,"In this paper, I describe methodological considerations for studies that aim
to evaluate the cognitive capacities of large language models (LLMs) using
language-based behavioral assessments. Drawing on three case studies from the
literature (a commonsense knowledge benchmark, a theory of mind evaluation, and
a test of syntactic agreement), I describe common pitfalls that might arise
when applying a cognitive test to an LLM. I then list 10 do's and don'ts that
should help design high-quality cognitive evaluations for AI systems. I
conclude by discussing four areas where the do's and don'ts are currently under
active discussion -- prompt sensitivity, cultural and linguistic diversity,
using LLMs as research assistants, and running evaluations on open vs. closed
LLMs. Overall, the goal of the paper is to contribute to the broader discussion
of best practices in the rapidly growing field of AI Psychology.",None,-1
22744367-1aa2-461f-9b4f-1e45cdacaf8f,Stabilizing RLHF through Advantage Model and Selective Rehearsal,0.323814,12,"Large Language Models (LLMs) have revolutionized natural language processing,
yet aligning these models with human values and preferences using RLHF remains
a significant challenge. This challenge is characterized by various
instabilities, such as reward hacking and catastrophic forgetting. In this
technical report, we propose two innovations to stabilize RLHF training: 1)
Advantage Model, which directly models advantage score i.e., extra reward
compared to the expected rewards and regulates score distributions across tasks
to prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic
forgetting by strategically selecting data for PPO training and knowledge
rehearsing. Our experimental analysis on public and proprietary datasets
reveals that the proposed methods not only increase stability in RLHF training
but also achieve higher reward scores and win rates.",None,-1
eaaa3fc9-3717-4ccb-bdfa-7e407e2017cb,SFD2: Semantic-guided Feature Detection and Description,0.90005,15,"Visual localization is a fundamental task for various applications including
autonomous driving and robotics. Prior methods focus on extracting large
amounts of often redundant locally reliable features, resulting in limited
efficiency and accuracy, especially in large-scale environments under
challenging conditions. Instead, we propose to extract globally reliable
features by implicitly embedding high-level semantics into both the detection
and description processes. Specifically, our semantic-aware detector is able to
detect keypoints from reliable regions (e.g. building, traffic lane) and
suppress unreliable areas (e.g. sky, car) implicitly instead of relying on
explicit semantic labels. This boosts the accuracy of keypoint matching by
reducing the number of features sensitive to appearance changes and avoiding
the need of additional segmentation networks at test time. Moreover, our
descriptors are augmented with semantics and have stronger discriminative
ability, providing more inliers at test time. Particularly, experiments on
long-term large-scale visual localization Aachen Day-Night and RobotCar-Seasons
datasets demonstrate that our model outperforms previous local features and
gives competitive accuracy to advanced matchers but is about 2 and 3 times
faster when using 2k and 4k keypoints, respectively.",None,-1
3194dddf-acda-43a2-b88c-0457eb99ad1d,Fine-Tashkeel: Finetuning Byte-Level Models for Accurate Arabic Text Diacritization,0.524996,2,"Most of previous work on learning diacritization of the Arabic language
relied on training models from scratch. In this paper, we investigate how to
leverage pre-trained language models to learn diacritization. We finetune
token-free pre-trained multilingual models (ByT5) to learn to predict and
insert missing diacritics in Arabic text, a complex task that requires
understanding the sentence semantics and the morphological structure of the
tokens. We show that we can achieve state-of-the-art on the diacritization task
with minimal amount of training and no feature engineering, reducing WER by
40%. We release our finetuned models for the greater benefit of the researchers
in the community.",None,-1
22abd9f5-6973-471c-824a-8c6da578463a,Improving Pacing in Long-Form Story Planning,0.0778589,1,"Existing LLM-based systems for writing long-form stories or story outlines
frequently suffer from unnatural pacing, whether glossing over important events
or over-elaborating on insignificant details, resulting in a jarring experience
for the reader. We propose a CONCrete Outline ConTrol (CONCOCT) system to
improve pacing when automatically generating story outlines. We first train a
concreteness evaluator to judge which of two events is more concrete
(low-level-detailed). This evaluator can then be used to control pacing in
hierarchical outline generation; in this work, we explore a vaguest-first
expansion procedure that aims for uniform pacing. We further use the evaluator
to filter new outline items based on predicted concreteness. Compared to a
baseline hierarchical outline generator, humans judge CONCOCT's pacing to be
more consistent over 57% of the time across multiple outline lengths; the gains
also translate to downstream stories. All code, data, and models are
open-sourced.",None,-1
cad6baa3-f348-4fa7-b30a-208aade330f9,Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer,0.43445,2,"Recent research has shown that independently trained encoders and decoders,
combined through a shared fixed-size representation, can achieve competitive
performance in speech-to-text translation. In this work, we show that this type
of approach can be further improved with multilingual training. We observe
significant improvements in zero-shot cross-modal speech translation, even
outperforming a supervised approach based on XLSR for several languages.",None,-1
8f659afb-cba8-4d08-956e-3d6dad722ad2,A Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3,0.832124,35,"LawGPT 1.0 is a virtual legal assistant built on the state-of-the-art
language model GPT-3, fine-tuned for the legal domain. The system is designed
to provide legal assistance to users in a conversational manner, helping them
with tasks such as answering legal questions, generating legal documents, and
providing legal advice. In this paper, we provide a brief overview of LawGPT
1.0, its architecture, and its performance on a set of legal benchmark tasks.
Please note that the detailed information about the model is protected by a
non-disclosure agreement (NDA) and cannot be disclosed in this report.",None,-1
52ed57cb-3213-40c5-920b-22e0366d3314,Balanced Supervised Contrastive Learning for Few-Shot Class-Incremental Learning,0.42661,2,"Few-shot class-incremental learning (FSCIL) presents the primary challenge of
balancing underfitting to a new session's task and forgetting the tasks from
previous sessions. To address this challenge, we develop a simple yet powerful
learning scheme that integrates effective methods for each core component of
the FSCIL network, including the feature extractor, base session classifiers,
and incremental session classifiers. In feature extractor training, our goal is
to obtain balanced generic representations that benefit both current viewable
and unseen or past classes. To achieve this, we propose a balanced supervised
contrastive loss that effectively balances these two objectives. In terms of
classifiers, we analyze and emphasize the importance of unifying initialization
methods for both the base and incremental session classifiers. Our method
demonstrates outstanding ability for new task learning and preventing
forgetting on CUB200, CIFAR100, and miniImagenet datasets, with significant
improvements over previous state-of-the-art methods across diverse metrics. We
conduct experiments to analyze the significance and rationale behind our
approach and visualize the effectiveness of our representations on new tasks.
Furthermore, we conduct diverse ablation studies to analyze the effects of each
module.",None,-1
67b4452a-d03d-42db-96b2-1a0b0e1cd93d,Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West,0.35998,15,"Large Language Models (LLMs), now used daily by millions of users, can encode
societal biases, exposing their users to representational harms. A large body
of scholarship on LLM bias exists but it predominantly adopts a Western-centric
frame and attends comparatively less to bias levels and potential harms in the
Global South. In this paper, we quantify stereotypical bias in popular LLMs
according to an Indian-centric frame and compare bias levels between the Indian
and Western contexts. To do this, we develop a novel dataset which we call
Indian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and
anti-stereotypical examples for caste and religion contexts. We find that the
majority of LLMs tested are strongly biased towards stereotypes in the Indian
context, especially as compared to the Western context. We finally investigate
Instruction Prompting as a simple intervention to mitigate such bias and find
that it significantly reduces both stereotypical and anti-stereotypical biases
in the majority of cases for GPT-3.5. The findings of this work highlight the
need for including more diverse voices when evaluating LLMs.",None,-1
0bdaa4d0-aef2-4486-b074-81d08a591b85,Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT -- A Text-to-SQL Parsing Comparison,0.624209,5,"The success of ChatGPT has ignited an AI race, with researchers striving to
develop new large language models (LLMs) that can match or surpass the language
understanding and generation abilities of commercial ones. In recent times, a
number of models have emerged, claiming performance near that of GPT-3.5 or
GPT-4 through various instruction-tuning methods. As practitioners of
Text-to-SQL parsing, we are grateful for their valuable contributions to
open-source research. However, it is important to approach these claims with a
sense of scrutiny and ascertain the actual effectiveness of these models.
Therefore, we pit six popular large language models against each other,
systematically evaluating their Text-to-SQL parsing capability on nine
benchmark datasets with five different prompting strategies, covering both
zero-shot and few-shot scenarios. Regrettably, the open-sourced models fell
significantly short of the performance achieved by closed-source models like
GPT-3.5, highlighting the need for further work to bridge the performance gap
between these models.",None,-1
186abe1b-f67a-4454-8483-2da55b16e892,Generative Novel View Synthesis with 3D-Aware Diffusion Models,0.996175,144,"We present a diffusion-based model for 3D-aware generative novel view
synthesis from as few as a single input image. Our model samples from the
distribution of possible renderings consistent with the input and, even in the
presence of ambiguity, is capable of rendering diverse and plausible novel
views. To achieve this, our method makes use of existing 2D diffusion backbones
but, crucially, incorporates geometry priors in the form of a 3D feature
volume. This latent feature field captures the distribution over possible scene
representations and improves our method's ability to generate view-consistent
novel renderings. In addition to generating novel views, our method has the
ability to autoregressively synthesize 3D-consistent sequences. We demonstrate
state-of-the-art results on synthetic renderings and room-scale scenes; we also
show compelling results for challenging, real-world objects.",None,-1
62c74a8a-d7aa-40bd-8304-fa7ffd4f1f20,Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models,0.233794,3,"Few-shot or zero-shot fact verification only relies on a few or no labeled
training examples. In this paper, we propose a novel method called ProToCo, to
\underline{Pro}mpt pre-trained language models (PLMs) \underline{To} be
\underline{Co}nsistent, for improving the factuality assessment capability of
PLMs in the few-shot and zero-shot settings. Given a claim-evidence pair,
ProToCo generates multiple variants of the claim with different relations and
frames a simple consistency mechanism as constraints for making compatible
predictions across these variants. We update PLMs by using parameter-efficient
fine-tuning (PEFT), leading to more accurate predictions in few-shot and
zero-shot fact verification tasks. Our experiments on three public verification
datasets show that ProToCo significantly outperforms state-of-the-art few-shot
fact verification baselines. With a small number of unlabeled instances,
ProToCo also outperforms the strong zero-shot learner T0 on zero-shot
verification. Compared to large PLMs using in-context learning (ICL) method,
ProToCo outperforms OPT-30B and the Self-Consistency-enabled OPT-6.7B model in
both few- and zero-shot settings.",None,-1
5385c84e-7e07-448d-ab18-9816c25e58e4,Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing,0.694765,29,"We present Impossible Distillation, a novel framework for paraphrasing and
sentence summarization, that distills a high-quality dataset and model from a
low-quality teacher that itself cannot perform these tasks. Unlike prior works
that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific
architecture, we hypothesize and verify the paraphrastic proximity intrinsic to
pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in
the LM distribution. By identifying and distilling generations from these
subspaces, Impossible Distillation produces a high-quality dataset and model
even from GPT2-scale LMs. We evaluate our method on multiple benchmarks
spanning unconstrained / syntax-controlled paraphrase generation and sentence
summarization. Our model with 770M parameters consistently outperforms strong
baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT
itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher
diversity and fidelity than up to 13 times larger datasets.",None,-1
83586ae9-94de-4774-9b36-8a6938870e10,Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation,0.286218,2,"Goal-oriented Script Generation is a new task of generating a list of steps
that can fulfill the given goal. In this paper, we propose to extend the task
from the perspective of cognitive theory. Instead of a simple flat structure,
the steps are typically organized hierarchically - Human often decompose a
complex task into subgoals, where each subgoal can be further decomposed into
steps. To establish the benchmark, we contribute a new dataset, propose several
baseline methods, and set up evaluation metrics. Both automatic and human
evaluation verify the high-quality of dataset, as well as the effectiveness of
incorporating subgoals into hierarchical script generation. Furthermore, We
also design and evaluate the model to discover subgoal, and find that it is a
bit more difficult to decompose the goals than summarizing from segmented
steps.",None,-1
352120a6-cf9d-44ee-8240-af3c6cdd3f14,R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces,0.264442,1,"This paper introduces Robust Spin (R-Spin), a data-efficient domain-specific
self-supervision method for speaker and noise-invariant speech representations
by learning discrete acoustic units with speaker-invariant clustering (Spin).
R-Spin resolves Spin's issues and enhances content representations by learning
to predict acoustic pieces. R-Spin offers a 12X reduction in computational
resources compared to previous state-of-the-art methods while outperforming
them in severely distorted speech scenarios. This paper provides detailed
analyses to show how discrete units contribute to speech encoder training and
improving robustness in diverse acoustic environments.",None,-1
21e54c28-af36-4b2c-a175-0e6fbf7034ed,ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus,0.831193,3,"We introduce \`{I}r\`{o}y\`{i}nSpeech, a new corpus influenced by the desire
to increase the amount of high quality, contemporary Yor\`{u}b\'{a} speech
data, which can be used for both Text-to-Speech (TTS) and Automatic Speech
Recognition (ASR) tasks. We curated about 23000 text sentences from news and
creative writing domains with the open license CC-BY-4.0. To encourage a
participatory approach to data creation, we provide 5000 curated sentences to
the Mozilla Common Voice platform to crowd-source the recording and validation
of Yor\`{u}b\'{a} speech data. In total, we created about 42 hours of speech
data recorded by 80 volunteers in-house, and 6 hours of validated recordings on
Mozilla Common Voice platform. Our TTS evaluation suggests that a
high-fidelity, general domain, single-speaker Yor\`{u}b\'{a} voice is possible
with as little as 5 hours of speech. Similarly, for ASR we obtained a baseline
word error rate (WER) of 23.8.",None,-1
b687a32c-6686-49d4-be6f-a30e04e2d023,AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing,0.710102,3,"The intensive care unit (ICU) is a specialized hospital space where
critically ill patients receive intensive care and monitoring. Comprehensive
monitoring is imperative in assessing patients conditions, in particular
acuity, and ultimately the quality of care. However, the extent of patient
monitoring in the ICU is limited due to time constraints and the workload on
healthcare providers. Currently, visual assessments for acuity, including fine
details such as facial expressions, posture, and mobility, are sporadically
captured, or not captured at all. These manual observations are subjective to
the individual, prone to documentation errors, and overburden care providers
with the additional workload. Artificial Intelligence (AI) enabled systems has
the potential to augment the patient visual monitoring and assessment due to
their exceptional learning capabilities. Such systems require robust annotated
data to train. To this end, we have developed pervasive sensing and data
processing system which collects data from multiple modalities depth images,
color RGB images, accelerometry, electromyography, sound pressure, and light
levels in ICU for developing intelligent monitoring systems for continuous and
granular acuity, delirium risk, pain, and mobility assessment. This paper
presents the Intelligent Intensive Care Unit (I2CU) system architecture we
developed for real-time patient monitoring and visual assessment.",None,-1
848d00bf-150a-4588-8ba1-45d71d5471d2,Real-Time Simultaneous Localization and Mapping with LiDAR intensity,0.582636,3,"We propose a novel real-time LiDAR intensity image-based simultaneous
localization and mapping method , which addresses the geometry degeneracy
problem in unstructured environments. Traditional LiDAR-based front-end
odometry mostly relies on geometric features such as points, lines and planes.
A lack of these features in the environment can lead to the failure of the
entire odometry system. To avoid this problem, we extract feature points from
the LiDAR-generated point cloud that match features identified in LiDAR
intensity images. We then use the extracted feature points to perform scan
registration and estimate the robot ego-movement. For the back-end, we jointly
optimize the distance between the corresponding feature points, and the point
to plane distance for planes identified in the map. In addition, we use the
features extracted from intensity images to detect loop closure candidates from
previous scans and perform pose graph optimization. Our experiments show that
our method can run in real time with high accuracy and works well with
illumination changes, low-texture, and unstructured environments.",None,-1
c9f56b6a-4a31-4438-8abe-9e6180afa35a,Speech-Gesture GAN: Gesture Generation for Robots and Embodied Agents,0.025813,1,"Embodied agents, in the form of virtual agents or social robots, are rapidly
becoming more widespread. In human-human interactions, humans use nonverbal
behaviours to convey their attitudes, feelings, and intentions. Therefore, this
capability is also required for embodied agents in order to enhance the quality
and effectiveness of their interactions with humans. In this paper, we propose
a novel framework that can generate sequences of joint angles from the speech
text and speech audio utterances. Based on a conditional Generative Adversarial
Network (GAN), our proposed neural network model learns the relationships
between the co-speech gestures and both semantic and acoustic features from the
speech input. In order to train our neural network model, we employ a public
dataset containing co-speech gestures with corresponding speech audio
utterances, which were captured from a single male native English speaker. The
results from both objective and subjective evaluations demonstrate the efficacy
of our gesture-generation framework for Robots and Embodied Agents.",None,-1
9907ea87-ecab-438e-b176-f267d1816214,Making Large Language Models Better Knowledge Miners for Online Marketing with Progressive Prompting Augmentation,0.396987,2,"Nowadays, the rapid development of mobile economy has promoted the
flourishing of online marketing campaigns, whose success greatly hinges on the
efficient matching between user preferences and desired marketing campaigns
where a well-established Marketing-oriented Knowledge Graph (dubbed as MoKG)
could serve as the critical ""bridge"" for preference propagation. In this paper,
we seek to carefully prompt a Large Language Model (LLM) with domain-level
knowledge as a better marketing-oriented knowledge miner for marketing-oriented
knowledge graph construction, which is however non-trivial, suffering from
several inevitable issues in real-world marketing scenarios, i.e.,
uncontrollable relation generation of LLMs,insufficient prompting ability of a
single prompt, the unaffordable deployment cost of LLMs. To this end, we
propose PAIR, a novel Progressive prompting Augmented mIning fRamework for
harvesting marketing-oriented knowledge graph with LLMs. In particular, we
reduce the pure relation generation to an LLM based adaptive relation filtering
process through the knowledge-empowered prompting technique. Next, we steer
LLMs for entity expansion with progressive prompting augmentation,followed by a
reliable aggregation with comprehensive consideration of both self-consistency
and semantic relatedness. In terms of online serving, we specialize in a small
and white-box PAIR (i.e.,LightPAIR),which is fine-tuned with a high-quality
corpus provided by a strong teacher-LLM. Extensive experiments and practical
applications in audience targeting verify the effectiveness of the proposed
(Light)PAIR.",None,-1
0c4a2155-82a3-4322-8c01-29be71c9dd78,SVDiff: Compact Parameter Space for Diffusion Fine-Tuning,0.970635,138,"Diffusion models have achieved remarkable success in text-to-image
generation, enabling the creation of high-quality images from text prompts or
other modalities. However, existing methods for customizing these models are
limited by handling multiple personalized subjects and the risk of overfitting.
Moreover, their large number of parameters is inefficient for model storage. In
this paper, we propose a novel approach to address these limitations in
existing text-to-image diffusion models for personalization. Our method
involves fine-tuning the singular values of the weight matrices, leading to a
compact and efficient parameter space that reduces the risk of overfitting and
language drifting. We also propose a Cut-Mix-Unmix data-augmentation technique
to enhance the quality of multi-subject image generation and a simple
text-based image editing framework. Our proposed SVDiff method has a
significantly smaller model size compared to existing methods (approximately
2,200 times fewer parameters compared with vanilla DreamBooth), making it more
practical for real-world applications.",None,-1
43766e22-6bad-47b5-867d-67d51165fe85,TagCLIP: Improving Discrimination Ability of Open-Vocabulary Semantic Segmentation,0.450764,11,"Recent success of Contrastive Language-Image Pre-training~(CLIP) has shown
great promise in pixel-level open-vocabulary learning tasks. A general paradigm
utilizes CLIP's text and patch embeddings to generate semantic masks. However,
existing models easily misidentify input pixels from unseen classes, thus
confusing novel classes with semantically-similar ones. In our work, we
disentangle the ill-posed optimization problem into two parallel processes: one
performs semantic matching individually, and the other judges reliability for
improving discrimination ability. Motivated by special tokens in language
modeling that represents sentence-level embeddings, we design a trusty token
that decouples the known and novel category prediction tendency. With almost no
extra overhead, we upgrade the pixel-level generalization capacity of existing
models effectively. Our TagCLIP (CLIP adapting with Trusty-guidance) boosts the
IoU of unseen classes by 7.4% and 1.7% on PASCAL VOC 2012 and COCO-Stuff 164K.",None,-1
1ba520ef-8482-4c2c-886d-c3603f998cb6,Humanoid Agents: Platform for Simulating Human-like Generative Agents,0.697962,25,"Just as computational simulations of atoms, molecules and cells have shaped
the way we study the sciences, true-to-life simulations of human-like agents
can be valuable tools for studying human behavior. We propose Humanoid Agents,
a system that guides Generative Agents to behave more like humans by
introducing three elements of System 1 processing: Basic needs (e.g. hunger,
health and energy), Emotion and Closeness in Relationships. Humanoid Agents are
able to use these dynamic elements to adapt their daily activities and
conversations with other agents, as supported with empirical experiments. Our
system is designed to be extensible to various settings, three of which we
demonstrate, as well as to other elements influencing human behavior (e.g.
empathy, moral values and cultural background). Our platform also includes a
Unity WebGL game interface for visualization and an interactive analytics
dashboard to show agent statuses over time. Our platform is available on
https://www.humanoidagents.com/ and code is on
https://github.com/HumanoidAgents/HumanoidAgents",None,-1
7c455475-f17b-4dfc-bf97-c20092a62687,Directed Acyclic Graphs With Tears,0.310599,3,"Bayesian network is a frequently-used method for fault detection and
diagnosis in industrial processes. The basis of Bayesian network is structure
learning which learns a directed acyclic graph (DAG) from data. However, the
search space will scale super-exponentially with the increase of process
variables, which makes the data-driven structure learning a challenging
problem. To this end, the DAGs with NOTEARs methods are being well studied not
only for their conversion of the discrete optimization into continuous
optimization problem but also their compatibility with deep learning framework.
Nevertheless, there still remain challenges for NOTEAR-based methods: 1) the
infeasible solution results from the gradient descent-based optimization
paradigm; 2) the truncation operation to promise the learned graph acyclic. In
this work, the reason for challenge 1) is analyzed theoretically, and a novel
method named DAGs with Tears method is proposed based on mix-integer
programming to alleviate challenge 2). In addition, prior knowledge is able to
incorporate into the new proposed method, making structure learning more
practical and useful in industrial processes. Finally, a numerical example and
an industrial example are adopted as case studies to demonstrate the
superiority of the developed method.",None,-1
4150298c-0a7f-4a05-9dc2-2d8a543d7917,Examining European Press Coverage of the Covid-19 No-Vax Movement: An NLP Framework,0.024625,1,"This paper examines how the European press dealt with the no-vax reactions
against the Covid-19 vaccine and the dis- and misinformation associated with
this movement. Using a curated dataset of 1786 articles from 19 European
newspapers on the anti-vaccine movement over a period of 22 months in
2020-2021, we used Natural Language Processing techniques including topic
modeling, sentiment analysis, semantic relationship with word embeddings,
political analysis, named entity recognition, and semantic networks, to
understand the specific role of the European traditional press in the
disinformation ecosystem. The results of this multi-angle analysis demonstrate
that the European well-established press actively opposed a variety of hoaxes
mainly spread on social media, and was critical of the anti-vax trend,
regardless of the political orientation of the newspaper. This confirms the
relevance of studying the role of high-quality press in the disinformation
ecosystem.",None,-1
520b94de-ab6f-4630-8aca-3bbdcbb5eca0,AnyFlow: Arbitrary Scale Optical Flow with Implicit Neural Representation,0.799138,8,"To apply optical flow in practice, it is often necessary to resize the input
to smaller dimensions in order to reduce computational costs. However,
downsizing inputs makes the estimation more challenging because objects and
motion ranges become smaller. Even though recent approaches have demonstrated
high-quality flow estimation, they tend to fail to accurately model small
objects and precise boundaries when the input resolution is lowered,
restricting their applicability to high-resolution inputs. In this paper, we
introduce AnyFlow, a robust network that estimates accurate flow from images of
various resolutions. By representing optical flow as a continuous
coordinate-based representation, AnyFlow generates outputs at arbitrary scales
from low-resolution inputs, demonstrating superior performance over prior works
in capturing tiny objects with detail preservation on a wide range of scenes.
We establish a new state-of-the-art performance of cross-dataset generalization
on the KITTI dataset, while achieving comparable accuracy on the online
benchmarks to other SOTA methods.",None,-1
6d23c438-a719-47b3-8d39-c9870412f1eb,Parameter-efficient Modularised Bias Mitigation via AdapterFusion,0.857569,18,"Large pre-trained language models contain societal biases and carry along
these biases to downstream tasks. Current in-processing bias mitigation
approaches (like adversarial training) impose debiasing by updating a model's
parameters, effectively transferring the model to a new, irreversible debiased
state. In this work, we propose a novel approach to develop stand-alone
debiasing functionalities separate from the model, which can be integrated into
the model on-demand, while keeping the core model untouched. Drawing from the
concept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing
with Adapter Modules) - a debiasing approach to first encapsulate arbitrary
bias mitigation functionalities into separate adapters, and then add them to
the model on-demand in order to deliver fairness qualities. We conduct a large
set of experiments on three classification tasks with gender, race, and age as
protected attributes. Our results show that DAM improves or maintains the
effectiveness of bias mitigation, avoids catastrophic forgetting in a
multi-attribute scenario, and maintains on-par task performance, while granting
parameter-efficiency and easy switching between the original and debiased
models.",None,-1
31ed07ec-d150-4754-9205-fb806debe962,Large Language Models Enable Few-Shot Clustering,0.999304,18,"Unlike traditional unsupervised clustering, semi-supervised clustering allows
users to provide meaningful structure to the data, which helps the clustering
algorithm to match the user's intent. Existing approaches to semi-supervised
clustering require a significant amount of feedback from an expert to improve
the clusters. In this paper, we ask whether a large language model can amplify
an expert's guidance to enable query-efficient, few-shot semi-supervised text
clustering. We show that LLMs are surprisingly effective at improving
clustering. We explore three stages where LLMs can be incorporated into
clustering: before clustering (improving input features), during clustering (by
providing constraints to the clusterer), and after clustering (using LLMs
post-correction). We find incorporating LLMs in the first two stages can
routinely provide significant improvements in cluster quality, and that LLMs
enable a user to make trade-offs between cost and accuracy to produce desired
clusters. We release our code and LLM prompts for the public to use.",None,-1
6344456f-89a0-497f-8a7a-d861a031a039,PRODIGy: a PROfile-based DIalogue Generation dataset,0.0406199,1,"Providing dialogue agents with a profile representation can improve their
consistency and coherence, leading to better conversations. However, current
profile-based dialogue datasets for training such agents contain either
explicit profile representations that are simple and dialogue-specific, or
implicit representations that are difficult to collect. In this work, we
propose a unified framework in which we bring together both standard and more
sophisticated profile representations by creating a new resource where each
dialogue is aligned with all possible speaker representations such as
communication style, biographies, and personality. This framework allows to
test several baselines built using generative language models with several
profile configurations. The automatic evaluation shows that profile-based
models have better generalisation capabilities than models trained on dialogues
only, both in-domain and cross-domain settings. These results are consistent
for fine-tuned models and instruction-based LLMs. Additionally, human
evaluation demonstrates a clear preference for generations consistent with both
profile and context. Finally, to account for possible privacy concerns, all
experiments are done under two configurations: inter-character and
intra-character. In the former, the LM stores the information about the
character in its internal representation, while in the latter, the LM does not
retain any personal information but uses it only at inference time.",None,-1
fd643521-fe8d-470b-827b-2e725bae1509,MAPS: A Noise-Robust Progressive Learning Approach for Source-Free Domain Adaptive Keypoint Detection,0.0939712,3,"Existing cross-domain keypoint detection methods always require accessing the
source data during adaptation, which may violate the data privacy law and pose
serious security concerns. Instead, this paper considers a realistic problem
setting called source-free domain adaptive keypoint detection, where only the
well-trained source model is provided to the target domain. For the challenging
problem, we first construct a teacher-student learning baseline by stabilizing
the predictions under data augmentation and network ensembles. Built on this,
we further propose a unified approach, Mixup Augmentation and Progressive
Selection (MAPS), to fully exploit the noisy pseudo labels of unlabeled target
data during training. On the one hand, MAPS regularizes the model to favor
simple linear behavior in-between the target samples via self-mixup
augmentation, preventing the model from over-fitting to noisy predictions. On
the other hand, MAPS employs the self-paced learning paradigm and progressively
selects pseudo-labeled samples from `easy' to `hard' into the training process
to reduce noise accumulation. Results on four keypoint detection datasets show
that MAPS outperforms the baseline and achieves comparable or even better
results in comparison to previous non-source-free counterparts.",None,-1
429225bd-c3a7-4c53-8722-bdf7daae9d30,MSdocTr-Lite: A Lite Transformer for Full Page Multi-script Handwriting Recognition,0.510608,5,"The Transformer has quickly become the dominant architecture for various
pattern recognition tasks due to its capacity for long-range representation.
However, transformers are data-hungry models and need large datasets for
training. In Handwritten Text Recognition (HTR), collecting a massive amount of
labeled data is a complicated and expensive task. In this paper, we propose a
lite transformer architecture for full-page multi-script handwriting
recognition. The proposed model comes with three advantages: First, to solve
the common problem of data scarcity, we propose a lite transformer model that
can be trained on a reasonable amount of data, which is the case of most HTR
public datasets, without the need for external data. Second, it can learn the
reading order at page-level thanks to a curriculum learning strategy, allowing
it to avoid line segmentation errors, exploit a larger context and reduce the
need for costly segmentation annotations. Third, it can be easily adapted to
other scripts by applying a simple transfer-learning process using only
page-level labeled images. Extensive experiments on different datasets with
different scripts (French, English, Spanish, and Arabic) show the effectiveness
of the proposed model.",None,-1
076ac997-5353-43dd-a9cf-80ec41c957dd,Explaining Autonomous Driving Actions with Visual Question Answering,0.347626,6,"The end-to-end learning ability of self-driving vehicles has achieved
significant milestones over the last decade owing to rapid advances in deep
learning and computer vision algorithms. However, as autonomous driving
technology is a safety-critical application of artificial intelligence (AI),
road accidents and established regulatory principles necessitate the need for
the explainability of intelligent action choices for self-driving vehicles. To
facilitate interpretability of decision-making in autonomous driving, we
present a Visual Question Answering (VQA) framework, which explains driving
actions with question-answering-based causal reasoning. To do so, we first
collect driving videos in a simulation environment using reinforcement learning
(RL) and extract consecutive frames from this log data uniformly for five
selected action categories. Further, we manually annotate the extracted frames
using question-answer pairs as justifications for the actions chosen in each
scenario. Finally, we evaluate the correctness of the VQA-predicted answers for
actions on unseen driving scenes. The empirical results suggest that the VQA
mechanism can provide support to interpret real-time decisions of autonomous
vehicles and help enhance overall driving safety.",None,-1
b108e2ce-2e46-4450-884d-6379de804191,Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models,0.103082,15,"Despite recent progress, it has been difficult to prevent semantic
hallucinations in generative Large Language Models. One common solution to this
is augmenting LLMs with a retrieval system and making sure that the generated
output is attributable to the retrieved information. Given this new added
constraint, it is plausible to expect that the overall quality of the output
will be affected, for example, in terms of fluency. Can scaling language models
help?
  Here we examine the relationship between fluency and attribution in LLMs
prompted with retrieved evidence in knowledge-heavy dialog settings. Our
experiments were implemented with a set of auto-metrics that are aligned with
human preferences. They were used to evaluate a large set of generations,
produced under varying parameters of LLMs and supplied context.
  We show that larger models tend to do much better in both fluency and
attribution, and that (naively) using top-k retrieval versus top-1 retrieval
improves attribution but hurts fluency. We next propose a recipe that could
allow smaller models to both close the gap with larger models and preserve the
benefits of top-k retrieval while avoiding its drawbacks.",None,-1
1506eb2b-edbb-4d60-9eaf-c27c6af37211,Hyperbolic Image-Text Representations,0.780582,19,"Visual and linguistic concepts naturally organize themselves in a hierarchy,
where a textual concept ""dog"" entails all images that contain dogs. Despite
being intuitive, current large-scale vision and language models such as CLIP do
not explicitly capture such hierarchy. We propose MERU, a contrastive model
that yields hyperbolic representations of images and text. Hyperbolic spaces
have suitable geometric properties to embed tree-like data, so MERU can better
capture the underlying hierarchy in image-text datasets. Our results show that
MERU learns a highly interpretable and structured representation space while
being competitive with CLIP's performance on standard multi-modal tasks like
image classification and image-text retrieval. Our code and models are
available at https://www.github.com/facebookresearch/meru",None,-1
0706c55d-d31e-44ff-94ec-f91b847907ea,Generative Knowledge Selection for Knowledge-Grounded Dialogues,0.679619,11,"Knowledge selection is the key in knowledge-grounded dialogues (KGD), which
aims to select an appropriate knowledge snippet to be used in the utterance
based on dialogue history. Previous studies mainly employ the classification
approach to classify each candidate snippet as ""relevant"" or ""irrelevant""
independently. However, such approaches neglect the interactions between
snippets, leading to difficulties in inferring the meaning of snippets.
Moreover, they lack modeling of the discourse structure of dialogue-knowledge
interactions. We propose a simple yet effective generative approach for
knowledge selection, called GenKS. GenKS learns to select snippets by
generating their identifiers with a sequence-to-sequence model. GenKS therefore
captures intra-knowledge interaction inherently through attention mechanisms.
Meanwhile, we devise a hyperlink mechanism to model the dialogue-knowledge
interactions explicitly. We conduct experiments on three benchmark datasets,
and verify GenKS achieves the best results on both knowledge selection and
response generation.",None,-1
0edaa83e-91df-4e69-9434-ae4045a860b8,Learning Diverse Features in Vision Transformers for Improved Generalization,0.121931,2,"Deep learning models often rely only on a small set of features even when
there is a rich set of predictive signals in the training data. This makes
models brittle and sensitive to distribution shifts. In this work, we first
examine vision transformers (ViTs) and find that they tend to extract robust
and spurious features with distinct attention heads. As a result of this
modularity, their performance under distribution shifts can be significantly
improved at test time by pruning heads corresponding to spurious features,
which we demonstrate using an ""oracle selection"" on validation data. Second, we
propose a method to further enhance the diversity and complementarity of the
learned features by encouraging orthogonality of the attention heads' input
gradients. We observe improved out-of-distribution performance on diagnostic
benchmarks (MNIST-CIFAR, Waterbirds) as a consequence of the enhanced diversity
of features and the pruning of undesirable heads.",None,-1
dd17f918-26ad-4fd1-8ac5-045ef339d1ac,Writing Assistants Should Model Social Factors of Language,0.113538,5,"Intelligent writing assistants powered by large language models (LLMs) are
more popular today than ever before, but their further widespread adoption is
precluded by sub-optimal performance. In this position paper, we argue that a
major reason for this sub-optimal performance and adoption is a singular focus
on the information content of language while ignoring its social aspects. We
analyze the different dimensions of these social factors in the context of
writing assistants and propose their incorporation into building smarter, more
effective, and truly personalized writing assistants that would enrich the user
experience and contribute to increased user adoption.",None,-1
90f00e13-22bc-4052-9d13-a203f3a49aa7,SWING: Balancing Coverage and Faithfulness for Dialogue Summarization,0.299509,10,"Missing information is a common issue of dialogue summarization where some
information in the reference summaries is not covered in the generated
summaries. To address this issue, we propose to utilize natural language
inference (NLI) models to improve coverage while avoiding introducing factual
inconsistencies. Specifically, we use NLI to compute fine-grained training
signals to encourage the model to generate content in the reference summaries
that have not been covered, as well as to distinguish between factually
consistent and inconsistent generated sentences. Experiments on the DialogSum
and SAMSum datasets confirm the effectiveness of the proposed approach in
balancing coverage and faithfulness, validated with automatic metrics and human
evaluations. Additionally, we compute the correlation between commonly used
automatic metrics with human judgments in terms of three different dimensions
regarding coverage and factual consistency to provide insight into the most
suitable metric for evaluating dialogue summaries.",None,-1
413e780f-9183-4b1e-b1c7-a0c1ace12f97,DREEAM: Guiding Attention with Evidence for Improving Document-Level Relation Extraction,0.999648,31,"Document-level relation extraction (DocRE) is the task of identifying all
relations between each entity pair in a document. Evidence, defined as
sentences containing clues for the relationship between an entity pair, has
been shown to help DocRE systems focus on relevant texts, thus improving
relation extraction. However, evidence retrieval (ER) in DocRE faces two major
issues: high memory consumption and limited availability of annotations. This
work aims at addressing these issues to improve the usage of ER in DocRE.
First, we propose DREEAM, a memory-efficient approach that adopts evidence
information as the supervisory signal, thereby guiding the attention modules of
the DocRE system to assign high weights to evidence. Second, we propose a
self-training strategy for DREEAM to learn ER from automatically-generated
evidence on massive data without evidence annotations. Experimental results
reveal that our approach exhibits state-of-the-art performance on the DocRED
benchmark for both DocRE and ER. To the best of our knowledge, DREEAM is the
first approach to employ ER self-training.",None,-1
a7bd7c3e-af1e-4411-84d6-92d742e717b2,Learning by Self-Explaining,0.0603785,1,"Current AI research mainly treats explanations as a means for model
inspection. Yet, this neglects findings from human psychology that describe the
benefit of self-explanations in an agent's learning process. Motivated by this,
we introduce a novel approach in the context of image classification, termed
Learning by Self-Explaining (LSX). LSX utilizes aspects of self-refining AI and
human-guided explanatory machine learning. The underlying idea is that a
learner model, in addition to optimizing for the original predictive task, is
further optimized based on explanatory feedback from an internal critic model.
Intuitively, a learner's explanations are considered ""useful"" if the internal
critic can perform the same task given these explanations. We provide an
overview of important components of LSX and, based on this, perform extensive
experimental evaluations via three different example instantiations. Our
results indicate improvements via Learning by Self-Explaining on several
levels: in terms of model generalization, reducing the influence of confounding
factors, and providing more task-relevant and faithful model explanations.
Overall, our work provides evidence for the potential of self-explaining within
the learning phase of an AI model.",None,-1
57aa06f7-db25-439b-afcf-9d48c43c13d0,Zero-Shot Co-salient Object Detection Framework,0.777138,3,"Co-salient Object Detection (CoSOD) endeavors to replicate the human visual
system's capacity to recognize common and salient objects within a collection
of images. Despite recent advancements in deep learning models, these models
still rely on training with well-annotated CoSOD datasets. The exploration of
training-free zero-shot CoSOD frameworks has been limited. In this paper,
taking inspiration from the zero-shot transfer capabilities of foundational
computer vision models, we introduce the first zero-shot CoSOD framework that
harnesses these models without any training process. To achieve this, we
introduce two novel components in our proposed framework: the group prompt
generation (GPG) module and the co-saliency map generation (CMP) module. We
evaluate the framework's performance on widely-used datasets and observe
impressive results. Our approach surpasses existing unsupervised methods and
even outperforms fully supervised methods developed before 2020, while
remaining competitive with some fully supervised methods developed before 2022.",None,-1
16c137ae-5ef9-4b96-a266-04b235bad63e,Multilingual Controllable Transformer-Based Lexical Simplification,0.144759,1,"Text is by far the most ubiquitous source of knowledge and information and
should be made easily accessible to as many people as possible; however, texts
often contain complex words that hinder reading comprehension and
accessibility. Therefore, suggesting simpler alternatives for complex words
without compromising meaning would help convey the information to a broader
audience. This paper proposes mTLS, a multilingual controllable
Transformer-based Lexical Simplification (LS) system fined-tuned with the T5
model. The novelty of this work lies in the use of language-specific prefixes,
control tokens, and candidates extracted from pre-trained masked language
models to learn simpler alternatives for complex words. The evaluation results
on three well-known LS datasets -- LexMTurk, BenchLS, and NNSEval -- show that
our model outperforms the previous state-of-the-art models like LSBert and
ConLS. Moreover, further evaluation of our approach on the part of the recent
TSAR-2022 multilingual LS shared-task dataset shows that our model performs
competitively when compared with the participating systems for English LS and
even outperforms the GPT-3 model on several metrics. Moreover, our model
obtains performance gains also for Spanish and Portuguese.",None,-1
d141b1cf-4a46-4648-8fb2-ed23fccbee07,CitySpec with Shield: A Secure Intelligent Assistant for Requirement Formalization,0.168808,2,"An increasing number of monitoring systems have been developed in smart
cities to ensure that the real-time operations of a city satisfy safety and
performance requirements. However, many existing city requirements are written
in English with missing, inaccurate, or ambiguous information. There is a high
demand for assisting city policymakers in converting human-specified
requirements to machine-understandable formal specifications for monitoring
systems. To tackle this limitation, we build CitySpec, the first intelligent
assistant system for requirement specification in smart cities. To create
CitySpec, we first collect over 1,500 real-world city requirements across
different domains (e.g., transportation and energy) from over 100 cities and
extract city-specific knowledge to generate a dataset of city vocabulary with
3,061 words. We also build a translation model and enhance it through
requirement synthesis and develop a novel online learning framework with
shielded validation. The evaluation results on real-world city requirements
show that CitySpec increases the sentence-level accuracy of requirement
specification from 59.02% to 86.64%, and has strong adaptability to a new city
and a new domain (e.g., the F1 score for requirements in Seattle increases from
77.6% to 93.75% with online learning). After the enhancement from the shield
function, CitySpec is now immune to most known textual adversarial inputs
(e.g., the attack success rate of DeepWordBug after the shield function is
reduced to 0% from 82.73%). We test the CitySpec with 18 participants from
different domains. CitySpec shows its strong usability and adaptability to
different domains, and also its robustness to malicious inputs.",None,-1
fefdd94e-8fcd-4496-b5a3-e6fdb6cc9cad,Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks,0.401432,2,"Continual learning (CL) has two main objectives: preventing catastrophic
forgetting (CF) and encouraging knowledge transfer (KT). The existing
literature mainly focused on overcoming CF. Some work has also been done on KT
when the tasks are similar. To our knowledge, only one method has been proposed
to learn a sequence of mixed tasks. However, these techniques still suffer from
CF and/or limited KT. This paper proposes a new CL method to achieve both. It
overcomes CF by isolating the knowledge of each task via discovering a
subnetwork for it. A soft-masking mechanism is also proposed to preserve the
previous knowledge and to enable the new task to leverage the past knowledge to
achieve KT. Experiments using classification, generation, information
extraction, and their mixture (i.e., heterogeneous tasks) show that the
proposed method consistently outperforms strong baselines.",None,-1
039f070f-d2ea-4111-b277-1abe0bb737ed,Measuring Sentiment Bias in Machine Translation,0.0924635,1,"Biases induced to text by generative models have become an increasingly large
topic in recent years. In this paper we explore how machine translation might
introduce a bias in sentiments as classified by sentiment analysis models. For
this, we compare three open access machine translation models for five
different languages on two parallel corpora to test if the translation process
causes a shift in sentiment classes recognized in the texts. Though our
statistic test indicate shifts in the label probability distributions, we find
none that appears consistent enough to assume a bias induced by the translation
process.",None,-1
11b7aa5c-6b18-4fc4-a812-a217e2a2fac6,Linear-Covariance Loss for End-to-End Learning of 6D Pose Estimation,0.335981,3,"Most modern image-based 6D object pose estimation methods learn to predict
2D-3D correspondences, from which the pose can be obtained using a PnP solver.
Because of the non-differentiable nature of common PnP solvers, these methods
are supervised via the individual correspondences. To address this, several
methods have designed differentiable PnP strategies, thus imposing supervision
on the pose obtained after the PnP step. Here, we argue that this conflicts
with the averaging nature of the PnP problem, leading to gradients that may
encourage the network to degrade the accuracy of individual correspondences. To
address this, we derive a loss function that exploits the ground truth pose
before solving the PnP problem. Specifically, we linearize the PnP solver
around the ground-truth pose and compute the covariance of the resulting pose
distribution. We then define our loss based on the diagonal covariance
elements, which entails considering the final pose estimate yet not suffering
from the PnP averaging issue. Our experiments show that our loss consistently
improves the pose estimation accuracy for both dense and sparse correspondence
based methods, achieving state-of-the-art results on both Linemod-Occluded and
YCB-Video.",None,-1
f692f071-2538-469f-976b-692b858348d4,Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization,0.274842,7,"Large Language Models (LLMs) are proficient in natural language processing
tasks, but their deployment is often restricted by extensive parameter sizes
and computational demands. This paper focuses on post-training quantization
(PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8)
quantization, to enhance computational efficiency -- a topic less explored
compared to weight-only quantization. We present two innovative techniques:
activation-quantization-aware scaling (AQAS) and sequence-length-aware
calibration (SLAC) to enhance PTQ by considering the combined effects on
weights and activations and aligning calibration sequence lengths to target
tasks. Moreover, we introduce dINT, a hybrid data format combining integer and
denormal representations, to address the underflow issue in W4A8 quantization,
where small values are rounded to zero. Through rigorous evaluations of LLMs,
including OPT and LLaMA, we demonstrate that our techniques significantly boost
task accuracies to levels comparable with full-precision models. By developing
arithmetic units compatible with dINT, we further confirm that our methods
yield a 2$\times$ hardware efficiency improvement compared to 8-bit integer MAC
unit.",None,-1
35a3e394-2925-431b-825a-f79e8b69116f,Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability,0.960111,35,"Large, multilingual language models exhibit surprisingly good zero- or
few-shot machine translation capabilities, despite having never seen the
intentionally-included translation examples provided to typical neural
translation systems. We investigate the role of incidental bilingualism -- the
unintentional consumption of bilingual signals, including translation examples
-- in explaining the translation capabilities of large language models, taking
the Pathways Language Model (PaLM) as a case study. We introduce a mixed-method
approach to measure and understand incidental bilingualism at scale. We show
that PaLM is exposed to over 30 million translation pairs across at least 44
languages. Furthermore, the amount of incidental bilingual content is highly
correlated with the amount of monolingual in-language content for non-English
languages. We relate incidental bilingual content to zero-shot prompts and show
that it can be used to mine new prompts to improve PaLM's out-of-English
zero-shot translation quality. Finally, in a series of small-scale ablations,
we show that its presence has a substantial impact on translation capabilities,
although this impact diminishes with model scale.",None,-1
69d0b023-060b-4b7f-84a9-6bdf28743f7a,Pink-Eggs Dataset V1: A Step Toward Invasive Species Management Using Deep Learning Embedded Solutions,0.318536,1,"We introduce a novel dataset consisting of images depicting pink eggs that
have been identified as Pomacea canaliculata eggs, accompanied by corresponding
bounding box annotations. The purpose of this dataset is to aid researchers in
the analysis of the spread of Pomacea canaliculata species by utilizing deep
learning techniques, as well as supporting other investigative pursuits that
require visual data pertaining to the eggs of Pomacea canaliculata. It is worth
noting, however, that the identity of the eggs in question is not definitively
established, as other species within the same taxonomic family have been
observed to lay similar-looking eggs in regions of the Americas. Therefore, a
crucial prerequisite to any decision regarding the elimination of these eggs
would be to establish with certainty whether they are exclusively attributable
to invasive Pomacea canaliculata or if other species are also involved. The
dataset is available at https://www.kaggle.com/datasets/deeshenzhen/pinkeggs",None,-1
fbdd628b-010e-41ba-999e-d65077ab6450,"ProsAudit, a prosodic benchmark for self-supervised speech models",0.100177,2,"We present ProsAudit, a benchmark in English to assess structural prosodic
knowledge in self-supervised learning (SSL) speech models. It consists of two
subtasks, their corresponding metrics, and an evaluation dataset. In the
protosyntax task, the model must correctly identify strong versus weak prosodic
boundaries. In the lexical task, the model needs to correctly distinguish
between pauses inserted between words and within words. We also provide human
evaluation scores on this benchmark. We evaluated a series of SSL models and
found that they were all able to perform above chance on both tasks, even when
evaluated on an unseen language. However, non-native models performed
significantly worse than native ones on the lexical task, highlighting the
importance of lexical knowledge in this task. We also found a clear effect of
size with models trained on more data performing better in the two subtasks.",None,-1
bf150ac9-fe88-49e2-8c6d-9fd0ad1c9f65,ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER,0.292782,2,"Prompt-based language models have produced encouraging results in numerous
applications, including Named Entity Recognition (NER) tasks. NER aims to
identify entities in a sentence and provide their types. However, the strong
performance of most available NER approaches is heavily dependent on the design
of discrete prompts and a verbalizer to map the model-predicted outputs to
entity categories, which are complicated undertakings. To address these
challenges, we present ContrastNER, a prompt-based NER framework that employs
both discrete and continuous tokens in prompts and uses a contrastive learning
approach to learn the continuous prompts and forecast entity types. The
experimental results demonstrate that ContrastNER obtains competitive
performance to the state-of-the-art NER methods in high-resource settings and
outperforms the state-of-the-art models in low-resource circumstances without
requiring extensive manual prompt engineering and verbalizer design.",None,-1
37647640-828a-42b8-af8b-6e8a78cb211f,Reflective Artificial Intelligence,0.0821224,5,"Artificial Intelligence (AI) is about making computers that do the sorts of
things that minds can do, and as we progress towards this goal, we tend to
increasingly delegate human tasks to machines. However, AI systems usually do
these tasks with an unusual imbalance of insight and understanding: new, deeper
insights are present, yet many important qualities that a human mind would have
previously brought to the activity are utterly absent. Therefore, it is crucial
to ask which features of minds have we replicated, which are missing, and if
that matters. One core feature that humans bring to tasks, when dealing with
the ambiguity, emergent knowledge, and social context presented by the world,
is reflection. Yet this capability is utterly missing from current mainstream
AI. In this paper we ask what reflective AI might look like. Then, drawing on
notions of reflection in complex systems, cognitive science, and agents, we
sketch an architecture for reflective AI agents, and highlight ways forward.",None,-1
f64a82aa-1068-45ff-957c-c7dd7cf26fca,A Deep Behavior Path Matching Network for Click-Through Rate Prediction,0.557947,3,"User behaviors on an e-commerce app not only contain different kinds of
feedback on items but also sometimes imply the cognitive clue of the user's
decision-making. For understanding the psychological procedure behind user
decisions, we present the behavior path and propose to match the user's current
behavior path with historical behavior paths to predict user behaviors on the
app. Further, we design a deep neural network for behavior path matching and
solve three difficulties in modeling behavior paths: sparsity, noise
interference, and accurate matching of behavior paths. In particular, we
leverage contrastive learning to augment user behavior paths, provide behavior
path self-activation to alleviate the effect of noise, and adopt a two-level
matching mechanism to identify the most appropriate candidate. Our model shows
excellent performance on two real-world datasets, outperforming the
state-of-the-art CTR model. Moreover, our model has been deployed on the
Meituan food delivery platform and has accumulated 1.6% improvement in CTR and
1.8% improvement in advertising revenue.",None,-1
7adc7092-2486-4582-8781-caaaaab56a1d,PromptST: Prompt-Enhanced Spatio-Temporal Multi-Attribute Prediction,0.73799,3,"In the era of information explosion, spatio-temporal data mining serves as a
critical part of urban management. Considering the various fields demanding
attention, e.g., traffic state, human activity, and social event, predicting
multiple spatio-temporal attributes simultaneously can alleviate regulatory
pressure and foster smart city construction. However, current research can not
handle the spatio-temporal multi-attribute prediction well due to the complex
relationships between diverse attributes. The key challenge lies in how to
address the common spatio-temporal patterns while tackling their distinctions.
In this paper, we propose an effective solution for spatio-temporal
multi-attribute prediction, PromptST. We devise a spatio-temporal transformer
and a parameter-sharing training scheme to address the common knowledge among
different spatio-temporal attributes. Then, we elaborate a spatio-temporal
prompt tuning strategy to fit the specific attributes in a lightweight manner.
Through the pretrain and prompt tuning phases, our PromptST is able to enhance
the specific spatio-temoral characteristic capture by prompting the backbone
model to fit the specific target attribute while maintaining the learned common
knowledge. Extensive experiments on real-world datasets verify that our
PromptST attains state-of-the-art performance. Furthermore, we also prove
PromptST owns good transferability on unseen spatio-temporal attributes, which
brings promising application potential in urban computing. The implementation
code is available to ease reproducibility.",None,-1
cd0e9f53-b30a-4842-9851-34ab69864214,Two-stage Pipeline for Multilingual Dialect Detection,0.0497358,2,"Dialect Identification is a crucial task for localizing various Large
Language Models. This paper outlines our approach to the VarDial 2023 shared
task. Here we have to identify three or two dialects from three languages each
which results in a 9-way classification for Track-1 and 6-way classification
for Track-2 respectively. Our proposed approach consists of a two-stage system
and outperforms other participants' systems and previous works in this domain.
We achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebase
is available publicly (https://github.com/ankit-vaidya19/EACL_VarDial2023).",None,-1
194ec922-94c0-4820-9a8a-4ee41c068208,Search for universal minimum drag resistance underwater vehicle hull using CFD,0.711708,9,"In Autonomous Underwater Vehicles (AUVs) design, hull resistance is an
important factor in determining the power requirements and range of vehicle and
consequently affect battery size, weight, and volume requirement of the design.
In this paper, we leverage on AI-based optimization algorithm along with
Computational Fluid Dynamics (CFD) simulation to study the optimal hull design
that minimizing the resistance. By running the CFD-based optimization at
different operating velocities and turbulence intensity, we want to
study/search the possibility of a universal design that will provide least
resistance/near-optimal design across all operating conditions (operating
velocity) and environmental conditions (turbulence intensity). Early result
demonstrated that the optimal design found at low velocity and low turbulence
condition performs very poor at high velocity and high turbulence conditions.
However, a design that is optimal at high velocity and high turbulence
conditions performs near-optimal across many considered velocity and turbulence
conditions.",None,-1
973ad423-f3aa-45ca-85b1-ccabf93345bc,Weakly Supervised Headline Dependency Parsing,0.30715,1,"English news headlines form a register with unique syntactic properties that
have been documented in linguistics literature since the 1930s. However,
headlines have received surprisingly little attention from the NLP syntactic
parsing community. We aim to bridge this gap by providing the first news
headline corpus of Universal Dependencies annotated syntactic dependency trees,
which enables us to evaluate existing state-of-the-art dependency parsers on
news headlines. To improve English news headline parsing accuracies, we develop
a projection method to bootstrap silver training data from unlabeled news
headline-article lead sentence pairs. Models trained on silver headline parses
demonstrate significant improvements in performance over models trained solely
on gold-annotated long-form texts. Ultimately, we find that, although projected
silver training data improves parser performance across different news outlets,
the improvement is moderated by constructions idiosyncratic to outlet.",None,-1
6a616a14-001f-4e37-adb0-fc697d6c0631,Probabilistic Adaptation of Text-to-Video Models,0.626257,15,"Large text-to-video models trained on internet-scale data have demonstrated
exceptional capabilities in generating high-fidelity videos from arbitrary
textual descriptions. However, adapting these models to tasks with limited
domain-specific data, such as animation or robotics videos, poses a significant
computational challenge, since finetuning a pretrained large model can be
prohibitively expensive. Inspired by how a small modifiable component (e.g.,
prompts, prefix-tuning) can adapt a large language model to perform new tasks
without requiring access to the model weights, we investigate how to adapt a
large pretrained text-to-video model to a variety of downstream domains and
tasks without finetuning. In answering this question, we propose Video Adapter,
which leverages the score function of a large pretrained video diffusion model
as a probabilistic prior to guide the generation of a task-specific small video
model. Our experiments show that Video Adapter is capable of incorporating the
broad knowledge and preserving the high fidelity of a large pretrained video
model in a task-specific small video model that is able to generate
high-quality yet specialized videos on a variety of tasks such as animation,
egocentric modeling, and modeling of simulated and real-world robotics data.
More videos can be found on the website https://video-adapter.github.io/.",None,-1
8c669073-9af8-4c8f-abd4-11c08fd65775,Deep Image Harmonization with Globally Guided Feature Transformation and Relation Distillation,0.416548,3,"Given a composite image, image harmonization aims to adjust the foreground
illumination to be consistent with background. Previous methods have explored
transforming foreground features to achieve competitive performance. In this
work, we show that using global information to guide foreground feature
transformation could achieve significant improvement. Besides, we propose to
transfer the foreground-background relation from real images to composite
images, which can provide intermediate supervision for the transformed encoder
features. Additionally, considering the drawbacks of existing harmonization
datasets, we also contribute a ccHarmony dataset which simulates the natural
illumination variation. Extensive experiments on iHarmony4 and our contributed
dataset demonstrate the superiority of our method. Our ccHarmony dataset is
released at https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.",None,-1
8cbf00b6-ecac-40d5-b2c2-1f169add1c3c,LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,0.998783,210,"Large language models (LLMs) have demonstrated remarkable zero-shot
generalization abilities: state-of-the-art chatbots can provide plausible
answers to many common questions that arise in daily life. However, so far,
LLMs cannot reliably solve long-horizon planning problems. By contrast,
classical planners, once a problem is given in a formatted way, can use
efficient search algorithms to quickly identify correct, or even optimal,
plans. In an effort to get the best of both worlds, this paper introduces
LLM+P, the first framework that incorporates the strengths of classical
planners into LLMs. LLM+P takes in a natural language description of a planning
problem, then returns a correct (or optimal) plan for solving that problem in
natural language. LLM+P does so by first converting the language description
into a file written in the planning domain definition language (PDDL), then
leveraging classical planners to quickly find a solution, and then translating
the found solution back into natural language. Along with LLM+P, we define a
diverse set of different benchmark problems taken from common planning
scenarios. Via a comprehensive set of experiments on these benchmark problems,
we find that LLM+P is able to provide optimal solutions for most problems,
while LLMs fail to provide even feasible plans for most problems.\footnote{The
code and results are publicly available at
https://github.com/Cranial-XIX/llm-pddl.git.",None,-1
262b0461-29ed-4d7a-903d-2654c8788594,LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents,0.0460174,1,"Recent advancements in reasoning abilities of Large Language Models (LLM) has
promoted their usage in problems that require high-level planning for robots
and artificial agents. However, current techniques that utilize LLMs for such
planning tasks make certain key assumptions such as, access to datasets that
permit finetuning, meticulously engineered prompts that only provide relevant
and essential information to the LLM, and most importantly, a deterministic
approach to allow execution of the LLM responses either in the form of existing
policies or plan operators. In this work, we propose LgTS (LLM-guided
Teacher-Student learning), a novel approach that explores the planning
abilities of LLMs to provide a graphical representation of the sub-goals to a
reinforcement learning (RL) agent that does not have access to the transition
dynamics of the environment. The RL agent uses Teacher-Student learning
algorithm to learn a set of successful policies for reaching the goal state
from the start state while simultaneously minimizing the number of
environmental interactions. Unlike previous methods that utilize LLMs, our
approach does not assume access to a propreitary or a fine-tuned LLM, nor does
it require pre-trained policies that achieve the sub-goals proposed by the LLM.
Through experiments on a gridworld based DoorKey domain and a search-and-rescue
inspired domain, we show that generating a graphical structure of sub-goals
helps in learning policies for the LLM proposed sub-goals and the
Teacher-Student learning algorithm minimizes the number of environment
interactions when the transition dynamics are unknown.",None,-1
10f14af4-05cb-49b2-9057-db262c21a2aa,"OpenSLU: A Unified, Modularized, and Extensible Toolkit for Spoken Language Understanding",0.0596619,2,"Spoken Language Understanding (SLU) is one of the core components of a
task-oriented dialogue system, which aims to extract the semantic meaning of
user queries (e.g., intents and slots). In this work, we introduce OpenSLU, an
open-source toolkit to provide a unified, modularized, and extensible toolkit
for spoken language understanding. Specifically, OpenSLU unifies 10 SLU models
for both single-intent and multi-intent scenarios, which support both
non-pretrained and pretrained models simultaneously. Additionally, OpenSLU is
highly modularized and extensible by decomposing the model architecture,
inference, and learning process into reusable modules, which allows researchers
to quickly set up SLU experiments with highly flexible configurations. OpenSLU
is implemented based on PyTorch, and released at
\url{https://github.com/LightChen233/OpenSLU}.",None,-1
b0acbeb4-3991-47f7-96ae-553fbf9427e3,SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks,0.868656,83,"We introduce SwiftSage, a novel agent framework inspired by the dual-process
theory of human cognition, designed to excel in action planning for complex
interactive reasoning tasks. SwiftSage integrates the strengths of behavior
cloning and prompting large language models (LLMs) to enhance task completion
performance. The framework comprises two primary modules: the Swift module,
representing fast and intuitive thinking, and the Sage module, emulating
deliberate thought processes. The Swift module is a small encoder-decoder LM
fine-tuned on the oracle agent's action trajectories, while the Sage module
employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a
heuristic method to harmoniously integrate the two modules, resulting in a more
efficient and robust problem-solving process. In 30 tasks from the ScienceWorld
benchmark, SwiftSage significantly outperforms other methods such as SayCan,
ReAct, and Reflexion, demonstrating its effectiveness in solving complex
interactive tasks.",None,-1
b8269ac7-596a-4449-8ca1-58fcf3b9a2e1,Unpaired Translation from Semantic Label Maps to Images by Leveraging Domain-Specific Simulations,0.0903188,1,"Photorealistic image generation from simulated label maps are necessitated in
several contexts, such as for medical training in virtual reality. With
conventional deep learning methods, this task requires images that are paired
with semantic annotations, which typically are unavailable. We introduce a
contrastive learning framework for generating photorealistic images from
simulated label maps, by learning from unpaired sets of both. Due to
potentially large scene differences between real images and label maps,
existing unpaired image translation methods lead to artifacts of scene
modification in synthesized images. We utilize simulated images as surrogate
targets for a contrastive loss, while ensuring consistency by utilizing
features from a reverse translation network. Our method enables bidirectional
label-image translations, which is demonstrated in a variety of scenarios and
datasets, including laparoscopy, ultrasound, and driving scenes. By comparing
with state-of-the-art unpaired translation methods, our proposed method is
shown to generate realistic and scene-accurate translations.",None,-1
badb04e5-f3c3-404a-aa76-0b675f07564e,Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships,0.318484,2,"Understanding interpersonal communication requires, in part, understanding
the social context and norms in which a message is said. However, current
methods for identifying offensive content in such communication largely operate
independent of context, with only a few approaches considering community norms
or prior conversation as context. Here, we introduce a new approach to
identifying inappropriate communication by explicitly modeling the social
relationship between the individuals. We introduce a new dataset of
contextually-situated judgments of appropriateness and show that large language
models can readily incorporate relationship information to accurately identify
appropriateness in a given context. Using data from online conversations and
movie dialogues, we provide insight into how the relationships themselves
function as implicit norms and quantify the degree to which context-sensitivity
is needed in different conversation settings. Further, we also demonstrate that
contextual-appropriateness judgments are predictive of other social factors
expressed in language such as condescension and politeness.",None,-1
9ec9f9d2-f87f-4e67-b5f0-d59f8730a236,SAT-Based PAC Learning of Description Logic Concepts,0.368482,6,"We propose bounded fitting as a scheme for learning description logic
concepts in the presence of ontologies. A main advantage is that the resulting
learning algorithms come with theoretical guarantees regarding their
generalization to unseen examples in the sense of PAC learning. We prove that,
in contrast, several other natural learning algorithms fail to provide such
guarantees. As a further contribution, we present the system SPELL which
efficiently implements bounded fitting for the description logic
$\mathcal{ELH}^r$ based on a SAT solver, and compare its performance to a
state-of-the-art learner.",None,-1
11206f0c-ea35-471c-8da3-8ee6baa74496,ZScribbleSeg: Zen and the Art of Scribble Supervised Medical Image Segmentation,0.310968,5,"Curating a large scale fully-annotated dataset can be both labour-intensive
and expertise-demanding, especially for medical images. To alleviate this
problem, we propose to utilize solely scribble annotations for weakly
supervised segmentation. Existing solutions mainly leverage selective losses
computed solely on annotated areas and generate pseudo gold standard
segmentation by propagating labels to adjacent areas. However, these methods
could suffer from the inaccurate and sometimes unrealistic pseudo segmentation
due to the insufficient supervision and incomplete shape features. Different
from previous efforts, we first investigate the principle of ''good scribble
annotations'', which leads to efficient scribble forms via supervision
maximization and randomness simulation. Furthermore, we introduce
regularization terms to encode the spatial relationship and shape prior, where
a new formulation is developed to estimate the mixture ratios of label classes.
These ratios are critical in identifying the unlabeled pixels for each class
and correcting erroneous predictions, thus the accurate estimation lays the
foundation for the incorporation of spatial prior. Finally, we integrate the
efficient scribble supervision with the prior into a unified framework, denoted
as ZScribbleSeg, and apply the method to multiple scenarios. Leveraging only
scribble annotations, ZScribbleSeg set new state-of-the-arts on four
segmentation tasks using ACDC, MSCMRseg, MyoPS and PPSS datasets.",None,-1
bbaf653c-8e0a-4024-96a4-f18362d34e52,Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning,0.0587663,2,"We propose a model-free reinforcement learning solution, namely the ASAP-Phi
framework, to encourage an agent to fulfill a formal specification ASAP. The
framework leverages a piece-wise reward function that assigns quantitative
semantic reward to traces not satisfying the specification, and a high constant
reward to the remaining. Then, it trains an agent with an actor-critic-based
algorithm, such as soft actor-critic (SAC), or deep deterministic policy
gradient (DDPG). Moreover, we prove that ASAP-Phi produces policies that
prioritize fulfilling a specification ASAP. Extensive experiments are run,
including ablation studies, on state-of-the-art benchmarks. Results show that
our framework succeeds in finding sufficiently fast trajectories for up to 97\%
test cases and defeats baselines.",None,-1
6033f06e-ec5a-48e5-a4b3-8c65de525767,Beta-VAE has 2 Behaviors: PCA or ICA?,0.0773592,1,"Beta-VAE is a very classical model for disentangled representation learning,
the use of an expanding bottleneck that allow information into the decoder
gradually is key to representation disentanglement as well as high-quality
reconstruction. During recent experiments on such fascinating structure, we
discovered that the total amount of latent variables can affect the
representation learnt by the network: with very few latent variables, the
network tend to learn the most important or principal variables, acting like a
PCA; with very large numbers of latent variables, the variables tend to be more
disentangled, and act like an ICA. Our assumption is that the competition
between latent variables while trying to gain the most information bandwidth
can lead to this phenomenon.",None,-1
58292d41-9f15-4aa0-8b54-87cc32516eb6,Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,0.453849,1,"CT images corrupted by metal artifacts have serious negative effects on
clinical diagnosis. Considering the difficulty of collecting paired data with
ground truth in clinical settings, unsupervised methods for metal artifact
reduction are of high interest. However, it is difficult for previous
unsupervised methods to retain structural information from CT images while
handling the non-local characteristics of metal artifacts. To address these
challenges, we proposed a novel Dense Transformer based Enhanced Coding Network
(DTEC-Net) for unsupervised metal artifact reduction. Specifically, we
introduce a Hierarchical Disentangling Encoder, supported by the high-order
dense process, and transformer to obtain densely encoded sequences with
long-range correspondence. Then, we present a second-order disentanglement
method to improve the dense sequence's decoding process. Extensive experiments
and model discussions illustrate DTEC-Net's effectiveness, which outperforms
the previous state-of-the-art methods on a benchmark dataset, and greatly
reduces metal artifacts while restoring richer texture details.",None,-1
724243a6-87b1-47a6-bd4d-c27e5efd58a5,Experiments with Detecting and Mitigating AI Deception,0.0294356,1,"How to detect and mitigate deceptive AI systems is an open problem for the
field of safe and trustworthy AI. We analyse two algorithms for mitigating
deception: The first is based on the path-specific objectives framework where
paths in the game that incentivise deception are removed. The second is based
on shielding, i.e., monitoring for unsafe policies and replacing them with a
safe reference policy. We construct two simple games and evaluate our
algorithms empirically. We find that both methods ensure that our agent is not
deceptive, however, shielding tends to achieve higher reward.",None,-1
8004099c-dded-4157-ae51-7a6c6f35037b,ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection,0.55537,2,"Fanfiction, a popular form of creative writing set within established
fictional universes, has gained a substantial online following. However,
ensuring the well-being and safety of participants has become a critical
concern in this community. The detection of triggering content, material that
may cause emotional distress or trauma to readers, poses a significant
challenge. In this paper, we describe our approach for the Trigger Detection
shared task at PAN CLEF 2023, where we want to detect multiple triggering
content in a given Fanfiction document. For this, we build a hierarchical model
that uses recurrence over Transformer-based language models. In our approach,
we first split long documents into smaller sized segments and use them to
fine-tune a Transformer model. Then, we extract feature embeddings from the
fine-tuned Transformer model, which are used as input in the training of
multiple LSTM models for trigger detection in a multi-label setting. Our model
achieves an F1-macro score of 0.372 and F1-micro score of 0.736 on the
validation set, which are higher than the baseline results shared at PAN CLEF
2023.",None,-1
bfaaf3f5-4c45-4a06-8917-562fb017cc69,Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations,0.999941,30,"In this paper, we present an innovative process-oriented math process reward
model called \textbf{Math-Shepherd}, which assigns a reward score to each step
of math problem solutions. The training of Math-Shepherd is achieved using
automatically constructed process-wise supervision data, breaking the
bottleneck of heavy reliance on manual annotation in existing work. We explore
the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}:
Math-Shepherd is utilized for reranking multiple outputs generated by Large
Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is
employed to reinforce LLMs with step-by-step Proximal Policy Optimization
(PPO). With Math-Shepherd, a series of open-source LLMs demonstrates
exceptional performance. For instance, the step-by-step PPO with Math-Shepherd
significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K
and 28.6\%$\to$33.0\% on MATH). The accuracy can be further enhanced to 89.1\%
and 43.5\% on GSM8K and MATH with the verification of Math-Shepherd,
respectively. We believe that automatic process supervision holds significant
potential for the future evolution of LLMs.",None,-1
954e29ff-0f3e-41f3-9a87-246353bc22c3,Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection with Cultural Features,0.418448,6,"The increasing ubiquity of language technology necessitates a shift towards
considering cultural diversity in the machine learning realm, particularly for
subjective tasks that rely heavily on cultural nuances, such as Offensive
Language Detection (OLD). Current understanding underscores that these tasks
are substantially influenced by cultural values, however, a notable gap exists
in determining if cultural features can accurately predict the success of
cross-cultural transfer learning for such subjective tasks. Addressing this,
our study delves into the intersection of cultural features and transfer
learning effectiveness. The findings reveal that cultural value surveys indeed
possess a predictive power for cross-cultural transfer learning success in OLD
tasks and that it can be further improved using offensive word distance. Based
on these results, we advocate for the integration of cultural information into
datasets. Additionally, we recommend leveraging data sources rich in cultural
information, such as surveys, to enhance cultural adaptability. Our research
signifies a step forward in the quest for more inclusive, culturally sensitive
language technologies.",None,-1
2bc8ad9e-6db6-4e2f-a60f-1c463a4fb8b4,Evolutionary approaches to explainable machine learning,0.202049,2,"Machine learning models are increasingly being used in critical sectors, but
their black-box nature has raised concerns about accountability and trust. The
field of explainable artificial intelligence (XAI) or explainable machine
learning (XML) has emerged in response to the need for human understanding of
these models. Evolutionary computing, as a family of powerful optimization and
learning tools, has significant potential to contribute to XAI/XML. In this
chapter, we provide a brief introduction to XAI/XML and review various
techniques in current use for explaining machine learning models. We then focus
on how evolutionary computing can be used in XAI/XML, and review some
approaches which incorporate EC techniques. We also discuss some open
challenges in XAI/XML and opportunities for future research in this field using
EC. Our aim is to demonstrate that evolutionary computing is well-suited for
addressing current problems in explainability, and to encourage further
exploration of these methods to contribute to the development of more
transparent, trustworthy and accountable machine learning models.",None,-1
fc25e56e-8939-4075-8cd1-04027d8cb28d,Sparse 3D Reconstruction via Object-Centric Ray Sampling,0.226125,2,"We propose a novel method for 3D object reconstruction from a sparse set of
views captured from a 360-degree calibrated camera rig. We represent the object
surface through a hybrid model that uses both an MLP-based neural
representation and a triangle mesh. A key contribution in our work is a novel
object-centric sampling scheme of the neural representation, where rays are
shared among all views. This efficiently concentrates and reduces the number of
samples used to update the neural model at each iteration. This sampling scheme
relies on the mesh representation to ensure also that samples are
well-distributed along its normals. The rendering is then performed efficiently
by a differentiable renderer. We demonstrate that this sampling scheme results
in a more effective training of the neural representation, does not require the
additional supervision of segmentation masks, yields state of the art 3D
reconstructions, and works with sparse views on the Google's Scanned Objects,
Tank and Temples and MVMC Car datasets. Code available at:
https://github.com/llukmancerkezi/ROSTER",None,-1
9ee91254-9834-46e5-bc26-e166a9689f84,Interpretable Motion Planner for Urban Driving via Hierarchical Imitation Learning,0.209651,4,"Learning-based approaches have achieved remarkable performance in the domain
of autonomous driving. Leveraging the impressive ability of neural networks and
large amounts of human driving data, complex patterns and rules of driving
behavior can be encoded as a model to benefit the autonomous driving system.
Besides, an increasing number of data-driven works have been studied in the
decision-making and motion planning module. However, the reliability and the
stability of the neural network is still full of uncertainty. In this paper, we
introduce a hierarchical planning architecture including a high-level
grid-based behavior planner and a low-level trajectory planner, which is highly
interpretable and controllable. As the high-level planner is responsible for
finding a consistent route, the low-level planner generates a feasible
trajectory. We evaluate our method both in closed-loop simulation and real
world driving, and demonstrate the neural network planner has outstanding
performance in complex urban autonomous driving scenarios.",None,-1
06a01169-b85b-43c8-a6c6-8666870f22eb,Safe Navigation: Training Autonomous Vehicles using Deep Reinforcement Learning in CARLA,0.118546,1,"Autonomous vehicles have the potential to revolutionize transportation, but
they must be able to navigate safely in traffic before they can be deployed on
public roads. The goal of this project is to train autonomous vehicles to make
decisions to navigate in uncertain environments using deep reinforcement
learning techniques using the CARLA simulator. The simulator provides a
realistic and urban environment for training and testing self-driving models.
Deep Q-Networks (DQN) are used to predict driving actions. The study involves
the integration of collision sensors, segmentation, and depth camera for better
object detection and distance estimation. The model is tested on 4 different
trajectories in presence of different types of 4-wheeled vehicles and
pedestrians. The segmentation and depth cameras were utilized to ensure
accurate localization of objects and distance measurement. Our proposed method
successfully navigated the self-driving vehicle to its final destination with a
high success rate without colliding with other vehicles, pedestrians, or going
on the sidewalk. To ensure the optimal performance of our reinforcement
learning (RL) models in navigating complex traffic scenarios, we implemented a
pre-processing step to reduce the state space. This involved processing the
images and sensor output before feeding them into the model. Despite
significantly decreasing the state space, our approach yielded robust models
that successfully navigated through traffic with high levels of safety and
accuracy.",None,-1
5b618f4d-d5d5-4aad-bb22-920c92dc09a6,Cryptocurrency Price Prediction using Twitter Sentiment Analysis,0.525252,3,"The cryptocurrency ecosystem has been the centre of discussion on many social
media platforms, following its noted volatility and varied opinions. Twitter is
rapidly being utilised as a news source and a medium for bitcoin discussion.
Our algorithm seeks to use historical prices and sentiment of tweets to
forecast the price of Bitcoin. In this study, we develop an end-to-end model
that can forecast the sentiment of a set of tweets (using a Bidirectional
Encoder Representations from Transformers - based Neural Network Model) and
forecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted
sentiment and other metrics like historical cryptocurrency price data, tweet
volume, a user's following, and whether or not a user is verified. The
sentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average
of real-time data, and test data. The mean absolute percent error for the price
prediction was 3.6%.",None,-1
560c4e6d-317d-4274-8781-bf98f4ccb23f,Semantic relatedness in DBpedia: A comparative and experimental assessment,0.407235,5,"Evaluating semantic relatedness of Web resources is still an open challenge.
This paper focuses on knowledge-based methods, which represent an alternative
to corpus-based approaches, and rely in general on the availability of
knowledge graphs. In particular, we have selected 10 methods from the existing
literature, that have been organized according to it adjacent resources, triple
patterns, and triple weights-based methods. They have been implemented and
evaluated by using DBpedia as reference RDF knowledge graph. Since DBpedia is
continuously evolving, the experimental results provided by these methods in
the literature are not comparable. For this reason, in this work, such methods
have been experimented by running them all at once on the same DBpedia release
and against 14 well-known golden datasets. On the basis of the correlation
values with human judgment obtained according to the experimental results,
weighting the RDF triples in combination with evaluating all the directed paths
linking the compared resources is the best strategy in order to compute
semantic relatedness in DBpedia.",None,-1
a514cf2b-c796-462f-b006-463a627456b6,From Visual Prompt Learning to Zero-Shot Transfer: Mapping Is All You Need,0.0850769,3,"Visual prompt learning, as a newly emerged technique, leverages the knowledge
learned by a large-scale pre-trained model and adapts it to downstream tasks
through the usage of prompts. While previous research has focused on designing
effective prompts, in this work, we argue that compared to prompt design, a
good mapping strategy matters more. In this sense, we propose SeMap, a more
effective mapping using the semantic alignment between the pre-trained model's
knowledge and the downstream task. Our experimental results show that SeMap can
largely boost the performance of visual prompt learning. Moreover, our
experiments show that SeMap is capable of achieving competitive zero-shot
transfer, indicating that it can perform the downstream task without any
fine-tuning on the corresponding dataset. This demonstrates the potential of
our proposed method to be used in a broader range of applications where the
zero-shot transfer is desired. Results suggest that our proposed SeMap could
lead to significant advancements in both visual prompt learning and zero-shot
transfer. We hope with SeMap, we can help the community move forward to more
efficient and lightweight utilization of large vision models.",None,-1
d9d78007-739a-4566-b340-dffe996fe848,Feature Reduction Method Comparison Towards Explainability and Efficiency in Cybersecurity Intrusion Detection Systems,0.329734,4,"In the realm of cybersecurity, intrusion detection systems (IDS) detect and
prevent attacks based on collected computer and network data. In recent
research, IDS models have been constructed using machine learning (ML) and deep
learning (DL) methods such as Random Forest (RF) and deep neural networks
(DNN). Feature selection (FS) can be used to construct faster, more
interpretable, and more accurate models. We look at three different FS
techniques; RF information gain (RF-IG), correlation feature selection using
the Bat Algorithm (CFS-BA), and CFS using the Aquila Optimizer (CFS-AO). Our
results show CFS-BA to be the most efficient of the FS methods, building in 55%
of the time of the best RF-IG model while achieving 99.99% of its accuracy.
This reinforces prior contributions attesting to CFS-BA's accuracy while
building upon the relationship between subset size, CFS score, and RF-IG score
in final results.",None,-1
3de989ba-53d9-4a14-9d49-ca440f961108,Guideline Learning for In-context Information Extraction,0.665379,9,"Large language models (LLMs) can perform a new task by merely conditioning on
task instructions and a few input-output examples, without optimizing any
parameters. This is called In-Context Learning (ICL). In-context Information
Extraction (IE) has recently garnered attention in the research community.
However, the performance of In-context IE generally lags behind the
state-of-the-art supervised expert models. We highlight a key reason for this
shortfall: underspecified task description. The limited-length context
struggles to thoroughly express the intricate IE task instructions and various
edge cases, leading to misalignment in task comprehension with humans. In this
paper, we propose a Guideline Learning (GL) framework for In-context IE which
reflectively learns and follows guidelines. During the learning phrase, GL
automatically synthesizes a set of guidelines based on a few error cases, and
during inference, GL retrieves helpful guidelines for better ICL. Moreover, we
propose a self-consistency-based active learning method to enhance the
efficiency of GL. Experiments on event extraction and relation extraction show
that GL can significantly improve the performance of in-context IE.",None,-1
c521f0e6-c5d6-4d78-8ceb-8eb4d1f82953,Channelformer: Attention based Neural Solution for Wireless Channel Estimation and Effective Online Training,0.635372,5,"In this paper, we propose an encoder-decoder neural architecture (called
Channelformer) to achieve improved channel estimation for orthogonal
frequency-division multiplexing (OFDM) waveforms in downlink scenarios. The
self-attention mechanism is employed to achieve input precoding for the input
features before processing them in the decoder. In particular, we implement
multi-head attention in the encoder and a residual convolutional neural
architecture as the decoder, respectively. We also employ a customized
weight-level pruning to slim the trained neural network with a fine-tuning
process, which reduces the computational complexity significantly to realize a
low complexity and low latency solution. This enables reductions of up to 70\%
in the parameters, while maintaining an almost identical performance compared
with the complete Channelformer. We also propose an effective online training
method based on the fifth generation (5G) new radio (NR) configuration for the
modern communication systems, which only needs the available information at the
receiver for online training. Using industrial standard channel models, the
simulations of attention-based solutions show superior estimation performance
compared with other candidate neural network methods for channel estimation.",None,-1
b09b1acc-c08b-4e3a-b74f-90a249959a06,An Outlook into the Future of Egocentric Vision,0.623474,14,"What will the future be? We wonder! In this survey, we explore the gap
between current research in egocentric vision and the ever-anticipated future,
where wearable computing, with outward facing cameras and digital overlays, is
expected to be integrated in our every day lives. To understand this gap, the
article starts by envisaging the future through character-based stories,
showcasing through examples the limitations of current technology. We then
provide a mapping between this future and previously defined research tasks.
For each task, we survey its seminal works, current state-of-the-art
methodologies and available datasets, then reflect on shortcomings that limit
its applicability to future research. Note that this survey focuses on software
models for egocentric vision, independent of any specific hardware. The paper
concludes with recommendations for areas of immediate explorations so as to
unlock our path to the future always-on, personalised and life-enhancing
egocentric vision.",None,-1
d3132798-bee0-4840-88dd-1517cc56f5cc,Rather a Nurse than a Physician -- Contrastive Explanations under Investigation,0.740519,4,"Contrastive explanations, where one decision is explained in contrast to
another, are supposed to be closer to how humans explain a decision than
non-contrastive explanations, where the decision is not necessarily referenced
to an alternative. This claim has never been empirically validated. We analyze
four English text-classification datasets (SST2, DynaSent, BIOS and
DBpedia-Animals). We fine-tune and extract explanations from three different
models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three
post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore
collect and release human rationale annotations for a subset of 100 samples
from the BIOS dataset for contrastive and non-contrastive settings. A
cross-comparison between model-based rationales and human annotations, both in
contrastive and non-contrastive settings, yields a high agreement between the
two settings for models as well as for humans. Moreover, model-based
explanations computed in both settings align equally well with human
rationales. Thus, we empirically find that humans do not necessarily explain in
a contrastive manner.9 pages, long paper at ACL 2022 proceedings.",None,-1
be2cda66-6789-49cc-b447-a2c8e25a06b0,Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection,0.679431,10,"Natural language processing models tend to learn and encode social biases
present in the data. One popular approach for addressing such biases is to
eliminate encoded information from the model's representations. However,
current methods are restricted to removing only linearly encoded information.
In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel
method for removing non-linear encoded concepts from neural representations.
Our method consists of iteratively training neural classifiers to predict a
particular attribute we seek to eliminate, followed by a projection of the
representation on a hypersurface, such that the classifiers become oblivious to
the target attribute. We evaluate the effectiveness of our method on the task
of removing gender and race information as sensitive attributes. Our results
demonstrate that IGBP is effective in mitigating bias through intrinsic and
extrinsic evaluations, with minimal impact on downstream task accuracy.",None,-1
095835e9-22a7-4858-8a63-05f1fe5e4fd3,Spatiotemporal Self-supervised Learning for Point Clouds in the Wild,0.910958,11,"Self-supervised learning (SSL) has the potential to benefit many
applications, particularly those where manually annotating data is cumbersome.
One such situation is the semantic segmentation of point clouds. In this
context, existing methods employ contrastive learning strategies and define
positive pairs by performing various augmentation of point clusters in a single
frame. As such, these methods do not exploit the temporal nature of LiDAR data.
In this paper, we introduce an SSL strategy that leverages positive pairs in
both the spatial and temporal domain. To this end, we design (i) a
point-to-cluster learning strategy that aggregates spatial information to
distinguish objects; and (ii) a cluster-to-cluster learning strategy based on
unsupervised object tracking that exploits temporal correspondences. We
demonstrate the benefits of our approach via extensive experiments performed by
self-supervised training on two large-scale LiDAR datasets and transferring the
resulting models to other point cloud segmentation benchmarks. Our results
evidence that our method outperforms the state-of-the-art point cloud SSL
methods.",None,-1
c5a93982-aa4c-4c12-a52c-58371c8b7c4d,Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation,0.659386,8,"In medical vision, different imaging modalities provide complementary
information. However, in practice, not all modalities may be available during
inference or even training. Previous approaches, e.g., knowledge distillation
or image synthesis, often assume the availability of full modalities for all
patients during training; this is unrealistic and impractical due to the
variability in data collection across sites. We propose a novel approach to
learn enhanced modality-agnostic representations by employing a meta-learning
strategy in training, even when only limited full modality samples are
available. Meta-learning enhances partial modality representations to full
modality representations by meta-training on partial modality data and
meta-testing on limited full modality samples. Additionally, we co-supervise
this feature enrichment by introducing an auxiliary adversarial learning
branch. More specifically, a missing modality detector is used as a
discriminator to mimic the full modality setting. Our segmentation framework
significantly outperforms state-of-the-art brain tumor segmentation techniques
in missing modality scenarios.",None,-1
6cd98528-4445-41f2-b898-ad7bf43aa1a1,Scaling Sentence Embeddings with Large Language Models,0.982438,10,"Large language models (LLMs) have recently garnered significant interest.
With in-context learning, LLMs achieve impressive results in various natural
language tasks. However, the application of LLMs to sentence embeddings remains
an area of ongoing research. In this work, we propose an in-context
learning-based method aimed at improving sentence embeddings performance. Our
approach involves adapting the previous prompt-based representation method for
autoregressive models, constructing a demonstration set that enables LLMs to
perform in-context learning, and scaling up the LLMs to different model sizes.
Through extensive experiments, in-context learning enables LLMs to generate
high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve
performance comparable to current contrastive learning methods. By scaling
model size, we find scaling to more than tens of billion parameters harms the
performance on semantic textual similarity (STS) tasks. However, the largest
model outperforms other counterparts and achieves the new state-of-the-art
result on transfer tasks. We also fine-tune LLMs with current contrastive
learning approach, and the 2.7B OPT model, incorporating our prompt-based
method, surpasses the performance of 4.8B ST5, achieving the new
state-of-the-art results on STS tasks. Our code is available at
https://github.com/kongds/scaling_sentemb.",None,-1
4b15597e-8a4d-4c3b-983b-0b5327093cca,Quantifying Consistency and Information Loss for Causal Abstraction Learning,0.345791,3,"Structural causal models provide a formalism to express causal relations
between variables of interest. Models and variables can represent a system at
different levels of abstraction, whereby relations may be coarsened and refined
according to the need of a modeller. However, switching between different
levels of abstraction requires evaluating a trade-off between the consistency
and the information loss among different models. In this paper we introduce a
family of interventional measures that an agent may use to evaluate such a
trade-off. We consider four measures suited for different tasks, analyze their
properties, and propose algorithms to evaluate and learn causal abstractions.
Finally, we illustrate the flexibility of our setup by empirically showing how
different measures and algorithmic choices may lead to different abstractions.",None,-1
3dbcfa90-bfc5-4f97-9aea-20a1c7a24d0a,MLOps with enhanced performance control and observability,0.214456,1,"The explosion of data and its ever increasing complexity in the last few
years, has made MLOps systems more prone to failure, and new tools need to be
embedded in such systems to avoid such failure. In this demo, we will introduce
crucial tools in the observability module of a MLOps system that target
difficult issues like data drfit and model version control for optimum model
selection. We believe integrating these features in our MLOps pipeline would go
a long way in building a robust system immune to early stage ML system
failures.",None,-1
d008a377-b9e8-4ea0-8535-f054442db789,Toward Sufficient Spatial-Frequency Interaction for Gradient-aware Underwater Image Enhancement,0.619645,2,"Underwater images suffer from complex and diverse degradation, which
inevitably affects the performance of underwater visual tasks. However, most
existing learning-based Underwater image enhancement (UIE) methods mainly
restore such degradations in the spatial domain, and rarely pay attention to
the fourier frequency information. In this paper, we develop a novel UIE
framework based on spatial-frequency interaction and gradient maps, namely
SFGNet, which consists of two stages. Specifically, in the first stage, we
propose a dense spatial-frequency fusion network (DSFFNet), mainly including
our designed dense fourier fusion block and dense spatial fusion block,
achieving sufficient spatial-frequency interaction by cross connections between
these two blocks. In the second stage, we propose a gradient-aware corrector
(GAC) to further enhance perceptual details and geometric structures of images
by gradient map. Experimental results on two real-world underwater image
datasets show that our approach can successfully enhance underwater images, and
achieves competitive performance in visual quality improvement. The code is
available at https://github.com/zhihefang/SFGNet.",None,-1
0e5738d9-5b02-44b4-a37e-ebc925d3fcd7,Modern Bayesian Experimental Design,0.963316,30,"Bayesian experimental design (BED) provides a powerful and general framework
for optimizing the design of experiments. However, its deployment often poses
substantial computational challenges that can undermine its practical use. In
this review, we outline how recent advances have transformed our ability to
overcome these challenges and thus utilize BED effectively, before discussing
some key areas for future development in the field.",None,-1
7f9d7464-f340-4325-bc66-80d23e719c93,Explicit Visual Prompting for Low-Level Structure Segmentations,0.999731,54,"We consider the generic problem of detecting low-level structures in images,
which includes segmenting the manipulated parts, identifying out-of-focus
pixels, separating shadow regions, and detecting concealed objects. Whereas
each such topic has been typically addressed with a domain-specific solution,
we show that a unified approach performs well across all of them. We take
inspiration from the widely-used pre-training and then prompt tuning protocols
in NLP and propose a new visual prompting model, named Explicit Visual
Prompting (EVP). Different from the previous visual prompting which is
typically a dataset-level implicit embedding, our key insight is to enforce the
tunable parameters focusing on the explicit visual content from each individual
image, i.e., the features from frozen patch embeddings and the input's
high-frequency components. The proposed EVP significantly outperforms other
parameter-efficient tuning protocols under the same amount of tunable
parameters (5.7% extra trainable parameters of each task). EVP also achieves
state-of-the-art performances on diverse low-level structure segmentation tasks
compared to task-specific solutions. Our code is available at:
https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",None,-1
d807ef5f-3676-4863-810f-24ccb67fbe12,Can LLMs facilitate interpretation of pre-trained language models?,0.544927,6,"Work done to uncover the knowledge encoded within pre-trained language models
rely on annotated corpora or human-in-the-loop methods. However, these
approaches are limited in terms of scalability and the scope of interpretation.
We propose using a large language model, ChatGPT, as an annotator to enable
fine-grained interpretation analysis of pre-trained language models. We
discover latent concepts within pre-trained language models by applying
agglomerative hierarchical clustering over contextualized representations and
then annotate these concepts using ChatGPT. Our findings demonstrate that
ChatGPT produces accurate and semantically richer annotations compared to
human-annotated concepts. Additionally, we showcase how GPT-based annotations
empower interpretation analysis methodologies of which we demonstrate two:
probing frameworks and neuron interpretation. To facilitate further exploration
and experimentation in the field, we make available a substantial ConceptNet
dataset (TCN) comprising 39,000 annotated concepts.",None,-1
fd8d8762-9e2a-48d6-8b0e-2142209eadb9,Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs,0.45925,20,"As foundation models continue to exponentially scale in size, efficient
methods of adaptation become increasingly critical. Parameter-efficient
fine-tuning (PEFT), a recent class of techniques that require only modifying a
small percentage of the model parameters, is currently the most popular method
for adapting large language models (LLMs). Several PEFT techniques have
recently been proposed with varying tradeoffs. We provide a comprehensive and
uniform benchmark of various PEFT techniques across a representative LLM, the
FLAN-T5 model, and evaluate model performance across different data scales of
classification and generation datasets. Based on this, we provide a framework
for choosing the optimal fine-tuning techniques given the task type and data
availability. Contrary to popular belief, we also empirically prove that PEFT
techniques converge slower than full tuning in low data scenarios, and posit
the amount of data required for PEFT methods to both perform well and converge
efficiently. Lastly, we further optimize these PEFT techniques by selectively
choosing which parts of the model to train, and find that these techniques can
be applied with significantly fewer parameters while maintaining and even
improving performance.",None,-1
2f0eb29f-f81b-4f4a-b800-c0465aafaa31,Spatial-Frequency Attention for Image Denoising,0.353981,7,"The recently developed transformer networks have achieved impressive
performance in image denoising by exploiting the self-attention (SA) in images.
However, the existing methods mostly use a relatively small window to compute
SA due to the quadratic complexity of it, which limits the model's ability to
model long-term image information. In this paper, we propose the
spatial-frequency attention network (SFANet) to enhance the network's ability
in exploiting long-range dependency. For spatial attention module (SAM), we
adopt dilated SA to model long-range dependency. In the frequency attention
module (FAM), we exploit more global information by using Fast Fourier
Transform (FFT) by designing a window-based frequency channel attention (WFCA)
block to effectively model deep frequency features and their dependencies. To
make our module applicable to images of different sizes and keep the model
consistency between training and inference, we apply window-based FFT with a
set of fixed window sizes. In addition, channel attention is computed on both
real and imaginary parts of the Fourier spectrum, which further improves
restoration performance. The proposed WFCA block can effectively model image
long-range dependency with acceptable complexity. Experiments on multiple
denoising benchmarks demonstrate the leading performance of SFANet network.",None,-1
839a269c-9dde-4d46-93f0-a47e1800433c,Exploiting the Textual Potential from Vision-Language Pre-training for Text-based Person Search,0.733832,6,"Text-based Person Search (TPS), is targeted on retrieving pedestrians to
match text descriptions instead of query images. Recent Vision-Language
Pre-training (VLP) models can bring transferable knowledge to downstream TPS
tasks, resulting in more efficient performance gains. However, existing TPS
methods improved by VLP only utilize pre-trained visual encoders, neglecting
the corresponding textual representation and breaking the significant modality
alignment learned from large-scale pre-training. In this paper, we explore the
full utilization of textual potential from VLP in TPS tasks. We build on the
proposed VLP-TPS baseline model, which is the first TPS model with both
pre-trained modalities. We propose the Multi-Integrity Description Constraints
(MIDC) to enhance the robustness of the textual modality by incorporating
different components of fine-grained corpus during training. Inspired by the
prompt approach for zero-shot classification with VLP models, we propose the
Dynamic Attribute Prompt (DAP) to provide a unified corpus of fine-grained
attributes as language hints for the image modality. Extensive experiments show
that our proposed TPS framework achieves state-of-the-art performance,
exceeding the previous best method by a margin.",None,-1
5eb9f7fc-ad77-48b3-a614-b3ba4c56cef6,Beyond Toxic: Toxicity Detection Datasets are Not Enough for Brand Safety,0.0455853,1,"The rapid growth in user generated content on social media has resulted in a
significant rise in demand for automated content moderation. Various methods
and frameworks have been proposed for the tasks of hate speech detection and
toxic comment classification. In this work, we combine common datasets to
extend these tasks to brand safety. Brand safety aims to protect commercial
branding by identifying contexts where advertisements should not appear and
covers not only toxicity, but also other potentially harmful content. As these
datasets contain different label sets, we approach the overall problem as a
binary classification task. We demonstrate the need for building brand safety
specific datasets via the application of common toxicity detection datasets to
a subset of brand safety and empirically analyze the effects of weighted
sampling strategies in text classification.",None,-1
9978a14e-5cf6-4c0a-954d-62572350da51,Automated Reading Passage Generation with OpenAI's Large Language Model,0.462899,6,"The widespread usage of computer-based assessments and individualized
learning platforms has resulted in an increased demand for the rapid production
of high-quality items. Automated item generation (AIG), the process of using
item models to generate new items with the help of computer technology, was
proposed to reduce reliance on human subject experts at each step of the
process. AIG has been used in test development for some time. Still, the use of
machine learning algorithms has introduced the potential to improve the
efficiency and effectiveness of the process greatly. The approach presented in
this paper utilizes OpenAI's latest transformer-based language model, GPT-3, to
generate reading passages. Existing reading passages were used in carefully
engineered prompts to ensure the AI-generated text has similar content and
structure to a fourth-grade reading passage. For each prompt, we generated
multiple passages, the final passage was selected according to the Lexile score
agreement with the original passage. In the final round, the selected passage
went through a simple revision by a human editor to ensure the text was free of
any grammatical and factual errors. All AI-generated passages, along with
original passages were evaluated by human judges according to their coherence,
appropriateness to fourth graders, and readability.",None,-1
2ea77c20-3f4e-4b22-98bc-0249ca1f5ace,Causal-Discovery Performance of ChatGPT in the context of Neuropathic Pain Diagnosis,0.702924,25,"ChatGPT has demonstrated exceptional proficiency in natural language
conversation, e.g., it can answer a wide range of questions while no previous
large language models can. Thus, we would like to push its limit and explore
its ability to answer causal discovery questions by using a medical benchmark
(Tu et al. 2019) in causal discovery.",None,-1
f62097ad-f8aa-4dc6-9015-d425d5eaf20b,GaitFormer: Revisiting Intrinsic Periodicity for Gait Recognition,0.324766,2,"Gait recognition aims to distinguish different walking patterns by analyzing
video-level human silhouettes, rather than relying on appearance information.
Previous research on gait recognition has primarily focused on extracting local
or global spatial-temporal representations, while overlooking the intrinsic
periodic features of gait sequences, which, when fully utilized, can
significantly enhance performance. In this work, we propose a plug-and-play
strategy, called Temporal Periodic Alignment (TPA), which leverages the
periodic nature and fine-grained temporal dependencies of gait patterns. The
TPA strategy comprises two key components. The first component is Adaptive
Fourier-transform Position Encoding (AFPE), which adaptively converts features
and discrete-time signals into embeddings that are sensitive to periodic
walking patterns. The second component is the Temporal Aggregation Module
(TAM), which separates embeddings into trend and seasonal components, and
extracts meaningful temporal correlations to identify primary components, while
filtering out random noise. We present a simple and effective baseline method
for gait recognition, based on the TPA strategy. Extensive experiments
conducted on three popular public datasets (CASIA-B, OU-MVLP, and GREW)
demonstrate that our proposed method achieves state-of-the-art performance on
multiple benchmark tests.",None,-1
47c12479-8b40-41dd-8e5c-931cb5d77593,Large Language Models can Learn Rules,0.981307,25,"When prompted with a few examples and intermediate steps, large language
models (LLMs) have demonstrated impressive performance in various reasoning
tasks. However, prompting methods that rely on implicit knowledge in an LLM
often generate incorrect answers when the implicit knowledge is wrong or
inconsistent with the task. To tackle this problem, we present
Hypotheses-to-Theories (HtT), a framework that learns a rule library for
reasoning with LLMs. HtT contains two stages, an induction stage and a
deduction stage. In the induction stage, an LLM is first asked to generate and
verify rules over a set of training examples. Rules that appear and lead to
correct answers sufficiently often are collected to form a rule library. In the
deduction stage, the LLM is then prompted to employ the learned rule library to
perform reasoning to answer test questions. Experiments on relational
reasoning, numerical reasoning and concept learning problems show that HtT
improves existing prompting methods, with an absolute gain of 10-30% in
accuracy. The learned rules are also transferable to different models and to
different forms of the same problem.",None,-1
4ccfd000-fec0-4701-a052-a1c42e84ba3a,Text revision in Scientific Writing Assistance: An Overview,0.164946,2,"Writing a scientific article is a challenging task as it is a highly codified
genre. Good writing skills are essential to properly convey ideas and results
of research work. Since the majority of scientific articles are currently
written in English, this exercise is all the more difficult for non-native
English speakers as they additionally have to face language issues. This
article aims to provide an overview of text revision in writing assistance in
the scientific domain.
  We will examine the specificities of scientific writing, including the format
and conventions commonly used in research articles.
  Additionally, this overview will explore the various types of writing
assistance tools available for text revision. Despite the evolution of the
technology behind these tools through the years, from rule-based approaches to
deep neural-based ones, challenges still exist (tools' accessibility, limited
consideration of the context, inexplicit use of discursive information, etc.)",None,-1
51d89fb2-1d81-4ddb-997d-02c89fd402e7,Target-Aware Spatio-Temporal Reasoning via Answering Questions in Dynamics Audio-Visual Scenarios,0.173986,2,"Audio-visual question answering (AVQA) is a challenging task that requires
multistep spatio-temporal reasoning over multimodal contexts. Recent works rely
on elaborate target-agnostic parsing of audio-visual scenes for spatial
grounding while mistreating audio and video as separate entities for temporal
grounding. This paper proposes a new target-aware joint spatio-temporal
grounding network for AVQA. It consists of two key components: the target-aware
spatial grounding module (TSG) and the single-stream joint audio-visual
temporal grounding module (JTG). The TSG can focus on audio-visual cues
relevant to the query subject by utilizing explicit semantics from the
question. Unlike previous two-stream temporal grounding modules that required
an additional audio-visual fusion module, JTG incorporates audio-visual fusion
and question-aware temporal grounding into one module with a simpler
single-stream architecture. The temporal synchronization between audio and
video in the JTG is facilitated by our proposed cross-modal synchrony loss
(CSL). Extensive experiments verified the effectiveness of our proposed method
over existing state-of-the-art methods.",None,-1
5a7fb0a7-36a1-4bae-911f-ca1e520e1496,Task-Driven Graph Attention for Hierarchical Relational Object Navigation,0.689056,3,"Embodied AI agents in large scenes often need to navigate to find objects. In
this work, we study a naturally emerging variant of the object navigation task,
hierarchical relational object navigation (HRON), where the goal is to find
objects specified by logical predicates organized in a hierarchical structure -
objects related to furniture and then to rooms - such as finding an apple on
top of a table in the kitchen. Solving such a task requires an efficient
representation to reason about object relations and correlate the relations in
the environment and in the task goal. HRON in large scenes (e.g. homes) is
particularly challenging due to its partial observability and long horizon,
which invites solutions that can compactly store the past information while
effectively exploring the scene. We demonstrate experimentally that scene
graphs are the best-suited representation compared to conventional
representations such as images or 2D maps. We propose a solution that uses
scene graphs as part of its input and integrates graph neural networks as its
backbone, with an integrated task-driven attention mechanism, and demonstrate
its better scalability and learning efficiency than state-of-the-art baselines.",None,-1
f19bc3a5-c000-4c7e-a0dd-9995ebf76327,Let Me Teach You: Pedagogical Foundations of Feedback for Language Models,0.15802,2,"Natural Language Feedback (NLF) is an increasingly popular mechanism for
aligning Large Language Models (LLMs) to human preferences. Despite the
diversity of the information it can convey, NLF methods are often hand-designed
and arbitrary, with little systematic grounding. At the same time, research in
learning sciences has long established several effective feedback models. In
this opinion piece, we compile ideas from pedagogy to introduce FELT, a
feedback framework for LLMs that outlines various characteristics of the
feedback space, and a feedback content taxonomy based on these variables,
providing a general mapping of the feedback space. In addition to streamlining
NLF designs, FELT also brings out new, unexplored directions for research in
NLF. We make our taxonomy available to the community, providing guides and
examples for mapping our categorizations to future research.",None,-1
0c95ff87-f3bd-494f-af82-32d6413f550f,"Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",0.71995,34,"Biomedical literature is growing rapidly, making it challenging to curate and
extract knowledge manually. Biomedical natural language processing (BioNLP)
techniques that can automatically extract information from biomedical
literature help alleviate this burden. Recently, large Language Models (LLMs),
such as GPT-3 and GPT-4, have gained significant attention for their impressive
performance. However, their effectiveness in BioNLP tasks and impact on method
development and downstream users remain understudied. This pilot study (1)
establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and
one-shot settings in eight BioNLP datasets across four applications: named
entity recognition, relation extraction, multi-label document classification,
and semantic similarity and reasoning, (2) examines the errors produced by the
LLMs and categorized the errors into three types: missingness, inconsistencies,
and unwanted artificial content, and (3) provides suggestions for using LLMs in
BioNLP applications. We make the datasets, baselines, and results publicly
available to the community via
https://github.com/qingyu-qc/gpt_bionlp_benchmark.",None,-1
57e6e97e-f33c-4096-8af2-1602a0132383,From Shortcuts to Triggers: Backdoor Defense with Denoised PoE,0.704826,11,"Language models are often at risk of diverse backdoor attacks, especially
data poisoning. Thus, it is important to investigate defense solutions for
addressing them. Existing backdoor defense methods mainly focus on backdoor
attacks with explicit triggers, leaving a universal defense against various
backdoor attacks with diverse triggers largely unexplored. In this paper, we
propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised
Product-of-Experts), which is inspired by the shortcut nature of backdoor
attacks, to defend various backdoor attacks. DPoE consists of two models: a
shallow model that captures the backdoor shortcuts and a main model that is
prevented from learning the backdoor shortcuts. To address the label flip
caused by backdoor attackers, DPoE incorporates a denoising design. Experiments
on SST-2 dataset show that DPoE significantly improves the defense performance
against various types of backdoor triggers including word-level,
sentence-level, and syntactic triggers. Furthermore, DPoE is also effective
under a more challenging but practical setting that mixes multiple types of
trigger.",None,-1
63790b73-df25-4c06-b528-e5bf11d5d49a,Young Labeled Faces in the Wild (YLFW): A Dataset for Children Faces Recognition,0.256139,2,"Face recognition has achieved outstanding performance in the last decade with
the development of deep learning techniques.
  Nowadays, the challenges in face recognition are related to specific
scenarios, for instance, the performance under diverse image quality, the
robustness for aging and edge cases of person age (children and elders),
distinguishing of related identities.
  In this set of problems, recognizing children's faces is one of the most
sensitive and important. One of the reasons for this problem is the existing
bias towards adults in existing face datasets.
  In this work, we present a benchmark dataset for children's face recognition,
which is compiled similarly to the famous face recognition benchmarks LFW,
CALFW, CPLFW, XQLFW and AgeDB.
  We also present a development dataset (separated into train and test parts)
for adapting face recognition models for face images of children.
  The proposed data is balanced for African, Asian, Caucasian, and Indian
races. To the best of our knowledge, this is the first standartized data tool
set for benchmarking and the largest collection for development for children's
face recognition. Several face recognition experiments are presented to
demonstrate the performance of the proposed data tool set.",None,-1
80bfeff2-62c8-4b19-93e2-965c7b30aa82,How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts,0.598961,11,"In recent years, there has been a rapid proliferation of AI-generated text,
primarily driven by the release of powerful pre-trained language models (PLMs).
To address the issue of misuse associated with AI-generated text, various
high-performing detectors have been developed, including the OpenAI detector
and the Stanford DetectGPT. In our study, we ask how reliable these detectors
are. We answer the question by designing a novel approach that can prompt any
PLM to generate text that evades these high-performing detectors. The proposed
approach suggests a universal evasive prompt, a novel type of soft prompt,
which guides PLMs in producing ""human-like"" text that can mislead the
detectors. The novel universal evasive prompt is achieved in two steps: First,
we create an evasive soft prompt tailored to a specific PLM through prompt
tuning; and then, we leverage the transferability of soft prompts to transfer
the learned evasive soft prompt from one PLM to another. Employing multiple
PLMs in various writing tasks, we conduct extensive experiments to evaluate the
efficacy of the evasive soft prompts in their evasion of state-of-the-art
detectors.",None,-1
0a968261-697a-49d2-98f1-51851aa939eb,PMatch: Paired Masked Image Modeling for Dense Geometric Matching,0.772557,7,"Dense geometric matching determines the dense pixel-wise correspondence
between a source and support image corresponding to the same 3D structure.
Prior works employ an encoder of transformer blocks to correlate the two-frame
features. However, existing monocular pretraining tasks, e.g., image
classification, and masked image modeling (MIM), can not pretrain the
cross-frame module, yielding less optimal performance. To resolve this, we
reformulate the MIM from reconstructing a single masked image to reconstructing
a pair of masked images, enabling the pretraining of transformer module.
Additionally, we incorporate a decoder into pretraining for improved upsampling
results. Further, to be robust to the textureless area, we propose a novel
cross-frame global matching module (CFGM). Since the most textureless area is
planar surfaces, we propose a homography loss to further regularize its
learning. Combined together, we achieve the State-of-The-Art (SoTA) performance
on geometric matching. Codes and models are available at
https://github.com/ShngJZ/PMatch.",None,-1
38ef875b-e4a5-4d18-b5fa-cc4fd9eb3cd8,"MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge",0.994336,7,"Solving mechanics problems using numerical methods requires comprehensive
intelligent capability of retrieving relevant knowledge and theory,
constructing and executing codes, analyzing the results, a task that has thus
far mainly been reserved for humans. While emerging AI methods can provide
effective approaches to solve end-to-end problems, for instance via the use of
deep surrogate models or various data analytics strategies, they often lack
physical intuition since knowledge is baked into the parametric complement
through training, offering less flexibility when it comes to incorporating
mathematical or physical insights. By leveraging diverse capabilities of
multiple dynamically interacting large language models (LLMs), we can overcome
the limitations of conventional approaches and develop a new class of
physics-inspired generative machine learning platform, here referred to as
MechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for
elasticity problems, via autonomous collaborations. A two-agent team can
effectively write, execute and self-correct code, in order to apply finite
element methods to solve classical elasticity problems in various flavors
(different boundary conditions, domain geometries, meshes, small/finite
deformation and linear/hyper-elastic constitutive laws, and others). For more
complex tasks, we construct a larger group of agents with enhanced division of
labor among planning, formulating, coding, executing and criticizing the
process and results. The agents mutually correct each other to improve the
overall team-work performance in understanding, formulating and validating the
solution. Our framework shows the potential of synergizing the intelligence of
language models, the reliability of physics-based modeling, and the dynamic
collaborations among diverse agents, opening novel avenues for automation of
solving engineering problems.",None,-1
ce8d2562-5ee5-4c3c-b913-551ce189fb35,General Method for Solving Four Types of SAT Problems,0.641623,1,"Existing methods provide varying algorithms for different types of Boolean
satisfiability problems (SAT), lacking a general solution framework.
Accordingly, this study proposes a unified framework DCSAT based on integer
programming and reinforcement learning (RL) algorithm to solve different types
of SAT problems such as MaxSAT, Weighted MaxSAT, PMS, WPMS. Specifically, we
first construct a consolidated integer programming representation for four
types of SAT problems by adjusting objective function coefficients. Secondly,
we construct an appropriate reinforcement learning models based on the 0-1
integer programming for SAT problems. Based on the binary tree search
structure, we apply the Monte Carlo tree search (MCTS) method on SAT problems.
Finally, we prove that this method can find all optimal Boolean assignments
based on Wiener-khinchin law of large Numbers. We experimentally verify that
this paradigm can prune the unnecessary search space to find the optimal
Boolean assignments for the problem. Furthermore, the proposed method can
provide diverse labels for supervised learning methods for SAT problems.",None,-1
b227ef49-5967-4331-8e89-65f71b81f009,Overcoming Prior Misspecification in Online Learning to Rank,0.122727,1,"The recent literature on online learning to rank (LTR) has established the
utility of prior knowledge to Bayesian ranking bandit algorithms. However, a
major limitation of existing work is the requirement for the prior used by the
algorithm to match the true prior. In this paper, we propose and analyze
adaptive algorithms that address this issue and additionally extend these
results to the linear and generalized linear models. We also consider scalar
relevance feedback on top of click feedback. Moreover, we demonstrate the
efficacy of our algorithms using both synthetic and real-world experiments.",None,-1
118ac28b-3b74-42ad-ac17-c6c7afbee96a,Modeling Relational Patterns for Logical Query Answering over Knowledge Graphs,0.0931086,1,"Answering first-order logical (FOL) queries over knowledge graphs (KG)
remains a challenging task mainly due to KG incompleteness. Query embedding
approaches this problem by computing the low-dimensional vector representations
of entities, relations, and logical queries. KGs exhibit relational patterns
such as symmetry and composition and modeling the patterns can further enhance
the performance of query embedding models. However, the role of such patterns
in answering FOL queries by query embedding models has not been yet studied in
the literature. In this paper, we fill in this research gap and empower FOL
queries reasoning with pattern inference by introducing an inductive bias that
allows for learning relation patterns. To this end, we develop a novel query
embedding method, RoConE, that defines query regions as geometric cones and
algebraic query operators by rotations in complex space. RoConE combines the
advantages of Cone as a well-specified geometric representation for query
embedding, and also the rotation operator as a powerful algebraic operation for
pattern inference. Our experimental results on several benchmark datasets
confirm the advantage of relational patterns for enhancing logical query
answering task.",None,-1
25c01cda-cecb-477b-82df-23e7a12eea45,Direct Parameterization of Lipschitz-Bounded Deep Networks,0.996865,25,"This paper introduces a new parameterization of deep neural networks (both
fully-connected and convolutional) with guaranteed $\ell^2$ Lipschitz bounds,
i.e. limited sensitivity to input perturbations. The Lipschitz guarantees are
equivalent to the tightest-known bounds based on certification via a
semidefinite program (SDP). We provide a ``direct'' parameterization, i.e., a
smooth mapping from $\mathbb R^N$ onto the set of weights satisfying the
SDP-based bound. Moreover, our parameterization is complete, i.e. a neural
network satisfies the SDP bound if and only if it can be represented via our
parameterization. This enables training using standard gradient methods,
without any inner approximation or computationally intensive tasks (e.g.
projections or barrier terms) for the SDP constraint. The new parameterization
can equivalently be thought of as either a new layer type (the \textit{sandwich
layer}), or a novel parameterization of standard feedforward networks with
parameter sharing between neighbouring layers. A comprehensive set of
experiments on image classification shows that sandwich layers outperform
previous approaches on both empirical and certified robust accuracy. Code is
available at \url{https://github.com/acfr/LBDN}.",None,-1
f1c28345-28b8-43bc-a584-29b8a59bd37e,TopEx: Topic-based Explanations for Model Comparison,0.0813301,2,"Meaningfully comparing language models is challenging with current
explanation methods. Current explanations are overwhelming for humans due to
large vocabularies or incomparable across models. We present TopEx, an
explanation method that enables a level playing field for comparing language
models via model-agnostic topics. We demonstrate how TopEx can identify
similarities and differences between DistilRoBERTa and GPT-2 on a variety of
NLP tasks.",None,-1
67c97e1c-dcca-47db-8eb3-4b9b42289ab0,SegGPT: Segmenting Everything In Context,0.982141,120,"We present SegGPT, a generalist model for segmenting everything in context.
We unify various segmentation tasks into a generalist in-context learning
framework that accommodates different kinds of segmentation data by
transforming them into the same format of images. The training of SegGPT is
formulated as an in-context coloring problem with random color mapping for each
data sample. The objective is to accomplish diverse tasks according to the
context, rather than relying on specific colors. After training, SegGPT can
perform arbitrary segmentation tasks in images or videos via in-context
inference, such as object instance, stuff, part, contour, and text. SegGPT is
evaluated on a broad range of tasks, including few-shot semantic segmentation,
video object segmentation, semantic segmentation, and panoptic segmentation.
Our results show strong capabilities in segmenting in-domain and out-of-domain
targets, either qualitatively or quantitatively.",None,-1
4e036096-4dcd-47bf-8db7-5ef0801072bb,Constraints First: A New MDD-based Model to Generate Sentences Under Constraints,0.0437489,2,"This paper introduces a new approach to generating strongly constrained
texts. We consider standardized sentence generation for the typical application
of vision screening. To solve this problem, we formalize it as a discrete
combinatorial optimization problem and utilize multivalued decision diagrams
(MDD), a well-known data structure to deal with constraints. In our context,
one key strength of MDD is to compute an exhaustive set of solutions without
performing any search. Once the sentences are obtained, we apply a language
model (GPT-2) to keep the best ones. We detail this for English and also for
French where the agreement and conjugation rules are known to be more complex.
Finally, with the help of GPT-2, we get hundreds of bona-fide candidate
sentences. When compared with the few dozen sentences usually available in the
well-known vision screening test (MNREAD), this brings a major breakthrough in
the field of standardized sentence generation. Also, as it can be easily
adapted for other languages, it has the potential to make the MNREAD test even
more valuable and usable. More generally, this paper highlights MDD as a
convincing alternative for constrained text generation, especially when the
constraints are hard to satisfy, but also for many other prospects.",None,-1
802f772c-a830-4928-a188-08a737f1dfb8,Improving the Inference of Topic Models via Infinite Latent State Replications,0.0379388,1,"In text mining, topic models are a type of probabilistic generative models
for inferring latent semantic topics from text corpus. One of the most popular
inference approaches to topic models is perhaps collapsed Gibbs sampling (CGS),
which typically samples one single topic label for each observed document-word
pair. In this paper, we aim at improving the inference of CGS for topic models.
We propose to leverage state augmentation technique by maximizing the number of
topic samples to infinity, and then develop a new inference approach, called
infinite latent state replication (ILR), to generate robust soft topic
assignment for each given document-word pair. Experimental results on the
publicly available datasets show that ILR outperforms CGS for inference of
existing established topic models.",None,-1
93d46a3f-b051-4bd0-bfd0-e41dff17f114,2-D SSM: A General Spatial Layer for Visual Transformers,0.622787,10,"A central objective in computer vision is to design models with appropriate
2-D inductive bias. Desiderata for 2D inductive bias include two-dimensional
position awareness, dynamic spatial locality, and translation and permutation
invariance. To address these goals, we leverage an expressive variation of the
multidimensional State Space Model (SSM). Our approach introduces efficient
parameterization, accelerated computation, and a suitable normalization scheme.
Empirically, we observe that incorporating our layer at the beginning of each
transformer block of Vision Transformers (ViT) significantly enhances
performance for multiple ViT backbones and across datasets. The new layer is
effective even with a negligible amount of additional parameters and inference
time. Ablation studies and visualizations demonstrate that the layer has a
strong 2-D inductive bias. For example, vision transformers equipped with our
layer exhibit effective performance even without positional encoding",None,-1
d014836a-e3a4-4c43-bb4a-9a40babd0735,Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving,0.999872,84,"Robotic perception requires the modeling of both 3D geometry and semantics.
Existing methods typically focus on estimating 3D bounding boxes, neglecting
finer geometric details and struggling to handle general, out-of-vocabulary
objects. 3D occupancy prediction, which estimates the detailed occupancy states
and semantics of a scene, is an emerging task to overcome these limitations. To
support 3D occupancy prediction, we develop a label generation pipeline that
produces dense, visibility-aware labels for any given scene. This pipeline
comprises three stages: voxel densification, occlusion reasoning, and
image-guided voxel refinement. We establish two benchmarks, derived from the
Waymo Open Dataset and the nuScenes Dataset, namely Occ3D-Waymo and
Occ3D-nuScenes benchmarks. Furthermore, we provide an extensive analysis of the
proposed dataset with various baseline models. Lastly, we propose a new model,
dubbed Coarse-to-Fine Occupancy (CTF-Occ) network, which demonstrates superior
performance on the Occ3D benchmarks. The code, data, and benchmarks are
released at https://tsinghua-mars-lab.github.io/Occ3D/.",None,-1
1dc308bc-d4bb-4656-a86a-c2c64774f7b6,In-Context Learning Unlocked for Diffusion Models,0.509422,45,"We present Prompt Diffusion, a framework for enabling in-context learning in
diffusion-based generative models. Given a pair of task-specific example
images, such as depth from/to image and scribble from/to image, and a text
guidance, our model automatically understands the underlying task and performs
the same task on a new query image following the text guidance. To achieve
this, we propose a vision-language prompt that can model a wide range of
vision-language tasks and a diffusion model that takes it as input. The
diffusion model is trained jointly over six different tasks using these
prompts. The resulting Prompt Diffusion model is the first diffusion-based
vision-language foundation model capable of in-context learning. It
demonstrates high-quality in-context generation on the trained tasks and
generalizes effectively to new, unseen vision tasks with their respective
prompts. Our model also shows compelling text-guided image editing results. Our
framework aims to facilitate research into in-context learning for computer
vision. We share our code and pre-trained models at
https://github.com/Zhendong-Wang/Prompt-Diffusion.",None,-1
90f7a7a8-0661-4c16-8d06-9bba4155cb6d,Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected,0.848982,29,"Meta AI Research has recently released SAM (Segment Anything Model) which is
trained on a large segmentation dataset of over 1 billion masks. As a
foundation model in the field of computer vision, SAM (Segment Anything Model)
has gained attention for its impressive performance in generic object
segmentation. Despite its strong capability in a wide range of zero-shot
transfer tasks, it remains unknown whether SAM can detect things in challenging
setups like transparent objects. In this work, we perform an empirical
evaluation of two glass-related challenging scenarios: mirror and transparent
objects. We found that SAM often fails to detect the glass in both scenarios,
which raises concern for deploying the SAM in safety-critical situations that
have various forms of glass.",None,-1
686f9626-2866-4631-89e2-03675ab9be0a,"KBody: Towards general, robust, and aligned monocular whole-body estimation",0.387632,3,"KBody is a method for fitting a low-dimensional body model to an image. It
follows a predict-and-optimize approach, relying on data-driven model estimates
for the constraints that will be used to solve for the body's parameters.
Acknowledging the importance of high quality correspondences, it leverages
``virtual joints"" to improve fitting performance, disentangles the optimization
between the pose and shape parameters, and integrates asymmetric distance
fields to strike a balance in terms of pose and shape capturing capacity, as
well as pixel alignment. We also show that generative model inversion offers a
strong appearance prior that can be used to complete partial human images and
used as a building block for generalized and robust monocular body fitting.
Project page: https://zokin.github.io/KBody.",None,-1
69db896f-a53b-44a9-84b3-a90db0a4bef2,Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks,0.311274,5,"Feature selection that selects an informative subset of variables from data
not only enhances the model interpretability and performance but also
alleviates the resource demands. Recently, there has been growing attention on
feature selection using neural networks. However, existing methods usually
suffer from high computational costs when applied to high-dimensional datasets.
In this paper, inspired by evolution processes, we propose a novel
resource-efficient supervised feature selection method using sparse neural
networks, named \enquote{NeuroFS}. By gradually pruning the uninformative
features from the input layer of a sparse neural network trained from scratch,
NeuroFS derives an informative subset of features efficiently. By performing
several experiments on $11$ low and high-dimensional real-world benchmarks of
different types, we demonstrate that NeuroFS achieves the highest ranking-based
score among the considered state-of-the-art supervised feature selection
models. The code is available on GitHub.",None,-1
9e9af261-c9b7-4fcc-8e2b-1d7ab6b0487b,ProtoCaps: A Fast and Non-Iterative Capsule Network Routing Method,0.433811,3,"Capsule Networks have emerged as a powerful class of deep learning
architectures, known for robust performance with relatively few parameters
compared to Convolutional Neural Networks (CNNs). However, their inherent
efficiency is often overshadowed by their slow, iterative routing mechanisms
which establish connections between Capsule layers, posing computational
challenges resulting in an inability to scale. In this paper, we introduce a
novel, non-iterative routing mechanism, inspired by trainable prototype
clustering. This innovative approach aims to mitigate computational complexity,
while retaining, if not enhancing, performance efficacy. Furthermore, we
harness a shared Capsule subspace, negating the need to project each
lower-level Capsule to each higher-level Capsule, thereby significantly
reducing memory requisites during training. Our approach demonstrates superior
results compared to the current best non-iterative Capsule Network and tests on
the Imagewoof dataset, which is too computationally demanding to handle
efficiently by iterative approaches. Our findings underscore the potential of
our proposed methodology in enhancing the operational efficiency and
performance of Capsule Networks, paving the way for their application in
increasingly complex computational scenarios. Code is available at
https://github.com/mileseverett/ProtoCaps.",None,-1
dfd1501e-09d7-4700-abdf-69c61fd4d6e9,Have it your way: Individualized Privacy Assignment for DP-SGD,0.714111,8,"When training a machine learning model with differential privacy, one sets a
privacy budget. This budget represents a maximal privacy violation that any
user is willing to face by contributing their data to the training set. We
argue that this approach is limited because different users may have different
privacy expectations. Thus, setting a uniform privacy budget across all points
may be overly conservative for some users or, conversely, not sufficiently
protective for others. In this paper, we capture these preferences through
individualized privacy budgets. To demonstrate their practicality, we introduce
a variant of Differentially Private Stochastic Gradient Descent (DP-SGD) which
supports such individualized budgets. DP-SGD is the canonical approach to
training models with differential privacy. We modify its data sampling and
gradient noising mechanisms to arrive at our approach, which we call
Individualized DP-SGD (IDP-SGD). Because IDP-SGD provides privacy guarantees
tailored to the preferences of individual users and their data points, we find
it empirically improves privacy-utility trade-offs.",None,-1
796edb1f-4f16-4145-8b5f-0a8589f7d1fd,BIM-GPT: a Prompt-Based Virtual Assistant Framework for BIM Information Retrieval,0.612375,6,"Efficient information retrieval (IR) from building information models (BIMs)
poses significant challenges due to the necessity for deep BIM knowledge or
extensive engineering efforts for automation. We introduce BIM-GPT, a
prompt-based virtual assistant (VA) framework integrating BIM and generative
pre-trained transformer (GPT) technologies to support NL-based IR. A prompt
manager and dynamic template generate prompts for GPT models, enabling
interpretation of NL queries, summarization of retrieved information, and
answering BIM-related questions. In tests on a BIM IR dataset, our approach
achieved 83.5% and 99.5% accuracy rates for classifying NL queries with no data
and 2% data incorporated in prompts, respectively. Additionally, we validated
the functionality of BIM-GPT through a VA prototype for a hospital building.
This research contributes to the development of effective and versatile VAs for
BIM IR in the construction industry, significantly enhancing BIM accessibility
and reducing engineering efforts and training data requirements for processing
NL queries.",None,-1
01a95438-2499-42f2-9706-a3b025746a40,Can LLMs Fix Issues with Reasoning Models? Towards More Likely Models for AI Planning,0.513725,2,"This is the first work to look at the application of large language models
(LLMs) for the purpose of model space edits in automated planning tasks. To set
the stage for this union, we explore two different flavors of model space
problems that have been studied in the AI planning literature and explore the
effect of an LLM on those tasks. We empirically demonstrate how the performance
of an LLM contrasts with combinatorial search (CS) -- an approach that has been
traditionally used to solve model space tasks in planning, both with the LLM in
the role of a standalone model space reasoner as well as in the role of a
statistical signal in concert with the CS approach as part of a two-stage
process. Our experiments show promising results suggesting further forays of
LLMs into the exciting world of model space reasoning for planning tasks in the
future.",None,-1
19ff7a2c-282c-44af-9ae0-cb99d3d1008a,ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes,0.411352,19,"Understanding the continuous states of objects is essential for task learning
and planning in the real world. However, most existing task learning benchmarks
assume discrete (e.g., binary) object goal states, which poses challenges for
the learning of complex tasks and transferring learned policy from simulated
environments to the real world. Furthermore, state discretization limits a
robot's ability to follow human instructions based on the grounding of actions
and states. To tackle these challenges, we present ARNOLD, a benchmark that
evaluates language-grounded task learning with continuous states in realistic
3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve
understanding object states and learning policies for continuous goals. To
promote language-instructed learning, we provide expert demonstrations with
template-generated language descriptions. We assess task performance by
utilizing the latest language-conditioned policy learning models. Our results
indicate that current models for language-conditioned manipulations continue to
experience significant challenges in novel goal-state generalizations, scene
generalizations, and object generalizations. These findings highlight the need
to develop new algorithms that address this gap and underscore the potential
for further research in this area. Project website:
https://arnold-benchmark.github.io.",None,-1
dd9db295-d16c-4593-8713-414864e57d63,On the Risk of Misinformation Pollution with Large Language Models,0.343358,54,"In this paper, we comprehensively investigate the potential misuse of modern
Large Language Models (LLMs) for generating credible-sounding misinformation
and its subsequent impact on information-intensive applications, particularly
Open-Domain Question Answering (ODQA) systems. We establish a threat model and
simulate potential misuse scenarios, both unintentional and intentional, to
assess the extent to which LLMs can be utilized to produce misinformation. Our
study reveals that LLMs can act as effective misinformation generators, leading
to a significant degradation in the performance of ODQA systems. To mitigate
the harm caused by LLM-generated misinformation, we explore three defense
strategies: prompting, misinformation detection, and majority voting. While
initial results show promising trends for these defensive strategies, much more
work needs to be done to address the challenge of misinformation pollution. Our
work highlights the need for further research and interdisciplinary
collaboration to address LLM-generated misinformation and to promote
responsible use of LLMs.",None,-1
6d24d20b-e153-4593-bf34-dd50ff14476c,In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning,0.27981,11,"In this note, we explore inference-time alignment through in-context
learning. We consider a vanilla pretrained language model Llama-2 before any
fine-tuning and retrieve an average of 9 demonstration alignment examples when
the model is prompted to follow chat-style instructions. Compared to direct
prompting, the in-context alignment without changing model weights leads to a
7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making
the vanilla language model comparable to strong baselines with alignment
fine-tuning.",None,-1
e1578e65-8b56-4dad-b633-6c6e34f2d181,From task structures to world models: What do LLMs know?,0.513251,8,"In what sense does a large language model have knowledge? The answer to this
question extends beyond the capabilities of a particular AI system, and
challenges our assumptions about the nature of knowledge and intelligence. We
answer by granting LLMs ""instrumental knowledge""; knowledge defined by a
certain set of abilities. We then ask how such knowledge is related to the more
ordinary, ""worldly"" knowledge exhibited by human agents, and explore this in
terms of the degree to which instrumental knowledge can be said to incorporate
the structured world models of cognitive science. We discuss ways LLMs could
recover degrees of worldly knowledge, and suggest such recovery will be
governed by an implicit, resource-rational tradeoff between world models and
task demands.",None,-1
f6acfe0b-4596-40a1-aa0b-038c4a7856ef,Multimodal Chain-of-Thought Reasoning in Language Models,0.964243,215,"Large language models (LLMs) have shown impressive performance on complex
reasoning by leveraging chain-of-thought (CoT) prompting to generate
intermediate reasoning chains as the rationale to infer the answer. However,
existing CoT studies have primarily focused on the language modality. We
propose Multimodal-CoT that incorporates language (text) and vision (images)
modalities into a two-stage framework that separates rationale generation and
answer inference. In this way, answer inference can leverage better generated
rationales that are based on multimodal information. Experimental results on
ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed
approach. With Multimodal-CoT, our model under 1 billion parameters achieves
state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates
that Multimodal-CoT offers the advantages of mitigating hallucination and
enhancing convergence speed. Code is publicly available at
https://github.com/amazon-science/mm-cot.",None,-1
622dd007-2662-42b4-ba6f-35e59b8680fb,MarkQA: A large scale KBQA dataset with numerical reasoning,0.449994,2,"While question answering over knowledge bases (KBQA) has shown progress in
addressing factoid questions, KBQA with numerical reasoning remains relatively
unexplored. In this paper, we focus on the complex numerical reasoning in KBQA
and propose a new task, NR-KBQA, which necessitates the ability to perform both
multi-hop reasoning and numerical reasoning. We design a logic form in Python
format called PyQL to represent the reasoning process of numerical reasoning
questions. To facilitate the development of NR-KBQA, we present a large dataset
called MarkQA, which is automatically constructed from a small set of seeds.
Each question in MarkQA is equipped with its corresponding SPARQL query,
alongside the step-by-step reasoning process in the QDMR format and PyQL
program. Experimental results of some state-of-the-art QA methods on the MarkQA
show that complex numerical reasoning in KBQA faces great challenges.",None,-1
2d671eb4-84f0-463a-a377-c41cda2307d1,Compacting Binary Neural Networks by Sparse Kernel Selection,0.146174,1,"Binary Neural Network (BNN) represents convolution weights with 1-bit values,
which enhances the efficiency of storage and computation. This paper is
motivated by a previously revealed phenomenon that the binary kernels in
successful BNNs are nearly power-law distributed: their values are mostly
clustered into a small number of codewords. This phenomenon encourages us to
compact typical BNNs and obtain further close performance through learning
non-repetitive kernels within a binary kernel subspace. Specifically, we regard
the binarization process as kernel grouping in terms of a binary codebook, and
our task lies in learning to select a smaller subset of codewords from the full
codebook. We then leverage the Gumbel-Sinkhorn technique to approximate the
codeword selection process, and develop the Permutation Straight-Through
Estimator (PSTE) that is able to not only optimize the selection process
end-to-end but also maintain the non-repetitive occupancy of selected
codewords. Experiments verify that our method reduces both the model size and
bit-wise computational costs, and achieves accuracy improvements compared with
state-of-the-art BNNs under comparable budgets.",None,-1
b7fe3c1d-a308-47e4-9490-2cb9a3d05d68,"Non-deterministic approximation operators: ultimate operators, semi-equilibrium semantics and aggregates (full version)",0.232246,1,"Approximation fixpoint theory (AFT) is an abstract and general algebraic
framework for studying the semantics of non-monotonic logics. In recent work,
AFT was generalized to non-deterministic operators, i.e.\ operators whose range
are sets of elements rather than single elements. In this paper, we make three
further contributions to non-deterministic AFT: (1) we define and study
ultimate approximations of non-deterministic operators, (2) we give an
algebraic formulation of the semi-equilibrium semantics by Amendola, et al.,
and (3) we generalize the characterisations of disjunctive logic programs to
disjunctive logic programs with aggregates.",None,-1
a8cdd2d1-5fe0-4974-9d96-9232daaef2be,PLay: Parametrically Conditioned Layout Generation using Latent Diffusion,0.974155,20,"Layout design is an important task in various design fields, including user
interface, document, and graphic design. As this task requires tedious manual
effort by designers, prior works have attempted to automate this process using
generative models, but commonly fell short of providing intuitive user controls
and achieving design objectives. In this paper, we build a conditional latent
diffusion model, PLay, that generates parametrically conditioned layouts in
vector graphic space from user-specified guidelines, which are commonly used by
designers for representing their design intents in current practices. Our
method outperforms prior works across three datasets on metrics including FID
and FD-VG, and in user study. Moreover, it brings a novel and interactive
experience to professional layout design processes.",None,-1
3185c662-662a-4625-9f52-f9781cab3fe6,ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer,0.643781,39,"Large-scale language models, like ChatGPT, have garnered significant media
attention and stunned the public with their remarkable capacity for generating
coherent text from short natural language prompts. In this paper, we aim to
conduct a systematic inspection of ChatGPT's performance in two controllable
generation tasks, with respect to ChatGPT's ability to adapt its output to
different target audiences (expert vs. layman) and writing styles (formal vs.
informal). Additionally, we evaluate the faithfulness of the generated text,
and compare the model's performance with human-authored texts. Our findings
indicate that the stylistic variations produced by humans are considerably
larger than those demonstrated by ChatGPT, and the generated texts diverge from
human samples in several characteristics, such as the distribution of word
types. Moreover, we observe that ChatGPT sometimes incorporates factual errors
or hallucinations when adapting the text to suit a specific style.",None,-1
083cc60c-6a9c-450e-8fe9-6d441a6976ef,SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems,0.0618953,1,"Data imbalance in training data often leads to biased predictions from
trained models, which in turn causes ethical and social issues. A
straightforward solution is to carefully curate training data, but given the
enormous scale of modern neural networks, this is prohibitively labor-intensive
and thus impractical. Inspired by recent developments in generative models,
this paper explores the potential of synthetic data to address the data
imbalance problem. To be specific, our method, dubbed SYNAuG, leverages
synthetic data to equalize the unbalanced distribution of training data. Our
experiments demonstrate that, although a domain gap between real and synthetic
data exists, training with SYNAuG followed by fine-tuning with a few real
samples allows to achieve impressive performance on diverse tasks with
different data imbalance issues, surpassing existing task-specific methods for
the same purpose.",None,-1
0407b1f6-fd41-4027-9d32-8a6ba59c6d60,A Planning Ontology to Represent and Exploit Planning Knowledge for Performance Efficiency,0.369173,2,"Ontologies are known for their ability to organize rich metadata, support the
identification of novel insights via semantic queries, and promote reuse. In
this paper, we consider the problem of automated planning, where the objective
is to find a sequence of actions that will move an agent from an initial state
of the world to a desired goal state. We hypothesize that given a large number
of available planners and diverse planning domains; they carry essential
information that can be leveraged to identify suitable planners and improve
their performance for a domain. We use data on planning domains and planners
from the International Planning Competition (IPC) to construct a planning
ontology and demonstrate via experiments in two use cases that the ontology can
lead to the selection of promising planners and improving their performance
using macros - a form of action ordering constraints extracted from planning
ontology. We also make the planning ontology and associated resources available
to the community to promote further research.",None,-1
b28b2d3a-cb0e-429a-a72b-c3f73b38d18b,HMC: Hierarchical Mesh Coarsening for Skeleton-free Motion Retargeting,0.0446512,1,"We present a simple yet effective method for skeleton-free motion
retargeting. Previous methods transfer motion between high-resolution meshes,
failing to preserve the inherent local-part motions in the mesh. Addressing
this issue, our proposed method learns the correspondence in a coarse-to-fine
fashion by integrating the retargeting process with a mesh-coarsening pipeline.
First, we propose a mesh-coarsening module that coarsens the mesh
representations for better motion transfer. This module improves the ability to
handle small-part motion and preserves the local motion interdependence between
neighboring mesh vertices. Furthermore, we leverage a hierarchical refinement
procedure to complement missing mesh details by gradually improving the
low-resolution mesh output with a higher-resolution one. We evaluate our method
on several well-known 3D character datasets, and it yields an average
improvement of 25% on point-wise mesh euclidean distance (PMD) against the
start-of-art method. Moreover, our qualitative results show that our method is
significantly helpful in preserving the moving consistency of different body
parts on the target character due to disentangling body-part structures and
mesh details in a hierarchical way.",None,-1
d5fe45c2-0a0e-4782-8ffb-00e13bd1f43e,FastMESH: Fast Surface Reconstruction by Hexagonal Mesh-based Neural Rendering,0.0534857,1,"Despite the promising results of multi-view reconstruction, the recent neural
rendering-based methods, such as implicit surface rendering (IDR) and volume
rendering (NeuS), not only incur a heavy computational burden on training but
also have the difficulties in disentangling the geometric and appearance.
Although having achieved faster training speed than implicit representation and
hash coding, the explicit voxel-based method obtains the inferior results on
recovering surface. To address these challenges, we propose an effective
mesh-based neural rendering approach, named FastMESH, which only samples at the
intersection of ray and mesh. A coarse-to-fine scheme is introduced to
efficiently extract the initial mesh by space carving. More importantly, we
suggest a hexagonal mesh model to preserve surface regularity by constraining
the second-order derivatives of vertices, where only low level of positional
encoding is engaged for neural rendering. The experiments demonstrate that our
approach achieves the state-of-the-art results on both reconstruction and novel
view synthesis. Besides, we obtain 10-fold acceleration on training comparing
to the implicit representation-based methods.",None,-1
24b8d3bb-a8ff-483b-a458-d864020b8438,Identification of Novel Classes for Improving Few-Shot Object Detection,0.504927,8,"Conventional training of deep neural networks requires a large number of the
annotated image which is a laborious and time-consuming task, particularly for
rare objects. Few-shot object detection (FSOD) methods offer a remedy by
realizing robust object detection using only a few training samples per class.
An unexplored challenge for FSOD is that instances from unlabeled novel classes
that do not belong to the fixed set of training classes appear in the
background. These objects behave similarly to label noise, leading to FSOD
performance degradation. We develop a semi-supervised algorithm to detect and
then utilize these unlabeled novel objects as positive samples during training
to improve FSOD performance. Specifically, we propose a hierarchical ternary
classification region proposal network (HTRPN) to localize the potential
unlabeled novel objects and assign them new objectness labels. Our improved
hierarchical sampling strategy for the region proposal network (RPN) also
boosts the perception ability of the object detection model for large objects.
Our experimental results indicate that our method is effective and outperforms
the existing state-of-the-art (SOTA) FSOD methods.",None,-1
61acb04e-5a53-489b-8d4e-82f0bc81fc4b,A Turing Test: Are AI Chatbots Behaviorally Similar to Humans?,0.229152,4,"We administer a Turing Test to AI Chatbots. We examine how Chatbots behave in
a suite of classic behavioral games that are designed to elicit characteristics
such as trust, fairness, risk-aversion, cooperation, \textit{etc.}, as well as
how they respond to a traditional Big-5 psychological survey that measures
personality traits. ChatGPT-4 exhibits behavioral and personality traits that
are statistically indistinguishable from a random human from tens of thousands
of human subjects from more than 50 countries. Chatbots also modify their
behavior based on previous experience and contexts ``as if'' they were learning
from the interactions, and change their behavior in response to different
framings of the same strategic situation. Their behaviors are often distinct
from average and modal human behaviors, in which case they tend to behave on
the more altruistic and cooperative end of the distribution. We estimate that
they act as if they are maximizing an average of their own and partner's
payoffs.",None,-1
962d9e54-dcde-4fd9-acd9-820d131e116a,Large language models can segment narrative events similarly to humans,0.255364,9,"Humans perceive discrete events such as ""restaurant visits"" and ""train rides""
in their continuous experience. One important prerequisite for studying human
event perception is the ability of researchers to quantify when one event ends
and another begins. Typically, this information is derived by aggregating
behavioral annotations from several observers. Here we present an alternative
computational approach where event boundaries are derived using a large
language model, GPT-3, instead of using human annotations. We demonstrate that
GPT-3 can segment continuous narrative text into events. GPT-3-annotated events
are significantly correlated with human event annotations. Furthermore, these
GPT-derived annotations achieve a good approximation of the ""consensus""
solution (obtained by averaging across human annotations); the boundaries
identified by GPT-3 are closer to the consensus, on average, than boundaries
identified by individual human annotators. This finding suggests that GPT-3
provides a feasible solution for automated event annotations, and it
demonstrates a further parallel between human cognition and prediction in large
language models. In the future, GPT-3 may thereby help to elucidate the
principles underlying human event perception.",None,-1
1a14a85c-db29-4191-8beb-ad455868909c,Learning to Read Analog Gauges from Synthetic Data,0.736403,1,"Manually reading and logging gauge data is time inefficient, and the effort
increases according to the number of gauges available. We present a computer
vision pipeline that automates the reading of analog gauges. We propose a
two-stage CNN pipeline that identifies the key structural components of an
analog gauge and outputs an angular reading. To facilitate the training of our
approach, a synthetic dataset is generated thus obtaining a set of realistic
analog gauges with their corresponding annotation. To validate our proposal, an
additional real-world dataset was collected with 4.813 manually curated images.
When compared against state-of-the-art methodologies, our method shows a
significant improvement of 4.55 in the average error, which is a 52% relative
improvement. The resources for this project will be made available at:
https://github.com/fuankarion/automatic-gauge-reading.",None,-1
47fa2231-4641-4816-be97-9a191e60088c,ZooPFL: Exploring Black-box Foundation Models for Personalized Federated Learning,0.638877,5,"When personalized federated learning (FL) meets large foundation models, new
challenges arise from various limitations in resources. In addition to typical
limitations such as data, computation, and communication costs, access to the
models is also often limited. This paper endeavors to solve both the challenges
of limited resources and personalization. i.e., distribution shifts between
clients. To do so, we propose a method named ZOOPFL that uses Zeroth-Order
Optimization for Personalized Federated Learning. ZOOPFL avoids direct
interference with the foundation models and instead learns to adapt its inputs
through zeroth-order optimization. In addition, we employ simple yet effective
linear projections to remap its predictions for personalization. To reduce the
computation costs and enhance personalization, we propose input surgery to
incorporate an auto-encoder with low-dimensional and client-specific
embeddings. We provide theoretical support for ZOOPFL to analyze its
convergence. Extensive empirical experiments on computer vision and natural
language processing tasks using popular foundation models demonstrate its
effectiveness for FL on black-box foundation models.",None,-1
1d3a267e-45f9-4692-ab7f-6e1106002638,Robust Wind Turbine Blade Segmentation from RGB Images in the Wild,0.780248,3,"With the relentless growth of the wind industry, there is an imperious need
to design automatic data-driven solutions for wind turbine maintenance. As
structural health monitoring mainly relies on visual inspections, the first
stage in any automatic solution is to identify the blade region on the image.
Thus, we propose a novel segmentation algorithm that strengthens the U-Net
results by a tailored loss, which pools the focal loss with a contiguity
regularization term. To attain top performing results, a set of additional
steps are proposed to ensure a reliable, generic, robust and efficient
algorithm. First, we leverage our prior knowledge on the images by filling the
holes enclosed by temporarily-classified blade pixels and by the image
boundaries. Subsequently, the mislead classified pixels are successfully
amended by training an on-the-fly random forest. Our algorithm demonstrates its
effectiveness reaching a non-trivial 97.39% of accuracy.",None,-1
7de9b1b1-d144-4ceb-bd2f-17501f3d7296,PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records,0.278155,3,"This paper describes PULSAR, our system submission at the ImageClef 2023
MediQA-Sum task on summarising patient-doctor dialogues into clinical records.
The proposed framework relies on domain-specific pre-training, to produce a
specialised language model which is trained on task-specific natural data
augmented by synthetic data generated by a black-box LLM. We find limited
evidence towards the efficacy of domain-specific pre-training and data
augmentation, while scaling up the language model yields the best performance
gains. Our approach was ranked second and third among 13 submissions on task B
of the challenge. Our code is available at https://github.com/yuping-wu/PULSAR.",None,-1
87cc90cc-17c2-47c3-9b0a-fa6953dcfaf4,Diffusion Action Segmentation,0.650794,33,"Temporal action segmentation is crucial for understanding long-form videos.
Previous works on this task commonly adopt an iterative refinement paradigm by
using multi-stage models. We propose a novel framework via denoising diffusion
models, which nonetheless shares the same inherent spirit of such iterative
refinement. In this framework, action predictions are iteratively generated
from random noise with input video features as conditions. To enhance the
modeling of three striking characteristics of human actions, including the
position prior, the boundary ambiguity, and the relational dependency, we
devise a unified masking strategy for the conditioning inputs in our framework.
Extensive experiments on three benchmark datasets, i.e., GTEA, 50Salads, and
Breakfast, are performed and the proposed method achieves superior or
comparable results to state-of-the-art methods, showing the effectiveness of a
generative approach for action segmentation.",None,-1
334b46f2-0a91-4fd9-bc0c-fb2ea65459bc,Exploring Visual Prompts for Whole Slide Image Classification with Multiple Instance Learning,0.181532,1,"Multiple instance learning (MIL) has emerged as a popular method for
classifying histopathology whole slide images (WSIs). However, existing
approaches typically rely on pre-trained models from large natural image
datasets, such as ImageNet, to generate instance features, which can be
sub-optimal due to the significant differences between natural images and
histopathology images that lead to a domain shift. In this paper, we present a
novel, simple yet effective method for learning domain-specific knowledge
transformation from pre-trained models to histopathology images. Our approach
entails using a prompt component to assist the pre-trained model in discerning
differences between the pre-trained dataset and the target histopathology
dataset, resulting in improved performance of MIL models. We validate our
method on two publicly available datasets, Camelyon16 and TCGA-NSCLC. Extensive
experimental results demonstrate the significant performance improvement of our
method for different MIL models and backbones. Upon publication of this paper,
we will release the source code for our method.",None,-1
34d62ec3-ad1c-4373-a603-39ca1c1ea15f,Learning Human Mesh Recovery in 3D Scenes,0.792739,10,"We present a novel method for recovering the absolute pose and shape of a
human in a pre-scanned scene given a single image. Unlike previous methods that
perform sceneaware mesh optimization, we propose to first estimate absolute
position and dense scene contacts with a sparse 3D CNN, and later enhance a
pretrained human mesh recovery network by cross-attention with the derived 3D
scene cues. Joint learning on images and scene geometry enables our method to
reduce the ambiguity caused by depth and occlusion, resulting in more
reasonable global postures and contacts. Encoding scene-aware cues in the
network also allows the proposed method to be optimization-free, and opens up
the opportunity for real-time applications. The experiments show that the
proposed network is capable of recovering accurate and physically-plausible
meshes by a single forward pass and outperforms state-of-the-art methods in
terms of both accuracy and speed.",None,-1
d6e18378-9eef-4031-bec4-b1ecefbffeef,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,0.258307,2,"Inspired by recent findings that generative diffusion models learn
semantically meaningful representations, we use them to discover the intrinsic
hierarchical structure in biomedical 3D images using unsupervised segmentation.
We show that features of diffusion models from different stages of a
U-Net-based ladder-like architecture capture different hierarchy levels in 3D
biomedical images. We design three losses to train a predictive unsupervised
segmentation network that encourages the decomposition of 3D volumes into
meaningful nested subvolumes that represent a hierarchy. First, we pretrain 3D
diffusion models and use the consistency of their features across subvolumes.
Second, we use the visual consistency between subvolumes. Third, we use the
invariance to photometric augmentations as a regularizer. Our models achieve
better performance than prior unsupervised structure discovery approaches on
challenging biologically-inspired synthetic datasets and on a real-world brain
tumor MRI dataset.",None,-1
5b5b5f6e-2d7e-4c9c-8a08-19e646d94bac,Poisoning Language Models During Instruction Tuning,0.791408,100,"Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on
datasets that contain user-submitted examples, e.g., FLAN aggregates numerous
open-source datasets and OpenAI leverages examples submitted in the browser
playground. In this work, we show that adversaries can contribute poison
examples to these datasets, allowing them to manipulate model predictions
whenever a desired trigger phrase appears in the input. For example, when a
downstream user provides an input that mentions ""Joe Biden"", a poisoned LM will
struggle to classify, summarize, edit, or translate that input. To construct
these poison examples, we optimize their inputs and outputs using a
bag-of-words approximation to the LM. We evaluate our method on open-source
instruction-tuned LMs. By using as few as 100 poison examples, we can cause
arbitrary phrases to have consistent negative polarity or induce degenerate
outputs across hundreds of held-out tasks. Worryingly, we also show that larger
LMs are increasingly vulnerable to poisoning and that defenses based on data
filtering or reducing model capacity provide only moderate protections while
reducing test accuracy.",None,-1
180521c7-9b2e-47f1-bb98-133c5c881458,Topological Data Analysis Guided Segment Anything Model Prompt Optimization for Zero-Shot Segmentation in Biological Imaging,0.0371441,2,"Emerging foundation models in machine learning are models trained on vast
amounts of data that have been shown to generalize well to new tasks. Often
these models can be prompted with multi-modal inputs that range from natural
language descriptions over images to point clouds. In this paper, we propose
topological data analysis (TDA) guided prompt optimization for the Segment
Anything Model (SAM) and show preliminary results in the biological image
segmentation domain. Our approach replaces the standard grid search approach
that is used in the original implementation and finds point locations based on
their topological significance. Our results show that the TDA optimized point
cloud is much better suited for finding small objects and massively reduces
computational complexity despite the extra step in scenarios which require many
segmentations.",None,-1
876d7bbf-fa6a-44b0-868e-9e422a8887fb,HGDNet: A Height-Hierarchy Guided Dual-Decoder Network for Single View Building Extraction and Height Estimation,0.684299,3,"Unifying the correlative single-view satellite image building extraction and
height estimation tasks indicates a promising way to share representations and
acquire generalist model for large-scale urban 3D reconstruction. However, the
common spatial misalignment between building footprints and
stereo-reconstructed nDSM height labels incurs degraded performance on both
tasks. To address this issue, we propose a Height-hierarchy Guided Dual-decoder
Network (HGDNet) to estimate building height. Under the guidance of synthesized
discrete height-hierarchy nDSM, auxiliary height-hierarchical building
extraction branch enhance the height estimation branch with implicit
constraints, yielding an accuracy improvement of more than 6% on the DFC 2023
track2 dataset. Additional two-stage cascade architecture is adopted to achieve
more accurate building extraction. Experiments on the DFC 2023 Track 2 dataset
shows the superiority of the proposed method in building height estimation
({\delta}1:0.8012), instance extraction (AP50:0.7730), and the final average
score 0.7871 ranks in the first place in test phase.",None,-1
42580c2e-3f45-4924-a454-5eb54efef903,LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM,0.10505,2,"Text embeddings are useful features for several NLP applications, such as
sentence similarity, text clustering, and semantic search. In this paper, we
present a Low-rank Adaptation with a Contrastive objective on top of 8-bit
Siamese-BLOOM, a multilingual large language model optimized to produce
semantically meaningful word embeddings. The innovation is threefold. First, we
cast BLOOM weights to 8-bit values. Second, we fine-tune BLOOM with a scalable
adapter (LoRA) and 8-bit Adam optimizer for sentence similarity classification.
Third, we apply a Siamese architecture on BLOOM model with a contrastive
objective to ease the multi-lingual labeled data scarcity. The experiment
results show the quality of learned embeddings from LACoS-BLOOM is proportional
to the number of model parameters and the amount of unlabeled training data.
With the parameter efficient fine-tuning design, we are able to run BLOOM 7.1
billion parameters end-to-end on a single GPU machine with 32GB memory.
Compared to previous solution Sentence-BERT, we achieve significant improvement
on both English and multi-lingual STS tasks.",None,-1
7fadfd04-3a91-4f70-b014-ea75d8007d8e,RIFT2: Speeding-up RIFT with A New Rotation-Invariance Technique,0.0621761,2,"Multimodal image matching is an important prerequisite for multisource image
information fusion. Compared with the traditional matching problem, multimodal
feature matching is more challenging due to the severe nonlinear radiation
distortion (NRD). Radiation-variation insensitive feature transform
(RIFT)~\cite{li2019rift} has shown very good robustness to NRD and become a
baseline method in multimodal feature matching. However, the high computational
cost for rotation invariance largely limits its usage in practice. In this
paper, we propose an improved RIFT method, called RIFT2. We develop a new
rotation invariance technique based on dominant index value, which avoids the
construction process of convolution sequence ring. Hence, it can speed up the
running time and reduce the memory consumption of the original RIFT by almost 3
times in theory. Extensive experiments show that RIFT2 achieves similar
matching performance to RIFT while being much faster and having less memory
consumption. The source code will be made publicly available in
\url{https://github.com/LJY-RS/RIFT2-multimodal-matching-rotation}",None,-1
78029f87-0e3c-4260-a95e-86a5c9d6d29b,Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation,0.526661,10,"Interpretability and efficiency are two important considerations for the
adoption of neural automatic metrics. In this work, we develop
strong-performing automatic metrics for reference-based summarization
evaluation, based on a two-stage evaluation pipeline that first extracts basic
information units from one text sequence and then checks the extracted units in
another sequence. The metrics we developed include two-stage metrics that can
provide high interpretability at both the fine-grained unit level and summary
level, and one-stage metrics that achieve a balance between efficiency and
interpretability. We make the developed tools publicly available at
https://github.com/Yale-LILY/AutoACU.",None,-1
84811e69-c077-4351-bb28-d6687eebefea,ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks,1.0,498,"Many NLP applications require manual data annotations for a variety of tasks,
notably to train classifiers or evaluate the performance of unsupervised
models. Depending on the size and degree of complexity, the tasks may be
conducted by crowd-workers on platforms such as MTurk as well as trained
annotators, such as research assistants. Using a sample of 2,382 tweets, we
demonstrate that ChatGPT outperforms crowd-workers for several annotation
tasks, including relevance, stance, topics, and frames detection. Specifically,
the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of
five tasks, while ChatGPT's intercoder agreement exceeds that of both
crowd-workers and trained annotators for all tasks. Moreover, the
per-annotation cost of ChatGPT is less than $0.003 -- about twenty times
cheaper than MTurk. These results show the potential of large language models
to drastically increase the efficiency of text classification.",None,-1
9c2658a4-55fd-405a-b126-c56708caa914,STRIDE: Street View-based Environmental Feature Detection and Pedestrian Collision Prediction,0.421792,1,"This paper introduces a novel benchmark to study the impact and relationship
of built environment elements on pedestrian collision prediction, intending to
enhance environmental awareness in autonomous driving systems to prevent
pedestrian injuries actively. We introduce a built environment detection task
in large-scale panoramic images and a detection-based pedestrian collision
frequency prediction task. We propose a baseline method that incorporates a
collision prediction module into a state-of-the-art detection model to tackle
both tasks simultaneously. Our experiments demonstrate a significant
correlation between object detection of built environment elements and
pedestrian collision frequency prediction. Our results are a stepping stone
towards understanding the interdependencies between built environment
conditions and pedestrian safety.",None,-1
e51a0d33-643f-4200-ae25-7fdd895bd1c7,A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization,0.337581,2,"Pre-trained language models (PLMs) have achieved outstanding achievements in
abstractive single-document summarization (SDS). However, such benefits may not
fully extend to multi-document summarization (MDS), where the handling of
cross-document information is more complex. Previous works either design new
MDS architectures or apply PLMs bluntly with concatenated source documents as a
reformulated SDS task. While the former does not utilize previous pre-training
efforts and may not generalize well across different domains, the latter may
not sufficiently attend to the intricate cross-document relationships unique to
MDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to
better utilize a PLM to facilitate multi-document interactions for the MDS
task. Across 10 MDS benchmarks from various domains, our method outperforms or
is competitive with the previous best models, including those with additional
MDS pre-training or with more parameters. It outperforms its corresponding PLM
backbone by up to 3 Rouge-L and is favored by humans.",None,-1
505e40d3-ba2f-41d2-af06-6fa10f749648,DreamDiffusion: Generating High-Quality Images from Brain EEG Signals,0.997986,22,"This paper introduces DreamDiffusion, a novel method for generating
high-quality images directly from brain electroencephalogram (EEG) signals,
without the need to translate thoughts into text. DreamDiffusion leverages
pre-trained text-to-image models and employs temporal masked signal modeling to
pre-train the EEG encoder for effective and robust EEG representations.
Additionally, the method further leverages the CLIP image encoder to provide
extra supervision to better align EEG, text, and image embeddings with limited
EEG-image pairs. Overall, the proposed method overcomes the challenges of using
EEG signals for image generation, such as noise, limited information, and
individual differences, and achieves promising results. Quantitative and
qualitative results demonstrate the effectiveness of the proposed method as a
significant step towards portable and low-cost ``thoughts-to-image'', with
potential applications in neuroscience and computer vision. The code is
available here \url{https://github.com/bbaaii/DreamDiffusion}.",None,-1
077f7b87-8297-4809-af28-db582a11f763,STTracker: Spatio-Temporal Tracker for 3D Single Object Tracking,0.480941,6,"3D single object tracking with point clouds is a critical task in 3D computer
vision. Previous methods usually input the last two frames and use the
predicted box to get the template point cloud in previous frame and the search
area point cloud in the current frame respectively, then use similarity-based
or motion-based methods to predict the current box. Although these methods
achieved good tracking performance, they ignore the historical information of
the target, which is important for tracking. In this paper, compared to
inputting two frames of point clouds, we input multi-frame of point clouds to
encode the spatio-temporal information of the target and learn the motion
information of the target implicitly, which could build the correlations among
different frames to track the target in the current frame efficiently.
Meanwhile, rather than directly using the point feature for feature fusion, we
first crop the point cloud features into many patches and then use sparse
attention mechanism to encode the patch-level similarity and finally fuse the
multi-frame features. Extensive experiments show that our method achieves
competitive results on challenging large-scale benchmarks (62.6% in KITTI and
49.66% in NuScenes).",None,-1
dd4a9d54-f1a6-463f-ac24-3b3b6c5b881c,iART: Learning from Demonstration for Assisted Robotic Therapy Using LSTM,0.134138,7,"In this paper, we present an intelligent Assistant for Robotic Therapy
(iART), that provides robotic assistance during 3D trajectory tracking tasks.
We propose a novel LSTM-based robot learning from demonstration (LfD) paradigm
to mimic a therapist's assistance behavior. iART presents a trajectory agnostic
LfD routine that can generalize learned behavior from a single trajectory to
any 3D shape. Once the therapist's behavior has been learned, iART enables the
patient to modify this behavior as per their preference. The system requires
only a single demonstration of 2 minutes and exhibits a mean accuracy of 91.41%
in predicting, and hence mimicking a therapist's assistance behavior. The
system delivers stable assistance in realtime and successfully reproduces
different types of assistance behaviors.",None,-1
0767f03f-6807-429c-b36d-3534fc3687de,Extrinsic Factors Affecting the Accuracy of Biomedical NER,0.375919,2,"Biomedical named entity recognition (NER) is a critial task that aims to
identify structured information in clinical text, which is often replete with
complex, technical terms and a high degree of variability. Accurate and
reliable NER can facilitate the extraction and analysis of important biomedical
information, which can be used to improve downstream applications including the
healthcare system. However, NER in the biomedical domain is challenging due to
limited data availability, as the high expertise, time, and expenses are
required to annotate its data. In this paper, by using the limited data, we
explore various extrinsic factors including the corpus annotation scheme, data
augmentation techniques, semi-supervised learning and Brill transformation, to
improve the performance of a NER model on a clinical text dataset (i2b2 2012,
\citet{sun-rumshisky-uzuner:2013}). Our experiments demonstrate that these
approaches can significantly improve the model's F1 score from original 73.74
to 77.55. Our findings suggest that considering different extrinsic factors and
combining these techniques is a promising approach for improving NER
performance in the biomedical domain where the size of data is limited.",None,-1
fd394f65-8f82-421c-9dd8-a12bfc742bb7,Notion of Explainable Artificial Intelligence -- An Empirical Investigation from A Users Perspective,0.149444,1,"The growing attention to artificial intelligence-based applications has led
to research interest in explainability issues. This emerging research attention
on explainable AI (XAI) advocates the need to investigate end user-centric
explainable AI. Thus, this study aims to investigate usercentric explainable AI
and considered recommendation systems as the study context. We conducted focus
group interviews to collect qualitative data on the recommendation system. We
asked participants about the end users' comprehension of a recommended item,
its probable explanation, and their opinion of making a recommendation
explainable. Our findings reveal that end users want a non-technical and
tailor-made explanation with on-demand supplementary information. Moreover, we
also observed users requiring an explanation about personal data usage,
detailed user feedback, and authentic and reliable explanations. Finally, we
propose a synthesized framework that aims at involving the end user in the
development process for requirements collection and validation.",None,-1
faf05366-adfe-4186-a5ab-834573966836,Just Noticeable Visual Redundancy Forecasting: A Deep Multimodal-driven Approach,0.614179,2,"Just noticeable difference (JND) refers to the maximum visual change that
human eyes cannot perceive, and it has a wide range of applications in
multimedia systems. However, most existing JND approaches only focus on a
single modality, and rarely consider the complementary effects of multimodal
information. In this article, we investigate the JND modeling from an
end-to-end homologous multimodal perspective, namely hmJND-Net. Specifically,
we explore three important visually sensitive modalities, including saliency,
depth, and segmentation. To better utilize homologous multimodal information,
we establish an effective fusion method via summation enhancement and
subtractive offset, and align homologous multimodal features based on a
self-attention driven encoder-decoder paradigm. Extensive experimental results
on eight different benchmark datasets validate the superiority of our hmJND-Net
over eight representative methods.",None,-1
336ebc7a-00ba-4e91-986f-a441491bd59a,Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision,0.47314,12,"Credibility signals represent a wide range of heuristics that are typically
used by journalists and fact-checkers to assess the veracity of online content.
Automating the task of credibility signal extraction, however, is very
challenging as it requires high-accuracy signal-specific extractors to be
trained, while there are currently no sufficiently large datasets annotated
with all credibility signals. This paper investigates whether large language
models (LLMs) can be prompted effectively with a set of 18 credibility signals
to produce weak labels for each signal. We then aggregate these potentially
noisy labels using weak supervision in order to predict content veracity. We
demonstrate that our approach, which combines zero-shot LLM credibility signal
labeling and weak supervision, outperforms state-of-the-art classifiers on two
misinformation datasets without using any ground-truth labels for training. We
also analyse the contribution of the individual credibility signals towards
predicting content veracity, which provides new valuable insights into their
role in misinformation detection.",None,-1
6e54b270-5007-4b3f-878a-4d463823c143,Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model,0.389701,12,"While large language models have proven effective in a huge range of
downstream applications, they often generate text that is problematic or lacks
a desired attribute. In this paper, we introduce Reward-Augmented Decoding
(RAD), a text generation procedure that uses a small unidirectional reward
model to encourage a language model to generate text that has certain
properties. Specifically, RAD uses the reward model to score generations as
they are produced and rescales sampling probabilities to favor high-reward
tokens. By using a unidirectional reward model, RAD can cache activations from
prior generation steps to decrease computational overhead. Through experiments
on generating non-toxic and sentiment-controlled text, we demonstrate that RAD
performs best among methods that change only the generation procedure and
matches the performance of state-of-the-art methods that involve re-training
the language model. We further validate that RAD is effective on very large
language models while incurring a minimal computational overhead.",None,-1
4971d49d-bdd0-4f11-a581-f09db2c7d3de,Instruction Position Matters in Sequence Generation with Large Language Models,0.438551,6,"Large language models (LLMs) are capable of performing conditional sequence
generation tasks, such as translation or summarization, through instruction
fine-tuning. The fine-tuning data is generally sequentially concatenated from a
specific task instruction, an input sentence, and the corresponding response.
Considering the locality modeled by the self-attention mechanism of LLMs, these
models face the risk of instruction forgetting when generating responses for
long input sentences. To mitigate this issue, we propose enhancing the
instruction-following capability of LLMs by shifting the position of task
instructions after the input sentences. Theoretical analysis suggests that our
straightforward method can alter the model's learning focus, thereby
emphasizing the training of instruction-following capabilities. Concurrently,
experimental results demonstrate that our approach consistently outperforms
traditional settings across various model scales (1B / 7B / 13B) and different
sequence generation tasks (translation and summarization), without any
additional data or annotation costs. Notably, our method significantly improves
the zero-shot performance on conditional sequence generation, e.g., up to 9.7
BLEU points on WMT zero-shot translation tasks.",None,-1
d07f1fb6-24a1-4cbe-8d19-f6c72d991330,TADA: Task-Agnostic Dialect Adapters for English,0.563746,10,"Large Language Models, the dominant starting point for Natural Language
Processing (NLP) applications, fail at a higher rate for speakers of English
dialects other than Standard American English (SAE). Prior work addresses this
using task-specific data or synthetic data augmentation, both of which require
intervention for each dialect and task pair. This poses a scalability issue
that prevents the broad adoption of robust dialectal English NLP. We introduce
a simple yet effective method for task-agnostic dialect adaptation by aligning
non-SAE dialects using adapters and composing them with task-specific adapters
from SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on
4 dialectal variants of the GLUE benchmark without task-specific supervision.",None,-1
b139d6d8-cfa8-47f2-8484-964bf7197af4,Framework for Quality Evaluation of Smart Roadside Infrastructure Sensors for Automated Driving Applications,0.684205,4,"The use of smart roadside infrastructure sensors is highly relevant for
future applications of connected and automated vehicles. External sensor
technology in the form of intelligent transportation system stations (ITS-Ss)
can provide safety-critical real-time information about road users in the form
of a digital twin. The choice of sensor setups has a major influence on the
downstream function as well as the data quality. To date, there is insufficient
research on which sensor setups result in which levels of ITS-S data quality.
We present a novel approach to perform detailed quality assessment for smart
roadside infrastructure sensors. Our framework is multimodal across different
sensor types and is evaluated on the DAIR-V2X dataset. We analyze the
composition of different lidar and camera sensors and assess them in terms of
accuracy, latency, and reliability. The evaluations show that the framework can
be used reliably for several future ITS-S applications.",None,-1
53e52477-13f8-4b9d-90c4-40946b109d04,Solving Travelling Thief Problems using Coordination Based Methods,0.517159,2,"A travelling thief problem (TTP) is a proxy to real-life problems such as
postal collection. TTP comprises an entanglement of a travelling salesman
problem (TSP) and a knapsack problem (KP) since items of KP are scattered over
cities of TSP, and a thief has to visit cities to collect items. In TTP, city
selection and item selection decisions need close coordination since the
thief's travelling speed depends on the knapsack's weight and the order of
visiting cities affects the order of item collection. Existing TTP solvers deal
with city selection and item selection separately, keeping decisions for one
type unchanged while dealing with the other type. This separation essentially
means very poor coordination between two types of decision. In this paper, we
first show that a simple local search based coordination approach does not work
in TTP. Then, to address the aforementioned problems, we propose a human
designed coordination heuristic that makes changes to collection plans during
exploration of cyclic tours. We further propose another human designed
coordination heuristic that explicitly exploits the cyclic tours in item
selections during collection plan exploration. Lastly, we propose a machine
learning based coordination heuristic that captures characteristics of the two
human designed coordination heuristics. Our proposed coordination based
approaches help our TTP solver significantly outperform existing
state-of-the-art TTP solvers on a set of benchmark problems. Our solver is
named Cooperation Coordination (CoCo) and its source code is available from
https://github.com/majid75/CoCo",None,-1
8cd961d2-753d-4d56-8918-5b27270ffbf5,Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition,0.787181,5,"Existing pedestrian attribute recognition (PAR) algorithms are mainly
developed based on a static image. However, the performance is not reliable for
images with challenging factors, such as heavy occlusion, motion blur, etc. In
this work, we propose to understand human attributes using video frames that
can make full use of temporal information. Specifically, we formulate the
video-based PAR as a vision-language fusion problem and adopt pre-trained big
models CLIP to extract the feature embeddings of given video frames. To better
utilize the semantic information, we take the attribute list as another input
and transform the attribute words/phrase into the corresponding sentence via
split, expand, and prompt. Then, the text encoder of CLIP is utilized for
language embedding. The averaged visual tokens and text tokens are concatenated
and fed into a fusion Transformer for multi-modal interactive learning. The
enhanced tokens will be fed into a classification head for pedestrian attribute
prediction. Extensive experiments on a large-scale video-based PAR dataset
fully validated the effectiveness of our proposed framework.",None,-1
58c70e8a-774d-445e-a0fa-ec6149a0fc2b,A real-time algorithm for human action recognition in RGB and thermal video,0.198632,1,"Monitoring the movement and actions of humans in video in real-time is an
important task. We present a deep learning based algorithm for human action
recognition for both RGB and thermal cameras. It is able to detect and track
humans and recognize four basic actions (standing, walking, running, lying) in
real-time on a notebook with a NVIDIA GPU. For this, it combines state of the
art components for object detection (Scaled YoloV4), optical flow (RAFT) and
pose estimation (EvoSkeleton). Qualitative experiments on a set of tunnel
videos show that the proposed algorithm works robustly for both RGB and thermal
video.",None,-1
4a6eb4cc-7dcd-41d3-b733-80583ba118a9,Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement,0.624176,18,"Few-shot learning (FSL) is popular due to its ability to adapt to novel
classes. Compared with inductive few-shot learning, transductive models
typically perform better as they leverage all samples of the query set. The two
existing classes of methods, prototype-based and graph-based, have the
disadvantages of inaccurate prototype estimation and sub-optimal graph
construction with kernel functions, respectively. In this paper, we propose a
novel prototype-based label propagation to solve these issues. Specifically,
our graph construction is based on the relation between prototypes and samples
rather than between samples. As prototypes are being updated, the graph
changes. We also estimate the label of each prototype instead of considering a
prototype be the class centre. On mini-ImageNet, tiered-ImageNet, CIFAR-FS and
CUB datasets, we show the proposed method outperforms other state-of-the-art
methods in transductive FSL and semi-supervised FSL when some unlabeled data
accompanies the novel few-shot task.",None,-1
b285f330-c40a-4c4f-99b9-332f48174b0e,AI Insights into Theoretical Physics and the Swampland Program: A Journey Through the Cosmos with ChatGPT,0.972376,16,"In this case study, we explore the capabilities and limitations of ChatGPT, a
natural language processing model developed by OpenAI, in the field of string
theoretical swampland conjectures. We find that it is effective at paraphrasing
and explaining concepts in a variety of styles, but not at genuinely connecting
concepts. It will provide false information with full confidence and make up
statements when necessary. However, its ingenious use of language can be
fruitful for identifying analogies and describing visual representations of
abstract concepts.",None,-1
2a47a2fe-387d-4c80-84ec-9f4e54f4c673,BioBLP: A Modular Framework for Learning on Multimodal Biomedical Knowledge Graphs,0.0807362,1,"Knowledge graphs (KGs) are an important tool for representing complex
relationships between entities in the biomedical domain. Several methods have
been proposed for learning embeddings that can be used to predict new links in
such graphs. Some methods ignore valuable attribute data associated with
entities in biomedical KGs, such as protein sequences, or molecular graphs.
Other works incorporate such data, but assume that entities can be represented
with the same data modality. This is not always the case for biomedical KGs,
where entities exhibit heterogeneous modalities that are central to their
representation in the subject domain.
  We propose a modular framework for learning embeddings in KGs with entity
attributes, that allows encoding attribute data of different modalities while
also supporting entities with missing attributes. We additionally propose an
efficient pretraining strategy for reducing the required training runtime. We
train models using a biomedical KG containing approximately 2 million triples,
and evaluate the performance of the resulting entity embeddings on the tasks of
link prediction, and drug-protein interaction prediction, comparing against
methods that do not take attribute data into account. In the standard link
prediction evaluation, the proposed method results in competitive, yet lower
performance than baselines that do not use attribute data. When evaluated in
the task of drug-protein interaction prediction, the method compares favorably
with the baselines. We find settings involving low degree entities, which make
up for a substantial amount of the set of entities in the KG, where our method
outperforms the baselines. Our proposed pretraining strategy yields
significantly higher performance while reducing the required training runtime.
  Our implementation is available at https://github.com/elsevier-AI-Lab/BioBLP .",None,-1
703c49ec-3c41-44c6-9e29-98c9fc135519,DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields,0.696845,22,"Recent works such as BARF and GARF can bundle adjust camera poses with neural
radiance fields (NeRF) which is based on coordinate-MLPs. Despite the
impressive results, these methods cannot be applied to Generalizable NeRFs
(GeNeRFs) which require image feature extractions that are often based on more
complicated 3D CNN or transformer architectures. In this work, we first analyze
the difficulties of jointly optimizing camera poses with GeNeRFs, and then
further propose our DBARF to tackle these issues. Our DBARF which bundle
adjusts camera poses by taking a cost feature map as an implicit cost function
can be jointly trained with GeNeRFs in a self-supervised manner. Unlike BARF
and its follow-up works, which can only be applied to per-scene optimized NeRFs
and need accurate initial camera poses with the exception of forward-facing
scenes, our method can generalize across scenes and does not require any good
initialization. Experiments show the effectiveness and generalization ability
of our DBARF when evaluated on real-world datasets. Our code is available at
\url{https://aibluefisher.github.io/dbarf}.",None,-1
8a4ae0fe-1742-417d-974f-251bb39cab2a,FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users,0.532479,4,"The federated learning (FL) technique was developed to mitigate data privacy
issues in the traditional machine learning paradigm. While FL ensures that a
user's data always remain with the user, the gradients are shared with the
centralized server to build the global model. This results in privacy leakage,
where the server can infer private information from the shared gradients. To
mitigate this flaw, the next-generation FL architectures proposed encryption
and anonymization techniques to protect the model updates from the server.
However, this approach creates other challenges, such as malicious users
sharing false gradients. Since the gradients are encrypted, the server is
unable to identify rogue users. To mitigate both attacks, this paper proposes a
novel FL algorithm based on a fully homomorphic encryption (FHE) scheme. We
develop a distributed multi-key additive homomorphic encryption scheme that
supports model aggregation in FL. We also develop a novel aggregation scheme
within the encrypted domain, utilizing users' non-poisoning rates, to
effectively address data poisoning attacks while ensuring privacy is preserved
by the proposed encryption scheme. Rigorous security, privacy, convergence, and
experimental analyses have been provided to show that FheFL is novel, secure,
and private, and achieves comparable accuracy at reasonable computational cost.",None,-1
3d996d78-bb75-4fa1-b4bd-173c004af255,Learning Customized Visual Models with Retrieval-Augmented Knowledge,0.660302,27,"Image-text contrastive learning models such as CLIP have demonstrated strong
task transfer ability. The high generality and usability of these visual models
is achieved via a web-scale data collection process to ensure broad concept
coverage, followed by expensive pre-training to feed all the knowledge into
model weights. Alternatively, we propose REACT, REtrieval-Augmented
CusTomization, a framework to acquire the relevant web knowledge to build
customized visual models for target domains. We retrieve the most relevant
image-text pairs (~3% of CLIP pre-training data) from the web-scale database as
external knowledge, and propose to customize the model by only training new
modualized blocks while freezing all the original weights. The effectiveness of
REACT is demonstrated via extensive experiments on classification, retrieval,
detection and segmentation tasks, including zero, few, and full-shot settings.
Particularly, on the zero-shot classification task, compared with CLIP, it
achieves up to 5.4% improvement on ImageNet and 3.7% on the ELEVATER benchmark
(20 datasets).",None,-1
9dd976f4-a668-46da-b499-0a39b10e77c6,Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for Robotic Grasping,0.283734,3,"Object segmentation for robotic grasping under dynamic conditions often faces
challenges such as occlusion, low light conditions, motion blur and object size
variance. To address these challenges, we propose a Deep Learning network that
fuses two types of visual signals, event-based data and RGB frame data. The
proposed Bimodal SegNet network has two distinct encoders, one for each signal
input and a spatial pyramidal pooling with atrous convolutions. Encoders
capture rich contextual information by pooling the concatenated features at
different resolutions while the decoder obtains sharp object boundaries. The
evaluation of the proposed method undertakes five unique image degradation
challenges including occlusion, blur, brightness, trajectory and scale variance
on the Event-based Segmentation (ESD) Dataset. The evaluation results show a
6-10\% segmentation accuracy improvement over state-of-the-art methods in terms
of mean intersection over the union and pixel accuracy. The model code is
available at https://github.com/sanket0707/Bimodal-SegNet.git",None,-1
0069b12b-f08b-477a-859c-0bc01302a63d,Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model,0.321628,14,"Latent diffusion models (LDMs) exhibit an impressive ability to produce
realistic images, yet the inner workings of these models remain mysterious.
Even when trained purely on images without explicit depth information, they
typically output coherent pictures of 3D scenes. In this work, we investigate a
basic interpretability question: does an LDM create and use an internal
representation of simple scene geometry? Using linear probes, we find evidence
that the internal activations of the LDM encode linear representations of both
3D depth data and a salient-object / background distinction. These
representations appear surprisingly early in the denoising process$-$well
before a human can easily make sense of the noisy images. Intervention
experiments further indicate these representations play a causal role in image
synthesis, and may be used for simple high-level editing of an LDM's output.
Project page: https://yc015.github.io/scene-representation-diffusion-model/",None,-1
436249c3-5db7-49d2-938a-84108f67e823,Integrating Audio-Visual Features for Multimodal Deepfake Detection,0.904969,9,"Deepfakes are AI-generated media in which an image or video has been
digitally modified. The advancements made in deepfake technology have led to
privacy and security issues. Most deepfake detection techniques rely on the
detection of a single modality. Existing methods for audio-visual detection do
not always surpass that of the analysis based on single modalities. Therefore,
this paper proposes an audio-visual-based method for deepfake detection, which
integrates fine-grained deepfake identification with binary classification. We
categorize the samples into four types by combining labels specific to each
single modality. This method enhances the detection under intra-domain and
cross-domain testing.",None,-1
e9036675-4ca5-416c-997a-47cc971e6bd5,GDB: Gated convolutions-based Document Binarization,0.426099,6,"Document binarization is a key pre-processing step for many document analysis
tasks. However, existing methods can not extract stroke edges finely, mainly
due to the fair-treatment nature of vanilla convolutions and the extraction of
stroke edges without adequate supervision by boundary-related information. In
this paper, we formulate text extraction as the learning of gating values and
propose an end-to-end gated convolutions-based network (GDB) to solve the
problem of imprecise stroke edge extraction. The gated convolutions are applied
to selectively extract the features of strokes with different attention. Our
proposed framework consists of two stages. Firstly, a coarse sub-network with
an extra edge branch is trained to get more precise feature maps by feeding a
priori mask and edge. Secondly, a refinement sub-network is cascaded to refine
the output of the first stage by gated convolutions based on the sharp edge.
For global information, GDB also contains a multi-scale operation to combine
local and global features. We conduct comprehensive experiments on ten Document
Image Binarization Contest (DIBCO) datasets from 2009 to 2019. Experimental
results show that our proposed methods outperform the state-of-the-art methods
in terms of all metrics on average and achieve top ranking on six benchmark
datasets.",None,-1
2937dc75-51d0-4add-a855-78c4b1fa9e75,PersonalTailor: Personalizing 2D Pattern Design from 3D Garment Point Clouds,0.413855,5,"Garment pattern design aims to convert a 3D garment to the corresponding 2D
panels and their sewing structure. Existing methods rely either on template
fitting with heuristics and prior assumptions, or on model learning with
complicated shape parameterization. Importantly, both approaches do not allow
for personalization of the output garment, which today has increasing demands.
To fill this demand, we introduce PersonalTailor: a personalized 2D pattern
design method, where the user can input specific constraints or demands (in
language or sketch) for personal 2D panel fabrication from 3D point clouds.
PersonalTailor first learns a multi-modal panel embeddings based on
unsupervised cross-modal association and attentive fusion. It then predicts a
binary panel masks individually using a transformer encoder-decoder framework.
Extensive experiments show that our PersonalTailor excels on both personalized
and standard pattern fabrication tasks.",None,-1
38d5296f-f941-47e8-bba2-d5048855f370,Improving Food Detection For Images From a Wearable Egocentric Camera,0.187624,8,"Diet is an important aspect of our health. Good dietary habits can contribute
to the prevention of many diseases and improve the overall quality of life. To
better understand the relationship between diet and health, image-based dietary
assessment systems have been developed to collect dietary information. We
introduce the Automatic Ingestion Monitor (AIM), a device that can be attached
to one's eye glasses. It provides an automated hands-free approach to capture
eating scene images. While AIM has several advantages, images captured by the
AIM are sometimes blurry. Blurry images can significantly degrade the
performance of food image analysis such as food detection. In this paper, we
propose an approach to pre-process images collected by the AIM imaging sensor
by rejecting extremely blurry images to improve the performance of food
detection.",None,-1
39658e96-8f25-4191-b54f-93b3a98bee26,San-BERT: Extractive Summarization for Sanskrit Documents using BERT and it's variants,0.22203,1,"In this work, we develop language models for the Sanskrit language, namely
Bidirectional Encoder Representations from Transformers (BERT) and its
variants: A Lite BERT (ALBERT), and Robustly Optimized BERT (RoBERTa) using
Devanagari Sanskrit text corpus. Then we extracted the features for the given
text from these models. We applied the dimensional reduction and clustering
techniques on the features to generate an extractive summary for a given
Sanskrit document. Along with the extractive text summarization techniques, we
have also created and released a Sanskrit Devanagari text corpus publicly.",None,-1
5d971ddc-ad69-490a-ac94-1b9deea5d1c1,RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model,1.0,62,"Leveraging the extensive training data from SA-1B, the Segment Anything Model
(SAM) demonstrates remarkable generalization and zero-shot capabilities.
However, as a category-agnostic instance segmentation method, SAM heavily
relies on prior manual guidance, including points, boxes, and coarse-grained
masks. Furthermore, its performance in remote sensing image segmentation tasks
remains largely unexplored and unproven. In this paper, we aim to develop an
automated instance segmentation approach for remote sensing images, based on
the foundational SAM model and incorporating semantic category information.
Drawing inspiration from prompt learning, we propose a method to learn the
generation of appropriate prompts for SAM. This enables SAM to produce
semantically discernible segmentation results for remote sensing images, a
concept we have termed RSPrompter. We also propose several ongoing derivatives
for instance segmentation tasks, drawing on recent advancements within the SAM
community, and compare their performance with RSPrompter. Extensive
experimental results, derived from the WHU building, NWPU VHR-10, and SSDD
datasets, validate the effectiveness of our proposed method. The code for our
method is publicly available at kychen.me/RSPrompter.",None,-1
22032e28-8d29-4eeb-93cc-409716c439bd,BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation,0.591861,9,"A novel 4K video frame interpolator based on bilateral transformer (BiFormer)
is proposed in this paper, which performs three steps: global motion
estimation, local motion refinement, and frame synthesis. First, in global
motion estimation, we predict symmetric bilateral motion fields at a coarse
scale. To this end, we propose BiFormer, the first transformer-based bilateral
motion estimator. Second, we refine the global motion fields efficiently using
blockwise bilateral cost volumes (BBCVs). Third, we warp the input frames using
the refined motion fields and blend them to synthesize an intermediate frame.
Extensive experiments demonstrate that the proposed BiFormer algorithm achieves
excellent interpolation performance on 4K datasets. The source codes are
available at https://github.com/JunHeum/BiFormer.",None,-1
d04fd091-6d05-463c-b889-476c62028b9e,Neuro-Symbolic World Models for Adapting to Open World Novelty,0.274187,2,"Open-world novelty--a sudden change in the mechanics or properties of an
environment--is a common occurrence in the real world. Novelty adaptation is an
agent's ability to improve its policy performance post-novelty. Most
reinforcement learning (RL) methods assume that the world is a closed, fixed
process. Consequentially, RL policies adapt inefficiently to novelties. To
address this, we introduce WorldCloner, an end-to-end trainable neuro-symbolic
world model for rapid novelty adaptation. WorldCloner learns an efficient
symbolic representation of the pre-novelty environment transitions, and uses
this transition model to detect novelty and efficiently adapt to novelty in a
single-shot fashion. Additionally, WorldCloner augments the policy learning
process using imagination-based adaptation, where the world model simulates
transitions of the post-novelty environment to help the policy adapt. By
blending ''imagined'' transitions with interactions in the post-novelty
environment, performance can be recovered with fewer total environment
interactions. Using environments designed for studying novelty in sequential
decision-making problems, we show that the symbolic world model helps its
neural policy adapt more efficiently than model-based and model-based
neural-only reinforcement learning methods.",None,-1
72376ee1-eb44-4d1f-a258-b4637df5c7a3,Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations,0.933434,16,"Unlike empathetic dialogues, the system in emotional support conversations
(ESC) is expected to not only convey empathy for comforting the help-seeker,
but also proactively assist in exploring and addressing their problems during
the conversation. In this work, we study the problem of mixed-initiative ESC
where the user and system can both take the initiative in leading the
conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC
systems with a tailor-designed schema that divides utterances into different
types with speaker roles and initiative types. Four emotional support metrics
are proposed to evaluate the mixed-initiative interactions. The analysis
reveals the necessity and challenges of building mixed-initiative ESC systems.
In the light of this, we propose a knowledge-enhanced mixed-initiative
framework (KEMI) for ESC, which retrieves actual case knowledge from a
large-scale mental health knowledge graph for generating mixed-initiative
responses. Experimental results on two ESC datasets show the superiority of
KEMI in both content-preserving evaluation and mixed initiative related
analyses.",None,-1
1bbc667e-8625-4b8b-bbbd-bb29501c6aab,EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras,0.559424,1,"Falls are significant and often fatal for vulnerable populations such as the
elderly. Previous works have addressed the detection of falls by relying on
data capture by a single sensor, images or accelerometers. In this work, we
rely on multimodal descriptors extracted from videos captured by egocentric
cameras. Our proposed method includes a late decision fusion layer that builds
on top of the extracted descriptors. Furthermore, we collect a new dataset on
which we assess our proposed approach. We believe this is the first public
dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects.
We conducted ablation experiments to assess the performance of individual
feature extractors, fusion of visual information, and fusion of both visual and
audio information. Moreover, we experimented with internal and external
cross-validation. Our results demonstrate that the fusion of audio and visual
information through late decision fusion improves detection performance, making
it a promising tool for fall prevention and mitigation.",None,-1
9627376e-fd0b-4bef-aa41-f37fe0674859,What Food Do We Tweet about on a Rainy Day?,0.0963629,1,"Food choice is a complex phenomenon shaped by factors such as taste,
ambience, culture or weather. In this paper, we explore food-related tweeting
in different weather conditions. We inspect a Latvian food tweet dataset
spanning the past decade in conjunction with a weather observation dataset
consisting of average temperature, precipitation, and other phenomena. We find
which weather conditions lead to specific food information sharing;
automatically classify tweet sentiment and discuss how it changes depending on
the weather. This research contributes to the growing area of large-scale
social network data understanding of food consumers' choices and perceptions.",None,-1
3466533f-b06a-4218-bcdc-9e5982b57778,Adversarial Alignment for Source Free Object Detection,0.601669,10,"Source-free object detection (SFOD) aims to transfer a detector pre-trained
on a label-rich source domain to an unlabeled target domain without seeing
source data. While most existing SFOD methods generate pseudo labels via a
source-pretrained model to guide training, these pseudo labels usually contain
high noises due to heavy domain discrepancy. In order to obtain better pseudo
supervisions, we divide the target domain into source-similar and
source-dissimilar parts and align them in the feature space by adversarial
learning. Specifically, we design a detection variance-based criterion to
divide the target domain. This criterion is motivated by a finding that larger
detection variances denote higher recall and larger similarity to the source
domain. Then we incorporate an adversarial module into a mean teacher framework
to drive the feature spaces of these two subsets indistinguishable. Extensive
experiments on multiple cross-domain object detection datasets demonstrate that
our proposed method consistently outperforms the compared SFOD methods.",None,-1
f89595fa-9933-44d3-b78e-58679b95ce82,Use neural networks to recognize students' handwritten letters and incorrect symbols,0.500889,1,"Correcting students' multiple-choice answers is a repetitive and mechanical
task that can be considered an image multi-classification task. Assuming
possible options are 'abcd' and the correct option is one of the four, some
students may write incorrect symbols or options that do not exist. In this
paper, five classifications were set up - four for possible correct options and
one for other incorrect writing. This approach takes into account the
possibility of non-standard writing options.",None,-1
52587c8e-0e1b-4dfb-99ee-97fe3b77cfa0,Can LLMs Follow Simple Rules?,0.108581,9,"As Large Language Models (LLMs) are deployed with increasing real-world
responsibilities, it is important to be able to specify and constrain the
behavior of these systems in a reliable manner. Model developers may wish to
set explicit rules for the model, such as ""do not generate abusive content"",
but these may be circumvented by jailbreaking techniques. Existing evaluations
of adversarial attacks and defenses on LLMs generally require either expensive
manual review or unreliable heuristic checks. To address this issue, we propose
Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework
for measuring rule-following ability in LLMs. RuLES consists of 14 simple text
scenarios in which the model is instructed to obey various rules while
interacting with the user. Each scenario has a programmatic evaluation function
to determine whether the model has broken any rules in a conversation. Our
evaluations of proprietary and open models show that almost all current models
struggle to follow scenario rules, even on straightforward test cases. We also
demonstrate that simple optimization attacks suffice to significantly increase
failure rates on test cases. We conclude by exploring two potential avenues for
improvement: test-time steering and supervised fine-tuning.",None,-1
5bf51285-1ca9-4b72-af11-82eaf29a3a74,IDAS: Intent Discovery with Abstractive Summarization,0.354914,6,"Intent discovery is the task of inferring latent intents from a set of
unlabeled utterances, and is a useful step towards the efficient creation of
new conversational agents. We show that recent competitive methods in intent
discovery can be outperformed by clustering utterances based on abstractive
summaries, i.e., ""labels"", that retain the core elements while removing
non-essential information. We contribute the IDAS approach, which collects a
set of descriptive utterance labels by prompting a Large Language Model,
starting from a well-chosen seed set of prototypical utterances, to bootstrap
an In-Context Learning procedure to generate labels for non-prototypical
utterances. The utterances and their resulting noisy labels are then encoded by
a frozen pre-trained encoder, and subsequently clustered to recover the latent
intents. For the unsupervised task (without any intent labels) IDAS outperforms
the state-of-the-art by up to +7.42% in standard cluster metrics for the
Banking, StackOverflow, and Transport datasets. For the semi-supervised task
(with labels for a subset of intents) IDAS surpasses 2 recent methods on the
CLINC benchmark without even using labeled data.",None,-1
ac59531d-3e9b-4ad2-b729-3e3a1c171160,DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents,0.954769,16,"Vision-language pretraining models have achieved great success in supporting
multimedia applications by understanding the alignments between images and
text. While existing vision-language pretraining models primarily focus on
understanding single image associated with a single piece of text, they often
ignore the alignment at the intra-document level, consisting of multiple
sentences with multiple images. In this work, we propose DocumentCLIP, a
salience-aware contrastive learning framework to enforce vision-language
pretraining models to comprehend the interaction between images and longer text
within documents. Our model is beneficial for the real-world multimodal
document understanding like news article, magazines, product descriptions,
which contain linguistically and visually richer content. To the best of our
knowledge, we are the first to explore multimodal intra-document links by
contrastive learning. In addition, we collect a large Wikipedia dataset for
pretraining, which provides various topics and structures. Experiments show
DocumentCLIP not only outperforms the state-of-the-art baselines in the
supervised setting, but also achieves the best zero-shot performance in the
wild after human evaluation. Our code is available at
https://github.com/FuxiaoLiu/DocumentCLIP.",None,-1
2fc1cb1b-720a-4b22-92c3-4eac0863718f,MADLAD-400: A Multilingual And Document-Level Large Audited Dataset,0.996631,40,"We introduce MADLAD-400, a manually audited, general domain 3T token
monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss
the limitations revealed by self-auditing MADLAD-400, and the role data
auditing had in the dataset creation process. We then train and release a
10.7B-parameter multilingual machine translation model on 250 billion tokens
covering over 450 languages using publicly available data, and find that it is
competitive with models that are significantly larger, and report the results
on different domains. In addition, we train a 8B-parameter language model, and
assess the results on few-shot translation. We make the baseline models
available to the research community.",None,-1
7053d634-6a60-428b-9550-8424e189f521,VicTR: Video-conditioned Text Representations for Activity Recognition,0.569299,6,"Vision-Language models (VLMs) have excelled in the image-domain -- especially
in zero-shot settings -- thanks to the availability of vast pretraining data
(i.e., paired image-text samples). However for videos, such paired data is not
as abundant. Therefore, video-VLMs are usually designed by adapting pretrained
image-VLMs to the video-domain, instead of training from scratch. All such
recipes rely on augmenting visual embeddings with temporal information (i.e.,
image $\rightarrow$ video), often keeping text embeddings unchanged or even
being discarded. In this paper, we argue the contrary, that better video-VLMs
can be designed by focusing more on augmenting text, rather than visual
information. More specifically, we introduce Video-conditioned Text
Representations (VicTR): a form of text embeddings optimized w.r.t. visual
embeddings, creating a more-flexible contrastive latent space. Our model can
further make use of freely-available semantic information, in the form of
visually-grounded auxiliary text (e.g. object or scene information). We
evaluate our model on few-shot, zero-shot (HMDB-51, UCF-101), short-form
(Kinetics-400) and long-form (Charades) activity recognition benchmarks,
showing strong performance among video-VLMs.",None,-1
f1ecfc0d-12bd-4b74-8a48-029d3c81c2d6,High-dimensional Clustering onto Hamiltonian Cycle,0.932976,6,"Clustering aims to group unlabelled samples based on their similarities. It
has become a significant tool for the analysis of high-dimensional data.
However, most of the clustering methods merely generate pseudo labels and thus
are unable to simultaneously present the similarities between different
clusters and outliers. This paper proposes a new framework called
High-dimensional Clustering onto Hamiltonian Cycle (HCHC) to solve the above
problems. First, HCHC combines global structure with local structure in one
objective function for deep clustering, improving the labels as relative
probabilities, to mine the similarities between different clusters while
keeping the local structure in each cluster. Then, the anchors of different
clusters are sorted on the optimal Hamiltonian cycle generated by the cluster
similarities and mapped on the circumference of a circle. Finally, a sample
with a higher probability of a cluster will be mapped closer to the
corresponding anchor. In this way, our framework allows us to appreciate three
aspects visually and simultaneously - clusters (formed by samples with high
probabilities), cluster similarities (represented as circular distances), and
outliers (recognized as dots far away from all clusters). The experiments
illustrate the superiority of HCHC.",None,-1
b7a8e543-a234-4f27-892a-03891f4d3bdf,SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,0.172844,4,"Deformable tracking and real-time estimation of 3D tissue motion is essential
to enable automation and image guidance applications in robotically assisted
surgery. Our model, Sparse Efficient Neural Depth and Deformation (SENDD),
extends prior 2D tracking work to estimate flow in 3D space. SENDD introduces
novel contributions of learned detection, and sparse per-point depth and 3D
flow estimation, all with less than half a million parameters. SENDD does this
by using graph neural networks of sparse keypoint matches to estimate both
depth and 3D flow anywhere. We quantify and benchmark SENDD on a
comprehensively labelled tissue dataset, and compare it to an equivalent 2D
flow model. SENDD performs comparably while enabling applications that 2D flow
cannot. SENDD can track points and estimate depth at 10fps on an NVIDIA RTX
4000 for 1280 tracked (query) points and its cost scales linearly with an
increasing/decreasing number of points. SENDD enables multiple downstream
applications that require estimation of 3D motion in stereo endoscopy.",None,-1
2884da29-c1c1-4dd8-9284-5d6c798d7d72,GLS-CSC: A Simple but Effective Strategy to Mitigate Chinese STM Models' Over-Reliance on Superficial Clue,0.681286,2,"Pre-trained models have achieved success in Chinese Short Text Matching (STM)
tasks, but they often rely on superficial clues, leading to a lack of robust
predictions. To address this issue, it is crucial to analyze and mitigate the
influence of superficial clues on STM models. Our study aims to investigate
their over-reliance on the edit distance feature, commonly used to measure the
semantic similarity of Chinese text pairs, which can be considered a
superficial clue. To mitigate STM models' over-reliance on superficial clues,
we propose a novel resampling training strategy called Gradually Learn Samples
Containing Superficial Clue (GLS-CSC). Through comprehensive evaluations of
In-Domain (I.D.), Robustness (Rob.), and Out-Of-Domain (O.O.D.) test sets, we
demonstrate that GLS-CSC outperforms existing methods in terms of enhancing the
robustness and generalization of Chinese STM models. Moreover, we conduct a
detailed analysis of existing methods and reveal their commonality.",None,-1
d22299a9-8e70-4173-bef6-05c8d84f9c4e,Multi-Agent Reinforcement Learning for Assessing False-Data Injection Attacks on Transportation Networks,0.356054,1,"The increasing reliance of drivers on navigation applications has made
transportation networks more susceptible to data-manipulation attacks by
malicious actors. Adversaries may exploit vulnerabilities in the data
collection or processing of navigation services to inject false information,
and to thus interfere with the drivers' route selection. Such attacks can
significantly increase traffic congestions, resulting in substantial waste of
time and resources, and may even disrupt essential services that rely on road
networks. To assess the threat posed by such attacks, we introduce a
computational framework to find worst-case data-injection attacks against
transportation networks. First, we devise an adversarial model with a threat
actor who can manipulate drivers by increasing the travel times that they
perceive on certain roads. Then, we employ hierarchical multi-agent
reinforcement learning to find an approximate optimal adversarial strategy for
data manipulation. We demonstrate the applicability of our approach through
simulating attacks on the Sioux Falls, ND network topology.",None,-1
9a187544-3f96-48a8-bcda-0c32e7f96491,RDMNet: Reliable Dense Matching Based Point Cloud Registration for Autonomous Driving,0.368171,5,"Point cloud registration is an important task in robotics and autonomous
driving to estimate the ego-motion of the vehicle. Recent advances following
the coarse-to-fine manner show promising potential in point cloud registration.
However, existing methods rely on good superpoint correspondences, which are
hard to be obtained reliably and efficiently, thus resulting in less robust and
accurate point cloud registration. In this paper, we propose a novel network,
named RDMNet, to find dense point correspondences coarse-to-fine and improve
final pose estimation based on such reliable correspondences. Our RDMNet uses a
devised 3D-RoFormer mechanism to first extract distinctive superpoints and
generates reliable superpoints matches between two point clouds. The proposed
3D-RoFormer fuses 3D position information into the transformer network,
efficiently exploiting point clouds' contextual and geometric information to
generate robust superpoint correspondences. RDMNet then propagates the sparse
superpoints matches to dense point matches using the neighborhood information
for accurate point cloud registration. We extensively evaluate our method on
multiple datasets from different environments. The experimental results
demonstrate that our method outperforms existing state-of-the-art approaches in
all tested datasets with a strong generalization ability.",None,-1
dc53935e-1372-4d32-98c4-cb1b580c9d64,In-Rack Test Tube Pose Estimation Using RGB-D Data,0.488279,1,"Accurate robotic manipulation of test tubes in biology and medical industries
is becoming increasingly important to address workforce shortages and improve
worker safety. The detection and localization of test tubes are essential for
the robots to successfully manipulate test tubes. In this paper, we present a
framework to detect and estimate poses for the in-rack test tubes using color
and depth data. The methodology involves the utilization of a YOLO object
detector to effectively classify and localize both the test tubes and the tube
racks within the provided image data. Subsequently, the pose of the tube rack
is estimated through point cloud registration techniques. During the process of
estimating the poses of the test tubes, we capitalize on constraints derived
from the arrangement of rack slots. By employing an optimization-based
algorithm, we effectively evaluate and refine the pose of the test tubes. This
strategic approach ensures the robustness of pose estimation, even when
confronted with noisy and incomplete point cloud data.",None,-1
328aeed9-1ae8-4734-870e-47dc5ff38580,Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments,0.667992,20,"Synthesizing interaction-involved human motions has been challenging due to
the high complexity of 3D environments and the diversity of possible human
behaviors within. We present LAMA, Locomotion-Action-MAnipulation, to
synthesize natural and plausible long-term human movements in complex indoor
environments. The key motivation of LAMA is to build a unified framework to
encompass a series of everyday motions including locomotion, scene interaction,
and object manipulation. Unlike existing methods that require motion data
""paired"" with scanned 3D scenes for supervision, we formulate the problem as a
test-time optimization by using human motion capture data only for synthesis.
LAMA leverages a reinforcement learning framework coupled with a motion
matching algorithm for optimization, and further exploits a motion editing
framework via manifold learning to cover possible variations in interaction and
manipulation. Throughout extensive experiments, we demonstrate that LAMA
outperforms previous approaches in synthesizing realistic motions in various
challenging scenarios. Project page: https://jiyewise.github.io/projects/LAMA/ .",None,-1
f6af92c4-b13d-445a-bdf5-f7957e95e026,A Simple and Effective Method of Cross-Lingual Plagiarism Detection,0.461543,2,"We present a simple cross-lingual plagiarism detection method applicable to a
large number of languages. The presented approach leverages open multilingual
thesauri for candidate retrieval task and pre-trained multilingual BERT-based
language models for detailed analysis. The method does not rely on machine
translation and word sense disambiguation when in use, and therefore is
suitable for a large number of languages, including under-resourced languages.
The effectiveness of the proposed approach is demonstrated for several existing
and new benchmarks, achieving state-of-the-art results for French, Russian, and
Armenian languages.",None,-1
f4e83151-f839-47bd-8590-1955b5769d74,Reconstruction Distortion of Learned Image Compression with Imperceptible Perturbations,0.406351,3,"Learned Image Compression (LIC) has recently become the trending technique
for image transmission due to its notable performance. Despite its popularity,
the robustness of LIC with respect to the quality of image reconstruction
remains under-explored. In this paper, we introduce an imperceptible attack
approach designed to effectively degrade the reconstruction quality of LIC,
resulting in the reconstructed image being severely disrupted by noise where
any object in the reconstructed images is virtually impossible. More
specifically, we generate adversarial examples by introducing a Frobenius
norm-based loss function to maximize the discrepancy between original images
and reconstructed adversarial examples. Further, leveraging the insensitivity
of high-frequency components to human vision, we introduce Imperceptibility
Constraint (IC) to ensure that the perturbations remain inconspicuous.
Experiments conducted on the Kodak dataset using various LIC models demonstrate
effectiveness. In addition, we provide several findings and suggestions for
designing future defenses.",None,-1
f431bd47-bb47-471f-8cd9-0d73f6dc99bd,Learning by Applying: A General Framework for Mathematical Reasoning via Enhancing Explicit Knowledge Learning,0.193715,10,"Mathematical reasoning is one of the crucial abilities of general artificial
intelligence, which requires machines to master mathematical logic and
knowledge from solving problems. However, existing approaches are not
transparent (thus not interpretable) in terms of what knowledge has been
learned and applied in the reasoning process. In this paper, we propose a
general Learning by Applying (LeAp) framework to enhance existing models
(backbones) in a principled way by explicit knowledge learning. In LeAp, we
perform knowledge learning in a novel problem-knowledge-expression paradigm,
with a Knowledge Encoder to acquire knowledge from problem data and a Knowledge
Decoder to apply knowledge for expression reasoning. The learned mathematical
knowledge, including word-word relations and word-operator relations, forms an
explicit knowledge graph, which bridges the knowledge ""learning"" and ""applying""
organically. Moreover, for problem solving, we design a semantics-enhanced
module and a reasoning-enhanced module that apply knowledge to improve the
problem comprehension and symbol reasoning abilities of any backbone,
respectively. We theoretically prove the superiority of LeAp's autonomous
learning mechanism. Experiments on three real-world datasets show that LeAp
improves all backbones' performances, learns accurate knowledge, and achieves a
more interpretable reasoning process.",None,-1
91a0f388-8560-4a5a-8ee0-81c35df052c1,Restore Anything Pipeline: Segment Anything Meets Image Restoration,0.607053,7,"Recent image restoration methods have produced significant advancements using
deep learning. However, existing methods tend to treat the whole image as a
single entity, failing to account for the distinct objects in the image that
exhibit individual texture properties. Existing methods also typically generate
a single result, which may not suit the preferences of different users. In this
paper, we introduce the Restore Anything Pipeline (RAP), a novel interactive
and per-object level image restoration approach that incorporates a
controllable model to generate different results that users may choose from.
RAP incorporates image segmentation through the recent Segment Anything Model
(SAM) into a controllable image restoration model to create a user-friendly
pipeline for several image restoration tasks. We demonstrate the versatility of
RAP by applying it to three common image restoration tasks: image deblurring,
image denoising, and JPEG artifact removal. Our experiments show that RAP
produces superior visual results compared to state-of-the-art methods. RAP
represents a promising direction for image restoration, providing users with
greater control, and enabling image restoration at an object level.",None,-1
79a1a327-edc4-494a-8a0f-edeba964bb7e,Boosting Video Object Segmentation via Space-time Correspondence Learning,0.66963,20,"Current top-leading solutions for video object segmentation (VOS) typically
follow a matching-based regime: for each query frame, the segmentation mask is
inferred according to its correspondence to previously processed and the first
annotated frames. They simply exploit the supervisory signals from the
groundtruth masks for learning mask prediction only, without posing any
constraint on the space-time correspondence matching, which, however, is the
fundamental building block of such regime. To alleviate this crucial yet
commonly ignored issue, we devise a correspondence-aware training framework,
which boosts matching-based VOS solutions by explicitly encouraging robust
correspondence matching during network learning. Through comprehensively
exploring the intrinsic coherence in videos on pixel and object levels, our
algorithm reinforces the standard, fully supervised training of mask
segmentation with label-free, contrastive correspondence learning. Without
neither requiring extra annotation cost during training, nor causing speed
delay during deployment, nor incurring architectural modification, our
algorithm provides solid performance gains on four widely used benchmarks,
i.e., DAVIS2016&2017, and YouTube-VOS2018&2019, on the top of famous
matching-based VOS solutions.",None,-1
8a1fe6eb-e55c-46ac-a49d-45879a46a98f,RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling Check,0.339914,1,"Chinese Spelling Check (CSC) refers to the detection and correction of
spelling errors in Chinese texts. In practical application scenarios, it is
important to make CSC models have the ability to correct errors across
different domains. In this paper, we propose a retrieval-augmented spelling
check framework called RSpell, which searches corresponding domain terms and
incorporates them into CSC models. Specifically, we employ pinyin fuzzy
matching to search for terms, which are combined with the input and fed into
the CSC model. Then, we introduce an adaptive process control mechanism to
dynamically adjust the impact of external knowledge on the model. Additionally,
we develop an iterative strategy for the RSpell framework to enhance reasoning
capabilities. We conducted experiments on CSC datasets in three domains: law,
medicine, and official document writing. The results demonstrate that RSpell
achieves state-of-the-art performance in both zero-shot and fine-tuning
scenarios, demonstrating the effectiveness of the retrieval-augmented CSC
framework. Our code is available at https://github.com/47777777/Rspell.",None,-1
1d5fea85-2c71-4d15-bca0-d5ee24931464,Efficiency 360: Efficient Vision Transformers,0.150911,5,"Transformers are widely used for solving tasks in natural language
processing, computer vision, speech, and music domains. In this paper, we talk
about the efficiency of transformers in terms of memory (the number of
parameters), computation cost (number of floating points operations), and
performance of models, including accuracy, the robustness of the model, and
fair \& bias-free features. We mainly discuss the vision transformer for the
image classification task. Our contribution is to introduce an efficient 360
framework, which includes various aspects of the vision transformer, to make it
more efficient for industrial applications. By considering those applications,
we categorize them into multiple dimensions such as privacy, robustness,
transparency, fairness, inclusiveness, continual learning, probabilistic
models, approximation, computational complexity, and spectral complexity. We
compare various vision transformer models based on their performance, the
number of parameters, and the number of floating point operations (FLOPs) on
multiple datasets.",None,-1
ba420174-5cfc-496c-87a5-a40e6b2218c5,Give Me More Details: Improving Fact-Checking with Latent Retrieval,0.361114,2,"Evidence plays a crucial role in automated fact-checking. When verifying
real-world claims, existing fact-checking systems either assume the evidence
sentences are given or use the search snippets returned by the search engine.
Such methods ignore the challenges of collecting evidence and may not provide
sufficient information to verify real-world claims. Aiming at building a better
fact-checking system, we propose to incorporate full text from source documents
as evidence and introduce two enriched datasets. The first one is a
multilingual dataset, while the second one is monolingual (English). We further
develop a latent variable model to jointly extract evidence sentences from
documents and perform claim verification. Experiments indicate that including
source documents can provide sufficient contextual clues even when gold
evidence sentences are not annotated. The proposed system is able to achieve
significant improvements upon best-reported models under different settings.",None,-1
29adefb2-9bae-4271-a96d-8bb056be869c,BotShape: A Novel Social Bots Detection Approach via Behavioral Patterns,0.706988,11,"An essential topic in online social network security is how to accurately
detect bot accounts and relieve their harmful impacts (e.g., misinformation,
rumor, and spam) on genuine users. Based on a real-world data set, we construct
behavioral sequences from raw event logs. After extracting critical
characteristics from behavioral time series, we observe differences between
bots and genuine users and similar patterns among bot accounts. We present a
novel social bot detection system BotShape, to automatically catch behavioral
sequences and characteristics as features for classifiers to detect bots. We
evaluate the detection performance of our system in ground-truth instances,
showing an average accuracy of 98.52% and an average f1-score of 96.65% on
various types of classifiers. After comparing it with other research, we
conclude that BotShape is a novel approach to profiling an account, which could
improve performance for most methods by providing significant behavioral
features.",None,-1
67a46229-b4de-4635-9072-0b7621598cbb,A Study on Deep CNN Structures for Defect Detection From Laser Ultrasonic Visualization Testing Images,0.288904,2,"The importance of ultrasonic nondestructive testing has been increasing in
recent years, and there are high expectations for the potential of laser
ultrasonic visualization testing, which combines laser ultrasonic testing with
scattered wave visualization technology. Even if scattered waves are
visualized, inspectors still need to carefully inspect the images. To automate
this, this paper proposes a deep neural network for automatic defect detection
and localization in LUVT images. To explore the structure of a neural network
suitable to this task, we compared the LUVT image analysis problem with the
generic object detection problem. Numerical experiments using real-world data
from a SUS304 flat plate showed that the proposed method is more effective than
the general object detection model in terms of prediction performance. We also
show that the computational time required for prediction is faster than that of
the general object detection model.",None,-1
d0e9c233-0974-41e7-91f5-42cc6d7c78af,Fast Adversarial CNN-based Perturbation Attack on No-Reference Image- and Video-Quality Metrics,0.586299,4,"Modern neural-network-based no-reference image- and video-quality metrics
exhibit performance as high as full-reference metrics. These metrics are widely
used to improve visual quality in computer vision methods and compare video
processing methods. However, these metrics are not stable to traditional
adversarial attacks, which can cause incorrect results. Our goal is to
investigate the boundaries of no-reference metrics applicability, and in this
paper, we propose a fast adversarial perturbation attack on no-reference
quality metrics. The proposed attack (FACPA) can be exploited as a
preprocessing step in real-time video processing and compression algorithms.
This research can yield insights to further aid in designing of stable
neural-network-based no-reference quality metrics.",None,-1
5c288234-2600-4086-8f5b-b62a0f5ed605,Environmental-Impact Based Multi-Agent Reinforcement Learning,0.254425,1,"To promote cooperation and strengthen the individual impact on the collective
outcome in social dilemmas, we propose the Environmental-impact Multi-Agent
Reinforcement Learning (EMuReL) method where each agent estimates the
""environmental impact"" of every other agent, that is, the difference in the
current environment state compared to the hypothetical environment in the
absence of that other agent. Inspired by the Inequity Aversion model, the agent
then compares its own reward with those of its fellows multiplied by their
environmental impacts. If its reward exceeds the scaled reward of one of its
fellows, the agent takes ""social responsibility"" toward that fellow by reducing
its own reward. Therefore, the less influential an agent is in reaching the
current state, the more social responsibility is taken by other agents.
Experiments in the Cleanup (resp. Harvest) test environment demonstrate that
agents trained based on EMuReL learn to cooperate more effectively and obtain
$54\%$ ($39\%$) and $20\%$ ($44\%$) more total rewards while preserving the
same cooperation levels compared to when they are trained based on the two
state-of-the-art reward reshaping methods inequity aversion and social
influence.",None,-1
a159de25-5fc4-432c-9c40-251e8d0cf0da,A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis,0.248883,14,"Mathematical reasoning in large language models (LMs) has garnered
significant attention in recent work, but there is a limited understanding of
how these models process and store information related to arithmetic tasks
within their architecture. In order to improve our understanding of this aspect
of language models, we present a mechanistic interpretation of
Transformer-based LMs on arithmetic questions using a causal mediation analysis
framework. By intervening on the activations of specific model components and
measuring the resulting changes in predicted probabilities, we identify the
subset of parameters responsible for specific predictions. This provides
insights into how information related to arithmetic is processed by LMs. Our
experimental results indicate that LMs process the input by transmitting the
information relevant to the query from mid-sequence early layers to the final
token using the attention mechanism. Then, this information is processed by a
set of MLP modules, which generate result-related information that is
incorporated into the residual stream. To assess the specificity of the
observed activation dynamics, we compare the effects of different model
components on arithmetic queries with other tasks, including number retrieval
from prompts and factual knowledge questions.",None,-1
4d4e3828-f818-4686-be5c-847dcb5d4476,Importance Filtering with Risk Models for Complex Driving Situations,0.104502,1,"Self-driving cars face complex driving situations with a large amount of
agents when moving in crowded cities. However, some of the agents are actually
not influencing the behavior of the self-driving car. Filtering out unimportant
agents would inherently simplify the behavior or motion planning task for the
system. The planning system can then focus on fewer agents to find optimal
behavior solutions for the ego~agent. This is helpful especially in terms of
computational efficiency. In this paper, therefore, the research topic of
importance filtering with driving risk models is introduced. We give an
overview of state-of-the-art risk models and present newly adapted risk models
for filtering. Their capability to filter out surrounding unimportant agents is
compared in a large-scale experiment. As it turns out, the novel trajectory
distance balances performance, robustness and efficiency well. Based on the
results, we can further derive a novel filter architecture with multiple filter
steps, for which risk models are recommended for each step, to further improve
the robustness. We are confident that this will enable current behavior
planning systems to better solve complex situations in everyday driving.",None,-1
2986efac-838f-481f-a479-314183df2dad,How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?,0.984575,11,"Recent large vision-language models such as CLIP have shown remarkable
out-of-distribution (OOD) detection and generalization performance. However,
their zero-shot in-distribution (ID) accuracy is often limited for downstream
datasets. Recent CLIP-based fine-tuning methods such as prompt learning have
demonstrated significant improvements in ID classification and OOD
generalization where OOD labels are available. Nonetheless, it remains unclear
whether the model is reliable to semantic shifts without OOD labels. In this
paper, we aim to bridge the gap and present a comprehensive study to understand
how fine-tuning impact OOD detection for few-shot downstream tasks. By framing
OOD detection as multi-modal concept matching, we establish a connection
between fine-tuning methods and various OOD scores. Our results suggest that a
proper choice of OOD scores is essential for CLIP-based fine-tuning. In
particular, the maximum concept matching (MCM) score provides a promising
solution consistently. We also show that prompt learning demonstrates the
state-of-the-art OOD detection performance over the zero-shot counterpart.",None,-1
26dff59b-2970-4b1d-a668-948550b52777,RECLIP: Resource-efficient CLIP by Training with Small Images,0.267802,6,"We present RECLIP (Resource-efficient CLIP), a simple method that minimizes
computational resource footprint for CLIP (Contrastive Language Image
Pretraining). Inspired by the notion of coarse-to-fine in computer vision, we
leverage small images to learn from large-scale language supervision
efficiently, and finetune the model with high-resolution data in the end. Since
the complexity of the vision transformer heavily depends on input image size,
our approach significantly reduces the training resource requirements both in
theory and in practice. Using the same batch size and training epoch, RECLIP
achieves highly competitive zero-shot classification and image-text retrieval
accuracy with 6 to 8x less computational resources and 7 to 9x fewer FLOPs than
the baseline. Compared to the state-of-the-art contrastive learning methods,
RECLIP demonstrates 5 to 59x training resource savings while maintaining highly
competitive zero-shot classification and retrieval performance. Finally, RECLIP
matches the state of the art in transfer learning to open-vocabulary detection
tasks, achieving 32 APr on LVIS. We hope this work will pave the path for the
broader research community to explore language supervised pretraining in
resource-friendly settings.",None,-1
b832f8de-c505-470c-a8e9-74bf18de2e45,"Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans",0.15158,2,"The recently introduced Controlled Text Reduction (CTR) task isolates the
text generation step within typical summarization-style tasks. It does so by
challenging models to generate coherent text conforming to pre-selected content
within the input text (``highlights''). This framing enables increased
modularity in summarization-like tasks, allowing to couple a single CTR model
with various content-selection setups and modules. However, there are currently
no reliable CTR models, while the performance of the existing baseline for the
task is mediocre, falling short of practical utility. Here, we address this gap
by introducing a high-quality, open-source CTR model that tackles two prior key
limitations: inadequate enforcement of the content-preservation constraint, and
suboptimal silver training data. Addressing these, we amplify the
content-preservation constraint in both training, via RL, and inference, via a
controlled decoding strategy. Further, we substantially improve the silver
training data quality via GPT-4 distillation. Overall, pairing the distilled
dataset with the highlight-adherence strategies yields marked gains over the
current baseline, of up to 30 ROUGE-L points, providing a reliable CTR model
for downstream use.",None,-1
2a10a22f-fe98-43b4-a5c2-91000c66bba1,Generative Plug and Play: Posterior Sampling for Inverse Problems,0.747906,7,"Over the past decade, Plug-and-Play (PnP) has become a popular method for
reconstructing images using a modular framework consisting of a forward and
prior model. The great strength of PnP is that an image denoiser can be used as
a prior model while the forward model can be implemented using more traditional
physics-based approaches. However, a limitation of PnP is that it reconstructs
only a single deterministic image.
  In this paper, we introduce Generative Plug-and-Play (GPnP), a generalization
of PnP to sample from the posterior distribution. As with PnP, GPnP has a
modular framework using a physics-based forward model and an image denoising
prior model. However, in GPnP these models are extended to become proximal
generators, which sample from associated distributions. GPnP applies these
proximal generators in alternation to produce samples from the posterior. We
present experimental simulations using the well-known BM3D denoiser. Our
results demonstrate that the GPnP method is robust, easy to implement, and
produces intuitively reasonable samples from the posterior for sparse
interpolation and tomographic reconstruction. Code to accompany this paper is
available at https://github.com/gbuzzard/generative-pnp-allerton .",None,-1
d39174b5-764d-4af2-8d05-aefece9c7c5e,MFT: Long-Term Tracking of Every Pixel,0.990078,16,"We propose MFT -- Multi-Flow dense Tracker -- a novel method for dense,
pixel-level, long-term tracking. The approach exploits optical flows estimated
not only between consecutive frames, but also for pairs of frames at
logarithmically spaced intervals. It selects the most reliable sequence of
flows on the basis of estimates of its geometric accuracy and the probability
of occlusion, both provided by a pre-trained CNN. We show that MFT achieves
competitive performance on the TAP-Vid benchmark, outperforming baselines by a
significant margin, and tracking densely orders of magnitude faster than the
state-of-the-art point-tracking methods. The method is insensitive to
medium-length occlusions and it is robustified by estimating flow with respect
to the reference frame, which reduces drift.",None,-1
20cb24f6-c47e-4c72-b3ae-14f3d04911cb,Unified View of Damage leaves Planimetry & Analysis Using Digital Images Processing Techniques,0.690356,5,"The detection of leaf diseases in plants generally involves visual
observation of patterns appearing on the leaf surface. However, there are many
diseases that are distinguished based on very subtle changes in these visually
observable patterns. This paper attempts to identify plant leaf diseases using
image processing techniques. The focus of this study is on the detection of
citrus leaf canker disease. Canker is a bacterial infection of leaves. Symptoms
of citrus cankers include brown spots on the leaves, often with a watery or
oily appearance. The spots (called lesions in botany) are usually yellow. It is
surrounded by a halo of the leaves and is found on both the top and bottom of
the leaf. This paper describes various methods that have been used to detect
citrus leaf canker disease. The methods used are histogram comparison and
k-means clustering. Using these methods, citrus canker development was detected
based on histograms generated based on leaf patterns. The results thus obtained
can be used, after consultation with experts in the field of agriculture, to
identify suitable treatments for the processes used.",None,-1
383b7e35-7a1f-4b00-afa3-c4ca7ce4d155,Causal Structure Learning Supervised by Large Language Model,0.883309,4,"Causal discovery from observational data is pivotal for deciphering complex
relationships. Causal Structure Learning (CSL), which focuses on deriving
causal Directed Acyclic Graphs (DAGs) from data, faces challenges due to vast
DAG spaces and data sparsity. The integration of Large Language Models (LLMs),
recognized for their causal reasoning capabilities, offers a promising
direction to enhance CSL by infusing it with knowledge-based causal inferences.
However, existing approaches utilizing LLMs for CSL have encountered issues,
including unreliable constraints from imperfect LLM inferences and the
computational intensity of full pairwise variable analyses. In response, we
introduce the Iterative LLM Supervised CSL (ILS-CSL) framework. ILS-CSL
innovatively integrates LLM-based causal inference with CSL in an iterative
process, refining the causal DAG using feedback from LLMs. This method not only
utilizes LLM resources more efficiently but also generates more robust and
high-quality structural constraints compared to previous methodologies. Our
comprehensive evaluation across eight real-world datasets demonstrates
ILS-CSL's superior performance, setting a new standard in CSL efficacy and
showcasing its potential to significantly advance the field of causal
discovery. The codes are available at
\url{https://github.com/tyMadara/ILS-CSL}.",None,-1
a8179bf9-0f01-4fc6-a4e6-33f6ff5f5a62,Modelling Temporal Document Sequences for Clinical ICD Coding,0.499543,5,"Past studies on the ICD coding problem focus on predicting clinical codes
primarily based on the discharge summary. This covers only a small fraction of
the notes generated during each hospital stay and leaves potential for
improving performance by analysing all the available clinical notes. We propose
a hierarchical transformer architecture that uses text across the entire
sequence of clinical notes in each hospital stay for ICD coding, and
incorporates embeddings for text metadata such as their position, time, and
type of note. While using all clinical notes increases the quantity of data
substantially, superconvergence can be used to reduce training costs. We
evaluate the model on the MIMIC-III dataset. Our model exceeds the prior
state-of-the-art when using only discharge summaries as input, and achieves
further performance improvements when all clinical notes are used as input.",None,-1
93a36a13-8c16-42b9-b30c-16c3c42c24b0,FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation,0.833426,16,"Recent attention in instance segmentation has focused on query-based models.
Despite being non-maximum suppression (NMS)-free and end-to-end, the
superiority of these models on high-accuracy real-time benchmarks has not been
well demonstrated. In this paper, we show the strong potential of query-based
models on efficient instance segmentation algorithm designs. We present
FastInst, a simple, effective query-based framework for real-time instance
segmentation. FastInst can execute at a real-time speed (i.e., 32.5 FPS) while
yielding an AP of more than 40 (i.e., 40.5 AP) on COCO test-dev without bells
and whistles. Specifically, FastInst follows the meta-architecture of recently
introduced Mask2Former. Its key designs include instance activation-guided
queries, dual-path update strategy, and ground truth mask-guided learning,
which enable us to use lighter pixel decoders, fewer Transformer decoder
layers, while achieving better performance. The experiments show that FastInst
outperforms most state-of-the-art real-time counterparts, including strong
fully convolutional baselines, in both speed and accuracy. Code can be found at
https://github.com/junjiehe96/FastInst .",None,-1
be595275-55d3-413c-b2a8-020daee1f01f,NeCo@ALQAC 2023: Legal Domain Knowledge Acquisition for Low-Resource Languages through Data Enrichment,0.501738,2,"In recent years, natural language processing has gained significant
popularity in various sectors, including the legal domain. This paper presents
NeCo Team's solutions to the Vietnamese text processing tasks provided in the
Automated Legal Question Answering Competition 2023 (ALQAC 2023), focusing on
legal domain knowledge acquisition for low-resource languages through data
enrichment. Our methods for the legal document retrieval task employ a
combination of similarity ranking and deep learning models, while for the
second task, which requires extracting an answer from a relevant legal article
in response to a question, we propose a range of adaptive techniques to handle
different question types. Our approaches achieve outstanding results on both
tasks of the competition, demonstrating the potential benefits and
effectiveness of question answering systems in the legal field, particularly
for low-resource languages.",None,-1
91686bac-7552-45af-983b-11077fa8ca9e,The moral authority of ChatGPT,0.909972,29,"ChatGPT is not only fun to chat with, but it also searches information,
answers questions, and gives advice. With consistent moral advice, it might
improve the moral judgment and decisions of users, who often hold contradictory
moral beliefs. Unfortunately, ChatGPT turns out highly inconsistent as a moral
advisor. Nonetheless, it influences users' moral judgment, we find in an
experiment, even if they know they are advised by a chatting bot, and they
underestimate how much they are influenced. Thus, ChatGPT threatens to corrupt
rather than improves users' judgment. These findings raise the question of how
to ensure the responsible use of ChatGPT and similar AI. Transparency is often
touted but seems ineffective. We propose training to improve digital literacy.",None,-1
6637aaf7-46e6-4bf0-8b5a-84fe609143ba,Orca 2: Teaching Small Language Models How to Reason,0.846302,67,"Orca 1 learns from rich signals, such as explanation traces, allowing it to
outperform conventional instruction-tuned models on benchmarks like BigBench
Hard and AGIEval. In Orca 2, we continue exploring how improved training
signals can enhance smaller LMs' reasoning abilities. Research on training
small LMs has often relied on imitation learning to replicate the output of
more capable models. We contend that excessive emphasis on imitation may
restrict the potential of smaller models. We seek to teach small LMs to employ
different solution strategies for different tasks, potentially different from
the one used by the larger model. For example, while larger models might
provide a direct answer to a complex task, smaller models may not have the same
capacity. In Orca 2, we teach the model various reasoning techniques
(step-by-step, recall then generate, recall-reason-generate, direct answer,
etc.). More crucially, we aim to help the model learn to determine the most
effective solution strategy for each task. We evaluate Orca 2 using a
comprehensive set of 15 diverse benchmarks (corresponding to approximately 100
tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of
similar size and attains performance levels similar or better to those of
models 5-10x larger, as assessed on complex tasks that test advanced reasoning
abilities in zero-shot settings. make Orca 2 weights publicly available at
aka.ms/orca-lm to support research on the development, evaluation, and
alignment of smaller LMs",None,-1
500879c0-cc51-4d2a-8494-7be13f43f0b7,PRP Rebooted: Advancing the State of the Art in FOND Planning,0.100012,1,"Fully Observable Non-Deterministic (FOND) planning is a variant of classical
symbolic planning in which actions are nondeterministic, with an action's
outcome known only upon execution. It is a popular planning paradigm with
applications ranging from robot planning to dialogue-agent design and reactive
synthesis. Over the last 20 years, a number of approaches to FOND planning have
emerged. In this work, we establish a new state of the art, following in the
footsteps of some of the most powerful FOND planners to date. Our planner, PR2,
decisively outperforms the four leading FOND planners, at times by a large
margin, in 17 of 18 domains that represent a comprehensive benchmark suite.
Ablation studies demonstrate the impact of various techniques we introduce,
with the largest improvement coming from our novel FOND-aware heuristic.",None,-1
fb241db8-3cc2-43c1-bb7e-d5767779ac05,GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer,0.731505,2,"Named Entity Recognition (NER) is essential in various Natural Language
Processing (NLP) applications. Traditional NER models are effective but limited
to a set of predefined entity types. In contrast, Large Language Models (LLMs)
can extract arbitrary entities through natural language instructions, offering
greater flexibility. However, their size and cost, particularly for those
accessed via APIs like ChatGPT, make them impractical in resource-limited
scenarios. In this paper, we introduce a compact NER model trained to identify
any type of entity. Leveraging a bidirectional transformer encoder, our model,
GLiNER, facilitates parallel entity extraction, an advantage over the slow
sequential token generation of LLMs. Through comprehensive testing, GLiNER
demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs
in zero-shot evaluations on various NER benchmarks.",None,-1
ba77c968-bce9-4424-b914-7d6d43459c70,Multimodality of AI for Education: Towards Artificial General Intelligence,0.888273,15,"This paper presents a comprehensive examination of how multimodal artificial
intelligence (AI) approaches are paving the way towards the realization of
Artificial General Intelligence (AGI) in educational contexts. It scrutinizes
the evolution and integration of AI in educational systems, emphasizing the
crucial role of multimodality, which encompasses auditory, visual, kinesthetic,
and linguistic modes of learning. This research delves deeply into the key
facets of AGI, including cognitive frameworks, advanced knowledge
representation, adaptive learning mechanisms, strategic planning, sophisticated
language processing, and the integration of diverse multimodal data sources. It
critically assesses AGI's transformative potential in reshaping educational
paradigms, focusing on enhancing teaching and learning effectiveness, filling
gaps in existing methodologies, and addressing ethical considerations and
responsible usage of AGI in educational settings. The paper also discusses the
implications of multimodal AI's role in education, offering insights into
future directions and challenges in AGI development. This exploration aims to
provide a nuanced understanding of the intersection between AI, multimodality,
and education, setting a foundation for future research and development in AGI.",None,-1
1f96303e-e8a1-4a72-bb8a-ccd4e5dcf817,Attention-based Point Cloud Edge Sampling,0.828372,20,"Point cloud sampling is a less explored research topic for this data
representation. The most commonly used sampling methods are still classical
random sampling and farthest point sampling. With the development of neural
networks, various methods have been proposed to sample point clouds in a
task-based learning manner. However, these methods are mostly generative-based,
rather than selecting points directly using mathematical statistics. Inspired
by the Canny edge detection algorithm for images and with the help of the
attention mechanism, this paper proposes a non-generative Attention-based Point
cloud Edge Sampling method (APES), which captures salient points in the point
cloud outline. Both qualitative and quantitative experimental results show the
superior performance of our sampling method on common benchmark tasks.",None,-1
6b08d18c-da6a-4413-bea4-d5bd2d069f72,SMATCH++: Standardized and Extended Evaluation of Semantic Graphs,0.741204,15,"The Smatch metric is a popular method for evaluating graph distances, as is
necessary, for instance, to assess the performance of semantic graph parsing
systems. However, we observe some issues in the metric that jeopardize
meaningful evaluation. E.g., opaque pre-processing choices can affect results,
and current graph-alignment solvers do not provide us with upper-bounds.
Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore,
adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)
are spread out, and lack a unifying framework.
  For better inspection, we divide the metric into three modules:
pre-processing, alignment, and scoring. Examining each module, we specify its
goals and diagnose potential issues, for which we discuss and test mitigation
strategies. For pre-processing, we show how to fully conform to annotation
guidelines that allow structurally deviating but valid graphs. For safer and
enhanced alignment, we show the feasibility of optimal alignment in a standard
evaluation setup, and develop a lossless graph compression method that shrinks
the search space and significantly increases efficiency. For improved scoring,
we propose standardized and extended metric calculation of fine-grained
sub-graph meaning aspects. Our code is available at
https://github.com/flipz357/smatchpp",None,-1
8d48283e-9aa3-4c3e-8c89-9d19a4118cb6,BAD: BiAs Detection for Large Language Models in the context of candidate screening,0.617272,5,"Application Tracking Systems (ATS) have allowed talent managers, recruiters,
and college admissions committees to process large volumes of potential
candidate applications efficiently. Traditionally, this screening process was
conducted manually, creating major bottlenecks due to the quantity of
applications and introducing many instances of human bias. The advent of large
language models (LLMs) such as ChatGPT and the potential of adopting methods to
current automated application screening raises additional bias and fairness
issues that must be addressed. In this project, we wish to identify and
quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the
context of candidate screening in order to demonstrate how the use of these
models could perpetuate existing biases and inequalities in the hiring process.",None,-1
f620c79f-2690-4189-b3dc-88458a3241eb,Incorporating Unlabelled Data into Bayesian Neural Networks,0.745654,7,"Conventional Bayesian Neural Networks (BNNs) cannot leverage unlabelled data
to improve their predictions. To overcome this limitation, we introduce
Self-Supervised Bayesian Neural Networks, which use unlabelled data to learn
improved prior predictive distributions by maximising an evidence lower bound
during an unsupervised pre-training step. With a novel methodology developed to
better understand prior predictive distributions, we then show that
self-supervised prior predictives capture image semantics better than
conventional BNN priors. In our empirical evaluations, we see that
self-supervised BNNs offer the label efficiency of self-supervised methods and
the uncertainty estimates of Bayesian methods, particularly outperforming
conventional BNNs in low-to-medium data regimes.",None,-1
0c743f25-5a02-4072-9c3d-6fc9db402d5c,Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large Neighborhood Search,0.56061,1,"Anytime multi-agent path finding (MAPF) is a promising approach to scalable
path optimization in large-scale multi-agent systems. State-of-the-art anytime
MAPF is based on Large Neighborhood Search (LNS), where a fast initial solution
is iteratively optimized by destroying and repairing a fixed number of parts,
i.e., the neighborhood, of the solution, using randomized destroy heuristics
and prioritized planning. Despite their recent success in various MAPF
instances, current LNS-based approaches lack exploration and flexibility due to
greedy optimization with a fixed neighborhood size which can lead to low
quality solutions in general. So far, these limitations have been addressed
with extensive prior effort in tuning or offline machine learning beyond actual
planning. In this paper, we focus on online learning in LNS and propose
Bandit-based Adaptive LArge Neighborhood search Combined with Exploration
(BALANCE). BALANCE uses a bi-level multi-armed bandit scheme to adapt the
selection of destroy heuristics and neighborhood sizes on the fly during
search. We evaluate BALANCE on multiple maps from the MAPF benchmark set and
empirically demonstrate cost improvements of at least 50% compared to
state-of-the-art anytime MAPF in large-scale scenarios. We find that Thompson
Sampling performs particularly well compared to alternative multi-armed bandit
algorithms.",None,-1
85c3ae6f-e70e-4de0-8521-5e4c7e4bc413,Likelihood-Based Generative Radiance Field with Latent Space Energy-Based Model for 3D-Aware Disentangled Image Representation,0.216564,4,"We propose the NeRF-LEBM, a likelihood-based top-down 3D-aware 2D image
generative model that incorporates 3D representation via Neural Radiance Fields
(NeRF) and 2D imaging process via differentiable volume rendering. The model
represents an image as a rendering process from 3D object to 2D image and is
conditioned on some latent variables that account for object characteristics
and are assumed to follow informative trainable energy-based prior models. We
propose two likelihood-based learning frameworks to train the NeRF-LEBM: (i)
maximum likelihood estimation with Markov chain Monte Carlo-based inference and
(ii) variational inference with the reparameterization trick. We study our
models in the scenarios with both known and unknown camera poses. Experiments
on several benchmark datasets demonstrate that the NeRF-LEBM can infer 3D
object structures from 2D images, generate 2D images with novel views and
objects, learn from incomplete 2D images, and learn from 2D images with known
or unknown camera poses.",None,-1
98de1593-df6e-4425-b1ae-a32dd77415c7,Unsupervised Deep Graph Matching Based on Cycle Consistency,0.494695,2,"We contribute to the sparsely populated area of unsupervised deep graph
matching with application to keypoint matching in images. Contrary to the
standard \emph{supervised} approach, our method does not require ground truth
correspondences between keypoint pairs. Instead, it is self-supervised by
enforcing consistency of matchings between images of the same object category.
As the matching and the consistency loss are discrete, their derivatives cannot
be straightforwardly used for learning. We address this issue in a principled
way by building our method upon the recent results on black-box differentiation
of combinatorial solvers. This makes our method exceptionally flexible, as it
is compatible with arbitrary network architectures and combinatorial solvers.
Our experimental evaluation suggests that our technique sets a new
state-of-the-art for unsupervised graph matching.",None,-1
784a6845-5eb7-4ec8-bd9d-99ee07fe03a2,Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra,0.769243,19,"To improve the accuracy of color image completion with missing entries, we
present a recovery method based on generalized higher-order scalars. We extend
the traditional second-order matrix model to a more comprehensive higher-order
matrix equivalent, called the ""t-matrix"" model, which incorporates a pixel
neighborhood expansion strategy to characterize the local pixel constraints.
This ""t-matrix"" model is then used to extend some commonly used matrix and
tensor completion algorithms to their higher-order versions. We perform
extensive experiments on various algorithms using simulated data and algorithms
on simulated data and publicly available images and compare their performance.
The results show that our generalized matrix completion model and the
corresponding algorithm compare favorably with their lower-order tensor and
conventional matrix counterparts.",None,-1
0640a59c-1cda-4354-a517-6e8b2b93ebed,HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection,0.705775,31,"The High-Resolution Transformer (HRFormer) can maintain high-resolution
representation and share global receptive fields. It is friendly towards
salient object detection (SOD) in which the input and output have the same
resolution. However, two critical problems need to be solved for two-modality
SOD. One problem is two-modality fusion. The other problem is the HRFormer
output's fusion. To address the first problem, a supplementary modality is
injected into the primary modality by using global optimization and an
attention mechanism to select and purify the modality at the input level. To
solve the second problem, a dual-direction short connection fusion module is
used to optimize the output features of HRFormer, thereby enhancing the
detailed representation of objects at the output level. The proposed model,
named HRTransNet, first introduces an auxiliary stream for feature extraction
of supplementary modality. Then, features are injected into the primary
modality at the beginning of each multi-resolution branch. Next, HRFormer is
applied to achieve forwarding propagation. Finally, all the output features
with different resolutions are aggregated by intra-feature and inter-feature
interactive transformers. Application of the proposed model results in
impressive improvement for driving two-modality SOD tasks, e.g., RGB-D, RGB-T,
and light field SOD.https://github.com/liuzywen/HRTransNet",None,-1
f93657f0-ae39-4f56-b471-50b92fa79b65,Semantic Information Marketing in The Metaverse: A Learning-Based Contract Theory Framework,0.0670925,1,"In this paper, we address the problem of designing incentive mechanisms by a
virtual service provider (VSP) to hire sensing IoT devices to sell their
sensing data to help creating and rendering the digital copy of the physical
world in the Metaverse. Due to the limited bandwidth, we propose to use
semantic extraction algorithms to reduce the delivered data by the sensing IoT
devices. Nevertheless, mechanisms to hire sensing IoT devices to share their
data with the VSP and then deliver the constructed digital twin to the
Metaverse users are vulnerable to adverse selection problem. The adverse
selection problem, which is caused by information asymmetry between the system
entities, becomes harder to solve when the private information of the different
entities are multi-dimensional. We propose a novel iterative contract design
and use a new variant of multi-agent reinforcement learning (MARL) to solve the
modelled multi-dimensional contract problem. To demonstrate the effectiveness
of our algorithm, we conduct extensive simulations and measure several key
performance metrics of the contract for the Metaverse. Our results show that
our designed iterative contract is able to incentivize the participants to
interact truthfully, which maximizes the profit of the VSP with minimal
individual rationality (IR) and incentive compatibility (IC) violation rates.
Furthermore, the proposed learning-based iterative contract framework has
limited access to the private information of the participants, which is to the
best of our knowledge, the first of its kind in addressing the problem of
adverse selection in incentive mechanisms.",None,-1
1ee4f7f6-ab59-4639-8cad-2794acbd709a,MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer,0.769474,15,"With the increasing demand for video understanding, video moment and
highlight detection (MHD) has emerged as a critical research topic. MHD aims to
localize all moments and predict clip-wise saliency scores simultaneously.
Despite progress made by existing DETR-based methods, we observe that these
methods coarsely fuse features from different modalities, which weakens the
temporal intra-modal context and results in insufficient cross-modal
interaction. To address this issue, we propose MH-DETR (Moment and Highlight
Detection Transformer) tailored for MHD. Specifically, we introduce a simple
yet efficient pooling operator within the uni-modal encoder to capture global
intra-modal context. Moreover, to obtain temporally aligned cross-modal
features, we design a plug-and-play cross-modal interaction module between the
encoder and decoder, seamlessly integrating visual and textual features.
Comprehensive experiments on QVHighlights, Charades-STA, Activity-Net, and
TVSum datasets show that MH-DETR outperforms existing state-of-the-art methods,
demonstrating its effectiveness and superiority. Our code is available at
https://github.com/YoucanBaby/MH-DETR.",None,-1
39c860f4-1436-4256-9b61-1d0b98ff50dc,Utterance Classification with Logical Neural Network: Explainable AI for Mental Disorder Diagnosis,0.251628,1,"In response to the global challenge of mental health problems, we proposes a
Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis
of mental disorders. Due to the lack of effective therapy coverage for mental
disorders, there is a need for an AI solution that can assist therapists with
the diagnosis. However, current Neural Network models lack explainability and
may not be trusted by therapists. The LNN is a Recurrent Neural Network
architecture that combines the learning capabilities of neural networks with
the reasoning capabilities of classical logic-based AI. The proposed system
uses input predicates from clinical interviews to output a mental disorder
class, and different predicate pruning techniques are used to achieve
scalability and higher scores. In addition, we provide an insight extraction
method to aid therapists with their diagnosis. The proposed system addresses
the lack of explainability of current Neural Network models and provides a more
trustworthy solution for mental disorder diagnosis.",None,-1
109f1995-05fa-4319-a72b-83866cb5abe9,Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection,0.843841,4,"Out-of-distribution (OOD) detection is essential for reliable and trustworthy
machine learning. Recent multi-modal OOD detection leverages textual
information from in-distribution (ID) class names for visual OOD detection, yet
it currently neglects the rich contextual information of ID classes. Large
language models (LLMs) encode a wealth of world knowledge and can be prompted
to generate descriptive features for each class. Indiscriminately using such
knowledge causes catastrophic damage to OOD detection due to LLMs'
hallucinations, as is observed by our analysis. In this paper, we propose to
apply world knowledge to enhance OOD detection performance through selective
generation from LLMs. Specifically, we introduce a consistency-based
uncertainty calibration method to estimate the confidence score of each
generation. We further extract visual objects from each image to fully
capitalize on the aforementioned world knowledge. Extensive experiments
demonstrate that our method consistently outperforms the state-of-the-art.",None,-1
22aebcf1-da61-4141-b72f-276fe76f34c1,RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue,0.687599,6,"Evaluating open-domain dialogue systems is challenging for reasons such as
the one-to-many problem, i.e., many appropriate responses other than just the
golden response. As of now, automatic evaluation methods need better
consistency with humans, while reliable human evaluation can be time- and
cost-intensive. To this end, we propose the Reference-Assisted Dialogue
Evaluation (RADE) approach under the multi-task learning framework, which
leverages the pre-created utterance as reference other than the gold response
to relief the one-to-many problem. Specifically, RADE explicitly compares
reference and the candidate response to predict their overall scores. Moreover,
an auxiliary response generation task enhances prediction via a shared encoder.
To support RADE, we extend three datasets with additional rated responses other
than just a golden response by human annotation. Experiments on our three
datasets and two existing benchmarks demonstrate the effectiveness of our
method, where Pearson, Spearman, and Kendall correlations with human evaluation
outperform state-of-the-art baselines.",None,-1
6de3a224-c55e-4673-8e0c-e9810c574807,Graph Decision Transformer,0.495718,9,"Offline reinforcement learning (RL) is a challenging task, whose objective is
to learn policies from static trajectory data without interacting with the
environment. Recently, offline RL has been viewed as a sequence modeling
problem, where an agent generates a sequence of subsequent actions based on a
set of static transition experiences. However, existing approaches that use
transformers to attend to all tokens naively can overlook the dependencies
between different tokens and limit long-term dependency learning. In this
paper, we propose the Graph Decision Transformer (GDT), a novel offline RL
approach that models the input sequence into a causal graph to capture
potential dependencies between fundamentally different concepts and facilitate
temporal and causal relationship learning. GDT uses a graph transformer to
process the graph inputs with relation-enhanced mechanisms, and an optional
sequence transformer to handle fine-grained spatial information in visual
tasks. Our experiments show that GDT matches or surpasses the performance of
state-of-the-art offline RL methods on image-based Atari and OpenAI Gym.",None,-1
dbcd6b69-7adb-4771-9df4-6b9f414a8294,ProphNet: Efficient Agent-Centric Motion Forecasting with Anchor-Informed Proposals,0.974765,22,"Motion forecasting is a key module in an autonomous driving system. Due to
the heterogeneous nature of multi-sourced input, multimodality in agent
behavior, and low latency required by onboard deployment, this task is
notoriously challenging. To cope with these difficulties, this paper proposes a
novel agent-centric model with anchor-informed proposals for efficient
multimodal motion prediction. We design a modality-agnostic strategy to
concisely encode the complex input in a unified manner. We generate diverse
proposals, fused with anchors bearing goal-oriented scene context, to induce
multimodal prediction that covers a wide range of future trajectories. Our
network architecture is highly uniform and succinct, leading to an efficient
model amenable for real-world driving deployment. Experiments reveal that our
agent-centric network compares favorably with the state-of-the-art methods in
prediction accuracy, while achieving scene-centric level inference latency.",None,-1
09eff274-679b-4bdd-98ff-5edef0a6ca8e,WHC: Weighted Hybrid Criterion for Filter Pruning on Convolutional Neural Networks,0.283692,5,"Filter pruning has attracted increasing attention in recent years for its
capacity in compressing and accelerating convolutional neural networks. Various
data-independent criteria, including norm-based and relationship-based ones,
were proposed to prune the most unimportant filters. However, these
state-of-the-art criteria fail to fully consider the dissimilarity of filters,
and thus might lead to performance degradation. In this paper, we first analyze
the limitation of relationship-based criteria with examples, and then introduce
a new data-independent criterion, Weighted Hybrid Criterion (WHC), to tackle
the problems of both norm-based and relationship-based criteria. By taking the
magnitude of each filter and the linear dependence between filters into
consideration, WHC can robustly recognize the most redundant filters, which can
be safely pruned without introducing severe performance degradation to
networks. Extensive pruning experiments in a simple one-shot manner demonstrate
the effectiveness of the proposed WHC. In particular, WHC can prune ResNet-50
on ImageNet with more than 42% of floating point operations reduced without any
performance loss in top-5 accuracy.",None,-1
7a15d322-2232-4761-ba6c-05584552b77d,Unsupervised Learning of Robust Spectral Shape Matching,0.801059,24,"We propose a novel learning-based approach for robust 3D shape matching. Our
method builds upon deep functional maps and can be trained in a fully
unsupervised manner. Previous deep functional map methods mainly focus on
predicting optimised functional maps alone, and then rely on off-the-shelf
post-processing to obtain accurate point-wise maps during inference. However,
this two-stage procedure for obtaining point-wise maps often yields sub-optimal
performance. In contrast, building upon recent insights about the relation
between functional maps and point-wise maps, we propose a novel unsupervised
loss to couple the functional maps and point-wise maps, and thereby directly
obtain point-wise maps without any post-processing. Our approach obtains
accurate correspondences not only for near-isometric shapes, but also for more
challenging non-isometric shapes and partial shapes, as well as shapes with
different discretisation or topological noise. Using a total of nine diverse
datasets, we extensively evaluate the performance and demonstrate that our
method substantially outperforms previous state-of-the-art methods, even
compared to recent supervised methods. Our code is available at
https://github.com/dongliangcao/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching.",None,-1
c85bdd53-f21a-48a2-b877-e523030d3f9c,Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,0.441373,22,"We consider the questions of whether or not large language models (LLMs) have
beliefs, and, if they do, how we might measure them. First, we evaluate two
existing approaches, one due to Azaria and Mitchell (2023) and the other to
Burns et al. (2022). We provide empirical results that show that these methods
fail to generalize in very basic ways. We then argue that, even if LLMs have
beliefs, these methods are unlikely to be successful for conceptual reasons.
Thus, there is still no lie-detector for LLMs. After describing our empirical
results we take a step back and consider whether or not we should expect LLMs
to have something like beliefs in the first place. We consider some recent
arguments aiming to show that LLMs cannot have beliefs. We show that these
arguments are misguided. We provide a more productive framing of questions
surrounding the status of beliefs in LLMs, and highlight the empirical nature
of the problem. We conclude by suggesting some concrete paths for future work.",None,-1
0832cf03-fe1d-4468-abd5-2d790e080a4c,InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery,0.994606,23,"The rapid evolution of artificial intelligence in drug discovery encounters
challenges with generalization and extensive training, yet Large Language
Models (LLMs) offer promise in reshaping interactions with complex molecular
data. Our novel contribution, InstructMol, a multi-modal LLM, effectively
aligns molecular structures with natural language via an instruction-tuning
approach, utilizing a two-stage training strategy that adeptly combines limited
domain-specific data with molecular and textual information. InstructMol
showcases substantial performance improvements in drug discovery-related
molecular tasks, surpassing leading LLMs and significantly reducing the gap
with specialized models, thereby establishing a robust foundation for a
versatile and dependable drug discovery assistant.",None,-1
7d3bf0e1-bdce-49da-9c26-2a94fbd34bfa,Code Models are Zero-shot Precondition Reasoners,0.452807,2,"One of the fundamental skills required for an agent acting in an environment
to complete tasks is the ability to understand what actions are plausible at
any given point. This work explores a novel use of code representations to
reason about action preconditions for sequential decision making tasks. Code
representations offer the flexibility to model procedural activities and
associated constraints as well as the ability to execute and verify constraint
satisfaction. Leveraging code representations, we extract action preconditions
from demonstration trajectories in a zero-shot manner using pre-trained code
models. Given these extracted preconditions, we propose a precondition-aware
action sampling strategy that ensures actions predicted by a policy are
consistent with preconditions. We demonstrate that the proposed approach
enhances the performance of few-shot policy learning approaches across
task-oriented dialog and embodied textworld benchmarks.",None,-1
209e75c9-1507-419a-8f1d-a37bebc759af,Where Would I Go Next? Large Language Models as Human Mobility Predictors,0.854218,26,"Accurate human mobility prediction underpins many important applications
across a variety of domains, including epidemic modelling, transport planning,
and emergency responses. Due to the sparsity of mobility data and the
stochastic nature of people's daily activities, achieving precise predictions
of people's locations remains a challenge. While recently developed large
language models (LLMs) have demonstrated superior performance across numerous
language-related tasks, their applicability to human mobility studies remains
unexplored. Addressing this gap, this article delves into the potential of LLMs
for human mobility prediction tasks. We introduce a novel method, LLM-Mob,
which leverages the language understanding and reasoning capabilities of LLMs
for analysing human mobility data. We present concepts of historical stays and
context stays to capture both long-term and short-term dependencies in human
movement and enable time-aware prediction by using time information of the
prediction target. Additionally, we design context-inclusive prompts that
enable LLMs to generate more accurate predictions. Comprehensive evaluations of
our method reveal that LLM-Mob excels in providing accurate and interpretable
predictions, highlighting the untapped potential of LLMs in advancing human
mobility prediction techniques. We posit that our research marks a significant
paradigm shift in human mobility modelling, transitioning from building complex
domain-specific models to harnessing general-purpose LLMs that yield accurate
predictions through language instructions. The code for this work is available
at https://github.com/xlwang233/LLM-Mob.",None,-1
62a3e23b-45c8-4788-b829-d3de5975541b,Uncertainty-Aware Cross-Modal Transfer Network for Sketch-Based 3D Shape Retrieval,0.252982,1,"In recent years, sketch-based 3D shape retrieval has attracted growing
attention. While many previous studies have focused on cross-modal matching
between hand-drawn sketches and 3D shapes, the critical issue of how to handle
low-quality and noisy samples in sketch data has been largely neglected. This
paper presents an uncertainty-aware cross-modal transfer network (UACTN) that
addresses this issue. UACTN decouples the representation learning of sketches
and 3D shapes into two separate tasks: classification-based sketch uncertainty
learning and 3D shape feature transfer. We first introduce an end-to-end
classification-based approach that simultaneously learns sketch features and
uncertainty, allowing uncertainty to prevent overfitting noisy sketches by
assigning different levels of importance to clean and noisy sketches. Then, 3D
shape features are mapped into the pre-learned sketch embedding space for
feature alignment. Extensive experiments and ablation studies on two benchmarks
demonstrate the superiority of our proposed method compared to state-of-the-art
methods.",None,-1
262143b3-20d1-42da-9952-38e120ee3a38,Unsupervised Learning for Combinatorial Optimization Needs Meta-Learning,0.764459,8,"A general framework of unsupervised learning for combinatorial optimization
(CO) is to train a neural network (NN) whose output gives a problem solution by
directly optimizing the CO objective. Albeit with some advantages over
traditional solvers, the current framework optimizes an averaged performance
over the distribution of historical problem instances, which misaligns with the
actual goal of CO that looks for a good solution to every future encountered
instance. With this observation, we propose a new objective of unsupervised
learning for CO where the goal of learning is to search for good initialization
for future problem instances rather than give direct solutions. We propose a
meta-learning-based training pipeline for this new objective. Our method
achieves good empirical performance. We observe that even just the initial
solution given by our model before fine-tuning can significantly outperform the
baselines under various evaluation settings including evaluation across
multiple datasets, and the case with big shifts in the problem scale. The
reason we conjecture is that meta-learning-based training lets the model be
loosely tied to each local optima for a training instance while being more
adaptive to the changes of optimization landscapes across instances.",None,-1
18570899-0b7f-4869-b7b1-463e652d9e09,On the challenges to learn from Natural Data Streams,0.212567,2,"In real-world contexts, sometimes data are available in form of Natural Data
Streams, i.e. data characterized by a streaming nature, unbalanced
distribution, data drift over a long time frame and strong correlation of
samples in short time ranges. Moreover, a clear separation between the
traditional training and deployment phases is usually lacking. This data
organization and fruition represents an interesting and challenging scenario
for both traditional Machine and Deep Learning algorithms and incremental
learning agents, i.e. agents that have the ability to incrementally improve
their knowledge through the past experience. In this paper, we investigate the
classification performance of a variety of algorithms that belong to various
research field, i.e. Continual, Streaming and Online Learning, that receives as
training input Natural Data Streams. The experimental validation is carried out
on three different datasets, expressly organized to replicate this challenging
setting.",None,-1
81f8e602-d81e-4e53-b496-12acfd51a59d,PlaneRecTR: Unified Query Learning for 3D Plane Recovery from a Single View,0.496942,2,"3D plane recovery from a single image can usually be divided into several
subtasks of plane detection, segmentation, parameter estimation and possibly
depth estimation. Previous works tend to solve this task by either extending
the RCNN-based segmentation network or the dense pixel embedding-based
clustering framework. However, none of them tried to integrate above related
subtasks into a unified framework but treat them separately and sequentially,
which we suspect is potentially a main source of performance limitation for
existing approaches. Motivated by this finding and the success of query-based
learning in enriching reasoning among semantic entities, in this paper, we
propose PlaneRecTR, a Transformer-based architecture, which for the first time
unifies all subtasks related to single-view plane recovery with a single
compact model. Extensive quantitative and qualitative experiments demonstrate
that our proposed unified learning achieves mutual benefits across subtasks,
obtaining a new state-of-the-art performance on public ScanNet and NYUv2-Plane
datasets. Codes are available at https://github.com/SJingjia/PlaneRecTR.",None,-1
0473a0d6-a040-46db-9914-517d35a0a758,Evaluating Temporal Observation-Based Causal Discovery Techniques Applied to Road Driver Behaviour,0.535372,5,"Autonomous robots are required to reason about the behaviour of dynamic
agents in their environment. The creation of models to describe these
relationships is typically accomplished through the application of causal
discovery techniques. However, as it stands observational causal discovery
techniques struggle to adequately cope with conditions such as causal sparsity
and non-stationarity typically seen during online usage in autonomous agent
domains. Meanwhile, interventional techniques are not always feasible due to
domain restrictions. In order to better explore the issues facing observational
techniques and promote further discussion of these topics we carry out a
benchmark across 10 contemporary observational temporal causal discovery
methods in the domain of autonomous driving. By evaluating these methods upon
causal scenes drawn from real world datasets in addition to those generated
synthetically we highlight where improvements need to be made in order to
facilitate the application of causal discovery techniques to the aforementioned
use-cases. Finally, we discuss potential directions for future work that could
help better tackle the difficulties currently experienced by state of the art
techniques.",None,-1
71c5c518-2661-495d-8d92-c5b17b765e4f,Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model,0.999999,31,"In this paper, we rethink the low-light image enhancement task and propose a
physically explainable and generative diffusion model for low-light image
enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the
physical model and the generative network. Furthermore, we hope to supplement
and even deduce the information missing in the low-light image through the
generative network. Therefore, Diff-Retinex formulates the low-light image
enhancement problem into Retinex decomposition and conditional image
generation. In the Retinex decomposition, we integrate the superiority of
attention in Transformer and meticulously design a Retinex Transformer
decomposition network (TDN) to decompose the image into illumination and
reflectance maps. Then, we design multi-path generative diffusion networks to
reconstruct the normal-light Retinex probability distribution and solve the
various degradations in these components respectively, including dark
illumination, noise, color deviation, loss of scene contents, etc. Owing to
generative diffusion model, Diff-Retinex puts the restoration of low-light
subtle detail into practice. Extensive experiments conducted on real-world
low-light datasets qualitatively and quantitatively demonstrate the
effectiveness, superiority, and generalization of the proposed method.",None,-1
7266c801-5759-4103-930b-b645eb3dfc37,Multi-Granularity Prompts for Topic Shift Detection in Dialogue,0.637737,3,"The goal of dialogue topic shift detection is to identify whether the current
topic in a conversation has changed or needs to change. Previous work focused
on detecting topic shifts using pre-trained models to encode the utterance,
failing to delve into the various levels of topic granularity in the dialogue
and understand dialogue contents. To address the above issues, we take a
prompt-based approach to fully extract topic information from dialogues at
multiple-granularity, i.e., label, turn, and topic. Experimental results on our
annotated Chinese Natural Topic Dialogue dataset CNTD and the publicly
available English TIAGE dataset show that the proposed model outperforms the
baselines. Further experiments show that the information extracted at different
levels of granularity effectively helps the model comprehend the conversation
topics.",None,-1
c0e30eb6-01bf-4142-8f3c-81bfb54bf970,Improving Contrastive Learning of Sentence Embeddings from AI Feedback,0.73365,27,"Contrastive learning has become a popular approach in natural language
processing, particularly for the learning of sentence embeddings. However, the
discrete nature of natural language makes it difficult to ensure the quality of
positive and negative sample pairs generated through data augmentation methods.
Although supervised contrastive learning can produce more accurate sample pairs
with human feedback labels, it still lacks fine-grained training signals. In
this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of
sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our
method utilizes AI feedback from large pre-trained language models (LLMs) to
construct sample pairs with fine-grained sample similarity scores to improve
contrastive learning. Besides, we combine human feedback and AI feedback to
provide better supervision signals for supervised contrastive learning of
sentence embeddings. Experimental results show that our method achieves
state-of-the-art performance on several semantic textual similarity (STS) and
transfer learning tasks compared to other unsupervised and supervised
contrastive learning methods.",None,-1
75065f06-6ba7-405a-92ff-76a6ce7a3246,SEPAL: Spatial Gene Expression Prediction from Local Graphs,0.219333,3,"Spatial transcriptomics is an emerging technology that aligns histopathology
images with spatially resolved gene expression profiling. It holds the
potential for understanding many diseases but faces significant bottlenecks
such as specialized equipment and domain expertise. In this work, we present
SEPAL, a new model for predicting genetic profiles from visual tissue
appearance. Our method exploits the biological biases of the problem by
directly supervising relative differences with respect to mean expression, and
leverages local visual context at every coordinate to make predictions using a
graph neural network. This approach closes the gap between complete locality
and complete globality in current methods. In addition, we propose a novel
benchmark that aims to better define the task by following current best
practices in transcriptomics and restricting the prediction variables to only
those with clear spatial patterns. Our extensive evaluation in two different
human breast cancer datasets indicates that SEPAL outperforms previous
state-of-the-art methods and other mechanisms of including spatial context.",None,-1
17bea033-fbc9-47d9-93c8-4b59c3318576,SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme Classification,0.337472,2,"Extreme classification (XC) involves predicting over large numbers of classes
(thousands to millions), with real-world applications like news article
classification and e-commerce product tagging. The zero-shot version of this
task requires generalization to novel classes without additional supervision.
In this paper, we develop SemSup-XC, a model that achieves state-of-the-art
zero-shot and few-shot performance on three XC datasets derived from legal,
e-commerce, and Wikipedia data. To develop SemSup-XC, we use automatically
collected semantic class descriptions to represent classes and facilitate
generalization through a novel hybrid matching module that matches input
instances to class descriptions using a combination of semantic and lexical
similarity. Trained with contrastive learning, SemSup-XC significantly
outperforms baselines and establishes state-of-the-art performance on all three
datasets considered, gaining up to 12 precision points on zero-shot and more
than 10 precision points on one-shot tests, with similar gains for recall@10.
Our ablation studies highlight the relative importance of our hybrid matching
module and automatically collected class descriptions.",None,-1
9f23eb9d-d6f6-490d-b5c5-0629a418716f,REST: Retrieval-Based Speculative Decoding,0.663257,24,"We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm
designed to speed up language model generation. The key insight driving the
development of REST is the observation that the process of text generation
often includes certain common phases and patterns. Unlike previous methods that
rely on a draft language model for speculative decoding, REST harnesses the
power of retrieval to generate draft tokens. This method draws from the
reservoir of existing knowledge, retrieving and employing relevant tokens based
on the current context. Its plug-and-play nature allows for seamless
integration and acceleration of any language models, all without necessitating
additional training. When benchmarked on 7B and 13B language models in a
single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on
code or text generation. The code of REST is available at
https://github.com/FasterDecoding/REST.",None,-1
1def7410-770f-4aef-96e2-50973d33524f,Focalized Contrastive View-invariant Learning for Self-supervised Skeleton-based Action Recognition,0.830364,9,"Learning view-invariant representation is a key to improving feature
discrimination power for skeleton-based action recognition. Existing approaches
cannot effectively remove the impact of viewpoint due to the implicit
view-dependent representations. In this work, we propose a self-supervised
framework called Focalized Contrastive View-invariant Learning (FoCoViL), which
significantly suppresses the view-specific information on the representation
space where the viewpoints are coarsely aligned. By maximizing mutual
information with an effective contrastive loss between multi-view sample pairs,
FoCoViL associates actions with common view-invariant properties and
simultaneously separates the dissimilar ones. We further propose an adaptive
focalization method based on pairwise similarity to enhance contrastive
learning for a clearer cluster boundary in the learned space. Different from
many existing self-supervised representation learning work that rely heavily on
supervised classifiers, FoCoViL performs well on both unsupervised and
supervised classifiers with superior recognition performance. Extensive
experiments also show that the proposed contrastive-based focalization
generates a more discriminative latent representation.",None,-1
23bead00-22a2-4e3e-a57a-5f9eca32f0e4,Disentangled Phonetic Representation for Chinese Spelling Correction,0.920234,5,"Chinese Spelling Correction (CSC) aims to detect and correct erroneous
characters in Chinese texts. Although efforts have been made to introduce
phonetic information (Hanyu Pinyin) in this task, they typically merge phonetic
representations with character representations, which tends to weaken the
representation effect of normal texts. In this work, we propose to disentangle
the two types of features to allow for direct interaction between textual and
phonetic information. To learn useful phonetic representations, we introduce a
pinyin-to-character objective to ask the model to predict the correct
characters based solely on phonetic information, where a separation mask is
imposed to disable attention from phonetic input to text. To avoid overfitting
the phonetics, we further design a self-distillation module to ensure that
semantic information plays a major role in the prediction. Extensive
experiments on three CSC benchmarks demonstrate the superiority of our method
in using phonetic information.",None,-1
3dd0c468-03d3-4599-b408-577a0faf19b2,Adaptive Spot-Guided Transformer for Consistent Local Feature Matching,0.636494,11,"Local feature matching aims at finding correspondences between a pair of
images. Although current detector-free methods leverage Transformer
architecture to obtain an impressive performance, few works consider
maintaining local consistency. Meanwhile, most methods struggle with large
scale variations. To deal with the above issues, we propose Adaptive
Spot-Guided Transformer (ASTR) for local feature matching, which jointly models
the local consistency and scale variations in a unified coarse-to-fine
architecture. The proposed ASTR enjoys several merits. First, we design a
spot-guided aggregation module to avoid interfering with irrelevant areas
during feature aggregation. Second, we design an adaptive scaling module to
adjust the size of grids according to the calculated depth information at fine
stage. Extensive experimental results on five standard benchmarks demonstrate
that our ASTR performs favorably against state-of-the-art methods. Our code
will be released on https://astr2023.github.io.",None,-1
99b081f5-4a03-4d1a-ac9e-f09631548272,Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness,0.573949,16,"Learning from raw high dimensional data via interaction with a given
environment has been effectively achieved through the utilization of deep
neural networks. Yet the observed degradation in policy performance caused by
imperceptible worst-case policy dependent translations along high sensitivity
directions (i.e. adversarial perturbations) raises concerns on the robustness
of deep reinforcement learning policies. In our paper, we show that these high
sensitivity directions do not lie only along particular worst-case directions,
but rather are more abundant in the deep neural policy landscape and can be
found via more natural means in a black-box setting. Furthermore, we show that
vanilla training techniques intriguingly result in learning more robust
policies compared to the policies learnt via the state-of-the-art adversarial
training techniques. We believe our work lays out intriguing properties of the
deep reinforcement learning policy manifold and our results can help to build
robust and generalizable deep reinforcement learning policies.",None,-1
aa9e68b3-6447-43e3-99a3-f6da54716e81,Compressing Context to Enhance Inference Efficiency of Large Language Models,0.142808,14,"Large language models (LLMs) achieved remarkable performance across various
tasks. However, they face challenges in managing long documents and extended
conversations, due to significantly increased computational requirements, both
in memory and inference time, and potential context truncation when the input
exceeds the LLM's fixed context length. This paper proposes a method called
Selective Context that enhances the inference efficiency of LLMs by identifying
and pruning redundancy in the input context to make the input more compact. We
test our approach using common data sources requiring long context processing:
arXiv papers, news articles, and long conversations, on tasks of summarisation,
question answering, and response generation. Experimental results show that
Selective Context significantly reduces memory cost and decreases generation
latency while maintaining comparable performance compared to that achieved when
full context is used. Specifically, we achieve a 50\% reduction in context
cost, resulting in a 36\% reduction in inference memory usage and a 32\%
reduction in inference time, while observing only a minor drop of .023 in
BERTscore and .038 in faithfulness on four downstream applications, indicating
that our method strikes a good balance between efficiency and performance.",None,-1
0af68721-586e-4689-b292-546cdb234c64,Summarizing Strategy Card Game AI Competition,0.493273,3,"This paper concludes five years of AI competitions based on Legends of Code
and Magic (LOCM), a small Collectible Card Game (CCG), designed with the goal
of supporting research and algorithm development. The game was used in a number
of events, including Community Contests on the CodinGame platform, and Strategy
Card Game AI Competition at the IEEE Congress on Evolutionary Computation and
IEEE Conference on Games. LOCM has been used in a number of publications
related to areas such as game tree search algorithms, neural networks,
evaluation functions, and CCG deckbuilding. We present the rules of the game,
the history of organized competitions, and a listing of the participant and
their approaches, as well as some general advice on organizing AI competitions
for the research community. Although the COG 2022 edition was announced to be
the last one, the game remains available and can be played using an online
leaderboard arena.",None,-1
08fb4661-bed9-49fd-8d30-629bc47c9ddd,Alzheimer Disease Classification through ASR-based Transcriptions: Exploring the Impact of Punctuation and Pauses,0.175283,1,"Alzheimer's Disease (AD) is the world's leading neurodegenerative disease,
which often results in communication difficulties. Analysing speech can serve
as a diagnostic tool for identifying the condition. The recent ADReSS challenge
provided a dataset for AD classification and highlighted the utility of manual
transcriptions. In this study, we used the new state-of-the-art Automatic
Speech Recognition (ASR) model Whisper to obtain the transcriptions, which also
include automatic punctuation. The classification models achieved test accuracy
scores of 0.854 and 0.833 combining the pretrained FastText word embeddings and
recurrent neural networks on manual and ASR transcripts respectively.
Additionally, we explored the influence of including pause information and
punctuation in the transcriptions. We found that punctuation only yielded minor
improvements in some cases, whereas pause encoding aided AD classification for
both manual and ASR transcriptions across all approaches investigated.",None,-1
ec57ba11-6878-412b-a72d-d73d4ff30bee,A Comparative Analysis of Techniques and Algorithms for Recognising Sign Language,0.414441,2,"Sign language is a visual language that enhances communication between people
and is frequently used as the primary form of communication by people with
hearing loss. Even so, not many people with hearing loss use sign language, and
they frequently experience social isolation. Therefore, it is necessary to
create human-computer interface systems that can offer hearing-impaired people
a social platform. Most commercial sign language translation systems now on the
market are sensor-based, pricey, and challenging to use. Although vision-based
systems are desperately needed, they must first overcome several challenges.
Earlier continuous sign language recognition techniques used hidden Markov
models, which have a limited ability to include temporal information. To get
over these restrictions, several machine learning approaches are being applied
to transform hand and sign language motions into spoken or written language. In
this study, we compare various deep learning techniques for recognising sign
language. Our survey aims to provide a comprehensive overview of the most
recent approaches and challenges in this field.",None,-1
247eb246-a5d2-435a-a630-dd84f3654d4b,OPT: One-shot Pose-Controllable Talking Head Generation,0.400356,4,"One-shot talking head generation produces lip-sync talking heads based on
arbitrary audio and one source face. To guarantee the naturalness and realness,
recent methods propose to achieve free pose control instead of simply editing
mouth areas. However, existing methods do not preserve accurate identity of
source face when generating head motions. To solve the identity mismatch
problem and achieve high-quality free pose control, we present One-shot
Pose-controllable Talking head generation network (OPT). Specifically, the
Audio Feature Disentanglement Module separates content features from audios,
eliminating the influence of speaker-specific information contained in
arbitrary driving audios. Later, the mouth expression feature is extracted from
the content feature and source face, during which the landmark loss is designed
to enhance the accuracy of facial structure and identity preserving quality.
Finally, to achieve free pose control, controllable head pose features from
reference videos are fed into the Video Generator along with the expression
feature and source face to generate new talking heads. Extensive quantitative
and qualitative experimental results verify that OPT generates high-quality
pose-controllable talking heads with no identity mismatch problem,
outperforming previous SOTA methods.",None,-1
c3a233b8-6cdc-4f37-b318-7563381bdc9e,Probabilistic Generative Modeling for Procedural Roundabout Generation for Developing Countries,0.414849,1,"Due to limited resources and fast economic growth, designing optimal
transportation road networks with traffic simulation and validation in a
cost-effective manner is vital for developing countries, where extensive manual
testing is expensive and often infeasible. Current rule-based road design
generators lack diversity, a key feature for design robustness. Generative Flow
Networks (GFlowNets) learn stochastic policies to sample from an unnormalized
reward distribution, thus generating high-quality solutions while preserving
their diversity. In this work, we formulate the problem of linking incident
roads to the circular junction of a roundabout by a Markov decision process,
and we leverage GFlowNets as the Junction-Art road generator. We compare our
method with related methods and our empirical results show that our method
achieves better diversity while preserving a high validity score.",None,-1
73008430-283c-4a7c-8edd-7546101e3d76,ChatGPT for Robotics: Design Principles and Model Abilities,0.999992,282,"This paper presents an experimental study regarding the use of OpenAI's
ChatGPT for robotics applications. We outline a strategy that combines design
principles for prompt engineering and the creation of a high-level function
library which allows ChatGPT to adapt to different robotics tasks, simulators,
and form factors. We focus our evaluations on the effectiveness of different
prompt engineering techniques and dialog strategies towards the execution of
various types of robotics tasks. We explore ChatGPT's ability to use free-form
dialog, parse XML tags, and to synthesize code, in addition to the use of
task-specific prompting functions and closed-loop reasoning through dialogues.
Our study encompasses a range of tasks within the robotics domain, from basic
logical, geometrical, and mathematical reasoning all the way to complex domains
such as aerial navigation, manipulation, and embodied agents. We show that
ChatGPT can be effective at solving several of such tasks, while allowing users
to interact with it primarily via natural language instructions. In addition to
these studies, we introduce an open-sourced research tool called PromptCraft,
which contains a platform where researchers can collaboratively upload and vote
on examples of good prompting schemes for robotics applications, as well as a
sample robotics simulator with ChatGPT integration, making it easier for users
to get started with using ChatGPT for robotics.",None,-1
33251d88-aa35-424f-8444-6b3a30fb9dd9,Sketch-an-Anchor: Sub-epoch Fast Model Adaptation for Zero-shot Sketch-based Image Retrieval,0.0514094,1,"Sketch-an-Anchor is a novel method to train state-of-the-art Zero-shot
Sketch-based Image Retrieval (ZSSBIR) models in under an epoch. Most studies
break down the problem of ZSSBIR into two parts: domain alignment between
images and sketches, inherited from SBIR, and generalization to unseen data,
inherent to the zero-shot protocol. We argue one of these problems can be
considerably simplified and re-frame the ZSSBIR problem around the
already-stellar yet underexplored Zero-shot Image-based Retrieval performance
of off-the-shelf models. Our fast-converging model keeps the single-domain
performance while learning to extract similar representations from sketches. To
this end we introduce our Semantic Anchors -- guiding embeddings learned from
word-based semantic spaces and features from off-the-shelf models -- and
combine them with our novel Anchored Contrastive Loss. Empirical evidence shows
we can achieve state-of-the-art performance on all benchmark datasets while
training for 100x less iterations than other methods.",None,-1
b3dfc184-be80-4d26-af17-aa36be3d420c,RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions,0.67988,49,"Generative AI models have shown impressive ability to produce images with
text prompts, which could benefit creativity in visual art creation and
self-expression. However, it is unclear how precisely the generated images
express contexts and emotions from the input texts. We explored the emotional
expressiveness of AI-generated images and developed RePrompt, an automatic
method to refine text prompts toward precise expression of the generated
images. Inspired by crowdsourced editing strategies, we curated intuitive text
features, such as the number and concreteness of nouns, and trained a proxy
model to analyze the feature effects on the AI-generated image. With model
explanations of the proxy model, we curated a rubric to adjust text prompts to
optimize image generation for precise emotion expression. We conducted
simulation and user studies, which showed that RePrompt significantly improves
the emotional expressiveness of AI-generated images, especially for negative
emotions.",None,-1
086b3d0d-6aff-4027-8bf9-5d7266b922a9,"AutoML in The Wild: Obstacles, Workarounds, and Expectations",0.343838,6,"Automated machine learning (AutoML) is envisioned to make ML techniques
accessible to ordinary users. Recent work has investigated the role of humans
in enhancing AutoML functionality throughout a standard ML workflow. However,
it is also critical to understand how users adopt existing AutoML solutions in
complex, real-world settings from a holistic perspective. To fill this gap,
this study conducted semi-structured interviews of AutoML users (N=19) focusing
on understanding (1) the limitations of AutoML encountered by users in their
real-world practices, (2) the strategies users adopt to cope with such
limitations, and (3) how the limitations and workarounds impact their use of
AutoML. Our findings reveal that users actively exercise user agency to
overcome three major challenges arising from customizability, transparency, and
privacy. Furthermore, users make cautious decisions about whether and how to
apply AutoML on a case-by-case basis. Finally, we derive design implications
for developing future AutoML solutions.",None,-1
0d88eb1d-d80b-4ba4-bb86-3b70dc3a80c0,Make a Choice! Knowledge Base Question Answering with In-Context Learning,0.619981,9,"Question answering over knowledge bases (KBQA) aims to answer factoid
questions with a given knowledge base (KB). Due to the large scale of KB,
annotated data is impossible to cover all fact schemas in KB, which poses a
challenge to the generalization ability of methods that require a sufficient
amount of annotated data. Recently, LLMs have shown strong few-shot performance
in many NLP tasks. We expect LLM can help existing methods improve their
generalization ability, especially in low-resource situations. In this paper,
we present McL-KBQA, a framework that incorporates the few-shot ability of LLM
into the KBQA method via ICL-based multiple choice and then improves the
effectiveness of the QA tasks. Experimental results on two KBQA datasets
demonstrate the competitive performance of McL-KBQA with strong improvements in
generalization. We expect to explore a new way to QA tasks from KBQA in
conjunction with LLM, how to generate answers normatively and correctly with
strong generalization.",None,-1
13bd8223-b00d-49f8-af8d-7ee3a9e11a6a,When Prompt-based Incremental Learning Does Not Meet Strong Pretraining,0.658794,8,"Incremental learning aims to overcome catastrophic forgetting when learning
deep networks from sequential tasks. With impressive learning efficiency and
performance, prompt-based methods adopt a fixed backbone to sequential tasks by
learning task-specific prompts. However, existing prompt-based methods heavily
rely on strong pretraining (typically trained on ImageNet-21k), and we find
that their models could be trapped if the potential gap between the pretraining
task and unknown future tasks is large. In this work, we develop a learnable
Adaptive Prompt Generator (APG). The key is to unify the prompt retrieval and
prompt learning processes into a learnable prompt generator. Hence, the whole
prompting process can be optimized to reduce the negative effects of the gap
between tasks effectively. To make our APG avoid learning ineffective
knowledge, we maintain a knowledge pool to regularize APG with the feature
distribution of each class. Extensive experiments show that our method
significantly outperforms advanced methods in exemplar-free incremental
learning without (strong) pretraining. Besides, under strong retraining, our
method also has comparable performance to existing prompt-based models, showing
that our method can still benefit from pretraining. Codes can be found at
https://github.com/TOM-tym/APG",None,-1
0793908c-fcb9-4d46-8919-913a094f285c,EdgeFace: Efficient Face Recognition Model for Edge Devices,0.722207,7,"In this paper, we present EdgeFace, a lightweight and efficient face
recognition network inspired by the hybrid architecture of EdgeNeXt. By
effectively combining the strengths of both CNN and Transformer models, and a
low rank linear layer, EdgeFace achieves excellent face recognition performance
optimized for edge devices. The proposed EdgeFace network not only maintains
low computational costs and compact storage, but also achieves high face
recognition accuracy, making it suitable for deployment on edge devices.
Extensive experiments on challenging benchmark face datasets demonstrate the
effectiveness and efficiency of EdgeFace in comparison to state-of-the-art
lightweight models and deep face recognition models. Our EdgeFace model with
1.77M parameters achieves state of the art results on LFW (99.73%), IJB-B
(92.67%), and IJB-C (94.85%), outperforming other efficient models with larger
computational complexities. The code to replicate the experiments will be made
available publicly.",None,-1
82825930-93fc-43e0-aeb0-6777531e474e,Continual Learning with Dirichlet Generative-based Rehearsal,0.502339,3,"Recent advancements in data-driven task-oriented dialogue systems (ToDs)
struggle with incremental learning due to computational constraints and
time-consuming issues. Continual Learning (CL) attempts to solve this by
avoiding intensive pre-training, but it faces the problem of catastrophic
forgetting (CF). While generative-based rehearsal CL methods have made
significant strides, generating pseudo samples that accurately reflect the
underlying task-specific distribution is still a challenge. In this paper, we
present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal
strategy for CL. Unlike the traditionally used Gaussian latent variable in the
Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and
versatility of the Dirichlet distribution to model the latent prior variable.
This enables it to efficiently capture sentence-level features of previous
tasks and effectively guide the generation of pseudo samples. In addition, we
introduce Jensen-Shannon Knowledge Distillation (JSKD), a robust logit-based
knowledge distillation method that enhances knowledge transfer during pseudo
sample generation. Our experiments confirm the efficacy of our approach in both
intent detection and slot-filling tasks, outperforming state-of-the-art
methods.",None,-1
47c670f3-b791-48c0-a166-8b162ff41393,Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks,0.632121,2,"In this paper, a new deep-learning architecture for solving the non-linear
Falkner-Skan equation is proposed. Using Legendre and Chebyshev neural blocks,
this approach shows how orthogonal polynomials can be used in neural networks
to increase the approximation capability of artificial neural networks. In
addition, utilizing the mathematical properties of these functions, we overcome
the computational complexity of the backpropagation algorithm by using the
operational matrices of the derivative. The efficiency of the proposed method
is carried out by simulating various configurations of the Falkner-Skan
equation.",None,-1
bffe7689-7e64-406d-8a4e-1e062137d9a9,ChatGPT-Powered Hierarchical Comparisons for Image Classification,0.433263,7,"The zero-shot open-vocabulary challenge in image classification is tackled by
pretrained vision-language models like CLIP, which benefit from incorporating
class-specific knowledge from large language models (LLMs) like ChatGPT.
However, biases in CLIP lead to similar descriptions for distinct but related
classes, prompting our novel image classification framework via hierarchical
comparisons: using LLMs to recursively group classes into hierarchies and
classifying images by comparing image-text embeddings at each hierarchy level,
resulting in an intuitive, effective, and explainable approach.",None,-1
6e7375b4-b9c3-4307-96ec-f403f7cec6a3,Context-Aware Selective Label Smoothing for Calibrating Sequence Recognition Model,0.210739,6,"Despite the success of deep neural network (DNN) on sequential data (i.e.,
scene text and speech) recognition, it suffers from the over-confidence problem
mainly due to overfitting in training with the cross-entropy loss, which may
make the decision-making less reliable. Confidence calibration has been
recently proposed as one effective solution to this problem. Nevertheless, the
majority of existing confidence calibration methods aims at non-sequential
data, which is limited if directly applied to sequential data since the
intrinsic contextual dependency in sequences or the class-specific statistical
prior is seldom exploited. To the end, we propose a Context-Aware Selective
Label Smoothing (CASLS) method for calibrating sequential data. The proposed
CASLS fully leverages the contextual dependency in sequences to construct
confusion matrices of contextual prediction statistics over different classes.
Class-specific error rates are then used to adjust the weights of smoothing
strength in order to achieve adaptive calibration. Experimental results on
sequence recognition tasks, including scene text recognition and speech
recognition, demonstrate that our method can achieve the state-of-the-art
performance.",None,-1
fb0bdbe2-95c7-4428-89cf-b317fb432f93,An Empirical Analysis of Range for 3D Object Detection,0.755755,7,"LiDAR-based 3D detection plays a vital role in autonomous navigation.
Surprisingly, although autonomous vehicles (AVs) must detect both near-field
objects (for collision avoidance) and far-field objects (for longer-term
planning), contemporary benchmarks focus only on near-field 3D detection.
However, AVs must detect far-field objects for safe navigation. In this paper,
we present an empirical analysis of far-field 3D detection using the long-range
detection dataset Argoverse 2.0 to better understand the problem, and share the
following insight: near-field LiDAR measurements are dense and optimally
encoded by small voxels, while far-field measurements are sparse and are better
encoded with large voxels. We exploit this observation to build a collection of
range experts tuned for near-vs-far field detection, and propose simple
techniques to efficiently ensemble models for long-range detection that improve
efficiency by 33% and boost accuracy by 3.2% CDS.",None,-1
2173792c-1a69-4e23-9b6e-baa4d8ae180d,Exploring Challenges and Opportunities to Support Designers in Learning to Co-create with AI-based Manufacturing Design Tools,0.9895,24,"AI-based design tools are proliferating in professional software to assist
engineering and industrial designers in complex manufacturing and design tasks.
These tools take on more agentic roles than traditional computer-aided design
tools and are often portrayed as ""co-creators."" Yet, working effectively with
such systems requires different skills than working with complex CAD tools
alone. To date, we know little about how engineering designers learn to work
with AI-based design tools. In this study, we observed trained designers as
they learned to work with two AI-based tools on a realistic design task. We
find that designers face many challenges in learning to effectively co-create
with current systems, including challenges in understanding and adjusting AI
outputs and in communicating their design goals. Based on our findings, we
highlight several design opportunities to better support designer-AI
co-creation.",None,-1
35137f42-dc42-432d-9b15-71e0e862f090,UNICORN: A Unified Backdoor Trigger Inversion Framework,0.765972,27,"The backdoor attack, where the adversary uses inputs stamped with triggers
(e.g., a patch) to activate pre-planted malicious behaviors, is a severe threat
to Deep Neural Network (DNN) models. Trigger inversion is an effective way of
identifying backdoor models and understanding embedded adversarial behaviors. A
challenge of trigger inversion is that there are many ways of constructing the
trigger. Existing methods cannot generalize to various types of triggers by
making certain assumptions or attack-specific constraints. The fundamental
reason is that existing work does not consider the trigger's design space in
their formulation of the inversion problem. This work formally defines and
analyzes the triggers injected in different spaces and the inversion problem.
Then, it proposes a unified framework to invert backdoor triggers based on the
formalization of triggers and the identified inner behaviors of backdoor models
from our analysis. Our prototype UNICORN is general and effective in inverting
backdoor triggers in DNNs. The code can be found at
https://github.com/RU-System-Software-and-Security/UNICORN.",None,-1
f49ddcf5-b519-407a-922e-d6fb7fcd56ff,Enhancing Psychological Counseling with Large Language Model: A Multifaceted Decision-Support System for Non-Professionals,0.998751,19,"In the contemporary landscape of social media, an alarming number of users
express negative emotions, some of which manifest as strong suicidal
intentions. This situation underscores a profound need for trained
psychological counselors who can enact effective mental interventions. However,
the development of these professionals is often an imperative but
time-consuming task. Consequently, the mobilization of non-professionals or
volunteers in this capacity emerges as a pressing concern. Leveraging the
capabilities of artificial intelligence, and in particular, the recent advances
in large language models, offers a viable solution to this challenge. This
paper introduces a novel model constructed on the foundation of large language
models to fully assist non-professionals in providing psychological
interventions on online user discourses. This framework makes it plausible to
harness the power of non-professional counselors in a meaningful way. A
comprehensive study was conducted involving ten professional psychological
counselors of varying expertise, evaluating the system across five critical
dimensions. The findings affirm that our system is capable of analyzing
patients' issues with relative accuracy and proffering professional-level
strategies recommendations, thereby enhancing support for non-professionals.
This research serves as a compelling validation of the application of large
language models in the field of psychology and lays the groundwork for a new
paradigm of community-based mental health support.",None,-1
bb60c3a8-f9c9-429d-afc0-6ada55c6e326,GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds,0.757016,18,"We study the problem of 3D semantic segmentation from raw point clouds.
Unlike existing methods which primarily rely on a large amount of human
annotations for training neural networks, we propose the first purely
unsupervised method, called GrowSP, to successfully identify complex semantic
classes for every point in 3D scenes, without needing any type of human labels
or pretrained models. The key to our approach is to discover 3D semantic
elements via progressive growing of superpoints. Our method consists of three
major components, 1) the feature extractor to learn per-point features from
input point clouds, 2) the superpoint constructor to progressively grow the
sizes of superpoints, and 3) the semantic primitive clustering module to group
superpoints into semantic elements for the final semantic segmentation. We
extensively evaluate our method on multiple datasets, demonstrating superior
performance over all unsupervised baselines and approaching the classic
fully-supervised PointNet. We hope our work could inspire more advanced methods
for unsupervised 3D semantic learning.",None,-1
bc097e55-e0ee-4e00-9147-bd92738cb7ba,SemHint-MD: Learning from Noisy Semantic Labels for Self-Supervised Monocular Depth Estimation,0.186522,1,"Without ground truth supervision, self-supervised depth estimation can be
trapped in a local minimum due to the gradient-locality issue of the
photometric loss. In this paper, we present a framework to enhance depth by
leveraging semantic segmentation to guide the network to jump out of the local
minimum. Prior works have proposed to share encoders between these two tasks or
explicitly align them based on priors like the consistency between edges in the
depth and segmentation maps. Yet, these methods usually require ground truth or
high-quality pseudo labels, which may not be easily accessible in real-world
applications. In contrast, we investigate self-supervised depth estimation
along with a segmentation branch that is supervised with noisy labels provided
by models pre-trained with limited data. We extend parameter sharing from the
encoder to the decoder and study the influence of different numbers of shared
decoder parameters on model performance. Also, we propose to use cross-task
information to refine current depth and segmentation predictions to generate
pseudo-depth and semantic labels for training. The advantages of the proposed
method are demonstrated through extensive experiments on the KITTI benchmark
and a downstream task for endoscopic tissue deformation tracking.",None,-1
2b99f852-7d83-4fd9-a544-be1961fed2b3,Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding,0.0353932,1,"End-to-end (E2E) spoken language understanding (SLU) systems that generate a
semantic parse from speech have become more promising recently. This approach
uses a single model that utilizes audio and text representations from
pre-trained speech recognition models (ASR), and outperforms traditional
pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems
still show weakness when text representation quality is low due to ASR
transcription errors. To overcome this issue, we propose a novel E2E SLU system
that enhances robustness to ASR errors by fusing audio and text representations
based on the estimated modality confidence of ASR hypotheses. We introduce two
novel techniques: 1) an effective method to encode the quality of ASR
hypotheses and 2) an effective approach to integrate them into E2E SLU models.
We show accuracy improvements on STOP dataset and share the analysis to
demonstrate the effectiveness of our approach.",None,-1
9eee17d1-5c94-4a7f-a44a-46fd7ffa9aa0,The 2nd Place Solution for 2023 Waymo Open Sim Agents Challenge,0.470094,3,"In this technical report, we present the 2nd place solution of 2023 Waymo
Open Sim Agents Challenge (WOSAC)[4]. We propose a simple yet effective
autoregressive method for simulating multi-agent behaviors, which is built upon
a well-known multimodal motion forecasting framework called Motion Transformer
(MTR)[5] with postprocessing algorithms applied. Our submission named MTR+++
achieves 0.4697 on the Realism Meta metric in 2023 WOSAC. Besides, a modified
model based on MTR named MTR_E is proposed after the challenge, which has a
better score 0.4911 and is ranked the 3rd on the leaderboard of WOSAC as of
June 25, 2023.",None,-1
952851af-1c1f-4332-a533-2366a52d1bd0,Addressing Variable Dependency in GNN-based SAT Solving,0.463687,2,"Boolean satisfiability problem (SAT) is fundamental to many applications.
Existing works have used graph neural networks (GNNs) for (approximate) SAT
solving. Typical GNN-based end-to-end SAT solvers predict SAT solutions
concurrently. We show that for a group of symmetric SAT problems, the
concurrent prediction is guaranteed to produce a wrong answer because it
neglects the dependency among Boolean variables in SAT problems. % We propose
AsymSAT, a GNN-based architecture which integrates recurrent neural networks to
generate dependent predictions for variable assignments. The experiment results
show that dependent variable prediction extends the solving capability of the
GNN-based method as it improves the number of solved SAT instances on large
test sets.",None,-1
423166f3-df15-412e-8b9f-06edb98f2cb8,MonoEdge: Monocular 3D Object Detection Using Local Perspectives,0.390041,5,"We propose a novel approach for monocular 3D object detection by leveraging
local perspective effects of each object. While the global perspective effect
shown as size and position variations has been exploited for monocular 3D
detection extensively, the local perspectives has long been overlooked. We
design a local perspective module to regress a newly defined variable named
keyedge-ratios as the parameterization of the local shape distortion to account
for the local perspective, and derive the object depth and yaw angle from it.
Theoretically, this module does not rely on the pixel-wise size or position in
the image of the objects, therefore independent of the camera intrinsic
parameters. By plugging this module in existing monocular 3D object detection
frameworks, we incorporate the local perspective distortion with global
perspective effect for monocular 3D reasoning, and we demonstrate the
effectiveness and superior performance over strong baseline methods in multiple
datasets.",None,-1
30400bea-90fc-40ee-a317-aeef07580f2e,Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,0.497749,3,"Cross-lingual semantic parsing transfers parsing capability from a
high-resource language (e.g., English) to low-resource languages with scarce
training data. Previous work has primarily considered silver-standard data
augmentation or zero-shot methods, however, exploiting few-shot gold data is
comparatively unexplored. We propose a new approach to cross-lingual semantic
parsing by explicitly minimizing cross-lingual divergence between probabilistic
latent variables using Optimal Transport. We demonstrate how this direct
guidance improves parsing from natural languages using fewer examples and less
training. We evaluate our method on two datasets, MTOP and MultiATIS++SQL,
establishing state-of-the-art results under a few-shot cross-lingual regime.
Ablation studies further reveal that our method improves performance even
without parallel input translations. In addition, we show that our model better
captures cross-lingual structure in the latent space to improve semantic
representation similarity.",None,-1
11250d78-3ef3-42c3-826b-43a33f9f907c,DiffuScene: Denoising Diffusion Models for Generative Indoor Scene Synthesis,0.32226,4,"We present DiffuScene for indoor 3D scene synthesis based on a novel scene
configuration denoising diffusion model. It generates 3D instance properties
stored in an unordered object set and retrieves the most similar geometry for
each object configuration, which is characterized as a concatenation of
different attributes, including location, size, orientation, semantics, and
geometry features. We introduce a diffusion network to synthesize a collection
of 3D indoor objects by denoising a set of unordered object attributes.
Unordered parametrization simplifies and eases the joint distribution
approximation. The shape feature diffusion facilitates natural object
placements, including symmetries. Our method enables many downstream
applications, including scene completion, scene arrangement, and
text-conditioned scene synthesis. Experiments on the 3D-FRONT dataset show that
our method can synthesize more physically plausible and diverse indoor scenes
than state-of-the-art methods. Extensive ablation studies verify the
effectiveness of our design choice in scene diffusion models.",None,-1
a1065af9-c7a5-49a2-b33e-106336394d45,NatCS: Eliciting Natural Customer Support Dialogues,0.0424852,1,"Despite growing interest in applications based on natural customer support
conversations, there exist remarkably few publicly available datasets that
reflect the expected characteristics of conversations in these settings.
Existing task-oriented dialogue datasets, which were collected to benchmark
dialogue systems mainly in written human-to-bot settings, are not
representative of real customer support conversations and do not provide
realistic benchmarks for systems that are applied to natural data. To address
this gap, we introduce NatCS, a multi-domain collection of spoken customer
service conversations. We describe our process for collecting synthetic
conversations between customers and agents based on natural language phenomena
observed in real conversations. Compared to previous dialogue datasets, the
conversations collected with our approach are more representative of real
human-to-human conversations along multiple metrics. Finally, we demonstrate
potential uses of NatCS, including dialogue act classification and intent
induction from conversations as potential applications, showing that dialogue
act annotations in NatCS provide more effective training data for modeling real
conversations compared to existing synthetic written datasets. We publicly
release NatCS to facilitate research in natural dialog systems",None,-1
0fbccdc0-1654-485c-b1ff-a15a21dd7ce8,KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models,0.427007,7,"Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches
in changing factual knowledge stored in the Language models. However, there is
a lack of research on whether present locating methods can pinpoint the exact
parameters embedding the desired knowledge. Moreover, although many researchers
have questioned the validity of locality hypothesis of factual knowledge, no
method is provided to test the a hypothesis for more in-depth discussion and
research. Therefore, we introduce KLoB, a benchmark examining three essential
properties that a reliable knowledge locating method should satisfy. KLoB can
serve as a benchmark for evaluating existing locating methods in language
models, and can contributes a method to reassessing the validity of locality
hypothesis of factual knowledge. Our is publicly available at
\url{https://github.com/juyiming/KLoB}.",None,-1
ee6aa693-97f5-41df-b571-8cee9100ed4e,Sign Language Translation from Instructional Videos,0.746216,16,"The advances in automatic sign language translation (SLT) to spoken languages
have been mostly benchmarked with datasets of limited size and restricted
domains. Our work advances the state of the art by providing the first baseline
results on How2Sign, a large and broad dataset.
  We train a Transformer over I3D video features, using the reduced BLEU as a
reference metric for validation, instead of the widely used BLEU score. We
report a result of 8.03 on the BLEU score, and publish the first open-source
implementation of its kind to promote further advances.",None,-1
9bffc9c8-074c-4d69-99f8-56f1bc80eaf8,Injecting Logical Constraints into Neural Networks via Straight-Through Estimators,0.986575,14,"Injecting discrete logical constraints into neural network learning is one of
the main challenges in neuro-symbolic AI. We find that a
straight-through-estimator, a method introduced to train binary neural
networks, could effectively be applied to incorporate logical constraints into
neural network learning. More specifically, we design a systematic way to
represent discrete logical constraints as a loss function; minimizing this loss
using gradient descent via a straight-through-estimator updates the neural
network's weights in the direction that the binarized outputs satisfy the
logical constraints. The experimental results show that by leveraging GPUs and
batch training, this method scales significantly better than existing
neuro-symbolic methods that require heavy symbolic computation for computing
gradients. Also, we demonstrate that our method applies to different types of
neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer
labeled data by learning directly from known constraints.",None,-1
d2ff5376-ab6c-472c-9a24-ef6e27a248d1,Rethinking Precision of Pseudo Label: Test-Time Adaptation via Complementary Learning,0.571189,8,"In this work, we propose a novel complementary learning approach to enhance
test-time adaptation (TTA), which has been proven to exhibit good performance
on testing data with distribution shifts such as corruptions. In test-time
adaptation tasks, information from the source domain is typically unavailable
and the model has to be optimized without supervision for test-time samples.
Hence, usual methods assign labels for unannotated data with the prediction by
a well-trained source model in an unsupervised learning framework. Previous
studies have employed unsupervised objectives, such as the entropy of model
predictions, as optimization targets to effectively learn features for
test-time samples. However, the performance of the model is easily compromised
by the quality of pseudo-labels, since inaccuracies in pseudo-labels introduce
noise to the model. Therefore, we propose to leverage the ""less probable
categories"" to decrease the risk of incorrect pseudo-labeling. The
complementary label is introduced to designate these categories. We highlight
that the risk function of complementary labels agrees with their Vanilla loss
formula under the conventional true label distribution. Experiments show that
the proposed learning algorithm achieves state-of-the-art performance on
different datasets and experiment settings.",None,-1
1ea8d40c-3d0e-41bd-83ed-26973e2b4200,A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces,0.170465,4,"We study semantic construal in grammatical constructions using large language
models. First, we project contextual word embeddings into three interpretable
semantic spaces, each defined by a different set of psycholinguistic feature
norms. We validate these interpretable spaces and then use them to
automatically derive semantic characterizations of lexical items in two
grammatical constructions: nouns in subject or object position within the same
sentence, and the AANN construction (e.g., `a beautiful three days'). We show
that a word in subject position is interpreted as more agentive than the very
same word in object position, and that the nouns in the AANN construction are
interpreted as more measurement-like than when in the canonical alternation.
Our method can probe the distributional meaning of syntactic constructions at a
templatic level, abstracted away from specific lexemes.",None,-1
fbeab4f1-ba57-47ec-aef4-6c8e813357b6,Speech Driven Video Editing via an Audio-Conditioned Diffusion Model,0.587901,13,"Taking inspiration from recent developments in visual generative tasks using
diffusion models, we propose a method for end-to-end speech-driven video
editing using a denoising diffusion model. Given a video of a talking person,
and a separate auditory speech recording, the lip and jaw motions are
re-synchronized without relying on intermediate structural representations such
as facial landmarks or a 3D face model. We show this is possible by
conditioning a denoising diffusion model on audio mel spectral features to
generate synchronised facial motion. Proof of concept results are demonstrated
on both single-speaker and multi-speaker video editing, providing a baseline
model on the CREMA-D audiovisual data set. To the best of our knowledge, this
is the first work to demonstrate and validate the feasibility of applying
end-to-end denoising diffusion models to the task of audio-driven video
editing.",None,-1
a39b380c-fee3-499a-8453-d643c0dfadf7,Trusting Your Evidence: Hallucinate Less with Context-aware Decoding,0.912161,92,"Language models (LMs) often struggle to pay enough attention to the input
context, and generate texts that are unfaithful or contain hallucinations. To
mitigate this issue, we present context-aware decoding (CAD), which follows a
contrastive output distribution that amplifies the difference between the
output probabilities when a model is used with and without context. Our
experiments show that CAD, without additional training, significantly improves
the faithfulness of different LM families, including OPT, GPT, LLaMA and
FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality
metrics). Furthermore, CAD is particularly effective in overriding a model's
prior knowledge when it contradicts the provided context, leading to
substantial improvements in tasks where resolving the knowledge conflict is
essential.",None,-1
f8e21502-d620-4f34-afd3-11257e47d17b,Communication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter,0.180921,3,"Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as a
promising paradigm for institutions with limited language resources. This
approach allows multiple institutions to act as clients and train a unified
model through model synchronization, rather than collecting sensitive data for
centralized training. This significantly reduces the cost of corpus collection
and preserves data privacy. However, as pre-trained language models (PLMs)
continue to increase in size, the communication cost for transmitting
parameters during synchronization has become a training speed bottleneck. In
this paper, we propose a communication-efficient Fed-MNMT framework that
addresses this issue by keeping PLMs frozen and only transferring lightweight
adapter modules between clients. Since different language pairs exhibit
substantial discrepancies in data distributions, adapter parameters of clients
may conflict with each other. To tackle this, we explore various clustering
strategies to group parameters for integration and mitigate the negative
effects of conflicting parameters. Experimental results demonstrate that our
framework reduces communication cost by over 98% while achieving similar or
even better performance compared to competitive baselines. Further analysis
reveals that clustering strategies effectively solve the problem of linguistic
discrepancy and pruning adapter modules further improves communication
efficiency.",None,-1
0bff5e12-7538-417d-9d9a-03abc30ccc02,X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation,0.482172,4,"An important component of human analysis of medical images and their context
is the ability to relate newly seen things to related instances in our memory.
In this paper we mimic this ability by using multi-modal retrieval augmentation
and apply it to several tasks in chest X-ray analysis. By retrieving similar
images and/or radiology reports we expand and regularize the case at hand with
additional knowledge, while maintaining factual knowledge consistency. The
method consists of two components. First, vision and language modalities are
aligned using a pre-trained CLIP model. To enforce that the retrieval focus
will be on detailed disease-related content instead of global visual appearance
it is fine-tuned using disease class information. Subsequently, we construct a
non-parametric retrieval index, which reaches state-of-the-art retrieval
levels. We use this index in our downstream tasks to augment image
representations through multi-head attention for disease classification and
report retrieval. We show that retrieval augmentation gives considerable
improvements on these tasks. Our downstream report retrieval even shows to be
competitive with dedicated report generation methods, paving the path for this
method in medical imaging.",None,-1
28d09d72-210f-437b-8b80-545e9055222a,Hybrid Open-set Segmentation with Synthetic Negative Data,0.106281,3,"Open-set segmentation can be conceived by complementing closed-set
classification with anomaly detection. Many of the existing dense anomaly
detectors operate through generative modelling of regular data or by
discriminating with respect to negative data. These two approaches optimize
different objectives and therefore exhibit different failure modes.
Consequently, we propose a novel anomaly score that fuses generative and
discriminative cues. Our score can be implemented by upgrading any closed-set
segmentation model with dense estimates of dataset posterior and unnormalized
data likelihood. The resulting dense hybrid open-set models require negative
training images that can be sampled from an auxiliary negative dataset, from a
jointly trained generative model, or from a mixture of both sources. We
evaluate our contributions on benchmarks for dense anomaly detection and
open-set segmentation. The experiments reveal strong open-set performance in
spite of negligible computational overhead.",None,-1
4426d839-cce2-4a51-8ca5-6a93399e2210,Calibrating Likelihoods towards Consistency in Summarization Models,0.123366,1,"Despite the recent advances in abstractive text summarization, current
summarization models still suffer from generating factually inconsistent
summaries, reducing their utility for real-world application. We argue that the
main reason for such behavior is that the summarization models trained with
maximum likelihood objective assign high probability to plausible sequences
given the context, but they often do not accurately rank sequences by their
consistency. In this work, we solve this problem by calibrating the likelihood
of model generated sequences to better align with a consistency metric measured
by natural language inference (NLI) models. The human evaluation study and
automatic metrics show that the calibrated models generate more consistent and
higher-quality summaries. We also show that the models trained using our method
return probabilities that are better aligned with the NLI scores, which
significantly increase reliability of summarization models.",None,-1
364ad91a-b9cc-4d70-8667-dc5aca85acd2,The WHY in Business Processes: Discovery of Causal Execution Dependencies,0.417993,2,"Unraveling the causal relationships among the execution of process activities
is a crucial element in predicting the consequences of process interventions
and making informed decisions regarding process improvements. Process discovery
algorithms exploit time precedence as their main source of model derivation.
Hence, a causal view can supplement process discovery, being a new perspective
in which relations reflect genuine cause-effect dependencies among the tasks.
This calls for faithful new techniques to discover the causal execution
dependencies among the tasks in the process. To this end, our work offers a
systematic approach to the unveiling of the causal business process by
leveraging an existing causal discovery algorithm over activity timing. In
addition, this work delves into a set of conditions under which process mining
discovery algorithms generate a model that is incongruent with the causal
business process model, and shows how the latter model can be methodologically
employed for a sound analysis of the process. Our methodology searches for such
discrepancies between the two models in the context of three causal patterns,
and derives a new view in which these inconsistencies are annotated over the
mined process model. We demonstrate our methodology employing two open process
mining algorithms, the IBM Process Mining tool, and the LiNGAM causal discovery
technique. We apply it on a synthesized dataset and on two open benchmark data
sets.",None,-1
29ec6f64-b098-46d8-8b1c-3d8b2f20089f,TopicGPT: A Prompt-based Topic Modeling Framework,0.817646,18,"Topic modeling is a well-established technique for exploring text corpora.
Conventional topic models (e.g., LDA) represent topics as bags of words that
often require ""reading the tea leaves"" to interpret; additionally, they offer
users minimal control over the formatting and specificity of resulting topics.
To tackle these issues, we introduce TopicGPT, a prompt-based framework that
uses large language models (LLMs) to uncover latent topics in a text
collection. TopicGPT produces topics that align better with human
categorizations compared to competing methods: it achieves a harmonic mean
purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for
the strongest baseline. Its topics are also interpretable, dispensing with
ambiguous bags of words in favor of topics with natural language labels and
associated free-form descriptions. Moreover, the framework is highly adaptable,
allowing users to specify constraints and modify topics without the need for
model retraining. By streamlining access to high-quality and interpretable
topics, TopicGPT represents a compelling, human-centered approach to topic
modeling.",None,-1
26964895-c532-4dd5-9a92-cb17f5a6db94,Shape of You: Precise 3D shape estimations for diverse body types,0.11275,1,"This paper presents Shape of You (SoY), an approach to improve the accuracy
of 3D body shape estimation for vision-based clothing recommendation systems.
While existing methods have successfully estimated 3D poses, there remains a
lack of work in precise shape estimation, particularly for diverse human
bodies. To address this gap, we propose two loss functions that can be readily
integrated into parametric 3D human reconstruction pipelines. Additionally, we
propose a test-time optimization routine that further improves quality. Our
method improves over the recent SHAPY method by 17.7% on the challenging SSP-3D
dataset. We consider our work to be a step towards a more accurate 3D shape
estimation system that works reliably on diverse body types and holds promise
for practical applications in the fashion industry.",None,-1
522765fe-8152-4fb1-bb46-ebda30812291,Benchmarking Probabilistic Deep Learning Methods for License Plate Recognition,0.774977,3,"Learning-based algorithms for automated license plate recognition implicitly
assume that the training and test data are well aligned. However, this may not
be the case under extreme environmental conditions, or in forensic applications
where the system cannot be trained for a specific acquisition device.
Predictions on such out-of-distribution images have an increased chance of
failing. But this failure case is oftentimes hard to recognize for a human
operator or an automated system. Hence, in this work we propose to model the
prediction uncertainty for license plate recognition explicitly. Such an
uncertainty measure allows to detect false predictions, indicating an analyst
when not to trust the result of the automated license plate recognition. In
this paper, we compare three methods for uncertainty quantification on two
architectures. The experiments on synthetic noisy or blurred low-resolution
images show that the predictive uncertainty reliably finds wrong predictions.
We also show that a multi-task combination of classification and
super-resolution improves the recognition performance by 109\% and the
detection of wrong predictions by 29 %.",None,-1
b81f34de-f080-4e74-ae2d-ee2589f41f1f,Analogue and Physical Reservoir Computing Using Water Waves,0.652678,8,"More than 3.5 billion people live in rural areas, where water and water
energy resources play an important role in ensuring sustainable and productive
rural economies. This article reviews and critically analyses the recent
advances in the field of analogue and reservoir computing that have been driven
by unique physical properties and energy of water waves. It also demonstrates
that analogue and reservoir computing hold the potential to bring artificial
intelligence closer to people living outside large cities, thus enabling them
to enjoy the benefits of novel technologies that already work in large cities
but are not readily available and suitable for regional communities.",None,-1
022b4555-5b81-4016-bc0f-f99216f10fa5,Toward Real-World Light Field Super-Resolution,0.560176,3,"Deep learning has opened up new possibilities for light field
super-resolution (SR), but existing methods trained on synthetic datasets with
simple degradations (e.g., bicubic downsampling) suffer from poor performance
when applied to complex real-world scenarios. To address this problem, we
introduce LytroZoom, the first real-world light field SR dataset capturing
paired low- and high-resolution light fields of diverse indoor and outdoor
scenes using a Lytro ILLUM camera. Additionally, we propose the Omni-Frequency
Projection Network (OFPNet), which decomposes the omni-frequency components and
iteratively enhances them through frequency projection operations to address
spatially variant degradation processes present in all frequency components.
Experiments demonstrate that models trained on LytroZoom outperform those
trained on synthetic datasets and are generalizable to diverse content and
devices. Quantitative and qualitative evaluations verify the superiority of
OFPNet. We believe this work will inspire future research in real-world light
field SR.",None,-1
82ff4590-42f8-4af2-b959-d13e01c29891,On the Benefits of 3D Pose and Tracking for Human Action Recognition,0.970457,16,"In this work we study the benefits of using tracking and 3D poses for action
recognition. To achieve this, we take the Lagrangian view on analysing actions
over a trajectory of human motion rather than at a fixed point in space. Taking
this stand allows us to use the tracklets of people to predict their actions.
In this spirit, first we show the benefits of using 3D pose to infer actions,
and study person-person interactions. Subsequently, we propose a Lagrangian
Action Recognition model by fusing 3D pose and contextualized appearance over
tracklets. To this end, our method achieves state-of-the-art performance on the
AVA v2.2 dataset on both pose only settings and on standard benchmark settings.
When reasoning about the action using only pose cues, our pose model achieves
+10.0 mAP gain over the corresponding state-of-the-art while our fused model
has a gain of +2.8 mAP over the best state-of-the-art model. Code and results
are available at: https://brjathu.github.io/LART",None,-1
760694d7-ea32-43ac-8d6d-8c4f91507aba,Monte-Carlo Tree Search for Multi-Agent Pathfinding: Preliminary Results,0.0940979,1,"In this work we study a well-known and challenging problem of Multi-agent
Pathfinding, when a set of agents is confined to a graph, each agent is
assigned a unique start and goal vertices and the task is to find a set of
collision-free paths (one for each agent) such that each agent reaches its
respective goal. We investigate how to utilize Monte-Carlo Tree Search (MCTS)
to solve the problem. Although MCTS was shown to demonstrate superior
performance in a wide range of problems like playing antagonistic games (e.g.
Go, Chess etc.), discovering faster matrix multiplication algorithms etc., its
application to the problem at hand was not well studied before. To this end we
introduce an original variant of MCTS, tailored to multi-agent pathfinding. The
crux of our approach is how the reward, that guides MCTS, is computed.
Specifically, we use individual paths to assist the agents with the the
goal-reaching behavior, while leaving them freedom to get off the track if it
is needed to avoid collisions. We also use a dedicated decomposition technique
to reduce the branching factor of the tree search procedure. Empirically we
show that the suggested method outperforms the baseline planning algorithm that
invokes heuristic search, e.g. A*, at each re-planning step.",None,-1
04075d7e-e0b2-4290-83c3-72d97f45735f,Large language models in medicine: the potentials and pitfalls,0.505043,15,"Large language models (LLMs) have been applied to tasks in healthcare,
ranging from medical exam questions to responding to patient questions. With
increasing institutional partnerships between companies producing LLMs and
healthcare systems, real world clinical application is coming closer to
reality. As these models gain traction, it is essential for healthcare
practitioners to understand what LLMs are, their development, their current and
potential applications, and the associated pitfalls when utilized in medicine.
This review and accompanying tutorial aim to give an overview of these topics
to aid healthcare practitioners in understanding the rapidly changing landscape
of LLMs as applied to medicine.",None,-1
30ab49ff-5ae7-4703-9d97-4733325517a1,Learning with Rejection for Abstractive Text Summarization,0.264664,5,"State-of-the-art abstractive summarization systems frequently hallucinate
content that is not supported by the source document, mainly due to noise in
the training dataset. Existing methods opt to drop the noisy samples or tokens
from the training set entirely, reducing the effective training set size and
creating an artificial propensity to copy words from the source. In this work,
we propose a training objective for abstractive summarization based on
rejection learning, in which the model learns whether or not to reject
potentially noisy tokens. We further propose a regularized decoding objective
that penalizes non-factual candidate summaries during inference by using the
rejection probability learned during training. We show that our method
considerably improves the factuality of generated summaries in automatic and
human evaluations when compared to five baseline models and that it does so
while increasing the abstractiveness of the generated summaries.",None,-1
9573c079-c1a9-4135-b4c8-032e86c03bc5,NN-Steiner: A Mixed Neural-algorithmic Approach for the Rectilinear Steiner Minimum Tree Problem,0.998851,2,"Recent years have witnessed rapid advances in the use of neural networks to
solve combinatorial optimization problems. Nevertheless, designing the ""right""
neural model that can effectively handle a given optimization problem can be
challenging, and often there is no theoretical understanding or justification
of the resulting neural model. In this paper, we focus on the rectilinear
Steiner minimum tree (RSMT) problem, which is of critical importance in IC
layout design and as a result has attracted numerous heuristic approaches in
the VLSI literature. Our contributions are two-fold. On the methodology front,
we propose NN-Steiner, which is a novel mixed neural-algorithmic framework for
computing RSMTs that leverages the celebrated PTAS algorithmic framework of
Arora to solve this problem (and other geometric optimization problems). Our
NN-Steiner replaces key algorithmic components within Arora's PTAS by suitable
neural components. In particular, NN-Steiner only needs four neural network
(NN) components that are called repeatedly within an algorithmic framework.
Crucially, each of the four NN components is only of bounded size independent
of input size, and thus easy to train. Furthermore, as the NN component is
learning a generic algorithmic step, once learned, the resulting mixed
neural-algorithmic framework generalizes to much larger instances not seen in
training. Our NN-Steiner, to our best knowledge, is the first neural
architecture of bounded size that has capacity to approximately solve RSMT (and
variants). On the empirical front, we show how NN-Steiner can be implemented
and demonstrate the effectiveness of our resulting approach, especially in
terms of generalization, by comparing with state-of-the-art methods (both
neural and non-neural based).",None,-1
d59d75b6-994d-4b1d-a1c2-9eb1d795fc4d,A Simple Explanation for the Phase Transition in Large Language Models with List Decoding,0.0212877,3,"Various recent experimental results show that large language models (LLM)
exhibit emergent abilities that are not present in small models. System
performance is greatly improved after passing a certain critical threshold of
scale. In this letter, we provide a simple explanation for such a phase
transition phenomenon. For this, we model an LLM as a sequence-to-sequence
random function. Instead of using instant generation at each step, we use a
list decoder that keeps a list of candidate sequences at each step and defers
the generation of the output sequence at the end. We show that there is a
critical threshold such that the expected number of erroneous candidate
sequences remains bounded when an LLM is below the threshold, and it grows
exponentially when an LLM is above the threshold. Such a threshold is related
to the basic reproduction number in a contagious disease.",None,-1
1a76ed6f-6378-4d3e-a661-a4f15aaa95cf,Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?,0.274587,1,"A hallmark of intelligence is the ability to use a familiar domain to make
inferences about a less familiar domain, known as analogical reasoning. In this
article, we delve into the performance of Large Language Models (LLMs) in
dealing with progressively complex analogies expressed in unstructured text. We
discuss analogies at four distinct levels of complexity: lexical analogies,
syntactic analogies, semantic analogies, and pragmatic analogies. As the
analogies become more complex, they require increasingly extensive, diverse
knowledge beyond the textual content, unlikely to be found in the lexical
co-occurrence statistics that power LLMs. To address this, we discuss the
necessity of employing Neuro-symbolic AI techniques that combine statistical
and symbolic AI, informing the representation of unstructured text to highlight
and augment relevant content, provide abstraction and guide the mapping
process. Our knowledge-informed approach maintains the efficiency of LLMs while
preserving the ability to explain analogies for pedagogical applications.",None,-1
73ceb786-f7dc-4139-8feb-ba7ac608f912,SVIT: Scaling up Visual Instruction Tuning,0.834127,67,"Thanks to the emerging of foundation models, the large language and vision
models are integrated to acquire the multimodal ability of visual captioning,
question answering, etc. Although existing multimodal models present impressive
performance of visual understanding and reasoning, their limits are still
largely under-explored due to the scarcity of high-quality instruction tuning
data. To push the limits of multimodal capability, we Scale up Visual
Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual
instruction tuning data including 1.6M conversation question-answer (QA) pairs,
1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed
image descriptions. Besides the volume, the proposed dataset is also featured
by the high quality and rich diversity, which is generated by prompting GPT-4
with the abundant manual annotations of images. We also propose a new data
recipe to select subset with better diversity and balance, which evokes model's
superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on
the proposed dataset, outperforms state-of-the-art Multimodal Large Language
Models on popular benchmarks. The data and code are publicly available at
https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.",None,-1
4cc57611-25a5-47e5-91a8-8b37671dde62,V1T: large-scale mouse V1 response prediction using a Vision Transformer,0.535673,4,"Accurate predictive models of the visual cortex neural response to natural
visual stimuli remain a challenge in computational neuroscience. In this work,
we introduce V1T, a novel Vision Transformer based architecture that learns a
shared visual and behavioral representation across animals. We evaluate our
model on two large datasets recorded from mouse primary visual cortex and
outperform previous convolution-based models by more than 12.7% in prediction
performance. Moreover, we show that the self-attention weights learned by the
Transformer correlate with the population receptive fields. Our model thus sets
a new benchmark for neural response prediction and can be used jointly with
behavioral and neural recordings to reveal meaningful characteristic features
of the visual cortex.",None,-1
43dc8c4a-e87d-4620-a711-1c1ee3ba87ab,Analysis of Recent Trends in Face Recognition Systems,0.406701,2,"With the tremendous advancements in face recognition technology, face
modality has been widely recognized as a significant biometric identifier in
establishing a person's identity rather than any other biometric trait like
fingerprints that require contact sensors. However, due to inter-class
similarities and intra-class variations, face recognition systems generate
false match and false non-match errors respectively. Recent research focuses on
improving the robustness of extracted features and the pre-processing
algorithms to enhance recognition accuracy. Since face recognition has been
extensively used for several applications ranging from law enforcement to
surveillance systems, the accuracy and performance of face recognition must be
the finest. In this paper various face recognition systems are discussed and
analysed like RPRV, LWKPCA, SVM Model, LTrP based SPM and a deep learning
framework for recognising images from CCTV. All these face recognition methods,
their implementations and performance evaluations are compared to derive the
best outcome for future developmental works.",None,-1
81b00e6a-90a1-4606-8b4f-80e9520905a0,Improved Diffusion-based Image Colorization via Piggybacked Models,0.591464,10,"Image colorization has been attracting the research interests of the
community for decades. However, existing methods still struggle to provide
satisfactory colorized results given grayscale images due to a lack of
human-like global understanding of colors. Recently, large-scale Text-to-Image
(T2I) models have been exploited to transfer the semantic information from the
text prompts to the image domain, where text provides a global control for
semantic objects in the image. In this work, we introduce a colorization model
piggybacking on the existing powerful T2I diffusion model. Our key idea is to
exploit the color prior knowledge in the pre-trained T2I diffusion model for
realistic and diverse colorization. A diffusion guider is designed to
incorporate the pre-trained weights of the latent diffusion model to output a
latent color prior that conforms to the visual semantics of the grayscale
input. A lightness-aware VQVAE will then generate the colorized result with
pixel-perfect alignment to the given grayscale image. Our model can also
achieve conditional colorization with additional inputs (e.g. user hints and
texts). Extensive experiments show that our method achieves state-of-the-art
performance in terms of perceptual quality.",None,-1
2920539c-8d5e-4df9-9524-dd040396921c,"ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis",0.661963,2,"The computational analysis of poetry is limited by the scarcity of tools to
automatically analyze and scan poems. In a multilingual settings, the problem
is exacerbated as scansion and rhyme systems only exist for individual
languages, making comparative studies very challenging and time consuming. In
this work, we present \textsc{Alberti}, the first multilingual pre-trained
large language model for poetry. Through domain-specific pre-training (DSP), we
further trained multilingual BERT on a corpus of over 12 million verses from 12
languages. We evaluated its performance on two structural poetry tasks: Spanish
stanza type classification, and metrical pattern prediction for Spanish,
English and German. In both cases, \textsc{Alberti} outperforms multilingual
BERT and other transformers-based models of similar sizes, and even achieves
state-of-the-art results for German when compared to rule-based systems,
demonstrating the feasibility and effectiveness of DSP in the poetry domain.",None,-1
e9e82884-f3d3-42fa-a5c4-80fdb03e572c,Do Differences in Values Influence Disagreements in Online Discussions?,0.896703,4,"Disagreements are common in online discussions. Disagreement may foster
collaboration and improve the quality of a discussion under some conditions.
Although there exist methods for recognizing disagreement, a deeper
understanding of factors that influence disagreement is lacking in the
literature. We investigate a hypothesis that differences in personal values are
indicative of disagreement in online discussions. We show how state-of-the-art
models can be used for estimating values in online discussions and how the
estimated values can be aggregated into value profiles. We evaluate the
estimated value profiles based on human-annotated agreement labels. We find
that the dissimilarity of value profiles correlates with disagreement in
specific cases. We also find that including value information in agreement
prediction improves performance.",None,-1
092edb3b-3bd5-4b0c-a7ea-4b2d35484d54,Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction,0.74759,8,"Recently, aspect sentiment quad prediction has received widespread attention
in the field of aspect-based sentiment analysis. Existing studies extract
quadruplets via pre-trained generative language models to paraphrase the
original sentence into a templated target sequence. However, previous works
only focus on what to generate but ignore what not to generate. We argue that
considering the negative samples also leads to potential benefits. In this
work, we propose a template-agnostic method to control the token-level
generation, which boosts original learning and reduces mistakes simultaneously.
Specifically, we introduce Monte Carlo dropout to understand the built-in
uncertainty of pre-trained language models, acquiring the noises and errors. We
further propose marginalized unlikelihood learning to suppress the
uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to
balance the effects of marginalized unlikelihood learning. Extensive
experiments on four public datasets demonstrate the effectiveness of our
approach on various generation templates.",None,-1
adacd07f-fa09-42b6-bb2d-49c02b5c35d7,Lightweight reranking for language model generations,0.050444,3,"Large Language Models (LLMs) can exhibit considerable variation in the
quality of their sampled outputs. Reranking and selecting the best generation
from the sampled set is a popular way of obtaining strong gains in generation
quality. In this paper, we present a novel approach for reranking LLM
generations. Unlike other techniques that might involve additional inferences
or training a specialized reranker, our approach relies on easy to compute
pairwise statistics between the generations that have minimal compute overhead.
We show that our approach can be formalized as an extension of self-consistency
and analyze its performance in that framework, theoretically as well as via
simulations. We show strong improvements for selecting the best k generations
for code generation tasks as well as robust improvements for the best
generation for the tasks of autoformalization, summarization, and translation.
While our approach only assumes black-box access to LLMs, we show that
additional access to token probabilities can improve performance even further.",None,-1
3ffab0dd-6f14-4426-9ed4-f5b6d4983386,Learning Global-Local Correspondence with Semantic Bottleneck for Logical Anomaly Detection,0.432017,6,"This paper presents a novel framework, named Global-Local Correspondence
Framework (GLCF), for visual anomaly detection with logical constraints. Visual
anomaly detection has become an active research area in various real-world
applications, such as industrial anomaly detection and medical disease
diagnosis. However, most existing methods focus on identifying local structural
degeneration anomalies and often fail to detect high-level functional anomalies
that involve logical constraints. To address this issue, we propose a
two-branch approach that consists of a local branch for detecting structural
anomalies and a global branch for detecting logical anomalies. To facilitate
local-global feature correspondence, we introduce a novel semantic bottleneck
enabled by the visual Transformer. Moreover, we develop feature estimation
networks for each branch separately to detect anomalies. Our proposed framework
is validated using various benchmarks, including industrial datasets, Mvtec AD,
Mvtec Loco AD, and the Retinal-OCT medical dataset. Experimental results show
that our method outperforms existing methods, particularly in detecting logical
anomalies.",None,-1
e300e96f-fc87-4a05-9549-dcaca40ca812,Self-Sufficient Framework for Continuous Sign Language Recognition,0.908278,6,"The goal of this work is to develop self-sufficient framework for Continuous
Sign Language Recognition (CSLR) that addresses key issues of sign language
recognition. These include the need for complex multi-scale features such as
hands, face, and mouth for understanding, and absence of frame-level
annotations. To this end, we propose (1) Divide and Focus Convolution (DFConv)
which extracts both manual and non-manual features without the need for
additional networks or annotations, and (2) Dense Pseudo-Label Refinement
(DPLR) which propagates non-spiky frame-level pseudo-labels by combining the
ground truth gloss sequence labels with the predicted sequence. We demonstrate
that our model achieves state-of-the-art performance among RGB-based methods on
large-scale CSLR benchmarks, PHOENIX-2014 and PHOENIX-2014-T, while showing
comparable results with better efficiency when compared to other approaches
that use multi-modality or extra annotations.",None,-1
8d746740-353a-4a16-ba24-31fe3aec815d,Enhancing Video Super-Resolution via Implicit Resampling-based Alignment,0.0789836,1,"In video super-resolution, it is common to use a frame-wise alignment to
support the propagation of information over time. The role of alignment is
well-studied for low-level enhancement in video, but existing works overlook a
critical step -- resampling. We show through extensive experiments that for
alignment to be effective, the resampling should preserve the reference
frequency spectrum while minimizing spatial distortions. However, most existing
works simply use a default choice of bilinear interpolation for resampling even
though bilinear interpolation has a smoothing effect and hinders
super-resolution. From these observations, we propose an implicit
resampling-based alignment. The sampling positions are encoded by a sinusoidal
positional encoding, while the value is estimated with a coordinate network and
a window-based cross-attention. We show that bilinear interpolation inherently
attenuates high-frequency information while an MLP-based coordinate network can
approximate more frequencies. Experiments on synthetic and real-world datasets
show that alignment with our proposed implicit resampling enhances the
performance of state-of-the-art frameworks with minimal impact on both compute
and parameters.",None,-1
4ebc42e4-4f44-49c4-939b-ae2f6c7c795e,"JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning",0.14196,4,"Instruction tuning has become an essential process for optimizing the
performance of large language models (LLMs). However, current text-to-text
instruction tuning methods, referred to as TextTuning, exhibit significant
limitations in terms of generalization, robustness, and controllability,
primarily due to the absence of explicit task structures. In this paper, we
introduce JsonTuning, a novel structure-to-structure approach for instruction
tuning. By utilizing the versatile and structured format of JSON to represent
tasks, JsonTuning enhances generalization by enabling the model to comprehend
essential task elements and their interrelations, improves robustness by
reducing ambiguity, and increases controllability by providing explicit control
over the output. We conduct a comprehensive comparative analysis between
JsonTuning and TextTuning using various language models and evaluation
benchmarks. Our experimental results demonstrate that JsonTuning consistently
outperforms TextTuning across a range of applications, showing marked
improvements in performance, robustness, and controllability. By addressing the
inherent limitations of TextTuning, JsonTuning reveals significant potential
for developing more effective and reliable LLMs capable of managing diverse
scenarios.",None,-1
540fcea5-14ea-475c-a19b-db73ded6defd,Neural Field Conditioning Strategies for 2D Semantic Segmentation,0.057297,1,"Neural fields are neural networks which map coordinates to a desired signal.
When a neural field should jointly model multiple signals, and not memorize
only one, it needs to be conditioned on a latent code which describes the
signal at hand. Despite being an important aspect, there has been little
research on conditioning strategies for neural fields. In this work, we explore
the use of neural fields as decoders for 2D semantic segmentation. For this
task, we compare three conditioning methods, simple concatenation of the latent
code, Feature Wise Linear Modulation (FiLM), and Cross-Attention, in
conjunction with latent codes which either describe the full image or only a
local region of the image. Our results show a considerable difference in
performance between the examined conditioning strategies. Furthermore, we show
that conditioning via Cross-Attention achieves the best results and is
competitive with a CNN-based decoder for semantic segmentation.",None,-1
f744cab7-c3d8-45fb-a5a1-e37ad060b9da,Inst-Inpaint: Instructing to Remove Objects with Diffusion Models,0.917994,26,"Image inpainting task refers to erasing unwanted pixels from images and
filling them in a semantically consistent and realistic way. Traditionally, the
pixels that are wished to be erased are defined with binary masks. From the
application point of view, a user needs to generate the masks for the objects
they would like to remove which can be time-consuming and prone to errors. In
this work, we are interested in an image inpainting algorithm that estimates
which object to be removed based on natural language input and removes it,
simultaneously. For this purpose, first, we construct a dataset named
GQA-Inpaint for this task. Second, we present a novel inpainting framework,
Inst-Inpaint, that can remove objects from images based on the instructions
given as text prompts. We set various GAN and diffusion-based baselines and run
experiments on synthetic and real image datasets. We compare methods with
different evaluation metrics that measure the quality and accuracy of the
models and show significant quantitative and qualitative improvements.",None,-1
32ee415e-e440-489f-adb4-8f49185d1008,"Exploring the Consistency, Quality and Challenges in Manual and Automated Coding of Free-text Diagnoses from Hospital Outpatient Letters",0.327337,1,"Coding of unstructured clinical free-text to produce interoperable structured
data is essential to improve direct care, support clinical communication and to
enable clinical research.However, manual clinical coding is difficult and time
consuming, which motivates the development and use of natural language
processing for automated coding. This work evaluates the quality and
consistency of both manual and automated clinical coding of diagnoses from
hospital outpatient letters. Using 100 randomly selected letters, two human
clinicians performed coding of diagnosis lists to SNOMED CT. Automated coding
was also performed using IMO's Concept Tagger. A gold standard was constructed
by a panel of clinicians from a subset of the annotated diagnoses. This was
used to evaluate the quality and consistency of both manual and automated
coding via (1) a distance-based metric, treating SNOMED CT as a graph, and (2)
a qualitative metric agreed upon by the panel of clinicians. Correlation
between the two metrics was also evaluated. Comparing human and
computer-generated codes to the gold standard, the results indicate that humans
slightly out-performed automated coding, while both performed notably better
when there was only a single diagnosis contained in the free-text description.
Automated coding was considered acceptable by the panel of clinicians in
approximately 90% of cases.",None,-1
a6456068-ab0b-4a3e-adb1-01b807981eba,CoLLD: Contrastive Layer-to-layer Distillation for Compressing Multilingual Pre-trained Speech Encoders,0.407268,2,"Large-scale self-supervised pre-trained speech encoders outperform
conventional approaches in speech recognition and translation tasks. Due to the
high cost of developing these large models, building new encoders for new tasks
and deploying them to on-device applications are infeasible. Prior studies
propose model compression methods to address this issue, but those works focus
on smaller models and less realistic tasks. Thus, we propose Contrastive
Layer-to-layer Distillation (CoLLD), a novel knowledge distillation method to
compress pre-trained speech encoders by leveraging masked prediction and
contrastive learning to train student models to copy the behavior of a large
teacher model. CoLLD outperforms prior methods and closes the gap between small
and large models on multilingual speech-to-text translation and recognition
benchmarks.",None,-1
4a4fbdcf-8e33-440c-878a-60a3862c19c4,"Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal",0.298788,3,"In imitation learning for planning, parameters of heuristic functions are
optimized against a set of solved problem instances. This work revisits the
necessary and sufficient conditions of strictly optimally efficient heuristics
for forward search algorithms, mainly A* and greedy best-first search, which
expand only states on the returned optimal path. It then proposes a family of
loss functions based on ranking tailored for a given variant of the forward
search algorithm. Furthermore, from a learning theory point of view, it
discusses why optimizing cost-to-goal \hstar\ is unnecessarily difficult. The
experimental comparison on a diverse set of problems unequivocally supports the
derived theory.",None,-1
f5b0e9c2-af59-4412-91ba-2ccf0e602484,Adaptation and Optimization of Automatic Speech Recognition (ASR) for the Maritime Domain in the Field of VHF Communication,0.219989,1,"This paper introduces a multilingual automatic speech recognizer (ASR) for
maritime radio communi-cation that automatically converts received VHF radio
signals into text. The challenges of maritime radio communication are described
at first, and the deep learning architecture of marFM consisting of audio
processing techniques and machine learning algorithms is presented.
Subsequently, maritime radio data of interest is analyzed and then used to
evaluate the transcription performance of our ASR model for various maritime
radio data.",None,-1
dd5432c2-6c0b-4901-bc1e-80961ac95f96,AdvFAS: A robust face anti-spoofing framework against adversarial examples,0.7578,3,"Ensuring the reliability of face recognition systems against presentation
attacks necessitates the deployment of face anti-spoofing techniques. Despite
considerable advancements in this domain, the ability of even the most
state-of-the-art methods to defend against adversarial examples remains
elusive. While several adversarial defense strategies have been proposed, they
typically suffer from constrained practicability due to inevitable trade-offs
between universality, effectiveness, and efficiency. To overcome these
challenges, we thoroughly delve into the coupled relationship between
adversarial detection and face anti-spoofing. Based on this, we propose a
robust face anti-spoofing framework, namely AdvFAS, that leverages two coupled
scores to accurately distinguish between correctly detected and wrongly
detected face images. Extensive experiments demonstrate the effectiveness of
our framework in a variety of settings, including different attacks, datasets,
and backbones, meanwhile enjoying high accuracy on clean examples. Moreover, we
successfully apply the proposed method to detect real-world adversarial
examples.",None,-1
503ed85a-9c1f-4881-a625-5c4c3a8bddc1,Fact-Checking Generative AI: Ontology-Driven Biological Graphs for Disease-Gene Link Verification,0.37116,2,"Since the launch of various generative AI tools, scientists have been
striving to evaluate their capabilities and contents, in the hope of
establishing trust in their generative abilities. Regulations and guidelines
are emerging to verify generated contents and identify novel uses. we aspire to
demonstrate how ChatGPT claims are checked computationally using the rigor of
network models. We aim to achieve fact-checking of the knowledge embedded in
biological graphs that were contrived from ChatGPT contents at the aggregate
level. We adopted a biological networks approach that enables the systematic
interrogation of ChatGPT's linked entities. We designed an ontology-driven
fact-checking algorithm that compares biological graphs constructed from
approximately 200,000 PubMed abstracts with counterparts constructed from a
dataset generated using the ChatGPT-3.5 Turbo model. In 10-samples of 250
randomly selected records a ChatGPT dataset of 1000 ""simulated"" articles , the
fact-checking link accuracy ranged from 70% to 86%. This study demonstrated
high accuracy of aggregate disease-gene links relationships found in
ChatGPT-generated texts.",None,-1
e872d741-a85b-4212-8a41-5e3380cb9729,Unlearnable Graph: Protecting Graphs from Unauthorized Exploitation,0.149928,3,"While the use of graph-structured data in various fields is becoming
increasingly popular, it also raises concerns about the potential unauthorized
exploitation of personal data for training commercial graph neural network
(GNN) models, which can compromise privacy. To address this issue, we propose a
novel method for generating unlearnable graph examples. By injecting delusive
but imperceptible noise into graphs using our Error-Minimizing Structural
Poisoning (EMinS) module, we are able to make the graphs unexploitable.
Notably, by modifying only $5\%$ at most of the potential edges in the graph
data, our method successfully decreases the accuracy from ${77.33\%}$ to
${42.47\%}$ on the COLLAB dataset.",None,-1
d0a61011-322b-4be5-a25e-3f072ba1d701,A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization,0.117655,1,"We prove Wasserstein inverse reinforcement learning enables the learner's
reward values to imitate the expert's reward values in a finite iteration for
multi-objective optimizations. Moreover, we prove Wasserstein inverse
reinforcement learning enables the learner's optimal solutions to imitate the
expert's optimal solutions for multi-objective optimizations with lexicographic
order.",None,-1
ed3f4c05-9c2e-4b13-be25-eb49ad3f364b,Improving Vietnamese Legal Question--Answering System based on Automatic Data Enrichment,0.0298851,1,"Question answering (QA) in law is a challenging problem because legal
documents are much more complicated than normal texts in terms of terminology,
structure, and temporal and logical relationships. It is even more difficult to
perform legal QA for low-resource languages like Vietnamese where labeled data
are rare and pre-trained language models are still limited. In this paper, we
try to overcome these limitations by implementing a Vietnamese article-level
retrieval-based legal QA system and introduce a novel method to improve the
performance of language models by improving data quality through weak labeling.
Our hypothesis is that in contexts where labeled data are limited, efficient
data enrichment can help increase overall performance. Our experiments are
designed to test multiple aspects, which demonstrate the effectiveness of the
proposed technique.",None,-1
52e457f3-b34a-4b23-b843-60364a23ad08,Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models,0.380881,13,"Large language models (LLMs) can perform a wide range of tasks by following
natural language instructions, without the necessity of task-specific
fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by
the quality of these instructions, and manually writing effective instructions
for each task is a laborious and subjective process. In this paper, we
introduce Auto-Instruct, a novel method to automatically improve the quality of
instructions provided to LLMs. Our method leverages the inherent generative
ability of LLMs to produce diverse candidate instructions for a given task, and
then ranks them using a scoring model trained on a variety of 575 existing NLP
tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both
human-written instructions and existing baselines of LLM-generated
instructions. Furthermore, our method exhibits notable generalizability even
with other LLMs that are not incorporated into its training process.",None,-1
3135a5d6-eb84-4c95-acfb-b620e237e448,A Multi-Modal Transformer Network for Action Detection,0.740992,8,"This paper proposes a novel multi-modal transformer network for detecting
actions in untrimmed videos. To enrich the action features, our transformer
network utilizes a new multi-modal attention mechanism that computes the
correlations between different spatial and motion modalities combinations.
Exploring such correlations for actions has not been attempted previously. To
use the motion and spatial modality more effectively, we suggest an algorithm
that corrects the motion distortion caused by camera movement. Such motion
distortion, common in untrimmed videos, severely reduces the expressive power
of motion features such as optical flow fields. Our proposed algorithm
outperforms the state-of-the-art methods on two public benchmarks, THUMOS14 and
ActivityNet. We also conducted comparative experiments on our new instructional
activity dataset, including a large set of challenging classroom videos
captured from elementary schools.",None,-1
d6a92e4f-eed0-4e6f-b79e-b868a78abaf0,Mask Detection and Classification in Thermal Face Images,0.581183,5,"Face masks are recommended to reduce the transmission of many viruses,
especially SARS-CoV-2. Therefore, the automatic detection of whether there is a
mask on the face, what type of mask is worn, and how it is worn is an important
research topic. In this work, the use of thermal imaging was considered to
analyze the possibility of detecting (localizing) a mask on the face, as well
as to check whether it is possible to classify the type of mask on the face.
The previously proposed dataset of thermal images was extended and annotated
with the description of a type of mask and a location of a mask within a face.
Different deep learning models were adapted. The best model for face mask
detection turned out to be the Yolov5 model in the ""nano"" version, reaching mAP
higher than 97% and precision of about 95%. High accuracy was also obtained for
mask type classification. The best results were obtained for the convolutional
neural network model built on an autoencoder initially trained in the thermal
image reconstruction problem. The pretrained encoder was used to train a
classifier which achieved an accuracy of 91%.",None,-1
1303169d-fe39-45f2-a15f-d1af2569c922,Density-based clustering with fully-convolutional networks for crowd flow detection from drones,0.689559,8,"Crowd analysis from drones has attracted increasing attention in recent times
due to the ease of use and affordable cost of these devices. However, how this
technology can provide a solution to crowd flow detection is still an
unexplored research question. To this end, we propose a crowd flow detection
method for video sequences shot by a drone. The method is based on a
fully-convolutional network that learns to perform crowd clustering in order to
detect the centroids of crowd-dense areas and track their movement in
consecutive frames. The proposed method proved effective and efficient when
tested on the Crowd Counting datasets of the VisDrone challenge, characterized
by video sequences rather than still images. The encouraging results show that
the proposed method could open up new ways of analyzing high-level crowd
behavior from drones.",None,-1
976fdf72-6e70-4e0d-9912-9ae980934c75,IMF: Interactive Multimodal Fusion Model for Link Prediction,0.8842,13,"Link prediction aims to identify potential missing triples in knowledge
graphs. To get better results, some recent studies have introduced multimodal
information to link prediction. However, these methods utilize multimodal
information separately and neglect the complicated interaction between
different modalities. In this paper, we aim at better modeling the
inter-modality information and thus introduce a novel Interactive Multimodal
Fusion (IMF) model to integrate knowledge from different modalities. To this
end, we propose a two-stage multimodal fusion framework to preserve
modality-specific knowledge as well as take advantage of the complementarity
between different modalities. Instead of directly projecting different
modalities into a unified space, our multimodal fusion module limits the
representations of different modalities independent while leverages bilinear
pooling for fusion and incorporates contrastive learning as additional
constraints. Furthermore, the decision fusion module delivers the learned
weighted average over the predictions of all modalities to better incorporate
the complementarity of different modalities. Our approach has been demonstrated
to be effective through empirical evaluations on several real-world datasets.
The implementation code is available online at
https://github.com/HestiaSky/IMF-Pytorch.",None,-1
671ab3ae-336f-4364-8914-488facd24631,Bright Channel Prior Attention for Multispectral Pedestrian Detection,0.53994,2,"Multispectral methods have gained considerable attention due to their
promising performance across various fields. However, most existing methods
cannot effectively utilize information from two modalities while optimizing
time efficiency. These methods often prioritize accuracy or time efficiency,
leaving room for improvement in their performance. To this end, we propose a
new method bright channel prior attention for enhancing pedestrian detection in
low-light conditions by integrating image enhancement and detection within a
unified framework. The method uses the V-channel of the HSV image of the
thermal image as an attention map to trigger the unsupervised auto-encoder for
visible light images, which gradually emphasizes pedestrian features across
layers. Moreover, we utilize unsupervised bright channel prior algorithms to
address light compensation in low light images. The proposed method includes a
self-attention enhancement module and a detection module, which work together
to improve object detection. An initial illumination map is estimated using the
BCP, guiding the learning of the self-attention map from the enhancement
network to obtain more informative representation focused on pedestrians. The
extensive experiments show effectiveness of the proposed method is demonstrated
through.",None,-1
95c749a6-b8a3-4ebc-ad86-427657273bb5,Inducing Stackelberg Equilibrium through Spatio-Temporal Sequential Decision-Making in Multi-Agent Reinforcement Learning,0.46406,4,"In multi-agent reinforcement learning (MARL), self-interested agents attempt
to establish equilibrium and achieve coordination depending on game structure.
However, existing MARL approaches are mostly bound by the simultaneous actions
of all agents in the Markov game (MG) framework, and few works consider the
formation of equilibrium strategies via asynchronous action coordination. In
view of the advantages of Stackelberg equilibrium (SE) over Nash equilibrium,
we construct a spatio-temporal sequential decision-making structure derived
from the MG and propose an N-level policy model based on a conditional
hypernetwork shared by all agents. This approach allows for asymmetric training
with symmetric execution, with each agent responding optimally conditioned on
the decisions made by superior agents. Agents can learn heterogeneous SE
policies while still maintaining parameter sharing, which leads to reduced cost
for learning and storage and enhanced scalability as the number of agents
increases. Experiments demonstrate that our method effectively converges to the
SE policies in repeated matrix game scenarios, and performs admirably in
immensely complex settings including cooperative tasks and mixed tasks.",None,-1
2119b207-eb8c-4fde-8fcb-5def5be86899,Target-Aware Generative Augmentations for Single-Shot Adaptation,0.169894,2,"In this paper, we address the problem of adapting models from a source domain
to a target domain, a task that has become increasingly important due to the
brittle generalization of deep neural networks. While several test-time
adaptation techniques have emerged, they typically rely on synthetic toolbox
data augmentations in cases of limited target data availability. We consider
the challenging setting of single-shot adaptation and explore the design of
augmentation strategies. We argue that augmentations utilized by existing
methods are insufficient to handle large distribution shifts, and hence propose
a new approach SiSTA, which first fine-tunes a generative model from the source
domain using a single-shot target, and then employs novel sampling strategies
for curating synthetic target data. Using experiments on a variety of
benchmarks, distribution shifts and image corruptions, we find that SiSTA
produces significantly improved generalization over existing baselines in face
attribute detection and multi-class object recognition. Furthermore, SiSTA
performs competitively to models obtained by training on larger target
datasets. Our codes can be accessed at https://github.com/Rakshith-2905/SiSTA.",None,-1
f47af5c1-9bbf-4102-9024-2d7ae0bfc127,All Things Considered: Detecting Partisan Events from News Media with Cross-Article Comparison,0.707932,2,"Public opinion is shaped by the information news media provide, and that
information in turn may be shaped by the ideological preferences of media
outlets. But while much attention has been devoted to media bias via overt
ideological language or topic selection, a more unobtrusive way in which the
media shape opinion is via the strategic inclusion or omission of partisan
events that may support one side or the other. We develop a latent
variable-based framework to predict the ideology of news articles by comparing
multiple articles on the same story and identifying partisan events whose
inclusion or omission reveals ideology. Our experiments first validate the
existence of partisan event selection, and then show that article alignment and
cross-document comparison detect partisan events and article ideology better
than competitive baselines. Our results reveal the high-level form of media
bias, which is present even among mainstream media with strong norms of
objectivity and nonpartisanship. Our codebase and dataset are available at
https://github.com/launchnlp/ATC.",None,-1
a74417db-1c7b-4f66-8bf3-6821ba5aaa44,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,0.243749,3,"Automatic examination of thin-prep cytologic test (TCT) slides can assist
pathologists in finding cervical abnormality for accurate and efficient cancer
screening. Current solutions mostly need to localize suspicious cells and
classify abnormality based on local patches, concerning the fact that whole
slide images of TCT are extremely large. It thus requires many annotations of
normal and abnormal cervical cells, to supervise the training of the
patch-level classifier for promising performance. In this paper, we propose
CellGAN to synthesize cytopathological images of various cervical cell types
for augmenting patch-level cell classification. Built upon a lightweight
backbone, CellGAN is equipped with a non-linear class mapping network to
effectively incorporate cell type information into image generation. We also
propose the Skip-layer Global Context module to model the complex spatial
relationship of the cells, and attain high fidelity of the synthesized images
through adversarial learning. Our experiments demonstrate that CellGAN can
produce visually plausible TCT cytopathological images for different cell
types. We also validate the effectiveness of using CellGAN to greatly augment
patch-level cell classification performance.",None,-1
aafb3da2-fcb2-427a-9c2d-2f4079ef4d21,"Mini-Giants: ""Small"" Language Models and Open Source Win-Win",0.0381123,1,"ChatGPT is phenomenal. However, it is prohibitively expensive to train and
refine such giant models. Fortunately, small language models are flourishing
and becoming more and more competent. We call them ""mini-giants"". We argue that
open source community like Kaggle and mini-giants will win-win in many ways,
technically, ethically and socially. In this article, we present a brief yet
rich background, discuss how to attain small language models, present a
comparative study of small language models and a brief discussion of evaluation
methods, discuss the application scenarios where small language models are most
needed in the real world, and conclude with discussion and outlook.",None,-1
887cc5e1-b1e4-497c-a485-a1f5a68a3163,Customising General Large Language Models for Specialised Emotion Recognition Tasks,0.526573,3,"The advent of large language models (LLMs) has gained tremendous attention
over the past year. Previous studies have shown the astonishing performance of
LLMs not only in other tasks but also in emotion recognition in terms of
accuracy, universality, explanation, robustness, few/zero-shot learning, and
others. Leveraging the capability of LLMs inevitably becomes an essential
solution for emotion recognition. To this end, we further comprehensively
investigate how LLMs perform in linguistic emotion recognition if we
concentrate on this specific task. Specifically, we exemplify a publicly
available and widely used LLM -- Chat General Language Model, and customise it
for our target by using two different modal adaptation techniques, i.e., deep
prompt tuning and low-rank adaptation. The experimental results obtained on six
widely used datasets present that the adapted LLM can easily outperform other
state-of-the-art but specialised deep models. This indicates the strong
transferability and feasibility of LLMs in the field of emotion recognition.",None,-1
1217c730-d3ae-4a82-9e90-8f9cf5c73b48,A GOA-Based Fault-Tolerant Trajectory Tracking Control for an Underwater Vehicle of Multi-Thruster System without Actuator Saturation,0.698725,5,"This paper proposes an intelligent fault-tolerant control (FTC) strategy to
tackle the trajectory tracking problem of an underwater vehicle (UV) under
thruster damage (power loss) cases and meanwhile resolve the actuator
saturation brought by the vehicle's physical constraints. In the proposed
control strategy, the trajectory tracking component is formed by a refined
backstepping algorithm that controls the velocity variation and a sliding mode
control deducts the torque/force outputs; the fault-tolerant component is
established based on a Grasshopper Optimization Algorithm (GOA), which provides
fast convergence speed as well as satisfactory accuracy of deducting optimized
reallocation of the thruster forces to compensate for the power loss in
different fault cases. Simulations with or without environmental perturbations
under different fault cases and comparisons to other traditional FTCs are
presented, thus verifying the effectiveness and robustness of the proposed
GOA-based fault-tolerant trajectory tracking design.",None,-1
63d2f15b-fa71-463f-ad97-510b7fc2b1f3,Automatic Model Selection with Large Language Models for Reasoning,0.339663,24,"Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two
distinct reasoning methods, each with its own strengths. CoT employs natural
language, offering flexibility and interpretability, while PAL utilizes
programming language, yielding more structured and rigorous logic. We introduce
a model selection method to combine the best of both worlds by employing a
large language model (LLM) to dynamically select between them. Our theoretical
analysis underscores the feasibility of this method, which is further
corroborated by empirical results. Our proposed method demonstrates significant
performance improvements across eight reasoning datasets with Codex, ChatGPT,
and GPT-4. Additionally, our method is complementary to self-consistency; when
integrated, it can further enhance performance while significantly reducing
computation costs. Moreover, we achieve new state-of-the-art results on GSM8K
and SVAMP, with respective accuracies of 96.8% and 93.7%. Our code, data and
prompts are available at https://github.com/XuZhao0/Model-Selection-Reasoning",None,-1
f7e02ac0-55ce-45b0-8f34-24d24c20918a,I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs,0.938915,25,"In this work, we present I$^2$-SDF, a new method for intrinsic indoor scene
reconstruction and editing using differentiable Monte Carlo raytracing on
neural signed distance fields (SDFs). Our holistic neural SDF-based framework
jointly recovers the underlying shapes, incident radiance and materials from
multi-view images. We introduce a novel bubble loss for fine-grained small
objects and error-guided adaptive sampling scheme to largely improve the
reconstruction quality on large-scale indoor scenes. Further, we propose to
decompose the neural radiance field into spatially-varying material of the
scene as a neural field through surface-based, differentiable Monte Carlo
raytracing and emitter semantic segmentations, which enables physically based
and photorealistic scene relighting and editing applications. Through a number
of qualitative and quantitative experiments, we demonstrate the superior
quality of our method on indoor scene reconstruction, novel view synthesis, and
scene editing compared to state-of-the-art baselines.",None,-1
515e0dd1-5d5b-4b1a-957a-432ec74e7abd,A Pathway Towards Responsible AI Generated Content,0.947196,50,"AI Generated Content (AIGC) has received tremendous attention within the past
few years, with content generated in the format of image, text, audio, video,
etc. Meanwhile, AIGC has become a double-edged sword and recently received much
criticism regarding its responsible usage. In this article, we focus on 8 main
concerns that may hinder the healthy development and deployment of AIGC in
practice, including risks from (1) privacy; (2) bias, toxicity, misinformation;
(3) intellectual property (IP); (4) robustness; (5) open source and
explanation; (6) technology abuse; (7) consent, credit, and compensation; (8)
environment. Additionally, we provide insights into the promising directions
for tackling these risks while constructing generative models, enabling AIGC to
be used more responsibly to truly benefit society.",None,-1
2ae72918-5a4f-4fe9-9422-aa3d223cc8a0,Image-Based Vehicle Classification by Synergizing Features from Supervised and Self-Supervised Learning Paradigms,0.0943729,3,"This paper introduces a novel approach to leverage features learned from both
supervised and self-supervised paradigms, to improve image classification
tasks, specifically for vehicle classification. Two state-of-the-art
self-supervised learning methods, DINO and data2vec, were evaluated and
compared for their representation learning of vehicle images. The former
contrasts local and global views while the latter uses masked prediction on
multi-layered representations. In the latter case, supervised learning is
employed to finetune a pretrained YOLOR object detector for detecting vehicle
wheels, from which definitive wheel positional features are retrieved. The
representations learned from these self-supervised learning methods were
combined with the wheel positional features for the vehicle classification
task. Particularly, a random wheel masking strategy was utilized to finetune
the previously learned representations in harmony with the wheel positional
features during the training of the classifier. Our experiments show that the
data2vec-distilled representations, which are consistent with our wheel masking
strategy, outperformed the DINO counterpart, resulting in a celebrated Top-1
classification accuracy of 97.2% for classifying the 13 vehicle classes defined
by the Federal Highway Administration.",None,-1
0365dd71-d8d7-4c10-9dc7-8f223b464776,Efficient CTC Regularization via Coarse Labels for End-to-End Speech Translation,0.365609,2,"For end-to-end speech translation, regularizing the encoder with the
Connectionist Temporal Classification (CTC) objective using the source
transcript or target translation as labels can greatly improve quality metrics.
However, CTC demands an extra prediction layer over the vocabulary space,
bringing in nonnegligible model parameters and computational overheads,
although this layer is typically not used for inference. In this paper, we
re-examine the need for genuine vocabulary labels for CTC for regularization
and explore strategies to reduce the CTC label space, targeting improved
efficiency without quality degradation. We propose coarse labeling for CTC
(CoLaCTC), which merges vocabulary labels via simple heuristic rules, such as
using truncation, division or modulo (MOD) operations. Despite its simplicity,
our experiments on 4 source and 8 target languages show that CoLaCTC with MOD
particularly can compress the label space aggressively to 256 and even further,
gaining training efficiency (1.18x ~ 1.77x speedup depending on the original
vocabulary size) yet still delivering comparable or better performance than the
CTC baseline. We also show that CoLaCTC successfully generalizes to CTC
regularization regardless of using transcript or translation for labeling.",None,-1
729873c2-d5eb-4d36-b7fb-79c966de1f58,Sketching the Future (STF): Applying Conditional Control Techniques to Text-to-Video Models,0.19515,5,"The proliferation of video content demands efficient and flexible neural
network based approaches for generating new video content. In this paper, we
propose a novel approach that combines zero-shot text-to-video generation with
ControlNet to improve the output of these models. Our method takes multiple
sketched frames as input and generates video output that matches the flow of
these frames, building upon the Text-to-Video Zero architecture and
incorporating ControlNet to enable additional input conditions. By first
interpolating frames between the inputted sketches and then running
Text-to-Video Zero using the new interpolated frames video as the control
technique, we leverage the benefits of both zero-shot text-to-video generation
and the robust control provided by ControlNet. Experiments demonstrate that our
method excels at producing high-quality and remarkably consistent video content
that more accurately aligns with the user's intended motion for the subject
within the video. We provide a comprehensive resource package, including a demo
video, project website, open-source GitHub repository, and a Colab playground
to foster further research and application of our proposed method.",None,-1
f3ed9bd5-c1f3-4e9e-bbdb-7e18099dd90f,Spelling convention sensitivity in neural language models,0.0254206,1,"We examine whether large neural language models, trained on very large
collections of varied English text, learn the potentially long-distance
dependency of British versus American spelling conventions, i.e., whether
spelling is consistently one or the other within model-generated strings. In
contrast to long-distance dependencies in non-surface underlying structure
(e.g., syntax), spelling consistency is easier to measure both in LMs and the
text corpora used to train them, which can provide additional insight into
certain observed model behaviors. Using a set of probe words unique to either
British or American English, we first establish that training corpora exhibit
substantial (though not total) consistency. A large T5 language model does
appear to internalize this consistency, though only with respect to observed
lexical items (not nonce words with British/American spelling patterns). We
further experiment with correcting for biases in the training data by
fine-tuning T5 on synthetic data that has been debiased, and find that
finetuned T5 remains only somewhat sensitive to spelling consistency. Further
experiments show GPT2 to be similarly limited.",None,-1
0e06ec3b-6c4f-4ac1-8797-eaeccfddba6d,DN at SemEval-2023 Task 12: Low-Resource Language Text Classification via Multilingual Pretrained Language Model Fine-tuning,0.115517,2,"In recent years, sentiment analysis has gained significant importance in
natural language processing. However, most existing models and datasets for
sentiment analysis are developed for high-resource languages, such as English
and Chinese, leaving low-resource languages, particularly African languages,
largely unexplored. The AfriSenti-SemEval 2023 Shared Task 12 aims to fill this
gap by evaluating sentiment analysis models on low-resource African languages.
In this paper, we present our solution to the shared task, where we employed
different multilingual XLM-R models with classification head trained on various
data, including those retrained in African dialects and fine-tuned on target
languages. Our team achieved the third-best results in Subtask B, Track 16:
Multilingual, demonstrating the effectiveness of our approach. While our model
showed relatively good results on multilingual data, it performed poorly in
some languages. Our findings highlight the importance of developing more
comprehensive datasets and models for low-resource African languages to advance
sentiment analysis research. We also provided the solution on the github
repository.",None,-1
10569332-bd03-4c07-ada1-13d96ee29c12,Time Series as Images: Vision Transformer for Irregularly Sampled Time Series,0.738281,8,"Irregularly sampled time series are increasingly prevalent, particularly in
medical domains. While various specialized methods have been developed to
handle these irregularities, effectively modeling their complex dynamics and
pronounced sparsity remains a challenge. This paper introduces a novel
perspective by converting irregularly sampled time series into line graph
images, then utilizing powerful pre-trained vision transformers for time series
classification in the same way as image classification. This method not only
largely simplifies specialized algorithm designs but also presents the
potential to serve as a universal framework for time series modeling.
Remarkably, despite its simplicity, our approach outperforms state-of-the-art
specialized algorithms on several popular healthcare and human activity
datasets. Especially in the rigorous leave-sensors-out setting where a portion
of variables is omitted during testing, our method exhibits strong robustness
against varying degrees of missing observations, achieving an impressive
improvement of 42.8% in absolute F1 score points over leading specialized
baselines even with half the variables masked. Code and data are available at
https://github.com/Leezekun/ViTST",None,-1
bda47b37-cc8a-44be-abd5-41f05a3331b6,ESCAPE: Countering Systematic Errors from Machine's Blind Spots via Interactive Visual Analysis,0.660058,5,"Classification models learn to generalize the associations between data
samples and their target classes. However, researchers have increasingly
observed that machine learning practice easily leads to systematic errors in AI
applications, a phenomenon referred to as AI blindspots. Such blindspots arise
when a model is trained with training samples (e.g., cat/dog classification)
where important patterns (e.g., black cats) are missing or
periphery/undesirable patterns (e.g., dogs with grass background) are
misleading towards a certain class. Even more sophisticated techniques cannot
guarantee to capture, reason about, and prevent the spurious associations. In
this work, we propose ESCAPE, a visual analytic system that promotes a
human-in-the-loop workflow for countering systematic errors. By allowing human
users to easily inspect spurious associations, the system facilitates users to
spontaneously recognize concepts associated misclassifications and evaluate
mitigation strategies that can reduce biased associations. We also propose two
statistical approaches, relative concept association to better quantify the
associations between a concept and instances, and debias method to mitigate
spurious associations. We demonstrate the utility of our proposed ESCAPE system
and statistical measures through extensive evaluation including quantitative
experiments, usage scenarios, expert interviews, and controlled user
experiments.",None,-1
c22319e5-d834-475f-90cf-1ca5b8539d2c,Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment identification across diverse communities,0.619073,1,"The prevalence and impact of toxic discussions online have made content
moderation crucial.Automated systems can play a vital role in identifying
toxicity, and reducing the reliance on human moderation.Nevertheless,
identifying toxic comments for diverse communities continues to present
challenges that are addressed in this paper.The two-part goal of this study is
to(1)identify intuitive variances from annotator disagreement using
quantitative analysis and (2)model the subjectivity of these viewpoints.To
achieve our goal, we published a new
dataset\footnote{\url{https://github.com/XXX}} with expert annotators'
annotations and used two other public datasets to identify the subjectivity of
toxicity.Then leveraging the Large Language Model(LLM),we evaluate the model's
ability to mimic diverse viewpoints on toxicity by varying size of the training
data and utilizing same set of annotators as the test set used during model
training and a separate set of annotators as the test set.We conclude that
subjectivity is evident across all annotator groups, demonstrating the
shortcomings of majority-rule voting. Moving forward, subjective annotations
should serve as ground truth labels for training models for domains like
toxicity in diverse communities.",None,-1
5d4e39e3-2ffa-475d-86d9-1697dc5d4cb8,The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages,0.505754,3,"Gender biases in language generation systems are challenging to mitigate. One
possible source for these biases is gender representation disparities in the
training and evaluation data. Despite recent progress in documenting this
problem and many attempts at mitigating it, we still lack shared methodology
and tooling to report gender representation in large datasets. Such
quantitative reporting will enable further mitigation, e.g., via data
augmentation. This paper describes the Gender-GAP Pipeline (for Gender-Aware
Polyglot Pipeline), an automatic pipeline to characterize gender representation
in large-scale datasets for 55 languages. The pipeline uses a multilingual
lexicon of gendered person-nouns to quantify the gender representation in text.
We showcase it to report gender representation in WMT training data and
development data for the News task, confirming that current data is skewed
towards masculine representation. Having unbalanced datasets may indirectly
optimize our systems towards outperforming one gender over the others. We
suggest introducing our gender quantification pipeline in current datasets and,
ideally, modifying them toward a balanced representation.",None,-1
bcb2589b-b7df-4b36-b1d4-ef81b7299c22,Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving,0.790069,4,"In recent years there have been remarkable advancements in autonomous
driving. While autonomous vehicles demonstrate high performance in closed-set
conditions, they encounter difficulties when confronted with unexpected
situations. At the same time, world models emerged in the field of model-based
reinforcement learning as a way to enable agents to predict the future
depending on potential actions. This led to outstanding results in sparse
reward and complex control tasks. This work provides an overview of how world
models can be leveraged to perform anomaly detection in the domain of
autonomous driving. We provide a characterization of world models and relate
individual components to previous works in anomaly detection to facilitate
further research in the field.",None,-1
0dda35a3-d26b-41cc-85f1-b26004fc57fe,Explicit Correspondence Matching for Generalizable Neural Radiance Fields,0.680604,17,"We present a new generalizable NeRF method that is able to directly
generalize to new unseen scenarios and perform novel view synthesis with as few
as two source views. The key to our approach lies in the explicitly modeled
correspondence matching information, so as to provide the geometry prior to the
prediction of NeRF color and density for volume rendering. The explicit
correspondence matching is quantified with the cosine similarity between image
features sampled at the 2D projections of a 3D point on different views, which
is able to provide reliable cues about the surface geometry. Unlike previous
methods where image features are extracted independently for each view, we
consider modeling the cross-view interactions via Transformer cross-attention,
which greatly improves the feature matching quality. Our method achieves
state-of-the-art results on different evaluation settings, with the experiments
showing a strong correlation between our learned cosine feature similarity and
volume density, demonstrating the effectiveness and superiority of our proposed
method. Code is at https://github.com/donydchen/matchnerf",None,-1
a4df94d3-e160-4443-94ec-ed20a1feba45,Toward Stronger Textual Attack Detectors,0.596943,2,"The landscape of available textual adversarial attacks keeps growing, posing
severe threats and raising concerns regarding the deep NLP system's integrity.
However, the crucial problem of defending against malicious attacks has only
drawn the attention of the NLP community. The latter is nonetheless
instrumental in developing robust and trustworthy systems. This paper makes two
important contributions in this line of search: (i) we introduce LAROUSSE, a
new framework to detect textual adversarial attacks and (ii) we introduce
STAKEOUT, a new benchmark composed of nine popular attack methods, three
datasets, and two pre-trained models. LAROUSSE is ready-to-use in production as
it is unsupervised, hyperparameter-free, and non-differentiable, protecting it
against gradient-based methods. Our new benchmark STAKEOUT allows for a robust
evaluation framework: we conduct extensive numerical experiments which
demonstrate that LAROUSSE outperforms previous methods, and which allows to
identify interesting factors of detection rate variations.",None,-1
5f95f1f1-cb5d-4614-84f5-070990da8783,PandaGPT: One Model To Instruction-Follow Them All,0.999999,161,"We present PandaGPT, an approach to emPower large lANguage moDels with visual
and Auditory instruction-following capabilities. Our pilot experiments show
that PandaGPT can perform complex tasks such as detailed image description
generation, writing stories inspired by videos, and answering questions about
audios. More interestingly, PandaGPT can take multimodal inputs simultaneously
and compose their semantics naturally. For example, PandaGPT can connect how
objects look in an image/video and how they sound in an audio. To do so,
PandaGPT combines the multimodal encoders from ImageBind and the large language
models from Vicuna. Notably, only aligned image-text pairs are required for the
training of PandaGPT. Thanks to the strong capability of ImageBind in embedding
data from different modalities into the same space, PandaGPT displays emergent,
i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g.,
video, audio, depth, thermal, and IMU). We hope that PandaGPT serves as an
initial step toward building AGI that can perceive and understand inputs in
different modalities holistically, as we humans do. Our project page is at
https://panda-gpt.github.io/.",None,-1
4c309588-2ca5-48c2-9cc6-3890b5114886,Randomized Adversarial Training via Taylor Expansion,0.552244,11,"In recent years, there has been an explosion of research into developing more
robust deep neural networks against adversarial examples. Adversarial training
appears as one of the most successful methods. To deal with both the robustness
against adversarial examples and the accuracy over clean examples, many works
develop enhanced adversarial training methods to achieve various trade-offs
between them. Leveraging over the studies that smoothed update on weights
during training may help find flat minima and improve generalization, we
suggest reconciling the robustness-accuracy trade-off from another perspective,
i.e., by adding random noise into deterministic weights. The randomized weights
enable our design of a novel adversarial training method via Taylor expansion
of a small Gaussian noise, and we show that the new adversarial training method
can flatten loss landscape and find flat minima. With PGD, CW, and Auto
Attacks, an extensive set of experiments demonstrate that our method enhances
the state-of-the-art adversarial training methods, boosting both robustness and
clean accuracy. The code is available at
https://github.com/Alexkael/Randomized-Adversarial-Training.",None,-1
6f4935a0-6c48-4790-a4ca-21db328b4ae0,Generalist: Decoupling Natural and Robust Generalization,0.450991,6,"Deep neural networks obtained by standard training have been constantly
plagued by adversarial examples. Although adversarial training demonstrates its
capability to defend against adversarial examples, unfortunately, it leads to
an inevitable drop in the natural generalization. To address the issue, we
decouple the natural generalization and the robust generalization from joint
training and formulate different training strategies for each one.
Specifically, instead of minimizing a global loss on the expectation over these
two generalization errors, we propose a bi-expert framework called
\emph{Generalist} where we simultaneously train base learners with task-aware
strategies so that they can specialize in their own fields. The parameters of
base learners are collected and combined to form a global learner at intervals
during the training process. The global learner is then distributed to the base
learners as initialized parameters for continued training. Theoretically, we
prove that the risks of Generalist will get lower once the base learners are
well trained. Extensive experiments verify the applicability of Generalist to
achieve high accuracy on natural examples while maintaining considerable
robustness to adversarial ones. Code is available at
https://github.com/PKU-ML/Generalist.",None,-1
1ad3a381-a611-4788-8e09-7a7edc089cc1,PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models,0.593796,12,"Prompts have significantly improved the performance of pretrained Large
Language Models (LLMs) on various downstream tasks recently, making them
increasingly indispensable for a diverse range of LLM application scenarios.
However, the backdoor vulnerability, a serious security threat that can
maliciously alter the victim model's normal predictions, has not been
sufficiently explored for prompt-based LLMs. In this paper, we present
POISONPROMPT, a novel backdoor attack capable of successfully compromising both
hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and
robustness of POISONPROMPT through extensive experiments on three popular
prompt methods, using six datasets and three widely used LLMs. Our findings
highlight the potential security threats posed by backdoor attacks on
prompt-based LLMs and emphasize the need for further research in this area.",None,-1
74564dae-7a60-4782-b836-15190b694e68,Activation Addition: Steering Language Models Without Optimization,0.912479,68,"Reliably controlling the behavior of large language models is a pressing open
problem. Existing methods include supervised finetuning, reinforcement learning
from human feedback, prompt engineering and guided decoding. We instead
investigate activation engineering: modifying activations at inference-time to
predictably alter model behavior. We bias the forward pass with a 'steering
vector' implicitly specified through natural language. Past work learned these
steering vectors; our Activation Addition (ActAdd) method instead computes them
by taking activation differences resulting from pairs of prompts. We
demonstrate ActAdd on a range of LLMs (LLaMA-3, OPT, GPT-2, and GPT-J),
obtaining SOTA on detoxification and negative-to-positive sentiment control.
Our approach yields inference-time control over high-level properties of output
like topic and sentiment while preserving performance on off-target tasks.
ActAdd takes far less compute and implementation effort than finetuning or
RLHF, allows users control through natural language, and its computational
overhead (as a fraction of inference time) appears stable or improving over
increasing model size.",None,-1
0ff7e8e7-7023-48a4-9f10-3e60d89231ff,Collaborative Discrepancy Optimization for Reliable Image Anomaly Localization,0.926908,31,"Most unsupervised image anomaly localization methods suffer from
overgeneralization because of the high generalization abilities of
convolutional neural networks, leading to unreliable predictions. To mitigate
the overgeneralization, this study proposes to collaboratively optimize normal
and abnormal feature distributions with the assistance of synthetic anomalies,
namely collaborative discrepancy optimization (CDO). CDO introduces a margin
optimization module and an overlap optimization module to optimize the two key
factors determining the localization performance, i.e., the margin and the
overlap between the discrepancy distributions (DDs) of normal and abnormal
samples. With CDO, a large margin and a small overlap between normal and
abnormal DDs are obtained, and the prediction reliability is boosted.
Experiments on MVTec2D and MVTec3D show that CDO effectively mitigates the
overgeneralization and achieves great anomaly localization performance with
real-time computation efficiency. A real-world automotive plastic parts
inspection application further demonstrates the capability of the proposed CDO.
Code is available on https://github.com/caoyunkang/CDO.",None,-1
779e2940-8083-4723-9dd4-58ea2f1fad65,Understanding Client Reactions in Online Mental Health Counseling,0.904181,5,"Communication success relies heavily on reading participants' reactions. Such
feedback is especially important for mental health counselors, who must
carefully consider the client's progress and adjust their approach accordingly.
However, previous NLP research on counseling has mainly focused on studying
counselors' intervention strategies rather than their clients' reactions to the
intervention. This work aims to fill this gap by developing a theoretically
grounded annotation framework that encompasses counselors' strategies and
client reaction behaviors. The framework has been tested against a large-scale,
high-quality text-based counseling dataset we collected over the past two years
from an online welfare counseling platform. Our study shows how clients react
to counselors' strategies, how such reactions affect the final counseling
outcomes, and how counselors can adjust their strategies in response to these
reactions. We also demonstrate that this study can help counselors
automatically predict their clients' states.",None,-1
4e48966f-a8d1-418c-9c61-8b4eafd05b7b,InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation,0.124435,1,"Diffusion models have garnered considerable interest in the field of text
generation. Several studies have explored text diffusion models with different
structures and applied them to various tasks, including named entity
recognition and summarization. However, there exists a notable disparity
between the ""easy-first"" text generation process of current diffusion models
and the ""keyword-first"" natural text generation process of humans, which has
received limited attention. To bridge this gap, we propose InfoDiffusion, a
non-autoregressive text diffusion model. Our approach introduces a
""keyinfo-first"" generation strategy and incorporates a noise schedule based on
the amount of text information. In addition, InfoDiffusion combines
self-conditioning with a newly proposed partially noising model structure.
Experimental results show that InfoDiffusion outperforms the baseline model in
terms of generation quality and diversity, as well as exhibiting higher
sampling efficiency.",None,-1
3f235241-dbdc-45eb-9811-fc82e8e158cc,A Geometric Notion of Causal Probing,0.237576,5,"The linear subspace hypothesis (Bolukbasi et al., 2016) states that, in a
language model's representation space, all information about a concept such as
verbal number is encoded in a linear subspace. Prior work has relied on
auxiliary classification tasks to identify and evaluate candidate subspaces
that might give support for this hypothesis. We instead give a set of intrinsic
criteria which characterize an ideal linear concept subspace and enable us to
identify the subspace using only the language model distribution. Our
information-theoretic framework accounts for spuriously correlated features in
the representation space (Kumar et al., 2022). As a byproduct of this analysis,
we hypothesize a causal process for how a language model might leverage
concepts during generation. Empirically, we find that LEACE (Belrose et al.,
2023) returns a one-dimensional subspace containing roughly half of total
concept information under our framework for verbal-number. Our causal
intervention for controlled generation shows that, for at least one concept,
the subspace returned by LEACE can be used to manipulate the concept value of
the generated word with precision.",None,-1
d281d37f-52c4-4ccd-aab2-e76239b5c8d2,MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations,0.351041,4,"Zero-shot classification enables text to be classified into classes not seen
during training. In this study, we examine the efficacy of zero-shot learning
models in classifying healthcare consultation responses from Doctors and AI
systems. The models evaluated include BART, BERT, XLM, XLM-R and DistilBERT.
The models were tested on three different datasets based on a binary and
multi-label analysis to identify the origins of text in health consultations
without any prior corpus training. According to our findings, the zero-shot
language models show a good understanding of language generally, but has
limitations when trying to classify doctor and AI responses to healthcare
consultations. This research provides a foundation for future research in the
field of medical text classification by informing the development of more
accurate methods of classifying text written by Doctors and AI systems in
health consultations.",None,-1
3c38df13-184a-481f-ab75-37ab88fcbb11,DreamIdentity: Improved Editability for Efficient Face-identity Preserved Image Generation,0.877061,15,"While large-scale pre-trained text-to-image models can synthesize diverse and
high-quality human-centric images, an intractable problem is how to preserve
the face identity for conditioned face images. Existing methods either require
time-consuming optimization for each face-identity or learning an efficient
encoder at the cost of harming the editability of models. In this work, we
present an optimization-free method for each face identity, meanwhile keeping
the editability for text-to-image models. Specifically, we propose a novel
face-identity encoder to learn an accurate representation of human faces, which
applies multi-scale face features followed by a multi-embedding projector to
directly generate the pseudo words in the text embedding space. Besides, we
propose self-augmented editability learning to enhance the editability of
models, which is achieved by constructing paired generated face and edited face
images using celebrity names, aiming at transferring mature ability of
off-the-shelf text-to-image models in celebrity faces to unseen faces.
Extensive experiments show that our methods can generate identity-preserved
images under different scenes at a much faster speed.",None,-1
0c5a3bc4-2d64-4282-9ef2-1c7fc26a87ed,Empathetic Response Generation via Emotion Cause Transition Graph,0.53496,9,"Empathetic dialogue is a human-like behavior that requires the perception of
both affective factors (e.g., emotion status) and cognitive factors (e.g.,
cause of the emotion). Besides concerning emotion status in early work, the
latest approaches study emotion causes in empathetic dialogue. These approaches
focus on understanding and duplicating emotion causes in the context to show
empathy for the speaker. However, instead of only repeating the contextual
causes, the real empathic response often demonstrate a logical and
emotion-centered transition from the causes in the context to those in the
responses. In this work, we propose an emotion cause transition graph to
explicitly model the natural transition of emotion causes between two adjacent
turns in empathetic dialogue. With this graph, the concept words of the emotion
causes in the next turn can be predicted and used by a specifically designed
concept-aware decoder to generate the empathic response. Automatic and human
experimental results on the benchmark dataset demonstrate that our method
produces more empathetic, coherent, informative, and specific responses than
existing models.",None,-1
14660284-1109-457b-91dd-cd32cf1d7760,CNN-based Methods for Object Recognition with High-Resolution Tactile Sensors,0.644145,73,"Novel high-resolution pressure-sensor arrays allow treating pressure readings
as standard images. Computer vision algorithms and methods such as
Convolutional Neural Networks (CNN) can be used to identify contact objects. In
this paper, a high-resolution tactile sensor has been attached to a robotic
end-effector to identify contacted objects. Two CNN-based approaches have been
employed to classify pressure images. These methods include a transfer learning
approach using a pre-trained CNN on an RGB-images dataset and a custom-made CNN
(TactNet) trained from scratch with tactile information. The transfer learning
approach can be carried out by retraining the classification layers of the
network or replacing these layers with an SVM. Overall, 11 configurations based
on these methods have been tested: 8 transfer learning-based, and 3
TactNet-based. Moreover, a study of the performance of the methods and a
comparative discussion with the current state-of-the-art on tactile object
recognition is presented.",None,-1
a63bbd39-05e9-433b-a495-7d70710272d6,Learning to Optimize for Reinforcement Learning,0.0574946,2,"In recent years, by leveraging more data, computation, and diverse tasks,
learned optimizers have achieved remarkable success in supervised learning,
outperforming classical hand-designed optimizers. Reinforcement learning (RL)
is essentially different from supervised learning, and in practice, these
learned optimizers do not work well even in simple RL tasks. We investigate
this phenomenon and identify two issues. First, the agent-gradient distribution
is non-independent and identically distributed, leading to inefficient
meta-training. Moreover, due to highly stochastic agent-environment
interactions, the agent-gradients have high bias and variance, which increases
the difficulty of learning an optimizer for RL. We propose pipeline training
and a novel optimizer structure with a good inductive bias to address these
issues, making it possible to learn an optimizer for reinforcement learning
from scratch. We show that, although only trained in toy tasks, our learned
optimizer can generalize to unseen complex tasks in Brax.",None,-1
38114a4f-8ba7-4a77-9bb7-3299cfcf7add,ETran: Energy-Based Transferability Estimation,0.738163,4,"This paper addresses the problem of ranking pre-trained models for object
detection and image classification. Selecting the best pre-trained model by
fine-tuning is an expensive and time-consuming task. Previous works have
proposed transferability estimation based on features extracted by the
pre-trained models. We argue that quantifying whether the target dataset is
in-distribution (IND) or out-of-distribution (OOD) for the pre-trained model is
an important factor in the transferability estimation. To this end, we propose
ETran, an energy-based transferability assessment metric, which includes three
scores: 1) energy score, 2) classification score, and 3) regression score. We
use energy-based models to determine whether the target dataset is OOD or IND
for the pre-trained model. In contrast to the prior works, ETran is applicable
to a wide range of tasks including classification, regression, and object
detection (classification+regression). This is the first work that proposes
transferability estimation for object detection task. Our extensive experiments
on four benchmarks and two tasks show that ETran outperforms previous works on
object detection and classification benchmarks by an average of 21% and 12%,
respectively, and achieves SOTA in transferability assessment.",None,-1
347cea89-ed3f-4bf8-b77e-a820ba822d41,A Novel Multi-scale Attention Feature Extraction Block for Aerial Remote Sensing Image Classification,0.313259,1,"Classification of very high-resolution (VHR) aerial remote sensing (RS)
images is a well-established research area in the remote sensing community as
it provides valuable spatial information for decision-making. Existing works on
VHR aerial RS image classification produce an excellent classification
performance; nevertheless, they have a limited capability to well-represent VHR
RS images having complex and small objects, thereby leading to performance
instability. As such, we propose a novel plug-and-play multi-scale attention
feature extraction block (MSAFEB) based on multi-scale convolution at two
levels with skip connection, producing discriminative/salient information at a
deeper/finer level. The experimental study on two benchmark VHR aerial RS image
datasets (AID and NWPU) demonstrates that our proposal achieves a
stable/consistent performance (minimum standard deviation of $0.002$) and
competent overall classification performance (AID: 95.85\% and NWPU: 94.09\%).",None,-1
f65c5823-c106-4512-811a-c3057e6c5df2,Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning,0.541887,1,"Adapting to regularities of the environment is critical for biological
organisms to anticipate events and plan. A prominent example is the circadian
rhythm corresponding to the internalization by organisms of the $24$-hour
period of the Earth's rotation. In this work, we study the emergence of
circadian-like rhythms in deep reinforcement learning agents. In particular, we
deployed agents in an environment with a reliable periodic variation while
solving a foraging task. We systematically characterize the agent's behavior
during learning and demonstrate the emergence of a rhythm that is endogenous
and entrainable. Interestingly, the internal rhythm adapts to shifts in the
phase of the environmental signal without any re-training. Furthermore, we show
via bifurcation and phase response curve analyses how artificial neurons
develop dynamics to support the internalization of the environmental rhythm.
From a dynamical systems view, we demonstrate that the adaptation proceeds by
the emergence of a stable periodic orbit in the neuron dynamics with a phase
response that allows an optimal phase synchronisation between the agent's
dynamics and the environmental rhythm.",None,-1
97a23dce-67bc-4eb8-a4f5-e4ac63b8b27d,Diffusion Models for Imperceptible and Transferable Adversarial Attack,0.831842,20,"Many existing adversarial attacks generate $L_p$-norm perturbations on image
RGB space. Despite some achievements in transferability and attack success
rate, the crafted adversarial examples are easily perceived by human eyes.
Towards visual imperceptibility, some recent works explore unrestricted attacks
without $L_p$-norm constraints, yet lacking transferability of attacking
black-box models. In this work, we propose a novel imperceptible and
transferable attack by leveraging both the generative and discriminative power
of diffusion models. Specifically, instead of direct manipulation in pixel
space, we craft perturbations in the latent space of diffusion models. Combined
with well-designed content-preserving structures, we can generate
human-insensitive perturbations embedded with semantic clues. For better
transferability, we further ""deceive"" the diffusion model which can be viewed
as an implicit recognition surrogate, by distracting its attention away from
the target regions. To our knowledge, our proposed method, DiffAttack, is the
first that introduces diffusion models into the adversarial attack field.
Extensive experiments on various model structures, datasets, and defense
methods have demonstrated the superiority of our attack over the existing
attack methods.",None,-1
a6b0917e-5653-4bae-b9aa-60b2b350aa3d,VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna,0.757909,6,"Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and
few-shot capabilities in Named Entity Recognition (NER). However, these models
can only be accessed via online APIs, which may cause data leak and
non-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot
NER framework based on the newly released open-source LLM -- Vicuna. VicunaNER
is a two-phase framework, where each phase leverages multi-turn dialogues with
Vicuna to recognize entities from texts. We name the second phase as
Re-Recognition, which recognizes those entities not recognized in the first
phase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues
in each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot
capacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.
Experimental results demonstrate that VicunaNER achieves superior performance
in both shot settings. Additionally, we conduct comprehensive investigations on
Vicuna from multiple perspectives.",None,-1
08347902-1d81-4cb2-afc9-b957e063082a,$R^3$-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL,0.864665,3,"While current tasks of converting natural language to SQL (NL2SQL) using
Foundation Models have shown impressive achievements, adapting these approaches
for converting natural language to Graph Query Language (NL2GQL) encounters
hurdles due to the distinct nature of GQL compared to SQL, alongside the
diverse forms of GQL. Moving away from traditional rule-based and slot-filling
methodologies, we introduce a novel approach, $R^3$-NL2GQL, integrating both
small and large Foundation Models for ranking, rewriting, and refining tasks.
This method leverages the interpretative strengths of smaller models for
initial ranking and rewriting stages, while capitalizing on the superior
generalization and query generation prowess of larger models for the final
transformation of natural language queries into GQL formats. Addressing the
scarcity of datasets in this emerging field, we have developed a bilingual
dataset, sourced from graph database manuals and selected open-source Knowledge
Graphs (KGs). Our evaluation of this methodology on this dataset demonstrates
its promising efficacy and robustness.",None,-1
660f9f13-f18a-4789-bd01-88fd82a7ec75,Zero-shot Generation of Training Data with Denoising Diffusion Probabilistic Model for Handwritten Chinese Character Recognition,0.993307,7,"There are more than 80,000 character categories in Chinese while most of them
are rarely used. To build a high performance handwritten Chinese character
recognition (HCCR) system supporting the full character set with a traditional
approach, many training samples need be collected for each character category,
which is both time-consuming and expensive. In this paper, we propose a novel
approach to transforming Chinese character glyph images generated from font
libraries to handwritten ones with a denoising diffusion probabilistic model
(DDPM). Training from handwritten samples of a small character set, the DDPM is
capable of mapping printed strokes to handwritten ones, which makes it possible
to generate photo-realistic and diverse style handwritten samples of unseen
character categories. Combining DDPM-synthesized samples of unseen categories
with real samples of other categories, we can build an HCCR system to support
the full character set. Experimental results on CASIA-HWDB dataset with 3,755
character categories show that the HCCR systems trained with synthetic samples
perform similarly with the one trained with real samples in terms of
recognition accuracy. The proposed method has the potential to address HCCR
with a larger vocabulary.",None,-1
0a0f2815-f4db-4f47-83fe-d2dfdde235f8,Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark,0.594047,31,"Recent model editing techniques promise to mitigate the problem of memorizing
false or outdated associations during LLM training. However, we show that these
techniques can introduce large unwanted side effects which are not detected by
existing specificity benchmarks. We extend the existing CounterFact benchmark
to include a dynamic component and dub our benchmark CounterFact+.
Additionally, we extend the metrics used for measuring specificity by a
principled KL divergence-based metric. We use this improved benchmark to
evaluate recent model editing techniques and find that they suffer from low
specificity. Our findings highlight the need for improved specificity
benchmarks that identify and prevent unwanted side effects.",None,-1
71480c08-5bc5-4f67-a131-f620606a98b3,LVRNet: Lightweight Image Restoration for Aerial Images under Low Visibility,0.267459,3,"Learning to recover clear images from images having a combination of
degrading factors is a challenging task. That being said, autonomous
surveillance in low visibility conditions caused by high pollution/smoke, poor
air quality index, low light, atmospheric scattering, and haze during a
blizzard becomes even more important to prevent accidents. It is thus crucial
to form a solution that can result in a high-quality image and is efficient
enough to be deployed for everyday use. However, the lack of proper datasets
available to tackle this task limits the performance of the previous methods
proposed. To this end, we generate the LowVis-AFO dataset, containing 3647
paired dark-hazy and clear images. We also introduce a lightweight deep
learning model called Low-Visibility Restoration Network (LVRNet). It
outperforms previous image restoration methods with low latency, achieving a
PSNR value of 25.744 and an SSIM of 0.905, making our approach scalable and
ready for practical use. The code and data can be found at
https://github.com/Achleshwar/LVRNet.",None,-1
36965a06-1b98-4ac9-9c50-9d5ac5875529,Multiple Thinking Achieving Meta-Ability Decoupling for Object Navigation,0.801062,7,"We propose a meta-ability decoupling (MAD) paradigm, which brings together
various object navigation methods in an architecture system, allowing them to
mutually enhance each other and evolve together. Based on the MAD paradigm, we
design a multiple thinking (MT) model that leverages distinct thinking to
abstract various meta-abilities. Our method decouples meta-abilities from three
aspects: input, encoding, and reward while employing the multiple thinking
collaboration (MTC) module to promote mutual cooperation between thinking. MAD
introduces a novel qualitative and quantitative interpretability system for
object navigation. Through extensive experiments on AI2-Thor and RoboTHOR, we
demonstrate that our method outperforms state-of-the-art (SOTA) methods on both
typical and zero-shot object navigation tasks.",None,-1
fbc5e1df-d440-4138-9c6e-ee5b53abe471,LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding,0.616426,5,"Visually-rich Document Understanding (VrDU) has attracted much research
attention over the past years. Pre-trained models on a large number of document
images with transformer-based backbones have led to significant performance
gains in this field. The major challenge is how to fusion the different
modalities (text, layout, and image) of the documents in a unified model with
different pre-training tasks. This paper focuses on improving text-layout
interactions and proposes a novel multi-modal pre-training model, LayoutMask.
LayoutMask uses local 1D position, instead of global 1D position, as layout
input and has two pre-training objectives: (1) Masked Language Modeling:
predicting masked tokens with two novel masking strategies; (2) Masked Position
Modeling: predicting masked 2D positions to improve layout representation
learning. LayoutMask can enhance the interactions between text and layout
modalities in a unified model and produce adaptive and robust multi-modal
representations for downstream tasks. Experimental results show that our
proposed method can achieve state-of-the-art results on a wide variety of VrDU
problems, including form understanding, receipt understanding, and document
image classification.",None,-1
97064003-7925-44f1-a06b-ade79b5453d4,Language Models as a Service: Overview of a New Paradigm and its Challenges,0.00852568,1,"Some of the most powerful language models currently are proprietary systems,
accessible only via (typically restrictive) web or software programming
interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm. In
contrast with scenarios where full model access is available, as in the case of
open-source models, such closed-off language models present specific challenges
for evaluating, benchmarking, and testing them. This paper has two goals: on
the one hand, we delineate how the aforementioned challenges act as impediments
to the accessibility, replicability, reliability, and trustworthiness of LMaaS.
We systematically examine the issues that arise from a lack of information
about language models for each of these four aspects. We conduct a detailed
analysis of existing solutions and put forth a number of considered
recommendations, and highlight the directions for future advancements. On the
other hand, it serves as a comprehensive resource for existing knowledge on
current, major LMaaS, offering a synthesized overview of the licences and
capabilities their interfaces offer.",None,-1
84e883f1-e4d8-40a7-8a77-853180ba3407,Auction-Based Scheduling,0.207711,1,"Many sequential decision-making tasks require satisfaction of multiple,
partially contradictory objectives. Existing approaches are monolithic, namely
all objectives are fulfilled using a single policy, which is a function that
selects a sequence of actions. We present auction-based scheduling, a modular
framework for multi-objective decision-making problems. Each objective is
fulfilled using a separate policy, and the policies can be independently
created, modified, and replaced. Understandably, different policies with
conflicting goals may choose conflicting actions at a given time. In order to
resolve conflicts, and compose policies, we employ a novel auction-based
mechanism. We allocate a bounded budget to each policy, and at each step, the
policies simultaneously bid from their available budgets for the privilege of
being scheduled and choosing an action. Policies express their scheduling
urgency using their bids and the bounded budgets ensure long-run scheduling
fairness. We lay the foundations of auction-based scheduling using path
planning problems on finite graphs with two temporal objectives. We present
decentralized algorithms to synthesize a pair of policies, their initially
allocated budgets, and bidding strategies. We consider three categories of
decentralized synthesis problems, parameterized by the assumptions that the
policies make on each other: (a) strong synthesis, with no assumptions and
strongest guarantees, (b) assume-admissible synthesis, with weakest rationality
assumptions, and (c) assume-guarantee synthesis, with explicit contract-based
assumptions. For reachability objectives, we show that, surprisingly,
decentralized assume-admissible synthesis is always possible when the
out-degrees of all vertices are at most two.",None,-1
5a120acf-fbee-443b-925e-298528f0ef1c,Compositional Probabilistic and Causal Inference using Tractable Circuit Models,0.461829,3,"Probabilistic circuits (PCs) are a class of tractable probabilistic models,
which admit efficient inference routines depending on their structural
properties. In this paper, we introduce md-vtrees, a novel structural
formulation of (marginal) determinism in structured decomposable PCs, which
generalizes previously proposed classes such as probabilistic sentential
decision diagrams. Crucially, we show how mdvtrees can be used to derive
tractability conditions and efficient algorithms for advanced inference queries
expressed as arbitrary compositions of basic probabilistic operations, such as
marginalization, multiplication and reciprocals, in a sound and generalizable
manner. In particular, we derive the first polytime algorithms for causal
inference queries such as backdoor adjustment on PCs. As a practical
instantiation of the framework, we propose MDNets, a novel PC architecture
using md-vtrees, and empirically demonstrate their application to causal
inference.",None,-1
4141ce9d-6717-42bd-8cd0-a3a775894ee3,Witscript 2: A System for Generating Improvised Jokes Without Wordplay,0.0621446,6,"A previous paper presented Witscript, a system for generating conversational
jokes that rely on wordplay. This paper extends that work by presenting
Witscript 2, which uses a large language model to generate conversational jokes
that rely on common sense instead of wordplay. Like Witscript, Witscript 2 is
based on joke-writing algorithms created by an expert comedy writer. Human
evaluators judged Witscript 2's responses to input sentences to be jokes 46% of
the time, compared to 70% of the time for human-written responses. This is
evidence that Witscript 2 represents another step toward giving a chatbot a
humanlike sense of humor.",None,-1
2be0d596-faf1-44d6-994d-990d991d0b3b,Back Translation for Speech-to-text Translation Without Transcripts,0.896315,9,"The success of end-to-end speech-to-text translation (ST) is often achieved
by utilizing source transcripts, e.g., by pre-training with automatic speech
recognition (ASR) and machine translation (MT) tasks, or by introducing
additional ASR and MT data. Unfortunately, transcripts are only sometimes
available since numerous unwritten languages exist worldwide. In this paper, we
aim to utilize large amounts of target-side monolingual data to enhance ST
without transcripts. Motivated by the remarkable success of back translation in
MT, we develop a back translation algorithm for ST (BT4ST) to synthesize pseudo
ST data from monolingual target data. To ease the challenges posed by
short-to-long generation and one-to-many mapping, we introduce self-supervised
discrete units and achieve back translation by cascading a target-to-unit model
and a unit-to-speech model. With our synthetic ST data, we achieve an average
boost of 2.3 BLEU on MuST-C En-De, En-Fr, and En-Es datasets. More experiments
show that our method is especially effective in low-resource scenarios.",None,-1
7dd69dd0-6dca-4616-aa9d-ed97543c82ba,Test-Time Adaptation with Perturbation Consistency Learning,0.0754629,1,"Currently, pre-trained language models (PLMs) do not cope well with the
distribution shift problem, resulting in models trained on the training set
failing in real test scenarios. To address this problem, the test-time
adaptation (TTA) shows great potential, which updates model parameters to suit
the test data at the testing time. Existing TTA methods rely on well-designed
auxiliary tasks or self-training strategies based on pseudo-label. However,
these methods do not achieve good trade-offs regarding performance gains and
computational costs. To obtain some insights into such a dilemma, we take two
representative TTA methods, i.e., Tent and OIL, for exploration and find that
stable prediction is the key to achieving a good balance. Accordingly, in this
paper, we propose perturbation consistency learning (PCL), a simple test-time
adaptation method to promote the model to make stable predictions for samples
with distribution shifts. Extensive experiments on adversarial robustness and
cross-lingual transferring demonstrate that our method can achieve higher or
comparable performance with less inference time over strong PLM backbones and
previous state-of-the-art TTA methods.",None,-1
10cab4d3-b2cf-43d5-8b1a-fe82745b3db1,YaRN: Efficient Context Window Extension of Large Language Models,0.564185,97,"Rotary Position Embeddings (RoPE) have been shown to effectively encode
positional information in transformer-based language models. However, these
models fail to generalize past the sequence length they were trained on. We
present YaRN (Yet another RoPE extensioN method), a compute-efficient method to
extend the context window of such models, requiring 10x less tokens and 2.5x
less training steps than previous methods. Using YaRN, we show that LLaMA
models can effectively utilize and extrapolate to context lengths much longer
than their original pre-training would allow, while also surpassing previous
the state-of-the-art at context window extension. In addition, we demonstrate
that YaRN exhibits the capability to extrapolate beyond the limited context of
a fine-tuning dataset. The models fine-tuned using YaRN has been made available
and reproduced online up to 128k context length at
https://github.com/jquesnelle/yarn",None,-1
2656fff6-f008-40e3-b0b5-4468938f5de2,Comparing Sentence-Level Suggestions to Message-Level Suggestions in AI-Mediated Communication,0.687926,11,"Traditionally, writing assistance systems have focused on short or even
single-word suggestions. Recently, large language models like GPT-3 have made
it possible to generate significantly longer natural-sounding suggestions,
offering more advanced assistance opportunities. This study explores the
trade-offs between sentence- vs. message-level suggestions for AI-mediated
communication. We recruited 120 participants to act as staffers from
legislators' offices who often need to respond to large volumes of constituent
concerns. Participants were asked to reply to emails with different types of
assistance. The results show that participants receiving message-level
suggestions responded faster and were more satisfied with the experience, as
they mainly edited the suggested drafts. In addition, the texts they wrote were
evaluated as more helpful by others. In comparison, participants receiving
sentence-level assistance retained a higher sense of agency, but took longer
for the task as they needed to plan the flow of their responses and decide when
to use suggestions. Our findings have implications for designing
task-appropriate communication assistance systems.",None,-1
0b991003-6e0d-478a-bbed-a0d60c34ed13,Agent Instructs Large Language Models to be General Zero-Shot Reasoners,0.436075,17,"We introduce a method to improve the zero-shot reasoning abilities of large
language models on general language understanding tasks. Specifically, we build
an autonomous agent to instruct the reasoning process of large language models.
We show this approach further unleashes the zero-shot reasoning abilities of
large language models to more tasks. We study the performance of our method on
a wide set of datasets spanning generation, classification, and reasoning. We
show that our method generalizes to most tasks and obtains state-of-the-art
zero-shot performance on 20 of the 29 datasets that we evaluate. For instance,
our method boosts the performance of state-of-the-art large language models by
a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and
GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement
in reasoning is striking, with an average increase of 10.5%. With our method,
Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.",None,-1
7497e0e3-e8af-4935-b34e-3ebc3dabf741,SVQNet: Sparse Voxel-Adjacent Query Network for 4D Spatio-Temporal LiDAR Semantic Segmentation,0.0912578,1,"LiDAR-based semantic perception tasks are critical yet challenging for
autonomous driving. Due to the motion of objects and static/dynamic occlusion,
temporal information plays an essential role in reinforcing perception by
enhancing and completing single-frame knowledge. Previous approaches either
directly stack historical frames to the current frame or build a 4D
spatio-temporal neighborhood using KNN, which duplicates computation and
hinders realtime performance. Based on our observation that stacking all the
historical points would damage performance due to a large amount of redundant
and misleading information, we propose the Sparse Voxel-Adjacent Query Network
(SVQNet) for 4D LiDAR semantic segmentation. To take full advantage of the
historical frames high-efficiently, we shunt the historical points into two
groups with reference to the current points. One is the Voxel-Adjacent
Neighborhood carrying local enhancing knowledge. The other is the Historical
Context completing the global knowledge. Then we propose new modules to select
and extract the instructive features from the two groups. Our SVQNet achieves
state-of-the-art performance in LiDAR semantic segmentation of the
SemanticKITTI benchmark and the nuScenes dataset.",None,-1
6053e6ca-1afe-47d4-be4e-00a3bb129156,Data Augmentation in Training CNNs: Injecting Noise to Images,0.0786989,12,"Noise injection is a fundamental tool for data augmentation, and yet there is
no widely accepted procedure to incorporate it with learning frameworks. This
study analyzes the effects of adding or applying different noise models of
varying magnitudes to Convolutional Neural Network (CNN) architectures. Noise
models that are distributed with different density functions are given common
magnitude levels via Structural Similarity (SSIM) metric in order to create an
appropriate ground for comparison. The basic results are conforming with the
most of the common notions in machine learning, and also introduce some novel
heuristics and recommendations on noise injection. The new approaches will
provide better understanding on optimal learning procedures for image
classification.",None,-1
29aab42d-df17-4a83-869b-7547ae66c8f9,Polynomial Neural Fields for Subband Decomposition and Manipulation,0.346051,13,"Neural fields have emerged as a new paradigm for representing signals, thanks
to their ability to do it compactly while being easy to optimize. In most
applications, however, neural fields are treated like black boxes, which
precludes many signal manipulation tasks. In this paper, we propose a new class
of neural fields called polynomial neural fields (PNFs). The key advantage of a
PNF is that it can represent a signal as a composition of a number of
manipulable and interpretable components without losing the merits of neural
fields representation. We develop a general theoretical framework to analyze
and design PNFs. We use this framework to design Fourier PNFs, which match
state-of-the-art performance in signal representation tasks that use neural
fields. In addition, we empirically demonstrate that Fourier PNFs enable signal
manipulation applications such as texture transfer and scale-space
interpolation. Code is available at https://github.com/stevenygd/PNF.",None,-1
74df6861-504a-4178-a27d-7f1e99515462,Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data,0.612433,3,"Foundation models are trained on vast amounts of data at scale using
self-supervised learning, enabling adaptation to a wide range of downstream
tasks. At test time, these models exhibit zero-shot capabilities through which
they can classify previously unseen (user-specified) categories. In this paper,
we address the problem of quantifying uncertainty in these zero-shot
predictions. We propose a heuristic approach for uncertainty estimation in
zero-shot settings using conformal prediction with web data. Given a set of
classes at test time, we conduct zero-shot classification with CLIP-style
models using a prompt template, e.g., ""an image of a <category>"", and use the
same template as a search query to source calibration data from the open web.
Given a web-based calibration set, we apply conformal prediction with a novel
conformity score that accounts for potential errors in retrieved web data. We
evaluate the utility of our proposed method in Biomedical foundation models;
our preliminary results show that web-based conformal prediction sets achieve
the target coverage with satisfactory efficiency on a variety of biomedical
datasets.",None,-1
386ae091-364f-4ad7-96a6-e0dcc402e31f,Towards Unifying Multi-Lingual and Cross-Lingual Summarization,0.951642,13,"To adapt text summarization to the multilingual world, previous work proposes
multi-lingual summarization (MLS) and cross-lingual summarization (CLS).
However, these two tasks have been studied separately due to the different
definitions, which limits the compatible and systematic research on both of
them. In this paper, we aim to unify MLS and CLS into a more general setting,
i.e., many-to-many summarization (M2MS), where a single model could process
documents in any language and generate their summaries also in any language. As
the first step towards M2MS, we conduct preliminary studies to show that M2MS
can better transfer task knowledge across different languages than MLS and CLS.
Furthermore, we propose Pisces, a pre-trained M2MS model that learns language
modeling, cross-lingual ability and summarization ability via three-stage
pre-training. Experimental results indicate that our Pisces significantly
outperforms the state-of-the-art baselines, especially in the zero-shot
directions, where there is no training data from the source-language documents
to the target-language summaries.",None,-1
73a86bf5-b6f9-4e39-81a8-9e17cbf33179,Salient Span Masking for Temporal Understanding,0.589817,7,"Salient Span Masking (SSM) has shown itself to be an effective strategy to
improve closed-book question answering performance. SSM extends general masked
language model pretraining by creating additional unsupervised training
sentences that mask a single entity or date span, thus oversampling factual
information. Despite the success of this paradigm, the span types and sampling
strategies are relatively arbitrary and not widely studied for other tasks.
Thus, we investigate SSM from the perspective of temporal tasks, where learning
a good representation of various temporal expressions is important. To that
end, we introduce Temporal Span Masking (TSM) intermediate training. First, we
find that SSM alone improves the downstream performance on three temporal tasks
by an avg. +5.8 points. Further, we are able to achieve additional improvements
(avg. +0.29 points) by adding the TSM task. These comprise the new best
reported results on the targeted tasks. Our analysis suggests that the
effectiveness of SSM stems from the sentences chosen in the training data
rather than the mask choice: sentences with entities frequently also contain
temporal expressions. Nonetheless, the additional targeted spans of TSM can
still improve performance, especially in a zero-shot context.",None,-1
21997a23-1da1-4f83-b141-4f0f20ed5a89,Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs,0.936479,6,"Though prompting LLMs with various reasoning structures produces reasoning
proofs along with answers, these proofs are not ensured to be causal and
reliable due to the inherent defects of LLMs. Tracking such deficiencies, we
present a neuro-symbolic integration method, in which a neural LLM is used to
represent the knowledge of the problem while an LLM-free symbolic solver is
adopted to do deliberative reasoning using the knowledge. Specifically, our
customized meta-interpreters allow the production of reasoning proofs and
support flexible search strategies. These reasoning proofs are ensured to be
causal and reliable because of the deterministic executing nature of the
symbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT
baseline by nearly double in accuracy and more than triple in proof similarity.
On GSM8K, our method also shows accuracy improvements and nearly doubled proof
similarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing",None,-1
d4fd22fc-c9db-4a1b-b557-d4932138e9b2,Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization,0.902196,8,"Weakly-supervised temporal action localization aims to localize and recognize
actions in untrimmed videos with only video-level category labels during
training. Without instance-level annotations, most existing methods follow the
Segment-based Multiple Instance Learning (S-MIL) framework, where the
predictions of segments are supervised by the labels of videos. However, the
objective for acquiring segment-level scores during training is not consistent
with the target for acquiring proposal-level scores during testing, leading to
suboptimal results. To deal with this problem, we propose a novel
Proposal-based Multiple Instance Learning (P-MIL) framework that directly
classifies the candidate proposals in both the training and testing stages,
which includes three key designs: 1) a surrounding contrastive feature
extraction module to suppress the discriminative short proposals by considering
the surrounding contrastive information, 2) a proposal completeness evaluation
module to inhibit the low-quality proposals with the guidance of the
completeness pseudo labels, and 3) an instance-level rank consistency loss to
achieve robust detection by leveraging the complementarity of RGB and FLOW
modalities. Extensive experimental results on two challenging benchmarks
including THUMOS14 and ActivityNet demonstrate the superior performance of our
method.",None,-1
22782646-dab2-4b81-8179-393168161e81,SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning,0.973693,34,"This study presents a thorough examination of various Generative Pretrained
Transformer (GPT) methodologies in sentiment analysis, specifically in the
context of Task 4 on the SemEval 2017 dataset. Three primary strategies are
employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2)
fine-tuning GPT models, and 3) an inventive approach to embedding
classification. The research yields detailed comparative insights among these
strategies and individual GPT models, revealing their unique strengths and
potential limitations. Additionally, the study compares these GPT-based
methodologies with other current, high-performing models previously used with
the same dataset. The results illustrate the significant superiority of the GPT
approaches in terms of predictive performance, more than 22\% in F1-score
compared to the state-of-the-art. Further, the paper sheds light on common
challenges in sentiment analysis tasks, such as understanding context and
detecting sarcasm. It underscores the enhanced capabilities of the GPT models
to effectively handle these complexities. Taken together, these findings
highlight the promising potential of GPT models in sentiment analysis, setting
the stage for future research in this field. The code can be found at
https://github.com/DSAatUSU/SentimentGPT",None,-1
276dae41-9277-46c4-8f39-4361f350f5c0,The Evolutionary Computation Methods No One Should Use,0.346538,3,"The center-bias (or zero-bias) operator has recently been identified as one
of the problems plaguing the benchmarking of evolutionary computation methods.
This operator lets the methods that utilize it easily optimize functions that
have their respective optima in the center of the feasible set. In this paper,
we describe a simple procedure that can be used to identify methods that
incorporate a center-bias operator and use it to investigate 90 evolutionary
computation methods that were published between 1987 and 2022. We show that
more than half (47 out of the 90) of the considered methods have the
center-bias problem. We also show that the center-bias is a relatively new
phenomenon (with the first identified method being from 2012), but its
inclusion has become extremely prevalent in the last few years. Lastly, we
briefly discuss the possible root causes of this issue.",None,-1
2c37c462-926e-4c28-939d-64a02b5823aa,Improving Synthetically Generated Image Detection in Cross-Concept Settings,0.60012,14,"New advancements for the detection of synthetic images are critical for
fighting disinformation, as the capabilities of generative AI models
continuously evolve and can lead to hyper-realistic synthetic imagery at
unprecedented scale and speed. In this paper, we focus on the challenge of
generalizing across different concept classes, e.g., when training a detector
on human faces and testing on synthetic animal images - highlighting the
ineffectiveness of existing approaches that randomly sample generated images to
train their models. By contrast, we propose an approach based on the premise
that the robustness of the detector can be enhanced by training it on realistic
synthetic images that are selected based on their quality scores according to a
probabilistic quality estimation model. We demonstrate the effectiveness of the
proposed approach by conducting experiments with generated images from two
seminal architectures, StyleGAN2 and Latent Diffusion, and using three
different concepts for each, so as to measure the cross-concept generalization
ability. Our results show that our quality-based sampling method leads to
higher detection performance for nearly all concepts, improving the overall
effectiveness of the synthetic image detectors.",None,-1
327ad7ac-0b10-4062-a434-572149ca2d1c,Causal Inference in Gene Regulatory Networks with GFlowNet: Towards Scalability in Large Systems,0.37019,1,"Understanding causal relationships within Gene Regulatory Networks (GRNs) is
essential for unraveling the gene interactions in cellular processes. However,
causal discovery in GRNs is a challenging problem for multiple reasons
including the existence of cyclic feedback loops and uncertainty that yields
diverse possible causal structures. Previous works in this area either ignore
cyclic dynamics (assume acyclic structure) or struggle with scalability. We
introduce Swift-DynGFN as a novel framework that enhances causal structure
learning in GRNs while addressing scalability concerns. Specifically,
Swift-DynGFN exploits gene-wise independence to boost parallelization and to
lower computational cost. Experiments on real single-cell RNA velocity and
synthetic GRN datasets showcase the advancement in learning causal structure in
GRNs and scalability in larger systems.",None,-1
b590baac-00e6-4b34-b3fd-97b891ebb55f,On Evaluating Multilingual Compositional Generalization with Translated Datasets,0.341653,3,"Compositional generalization allows efficient learning and human-like
inductive biases. Since most research investigating compositional
generalization in NLP is done on English, important questions remain
underexplored. Do the necessary compositional generalization abilities differ
across languages? Can models compositionally generalize cross-lingually? As a
first step to answering these questions, recent work used neural machine
translation to translate datasets for evaluating compositional generalization
in semantic parsing. However, we show that this entails critical semantic
distortion. To address this limitation, we craft a faithful rule-based
translation of the MCWQ dataset from English to Chinese and Japanese. Even with
the resulting robust benchmark, which we call MCWQ-R, we show that the
distribution of compositions still suffers due to linguistic divergences, and
that multilingual models still struggle with cross-lingual compositional
generalization. Our dataset and methodology will be useful resources for the
study of cross-lingual compositional generalization in other tasks.",None,-1
17fd6e25-1807-4e8b-92ed-adccd4fec514,Fairness in Language Models Beyond English: Gaps and Challenges,0.502857,15,"With language models becoming increasingly ubiquitous, it has become
essential to address their inequitable treatment of diverse demographic groups
and factors. Most research on evaluating and mitigating fairness harms has been
concentrated on English, while multilingual models and non-English languages
have received comparatively little attention. This paper presents a survey of
fairness in multilingual and non-English contexts, highlighting the
shortcomings of current research and the difficulties faced by methods designed
for English. We contend that the multitude of diverse cultures and languages
across the world makes it infeasible to achieve comprehensive coverage in terms
of constructing fairness datasets. Thus, the measurement and mitigation of
biases must evolve beyond the current dataset-driven practices that are
narrowly focused on specific dimensions and types of biases and, therefore,
impossible to scale across languages and cultures.",None,-1
77ea5df1-340a-4c2f-be76-f833066fa366,GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning,0.627006,6,"This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared
task, encompassing both subtask A and subtask B. We approach the task as a
dialogue summarization problem and implement two distinct pipelines: (a) a
fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b)
few-shot in-context learning (ICL) using a large language model, GPT-4. Both
methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1
(deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421,
respectively. Additionally, we predict the associated section headers using
RoBERTa and SciBERT based classification models. Our team ranked fourth among
all teams, while each team is allowed to submit three runs as part of their
submission. We also utilize expert annotations to demonstrate that the notes
generated through the ICL GPT-4 are better than all other baselines. The code
for our submission is available.",None,-1
2e595c59-3124-4846-9527-5b4d5a390624,Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models,0.475877,2,"Recent work in speech-to-speech translation (S2ST) has focused primarily on
offline settings, where the full input utterance is available before any output
is given. This, however, is not reasonable in many real-world scenarios. In
latency-sensitive applications, rather than waiting for the full utterance,
translations should be spoken as soon as the information in the input is
present. In this work, we introduce a system for simultaneous S2ST targeting
real-world use cases. Our system supports translation from 57 languages to
English with tunable parameters for dynamically adjusting the latency of the
output -- including four policies for determining when to speak an output
sequence. We show that these policies achieve offline-level accuracy with
minimal increases in latency over a Greedy (wait-$k$) baseline. We open-source
our evaluation code and interactive test script to aid future SimulS2ST
research and application development.",None,-1
f054f2c5-eda7-42bd-9f14-5551435aae55,Implicit State and Goals in QBF Encodings for Positional Games (extended version),0.797707,4,"We address two bottlenecks for concise QBF encodings of maker-breaker
positional games, like Hex and Tic-Tac-Toe. Our baseline is a QBF encoding with
explicit variables for board positions and an explicit representation of
winning configurations. The first improvement is inspired by lifted planning
and avoids variables for explicit board positions, introducing a universal
quantifier representing a symbolic board state. The second improvement
represents the winning configurations implicitly, exploiting their structure.
The paper evaluates the size of several encodings, depending on board size and
game depth. It also reports the performance of QBF solvers on these encodings.
We evaluate the techniques on Hex instances and also apply them to Harary's
Tic-Tac-Toe. In particular, we study scalability to 19$\times$19 boards, played
in human Hex tournaments.",None,-1
310a657b-04d9-4a82-a4ee-79c034f0aa31,OmniTracker: Unifying Object Tracking by Tracking-with-Detection,0.585013,9,"Object tracking (OT) aims to estimate the positions of target objects in a
video sequence. Depending on whether the initial states of target objects are
specified by provided annotations in the first frame or the categories, OT
could be classified as instance tracking (e.g., SOT and VOS) and category
tracking (e.g., MOT, MOTS, and VIS) tasks. Combing the advantages of the best
practices developed in both communities, we propose a novel
tracking-with-detection paradigm, where tracking supplements appearance priors
for detection and detection provides tracking with candidate bounding boxes for
association. Equipped with such a design, a unified tracking model,
OmniTracker, is further presented to resolve all the tracking tasks with a
fully shared network architecture, model weights, and inference pipeline.
Extensive experiments on 7 tracking datasets, including LaSOT, TrackingNet,
DAVIS16-17, MOT17, MOTS20, and YTVIS19, demonstrate that OmniTracker achieves
on-par or even better results than both task-specific and unified tracking
models.",None,-1
9485cbe0-8f90-4579-82fb-cd7698560053,The Algonauts Project 2023 Challenge: UARK-UAlbany Team Solution,0.508964,4,"This work presents our solutions to the Algonauts Project 2023 Challenge. The
primary objective of the challenge revolves around employing computational
models to anticipate brain responses captured during participants' observation
of intricate natural visual scenes. The goal is to predict brain responses
across the entire visual brain, as it is the region where the most reliable
responses to images have been observed. We constructed an image-based brain
encoder through a two-step training process to tackle this challenge.
Initially, we created a pretrained encoder using data from all subjects. Next,
we proceeded to fine-tune individual subjects. Each step employed different
training strategies, such as different loss functions and objectives, to
introduce diversity. Ultimately, our solution constitutes an ensemble of
multiple unique encoders. The code is available at
https://github.com/uark-cviu/Algonauts2023",None,-1
be47d1ea-cea8-4a47-91bb-b133529f8c87,Grand Challenge On Detecting Cheapfakes,0.0942796,4,"Cheapfake is a recently coined term that encompasses non-AI (""cheap"")
manipulations of multimedia content. Cheapfakes are known to be more prevalent
than deepfakes. Cheapfake media can be created using editing software for
image/video manipulations, or even without using any software, by simply
altering the context of an image/video by sharing the media alongside
misleading claims. This alteration of context is referred to as out-of-context
(OOC) misuse of media. OOC media is much harder to detect than fake media,
since the images and videos are not tampered. In this challenge, we focus on
detecting OOC images, and more specifically the misuse of real photographs with
conflicting image captions in news items. The aim of this challenge is to
develop and benchmark models that can be used to detect whether given samples
(news image and associated captions) are OOC, based on the recently compiled
COSMOS dataset.",None,-1
1ec776ee-ff0d-41ec-8981-1cd43f105748,"Physics of Language Models: Part 3.2, Knowledge Manipulation",0.999646,35,"Language models can store vast amounts of factual knowledge, but their
ability to use this knowledge for logical reasoning remains questionable. This
paper explores a language model's ability to manipulate its stored knowledge
during inference. We focus on four manipulation types: retrieval (e.g., ""What
is person A's attribute X""), classification (e.g., ""Is A's attribute X even or
odd?""), comparison (e.g., ""Is A greater than B in attribute X?"") and inverse
search (e.g., ""Which person's attribute X equals T?"")
  We observe that pre-trained language models like GPT2/3/4 excel in knowledge
retrieval but struggle with simple classification or comparison tasks unless
Chain of Thoughts (CoTs) are employed during both training and inference. They
also perform poorly in inverse knowledge search, irrespective of the prompts.
Our primary contribution is a synthetic dataset for a controlled experiment
that confirms these inherent weaknesses: a language model cannot efficiently
manipulate knowledge from pre-training data, even when such knowledge is
perfectly stored and fully extractable in the models, and despite adequate
instruct fine-tuning.",None,-1
69bc9c74-1fee-4eee-a3ee-140a4e5f6e05,Implicit Stacked Autoregressive Model for Video Prediction,0.656266,7,"Future frame prediction has been approached through two primary methods:
autoregressive and non-autoregressive. Autoregressive methods rely on the
Markov assumption and can achieve high accuracy in the early stages of
prediction when errors are not yet accumulated. However, their performance
tends to decline as the number of time steps increases. In contrast,
non-autoregressive methods can achieve relatively high performance but lack
correlation between predictions for each time step. In this paper, we propose
an Implicit Stacked Autoregressive Model for Video Prediction (IAM4VP), which
is an implicit video prediction model that applies a stacked autoregressive
method. Like non-autoregressive methods, stacked autoregressive methods use the
same observed frame to estimate all future frames. However, they use their own
predictions as input, similar to autoregressive methods. As the number of time
steps increases, predictions are sequentially stacked in the queue. To evaluate
the effectiveness of IAM4VP, we conducted experiments on three common future
frame prediction benchmark datasets and weather\&climate prediction benchmark
datasets. The results demonstrate that our proposed model achieves
state-of-the-art performance.",None,-1
1989f249-f610-44d5-8bf2-2ba4a046522b,"More Synergy, Less Redundancy: Exploiting Joint Mutual Information for Self-Supervised Learning",0.194958,4,"Self-supervised learning (SSL) is now a serious competitor for supervised
learning, even though it does not require data annotation. Several baselines
have attempted to make SSL models exploit information about data distribution,
and less dependent on the augmentation effect. However, there is no clear
consensus on whether maximizing or minimizing the mutual information between
representations of augmentation views practically contribute to improvement or
degradation in performance of SSL models. This paper is a fundamental work
where, we investigate role of mutual information in SSL, and reformulate the
problem of SSL in the context of a new perspective on mutual information. To
this end, we consider joint mutual information from the perspective of partial
information decomposition (PID) as a key step in \textbf{reliable multivariate
information measurement}. PID enables us to decompose joint mutual information
into three important components, namely, unique information, redundant
information and synergistic information. Our framework aims for minimizing the
redundant information between views and the desired target representation while
maximizing the synergistic information at the same time. Our experiments lead
to a re-calibration of two redundancy reduction baselines, and a proposal for a
new SSL training protocol. Extensive experimental results on multiple datasets
and two downstream tasks show the effectiveness of this framework.",None,-1
d47d67bb-9955-4067-adae-ca2e250ed500,Affinity-based Attention in Self-supervised Transformers Predicts Dynamics of Object Grouping in Humans,0.314388,5,"The spreading of attention has been proposed as a mechanism for how humans
group features to segment objects. However, such a mechanism has not yet been
implemented and tested in naturalistic images. Here, we leverage the feature
maps from self-supervised vision Transformers and propose a model of human
object-based attention spreading and segmentation. Attention spreads within an
object through the feature affinity signal between different patches of the
image. We also collected behavioral data on people grouping objects in natural
images by judging whether two dots are on the same object or on two different
objects. We found that our models of affinity spread that were built on feature
maps from the self-supervised Transformers showed significant improvement over
baseline and CNN based models on predicting reaction time patterns of humans,
despite not being trained on the task or with any other object labels. Our work
provides new benchmarks for evaluating models of visual representation learning
including Transformers.",None,-1
2f555f0b-95d4-4655-858c-38c89500aa68,Efficient Explainable Face Verification based on Similarity Score Argument Backpropagation,0.710283,5,"Explainable Face Recognition is gaining growing attention as the use of the
technology is gaining ground in security-critical applications. Understanding
why two faces images are matched or not matched by a given face recognition
system is important to operators, users, anddevelopers to increase trust,
accountability, develop better systems, and highlight unfair behavior. In this
work, we propose xSSAB, an approach to back-propagate similarity score-based
arguments that support or oppose the face matching decision to visualize
spatial maps that indicate similar and dissimilar areas as interpreted by the
underlying FR model. Furthermore, we present Patch-LFW, a new explainable face
verification benchmark that enables along with a novel evaluation protocol, the
first quantitative evaluation of the validity of similarity and dissimilarity
maps in explainable face recognition approaches. We compare our efficient
approach to state-of-the-art approaches demonstrating a superior trade-off
between efficiency and performance. The code as well as the proposed Patch-LFW
is publicly available at: https://github.com/marcohuber/xSSAB.",None,-1
2ba4bb46-4738-4eec-8c67-fcc623e3f93d,SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge,0.59387,9,"Large Language Models (LLMs) have demonstrated impressive planning abilities
due to their vast ""world knowledge"". Yet, obtaining plans that are both
feasible (grounded in affordances) and cost-effective (in plan length), remains
a challenge, despite recent progress. This contrasts with heuristic planning
methods that employ domain knowledge (formalized in action models such as PDDL)
and heuristic search to generate feasible, optimal plans. Inspired by this, we
propose to combine the power of LLMs and heuristic planning by leveraging the
world knowledge of LLMs and the principles of heuristic search. Our approach,
SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain
knowledge, that evaluates actions' feasibility (Can) and long-term
reward/payoff (Pay), and heuristic search to select the best sequence of
actions. Our contributions are (1) a novel framing of the LLM planning problem
in the context of heuristic planning, (2) integrating grounding and
cost-effective elements into the generated plans, and (3) using heuristic
search over actions. Our extensive evaluations show that our model surpasses
other LLM planning approaches.",None,-1
27c49260-c9c4-46c5-81e2-ed883228347a,ABC: Attention with Bilinear Correlation for Infrared Small Target Detection,0.684789,6,"Infrared small target detection (ISTD) has a wide range of applications in
early warning, rescue, and guidance. However, CNN based deep learning methods
are not effective at segmenting infrared small target (IRST) that it lack of
clear contour and texture features, and transformer based methods also struggle
to achieve significant results due to the absence of convolution induction
bias. To address these issues, we propose a new model called attention with
bilinear correlation (ABC), which is based on the transformer architecture and
includes a convolution linear fusion transformer (CLFT) module with a novel
attention mechanism for feature extraction and fusion, which effectively
enhances target features and suppresses noise. Additionally, our model includes
a u-shaped convolution-dilated convolution (UCDC) module located deeper layers
of the network, which takes advantage of the smaller resolution of deeper
features to obtain finer semantic information. Experimental results on public
datasets demonstrate that our approach achieves state-of-the-art performance.
Code is available at https://github.com/PANPEIWEN/ABC",None,-1
00df4005-587a-40a2-a9fe-421786476256,Anaphor Assisted Document-Level Relation Extraction,0.681559,2,"Document-level relation extraction (DocRE) involves identifying relations
between entities distributed in multiple sentences within a document. Existing
methods focus on building a heterogeneous document graph to model the internal
structure of an entity and the external interaction between entities. However,
there are two drawbacks in existing methods. On one hand, anaphor plays an
important role in reasoning to identify relations between entities but is
ignored by these methods. On the other hand, these methods achieve
cross-sentence entity interactions implicitly by utilizing a document or
sentences as intermediate nodes. Such an approach has difficulties in learning
fine-grained interactions between entities across different sentences,
resulting in sub-optimal performance. To address these issues, we propose an
Anaphor-Assisted (AA) framework for DocRE tasks. Experimental results on the
widely-used datasets demonstrate that our model achieves a new state-of-the-art
performance.",None,-1
68477276-63dc-4335-9356-da50c9aa0ee9,Towards Diverse and Effective Question-Answer Pair Generation from Children Storybooks,0.195664,3,"Recent advances in QA pair generation (QAG) have raised interest in applying
this technique to the educational field. However, the diversity of QA types
remains a challenge despite its contributions to comprehensive learning and
assessment of children. In this paper, we propose a QAG framework that enhances
QA type diversity by producing different interrogative sentences and
implicit/explicit answers. Our framework comprises a QFS-based answer
generator, an iterative QA generator, and a relevancy-aware ranker. The two
generators aim to expand the number of candidates while covering various types.
The ranker trained on the in-context negative samples clarifies the top-N
outputs based on the ranking score. Extensive evaluations and detailed analyses
demonstrate that our approach outperforms previous state-of-the-art results by
significant margins, achieving improved diversity and quality. Our
task-oriented processes are consistent with real-world demand, which highlights
our system's high applicability.",None,-1
712934e8-7bd9-463f-961f-b0716c7be5c0,Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models,0.109312,4,"Recent studies have revealed that the widely-used Pre-trained Language Models
(PLMs) propagate societal biases from the large unmoderated pre-training
corpora. Existing solutions require debiasing training processes and datasets
for debiasing, which are resource-intensive and costly. Furthermore, these
methods hurt the PLMs' performance on downstream tasks. In this study, we
propose Gender-tuning, which debiases the PLMs through fine-tuning on
downstream tasks' datasets. For this aim, Gender-tuning integrates Masked
Language Modeling (MLM) training objectives into fine-tuning's training
process. Comprehensive experiments show that Gender-tuning outperforms the
state-of-the-art baselines in terms of average gender bias scores in PLMs while
improving PLMs' performance on downstream tasks solely using the downstream
tasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM
that works with original fine-tuning.",None,-1
eedbedfb-0336-4df6-8f3f-65e917adaf9f,Flows: Building Blocks of Reasoning and Collaborating AI,0.807317,11,"Recent advances in artificial intelligence (AI) have produced highly capable
and controllable systems. This creates unprecedented opportunities for
structured reasoning as well as collaboration among multiple AI systems and
humans. To fully realize this potential, it is essential to develop a
principled way of designing and studying such structured interactions. For this
purpose, we introduce the conceptual framework Flows. Flows are self-contained
building blocks of computation, with an isolated state, communicating through a
standardized message-based interface. This modular design simplifies the
process of creating Flows by allowing them to be recursively composed into
arbitrarily nested interactions and is inherently concurrency-friendly.
Crucially, any interaction can be implemented using this framework, including
prior work on AI-AI and human-AI interactions, prompt engineering schemes, and
tool augmentation. We demonstrate the potential of Flows on competitive coding,
a challenging task on which even GPT-4 struggles. Our results suggest that
structured reasoning and collaboration substantially improve generalization,
with AI-only Flows adding +21 and human-AI Flows adding +54 absolute points in
terms of solve rate. To support rapid and rigorous research, we introduce the
aiFlows library embodying Flows. The aiFlows library is available at
https://github.com/epfl-dlab/aiflows. Data and Flows for reproducing our
experiments are available at https://github.com/epfl-dlab/cc_flows.",None,-1
3c8e227c-8a13-4c87-a9c9-efc0535626f7,Latent Feature Relation Consistency for Adversarial Robustness,0.181171,2,"Deep neural networks have been applied in many computer vision tasks and
achieved state-of-the-art performance. However, misclassification will occur
when DNN predicts adversarial examples which add human-imperceptible
adversarial noise to natural examples. This limits the application of DNN in
security-critical fields. To alleviate this problem, we first conducted an
empirical analysis of the latent features of both adversarial and natural
examples and found the similarity matrix of natural examples is more compact
than those of adversarial examples. Motivated by this observation, we propose
\textbf{L}atent \textbf{F}eature \textbf{R}elation \textbf{C}onsistency
(\textbf{LFRC}), which constrains the relation of adversarial examples in
latent space to be consistent with the natural examples. Importantly, our LFRC
is orthogonal to the previous method and can be easily combined with them to
achieve further improvement. To demonstrate the effectiveness of LFRC, we
conduct extensive experiments using different neural networks on benchmark
datasets. For instance, LFRC can bring 0.78\% further improvement compared to
AT, and 1.09\% improvement compared to TRADES, against AutoAttack on CIFAR10.
Code is available at https://github.com/liuxingbin/LFRC.",None,-1
13be8f37-5872-4102-a45e-2d5c2782603d,Semantic Channel Equalizer: Modelling Language Mismatch in Multi-User Semantic Communications,0.700326,6,"We consider a multi-user semantic communications system in which agents
(transmitters and receivers) interact through the exchange of semantic messages
to convey meanings. In this context, languages are instrumental in structuring
the construction and consolidation of knowledge, influencing conceptual
representation and semantic extraction and interpretation. Yet, the crucial
role of languages in semantic communications is often overlooked. When this is
not the case, agent languages are assumed compatible and unambiguously
interoperable, ignoring practical limitations that may arise due to language
mismatching. This is the focus of this work. When agents use distinct
languages, message interpretation is prone to semantic noise resulting from
critical distortion introduced by semantic channels. To address this problem,
this paper proposes a new semantic channel equalizer to counteract and limit
the critical ambiguity in message interpretation. Our proposed solution models
the mismatch of languages with measurable transformations over semantic
representation spaces. We achieve this using optimal transport theory, where we
model such transformations as transportation maps. Then, to recover at the
receiver the meaning intended by the teacher we operate semantic equalization
to compensate for the transformation introduced by the semantic channel, either
before transmission and/or after the reception of semantic messages. We
implement the proposed approach as an operation over a codebook of
transformations specifically designed for successful communication. Numerical
results show that the proposed semantic channel equalizer outperforms
traditional approaches in terms of operational complexity and transmission
accuracy.",None,-1
2b8cd9f0-18dd-4786-9f28-8d6066b18e3f,Multi-Modal Evaluation Approach for Medical Image Segmentation,0.0768659,1,"Manual segmentation of medical images (e.g., segmenting tumors in CT scans)
is a high-effort task that can be accelerated with machine learning techniques.
However, selecting the right segmentation approach depends on the evaluation
function, particularly in medical image segmentation where we must deal with
dependency between voxels. For instance, in contrast to classical systems where
the predictions are either correct or incorrect, predictions in medical image
segmentation may be partially correct and incorrect simultaneously. In this
paper, we explore this expressiveness to extract the useful properties of these
systems and formally define a novel multi-modal evaluation (MME) approach to
measure the effectiveness of different segmentation methods. This approach
improves the segmentation evaluation by introducing new relevant and
interpretable characteristics, including detection property, boundary
alignment, uniformity, total volume, and relative volume. Our proposed approach
is open-source and publicly available for use. We have conducted several
reproducible experiments, including the segmentation of pancreas, liver tumors,
and multi-organs datasets, to show the applicability of the proposed approach.",None,-1
c663983c-c8c1-4952-bc75-2cbb3cc7a882,Long-range Meta-path Search on Large-scale Heterogeneous Graphs,0.191071,1,"Utilizing long-range dependency, though extensively studied in homogeneous
graphs, has not been well investigated on heterogeneous graphs. Addressing this
research gap presents two major challenges. The first is to alleviate
computational costs while endeavoring to leverage as much effective information
as possible in the presence of heterogeneity. The second involves overcoming
the well-known over-smoothing issue occurring in various graph neural networks.
To this end, we investigate the importance of different meta-paths and
introduce an automatic framework for utilizing long-range dependency on
heterogeneous graphs, denoted as Long-range Meta-path Search through
Progressive Sampling (LMSPS). Specifically, we develop a search space with all
meta-paths related to the target node type. By employing a progressive sampling
algorithm, LMSPS dynamically shrinks the search space with hop-independent time
complexity. Utilizing a sampling evaluation strategy as the guidance, LMSPS
conducts a specialized and effective meta-path selection. Subsequently, only
effective meta-paths are employed for retraining to reduce costs and overcome
the over-smoothing issue. Extensive experiments on various heterogeneous
datasets demonstrate that LMSPS discovers effective long-range meta-paths and
outperforms the state-of-the-art. Besides, it ranks top-1 on the leaderboards
of \texttt{ogbn-mag} in Open Graph Benchmark. Our code is available at
https://github.com/JHL-HUST/LDMLP.",None,-1
a8519837-7ef7-4047-9c14-64883b97cfc3,Large Language Models for Propaganda Detection,0.166213,4,"The prevalence of propaganda in our digital society poses a challenge to
societal harmony and the dissemination of truth. Detecting propaganda through
NLP in text is challenging due to subtle manipulation techniques and contextual
dependencies. To address this issue, we investigate the effectiveness of modern
Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.
We conduct experiments using the SemEval-2020 task 11 dataset, which features
news articles labeled with 14 propaganda techniques as a multi-label
classification problem. Five variations of GPT-3 and GPT-4 are employed,
incorporating various prompt engineering and fine-tuning strategies across the
different models. We evaluate the models' performance by assessing metrics such
as $F1$ score, $Precision$, and $Recall$, comparing the results with the
current state-of-the-art approach using RoBERTa. Our findings demonstrate that
GPT-4 achieves comparable results to the current state-of-the-art. Further,
this study analyzes the potential and challenges of LLMs in complex tasks like
propaganda detection.",None,-1
8afda243-2faa-4494-a466-2f3340fc1e71,"Metaverse: Requirements, Architecture, Standards, Status, Challenges, and Perspectives",0.671357,17,"The Metaverse is driving the next wave of innovation for new opportunities by
replacing the digital world (Internet) with the virtual world through a single,
shared, immersive, persistent 3D virtual space. In this paper, we present
requirements, architecture, standards, challenges, and solutions for Metaverse.
Specifically, we provide Metaverse architecture and requirements, and different
standards for Metaverse which serve as the basis for the development and
deployment. Moreover, we present recent status, challenges such as integration
of AI and Metaverse, security and privacy in Metaverse, etc., and perspectives
and solutions.",None,-1
0a9c07da-872a-4ab2-be42-79ec9ce84a76,Semantic 3D-aware Portrait Synthesis and Manipulation Based on Compositional Neural Radiance Field,0.276333,9,"Recently 3D-aware GAN methods with neural radiance field have developed
rapidly. However, current methods model the whole image as an overall neural
radiance field, which limits the partial semantic editability of synthetic
results. Since NeRF renders an image pixel by pixel, it is possible to split
NeRF in the spatial dimension. We propose a Compositional Neural Radiance Field
(CNeRF) for semantic 3D-aware portrait synthesis and manipulation. CNeRF
divides the image by semantic regions and learns an independent neural radiance
field for each region, and finally fuses them and renders the complete image.
Thus we can manipulate the synthesized semantic regions independently, while
fixing the other parts unchanged. Furthermore, CNeRF is also designed to
decouple shape and texture within each semantic region. Compared to
state-of-the-art 3D-aware GAN methods, our approach enables fine-grained
semantic region manipulation, while maintaining high-quality 3D-consistent
synthesis. The ablation studies show the effectiveness of the structure and
loss function used by our method. In addition real image inversion and cartoon
portrait 3D editing experiments demonstrate the application potential of our
method.",None,-1
9dd7e023-46ea-4d4c-9515-23818d01d38c,Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion,0.803374,6,"Temporal Knowledge graph completion (TKGC) is a crucial task that involves
reasoning at known timestamps to complete the missing part of facts and has
attracted more and more attention in recent years. Most existing methods focus
on learning representations based on graph neural networks while inaccurately
extracting information from timestamps and insufficiently utilizing the implied
information in relations. To address these problems, we propose a novel TKGC
model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We
convert a series of sampled quadruples into pre-trained language model inputs
and convert intervals between timestamps into different prompts to make
coherent sentences with implicit semantic information. We train our model with
a masking strategy to convert TKGC task into a masked token prediction task,
which can leverage the semantic information in pre-trained language models.
Experiments on three benchmark datasets and extensive analysis demonstrate that
our model has great competitiveness compared to other models with four metrics.
Our model can effectively incorporate information from temporal knowledge
graphs into the language models.",None,-1
0d819f7d-4480-49ad-acd2-e278da6692ab,An Algorithm and Complexity Results for Causal Unit Selection,0.438532,2,"The unit selection problem aims to identify objects, called units, that are
most likely to exhibit a desired mode of behavior when subjected to stimuli
(e.g., customers who are about to churn but would change their mind if
encouraged). Unit selection with counterfactual objective functions was
introduced relatively recently with existing work focusing on bounding a
specific class of objective functions, called the benefit functions, based on
observational and interventional data -- assuming a fully specified model is
not available to evaluate these functions. We complement this line of work by
proposing the first exact algorithm for finding optimal units given a broad
class of causal objective functions and a fully specified structural causal
model (SCM). We show that unit selection under this class of objective
functions is $\text{NP}^\text{PP}$-complete but is $\text{NP}$-complete when
unit variables correspond to all exogenous variables in the SCM. We also
provide treewidth-based complexity bounds on our proposed algorithm while
relating it to a well-known algorithm for Maximum a Posteriori (MAP) inference.",None,-1
363e10e0-d71f-4bc4-a504-49fa42a354bf,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,0.749905,11,"Monocular Depth Estimation (MDE) is a critical component in applications such
as autonomous driving. There are various attacks against MDE networks. These
attacks, especially the physical ones, pose a great threat to the security of
such systems. Traditional adversarial training method requires ground-truth
labels hence cannot be directly applied to self-supervised MDE that does not
have ground-truth depth. Some self-supervised model hardening techniques (e.g.,
contrastive learning) ignore the domain knowledge of MDE and can hardly achieve
optimal performance. In this work, we propose a novel adversarial training
method for self-supervised MDE models based on view synthesis without using
ground-truth depth. We improve adversarial robustness against physical-world
attacks using L0-norm-bounded perturbation in training. We compare our method
with supervised learning based and contrastive learning based methods that are
tailored for MDE. Results on two representative MDE networks show that we
achieve better robustness against various adversarial attacks with nearly no
benign performance degradation.",None,-1
2b0174bd-3d01-4ff5-9b9a-69248a28ad31,Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero,0.707133,13,"Artificial Intelligence (AI) systems have made remarkable progress, attaining
super-human performance across various domains. This presents us with an
opportunity to further human knowledge and improve human expert performance by
leveraging the hidden knowledge encoded within these highly performant AI
systems. Yet, this knowledge is often hard to extract, and may be hard to
understand or learn from. Here, we show that this is possible by proposing a
new method that allows us to extract new chess concepts in AlphaZero, an AI
system that mastered the game of chess via self-play without human supervision.
Our analysis indicates that AlphaZero may encode knowledge that extends beyond
the existing human knowledge, but knowledge that is ultimately not beyond human
grasp, and can be successfully learned from. In a human study, we show that
these concepts are learnable by top human experts, as four top chess
grandmasters show improvements in solving the presented concept prototype
positions. This marks an important first milestone in advancing the frontier of
human knowledge by leveraging AI; a development that could bear profound
implications and help us shape how we interact with AI systems across many AI
applications.",None,-1
6ab26af8-ce70-432d-bf9f-89cfa6bf45c0,Prompt Algebra for Task Composition,0.0796564,5,"We investigate whether prompts learned independently for different tasks can
be later combined through prompt algebra to obtain a model that supports
composition of tasks. We consider Visual Language Models (VLM) with prompt
tuning as our base classifier and formally define the notion of prompt algebra.
We propose constrained prompt tuning to improve performance of the composite
classifier. In the proposed scheme, prompts are constrained to appear in the
lower dimensional subspace spanned by the basis vectors of the pre-trained
vocabulary. Further regularization is added to ensure that the learned prompt
is grounded correctly to the existing pre-trained vocabulary. We demonstrate
the effectiveness of our method on object classification and object-attribute
classification datasets. On average, our composite model obtains classification
accuracy within 2.5% of the best base model. On UTZappos it improves
classification accuracy over the best base model by 8.45% on average.",None,-1
209b2caf-8234-454e-acc0-56a8efa521ba,Single Sequence Prediction over Reasoning Graphs for Multi-hop QA,0.171702,2,"Recent generative approaches for multi-hop question answering (QA) utilize
the fusion-in-decoder method~\cite{izacard-grave-2021-leveraging} to generate a
single sequence output which includes both a final answer and a reasoning path
taken to arrive at that answer, such as passage titles and key facts from those
passages. While such models can lead to better interpretability and high
quantitative scores, they often have difficulty accurately identifying the
passages corresponding to key entities in the context, resulting in incorrect
passage hops and a lack of faithfulness in the reasoning path. To address this,
we propose a single-sequence prediction method over a local reasoning graph
(\model)\footnote{Code/Models will be released at
\url{https://github.com/gowtham1997/SeqGraph}} that integrates a graph
structure connecting key entities in each context passage to relevant
subsequent passages for each question. We use a graph neural network to encode
this graph structure and fuse the resulting representations into the entity
representations of the model. Our experiments show significant improvements in
answer exact-match/F1 scores and faithfulness of grounding in the reasoning
path on the HotpotQA dataset and achieve state-of-the-art numbers on the
Musique dataset with only up to a 4\% increase in model parameters.",None,-1
2de89f6d-4dfd-487f-8cb6-57e077eb6f09,Future Aware Pricing and Matching for Sustainable On-demand Ride Pooling,0.655846,1,"The popularity of on-demand ride pooling is owing to the benefits offered to
customers (lower prices), taxi drivers (higher revenue), environment (lower
carbon footprint due to fewer vehicles) and aggregation companies like Uber
(higher revenue). To achieve these benefits, two key interlinked challenges
have to be solved effectively: (a) pricing -- setting prices to customer
requests for taxis; and (b) matching -- assignment of customers (that accepted
the prices) to taxis/cars. Traditionally, both these challenges have been
studied individually and using myopic approaches (considering only current
requests), without considering the impact of current matching on addressing
future requests. In this paper, we develop a novel framework that handles the
pricing and matching problems together, while also considering the future
impact of the pricing and matching decisions. In our experimental results on a
real-world taxi dataset, we demonstrate that our framework can significantly
improve revenue (up to 17% and on average 6.4%) in a sustainable manner by
reducing the number of vehicles (up to 14% and on average 10.6%) required to
obtain a given fixed revenue and the overall distance travelled by vehicles (up
to 11.1% and on average 3.7%). That is to say, we are able to provide an ideal
win-win scenario for all stakeholders (customers, drivers, aggregator,
environment) involved by obtaining higher revenue for customers, drivers,
aggregator (ride pooling company) while being good for the environment (due to
fewer number of vehicles on the road and lesser fuel consumed).",None,-1
7667d256-db2b-4cad-b719-8dff0fb9ad68,Learnable Differencing Center for Nighttime Depth Perception,0.598057,3,"Depth completion is the task of recovering dense depth maps from sparse ones,
usually with the help of color images. Existing image-guided methods perform
well on daytime depth perception self-driving benchmarks, but struggle in
nighttime scenarios with poor visibility and complex illumination. To address
these challenges, we propose a simple yet effective framework called LDCNet.
Our key idea is to use Recurrent Inter-Convolution Differencing (RICD) and
Illumination-Affinitive Intra-Convolution Differencing (IAICD) to enhance the
nighttime color images and reduce the negative effects of the varying
illumination, respectively. RICD explicitly estimates global illumination by
differencing two convolutions with different kernels, treating the
small-kernel-convolution feature as the center of the large-kernel-convolution
feature in a new perspective. IAICD softly alleviates local relative light
intensity by differencing a single convolution, where the center is dynamically
aggregated based on neighboring pixels and the estimated illumination map in
RICD. On both nighttime depth completion and depth estimation tasks, extensive
experiments demonstrate the effectiveness of our LDCNet, reaching the state of
the art.",None,-1
63b5f17b-ff82-4c77-a277-84fecfdd279c,Coarse Is Better? A New Pipeline Towards Self-Supervised Learning with Uncurated Images,0.155583,3,"Most self-supervised learning (SSL) methods often work on curated datasets
where the object-centric assumption holds. This assumption breaks down in
uncurated images. Existing scene image SSL methods try to find the two views
from original scene images that are well matched or dense, which is both
complex and computationally heavy. This paper proposes a conceptually different
pipeline: first find regions that are coarse objects (with adequate
objectness), crop them out as pseudo object-centric images, then any SSL method
can be directly applied as in a real object-centric dataset. That is, coarse
crops benefits scene images SSL. A novel cropping strategy that produces coarse
object box is proposed. The new pipeline and cropping strategy successfully
learn quality features from uncurated datasets without ImageNet. Experiments
show that our pipeline outperforms existing SSL methods (MoCo-v2, DenseCL and
MAE) on classification, detection and segmentation tasks. We further conduct
extensively ablations to verify that: 1) the pipeline do not rely on pretrained
models; 2) the cropping strategy is better than existing object discovery
methods; 3) our method is not sensitive to hyperparameters and data
augmentations.",None,-1
3e36d20e-8ec2-4f75-9508-f3ef64e3a632,R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras,0.687387,14,"Dense 3D reconstruction and ego-motion estimation are key challenges in
autonomous driving and robotics. Compared to the complex, multi-modal systems
deployed today, multi-camera systems provide a simpler, low-cost alternative.
However, camera-based 3D reconstruction of complex dynamic scenes has proven
extremely difficult, as existing solutions often produce incomplete or
incoherent results. We propose R3D3, a multi-camera system for dense 3D
reconstruction and ego-motion estimation. Our approach iterates between
geometric estimation that exploits spatial-temporal information from multiple
cameras, and monocular depth refinement. We integrate multi-camera feature
correlation and dense bundle adjustment operators that yield robust geometric
depth and pose estimates. To improve reconstruction where geometric depth is
unreliable, e.g. for moving objects or low-textured regions, we introduce
learnable scene priors via a depth refinement network. We show that this design
enables a dense, consistent 3D reconstruction of challenging, dynamic outdoor
environments. Consequently, we achieve state-of-the-art dense depth prediction
on the DDAD and NuScenes benchmarks.",None,-1
5eb85344-c5a1-4054-8cd1-59f9994d3b7c,Context-Gloss Augmentation for Improving Arabic Target Sense Verification,0.770509,5,"Arabic language lacks semantic datasets and sense inventories. The most
common semantically-labeled dataset for Arabic is the ArabGlossBERT, a
relatively small dataset that consists of 167K context-gloss pairs (about 60K
positive and 107K negative pairs), collected from Arabic dictionaries. This
paper presents an enrichment to the ArabGlossBERT dataset, by augmenting it
using (Arabic-English-Arabic) machine back-translation. Augmentation increased
the dataset size to 352K pairs (149K positive and 203K negative pairs). We
measure the impact of augmentation using different data configurations to
fine-tune BERT on target sense verification (TSV) task. Overall, the accuracy
ranges between 78% to 84% for different data configurations. Although our
approach performed at par with the baseline, we did observe some improvements
for some POS tags in some experiments. Furthermore, our fine-tuned models are
trained on a larger dataset covering larger vocabulary and contexts. We provide
an in-depth analysis of the accuracy for each part-of-speech (POS).",None,-1
720c0ce8-3498-4fc9-b60c-0d263941eb7d,Deep Integrated Explanations,0.198232,5,"This paper presents Deep Integrated Explanations (DIX) - a universal method
for explaining vision models. DIX generates explanation maps by integrating
information from the intermediate representations of the model, coupled with
their corresponding gradients. Through an extensive array of both objective and
subjective evaluations spanning diverse tasks, datasets, and model
configurations, we showcase the efficacy of DIX in generating faithful and
accurate explanation maps, while surpassing current state-of-the-art methods.",None,-1
4d46500f-e891-440a-87c6-78a945d8974c,Parallel Data Helps Neural Entity Coreference Resolution,0.423731,1,"Coreference resolution is the task of finding expressions that refer to the
same entity in a text. Coreference models are generally trained on monolingual
annotated data but annotating coreference is expensive and challenging.
Hardmeier et al.(2013) have shown that parallel data contains latent anaphoric
knowledge, but it has not been explored in end-to-end neural models yet. In
this paper, we propose a simple yet effective model to exploit coreference
knowledge from parallel data. In addition to the conventional modules learning
coreference from annotations, we introduce an unsupervised module to capture
cross-lingual coreference knowledge. Our proposed cross-lingual model achieves
consistent improvements, up to 1.74 percentage points, on the OntoNotes 5.0
English dataset using 9 different synthetic parallel datasets. These
experimental results confirm that parallel data can provide additional
coreference knowledge which is beneficial to coreference resolution tasks.",None,-1
6bb6c44f-2657-4ab5-aea2-b0d11087f41c,MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning,0.673595,3,"While large language models (LLMs) equipped with techniques like
chain-of-thought prompting have demonstrated impressive capabilities, they
still fall short in their ability to reason robustly in complex settings.
However, evaluating LLM reasoning is challenging because system capabilities
continue to grow while benchmark datasets for tasks like logical deduction have
remained static. We introduce MuSR, a dataset for evaluating language models on
multistep soft reasoning tasks specified in a natural language narrative. This
dataset has two crucial features. First, it is created through a novel
neurosymbolic synthetic-to-natural generation algorithm, enabling the
construction of complex reasoning instances that challenge GPT-4 (e.g., murder
mysteries roughly 1000 words in length) and which can be scaled further as more
capable LLMs are released. Second, our dataset instances are free text
narratives corresponding to real-world domains of reasoning; this makes it
simultaneously much more challenging than other synthetically-crafted
benchmarks while remaining realistic and tractable for human annotators to
solve with high accuracy. We evaluate a range of LLMs and prompting techniques
on this dataset and characterize the gaps that remain for techniques like
chain-of-thought to perform robust reasoning.",None,-1
939c4dd5-2b72-4768-8770-8eebcd4079d7,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,0.634853,5,"Despite the impressive results achieved by many existing Structure from
Motion (SfM) approaches, there is still a need to improve the robustness,
accuracy, and efficiency on large-scale scenes with many outlier matches and
sparse view graphs. In this paper, we propose AdaSfM: a coarse-to-fine adaptive
SfM approach that is scalable to large-scale and challenging datasets. Our
approach first does a coarse global SfM which improves the reliability of the
view graph by leveraging measurements from low-cost sensors such as Inertial
Measurement Units (IMUs) and wheel encoders. Subsequently, the view graph is
divided into sub-scenes that are refined in parallel by a fine local
incremental SfM regularised by the result from the coarse global SfM to improve
the camera registration accuracy and alleviate scene drifts. Finally, our
approach uses a threshold-adaptive strategy to align all local reconstructions
to the coordinate frame of global SfM. Extensive experiments on large-scale
benchmark datasets show that our approach achieves state-of-the-art accuracy
and efficiency.",None,-1
9c18b6fe-df1a-4c29-ae07-af8fb0ee6cf9,TSSR: A Truncated and Signed Square Root Activation Function for Neural Networks,0.195881,1,"Activation functions are essential components of neural networks. In this
paper, we introduce a new activation function called the Truncated and Signed
Square Root (TSSR) function. This function is distinctive because it is odd,
nonlinear, monotone and differentiable. Its gradient is continuous and always
positive. Thanks to these properties, it has the potential to improve the
numerical stability of neural networks. Several experiments confirm that the
proposed TSSR has better performance than other stat-of-the-art activation
functions. The proposed function has significant implications for the
development of neural network models and can be applied to a wide range of
applications in fields such as computer vision, natural language processing,
and speech recognition.",None,-1
356c487a-788e-4ff0-a9e5-c6ec2fde7f2d,Adaptive Prompt Learning with Distilled Connective Knowledge for Implicit Discourse Relation Recognition,0.273447,1,"Implicit discourse relation recognition (IDRR) aims at recognizing the
discourse relation between two text segments without an explicit connective.
Recently, the prompt learning has just been applied to the IDRR task with great
performance improvements over various neural network-based approaches. However,
the discrete nature of the state-art-of-art prompting approach requires manual
design of templates and answers, a big hurdle for its practical applications.
In this paper, we propose a continuous version of prompt learning together with
connective knowledge distillation, called AdaptPrompt, to reduce manual design
efforts via continuous prompting while further improving performance via
knowledge transfer. In particular, we design and train a few virtual tokens to
form continuous templates and automatically select the most suitable one by
gradient search in the embedding space. We also design an answer-relation
mapping rule to generate a few virtual answers as the answer space.
Furthermore, we notice the importance of annotated connectives in the training
dataset and design a teacher-student architecture for knowledge transfer.
Experiments on the up-to-date PDTB Corpus V3.0 validate our design objectives
in terms of the better relation recognition performance over the
state-of-the-art competitors.",None,-1
e7b2737b-ce19-450f-bd0b-415b720c7628,Emptying the Ocean with a Spoon: Should We Edit Models?,0.6024,17,"We call into question the recently popularized method of direct model editing
as a means of correcting factual errors in LLM generations. We contrast model
editing with three similar but distinct approaches that pursue better defined
objectives: (1) retrieval-based architectures, which decouple factual memory
from inference and linguistic capabilities embodied in LLMs; (2) concept
erasure methods, which aim at preventing systemic bias in generated text; and
(3) attribution methods, which aim at grounding generations into identified
textual sources. We argue that direct model editing cannot be trusted as a
systematic remedy for the disadvantages inherent to LLMs, and while it has
proven potential in improving model explainability, it opens risks by
reinforcing the notion that models can be trusted for factuality. We call for
cautious promotion and application of model editing as part of the LLM
deployment process, and for responsibly limiting the use cases of LLMs to those
not relying on editing as a critical component.",None,-1
cdafa630-e9c1-4d2e-adb9-961192aa1099,Giraffe: Adventures in Expanding Context Lengths in LLMs,0.785381,22,"Modern large language models (LLMs) that rely on attention mechanisms are
typically trained with fixed context lengths which enforce upper limits on the
length of input sequences that they can handle at evaluation time. To use these
models on sequences longer than the train-time context length, one might employ
techniques from the growing family of context length extrapolation methods --
most of which focus on modifying the system of positional encodings used in the
attention mechanism to indicate where tokens or activations are located in the
input sequence. We conduct a wide survey of existing methods of context length
extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own
design as well -- in particular, a new truncation strategy for modifying the
basis for the position encoding.
  We test these methods using three new evaluation tasks (FreeFormQA,
AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to
be less fine-grained as a measure of long context performance of LLMs. We
release the three tasks publicly as datasets on HuggingFace. We discover that
linear scaling is the best method for extending context length, and show that
further gains can be achieved by using longer scales at evaluation time. We
also discover promising extrapolation capabilities in the truncated basis. To
support further research in this area, we release three new 13B parameter
long-context models which we call Giraffe: 4k and 16k context models trained
from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We
also release the code to replicate our results.",None,-1
05af65aa-96ff-4cd7-97d3-2794a8b4fc87,Adversarial Training For Low-Resource Disfluency Correction,0.443974,4,"Disfluencies commonly occur in conversational speech. Speech with
disfluencies can result in noisy Automatic Speech Recognition (ASR)
transcripts, which affects downstream tasks like machine translation. In this
paper, we propose an adversarially-trained sequence-tagging model for
Disfluency Correction (DC) that utilizes a small amount of labeled real
disfluent data in conjunction with a large amount of unlabeled data. We show
the benefit of our proposed technique, which crucially depends on synthetically
generated disfluent data, by evaluating it for DC in three Indian languages-
Bengali, Hindi, and Marathi (all from the Indo-Aryan family). Our technique
also performs well in removing stuttering disfluencies in ASR transcripts
introduced by speech impairments. We achieve an average 6.15 points improvement
in F1-score over competitive baselines across all three languages mentioned. To
the best of our knowledge, we are the first to utilize adversarial training for
DC and use it to correct stuttering disfluencies in English, establishing a new
benchmark for this task.",None,-1
67a768c8-b9df-4fd1-9484-efdf2c89d7d7,AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web,0.81599,20,"Existing datasets for automated fact-checking have substantial limitations,
such as relying on artificial claims, lacking annotations for evidence and
intermediate reasoning, or including evidence published after the claim. In
this paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims
covering fact-checks by 50 different organizations. Each claim is annotated
with question-answer pairs supported by evidence available online, as well as
textual justifications explaining how the evidence combines to produce a
verdict. Through a multi-round annotation process, we avoid common pitfalls
including context dependence, evidence insufficiency, and temporal leakage, and
reach a substantial inter-annotator agreement of $\kappa=0.619$ on verdicts. We
develop a baseline as well as an evaluation scheme for verifying claims through
several question-answering steps against the open web.",None,-1
ed66b9bb-61ee-4335-a081-6bb7d5adfdf0,Constrained Empirical Risk Minimization: Theory and Practice,0.19885,1,"Deep Neural Networks (DNNs) are widely used for their ability to effectively
approximate large classes of functions. This flexibility, however, makes the
strict enforcement of constraints on DNNs an open problem. Here we present a
framework that, under mild assumptions, allows the exact enforcement of
constraints on parameterized sets of functions such as DNNs. Instead of
imposing ""soft'' constraints via additional terms in the loss, we restrict (a
subset of) the DNN parameters to a submanifold on which the constraints are
satisfied exactly throughout the entire training procedure. We focus on
constraints that are outside the scope of equivariant networks used in
Geometric Deep Learning. As a major example of the framework, we restrict
filters of a Convolutional Neural Network (CNN) to be wavelets, and apply these
wavelet networks to the task of contour prediction in the medical domain.",None,-1
4bc19c99-fb0d-465d-b383-2de204113d31,From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues,0.994528,22,"Understanding emotions during conversation is a fundamental aspect of human
communication, driving NLP research for Emotion Recognition in Conversation
(ERC). While considerable research has focused on discerning emotions of
individual speakers in monolingual dialogues, understanding the emotional
dynamics in code-mixed conversations has received relatively less attention.
This motivates our undertaking of ERC for code-mixed conversations in this
study. Recognizing that emotional intelligence encompasses a comprehension of
worldly knowledge, we propose an innovative approach that integrates
commonsense information with dialogue context to facilitate a deeper
understanding of emotions. To achieve this, we devise an efficient pipeline
that extracts relevant commonsense from existing knowledge graphs based on the
code-mixed input. Subsequently, we develop an advanced fusion technique that
seamlessly combines the acquired commonsense information with the dialogue
representation obtained from a dedicated dialogue understanding module. Our
comprehensive experimentation showcases the substantial performance improvement
obtained through the systematic incorporation of commonsense in ERC. Both
quantitative assessments and qualitative analyses further corroborate the
validity of our hypothesis, reaffirming the pivotal role of commonsense
integration in enhancing ERC.",None,-1
fc062c03-e15b-4465-b54b-ac0543d3ee91,ViT-DAE: Transformer-driven Diffusion Autoencoder for Histopathology Image Analysis,0.616197,4,"Generative AI has received substantial attention in recent years due to its
ability to synthesize data that closely resembles the original data source.
While Generative Adversarial Networks (GANs) have provided innovative
approaches for histopathological image analysis, they suffer from limitations
such as mode collapse and overfitting in discriminator. Recently, Denoising
Diffusion models have demonstrated promising results in computer vision. These
models exhibit superior stability during training, better distribution
coverage, and produce high-quality diverse images. Additionally, they display a
high degree of resilience to noise and perturbations, making them well-suited
for use in digital pathology, where images commonly contain artifacts and
exhibit significant variations in staining. In this paper, we present a novel
approach, namely ViT-DAE, which integrates vision transformers (ViT) and
diffusion autoencoders for high-quality histopathology image synthesis. This
marks the first time that ViT has been introduced to diffusion autoencoders in
computational pathology, allowing the model to better capture the complex and
intricate details of histopathology images. We demonstrate the effectiveness of
ViT-DAE on three publicly available datasets. Our approach outperforms recent
GAN-based and vanilla DAE methods in generating realistic images.",None,-1
ea1342a6-ef1f-4660-848b-9554a5814f40,"Abductive Reasoning with the GPT-4 Language Model: Case studies from criminal investigation, medical practice, scientific research",0.0229043,1,"This study evaluates the GPT-4 Large Language Model's abductive reasoning in
complex fields like medical diagnostics, criminology, and cosmology. Using an
interactive interview format, the AI assistant demonstrated reliability in
generating and selecting hypotheses. It inferred plausible medical diagnoses
based on patient data and provided potential causes and explanations in
criminology and cosmology. The results highlight the potential of LLMs in
complex problem-solving and the need for further research to maximize their
practical applications.",None,-1
79cad5f5-e677-4fa0-8672-d01d44ed977a,Target-Agnostic Gender-Aware Contrastive Learning for Mitigating Bias in Multilingual Machine Translation,0.388172,5,"Gender bias is a significant issue in machine translation, leading to ongoing
research efforts in developing bias mitigation techniques. However, most works
focus on debiasing bilingual models without much consideration for multilingual
systems. In this paper, we specifically target the gender bias issue of
multilingual machine translation models for unambiguous cases where there is a
single correct translation, and propose a bias mitigation method based on a
novel approach. Specifically, we propose Gender-Aware Contrastive Learning,
GACL, which encodes contextual gender information into the representations of
non-explicit gender words. Our method is target language-agnostic and is
applicable to pre-trained multilingual machine translation models via
fine-tuning. Through multilingual evaluation, we show that our approach
improves gender accuracy by a wide margin without hampering translation
performance. We also observe that incorporated gender information transfers and
benefits other target languages regarding gender accuracy. Finally, we
demonstrate that our method is applicable and beneficial to models of various
sizes.",None,-1
47118377-1fbc-4f19-a559-c13534d7e599,Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings,0.34878,2,"A recent body of work has demonstrated that Transformer embeddings can be
linearly decomposed into well-defined sums of factors, that can in turn be
related to specific network inputs or components. There is however still a
dearth of work studying whether these mathematical reformulations are
empirically meaningful. In the present work, we study representations from
machine-translation decoders using two of such embedding decomposition methods.
Our results indicate that, while decomposition-derived indicators effectively
correlate with model performance, variation across different runs suggests a
more nuanced take on this question. The high variability of our measurements
indicate that geometry reflects model-specific characteristics more than it
does sentence-specific computations, and that similar training conditions do
not guarantee similar vector spaces.",None,-1
14adc03d-8101-4944-95a8-43ddebfe2848,TOAST: Transfer Learning via Attention Steering,0.119447,3,"Transfer learning involves adapting a pre-trained model to novel downstream
tasks. However, we observe that current transfer learning methods often fail to
focus on task-relevant features. In this work, we explore refocusing model
attention for transfer learning. We introduce Top-Down Attention Steering
(TOAST), a novel transfer learning algorithm that keeps the pre-trained
backbone frozen, selects task-relevant features in the output, and feeds those
features back to the model to steer the attention to the task-specific
features. By refocusing the attention only, TOAST achieves state-of-the-art
results on a number of transfer learning benchmarks, while having a small
number of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt
tuning, TOAST substantially improves performance across a range of fine-grained
visual classification datasets (e.g., 81.1% -> 86.2% on FGVC). TOAST also
outperforms the fully fine-tuned Alpaca and Vicuna models on
instruction-following language generation. Code is available at
https://github.com/bfshi/TOAST.",None,-1
1939aa8f-22d7-4d44-ae91-acb3dd9060ac,Omni Aggregation Networks for Lightweight Image Super-Resolution,0.938946,26,"While lightweight ViT framework has made tremendous progress in image
super-resolution, its uni-dimensional self-attention modeling, as well as
homogeneous aggregation scheme, limit its effective receptive field (ERF) to
include more comprehensive interactions from both spatial and channel
dimensions. To tackle these drawbacks, this work proposes two enhanced
components under a new Omni-SR architecture. First, an Omni Self-Attention
(OSA) block is proposed based on dense interaction principle, which can
simultaneously model pixel-interaction from both spatial and channel
dimensions, mining the potential correlations across omni-axis (i.e., spatial
and channel). Coupling with mainstream window partitioning strategies, OSA can
achieve superior performance with compelling computational budgets. Second, a
multi-scale interaction scheme is proposed to mitigate sub-optimal ERF (i.e.,
premature saturation) in shallow models, which facilitates local propagation
and meso-/global-scale interactions, rendering an omni-scale aggregation
building block. Extensive experiments demonstrate that Omni-SR achieves
record-high performance on lightweight super-resolution benchmarks (e.g., 26.95
dB@Urban100 $\times 4$ with only 792K parameters). Our code is available at
\url{https://github.com/Francis0625/Omni-SR}.",None,-1
20a4b4ef-cf76-4126-a6df-9c11a943f3cd,Performance of Data Augmentation Methods for Brazilian Portuguese Text Classification,0.0502076,1,"Improving machine learning performance while increasing model generalization
has been a constantly pursued goal by AI researchers. Data augmentation
techniques are often used towards achieving this target, and most of its
evaluation is made using English corpora. In this work, we took advantage of
different existing data augmentation methods to analyze their performances
applied to text classification problems using Brazilian Portuguese corpora. As
a result, our analysis shows some putative improvements in using some of these
techniques; however, it also suggests further exploitation of language bias and
non-English text data scarcity.",None,-1
7d1dd196-9eca-4ebc-a9c3-a9c7d885091e,EVA-CLIP: Improved Training Techniques for CLIP at Scale,0.999963,209,"Contrastive language-image pre-training, CLIP for short, has gained
increasing attention for its potential in various scenarios. In this paper, we
propose EVA-CLIP, a series of models that significantly improve the efficiency
and effectiveness of CLIP training. Our approach incorporates new techniques
for representation learning, optimization, and augmentation, enabling EVA-CLIP
to achieve superior performance compared to previous CLIP models with the same
number of parameters but significantly smaller training costs. Notably, our
largest 5.0B-parameter EVA-02-CLIP-E/14+ with only 9 billion seen samples
achieves 82.0 zero-shot top-1 accuracy on ImageNet-1K val. A smaller
EVA-02-CLIP-L/14+ with only 430 million parameters and 6 billion seen samples
achieves 80.4 zero-shot top-1 accuracy on ImageNet-1K val. To facilitate open
access and open research, we release the complete suite of EVA-CLIP to the
community at https://github.com/baaivision/EVA/tree/master/EVA-CLIP.",None,-1
b0df0ef9-0f1b-471e-a8b6-df8e457a136b,Conformal Prediction with Large Language Models for Multi-Choice Question Answering,0.915801,36,"As large language models continue to be widely developed, robust uncertainty
quantification techniques will become crucial for their safe deployment in
high-stakes scenarios. In this work, we explore how conformal prediction can be
used to provide uncertainty quantification in language models for the specific
task of multiple-choice question-answering. We find that the uncertainty
estimates from conformal prediction are tightly correlated with prediction
accuracy. This observation can be useful for downstream applications such as
selective classification and filtering out low-quality predictions. We also
investigate the exchangeability assumption required by conformal prediction to
out-of-subject questions, which may be a more realistic scenario for many
practical applications. Our work contributes towards more trustworthy and
reliable usage of large language models in safety-critical situations, where
robust guarantees of error rate are required.",None,-1
3dd646fb-9f7e-4e31-bdca-10193b4cfdb8,Learning to Compress Unmanned Aerial Vehicle (UAV) Captured Video: Benchmark and Analysis,0.377661,2,"During the past decade, the Unmanned-Aerial-Vehicles (UAVs) have attracted
increasing attention due to their flexible, extensive, and dynamic
space-sensing capabilities. The volume of video captured by UAVs is
exponentially growing along with the increased bitrate generated by the
advancement of the sensors mounted on UAVs, bringing new challenges for
on-device UAV storage and air-ground data transmission. Most existing video
compression schemes were designed for natural scenes without consideration of
specific texture and view characteristics of UAV videos. In this work, we first
contribute a detailed analysis of the current state of the field of UAV video
coding. Then we propose to establish a novel task for learned UAV video coding
and construct a comprehensive and systematic benchmark for such a task, present
a thorough review of high quality UAV video datasets and benchmarks, and
contribute extensive rate-distortion efficiency comparison of learned and
conventional codecs after. Finally, we discuss the challenges of encoding UAV
videos. It is expected that the benchmark will accelerate the research and
development in video coding on drone platforms.",None,-1
2203a033-7a7d-454a-807f-4bb4b49d0b2b,FECANet: Boosting Few-Shot Semantic Segmentation with Feature-Enhanced Context-Aware Network,0.878276,21,"Few-shot semantic segmentation is the task of learning to locate each pixel
of the novel class in the query image with only a few annotated support images.
The current correlation-based methods construct pair-wise feature correlations
to establish the many-to-many matching because the typical prototype-based
approaches cannot learn fine-grained correspondence relations. However, the
existing methods still suffer from the noise contained in naive correlations
and the lack of context semantic information in correlations. To alleviate
these problems mentioned above, we propose a Feature-Enhanced Context-Aware
Network (FECANet). Specifically, a feature enhancement module is proposed to
suppress the matching noise caused by inter-class local similarity and enhance
the intra-class relevance in the naive correlation. In addition, we propose a
novel correlation reconstruction module that encodes extra correspondence
relations between foreground and background and multi-scale context semantic
features, significantly boosting the encoder to capture a reliable matching
pattern. Experiments on PASCAL-$5^i$ and COCO-$20^i$ datasets demonstrate that
our proposed FECANet leads to remarkable improvement compared to previous
state-of-the-arts, demonstrating its effectiveness.",None,-1
9072cbce-7e1f-4409-b595-9d19dc4a681b,That Label's Got Style: Handling Label Style Bias for Uncertain Image Segmentation,0.0627534,2,"Segmentation uncertainty models predict a distribution over plausible
segmentations for a given input, which they learn from the annotator variation
in the training set. However, in practice these annotations can differ
systematically in the way they are generated, for example through the use of
different labeling tools. This results in datasets that contain both data
variability and differing label styles. In this paper, we demonstrate that
applying state-of-the-art segmentation uncertainty models on such datasets can
lead to model bias caused by the different label styles. We present an updated
modelling objective conditioning on labeling style for aleatoric uncertainty
estimation, and modify two state-of-the-art-architectures for segmentation
uncertainty accordingly. We show with extensive experiments that this method
reduces label style bias, while improving segmentation performance, increasing
the applicability of segmentation uncertainty models in the wild. We curate two
datasets, with annotations in different label styles, which we will make
publicly available along with our code upon publication.",None,-1
1ade54cd-5b8c-4152-ba26-20c6fc09739d,The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs,0.548379,5,"Human beings are social creatures. We routinely reason about other agents,
and a crucial component of this social reasoning is inferring people's goals as
we learn about their actions. In many settings, we can perform intuitive but
reliable goal inference from language descriptions of agents, actions, and the
background environments. In this paper, we study this process of language
driving and influencing social reasoning in a probabilistic goal inference
domain. We propose a neuro-symbolic model that carries out goal inference from
linguistic inputs of agent scenarios. The ""neuro"" part is a large language
model (LLM) that translates language descriptions to code representations, and
the ""symbolic"" part is a Bayesian inverse planning engine. To test our model,
we design and run a human experiment on a linguistic goal inference task. Our
model closely matches human response patterns and better predicts human
judgements than using an LLM alone.",None,-1
fff648e4-f4fb-4d5d-adac-3447a1cd5757,Waving Goodbye to Low-Res: A Diffusion-Wavelet Approach for Image Super-Resolution,0.47626,6,"This paper presents a novel Diffusion-Wavelet (DiWa) approach for
Single-Image Super-Resolution (SISR). It leverages the strengths of Denoising
Diffusion Probabilistic Models (DDPMs) and Discrete Wavelet Transformation
(DWT). By enabling DDPMs to operate in the DWT domain, our DDPM models
effectively hallucinate high-frequency information for super-resolved images on
the wavelet spectrum, resulting in high-quality and detailed reconstructions in
image space. Quantitatively, we outperform state-of-the-art diffusion-based
SISR methods, namely SR3 and SRDiff, regarding PSNR, SSIM, and LPIPS on both
face (8x scaling) and general (4x scaling) SR benchmarks. Meanwhile, using DWT
enabled us to use fewer parameters than the compared models: 92M parameters
instead of 550M compared to SR3 and 9.3M instead of 12M compared to SRDiff.
Additionally, our method outperforms other state-of-the-art generative methods
on classical general SR datasets while saving inference time. Finally, our work
highlights its potential for various applications.",None,-1
8c9b63dc-760c-4fdb-aa6e-4afcba625f4c,Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games,0.745668,5,"Self-play (SP) is a popular multi-agent reinforcement learning (MARL)
framework for solving competitive games, where each agent optimizes policy by
treating others as part of the environment. Despite the empirical successes,
the theoretical properties of SP-based methods are limited to two-player
zero-sum games. However, for mixed cooperative-competitive games where agents
on the same team need to cooperate with each other, we can show a simple
counter-example where SP-based methods cannot converge to a global Nash
equilibrium (NE) with high probability. Alternatively, Policy-Space Response
Oracles (PSRO) is an iterative framework for learning NE, where the best
responses w.r.t. previous policies are learned in each iteration. PSRO can be
directly extended to mixed cooperative-competitive settings by jointly learning
team best responses with all convergence properties unchanged. However, PSRO
requires repeatedly training joint policies from scratch till convergence,
which makes it hard to scale to complex games. In this work, we develop a novel
algorithm, Fictitious Cross-Play (FXP), which inherits the benefits from both
frameworks. FXP simultaneously trains an SP-based main policy and a counter
population of best response policies. The main policy is trained by fictitious
self-play and cross-play against the counter population, while the counter
policies are trained as the best responses to the main policy's past versions.
We validate our method in matrix games and show that FXP converges to global
NEs while SP methods fail. We also conduct experiments in a gridworld domain,
where FXP achieves higher Elo ratings and lower exploitabilities than
baselines, and a more challenging football game, where FXP defeats SOTA models
with over 94% win rate.",None,-1
973e71d9-3f05-4e07-bb03-f3e937a4e8dd,Learning Occupancy for Monocular 3D Object Detection,0.708842,9,"Monocular 3D detection is a challenging task due to the lack of accurate 3D
information. Existing approaches typically rely on geometry constraints and
dense depth estimates to facilitate the learning, but often fail to fully
exploit the benefits of three-dimensional feature extraction in frustum and 3D
space. In this paper, we propose \textbf{OccupancyM3D}, a method of learning
occupancy for monocular 3D detection. It directly learns occupancy in frustum
and 3D space, leading to more discriminative and informative 3D features and
representations. Specifically, by using synchronized raw sparse LiDAR point
clouds, we define the space status and generate voxel-based occupancy labels.
We formulate occupancy prediction as a simple classification problem and design
associated occupancy losses. Resulting occupancy estimates are employed to
enhance original frustum/3D features. As a result, experiments on KITTI and
Waymo open datasets demonstrate that the proposed method achieves a new state
of the art and surpasses other methods by a significant margin. Codes and
pre-trained models will be available at
\url{https://github.com/SPengLiang/OccupancyM3D}.",None,-1
997e5087-5394-4c73-9cf3-8ce1163fa8c3,Learning Concise and Descriptive Attributes for Visual Recognition,0.932853,28,"Recent advances in foundation models present new opportunities for
interpretable visual recognition -- one can first query Large Language Models
(LLMs) to obtain a set of attributes that describe each class, then apply
vision-language models to classify images via these attributes. Pioneering work
shows that querying thousands of attributes can achieve performance competitive
with image features. However, our further investigation on 8 datasets reveals
that LLM-generated attributes in a large quantity perform almost the same as
random words. This surprising finding suggests that significant noise may be
present in these attributes. We hypothesize that there exist subsets of
attributes that can maintain the classification performance with much smaller
sizes, and propose a novel learning-to-search method to discover those concise
sets of attributes. As a result, on the CUB dataset, our method achieves
performance close to that of massive LLM-generated attributes (e.g., 10k
attributes for CUB), yet using only 32 attributes in total to distinguish 200
bird species. Furthermore, our new paradigm demonstrates several additional
benefits: higher interpretability and interactivity for humans, and the ability
to summarize knowledge for a recognition task.",None,-1
56d34426-a1d7-46bd-be75-406bc20fda80,Tab-CoT: Zero-shot Tabular Chain of Thought,0.336235,19,"The chain-of-though (CoT) prompting methods were successful in various
natural language processing (NLP) tasks thanks to their ability to unveil the
underlying complex reasoning processes. Such reasoning processes typically
exhibit implicitly structured steps. Recent efforts also started investigating
methods to encourage more explicitly structured reasoning procedures to be
captured. In this work, we propose Tab-CoT, a novel tabular-format CoT
prompting method, which allows the complex reasoning process to be explicitly
modelled in a highly structured manner. Despite its simplicity, we show that
our approach is capable of performing reasoning across multiple dimensions
(i.e., both rows and columns). We demonstrate our approach's strong zero-shot
and few-shot capabilities through extensive experiments on a range of reasoning
tasks.",None,-1
22789fd9-e4f8-4bd9-9b97-6fb0d8f1f887,Perspectives on the State and Future of Deep Learning - 2023,0.0858481,2,"The goal of this series is to chronicle opinions and issues in the field of
machine learning as they stand today and as they change over time. The plan is
to host this survey periodically until the AI singularity
paperclip-frenzy-driven doomsday, keeping an updated list of topical questions
and interviewing new community members for each edition. In this issue, we
probed people's opinions on interpretable AI, the value of benchmarking in
modern NLP, the state of progress towards understanding deep learning, and the
future of academia.",None,-1
d89fe4ac-a9b8-400c-a605-36a10e6c9f31,Unsupervised Out-of-Distribution Detection with Diffusion Inpainting,0.915165,17,"Unsupervised out-of-distribution detection (OOD) seeks to identify
out-of-domain data by learning only from unlabeled in-domain data. We present a
novel approach for this task - Lift, Map, Detect (LMD) - that leverages recent
advancement in diffusion models. Diffusion models are one type of generative
models. At their core, they learn an iterative denoising process that gradually
maps a noisy image closer to their training manifolds. LMD leverages this
intuition for OOD detection. Specifically, LMD lifts an image off its original
manifold by corrupting it, and maps it towards the in-domain manifold with a
diffusion model. For an out-of-domain image, the mapped image would have a
large distance away from its original manifold, and LMD would identify it as
OOD accordingly. We show through extensive experiments that LMD achieves
competitive performance across a broad variety of datasets. Code can be found
at https://github.com/zhenzhel/lift_map_detect.",None,-1
e5b58002-9477-4765-89ce-49fc83093cb1,DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4,0.199629,4,"Human preference judgments are pivotal in guiding large language models
(LLMs) to produce outputs that align with human values. Human evaluations are
also used in summarization tasks to compare outputs from various systems,
complementing existing automatic metrics. Despite their significance, however,
there has been limited research probing these pairwise or $k$-wise comparisons.
The collective impact and relative importance of factors such as output length,
informativeness, fluency, and factual consistency are still not well
understood. It is also unclear if there are other hidden factors influencing
human judgments. In this paper, we conduct an in-depth examination of a
collection of pairwise human judgments released by OpenAI. Utilizing the
Bradley-Terry-Luce (BTL) model, we reveal the inherent preferences embedded in
these human judgments. We find that the most favored factors vary across tasks
and genres, whereas the least favored factors tend to be consistent, e.g.,
outputs are too brief, contain excessive off-focus content or hallucinated
facts. Our findings have implications on the construction of balanced datasets
in human preference evaluations, which is a crucial step in shaping the
behaviors of future LLMs.",None,-1
17ceb794-dcfc-4727-a0ee-6611cdc29420,Exploring Partial Knowledge Base Inference in Biomedical Entity Linking,0.437878,2,"Biomedical entity linking (EL) consists of named entity recognition (NER) and
named entity disambiguation (NED). EL models are trained on corpora labeled by
a predefined KB. However, it is a common scenario that only entities within a
subset of the KB are precious to stakeholders. We name this scenario partial
knowledge base inference: training an EL model with one KB and inferring on the
part of it without further training. In this work, we give a detailed
definition and evaluation procedures for this practically valuable but
significantly understudied scenario and evaluate methods from three
representative EL paradigms. We construct partial KB inference benchmarks and
witness a catastrophic degradation in EL performance due to dramatically
precision drop. Our findings reveal these EL paradigms can not correctly handle
unlinkable mentions (NIL), so they are not robust to partial KB inference. We
also propose two simple-and-effective redemption methods to combat the NIL
issue with little computational overhead. Codes are released at
https://github.com/Yuanhy1997/PartialKB-EL.",None,-1
1a35fc2a-c2d3-4670-b39a-97e81a707c78,WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models,0.853202,16,"The open road poses many challenges to autonomous perception, including poor
visibility from extreme weather conditions. Models trained on good-weather
datasets frequently fail at detection in these out-of-distribution settings. To
aid adversarial robustness in perception, we introduce WEDGE (WEather images by
DALL-E GEneration): a synthetic dataset generated with a vision-language
generative model via prompting. WEDGE consists of 3360 images in 16 extreme
weather conditions manually annotated with 16513 bounding boxes, supporting
research in the tasks of weather classification and 2D object detection. We
have analyzed WEDGE from research standpoints, verifying its effectiveness for
extreme-weather autonomous perception. We establish baseline performance for
classification and detection with 53.87% test accuracy and 45.41 mAP. Most
importantly, WEDGE can be used to fine-tune state-of-the-art detectors,
improving SOTA performance on real-world weather benchmarks (such as DAWN) by
4.48 AP for well-generated classes like trucks. WEDGE has been collected under
OpenAI's terms of use and is released for public use under the CC BY-NC-SA 4.0
license. The repository for this work and dataset is available at
https://infernolia.github.io/WEDGE.",None,-1
08800021-8863-454f-9824-bd9431988840,AutoLabel: CLIP-based framework for Open-set Video Domain Adaptation,0.451543,8,"Open-set Unsupervised Video Domain Adaptation (OUVDA) deals with the task of
adapting an action recognition model from a labelled source domain to an
unlabelled target domain that contains ""target-private"" categories, which are
present in the target but absent in the source. In this work we deviate from
the prior work of training a specialized open-set classifier or weighted
adversarial learning by proposing to use pre-trained Language and Vision Models
(CLIP). The CLIP is well suited for OUVDA due to its rich representation and
the zero-shot recognition capabilities. However, rejecting target-private
instances with the CLIP's zero-shot protocol requires oracle knowledge about
the target-private label names. To circumvent the impossibility of the
knowledge of label names, we propose AutoLabel that automatically discovers and
generates object-centric compositional candidate target-private class names.
Despite its simplicity, we show that CLIP when equipped with AutoLabel can
satisfactorily reject the target-private instances, thereby facilitating better
alignment between the shared classes of the two domains. The code is available.",None,-1
c49d0469-4b44-4152-a238-de62f5b7c6f5,SoDaCam: Software-defined Cameras via Single-Photon Imaging,0.185984,3,"Reinterpretable cameras are defined by their post-processing capabilities
that exceed traditional imaging. We present ""SoDaCam"" that provides
reinterpretable cameras at the granularity of photons, from photon-cubes
acquired by single-photon devices. Photon-cubes represent the spatio-temporal
detections of photons as a sequence of binary frames, at frame-rates as high as
100 kHz. We show that simple transformations of the photon-cube, or photon-cube
projections, provide the functionality of numerous imaging systems including:
exposure bracketing, flutter shutter cameras, video compressive systems, event
cameras, and even cameras that move during exposure. Our photon-cube
projections offer the flexibility of being software-defined constructs that are
only limited by what is computable, and shot-noise. We exploit this flexibility
to provide new capabilities for the emulated cameras. As an added benefit, our
projections provide camera-dependent compression of photon-cubes, which we
demonstrate using an implementation of our projections on a novel compute
architecture that is designed for single-photon imaging.",None,-1
2d31df11-7eb7-4b97-b62c-4c9c21b445af,MTS-Mixers: Multivariate Time Series Forecasting via Factorized Temporal and Channel Mixing,0.8702,31,"Multivariate time series forecasting has been widely used in various
practical scenarios. Recently, Transformer-based models have shown significant
potential in forecasting tasks due to the capture of long-range dependencies.
However, recent studies in the vision and NLP fields show that the role of
attention modules is not clear, which can be replaced by other token
aggregation operations. This paper investigates the contributions and
deficiencies of attention mechanisms on the performance of time series
forecasting. Specifically, we find that (1) attention is not necessary for
capturing temporal dependencies, (2) the entanglement and redundancy in the
capture of temporal and channel interaction affect the forecasting performance,
and (3) it is important to model the mapping between the input and the
prediction sequence. To this end, we propose MTS-Mixers, which use two
factorized modules to capture temporal and channel dependencies. Experimental
results on several real-world datasets show that MTS-Mixers outperform existing
Transformer-based models with higher efficiency.",None,-1
3cf5329d-7dcb-4e24-8bfd-3bc02d48c5d0,"Learn over Past, Evolve for Future: Forecasting Temporal Trends for Fake News Detection",0.866978,11,"Fake news detection has been a critical task for maintaining the health of
the online news ecosystem. However, very few existing works consider the
temporal shift issue caused by the rapidly-evolving nature of news data in
practice, resulting in significant performance degradation when training on
past data and testing on future data. In this paper, we observe that the
appearances of news events on the same topic may display discernible patterns
over time, and posit that such patterns can assist in selecting training
instances that could make the model adapt better to future data. Specifically,
we design an effective framework FTT (Forecasting Temporal Trends), which could
forecast the temporal distribution patterns of news data and then guide the
detector to fast adapt to future distribution. Experiments on the real-world
temporally split dataset demonstrate the superiority of our proposed framework.
The code is available at https://github.com/ICTMCG/FTT-ACL23.",None,-1
3109ba93-1473-43e3-9fdb-9331b05c6937,Hallucinated Adversarial Control for Conservative Offline Policy Evaluation,0.473275,8,"We study the problem of conservative off-policy evaluation (COPE) where given
an offline dataset of environment interactions, collected by other agents, we
seek to obtain a (tight) lower bound on a policy's performance. This is crucial
when deciding whether a given policy satisfies certain minimal
performance/safety criteria before it can be deployed in the real world. To
this end, we introduce HAMBO, which builds on an uncertainty-aware learned
model of the transition dynamics. To form a conservative estimate of the
policy's performance, HAMBO hallucinates worst-case trajectories that the
policy may take, within the margin of the models' epistemic confidence regions.
We prove that the resulting COPE estimates are valid lower bounds, and, under
regularity conditions, show their convergence to the true expected return.
Finally, we discuss scalable variants of our approach based on Bayesian Neural
Networks and empirically demonstrate that they yield reliable and tight lower
bounds in various continuous control environments.",None,-1
f937b3cf-30aa-43dd-9c54-19dbed159bef,Learning-based solutions to nonlinear hyperbolic PDEs: Empirical insights on generalization errors,0.490839,4,"We study learning weak solutions to nonlinear hyperbolic partial differential
equations (H-PDE), which have been difficult to learn due to discontinuities in
their solutions. We use a physics-informed variant of the Fourier Neural
Operator ($\pi$-FNO) to learn the weak solutions. We empirically quantify the
generalization/out-of-sample error of the $\pi$-FNO solver as a function of
input complexity, i.e., the distributions of initial and boundary conditions.
Our testing results show that $\pi$-FNO generalizes well to unseen initial and
boundary conditions. We find that the generalization error grows linearly with
input complexity. Further, adding a physics-informed regularizer improved the
prediction of discontinuities in the solution. We use the
Lighthill-Witham-Richards (LWR) traffic flow model as a guiding example to
illustrate the results.",None,-1
40e81b24-1c2e-402c-a58a-c19b6819d40d,Interweaved Graph and Attention Network for 3D Human Pose Estimation,0.176704,2,"Despite substantial progress in 3D human pose estimation from a single-view
image, prior works rarely explore global and local correlations, leading to
insufficient learning of human skeleton representations. To address this issue,
we propose a novel Interweaved Graph and Attention Network (IGANet) that allows
bidirectional communications between graph convolutional networks (GCNs) and
attentions. Specifically, we introduce an IGA module, where attentions are
provided with local information from GCNs and GCNs are injected with global
information from attentions. Additionally, we design a simple yet effective
U-shaped multi-layer perceptron (uMLP), which can capture multi-granularity
information for body joints. Extensive experiments on two popular benchmark
datasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate our
proposed method.The results show that IGANet achieves state-of-the-art
performance on both datasets. Code is available at
https://github.com/xiu-cs/IGANet.",None,-1
1c7956c7-dc03-4db7-aa11-0ce71af84ce6,Make Text Unlearnable: Exploiting Effective Patterns to Protect Personal Data,0.399462,4,"This paper addresses the ethical concerns arising from the use of
unauthorized public data in deep learning models and proposes a novel solution.
Specifically, building on the work of Huang et al. (2021), we extend their
bi-level optimization approach to generate unlearnable text using a
gradient-based search technique. However, although effective, this approach
faces practical limitations, including the requirement of batches of instances
and model architecture knowledge that is not readily accessible to ordinary
users with limited access to their own data. Furthermore, even with
semantic-preserving constraints, unlearnable noise can alter the text's
semantics. To address these challenges, we extract simple patterns from
unlearnable text produced by bi-level optimization and demonstrate that the
data remains unlearnable for unknown models. Additionally, these patterns are
not instance- or dataset-specific, allowing users to readily apply them to text
classification and question-answering tasks, even if only a small proportion of
users implement them on their public content. We also open-source codes to
generate unlearnable text and assess unlearnable noise to benefit the public
and future studies.",None,-1
219b365f-1d60-40a4-8916-2d25891eecdf,Incremental Generalized Category Discovery,0.787969,8,"We explore the problem of Incremental Generalized Category Discovery (IGCD).
This is a challenging category incremental learning setting where the goal is
to develop models that can correctly categorize images from previously seen
categories, in addition to discovering novel ones. Learning is performed over a
series of time steps where the model obtains new labeled and unlabeled data,
and discards old data, at each iteration. The difficulty of the problem is
compounded in our generalized setting as the unlabeled data can contain images
from categories that may or may not have been observed before. We present a new
method for IGCD which combines non-parametric categorization with efficient
image sampling to mitigate catastrophic forgetting. To quantify performance, we
propose a new benchmark dataset named iNatIGCD that is motivated by a
real-world fine-grained visual categorization task. In our experiments we
outperform existing related methods",None,-1
ec075b30-0087-4641-88f2-13f4f0f7a0c7,Understanding Why ViT Trains Badly on Small Datasets: An Intuitive Perspective,0.285661,9,"Vision transformer (ViT) is an attention neural network architecture that is
shown to be effective for computer vision tasks. However, compared to ResNet-18
with a similar number of parameters, ViT has a significantly lower evaluation
accuracy when trained on small datasets. To facilitate studies in related
fields, we provide a visual intuition to help understand why it is the case. We
first compare the performance of the two models and confirm that ViT has less
accuracy than ResNet-18 when trained on small datasets. We then interpret the
results by showing attention map visualization for ViT and feature map
visualization for ResNet-18. The difference is further analyzed through a
representation similarity perspective. We conclude that the representation of
ViT trained on small datasets is hugely different from ViT trained on large
datasets, which may be the reason why the performance drops a lot on small
datasets.",None,-1
6678e523-d8da-48af-a82a-2170c744cbce,DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models,0.196606,1,"Hand mesh reconstruction from the monocular image is a challenging task due
to its depth ambiguity and severe occlusion, there remains a non-unique mapping
between the monocular image and hand mesh. To address this, we develop
DiffHand, the first diffusion-based framework that approaches hand mesh
reconstruction as a denoising diffusion process. Our one-stage pipeline
utilizes noise to model the uncertainty distribution of the intermediate hand
mesh in a forward process. We reformulate the denoising diffusion process to
gradually refine noisy hand mesh and then select mesh with the highest
probability of being correct based on the image itself, rather than relying on
2D joints extracted beforehand. To better model the connectivity of hand
vertices, we design a novel network module called the cross-modality decoder.
Extensive experiments on the popular benchmarks demonstrate that our method
outperforms the state-of-the-art hand mesh reconstruction approaches by
achieving 5.8mm PA-MPJPE on the Freihand test set, 4.98mm PA-MPJPE on the
DexYCB test set.",None,-1
8d1c10c0-1f52-4180-b9e8-f99a82d35a3c,Mediapipe and CNNs for Real-Time ASL Gesture Recognition,0.726769,4,"This research paper describes a realtime system for identifying American Sign
Language (ASL) movements that employs modern computer vision and machine
learning approaches. The suggested method makes use of the Mediapipe library
for feature extraction and a Convolutional Neural Network (CNN) for ASL gesture
classification. The testing results show that the suggested system can detect
all ASL alphabets with an accuracy of 99.95%, indicating its potential for use
in communication devices for people with hearing impairments. The proposed
approach can also be applied to additional sign languages with similar hand
motions, potentially increasing the quality of life for people with hearing
loss. Overall, the study demonstrates the effectiveness of using Mediapipe and
CNN for real-time sign language recognition, making a significant contribution
to the field of computer vision and machine learning.",None,-1
e16cc958-cadc-444f-91f8-a4b2b1e45d8c,The Treasure Beneath Multiple Annotations: An Uncertainty-aware Edge Detector,0.811518,16,"Deep learning-based edge detectors heavily rely on pixel-wise labels which
are often provided by multiple annotators. Existing methods fuse multiple
annotations using a simple voting process, ignoring the inherent ambiguity of
edges and labeling bias of annotators. In this paper, we propose a novel
uncertainty-aware edge detector (UAED), which employs uncertainty to
investigate the subjectivity and ambiguity of diverse annotations.
Specifically, we first convert the deterministic label space into a learnable
Gaussian distribution, whose variance measures the degree of ambiguity among
different annotations. Then we regard the learned variance as the estimated
uncertainty of the predicted edge maps, and pixels with higher uncertainty are
likely to be hard samples for edge detection. Therefore we design an adaptive
weighting loss to emphasize the learning from those pixels with high
uncertainty, which helps the network to gradually concentrate on the important
pixels. UAED can be combined with various encoder-decoder backbones, and the
extensive experiments demonstrate that UAED achieves superior performance
consistently across multiple edge detection benchmarks. The source code is
available at \url{https://github.com/ZhouCX117/UAED}",None,-1
ce80197f-3a40-404a-8581-a2d224f2eb1a,Conversational Agents and Children: Let Children Learn,0.0620104,1,"Using online information discovery as a case study, in this position paper we
discuss the need to design, develop, and deploy (conversational) agents that
can -- non-intrusively -- guide children in their quest for online resources
rather than simply finding resources for them. We argue that agents should ""let
children learn"" and should be built to take on a teacher-facilitator function,
allowing children to develop their technical and critical thinking abilities as
they interact with varied technology in a broad range of use cases.",None,-1
ed20b9d4-752f-4696-ad03-632f4e97c5c0,Exploring Invariant Representation for Visible-Infrared Person Re-Identification,0.738437,6,"Cross-spectral person re-identification, which aims to associate identities
to pedestrians across different spectra, faces a main challenge of the modality
discrepancy. In this paper, we address the problem from both image-level and
feature-level in an end-to-end hybrid learning framework named robust feature
mining network (RFM). In particular, we observe that the reflective intensity
of the same surface in photos shot in different wavelengths could be
transformed using a linear model. Besides, we show the variable linear factor
across the different surfaces is the main culprit which initiates the modality
discrepancy. We integrate such a reflection observation into an image-level
data augmentation by proposing the linear transformation generator (LTG).
Moreover, at the feature level, we introduce a cross-center loss to explore a
more compact intra-class distribution and modality-aware spatial attention to
take advantage of textured regions more efficiently. Experiment results on two
standard cross-spectral person re-identification datasets, i.e., RegDB and
SYSU-MM01, have demonstrated state-of-the-art performance.",None,-1
7827d154-fc9a-469f-9983-08c7ef99bf7a,Erasing Concepts from Diffusion Models,0.849365,127,"Motivated by recent advancements in text-to-image diffusion, we study erasure
of specific concepts from the model's weights. While Stable Diffusion has shown
promise in producing explicit or realistic artwork, it has raised concerns
regarding its potential for misuse. We propose a fine-tuning method that can
erase a visual concept from a pre-trained diffusion model, given only the name
of the style and using negative guidance as a teacher. We benchmark our method
against previous approaches that remove sexually explicit content and
demonstrate its effectiveness, performing on par with Safe Latent Diffusion and
censored training. To evaluate artistic style removal, we conduct experiments
erasing five modern artists from the network and conduct a user study to assess
the human perception of the removed styles. Unlike previous methods, our
approach can remove concepts from a diffusion model permanently rather than
modifying the output at the inference time, so it cannot be circumvented even
if a user has access to model weights. Our code, data, and results are
available at https://erasing.baulab.info/",None,-1
7ca5c37f-47d7-409b-9bf3-208b1f6eb7c9,Addressing Cold Start Problem for End-to-end Automatic Speech Scoring,0.266786,2,"Integrating automatic speech scoring/assessment systems has become a critical
aspect of second-language speaking education. With self-supervised learning
advancements, end-to-end speech scoring approaches have exhibited promising
results. However, this study highlights the significant decrease in the
performance of speech scoring systems in new question contexts, thereby
identifying this as a cold start problem in terms of items. With the finding of
cold-start phenomena, this paper seeks to alleviate the problem by following
methods: 1) prompt embeddings, 2) question context embeddings using BERT or
CLIP models, and 3) choice of the pretrained acoustic model. Experiments are
conducted on TOEIC speaking test datasets collected from
English-as-a-second-language (ESL) learners rated by professional TOEIC
speaking evaluators. The results demonstrate that the proposed framework not
only exhibits robustness in a cold-start environment but also outperforms the
baselines for known content.",None,-1
e2fde4e9-d123-4093-9d0d-30f76be45bcf,Summarization with Precise Length Control,0.0893272,1,"Many applications of text generation such as summarization benefit from
accurately controlling the text length. Existing approaches on
length-controlled summarization either result in degraded performance or can
only control the length approximately. In this work, we present a framework to
generate summaries with precisely the specified number of tokens or sentences,
while maintaining or even improving the text quality. In addition, we jointly
train the models to predict the lengths, so our model can generate summaries
with optimal length. We evaluate the proposed framework on the CNNDM dataset
and show improved performance compared to existing methods.",None,-1
1225cbae-38da-41d5-a4a7-10c4b2915f59,UAlberta at SemEval-2023 Task 1: Context Augmentation and Translation for Multilingual Visual Word Sense Disambiguation,0.550671,1,"We describe the systems of the University of Alberta team for the
SemEval-2023 Visual Word Sense Disambiguation (V-WSD) Task. We present a novel
algorithm that leverages glosses retrieved from BabelNet, in combination with
text and image encoders. Furthermore, we compare language-specific encoders
against the application of English encoders to translated texts. As the
contexts given in the task datasets are extremely short, we also experiment
with augmenting these contexts with descriptions generated by a language model.
This yields substantial improvements in accuracy. We describe and evaluate
additional V-WSD methods which use image generation and text-conditioned image
segmentation. Overall, the results of our official submission rank us 18 out of
56 teams. Some of our unofficial results are even better than the official
ones. Our code is publicly available at https://github.com/UAlberta-NLP/v-wsd.",None,-1
84b0be41-a512-4041-a2de-a47e2f68ffa5,NormBank: A Knowledge Bank of Situational Social Norms,0.997417,22,"We present NormBank, a knowledge bank of 155k situational norms. This
resource is designed to ground flexible normative reasoning for interactive,
assistive, and collaborative AI systems. Unlike prior commonsense resources,
NormBank grounds each inference within a multivalent sociocultural frame, which
includes the setting (e.g., restaurant), the agents' contingent roles (waiter,
customer), their attributes (age, gender), and other physical, social, and
cultural constraints (e.g., the temperature or the country of operation). In
total, NormBank contains 63k unique constraints from a taxonomy that we
introduce and iteratively refine here. Constraints then apply in different
combinations to frame social norms. Under these manipulations, norms are
non-monotonic - one can cancel an inference by updating its frame even
slightly. Still, we find evidence that neural models can help reliably extend
the scope and coverage of NormBank. We further demonstrate the utility of this
resource with a series of transfer experiments.",None,-1
693351db-159c-4824-bf35-50997e019ec5,NerVE: Neural Volumetric Edges for Parametric Curve Extraction from Point Cloud,0.922434,9,"Extracting parametric edge curves from point clouds is a fundamental problem
in 3D vision and geometry processing. Existing approaches mainly rely on
keypoint detection, a challenging procedure that tends to generate noisy
output, making the subsequent edge extraction error-prone. To address this
issue, we propose to directly detect structured edges to circumvent the
limitations of the previous point-wise methods. We achieve this goal by
presenting NerVE, a novel neural volumetric edge representation that can be
easily learned through a volumetric learning framework. NerVE can be seamlessly
converted to a versatile piece-wise linear (PWL) curve representation, enabling
a unified strategy for learning all types of free-form curves. Furthermore, as
NerVE encodes rich structural information, we show that edge extraction based
on NerVE can be reduced to a simple graph search problem. After converting
NerVE to the PWL representation, parametric curves can be obtained via
off-the-shelf spline fitting algorithms. We evaluate our method on the
challenging ABC dataset. We show that a simple network based on NerVE can
already outperform the previous state-of-the-art methods by a great margin.
Project page: https://dongdu3.github.io/projects/2023/NerVE/.",None,-1
01cfff77-ddcf-4160-bba4-f715c29433ec,A Satellite Imagery Dataset for Long-Term Sustainable Development in United States Cities,0.224911,3,"Cities play an important role in achieving sustainable development goals
(SDGs) to promote economic growth and meet social needs. Especially satellite
imagery is a potential data source for studying sustainable urban development.
However, a comprehensive dataset in the United States (U.S.) covering multiple
cities, multiple years, multiple scales, and multiple indicators for SDG
monitoring is lacking. To support the research on SDGs in U.S. cities, we
develop a satellite imagery dataset using deep learning models for five SDGs
containing 25 sustainable development indicators. The proposed dataset covers
the 100 most populated U.S. cities and corresponding Census Block Groups from
2014 to 2023. Specifically, we collect satellite imagery and identify objects
with state-of-the-art object detection and semantic segmentation models to
observe cities' bird's-eye view. We further gather population, nighttime light,
survey, and built environment data to depict SDGs regarding poverty, health,
education, inequality, and living environment. We anticipate the dataset to
help urban policymakers and researchers to advance SDGs-related studies,
especially applying satellite imagery to monitor long-term and multi-scale SDGs
in cities.",None,-1
296c81b2-1a75-4099-9c58-eedc4af61374,Multi-Layer Attention-Based Explainability via Transformers for Tabular Data,0.231726,2,"We propose a graph-oriented attention-based explainability method for tabular
data. Tasks involving tabular data have been solved mostly using traditional
tree-based machine learning models which have the challenges of feature
selection and engineering. With that in mind, we consider a transformer
architecture for tabular data, which is amenable to explainability, and present
a novel way to leverage self-attention mechanism to provide explanations by
taking into account the attention matrices of all heads and layers as a whole.
The matrices are mapped to a graph structure where groups of features
correspond to nodes and attention values to arcs. By finding the maximum
probability paths in the graph, we identify groups of features providing larger
contributions to explain the model's predictions. To assess the quality of
multi-layer attention-based explanations, we compare them with popular
attention-, gradient-, and perturbation-based explanability methods.",None,-1
67846cba-1158-4743-8bbd-4aad5057ab01,Leveraging Auxiliary Domain Parallel Data in Intermediate Task Fine-tuning for Low-resource Translation,0.198025,2,"NMT systems trained on Pre-trained Multilingual Sequence-Sequence (PMSS)
models flounder when sufficient amounts of parallel data is not available for
fine-tuning. This specifically holds for languages missing/under-represented in
these models. The problem gets aggravated when the data comes from different
domains. In this paper, we show that intermediate-task fine-tuning (ITFT) of
PMSS models is extremely beneficial for domain-specific NMT, especially when
target domain data is limited/unavailable and the considered languages are
missing or under-represented in the PMSS model. We quantify the domain-specific
results variations using a domain-divergence test, and show that ITFT can
mitigate the impact of domain divergence to some extent.",None,-1
dd9f3104-8d50-4ec6-a34a-f490b68d293c,Interpretable Deep Learning for Forecasting Online Advertising Costs: Insights from the Competitive Bidding Landscape,0.317826,1,"As advertisers increasingly shift their budgets toward digital advertising,
forecasting advertising costs is essential for making budget plans to optimize
marketing campaign returns. In this paper, we perform a comprehensive study
using a variety of time-series forecasting methods to predict daily average
cost-per-click (CPC) in the online advertising market. We show that forecasting
advertising costs would benefit from multivariate models using covariates from
competitors' CPC development identified through time-series clustering. We
further interpret the results by analyzing feature importance and temporal
attention. Finally, we show that our approach has several advantages over
models that individual advertisers might build based solely on their collected
data.",None,-1
77a828cb-1c28-4b02-a39c-737b9a2a5cbb,Control4D: Efficient 4D Portrait Editing with Text,0.378855,5,"We introduce Control4D, an innovative framework for editing dynamic 4D
portraits using text instructions. Our method addresses the prevalent
challenges in 4D editing, notably the inefficiencies of existing 4D
representations and the inconsistent editing effect caused by diffusion-based
editors. We first propose GaussianPlanes, a novel 4D representation that makes
Gaussian Splatting more structured by applying plane-based decomposition in 3D
space and time. This enhances both efficiency and robustness in 4D editing.
Furthermore, we propose to leverage a 4D generator to learn a more continuous
generation space from inconsistent edited images produced by the
diffusion-based editor, which effectively improves the consistency and quality
of 4D editing. Comprehensive evaluation demonstrates the superiority of
Control4D, including significantly reduced training time, high-quality
rendering, and spatial-temporal consistency in 4D portrait editing. The link to
our project website is https://control4darxiv.github.io.",None,-1
55e07938-6a44-407c-a2b3-f4f467bee768,Revisit and Outstrip Entity Alignment: A Perspective of Generative Models,0.166545,5,"Recent embedding-based methods have achieved great successes in exploiting
entity alignment from knowledge graph (KG) embeddings of multiple modalities.
In this paper, we study embedding-based entity alignment (EEA) from a
perspective of generative models. We show that EEA shares similarities with
typical generative models and prove the effectiveness of the recently developed
generative adversarial network (GAN)-based EEA methods theoretically. We then
reveal that their incomplete objective limits the capacity on both entity
alignment and entity synthesis (i.e., generating new entities). We mitigate
this problem by introducing a generative EEA (GEEA) framework with the proposed
mutual variational autoencoder (M-VAE) as the generative model. M-VAE enables
entity conversion between KGs and generation of new entities from random noise
vectors. We demonstrate the power of GEEA with theoretical analysis and
empirical experiments on both entity alignment and entity synthesis tasks.",None,-1
c5ec1e35-ed48-44d4-a74b-bb389f1f611c,Multi-Modal Mutual Attention and Iterative Interaction for Referring Image Segmentation,0.755187,20,"We address the problem of referring image segmentation that aims to generate
a mask for the object specified by a natural language expression. Many recent
works utilize Transformer to extract features for the target object by
aggregating the attended visual regions. However, the generic attention
mechanism in Transformer only uses the language input for attention weight
calculation, which does not explicitly fuse language features in its output.
Thus, its output feature is dominated by vision information, which limits the
model to comprehensively understand the multi-modal information, and brings
uncertainty for the subsequent mask decoder to extract the output mask. To
address this issue, we propose Multi-Modal Mutual Attention ($\mathrm{M^3Att}$)
and Multi-Modal Mutual Decoder ($\mathrm{M^3Dec}$) that better fuse information
from the two input modalities. Based on {$\mathrm{M^3Dec}$}, we further propose
Iterative Multi-modal Interaction ($\mathrm{IMI}$) to allow continuous and
in-depth interactions between language and vision features. Furthermore, we
introduce Language Feature Reconstruction ($\mathrm{LFR}$) to prevent the
language information from being lost or distorted in the extracted feature.
Extensive experiments show that our proposed approach significantly improves
the baseline and outperforms state-of-the-art referring image segmentation
methods on RefCOCO series datasets consistently.",None,-1
d54eebfb-0275-435e-8b8b-acb42bc598ee,GART: Gaussian Articulated Template Models,0.682926,23,"We introduce Gaussian Articulated Template Model GART, an explicit,
efficient, and expressive representation for non-rigid articulated subject
capturing and rendering from monocular videos. GART utilizes a mixture of
moving 3D Gaussians to explicitly approximate a deformable subject's geometry
and appearance. It takes advantage of a categorical template model prior (SMPL,
SMAL, etc.) with learnable forward skinning while further generalizing to more
complex non-rigid deformations with novel latent bones. GART can be
reconstructed via differentiable rendering from monocular videos in seconds or
minutes and rendered in novel poses faster than 150fps.",None,-1
5ebe48d8-4aa7-4ac8-be6e-620a87f6ec33,An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP),0.954709,43,"We study the performance of a commercially available large language model
(LLM) known as ChatGPT on math word problems (MWPs) from the dataset DRAW-1K.
To our knowledge, this is the first independent evaluation of ChatGPT. We found
that ChatGPT's performance changes dramatically based on the requirement to
show its work, failing 20% of the time when it provides work compared with 84%
when it does not. Further several factors about MWPs relating to the number of
unknowns and number of operations that lead to a higher probability of failure
when compared with the prior, specifically noting (across all experiments) that
the probability of failure increases linearly with the number of addition and
subtraction operations. We also have released the dataset of ChatGPT's
responses to the MWPs to support further work on the characterization of LLM
performance and present baseline machine learning models to predict if ChatGPT
can correctly answer an MWP. We have released a dataset comprised of ChatGPT's
responses to support further research in this area.",None,-1
eb4e1844-38f8-42f7-856c-5739f234f3c2,dacl10k: Benchmark for Semantic Bridge Damage Segmentation,0.737927,4,"Reliably identifying reinforced concrete defects (RCDs)plays a crucial role
in assessing the structural integrity, traffic safety, and long-term durability
of concrete bridges, which represent the most common bridge type worldwide.
Nevertheless, available datasets for the recognition of RCDs are small in terms
of size and class variety, which questions their usability in real-world
scenarios and their role as a benchmark. Our contribution to this problem is
""dacl10k"", an exceptionally diverse RCD dataset for multi-label semantic
segmentation comprising 9,920 images deriving from real-world bridge
inspections. dacl10k distinguishes 12 damage classes as well as 6 bridge
components that play a key role in the building assessment and recommending
actions, such as restoration works, traffic load limitations or bridge
closures. In addition, we examine baseline models for dacl10k which are
subsequently evaluated. The best model achieves a mean intersection-over-union
of 0.42 on the test set. dacl10k, along with our baselines, will be openly
accessible to researchers and practitioners, representing the currently biggest
dataset regarding number of images and class diversity for semantic
segmentation in the bridge inspection domain.",None,-1
5d46ec9f-776b-4589-b315-f0ddf1ed4893,Designing Long-term Group Fair Policies in Dynamical Systems,0.152956,1,"Neglecting the effect that decisions have on individuals (and thus, on the
underlying data distribution) when designing algorithmic decision-making
policies may increase inequalities and unfairness in the long term - even if
fairness considerations were taken in the policy design process. In this paper,
we propose a novel framework for achieving long-term group fairness in
dynamical systems, in which current decisions may affect an individual's
features in the next step, and thus, future decisions. Specifically, our
framework allows us to identify a time-independent policy that converges, if
deployed, to the targeted fair stationary state of the system in the long term,
independently of the initial data distribution. We model the system dynamics
with a time-homogeneous Markov chain and optimize the policy leveraging the
Markov chain convergence theorem to ensure unique convergence. We provide
examples of different targeted fair states of the system, encompassing a range
of long-term goals for society and policymakers. Furthermore, we show how our
approach facilitates the evaluation of different long-term targets by examining
their impact on the group-conditional population distribution in the long term
and how it evolves until convergence.",None,-1
8c5063e0-2adc-4fea-8f22-63451cdb4c73,Pushing the Envelope for Depth-Based Semi-Supervised 3D Hand Pose Estimation with Consistency Training,0.290024,2,"Despite the significant progress that depth-based 3D hand pose estimation
methods have made in recent years, they still require a large amount of labeled
training data to achieve high accuracy. However, collecting such data is both
costly and time-consuming. To tackle this issue, we propose a semi-supervised
method to significantly reduce the dependence on labeled training data. The
proposed method consists of two identical networks trained jointly: a teacher
network and a student network. The teacher network is trained using both the
available labeled and unlabeled samples. It leverages the unlabeled samples via
a loss formulation that encourages estimation equivariance under a set of
affine transformations. The student network is trained using the unlabeled
samples with their pseudo-labels provided by the teacher network. For inference
at test time, only the student network is used. Extensive experiments
demonstrate that the proposed method outperforms the state-of-the-art
semi-supervised methods by large margins.",None,-1
84f57889-91d0-41f6-a745-e0b70ff40bd8,mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences,0.156515,4,"We present our work on developing a multilingual, efficient text-to-text
transformer that is suitable for handling long inputs. This model, called
mLongT5, builds upon the architecture of LongT5, while leveraging the
multilingual datasets used for pretraining mT5 and the pretraining tasks of
UL2. We evaluate this model on a variety of multilingual summarization and
question-answering tasks, and the results show stronger performance for mLongT5
when compared to existing multilingual models such as mBART or M-BERT.",None,-1
daab5080-a3c4-46f3-afe1-06edf0904244,Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention,0.385735,6,"Large transformer models have achieved state-of-the-art results in numerous
natural language processing tasks. Among the pivotal components of the
transformer architecture, the attention mechanism plays a crucial role in
capturing token interactions within sequences through the utilization of
softmax function.
  Conversely, linear attention presents a more computationally efficient
alternative by approximating the softmax operation with linear complexity.
However, it exhibits substantial performance degradation when compared to the
traditional softmax attention mechanism.
  In this paper, we bridge the gap in our theoretical understanding of the
reasons behind the practical performance gap between softmax and linear
attention. By conducting a comprehensive comparative analysis of these two
attention mechanisms, we shed light on the underlying reasons for why softmax
attention outperforms linear attention in most scenarios.",None,-1
f54a44fd-ce32-4d14-af9d-a278b8dcfa36,3D View Prediction Models of the Dorsal Visual Stream,0.676852,2,"Deep neural network representations align well with brain activity in the
ventral visual stream. However, the primate visual system has a distinct dorsal
processing stream with different functional properties. To test if a model
trained to perceive 3D scene geometry aligns better with neural responses in
dorsal visual areas, we trained a self-supervised geometry-aware recurrent
neural network (GRNN) to predict novel camera views using a 3D feature memory.
We compared GRNN to self-supervised baseline models that have been shown to
align well with ventral regions using the large-scale fMRI Natural Scenes
Dataset (NSD). We found that while the baseline models accounted better for
ventral brain regions, GRNN accounted for a greater proportion of variance in
dorsal brain regions. Our findings demonstrate the potential for using
task-relevant models to probe representational differences across visual
streams.",None,-1
4c68e929-2120-49c8-b596-8e48d148c01d,Evaluating Shutdown Avoidance of Language Models in Textual Scenarios,0.0283411,4,"Recently, there has been an increase in interest in evaluating large language
models for emergent and dangerous capabilities. Importantly, agents could
reason that in some scenarios their goal is better achieved if they are not
turned off, which can lead to undesirable behaviors. In this paper, we
investigate the potential of using toy textual scenarios to evaluate
instrumental reasoning and shutdown avoidance in language models such as GPT-4
and Claude. Furthermore, we explore whether shutdown avoidance is merely a
result of simple pattern matching between the dataset and the prompt or if it
is a consistent behaviour across different environments and variations.
  We evaluated behaviours manually and also experimented with using language
models for automatic evaluations, and these evaluations demonstrate that simple
pattern matching is likely not the sole contributing factor for shutdown
avoidance. This study provides insights into the behaviour of language models
in shutdown avoidance scenarios and inspires further research on the use of
textual scenarios for evaluations.",None,-1
9abc31d6-a2e3-4857-891e-51659e06a692,A Dual-level Detection Method for Video Copy Detection,0.148053,2,"With the development of multimedia technology, Video Copy Detection has been
a crucial problem for social media platforms. Meta AI hold Video Similarity
Challenge on CVPR 2023 to push the technology forward. In this paper, we share
our winner solutions on both tracks to help progress in this area. For
Descriptor Track, we propose a dual-level detection method with Video Editing
Detection (VED) and Frame Scenes Detection (FSD) to tackle the core challenges
on Video Copy Detection. Experimental results demonstrate the effectiveness and
efficiency of our proposed method. Code is available at
https://github.com/FeipengMa6/VSC22-Submission.",None,-1
eb447eee-cbeb-4f9b-9bae-27b6511c8ad0,Who's in Charge? Roles and Responsibilities of Decision-Making Components in Conversational Robots,0.376197,2,"Software architectures for conversational robots typically consist of
multiple modules, each designed for a particular processing task or
functionality. Some of these modules are developed for the purpose of making
decisions about the next action that the robot ought to perform in the current
context. Those actions may relate to physical movements, such as driving
forward or grasping an object, but may also correspond to communicative acts,
such as asking a question to the human user. In this position paper, we reflect
on the organization of those decision modules in human-robot interaction
platforms. We discuss the relative benefits and limitations of modular vs.
end-to-end architectures, and argue that, despite the increasing popularity of
end-to-end approaches, modular architectures remain preferable when developing
conversational robots designed to execute complex tasks in collaboration with
human users. We also show that most practical HRI architectures tend to be
either robot-centric or dialogue-centric, depending on where developers wish to
place the ``command center'' of their system. While those design choices may be
justified in some application domains, they also limit the robot's ability to
flexibly interleave physical movements and conversational behaviours. We
contend that architectures placing ``action managers'' and ``interaction
managers'' on an equal footing may provide the best path forward for future
human-robot interaction systems.",None,-1
11ece9d7-78a8-4ca9-b80a-d3bc8794ea89,Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks,0.0569837,2,"Large language models have demonstrated robust performance on various
language tasks using zero-shot or few-shot learning paradigms. While being
actively researched, multimodal models that can additionally handle images as
input have yet to catch up in size and generality with language-only models. In
this work, we ask whether language-only models can be utilised for tasks that
require visual input -- but also, as we argue, often require a strong reasoning
component. Similar to some recent related work, we make visual information
accessible to the language model using separate verbalisation models.
Specifically, we investigate the performance of open-source, open-access
language models against GPT-3 on five vision-language tasks when given
textually-encoded visual information. Our results suggest that language models
are effective for solving vision-language tasks even with limited samples. This
approach also enhances the interpretability of a model's output by providing a
means of tracing the output back through the verbalised image content.",None,-1
636680be-69ad-4ce5-a1d2-716017d2a0db,Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding,0.348089,14,"Recently, large pretrained language models have demonstrated strong language
understanding capabilities. This is particularly reflected in their zero-shot
and in-context learning abilities on downstream tasks through prompting. To
assess their impact on spoken language understanding (SLU), we evaluate several
such models like ChatGPT and OPT of different sizes on multiple benchmarks. We
verify the emergent ability unique to the largest models as they can reach
intent classification accuracy close to that of supervised models with zero or
few shots on various languages given oracle transcripts. By contrast, the
results for smaller models fitting a single GPU fall far behind. We note that
the error cases often arise from the annotation scheme of the dataset;
responses from ChatGPT are still reasonable. We show, however, that the model
is worse at slot filling, and its performance is sensitive to ASR errors,
suggesting serious challenges for the application of those textual models on
SLU.",None,-1
40975231-0759-46a7-a01b-293656344f89,Just a Glimpse: Rethinking Temporal Information for Video Continual Learning,0.280267,3,"Class-incremental learning is one of the most important settings for the
study of Continual Learning, as it closely resembles real-world application
scenarios. With constrained memory sizes, catastrophic forgetting arises as the
number of classes/tasks increases. Studying continual learning in the video
domain poses even more challenges, as video data contains a large number of
frames, which places a higher burden on the replay memory. The current common
practice is to sub-sample frames from the video stream and store them in the
replay memory. In this paper, we propose SMILE a novel replay mechanism for
effective video continual learning based on individual/single frames. Through
extensive experimentation, we show that under extreme memory constraints, video
diversity plays a more significant role than temporal information. Therefore,
our method focuses on learning from a small number of frames that represent a
large number of unique videos. On three representative video datasets,
Kinetics, UCF101, and ActivityNet, the proposed method achieves
state-of-the-art performance, outperforming the previous state-of-the-art by up
to 21.49%.",None,-1
06f762bf-06ff-4a16-8df8-4ceced1271e7,Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities,0.649518,6,"One of the challenges of studying common neurological disorders is disease
heterogeneity including differences in causes, neuroimaging characteristics,
comorbidities, or genetic variation. Normative modelling has become a popular
method for studying such cohorts where the 'normal' behaviour of a
physiological system is modelled and can be used at subject level to detect
deviations relating to disease pathology. For many heterogeneous diseases, we
expect to observe abnormalities across a range of neuroimaging and biological
variables. However, thus far, normative models have largely been developed for
studying a single imaging modality. We aim to develop a multi-modal normative
modelling framework where abnormality is aggregated across variables of
multiple modalities and is better able to detect deviations than uni-modal
baselines. We propose two multi-modal VAE normative models to detect subject
level deviations across T1 and DTI data. Our proposed models were better able
to detect diseased individuals, capture disease severity, and correlate with
patient cognition than baseline approaches. We also propose a multivariate
latent deviation metric, measuring deviations from the joint latent space,
which outperformed feature-based metrics.",None,-1
1f372d43-c2f5-4db3-9380-20bf8087bbba,Scene-Aware Feature Matching,0.335546,1,"Current feature matching methods focus on point-level matching, pursuing
better representation learning of individual features, but lacking further
understanding of the scene. This results in significant performance degradation
when handling challenging scenes such as scenes with large viewpoint and
illumination changes. To tackle this problem, we propose a novel model named
SAM, which applies attentional grouping to guide Scene-Aware feature Matching.
SAM handles multi-level features, i.e., image tokens and group tokens, with
attention layers, and groups the image tokens with the proposed token grouping
module. Our model can be trained by ground-truth matches only and produce
reasonable grouping results. With the sense-aware grouping guidance, SAM is not
only more accurate and robust but also more interpretable than conventional
feature matching models. Sufficient experiments on various applications,
including homography estimation, pose estimation, and image matching,
demonstrate that our model achieves state-of-the-art performance.",None,-1
fb8e08a6-993a-4c54-887d-432468305251,A Meta-Learning Perspective on Transformers for Causal Language Modeling,0.105733,2,"The Transformer architecture has become prominent in developing large causal
language models. However, mechanisms to explain its capabilities are not well
understood. Focused on the training process, here we establish a meta-learning
view of the Transformer architecture when trained for the causal language
modeling task, by explicating an inner optimization process within the
Transformer. Further, within the inner optimization, we discover and
theoretically analyze a special characteristic of the norms of learned token
representations within Transformer-based causal language models. Our analysis
is supported by experiments in various settings.",None,-1
2056718d-5f58-46a6-ba60-4bc0490edd04,Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?,0.917292,14,"Making moral judgments is an essential step toward developing ethical AI
systems. Prevalent approaches are mostly implemented in a bottom-up manner,
which uses a large set of annotated data to train models based on crowd-sourced
opinions about morality. These approaches have been criticized for
overgeneralizing the moral stances of a limited group of annotators and lacking
explainability. This work proposes a flexible top-down framework to steer
(Large) Language Models (LMs) to perform moral reasoning with well-established
moral theories from interdisciplinary research. The theory-guided top-down
framework can incorporate various moral theories. Our experiments demonstrate
the effectiveness of the proposed framework on datasets derived from moral
theories. Furthermore, we show the alignment between different moral theories
and existing morality datasets. Our analysis exhibits the potential and flaws
in existing resources (models and datasets) in developing explainable moral
judgment-making systems.",None,-1
9f80a3ca-ceb4-4f23-9448-fba3552b7e6b,Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers,0.79087,17,"In recent years machine translation has become very successful for
high-resource language pairs. This has also sparked new interest in research on
the automatic translation of low-resource languages, including Indigenous
languages. However, the latter are deeply related to the ethnic and cultural
groups that speak (or used to speak) them. The data collection, modeling and
deploying machine translation systems thus result in new ethical questions that
must be addressed. Motivated by this, we first survey the existing literature
on ethical considerations for the documentation, translation, and general
natural language processing for Indigenous languages. Afterward, we conduct and
analyze an interview study to shed light on the positions of community leaders,
teachers, and language activists regarding ethical concerns for the automatic
translation of their languages. Our results show that the inclusion, at
different degrees, of native speakers and community members is vital to
performing better and more ethical research on Indigenous languages.",None,-1
081bb7c8-485c-4197-894d-569948dcea1b,Description-Based Text Similarity,0.397039,3,"Identifying texts with a given semantics is central for many information
seeking scenarios. Similarity search over vector embeddings appear to be
central to this ability, yet the similarity reflected in current text
embeddings is corpus-driven, and is inconsistent and sub-optimal for many use
cases. What, then, is a good notion of similarity for effective retrieval of
text?
  We identify the need to search for texts based on abstract descriptions of
their content, and the corresponding notion of \emph{description based
similarity}. We demonstrate the inadequacy of current text embeddings and
propose an alternative model that significantly improves when used in standard
nearest neighbor search. The model is trained using positive and negative pairs
sourced through prompting a LLM, demonstrating how data from LLMs can be used
for creating new capabilities not immediately possible using the original
model.",None,-1
e0217a79-4614-4cdb-9e94-18921dcb82cd,Advancing Referring Expression Segmentation Beyond Single Image,0.794147,8,"Referring Expression Segmentation (RES) is a widely explored multi-modal
task, which endeavors to segment the pre-existing object within a single image
with a given linguistic expression. However, in broader real-world scenarios,
it is not always possible to determine if the described object exists in a
specific image. Typically, we have a collection of images, some of which may
contain the described objects. The current RES setting curbs its practicality
in such situations. To overcome this limitation, we propose a more realistic
and general setting, named Group-wise Referring Expression Segmentation (GRES),
which expands RES to a collection of related images, allowing the described
objects to be present in a subset of input images. To support this new setting,
we introduce an elaborately compiled dataset named Grouped Referring Dataset
(GRD), containing complete group-wise annotations of target objects described
by given expressions. We also present a baseline method named Grouped Referring
Segmenter (GRSer), which explicitly captures the language-vision and
intra-group vision-vision interactions to achieve state-of-the-art results on
the proposed GRES and related tasks, such as Co-Salient Object Detection and
RES. Our dataset and codes will be publicly released in
https://github.com/yixuan730/group-res.",None,-1
660c446c-8c18-4a17-8bd8-ccacfc146c2f,Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis,0.715418,7,"Multimodal relation extraction (MRE) is the task of identifying the semantic
relationships between two entities based on the context of the sentence image
pair. Existing retrieval-augmented approaches mainly focused on modeling the
retrieved textual knowledge, but this may not be able to accurately identify
complex relations. To improve the prediction, this research proposes to
retrieve textual and visual evidence based on the object, sentence, and whole
image. We further develop a novel approach to synthesize the object-level,
image-level, and sentence-level information for better reasoning between the
same and different modalities. Extensive experiments and analyses show that the
proposed method is able to effectively select and compare evidence across
modalities and significantly outperforms state-of-the-art models.",None,-1
27bc526f-1f99-4302-9177-2ee2a158eee9,PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction,1.0,24,"Vectorized high-definition map online construction has garnered considerable
attention in the field of autonomous driving research. Most existing approaches
model changeable map elements using a fixed number of points, or predict local
maps in a two-stage autoregressive manner, which may miss essential details and
lead to error accumulation. Towards precise map element learning, we propose a
simple yet effective architecture named PivotNet, which adopts unified
pivot-based map representations and is formulated as a direct set prediction
paradigm. Concretely, we first propose a novel point-to-line mask module to
encode both the subordinate and geometrical point-line priors in the network.
Then, a well-designed pivot dynamic matching module is proposed to model the
topology in dynamic point sequences by introducing the concept of sequence
matching. Furthermore, to supervise the position and topology of the vectorized
point predictions, we propose a dynamic vectorized sequence loss. Extensive
experiments and ablations show that PivotNet is remarkably superior to other
SOTAs by 5.9 mAP at least. The code will be available soon.",None,-1
9f88bda5-2684-4f74-ab37-0e9fe87c24ef,JobRecoGPT -- Explainable job recommendations using LLMs,0.21611,3,"In today's rapidly evolving job market, finding the right opportunity can be
a daunting challenge. With advancements in the field of AI, computers can now
recommend suitable jobs to candidates. However, the task of recommending jobs
is not same as recommending movies to viewers. Apart from must-have criteria,
like skills and experience, there are many subtle aspects to a job which can
decide if it is a good fit or not for a given candidate. Traditional approaches
can capture the quantifiable aspects of jobs and candidates, but a substantial
portion of the data that is present in unstructured form in the job
descriptions and resumes is lost in the process of conversion to structured
format. As of late, Large Language Models (LLMs) have taken over the AI field
by storm with extraordinary performance in fields where text-based data is
available. Inspired by the superior performance of LLMs, we leverage their
capability to understand natural language for capturing the information that
was previously getting lost during the conversion of unstructured data to
structured form. To this end, we compare performance of four different
approaches for job recommendations namely, (i) Content based deterministic,
(ii) LLM guided, (iii) LLM unguided, and (iv) Hybrid. In this study, we present
advantages and limitations of each method and evaluate their performance in
terms of time requirements.",None,-1
4e6177e5-c74b-499f-9079-5f162d52db32,Introduction to Presentation Attacks in Signature Biometrics and Recent Advances,0.0710573,1,"Applications based on biometric authentication have received a lot of
interest in the last years due to the breathtaking results obtained using
personal traits such as face or fingerprint. However, it is important not to
forget that these biometric systems have to withstand different types of
possible attacks. This chapter carries out an analysis of different
Presentation Attack (PA) scenarios for on-line handwritten signature
verification. The main contributions of this chapter are: i) an updated
overview of representative methods for Presentation Attack Detection (PAD) in
signature biometrics; ii) a description of the different levels of PAs existing
in on-line signature verification regarding the amount of information available
to the impostor, as well as the training, effort, and ability to perform the
forgeries; and iii) an evaluation of the system performance in signature
biometrics under different scenarios considering recent publicly available
signature databases, DeepSignDB and SVC2021_EvalDB. This work is in line with
recent efforts in the Common Criteria standardization community towards
security evaluation of biometric systems.",None,-1
3e1f2f8d-e47c-4da1-8763-e3e1146b0d82,Zero-shot Faithful Factual Error Correction,0.970125,25,"Faithfully correcting factual errors is critical for maintaining the
integrity of textual knowledge bases and preventing hallucinations in
sequence-to-sequence models. Drawing on humans' ability to identify and correct
factual errors, we present a zero-shot framework that formulates questions
about input claims, looks for correct answers in the given evidence, and
assesses the faithfulness of each correction based on its consistency with the
evidence. Our zero-shot framework outperforms fully-supervised approaches, as
demonstrated by experiments on the FEVER and SciFact datasets, where our
outputs are shown to be more faithful. More importantly, the decomposability
nature of our framework inherently provides interpretability. Additionally, to
reveal the most suitable metrics for evaluating factual error corrections, we
analyze the correlation between commonly used metrics with human judgments in
terms of three different dimensions regarding intelligibility and faithfulness.",None,-1
824fda89-2ce5-45ee-8589-afd445353c92,Implementing BERT and fine-tuned RobertA to detect AI generated news by ChatGPT,0.722989,7,"The abundance of information on social media has increased the necessity of
accurate real-time rumour detection. Manual techniques of identifying and
verifying fake news generated by AI tools are impracticable and time-consuming
given the enormous volume of information generated every day. This has sparked
an increase in interest in creating automated systems to find fake news on the
Internet. The studies in this research demonstrate that the BERT and RobertA
models with fine-tuning had the best success in detecting AI generated news.
With a score of 98%, tweaked RobertA in particular showed excellent precision.
In conclusion, this study has shown that neural networks can be used to
identify bogus news AI generation news created by ChatGPT. The RobertA and BERT
models' excellent performance indicates that these models can play a critical
role in the fight against misinformation.",None,-1
1d82465f-e5b6-4401-b6f3-ab29310d151d,Can Voice Assistants Sound Cute? Towards a Model of Kawaii Vocalics,0.905356,4,"The Japanese notion of ""kawaii"" or expressions of cuteness, vulnerability,
and/or charm is a global cultural export. Work has explored kawaii-ness as a
design feature and factor of user experience in the visual appearance,
nonverbal behaviour, and sound of robots and virtual characters. In this
initial work, we consider whether voices can be kawaii by exploring the vocal
qualities of voice assistant speech, i.e., kawaii vocalics. Drawing from an
age-inclusive model of kawaii, we ran a user perceptions study on the
kawaii-ness of younger- and older-sounding Japanese computer voices. We found
that kawaii-ness intersected with perceptions of gender and age, i.e., gender
ambiguous and girlish, as well as VA features, i.e., fluency and artificiality.
We propose an initial model of kawaii vocalics to be validated through the
identification and study of vocal qualities, cognitive appraisals, behavioural
responses, and affective reports.",None,-1
455cc4d7-e010-4e85-8e35-6054bb6180e3,A Fully First-Order Method for Stochastic Bilevel Optimization,0.978149,37,"We consider stochastic unconstrained bilevel optimization problems when only
the first-order gradient oracles are available. While numerous optimization
methods have been proposed for tackling bilevel problems, existing methods
either tend to require possibly expensive calculations regarding Hessians of
lower-level objectives, or lack rigorous finite-time performance guarantees. In
this work, we propose a Fully First-order Stochastic Approximation (F2SA)
method, and study its non-asymptotic convergence properties. Specifically, we
show that F2SA converges to an $\epsilon$-stationary solution of the bilevel
problem after $\epsilon^{-7/2}, \epsilon^{-5/2}$, and $\epsilon^{-3/2}$
iterations (each iteration using $O(1)$ samples) when stochastic noises are in
both level objectives, only in the upper-level objective, and not present
(deterministic settings), respectively. We further show that if we employ
momentum-assisted gradient estimators, the iteration complexities can be
improved to $\epsilon^{-5/2}, \epsilon^{-4/2}$, and $\epsilon^{-3/2}$,
respectively. We demonstrate even superior practical performance of the
proposed method over existing second-order based approaches on MNIST
data-hypercleaning experiments.",None,-1
6d7217ff-0730-47ef-bca5-63c108d631c2,FrameFinder: Explorative Multi-Perspective Framing Extraction from News Headlines,0.148889,1,"Revealing the framing of news articles is an important yet neglected task in
information seeking and retrieval. In the present work, we present FrameFinder,
an open tool for extracting and analyzing frames in textual data. FrameFinder
visually represents the frames of text from three perspectives, i.e., (i) frame
labels, (ii) frame dimensions, and (iii) frame structure. By analyzing the
well-established gun violence frame corpus, we demonstrate the merits of our
proposed solution to support social science research and call for subsequent
integration into information interactions.",None,-1
94d25624-592c-4afb-b53f-416bf2e9f84d,Peer attention enhances student learning,0.0567606,1,"Human visual attention is susceptible to social influences. In education,
peer effects impact student learning, but their precise role in modulating
attention remains unclear. Our experiment (N=311) demonstrates that displaying
peer visual attention regions when students watch online course videos enhances
their focus and engagement. However, students retain adaptability in following
peer attention cues. Overall, guided peer attention improves learning
experiences and outcomes. These findings elucidate how peer visual attention
shapes students' gaze patterns, deepening understanding of peer influence on
learning. They also offer insights into designing adaptive online learning
interventions leveraging peer attention modelling to optimize student
attentiveness and success.",None,-1
34bd4c0f-fc07-40cc-8751-cf6af66751fb,Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems,0.387991,8,"Clean-label (CL) attack is a form of data poisoning attack where an adversary
modifies only the textual input of the training data, without requiring access
to the labeling function. CL attacks are relatively unexplored in NLP, as
compared to label flipping (LF) attacks, where the latter additionally requires
access to the labeling function as well. While CL attacks are more resilient to
data sanitization and manual relabeling methods than LF attacks, they often
demand as high as ten times the poisoning budget than LF attacks. In this work,
we first introduce an Adversarial Clean Label attack which can adversarially
perturb in-class training examples for poisoning the training set. We then show
that an adversary can significantly bring down the data requirements for a CL
attack, using the aforementioned approach, to as low as 20% of the data
otherwise required. We then systematically benchmark and analyze a number of
defense methods, for both LF and CL attacks, some previously employed solely
for LF attacks in the textual domain and others adapted from computer vision.
We find that text-specific defenses greatly vary in their effectiveness
depending on their properties.",None,-1
bf05bd4c-05bb-43c8-bc9b-76421b7037c9,ScrollNet: Dynamic Weight Importance for Continual Learning,0.193988,1,"The principle underlying most existing continual learning (CL) methods is to
prioritize stability by penalizing changes in parameters crucial to old tasks,
while allowing for plasticity in other parameters. The importance of weights
for each task can be determined either explicitly through learning a
task-specific mask during training (e.g., parameter isolation-based approaches)
or implicitly by introducing a regularization term (e.g., regularization-based
approaches). However, all these methods assume that the importance of weights
for each task is unknown prior to data exposure. In this paper, we propose
ScrollNet as a scrolling neural network for continual learning. ScrollNet can
be seen as a dynamic network that assigns the ranking of weight importance for
each task before data exposure, thus achieving a more favorable
stability-plasticity tradeoff during sequential task learning by reassigning
this ranking for different tasks. Additionally, we demonstrate that ScrollNet
can be combined with various CL methods, including regularization-based and
replay-based approaches. Experimental results on CIFAR100 and TinyImagenet
datasets show the effectiveness of our proposed method. We release our code at
https://github.com/FireFYF/ScrollNet.git.",None,-1
5b8b5dd4-3a6d-4c7e-b3f6-87787f33a317,"Towards socially-competent and culturally-adaptive artificial agents Expressive order, interactional disruptions and recovery strategies",0.171146,1,"The development of artificial agents for social interaction pushes to enrich
robots with social skills and knowledge about (local) social norms. One
possibility is to distinguish the expressive and the functional orders during a
human-robot interaction. The overarching aim of this work is to set a framework
to make the artificial agent socially-competent beyond dyadic
interaction-interaction in varying multi-party social situations-and beyond
individual-based user personalization, thereby enlarging the current conception
of ""culturally-adaptive"". The core idea is to provide the artificial agent with
the capability to handle different kinds of interactional disruptions, and
associated recovery strategies, in microsociology. The result is obtained by
classifying functional and social disruptions, and by investigating the
requirements a robot's architecture should satisfy to exploit such knowledge.
The paper also highlights how this level of competence is achieved by focusing
on just three dimensions: (i) social capability, (ii) relational role, and
(iii) proximity, leaving aside the further complexity of full-fledged
human-human interactions. Without going into technical aspects, End-to-end
Data-driven Architectures and Modular Architectures are discussed to evaluate
the degree to which they can exploit this new set of social and cultural
knowledge. Finally, a list of general requirements for such agents is proposed.",None,-1
f545c4a2-1b74-400c-8978-06e4ee200b7b,Data Quality-aware Mixed-precision Quantization via Hybrid Reinforcement Learning,0.905327,12,"Mixed-precision quantization mostly predetermines the model bit-width
settings before actual training due to the non-differential bit-width sampling
process, obtaining sub-optimal performance. Worse still, the conventional
static quality-consistent training setting, i.e., all data is assumed to be of
the same quality across training and inference, overlooks data quality changes
in real-world applications which may lead to poor robustness of the quantized
models. In this paper, we propose a novel Data Quality-aware Mixed-precision
Quantization framework, dubbed DQMQ, to dynamically adapt quantization
bit-widths to different data qualities. The adaption is based on a bit-width
decision policy that can be learned jointly with the quantization training.
Concretely, DQMQ is modeled as a hybrid reinforcement learning (RL) task that
combines model-based policy optimization with supervised quantization training.
By relaxing the discrete bit-width sampling to a continuous probability
distribution that is encoded with few learnable parameters, DQMQ is
differentiable and can be directly optimized end-to-end with a hybrid
optimization target considering both task performance and quantization
benefits. Trained on mixed-quality image datasets, DQMQ can implicitly select
the most proper bit-width for each layer when facing uneven input qualities.
Extensive experiments on various benchmark datasets and networks demonstrate
the superiority of DQMQ against existing fixed/mixed-precision quantization
methods.",None,-1
14188bfa-3f84-4e41-a6fa-7cc511b5a0fe,POE: Process of Elimination for Multiple Choice Reasoning,0.210702,3,"Language models (LMs) are capable of conducting in-context learning for
multiple choice reasoning tasks, but the options in these tasks are treated
equally. As humans often first eliminate wrong options before picking the final
correct answer, we argue a similar two-step strategy can make LMs better at
these tasks. To this end, we present the Process of Elimination (POE), a
two-step scoring method. In the first step, POE scores each option, and
eliminates seemingly wrong options. In the second step, POE masks these wrong
options, and makes the final prediction from the remaining options. Zero-shot
experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a
following analysis finds our method to be especially performant on logical
reasoning tasks. We further analyze the effect of masks, and show that POE
applies to few-shot settings and large language models (LLMs) like ChatGPT.",None,-1
3dcaaee7-b18c-423c-8c32-4477da693c9b,FaceLit: Neural 3D Relightable Faces,0.356079,10,"We propose a generative framework, FaceLit, capable of generating a 3D face
that can be rendered at various user-defined lighting conditions and views,
learned purely from 2D images in-the-wild without any manual annotation. Unlike
existing works that require careful capture setup or human labor, we rely on
off-the-shelf pose and illumination estimators. With these estimates, we
incorporate the Phong reflectance model in the neural volume rendering
framework. Our model learns to generate shape and material properties of a face
such that, when rendered according to the natural statistics of pose and
illumination, produces photorealistic face images with multiview 3D and
illumination consistency. Our method enables photorealistic generation of faces
with explicit illumination and view controls on multiple datasets - FFHQ,
MetFaces and CelebA-HQ. We show state-of-the-art photorealism among 3D aware
GANs on FFHQ dataset achieving an FID score of 3.5.",None,-1
9d4745a0-84eb-4966-93d0-9290fe2e9f13,A Cross-Linguistic Pressure for Uniform Information Density in Word Order,0.540743,2,"While natural languages differ widely in both canonical word order and word
order flexibility, their word orders still follow shared cross-linguistic
statistical patterns, often attributed to functional pressures. In the effort
to identify these pressures, prior work has compared real and counterfactual
word orders. Yet one functional pressure has been overlooked in such
investigations: the uniform information density (UID) hypothesis, which holds
that information should be spread evenly throughout an utterance. Here, we ask
whether a pressure for UID may have influenced word order patterns
cross-linguistically. To this end, we use computational models to test whether
real orders lead to greater information uniformity than counterfactual orders.
In our empirical study of 10 typologically diverse languages, we find that: (i)
among SVO languages, real word orders consistently have greater uniformity than
reverse word orders, and (ii) only linguistically implausible counterfactual
orders consistently exceed the uniformity of real orders. These findings are
compatible with a pressure for information uniformity in the development and
usage of natural languages.",None,-1
4e1bed2b-dfc8-495c-b570-0de837006dc4,Quantum Heavy-tailed Bandits,0.261865,2,"In this paper, we study multi-armed bandits (MAB) and stochastic linear
bandits (SLB) with heavy-tailed rewards and quantum reward oracle. Unlike the
previous work on quantum bandits that assumes bounded/sub-Gaussian
distributions for rewards, here we investigate the quantum bandits problem
under a weaker assumption that the distributions of rewards only have bounded
$(1+v)$-th moment for some $v\in (0,1]$. In order to achieve regret
improvements for heavy-tailed bandits, we first propose a new quantum mean
estimator for heavy-tailed distributions, which is based on the Quantum Monte
Carlo Mean Estimator and achieves a quadratic improvement of estimation error
compared to the classical one. Based on our quantum mean estimator, we focus on
quantum heavy-tailed MAB and SLB and propose quantum algorithms based on the
Upper Confidence Bound (UCB) framework for both problems with
$\Tilde{O}(T^{\frac{1-v}{1+v}})$ regrets, polynomially improving the dependence
in terms of $T$ as compared to classical (near) optimal regrets of
$\Tilde{O}(T^{\frac{1}{1+v}})$, where $T$ is the number of rounds. Finally,
experiments also support our theoretical results and show the effectiveness of
our proposed methods.",None,-1
08bc5a2e-4da2-4d46-9373-70f173d57f5e,RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture,0.803171,18,"The techniques for 3D indoor scene capturing are widely used, but the meshes
produced leave much to be desired. In this paper, we propose ""RoomDreamer"",
which leverages powerful natural language to synthesize a new room with a
different style. Unlike existing image synthesis methods, our work addresses
the challenge of synthesizing both geometry and texture aligned to the input
scene structure and prompt simultaneously. The key insight is that a scene
should be treated as a whole, taking into account both scene texture and
geometry. The proposed framework consists of two significant components:
Geometry Guided Diffusion and Mesh Optimization. Geometry Guided Diffusion for
3D Scene guarantees the consistency of the scene style by applying the 2D prior
to the entire scene simultaneously. Mesh Optimization improves the geometry and
texture jointly and eliminates the artifacts in the scanned scene. To validate
the proposed method, real indoor scenes scanned with smartphones are used for
extensive experiments, through which the effectiveness of our method is
demonstrated.",None,-1
7c59d3d3-3f5c-473a-b739-02a52600df1b,JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning,0.302063,3,"In the ongoing wave of impact driven by large language models (LLMs) like
ChatGPT, the adaptation of LLMs to medical domain has emerged as a crucial
research frontier. Since mainstream LLMs tend to be designed for
general-purpose applications, constructing a medical LLM through domain
adaptation is a huge challenge. While instruction-tuning is used to fine-tune
some LLMs, its precise roles in domain adaptation remain unknown. Here we show
the contribution of LoRA-based instruction-tuning to performance in Japanese
medical question-answering tasks. In doing so, we employ a multifaceted
evaluation for multiple-choice questions, including scoring based on ""Exact
match"" and ""Gestalt distance"" in addition to the conventional accuracy. Our
findings suggest that LoRA-based instruction-tuning can partially incorporate
domain-specific knowledge into LLMs, with larger models demonstrating more
pronounced effects. Furthermore, our results underscore the potential of
adapting English-centric models for Japanese applications in domain adaptation,
while also highlighting the persisting limitations of Japanese-centric models.
This initiative represents a pioneering effort in enabling medical institutions
to fine-tune and operate models without relying on external services.",None,-1
99011642-bab9-47ab-a743-eaa9ba92ce89,Context-Aware Transformer for 3D Point Cloud Automatic Annotation,0.434075,4,"3D automatic annotation has received increased attention since manually
annotating 3D point clouds is laborious. However, existing methods are usually
complicated, e.g., pipelined training for 3D foreground/background
segmentation, cylindrical object proposals, and point completion. Furthermore,
they often overlook the inter-object feature relation that is particularly
informative to hard samples for 3D annotation. To this end, we propose a simple
yet effective end-to-end Context-Aware Transformer (CAT) as an automated 3D-box
labeler to generate precise 3D box annotations from 2D boxes, trained with a
small number of human annotations. We adopt the general encoder-decoder
architecture, where the CAT encoder consists of an intra-object encoder (local)
and an inter-object encoder (global), performing self-attention along the
sequence and batch dimensions, respectively. The former models intra-object
interactions among points, and the latter extracts feature relations among
different objects, thus boosting scene-level understanding. Via local and
global encoders, CAT can generate high-quality 3D box annotations with a
streamlined workflow, allowing it to outperform existing state-of-the-art by up
to 1.79% 3D AP on the hard task of the KITTI test set.",None,-1
f34c142c-a718-4e46-adcf-a5ec0332ae81,CalibNet: Dual-branch Cross-modal Calibration for RGB-D Salient Instance Segmentation,0.454504,2,"We propose a novel approach for RGB-D salient instance segmentation using a
dual-branch cross-modal feature calibration architecture called CalibNet. Our
method simultaneously calibrates depth and RGB features in the kernel and mask
branches to generate instance-aware kernels and mask features. CalibNet
consists of three simple modules, a dynamic interactive kernel (DIK) and a
weight-sharing fusion (WSF), which work together to generate effective
instance-aware kernels and integrate cross-modal features. To improve the
quality of depth features, we incorporate a depth similarity assessment (DSA)
module prior to DIK and WSF. In addition, we further contribute a new DSIS
dataset, which contains 1,940 images with elaborate instance-level annotations.
Extensive experiments on three challenging benchmarks show that CalibNet yields
a promising result, i.e., 58.0% AP with 320*480 input size on the COME15K-N
test set, which significantly surpasses the alternative frameworks. Our code
and dataset are available at: https://github.com/PJLallen/CalibNet.",None,-1
f3226198-9a1c-4a8c-8170-e9ac2e7d6c43,"Better ""CMOS"" Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution",0.350254,4,"Most of the existing blind image Super-Resolution (SR) methods assume that
the blur kernels are space-invariant. However, the blur involved in real
applications are usually space-variant due to object motion, out-of-focus,
etc., resulting in severe performance drop of the advanced SR methods. To
address this problem, we firstly introduce two new datasets with out-of-focus
blur, i.e., NYUv2-BSR and Cityscapes-BSR, to support further researches of
blind SR with space-variant blur. Based on the datasets, we design a novel
Cross-MOdal fuSion network (CMOS) that estimate both blur and semantics
simultaneously, which leads to improved SR results. It involves a feature
Grouping Interactive Attention (GIA) module to make the two modalities interact
more effectively and avoid inconsistency. GIA can also be used for the
interaction of other features because of the universality of its structure.
Qualitative and quantitative experiments compared with state-of-the-art methods
on above datasets and real-world images demonstrate the superiority of our
method, e.g., obtaining PSNR/SSIM by +1.91/+0.0048 on NYUv2-BSR than MANet.",None,-1
32084fa0-a872-4920-b7b0-4852455b3c6a,A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores,0.0362095,1,"Large Language Models (LLMs), such as ChatGPT, have achieved impressive
milestones in natural language processing (NLP). Despite their impressive
performance, the models are known to pose important risks. As these models are
deployed in real-world applications, a systematic understanding of different
risks posed by these models on tasks such as natural language inference (NLI),
is much needed. In this paper, we define and formalize two distinct types of
risk: decision risk and composite risk. We also propose a risk-centric
evaluation framework, and four novel metrics, for assessing LLMs on these risks
in both in-domain and out-of-domain settings. Finally, we propose a
risk-adjusted calibration method called DwD for helping LLMs minimize these
risks in an overall NLI architecture. Detailed experiments, using four NLI
benchmarks, three baselines and two LLMs, including ChatGPT, show both the
practical utility of the evaluation framework, and the efficacy of DwD in
reducing decision and composite risk. For instance, when using DwD, an
underlying LLM is able to address an extra 20.1% of low-risk inference tasks
(but which the LLM erroneously deems high-risk without risk adjustment) and
skip a further 19.8% of high-risk tasks, which would have been answered
incorrectly.",None,-1
2c32e4c8-d4db-4036-b50d-7676f417a09b,Improving Joint Speech-Text Representations Without Alignment,0.434204,3,"The last year has seen astonishing progress in text-prompted image generation
premised on the idea of a cross-modal representation space in which the text
and image domains are represented jointly. In ASR, this idea has found
application as joint speech-text encoders that can scale to the capacities of
very large parameter models by being trained on both unpaired speech and text.
While these methods show promise, they have required special treatment of the
sequence-length mismatch inherent in speech and text, either by up-sampling
heuristics or an explicit alignment model. In this work, we offer evidence that
joint speech-text encoders naturally achieve consistent representations across
modalities by disregarding sequence length, and argue that consistency losses
could forgive length differences and simply assume the best alignment. We show
that such a loss improves downstream WER in both a large-parameter monolingual
and multilingual system.",None,-1
06f0f0f3-c2c5-4ed7-a16b-1fc67d2d66dc,Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer,0.616643,30,"Diffusion models have shown great promise in text-guided image style
transfer, but there is a trade-off between style transformation and content
preservation due to their stochastic nature. Existing methods require
computationally expensive fine-tuning of diffusion models or additional neural
network. To address this, here we propose a zero-shot contrastive loss for
diffusion models that doesn't require additional fine-tuning or auxiliary
networks. By leveraging patch-wise contrastive loss between generated samples
and original image embeddings in the pre-trained diffusion model, our method
can generate images with the same semantic content as the source image in a
zero-shot manner. Our approach outperforms existing methods while preserving
content and requiring no additional training, not only for image style transfer
but also for image-to-image translation and manipulation. Our experimental
results validate the effectiveness of our proposed method.",None,-1
231790db-a4eb-48ed-b267-9f475f104aa6,Learning to Generate Equitable Text in Dialogue from Biased Training Data,0.123227,6,"The ingrained principles of fairness in a dialogue system's decision-making
process and generated responses are crucial for user engagement, satisfaction,
and task achievement. Absence of equitable and inclusive principles can hinder
the formation of common ground, which in turn negatively impacts the overall
performance of the system. For example, misusing pronouns in a user interaction
may cause ambiguity about the intended subject. Yet, there is no comprehensive
study of equitable text generation in dialogue. Aptly, in this work, we use
theories of computational learning to study this problem. We provide formal
definitions of equity in text generation, and further, prove formal connections
between learning human-likeness and learning equity: algorithms for improving
equity ultimately reduce to algorithms for improving human-likeness (on
augmented data). With this insight, we also formulate reasonable conditions
under which text generation algorithms can learn to generate equitable text
without any modifications to the biased training data on which they learn. To
exemplify our theory in practice, we look at a group of algorithms for the
GuessWhat?! visual dialogue game and, using this example, test our theory
empirically. Our theory accurately predicts relative-performance of multiple
algorithms in generating equitable text as measured by both human and automated
evaluation.",None,-1
ca558118-933b-4f55-9d24-f448e426867b,Weight-Inherited Distillation for Task-Agnostic BERT Compression,0.476911,5,"Knowledge Distillation (KD) is a predominant approach for BERT compression.
Previous KD-based methods focus on designing extra alignment losses for the
student model to mimic the behavior of the teacher model. These methods
transfer the knowledge in an indirect way. In this paper, we propose a novel
Weight-Inherited Distillation (WID), which directly transfers knowledge from
the teacher. WID does not require any additional alignment loss and trains a
compact student by inheriting the weights, showing a new perspective of
knowledge distillation. Specifically, we design the row compactors and column
compactors as mappings and then compress the weights via structural
re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show
that WID outperforms previous state-of-the-art KD-based baselines. Further
analysis indicates that WID can also learn the attention patterns from the
teacher model without any alignment loss on attention distributions. The code
is available at https://github.com/wutaiqiang/WID-NAACL2024.",None,-1
4aa997b5-7977-4e36-92af-1cfa978790b9,Contrastive Loss is All You Need to Recover Analogies as Parallel Lines,0.352717,2,"While static word embedding models are known to represent linguistic
analogies as parallel lines in high-dimensional space, the underlying mechanism
as to why they result in such geometric structures remains obscure. We find
that an elementary contrastive-style method employed over distributional
information performs competitively with popular word embedding models on
analogy recovery tasks, while achieving dramatic speedups in training time.
Further, we demonstrate that a contrastive loss is sufficient to create these
parallel structures in word embeddings, and establish a precise relationship
between the co-occurrence statistics and the geometric structure of the
resulting word embeddings.",None,-1
a3697916-79fd-4456-94aa-fa23512bb62a,Student Classroom Behavior Detection based on Improved YOLOv7,0.282319,2,"Accurately detecting student behavior in classroom videos can aid in
analyzing their classroom performance and improving teaching effectiveness.
However, the current accuracy rate in behavior detection is low. To address
this challenge, we propose the Student Classroom Behavior Detection method,
based on improved YOLOv7. First, we created the Student Classroom Behavior
dataset (SCB-Dataset), which includes 18.4k labels and 4.2k images, covering
three behaviors: hand raising, reading, and writing. To improve detection
accuracy in crowded scenes, we integrated the biformer attention module and
Wise-IoU into the YOLOv7 network. Finally, experiments were conducted on the
SCB-Dataset, and the model achieved an mAP@0.5 of 79%, resulting in a 1.8%
improvement over previous results. The SCB-Dataset and code are available for
download at: https://github.com/Whiffe/SCB-dataset.",None,-1
6ec36090-0c69-4b1e-9fbe-80567beb9340,Can we trust the evaluation on ChatGPT?,0.796242,66,"ChatGPT, the first large language model (LLM) with mass adoption, has
demonstrated remarkable performance in numerous natural language tasks. Despite
its evident usefulness, evaluating ChatGPT's performance in diverse problem
domains remains challenging due to the closed nature of the model and its
continuous updates via Reinforcement Learning from Human Feedback (RLHF). We
highlight the issue of data contamination in ChatGPT evaluations, with a case
study of the task of stance detection. We discuss the challenge of preventing
data contamination and ensuring fair model evaluation in the age of closed and
continuously trained models.",None,-1
c0fb7066-e0fc-4264-9e6a-46346927fffa,See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data,0.944897,10,"Zero-shot point cloud segmentation aims to make deep models capable of
recognizing novel objects in point cloud that are unseen in the training phase.
Recent trends favor the pipeline which transfers knowledge from seen classes
with labels to unseen classes without labels. They typically align visual
features with semantic features obtained from word embedding by the supervision
of seen classes' annotations. However, point cloud contains limited information
to fully match with semantic features. In fact, the rich appearance information
of images is a natural complement to the textureless point cloud, which is not
well explored in previous literature. Motivated by this, we propose a novel
multi-modal zero-shot learning method to better utilize the complementary
information of point clouds and images for more accurate visual-semantic
alignment. Extensive experiments are performed in two popular benchmarks, i.e.,
SemanticKITTI and nuScenes, and our method outperforms current SOTA methods
with 52% and 49% improvement on average for unseen class mIoU, respectively.",None,-1
1b003adc-dbf1-48d7-94fe-b3190bee8951,Whether you can locate or not? Interactive Referring Expression Generation,0.214971,2,"Referring Expression Generation (REG) aims to generate unambiguous Referring
Expressions (REs) for objects in a visual scene, with a dual task of Referring
Expression Comprehension (REC) to locate the referred object. Existing methods
construct REG models independently by using only the REs as ground truth for
model training, without considering the potential interaction between REG and
REC models. In this paper, we propose an Interactive REG (IREG) model that can
interact with a real REC model, utilizing signals indicating whether the object
is located and the visual region located by the REC model to gradually modify
REs. Our experimental results on three RE benchmark datasets, RefCOCO,
RefCOCO+, and RefCOCOg show that IREG outperforms previous state-of-the-art
methods on popular evaluation metrics. Furthermore, a human evaluation shows
that IREG generates better REs with the capability of interaction.",None,-1
365e10ed-70ff-4c07-96da-44e2fb926c67,Deep Image Compression Using Scene Text Quality Assessment,0.402196,3,"Image compression is a fundamental technology for Internet communication
engineering. However, a high compression rate with general methods may degrade
images, resulting in unreadable texts. In this paper, we propose an image
compression method for maintaining text quality. We developed a scene text
image quality assessment model to assess text quality in compressed images. The
assessment model iteratively searches for the best-compressed image holding
high-quality text. Objective and subjective results showed that the proposed
method was superior to existing methods. Furthermore, the proposed assessment
model outperformed other deep-learning regression models.",None,-1
51390cab-2b87-4694-941c-f16d7cf4f0e7,LEGO-Prover: Neural Theorem Proving with Growing Libraries,0.999444,12,"Despite the success of large language models (LLMs), the task of theorem
proving still remains one of the hardest reasoning tasks that is far from being
fully solved. Prior methods using language models have demonstrated promising
results, but they still struggle to prove even middle school level theorems.
One common limitation of these methods is that they assume a fixed theorem
library during the whole theorem proving process. However, as we all know,
creating new useful theorems or even new theories is not only helpful but
crucial and necessary for advancing mathematics and proving harder and deeper
results. In this work, we present LEGO-Prover, which employs a growing skill
library containing verified lemmas as skills to augment the capability of LLMs
used in theorem proving. By constructing the proof modularly, LEGO-Prover
enables LLMs to utilize existing skills retrieved from the library and to
create new skills during the proving process. These skills are further evolved
(by prompting an LLM) to enrich the library on another scale. Modular and
reusable skills are constantly added to the library to enable tackling
increasingly intricate mathematical problems. Moreover, the learned library
further bridges the gap between human proofs and formal proofs by making it
easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass
rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%).
During the proving process, LEGO-Prover also manages to generate over 20,000
skills (theorems/lemmas) and adds them to the growing library. Our ablation
study indicates that these newly added skills are indeed helpful for proving
theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We
also release our code and all the generated skills.",None,-1
f76d584f-87cf-447b-9b6b-ce39d068a302,Improving Generalization in Language Model-Based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-Based Techniques,0.706398,9,"Compositional and domain generalization present significant challenges in
semantic parsing, even for state-of-the-art semantic parsers based on
pre-trained language models (LMs). In this study, we empirically investigate
improving an LM's generalization in semantic parsing with two simple
techniques: at the token level, we introduce a token preprocessing method to
preserve the semantic boundaries of tokens produced by LM tokenizers; at the
sequence level, we propose to use special tokens to mark the boundaries of
components aligned between input and output. Our experimental results on two
text-to-SQL semantic parsing datasets show that our token preprocessing,
although simple, can substantially improve the LM performance on both types of
generalization, and our component boundary marking method is particularly
helpful for compositional generalization.",None,-1
4a6793c4-7b28-4c52-a35d-8a84253f890e,KNSE: A Knowledge-aware Natural Language Inference Framework for Dialogue Symptom Status Recognition,0.694012,3,"Symptom diagnosis in medical conversations aims to correctly extract both
symptom entities and their status from the doctor-patient dialogue. In this
paper, we propose a novel framework called KNSE for symptom status recognition
(SSR), where the SSR is formulated as a natural language inference (NLI) task.
For each mentioned symptom in a dialogue window, we first generate knowledge
about the symptom and hypothesis about status of the symptom, to form a
(premise, knowledge, hypothesis) triplet. The BERT model is then used to encode
the triplet, which is further processed by modules including utterance
aggregation, self-attention, cross-attention, and GRU to predict the symptom
status. Benefiting from the NLI formalization, the proposed framework can
encode more informative prior knowledge to better localize and track symptom
status, which can effectively improve the performance of symptom status
recognition. Preliminary experiments on Chinese medical dialogue datasets show
that KNSE outperforms previous competitive baselines and has advantages in
cross-disease and cross-symptom scenarios.",None,-1
06facb8f-ae88-415e-8e76-3e2337078789,Large Language Models Can Be Easily Distracted by Irrelevant Context,0.998703,277,"Large language models have achieved impressive performance on various natural
language processing tasks. However, so far they have been evaluated primarily
on benchmarks where all information in the input context is relevant for
solving the task. In this work, we investigate the distractibility of large
language models, i.e., how the model problem-solving accuracy can be influenced
by irrelevant context. In particular, we introduce Grade-School Math with
Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant
information in the problem description. We use this benchmark to measure the
distractibility of cutting-edge prompting techniques for large language models,
and find that the model performance is dramatically decreased when irrelevant
information is included. We also identify several approaches for mitigating
this deficiency, such as decoding with self-consistency and adding to the
prompt an instruction that tells the language model to ignore the irrelevant
information.",None,-1
f8381ecf-af05-4b0b-94bc-0551ab958970,MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset,0.72586,6,"Sentence Boundary Detection (SBD) is one of the foundational building blocks
of Natural Language Processing (NLP), with incorrectly split sentences heavily
influencing the output quality of downstream tasks. It is a challenging task
for algorithms, especially in the legal domain, considering the complex and
different sentence structures used. In this work, we curated a diverse
multilingual legal dataset consisting of over 130'000 annotated sentences in 6
languages. Our experimental results indicate that the performance of existing
SBD models is subpar on multilingual legal data. We trained and tested
monolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers,
demonstrating state-of-the-art performance. We also show that our multilingual
models outperform all baselines in the zero-shot setting on a Portuguese test
set. To encourage further research and development by the community, we have
made our dataset, models, and code publicly available.",None,-1
c84292f3-7290-4c79-9533-803612e47c0c,Learn to Not Link: Exploring NIL Prediction in Entity Linking,0.467514,5,"Entity linking models have achieved significant success via utilizing
pretrained language models to capture semantic features. However, the NIL
prediction problem, which aims to identify mentions without a corresponding
entity in the knowledge base, has received insufficient attention. We
categorize mentions linking to NIL into Missing Entity and Non-Entity Phrase,
and propose an entity linking dataset NEL that focuses on the NIL prediction
problem. NEL takes ambiguous entities as seeds, collects relevant mention
context in the Wikipedia corpus, and ensures the presence of mentions linking
to NIL by human annotation and entity masking. We conduct a series of
experiments with the widely used bi-encoder and cross-encoder entity linking
models, results show that both types of NIL mentions in training data have a
significant influence on the accuracy of NIL prediction. Our code and dataset
can be accessed at https://github.com/solitaryzero/NIL_EL",None,-1
4650d7d1-26b9-48a2-beec-be42e4bdf164,Assessing the Reliability of Large Language Model Knowledge,0.158461,6,"Large language models (LLMs) have been treated as knowledge bases due to
their strong performance in knowledge probing tasks. LLMs are typically
evaluated using accuracy, yet this metric does not capture the vulnerability of
LLMs to hallucination-inducing factors like prompt and context variability. How
do we evaluate the capabilities of LLMs to consistently produce factually
correct answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe
(MONITOR), a novel metric designed to directly measure LLMs' factual
reliability. MONITOR computes the distance between the probability
distributions of a valid output and its counterparts produced by the same LLM
probing the same fact using different styles of prompts and
contexts.Experiments on a comprehensive range of 12 LLMs demonstrate the
effectiveness of MONITOR in evaluating the factual reliability of LLMs while
maintaining a low computational overhead. In addition, we release the FKTC
(Factual Knowledge Test Corpus) test set, containing 210,158 prompts in total
to foster research along this line (https://github.com/Vicky-Wil/MONITOR).",None,-1
31334cd0-68e3-45fa-8663-52d0564ef927,Adaptive manifold for imbalanced transductive few-shot learning,0.20804,1,"Transductive few-shot learning algorithms have showed substantially superior
performance over their inductive counterparts by leveraging the unlabeled
queries. However, the vast majority of such methods are evaluated on perfectly
class-balanced benchmarks. It has been shown that they undergo remarkable drop
in performance under a more realistic, imbalanced setting. To this end, we
propose a novel algorithm to address imbalanced transductive few-shot learning,
named Adaptive Manifold. Our method exploits the underlying manifold of the
labeled support examples and unlabeled queries by using manifold similarity to
predict the class probability distribution per query. It is parameterized by
one centroid per class as well as a set of graph-specific parameters that
determine the manifold. All parameters are optimized through a loss function
that can be tuned towards class-balanced or imbalanced distributions. The
manifold similarity shows substantial improvement over Euclidean distance,
especially in the 1-shot setting. Our algorithm outperforms or is on par with
other state of the art methods in three benchmark datasets, namely
miniImageNet, tieredImageNet and CUB, and three different backbones, namely
ResNet-18, WideResNet-28-10 and DenseNet-121. In certain cases, our algorithm
outperforms the previous state of the art by as much as 4.2%.",None,-1
e0c0b9d5-0e52-4446-b072-e1b905cca266,Diagnosing and Rectifying Vision Models using Language,0.648431,21,"Recent multi-modal contrastive learning models have demonstrated the ability
to learn an embedding space suitable for building strong vision classifiers, by
leveraging the rich information in large-scale image-caption datasets. Our work
highlights a distinct advantage of this multi-modal embedding space: the
ability to diagnose vision classifiers through natural language. The
traditional process of diagnosing model behaviors in deployment settings
involves labor-intensive data acquisition and annotation. Our proposed method
can discover high-error data slices, identify influential attributes and
further rectify undesirable model behaviors, without requiring any visual data.
Through a combination of theoretical explanation and empirical verification, we
present conditions under which classifiers trained on embeddings from one
modality can be equivalently applied to embeddings from another modality. On a
range of image datasets with known error slices, we demonstrate that our method
can effectively identify the error slices and influential attributes, and can
further use language to rectify failure modes of the classifier.",None,-1
7c69bb95-2383-4d25-9d86-f789401e7d04,ChatHaruhi: Reviving Anime Character in Reality via Large Language Model,0.658915,13,"Role-playing chatbots built on large language models have drawn interest, but
better techniques are needed to enable mimicking specific fictional characters.
We propose an algorithm that controls language models via an improved prompt
and memories of the character extracted from scripts. We construct ChatHaruhi,
a dataset covering 32 Chinese / English TV / anime characters with over 54k
simulated dialogues. Both automatic and human evaluations show our approach
improves role-playing ability over baselines. Code and data are available at
https://github.com/LC1332/Chat-Haruhi-Suzumiya .",None,-1
7dfe3f22-6e3d-4f65-8a2d-618358076405,Sequential Integrated Gradients: a simple but effective method for explaining language models,0.445952,17,"Several explanation methods such as Integrated Gradients (IG) can be
characterised as path-based methods, as they rely on a straight line between
the data and an uninformative baseline. However, when applied to language
models, these methods produce a path for each word of a sentence
simultaneously, which could lead to creating sentences from interpolated words
either having no clear meaning, or having a significantly different meaning
compared to the original sentence. In order to keep the meaning of these
sentences as close as possible to the original one, we propose Sequential
Integrated Gradients (SIG), which computes the importance of each word in a
sentence by keeping fixed every other words, only creating interpolations
between the baseline and the word of interest. Moreover, inspired by the
training procedure of several language models, we also propose to replace the
baseline token ""pad"" with the trained token ""mask"". While being a simple
improvement over the original IG method, we show on various models and datasets
that SIG proves to be a very effective method for explaining language models.",None,-1
7348bd54-95b3-46b8-8267-3cb8130adafd,Neural Compositional Rule Learning for Knowledge Graph Reasoning,0.498224,9,"Learning logical rules is critical to improving reasoning in KGs. This is due
to their ability to provide logical and interpretable explanations when used
for predictions, as well as their ability to generalize to other tasks,
domains, and data. While recent methods have been proposed to learn logical
rules, the majority of these methods are either restricted by their
computational complexity and can not handle the large search space of
large-scale KGs, or show poor generalization when exposed to data outside the
training set. In this paper, we propose an end-to-end neural model for learning
compositional logical rules called NCRL. NCRL detects the best compositional
structure of a rule body, and breaks it into small compositions in order to
infer the rule head. By recurrently merging compositions in the rule body with
a recurrent attention unit, NCRL finally predicts a single rule head.
Experimental results show that NCRL learns high-quality rules, as well as being
generalizable. Specifically, we show that NCRL is scalable, efficient, and
yields state-of-the-art results for knowledge graph completion on large-scale
KGs. Moreover, we test NCRL for systematic generalization by learning to reason
on small-scale observed graphs and evaluating on larger unseen ones.",None,-1
4b8c5116-43b9-409b-9b97-13f0b6a481b8,Cross-lingual Transfer Can Worsen Bias in Sentiment Analysis,0.260814,3,"Sentiment analysis (SA) systems are widely deployed in many of the world's
languages, and there is well-documented evidence of demographic bias in these
systems. In languages beyond English, scarcer training data is often
supplemented with transfer learning using pre-trained models, including
multilingual models trained on other languages. In some cases, even supervision
data comes from other languages. Does cross-lingual transfer also import new
biases? To answer this question, we use counterfactual evaluation to test
whether gender or racial biases are imported when using cross-lingual transfer,
compared to a monolingual transfer setting. Across five languages, we find that
systems using cross-lingual transfer usually become more biased than their
monolingual counterparts. We also find racial biases to be much more prevalent
than gender biases. To spur further research on this topic, we release the
sentiment models we used for this study, and the intermediate checkpoints
throughout training, yielding 1,525 distinct models; we also release our
evaluation code.",None,-1
88de5df8-3d8b-4b39-93d8-8b4e86d92419,"Explaining Groups of Instances Counterfactually for XAI: A Use Case, Algorithm and User Study for Group-Counterfactuals",0.428229,7,"Counterfactual explanations are an increasingly popular form of post hoc
explanation due to their (i) applicability across problem domains, (ii)
proposed legal compliance (e.g., with GDPR), and (iii) reliance on the
contrastive nature of human explanation. Although counterfactual explanations
are normally used to explain individual predictive-instances, we explore a
novel use case in which groups of similar instances are explained in a
collective fashion using ``group counterfactuals'' (e.g., to highlight a
repeating pattern of illness in a group of patients). These group
counterfactuals meet a human preference for coherent, broad explanations
covering multiple events/instances. A novel, group-counterfactual algorithm is
proposed to generate high-coverage explanations that are faithful to the
to-be-explained model. This explanation strategy is also evaluated in a large,
controlled user study (N=207), using objective (i.e., accuracy) and subjective
(i.e., confidence, explanation satisfaction, and trust) psychological measures.
The results show that group counterfactuals elicit modest but definite
improvements in people's understanding of an AI system. The implications of
these findings for counterfactual methods and for XAI are discussed.",None,-1
b86bc976-94b0-4094-aec5-42ffebc70cdc,Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning,0.411869,2,"Retrosynthesis consists of breaking down a chemical compound recursively
step-by-step into molecular precursors until a set of commercially available
molecules is found with the goal to provide a synthesis route. Its two primary
research directions, single-step retrosynthesis prediction, which models the
chemical reaction logic, and multi-step synthesis planning, which tries to find
the correct sequence of reactions, are inherently intertwined. Still, this
connection is not reflected in contemporary research. In this work, we combine
these two major research directions by applying multiple single-step
retrosynthesis models within multi-step synthesis planning and analyzing their
impact using public and proprietary reaction data. We find a disconnection
between high single-step performance and potential route-finding success,
suggesting that single-step models must be evaluated within synthesis planning
in the future. Furthermore, we show that the commonly used single-step
retrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluation
task does not represent model performance and scalability on larger and more
diverse datasets. For multi-step synthesis planning, we show that the choice of
the single-step model can improve the overall success rate of synthesis
planning by up to +28% compared to the commonly used baseline model. Finally,
we show that each single-step model finds unique synthesis routes, and differs
in aspects such as route-finding success, the number of found synthesis routes,
and chemical validity, making the combination of single-step retrosynthesis
prediction and multi-step synthesis planning a crucial aspect when developing
future methods.",None,-1
37bd1dbd-2e23-4846-887e-eca13d5fd58d,Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark,0.994129,47,"Recent studies have demonstrated promising potential of ChatGPT for various
text annotation and classification tasks. However, ChatGPT is non-deterministic
which means that, as with human coders, identical input can lead to different
outputs. Given this, it seems appropriate to test the reliability of ChatGPT.
Therefore, this study investigates the consistency of ChatGPT's zero-shot
capabilities for text annotation and classification, focusing on different
model parameters, prompt variations, and repetitions of identical inputs. Based
on the real-world classification task of differentiating website texts into
news and not news, results show that consistency in ChatGPT's classification
output can fall short of scientific thresholds for reliability. For example,
even minor wording alterations in prompts or repeating the identical input can
lead to varying outputs. Although pooling outputs from multiple repetitions can
improve reliability, this study advises caution when using ChatGPT for
zero-shot text annotation and underscores the need for thorough validation,
such as comparison against human-annotated data. The unsupervised application
of ChatGPT for text annotation and classification is not recommended.",None,-1
dd6eade6-9e50-4f0e-9cc9-bd23f0dc81ea,Evaluating the Robustness of Machine Reading Comprehension Models to Low Resource Entity Renaming,0.424571,2,"Question answering (QA) models have shown compelling results in the task of
Machine Reading Comprehension (MRC). Recently these systems have proved to
perform better than humans on held-out test sets of datasets e.g. SQuAD, but
their robustness is not guaranteed. The QA model's brittleness is exposed when
evaluated on adversarial generated examples by a performance drop. In this
study, we explore the robustness of MRC models to entity renaming, with
entities from low-resource regions such as Africa. We propose EntSwap, a method
for test-time perturbations, to create a test set whose entities have been
renamed. In particular, we rename entities of type: country, person,
nationality, location, organization, and city, to create AfriSQuAD2. Using the
perturbed test set, we evaluate the robustness of three popular MRC models. We
find that compared to base models, large models perform well comparatively on
novel entities. Furthermore, our analysis indicates that entity type person
highly challenges the MRC models' performance.",None,-1
7250cedb-9316-4b2d-964c-90aa7117ac73,Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?,0.74298,20,"Large language models (LLMs) such as ChatGPT are increasingly being used for
various use cases, including text content generation at scale. Although
detection methods for such AI-generated text exist already, we investigate
ChatGPT's performance as a detector on such AI-generated text, inspired by
works that use ChatGPT as a data labeler or annotator. We evaluate the
zero-shot performance of ChatGPT in the task of human-written vs. AI-generated
text detection, and perform experiments on publicly available datasets. We
empirically investigate if ChatGPT is symmetrically effective in detecting
AI-generated or human-written text. Our findings provide insight on how ChatGPT
and similar LLMs may be leveraged in automated detection pipelines by simply
focusing on solving a specific aspect of the problem and deriving the rest from
that solution. All code and data is available at
https://github.com/AmritaBh/ChatGPT-as-Detector.",None,-1
55ae226e-6607-4f42-a4e7-b5c824ea525d,RoboCLIP: One Demonstration is Enough to Learn Robot Policies,0.906173,20,"Reward specification is a notoriously difficult problem in reinforcement
learning, requiring extensive expert supervision to design robust reward
functions. Imitation learning (IL) methods attempt to circumvent these problems
by utilizing expert demonstrations but typically require a large number of
in-domain expert demonstrations. Inspired by advances in the field of
Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation
learning method that uses a single demonstration (overcoming the large data
requirement) in the form of a video demonstration or a textual description of
the task to generate rewards without manual reward function design.
Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like
videos of humans solving the task for reward generation, circumventing the need
to have the same demonstration and deployment domains. RoboCLIP utilizes
pretrained VLMs without any finetuning for reward generation. Reinforcement
learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher
zero-shot performance than competing imitation learning methods on downstream
robot manipulation tasks, doing so using only one video/text demonstration.",None,-1
13fed991-a729-4696-8b99-c6ddf7f94561,Continuous Trajectory Generation Based on Two-Stage GAN,0.388003,11,"Simulating the human mobility and generating large-scale trajectories are of
great use in many real-world applications, such as urban planning, epidemic
spreading analysis, and geographic privacy protect. Although many previous
works have studied the problem of trajectory generation, the continuity of the
generated trajectories has been neglected, which makes these methods useless
for practical urban simulation scenarios. To solve this problem, we propose a
novel two-stage generative adversarial framework to generate the continuous
trajectory on the road network, namely TS-TrajGen, which efficiently integrates
prior domain knowledge of human mobility with model-free learning paradigm.
Specifically, we build the generator under the human mobility hypothesis of the
A* algorithm to learn the human mobility behavior. For the discriminator, we
combine the sequential reward with the mobility yaw reward to enhance the
effectiveness of the generator. Finally, we propose a novel two-stage
generation process to overcome the weak point of the existing stochastic
generation process. Extensive experiments on two real-world datasets and two
case studies demonstrate that our framework yields significant improvements
over the state-of-the-art methods.",None,-1
3ee22909-1e94-42fd-9f68-445258c16375,Continual Learning Under Language Shift,0.403731,2,"The recent increase in data and model scale for language model pre-training
has led to huge training costs. In scenarios where new data become available
over time, updating a model instead of fully retraining it would therefore
provide significant gains. We study the pros and cons of updating a language
model when new data comes from new languages -- the case of continual learning
under language shift. Starting from a monolingual English language model, we
incrementally add data from Danish, Icelandic, and Norwegian to investigate how
forward and backward transfer effects depend on pre-training order and
characteristics of languages, for three different model sizes. Our results show
that, while forward transfer is largely positive and independent of language
order, backward transfer can be positive or negative depending on the order and
characteristics of new languages. We explore a number of potentially
explanatory factors and find that a combination of language contamination and
syntactic similarity best fits our results.",None,-1
36e4e50a-1127-4f00-8f69-0e044756255c,Chatbots as Problem Solvers: Playing Twenty Questions with Role Reversals,0.138944,3,"New chat AI applications like ChatGPT offer an advanced understanding of
question context and memory across multi-step tasks, such that experiments can
test its deductive reasoning. This paper proposes a multi-role and multi-step
challenge, where ChatGPT plays the classic twenty-questions game but
innovatively switches roles from the questioner to the answerer. The main
empirical result establishes that this generation of chat applications can
guess random object names in fewer than twenty questions (average, 12) and
correctly guess 94% of the time across sixteen different experimental setups.
The research introduces four novel cases where the chatbot fields the
questions, asks the questions, both question-answer roles, and finally tries to
guess appropriate contextual emotions. One task that humans typically fail but
trained chat applications complete involves playing bilingual games of twenty
questions (English answers to Spanish questions). Future variations address
direct problem-solving using a similar inquisitive format to arrive at novel
outcomes deductively, such as patentable inventions or combination thinking.
Featured applications of this dialogue format include complex protein designs,
neuroscience metadata, and child development educational materials.",None,-1
d9b05e03-8fb8-49b3-ba69-60a2b04a1e4a,Therbligs in Action: Video Understanding through Motion Primitives,0.523283,6,"In this paper we introduce a rule-based, compositional, and hierarchical
modeling of action using Therbligs as our atoms. Introducing these atoms
provides us with a consistent, expressive, contact-centered representation of
action. Over the atoms we introduce a differentiable method of rule-based
reasoning to regularize for logical consistency. Our approach is complementary
to other approaches in that the Therblig-based representations produced by our
architecture augment rather than replace existing architectures'
representations. We release the first Therblig-centered annotations over two
popular video datasets - EPIC Kitchens 100 and 50-Salads. We also broadly
demonstrate benefits to adopting Therblig representations through evaluation on
the following tasks: action segmentation, action anticipation, and action
recognition - observing an average 10.5\%/7.53\%/6.5\% relative improvement,
respectively, over EPIC Kitchens and an average 8.9\%/6.63\%/4.8\% relative
improvement, respectively, over 50 Salads. Code and data will be made publicly
available.",None,-1
d812d48c-750a-4573-a0e4-c1b5ee1e734d,Learning Universal Policies via Text-Guided Video Generation,0.975186,103,"A goal of artificial intelligence is to construct an agent that can solve a
wide variety of tasks. Recent progress in text-guided image synthesis has
yielded models with an impressive ability to generate complex novel images,
exhibiting combinatorial generalization across domains. Motivated by this
success, we investigate whether such tools can be used to construct more
general-purpose agents. Specifically, we cast the sequential decision making
problem as a text-conditioned video generation problem, where, given a
text-encoded specification of a desired goal, a planner synthesizes a set of
future frames depicting its planned actions in the future, after which control
actions are extracted from the generated video. By leveraging text as the
underlying goal specification, we are able to naturally and combinatorially
generalize to novel goals. The proposed policy-as-video formulation can further
represent environments with different state and action spaces in a unified
space of images, which, for example, enables learning and generalization across
a variety of robot manipulation tasks. Finally, by leveraging pretrained
language embeddings and widely available videos from the internet, the approach
enables knowledge transfer through predicting highly realistic video plans for
real robots.",None,-1
fe304db5-2292-4aca-88c6-b919981eefa4,Do Language Models' Words Refer?,0.0226404,3,"What do language models (LMs) do with language? Everyone agrees that they can
produce sequences of (mostly) coherent strings of English. But do those
sentences mean something, or are LMs simply babbling in a convincing simulacrum
of language use? Here we will address one aspect of this broad question:
whether LMs' words can refer, that is, achieve ""word-to-world"" connections.
There is prima facie reason to think they do not since LMs do not interact with
the world in the way that ordinary language users do. Drawing on insights from
the externalist tradition in philosophy of language, we argue that those
appearances are misleading: even if the inputs to an LM are simply strings of
text, they are strings of text with natural histories, and that may suffice to
put LMs' words into referential contact with the external world.",None,-1
85d876a0-e358-42de-ac4b-cf0aa3cdf73c,MeshDiffusion: Score-based Generative 3D Mesh Modeling,0.974273,91,"We consider the task of generating realistic 3D shapes, which is useful for a
variety of applications such as automatic scene generation and physical
simulation. Compared to other 3D representations like voxels and point clouds,
meshes are more desirable in practice, because (1) they enable easy and
arbitrary manipulation of shapes for relighting and simulation, and (2) they
can fully leverage the power of modern graphics pipelines which are mostly
optimized for meshes. Previous scalable methods for generating meshes typically
rely on sub-optimal post-processing, and they tend to produce overly-smooth or
noisy surfaces without fine-grained geometric details. To overcome these
shortcomings, we take advantage of the graph structure of meshes and use a
simple yet very effective generative modeling method to generate 3D meshes.
Specifically, we represent meshes with deformable tetrahedral grids, and then
train a diffusion model on this direct parametrization. We demonstrate the
effectiveness of our model on multiple generative tasks.",None,-1
af32bf6b-ee42-49f7-89a6-7c30532ff01c,ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text,0.848816,26,"We present an overview of the ArAIEval shared task, organized as part of the
first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two
tasks over Arabic text: (i) persuasion technique detection, focusing on
identifying persuasion techniques in tweets and news articles, and (ii)
disinformation detection in binary and multiclass setups over tweets. A total
of 20 teams participated in the final evaluation phase, with 14 and 16 teams
participating in Tasks 1 and 2, respectively. Across both tasks, we observed
that fine-tuning transformer models such as AraBERT was at the core of the
majority of the participating systems. We provide a description of the task
setup, including a description of the dataset construction and the evaluation
setup. We further give a brief overview of the participating systems. All
datasets and evaluation scripts from the shared task are released to the
research community. (https://araieval.gitlab.io/) We hope this will enable
further research on these important tasks in Arabic.",None,-1
45dc8ba3-ed91-46aa-b76f-ea63c7607c3d,"Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",0.994193,36,"Conversational systems based on Large Language Models (LLMs), such as
ChatGPT, show exceptional proficiency in context understanding and response
generation. However, despite their impressive capabilities, they still possess
limitations, such as providing randomly-guessed answers to ambiguous queries or
failing to refuse users' requests, both of which are considered aspects of a
conversational agent's proactivity. This raises the question of whether
LLM-based conversational systems are equipped to handle proactive dialogue
problems. In this work, we conduct a comprehensive analysis of LLM-based
conversational systems, specifically focusing on three aspects of proactive
dialogue systems: clarification, target-guided, and non-collaborative
dialogues. To trigger the proactivity of LLMs, we propose the Proactive
Chain-of-Thought prompting scheme, which augments LLMs with the goal planning
capability over descriptive reasoning chains. Empirical findings are discussed
to promote future studies on LLM-based proactive dialogue systems.",None,-1
c7910dcf-8dba-49f0-95de-dbf5d9a8bf55,What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations,0.755915,10,"We propose and address a new generalisation problem: can a model trained for
action recognition successfully classify actions when they are performed within
a previously unseen scenario and in a previously unseen location? To answer
this question, we introduce the Action Recognition Generalisation Over
scenarios and locations dataset (ARGO1M), which contains 1.1M video clips from
the large-scale Ego4D dataset, across 10 scenarios and 13 locations. We
demonstrate recognition models struggle to generalise over 10 proposed test
splits, each of an unseen scenario in an unseen location. We thus propose CIR,
a method to represent each video as a Cross-Instance Reconstruction of videos
from other domains. Reconstructions are paired with text narrations to guide
the learning of a domain generalisable representation. We provide extensive
analysis and ablations on ARGO1M that show CIR outperforms prior domain
generalisation works on all test splits. Code and data:
https://chiaraplizz.github.io/what-can-a-cook/.",None,-1
27633444-711d-4da7-b4fe-294a58345d6a,AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction,0.518131,3,"In this work, we present a multimodal solution to the problem of 4D face
reconstruction from monocular videos. 3D face reconstruction from 2D images is
an under-constrained problem due to the ambiguity of depth. State-of-the-art
methods try to solve this problem by leveraging visual information from a
single image or video, whereas 3D mesh animation approaches rely more on audio.
However, in most cases (e.g. AR/VR applications), videos include both visual
and speech information. We propose AVFace that incorporates both modalities and
accurately reconstructs the 4D facial and lip motion of any speaker, without
requiring any 3D ground truth for training. A coarse stage estimates the
per-frame parameters of a 3D morphable model, followed by a lip refinement, and
then a fine stage recovers facial geometric details. Due to the temporal audio
and video information captured by transformer-based modules, our method is
robust in cases when either modality is insufficient (e.g. face occlusions).
Extensive qualitative and quantitative evaluation demonstrates the superiority
of our method over the current state-of-the-art.",None,-1
2b63866c-2dd2-483e-9e8d-01b1096a6027,PV2TEA: Patching Visual Modality to Textual-Established Information Extraction,0.147802,2,"Information extraction, e.g., attribute value extraction, has been
extensively studied and formulated based only on text. However, many attributes
can benefit from image-based extraction, like color, shape, pattern, among
others. The visual modality has long been underutilized, mainly due to
multimodal annotation difficulty. In this paper, we aim to patch the visual
modality to the textual-established attribute information extractor. The
cross-modality integration faces several unique challenges: (C1) images and
textual descriptions are loosely paired intra-sample and inter-samples; (C2)
images usually contain rich backgrounds that can mislead the prediction; (C3)
weakly supervised labels from textual-established extractors are biased for
multimodal training. We present PV2TEA, an encoder-decoder architecture
equipped with three bias reduction schemes: (S1) Augmented label-smoothed
contrast to improve the cross-modality alignment for loosely-paired image and
text; (S2) Attention-pruning that adaptively distinguishes the visual
foreground; (S3) Two-level neighborhood regularization that mitigates the label
textual bias via reliability estimation. Empirical results on real-world
e-Commerce datasets demonstrate up to 11.74% absolute (20.97% relatively) F1
increase over unimodal baselines.",None,-1
6580493a-2a34-4c91-b0c4-33f33a7d6330,Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition,0.781288,8,"We tackle the data scarcity challenge in few-shot point cloud recognition of
3D objects by using a joint prediction from a conventional 3D model and a
well-trained 2D model. Surprisingly, such an ensemble, though seems trivial,
has hardly been shown effective in recent 2D-3D models. We find out the crux is
the less effective training for the ''joint hard samples'', which have high
confidence prediction on different wrong labels, implying that the 2D and 3D
models do not collaborate well. To this end, our proposed invariant training
strategy, called InvJoint, does not only emphasize the training more on the
hard samples, but also seeks the invariance between the conflicting 2D and 3D
ambiguous predictions. InvJoint can learn more collaborative 2D and 3D
representations for better ensemble. Extensive experiments on 3D shape
classification with widely adopted ModelNet10/40, ScanObjectNN and Toys4K, and
shape retrieval with ShapeNet-Core validate the superiority of our InvJoint.",None,-1
28bbc089-9bbc-4fee-9979-a0951b6ed3bc,Learning with Exposure Constraints in Recommendation Systems,0.414813,7,"Recommendation systems are dynamic economic systems that balance the needs of
multiple stakeholders. A recent line of work studies incentives from the
content providers' point of view. Content providers, e.g., vloggers and
bloggers, contribute fresh content and rely on user engagement to create
revenue and finance their operations. In this work, we propose a contextual
multi-armed bandit setting to model the dependency of content providers on
exposure. In our model, the system receives a user context in every round and
has to select one of the arms. Every arm is a content provider who must receive
a minimum number of pulls every fixed time period (e.g., a month) to remain
viable in later rounds; otherwise, the arm departs and is no longer available.
The system aims to maximize the users' (content consumers) welfare. To that
end, it should learn which arms are vital and ensure they remain viable by
subsidizing arm pulls if needed. We develop algorithms with sub-linear regret,
as well as a lower bound that demonstrates that our algorithms are optimal up
to logarithmic factors.",None,-1
65a67e87-5873-489e-845f-695838dcc030,FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models,0.093711,11,"Using model weights pretrained on a high-resource language as a warm start
can reduce the need for data and compute to obtain high-quality language models
for other, especially low-resource, languages. However, if we want to use a new
tokenizer specialized for the target language, we cannot transfer the source
model's embedding matrix. In this paper, we propose FOCUS - Fast Overlapping
Token Combinations Using Sparsemax, a novel embedding initialization method
that initializes the embedding matrix effectively for a new tokenizer based on
information in the source model's embedding matrix. FOCUS represents newly
added tokens as combinations of tokens in the overlap of the source and target
vocabularies. The overlapping tokens are selected based on semantic similarity
in an auxiliary static token embedding space. We focus our study on using the
multilingual XLM-R as a source model and empirically show that FOCUS
outperforms random initialization and previous work in language modeling and on
a range of downstream tasks (NLI, QA, and NER).",None,-1
74de9ea0-eac1-428c-8231-c3134f6c4f2b,Specification of MiniDemographicABM.jl: A simplified agent-based demographic model of the UK,0.115318,2,"This documentation specifies a simplified non-calibrated demographic
agent-based model of the UK, a largely simplified version of the Lone Parent
Model presented in [Gostolil and Silverman 2020]. In the presented model,
individuals of an initial population are subject to ageing, deaths, births,
divorces and marriages throughout a simplified map of towns of the UK. The
specification employs the formal terminology presented in [Elsheikh 2023a]. The
main purpose of the model is to explore and exploit capabilities of the
state-of-the-art Agents.jl Julia package [Datseris2022] in the context of
demographic modeling applications. Implementation is provided via the Julia
package MiniDemographicABM.jl [Elsheikh 2023b]. A specific simulation is
progressed with a user-defined simulation fixed step size on a hourly, daily,
weekly, monthly basis or even an arbitrary user-defined clock rate. The model
can serve for comparative studies if implemented in other agent-based modelling
frameworks and programming languages. Moreover, the model serves as a base
implementation to be adjusted to realistic large-scale socio-economics,
pandemics or immigration studies mainly within a demographic context.",None,-1
7bcd7eee-57ea-4656-9ffe-01865ab027c0,Image Captioners Sometimes Tell More Than Images They See,0.074502,1,"Image captioning, a.k.a. ""image-to-text,"" which generates descriptive text
from given images, has been rapidly developing throughout the era of deep
learning. To what extent is the information in the original image preserved in
the descriptive text generated by an image captioner? To answer that question,
we have performed experiments involving the classification of images from
descriptive text alone, without referring to the images at all, and compared
results with those from standard image-based classifiers. We have evaluate
several image captioning models with respect to a disaster image classification
task, CrisisNLP, and show that descriptive text classifiers can sometimes
achieve higher accuracy than standard image-based classifiers. Further, we show
that fusing an image-based classifier with a descriptive text classifier can
provide improvement in accuracy.",None,-1
e3aa484e-1727-4656-9a82-2f82bc7f6d59,Loss-Curvature Matching for Dataset Selection and Condensation,0.551799,12,"Training neural networks on a large dataset requires substantial
computational costs. Dataset reduction selects or synthesizes data instances
based on the large dataset, while minimizing the degradation in generalization
performance from the full dataset. Existing methods utilize the neural network
during the dataset reduction procedure, so the model parameter becomes
important factor in preserving the performance after reduction. By depending
upon the importance of parameters, this paper introduces a new reduction
objective, coined LCMat, which Matches the Loss Curvatures of the original
dataset and reduced dataset over the model parameter space, more than the
parameter point. This new objective induces a better adaptation of the reduced
dataset on the perturbed parameter region than the exact point matching.
Particularly, we identify the worst case of the loss curvature gap from the
local parameter region, and we derive the implementable upper bound of such
worst-case with theoretical analyses. Our experiments on both coreset selection
and condensation benchmarks illustrate that LCMat shows better generalization
performances than existing baselines.",None,-1
cadcbe5d-d5d9-4834-a816-c66d50f1c1c9,Compressed Models Decompress Race Biases: What Quantized Models Forget for Fair Face Recognition,0.734999,5,"With the ever-growing complexity of deep learning models for face
recognition, it becomes hard to deploy these systems in real life. Researchers
have two options: 1) use smaller models; 2) compress their current models.
Since the usage of smaller models might lead to concerning biases, compression
gains relevance. However, compressing might be also responsible for an increase
in the bias of the final model. We investigate the overall performance, the
performance on each ethnicity subgroup and the racial bias of a
State-of-the-Art quantization approach when used with synthetic and real data.
This analysis provides a few more details on potential benefits of performing
quantization with synthetic data, for instance, the reduction of biases on the
majority of test scenarios. We tested five distinct architectures and three
different training datasets. The models were evaluated on a fourth dataset
which was collected to infer and compare the performance of face recognition
models on different ethnicity.",None,-1
2f3a9c0a-88ec-4af1-a492-d4f04ac62fb9,LostNet: A smart way for lost and find,0.917915,1,"Due to the enormous population growth of cities in recent years, objects are
frequently lost and unclaimed on public transportation, in restaurants, or any
other public areas. While services like Find My iPhone can easily identify lost
electronic devices, more valuable objects cannot be tracked in an intelligent
manner, making it impossible for administrators to reclaim a large number of
lost and found items in a timely manner. We present a method that significantly
reduces the complexity of searching by comparing previous images of lost and
recovered things provided by the owner with photos taken when registered lost
and found items are received. In this research, we will primarily design a
photo matching network by combining the fine-tuning method of MobileNetv2 with
CBAM Attention and using the Internet framework to develop an online lost and
found image identification system. Our implementation gets a testing accuracy
of 96.8% using only 665.12M GLFOPs and 3.5M training parameters. It can
recognize practice images and can be run on a regular laptop.",None,-1
e0c51510-2f6b-41d1-bf6e-605f050fb5b8,Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning,0.089403,2,"The Bidirectional Encoder Representations from Transformers (BERT) were
proposed in the natural language process (NLP) and shows promising results.
Recently researchers applied the BERT to source-code representation learning
and reported some good news on several downstream tasks. However, in this
paper, we illustrated that current methods cannot effectively understand the
logic of source codes. The representation of source code heavily relies on the
programmer-defined variable and function names. We design and implement a set
of experiments to demonstrate our conjecture and provide some insights for
future works.",None,-1
7254bc5c-43b2-4182-83f2-b00d8f0035a1,Embracing Compact and Robust Architectures for Multi-Exposure Image Fusion,0.127561,1,"In recent years, deep learning-based methods have achieved remarkable
progress in multi-exposure image fusion. However, existing methods rely on
aligned image pairs, inevitably generating artifacts when faced with device
shaking in real-world scenarios. Moreover, these learning-based methods are
built on handcrafted architectures and operations by increasing network depth
or width, neglecting different exposure characteristics. As a result, these
direct cascaded architectures with redundant parameters fail to achieve highly
effective inference time and lead to massive computation. To alleviate these
issues, in this paper, we propose a search-based paradigm, involving
self-alignment and detail repletion modules for robust multi-exposure image
fusion. By utilizing scene relighting and deformable convolutions, the
self-alignment module can accurately align images despite camera movement.
Furthermore, by imposing a hardware-sensitive constraint, we introduce neural
architecture search to discover compact and efficient networks, investigating
effective feature representation for fusion. We realize the state-of-the-art
performance in comparison to various competitive schemes, yielding a 4.02% and
29.34% improvement in PSNR for general and misaligned scenarios, respectively,
while reducing inference time by 68.1%. The source code will be available at
https://github.com/LiuZhu-CV/CRMEF.",None,-1
6d0b76d9-3261-4762-be2e-7e1a793cf13e,CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning,0.454876,27,"Commonsense reasoning, aiming at endowing machines with a human-like ability
to make situational presumptions, is extremely challenging to generalize. For
someone who barely knows about ""meditation,"" while is knowledgeable about
""singing,"" he can still infer that ""meditation makes people relaxed"" from the
existing knowledge that ""singing makes people relaxed"" by first conceptualizing
""singing"" as a ""relaxing event"" and then instantiating that event to
""meditation."" This process, known as conceptual induction and deduction, is
fundamental to commonsense reasoning while lacking both labeled data and
methodologies to enhance commonsense modeling. To fill such a research gap, we
propose CAT (Contextualized ConceptuAlization and InsTantiation), a
semi-supervised learning framework that integrates event conceptualization and
instantiation to conceptualize commonsense knowledge bases at scale. Extensive
experiments show that our framework achieves state-of-the-art performances on
two conceptualization tasks, and the acquired abstract commonsense knowledge
can significantly improve commonsense inference modeling. Our code, data, and
fine-tuned models are publicly available at
https://github.com/HKUST-KnowComp/CAT.",None,-1
2a1eccae-4231-409a-ba84-a8eb9197eedc,STHG: Spatial-Temporal Heterogeneous Graph Learning for Advanced Audio-Visual Diarization,0.336018,2,"This report introduces our novel method named STHG for the Audio-Visual
Diarization task of the Ego4D Challenge 2023. Our key innovation is that we
model all the speakers in a video using a single, unified heterogeneous graph
learning framework. Unlike previous approaches that require a separate
component solely for the camera wearer, STHG can jointly detect the speech
activities of all people including the camera wearer. Our final method obtains
61.1% DER on the test set of Ego4D, which significantly outperforms all the
baselines as well as last year's winner. Our submission achieved 1st place in
the Ego4D Challenge 2023. We additionally demonstrate that applying the
off-the-shelf speech recognition system to the diarized speech segments by STHG
produces a competitive performance on the Speech Transcription task of this
challenge.",None,-1
9a09b71e-5973-499a-9cb2-9b914198d5b8,DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition,0.594126,2,"Graph Convolutional Networks (GCNs) have been widely used in skeleton-based
human action recognition. In GCN-based methods, the spatio-temporal graph is
fundamental for capturing motion patterns. However, existing approaches ignore
the physical dependency and synchronized spatio-temporal correlations between
joints, which limits the representation capability of GCNs. To solve these
problems, we construct the directed diffusion graph for action modeling and
introduce the activity partition strategy to optimize the weight sharing
mechanism of graph convolution kernels. In addition, we present the
spatio-temporal synchronization encoder to embed synchronized spatio-temporal
semantics. Finally, we propose Directed Diffusion Graph Convolutional Network
(DD-GCN) for action recognition, and the experiments on three public datasets:
NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-art
performance of our method.",None,-1
1b90318a-d03b-4ef0-973d-bedb64181a69,REFinD: Relation Extraction Financial Dataset,0.722532,9,"A number of datasets for Relation Extraction (RE) have been created to aide
downstream tasks such as information retrieval, semantic search, question
answering and textual entailment. However, these datasets fail to capture
financial-domain specific challenges since most of these datasets are compiled
using general knowledge sources such as Wikipedia, web-based text and news
articles, hindering real-life progress and adoption within the financial world.
To address this limitation, we propose REFinD, the first large-scale annotated
dataset of relations, with $\sim$29K instances and 22 relations amongst 8 types
of entity pairs, generated entirely over financial documents. We also provide
an empirical evaluation with various state-of-the-art models as benchmarks for
the RE task and highlight the challenges posed by our dataset. We observed that
various state-of-the-art deep learning models struggle with numeric inference,
relational and directional ambiguity.",None,-1
7e66f45f-94b7-42f2-a1f0-1c47be1d4f75,Towards Theory-based Moral AI: Moral AI with Aggregating Models Based on Normative Ethical Theory,0.171595,1,"Moral AI has been studied in the fields of philosophy and artificial
intelligence. Although most existing studies are only theoretical, recent
developments in AI have made it increasingly necessary to implement AI with
morality. On the other hand, humans are under the moral uncertainty of not
knowing what is morally right. In this paper, we implement the Maximizing
Expected Choiceworthiness (MEC) algorithm, which aggregates outputs of models
based on three normative theories of normative ethics to generate the most
appropriate output. MEC is a method for making appropriate moral judgments
under moral uncertainty. Our experimental results suggest that the output of
MEC correlates to some extent with commonsense morality and that MEC can
produce equally or more appropriate output than existing methods.",None,-1
a1f9d476-9b0a-4191-b932-17298feb53ba,Tracr: Compiled Transformers as a Laboratory for Interpretability,0.746284,38,"We show how to ""compile"" human-readable programs into standard decoder-only
transformer models. Our compiler, Tracr, generates models with known structure.
This structure can be used to design experiments. For example, we use it to
study ""superposition"" in transformers that execute multi-step algorithms.
Additionally, the known structure of Tracr-compiled models can serve as
ground-truth for evaluating interpretability methods. Commonly, because the
""programs"" learned by transformers are unknown it is unclear whether an
interpretation succeeded. We demonstrate our approach by implementing and
examining programs including computing token frequencies, sorting, and
parenthesis checking. We provide an open-source implementation of Tracr at
https://github.com/google-deepmind/tracr.",None,-1
fbab80d3-cc62-4149-b359-49fa04f864ff,Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks,0.882324,6,"Arguments often do not make explicit how a conclusion follows from its
premises. To compensate for this lack, we enrich arguments with structured
background knowledge to support knowledge-intense argumentation tasks. We
present a new unsupervised method for constructing Contextualized Commonsense
Knowledge Graphs (CCKGs) that selects contextually relevant knowledge from
large knowledge graphs (KGs) efficiently and at high quality. Our work goes
beyond context-insensitive knowledge extraction heuristics by computing
semantic similarity between KG triplets and textual arguments. Using these
triplet similarities as weights, we extract contextualized knowledge paths that
connect a conclusion to its premise, while maximizing similarity to the
argument. We combine multiple paths into a CCKG that we optionally prune to
reduce noise and raise precision. Intrinsic evaluation of the quality of our
graphs shows that our method is effective for (re)constructing human
explanation graphs. Manual evaluations in a large-scale knowledge selection
setup confirm high recall and precision of implicit CSK in the CCKGs. Finally,
we demonstrate the effectiveness of CCKGs in a knowledge-insensitive argument
quality rating task, outperforming strong baselines and rivaling a GPT-3 based
system.",None,-1
1bbbab6d-8003-44ad-af70-103f42082ac7,[Re] CLRNet: Cross Layer Refinement Network for Lane Detection,0.101083,1,"The following work is a reproducibility report for CLRNet: Cross Layer
Refinement Network for Lane Detection. The basic code was made available by the
author. The paper proposes a novel Cross Layer Refinement Network to utilize
both high and low level features for lane detection. The authors assert that
the proposed technique sets the new state-of-the-art on three lane-detection
benchmarks",None,-1
43e21d61-7845-4e24-8ffe-019d3e253e92,Champion Solution for the WSDM2023 Toloka VQA Challenge,0.066922,2,"In this report, we present our champion solution to the WSDM2023 Toloka
Visual Question Answering (VQA) Challenge. Different from the common VQA and
visual grounding (VG) tasks, this challenge involves a more complex scenario,
i.e. inferring and locating the object implicitly specified by the given
interrogative question. For this task, we leverage ViT-Adapter, a
pre-training-free adapter network, to adapt multi-modal pre-trained
Uni-Perceiver for better cross-modal localization. Our method ranks first on
the leaderboard, achieving 77.5 and 76.347 IoU on public and private test sets,
respectively. It shows that ViT-Adapter is also an effective paradigm for
adapting the unified perception model to vision-language downstream tasks. Code
and models will be released at
https://github.com/czczup/ViT-Adapter/tree/main/wsdm2023.",None,-1
72dffa19-4385-4748-9b43-31a8376405e9,SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model,0.209392,6,"We present SkillGPT, a tool for skill extraction and standardization (SES)
from free-style job descriptions and user profiles with an open-source Large
Language Model (LLM) as backbone. Most previous methods for similar tasks
either need supervision or rely on heavy data-preprocessing and feature
engineering. Directly prompting the latest conversational LLM for standard
skills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes
a LLM to perform its tasks in steps via summarization and vector similarity
search, to balance speed with precision. The backbone LLM of SkillGPT is based
on Llama, free for academic use and thus useful for exploratory research and
prototype development. Hence, our cost-free SkillGPT gives users the
convenience of conversational SES, efficiently and reliably.",None,-1
2c24fad6-0313-4a89-beef-365a6a3e79cb,PSDR-Room: Single Photo to Scene using Differentiable Rendering,0.237358,6,"A 3D digital scene contains many components: lights, materials and
geometries, interacting to reach the desired appearance. Staging such a scene
is time-consuming and requires both artistic and technical skills. In this
work, we propose PSDR-Room, a system allowing to optimize lighting as well as
the pose and materials of individual objects to match a target image of a room
scene, with minimal user input. To this end, we leverage a recent path-space
differentiable rendering approach that provides unbiased gradients of the
rendering with respect to geometry, lighting, and procedural materials,
allowing us to optimize all of these components using gradient descent to
visually match the input photo appearance. We use recent single-image scene
understanding methods to initialize the optimization and search for appropriate
3D models and materials. We evaluate our method on real photographs of indoor
scenes and demonstrate the editability of the resulting scene components.",None,-1
4270a466-2391-46ff-bee7-091939d995b6,Multiscale Memory Comparator Transformer for Few-Shot Video Segmentation,0.155777,1,"Few-shot video segmentation is the task of delineating a specific novel class
in a query video using few labelled support images. Typical approaches compare
support and query features while limiting comparisons to a single feature layer
and thereby ignore potentially valuable information. We present a meta-learned
Multiscale Memory Comparator (MMC) for few-shot video segmentation that
combines information across scales within a transformer decoder. Typical
multiscale transformer decoders for segmentation tasks learn a compressed
representation, their queries, through information exchange across scales.
Unlike previous work, we instead preserve the detailed feature maps during
across scale information exchange via a multiscale memory transformer decoding
to reduce confusion between the background and novel class. Integral to the
approach, we investigate multiple forms of information exchange across scales
in different tasks and provide insights with empirical evidence on which to use
in each task. The overall comparisons among query and support features benefit
from both rich semantics and precise localization. We demonstrate our approach
primarily on few-shot video object segmentation and an adapted version on the
fully supervised counterpart. In all cases, our approach outperforms the
baseline and yields state-of-the-art performance. Our code is publicly
available at https://github.com/MSiam/MMC-MultiscaleMemory.",None,-1
2c8537cd-8106-425a-8b37-e268af0b113c,Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain,0.523763,2,"Many NLP tasks, although well-resolved for general English, face challenges
in specific domains like fantasy literature. This is evident in Named Entity
Recognition (NER), which detects and categorizes entities in text. We analyzed
10 NER models on 7 Dungeons and Dragons (D&D) adventure books to assess
domain-specific performance. Using open-source Large Language Models, we
annotated named entities in these books and evaluated each model's precision.
Our findings indicate that, without modifications, Flair, Trankit, and Spacy
outperform others in identifying named entities in the D&D context.",None,-1
1c34f595-ad04-4813-9d36-becda5afe216,SummIt: Iterative Text Summarization via ChatGPT,0.954572,33,"Text summarization systems have made significant progress in recent years,
but typically generate summaries in one single step. However, the one-shot
summarization setting is sometimes inadequate, as the generated summary may
contain hallucinations or overlook essential details related to the reader's
interests. This paper addresses this limitation by proposing SummIt, an
iterative text summarization framework based on large language models like
ChatGPT. Our framework enables the model to refine the generated summary
iteratively through self-evaluation and feedback, resembling humans' iterative
process when drafting and revising summaries. Furthermore, we explore the
potential benefits of integrating knowledge and topic extractors into the
framework to enhance summary faithfulness and controllability. We automatically
evaluate the performance of our framework on three benchmark summarization
datasets. We also conduct a human evaluation to validate the effectiveness of
the iterative refinements and identify a potential issue of over-correction.",None,-1
484b4a10-e957-4a48-bff1-17e0e182120f,Analysis over vision-based models for pedestrian action anticipation,0.527633,2,"Anticipating human actions in front of autonomous vehicles is a challenging
task. Several papers have recently proposed model architectures to address this
problem by combining multiple input features to predict pedestrian crossing
actions. This paper focuses specifically on using images of the pedestrian's
context as an input feature. We present several spatio-temporal model
architectures that utilize standard CNN and Transformer modules to serve as a
backbone for pedestrian anticipation. However, the objective of this paper is
not to surpass state-of-the-art benchmarks but rather to analyze the positive
and negative predictions of these models. Therefore, we provide insights on the
explainability of vision-based Transformer models in the context of pedestrian
action prediction. We will highlight cases where the model can achieve correct
quantitative results but falls short in providing human-like explanations
qualitatively, emphasizing the importance of investing in explainability for
pedestrian action anticipation problems.",None,-1
8c1412ed-a8df-4093-a3a5-86b3cd5ef960,Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents,0.988383,22,"Human-like chatbots necessitate the use of commonsense reasoning in order to
effectively comprehend and respond to implicit information present within
conversations. Achieving such coherence and informativeness in responses,
however, is a non-trivial task. Even for large language models (LLMs), the task
of identifying and aggregating key evidence within a single hop presents a
substantial challenge. This complexity arises because such evidence is
scattered across multiple turns in a conversation, thus necessitating
integration over multiple hops. Hence, our focus is to facilitate such
multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought
(CoT) reasoning. To this end, we propose a knowledge distillation framework
that leverages LLMs as unreliable teachers and selectively distills consistent
and helpful rationales via alignment filters. We further present DOCTOR, a
DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for
response generation. We conduct extensive experiments to show that enhancing
dialogue agents with high-quality rationales from DOCTOR significantly improves
the quality of their responses.",None,-1
86b927f6-3316-4638-92c3-a7f8c6eafe0f,An Adaptive Method for Camera Attribution under Complex Radial Distortion Corrections,0.100828,2,"Radial correction distortion, applied by in-camera or out-camera
software/firmware alters the supporting grid of the image so as to hamper
PRNU-based camera attribution. Existing solutions to deal with this problem try
to invert/estimate the correction using radial transformations parameterized
with few variables in order to restrain the computational load; however, with
ever more prevalent complex distortion corrections their performance is
unsatisfactory. In this paper we propose an adaptive algorithm that by dividing
the image into concentric annuli is able to deal with sophisticated corrections
like those applied out-camera by third party software like Adobe Lightroom,
Photoshop, Gimp and PT-Lens. We also introduce a statistic called cumulative
peak of correlation energy (CPCE) that allows for an efficient early stopping
strategy. Experiments on a large dataset of in-camera and out-camera radially
corrected images show that our solution improves the state of the art in terms
of both accuracy and computational cost.",None,-1
407927e6-2227-467a-ab85-784f923f6911,DiffuDetox: A Mixed Diffusion Model for Text Detoxification,0.0423323,4,"Text detoxification is a conditional text generation task aiming to remove
offensive content from toxic text. It is highly useful for online forums and
social media, where offensive content is frequently encountered. Intuitively,
there are diverse ways to detoxify sentences while preserving their meanings,
and we can select from detoxified sentences before displaying text to users.
Conditional diffusion models are particularly suitable for this task given
their demonstrated higher generative diversity than existing conditional text
generation models based on language models. Nonetheless, text fluency declines
when they are trained with insufficient data, which is the case for this task.
In this work, we propose DiffuDetox, a mixed conditional and unconditional
diffusion model for text detoxification. The conditional model takes toxic text
as the condition and reduces its toxicity, yielding a diverse set of detoxified
sentences. The unconditional model is trained to recover the input text, which
allows the introduction of additional fluent text for training and thus ensures
text fluency. Extensive experimental results and in-depth analysis demonstrate
the effectiveness of our proposed DiffuDetox.",None,-1
933fb5c7-54de-432d-8c00-5bfdb98273e1,PUPS: Point Cloud Unified Panoptic Segmentation,0.805893,8,"Point cloud panoptic segmentation is a challenging task that seeks a holistic
solution for both semantic and instance segmentation to predict groupings of
coherent points. Previous approaches treat semantic and instance segmentation
as surrogate tasks, and they either use clustering methods or bounding boxes to
gather instance groupings with costly computation and hand-crafted designs in
the instance segmentation task. In this paper, we propose a simple but
effective point cloud unified panoptic segmentation (PUPS) framework, which use
a set of point-level classifiers to directly predict semantic and instance
groupings in an end-to-end manner. To realize PUPS, we introduce bipartite
matching to our training pipeline so that our classifiers are able to
exclusively predict groupings of instances, getting rid of hand-crafted
designs, e.g. anchors and Non-Maximum Suppression (NMS). In order to achieve
better grouping results, we utilize a transformer decoder to iteratively refine
the point classifiers and develop a context-aware CutMix augmentation to
overcome the class imbalance problem. As a result, PUPS achieves 1st place on
the leader board of SemanticKITTI panoptic segmentation task and
state-of-the-art results on nuScenes.",None,-1
2c0152cb-0ec6-45c6-9e7e-011461e09b9d,Strata-NeRF : Neural Radiance Fields for Stratified Scenes,0.24922,3,"Neural Radiance Field (NeRF) approaches learn the underlying 3D
representation of a scene and generate photo-realistic novel views with high
fidelity. However, most proposed settings concentrate on modelling a single
object or a single level of a scene. However, in the real world, we may capture
a scene at multiple levels, resulting in a layered capture. For example,
tourists usually capture a monument's exterior structure before capturing the
inner structure. Modelling such scenes in 3D with seamless switching between
levels can drastically improve immersive experiences. However, most existing
techniques struggle in modelling such scenes. We propose Strata-NeRF, a single
neural radiance field that implicitly captures a scene with multiple levels.
Strata-NeRF achieves this by conditioning the NeRFs on Vector Quantized (VQ)
latent representations which allow sudden changes in scene structure. We
evaluate the effectiveness of our approach in multi-layered synthetic dataset
comprising diverse scenes and then further validate its generalization on the
real-world RealEstate10K dataset. We find that Strata-NeRF effectively captures
stratified scenes, minimizes artifacts, and synthesizes high-fidelity views
compared to existing approaches.",None,-1
c03456d5-cf7c-49b0-8dae-c0a7f10553b8,Does Synthetic Data Make Large Language Models More Efficient?,0.036312,1,"Natural Language Processing (NLP) has undergone transformative changes with
the advent of deep learning methodologies. One challenge persistently
confronting researchers is the scarcity of high-quality, annotated datasets
that drive these models. This paper explores the nuances of synthetic data
generation in NLP, with a focal point on template-based question generation. By
assessing its advantages, including data augmentation potential and the
introduction of structured variety, we juxtapose these benefits against
inherent limitations, such as the risk of overfitting and the constraints posed
by pre-defined templates. Drawing from empirical evaluations, we demonstrate
the impact of template-based synthetic data on the performance of modern
transformer models. We conclude by emphasizing the delicate balance required
between synthetic and real-world data, and the future trajectories of
integrating synthetic data in model training pipelines. The findings aim to
guide NLP practitioners in harnessing synthetic data's potential, ensuring
optimal model performance in diverse applications.",None,-1
2aecbe1b-2734-4f04-b485-8710a6e19aef,MRecGen: Multimodal Appropriate Reaction Generator,0.815906,2,"Verbal and non-verbal human reaction generation is a challenging task, as
different reactions could be appropriate for responding to the same behaviour.
This paper proposes the first multiple and multimodal (verbal and nonverbal)
appropriate human reaction generation framework that can generate appropriate
and realistic human-style reactions (displayed in the form of synchronised
text, audio and video streams) in response to an input user behaviour. This
novel technique can be applied to various human-computer interaction scenarios
by generating appropriate virtual agent/robot behaviours. Our demo is available
at \url{https://github.com/SSYSteve/MRecGen}.",None,-1
98fa507b-83de-4c93-a6b1-3a2a2b11a27b,Evaluation of African American Language Bias in Natural Language Generation,0.323077,10,"We evaluate how well LLMs understand African American Language (AAL) in
comparison to their performance on White Mainstream English (WME), the
encouraged ""standard"" form of English taught in American classrooms. We measure
LLM performance using automatic metrics and human judgments for two tasks: a
counterpart generation task, where a model generates AAL (or WME) given WME (or
AAL), and a masked span prediction (MSP) task, where models predict a phrase
that was removed from their input. Our contributions include: (1) evaluation of
six pre-trained, large language models on the two language generation tasks;
(2) a novel dataset of AAL text from multiple contexts (social media, hip-hop
lyrics, focus groups, and linguistic interviews) with human-annotated
counterparts in WME; and (3) documentation of model performance gaps that
suggest bias and identification of trends in lack of understanding of AAL
features.",None,-1
41f41089-eb69-4c1f-b712-f6fff0689e88,SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark,0.411183,27,"Large language models (LLMs) have shown the potential to be integrated into
human daily lives. Therefore, user preference is the most critical criterion
for assessing LLMs' performance in real-world scenarios. However, existing
benchmarks mainly focus on measuring models' accuracy using multi-choice
questions, which limits the understanding of their capabilities in real
applications. We fill this gap by proposing a comprehensive Chinese benchmark
SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE
encompasses three sub-tasks: actual users' queries and ratings derived from an
LLM battle platform (CArena), open-ended questions with single and
multiple-turn dialogues (OPEN), and closed-ended questions with the same stems
as open-ended single-turn ones (CLOSE). Our study shows that accuracy on
closed-ended questions is insufficient to reflect human preferences achieved on
open-ended ones. At the same time, they can complement each other to predict
actual user preferences. We also demonstrate that GPT-4 is a reliable judge to
automatically evaluate human preferences on open-ended questions in a Chinese
context. Our benchmark will be released at https://www.CLUEbenchmarks.com",None,-1
e12b0497-5eb0-492b-8b29-a1c9d045e7cb,Robust Dynamic Radiance Fields,0.971927,84,"Dynamic radiance field reconstruction methods aim to model the time-varying
structure and appearance of a dynamic scene. Existing methods, however, assume
that accurate camera poses can be reliably estimated by Structure from Motion
(SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often
fail or produce erroneous poses on challenging videos with highly dynamic
objects, poorly textured surfaces, and rotating camera motion. We address this
robustness issue by jointly estimating the static and dynamic radiance fields
along with the camera parameters (poses and focal length). We demonstrate the
robustness of our approach via extensive quantitative and qualitative
experiments. Our results show favorable performance over the state-of-the-art
dynamic view synthesis methods.",None,-1
f1454ebd-89e0-4022-96d3-136c926ac949,Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking,0.842551,15,"Standard Full-Data classifiers in NLP demand thousands of labeled examples,
which is impractical in data-limited domains. Few-shot methods offer an
alternative, utilizing contrastive learning techniques that can be effective
with as little as 20 examples per class. Similarly, Large Language Models
(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.
However, the performance-cost trade-offs of these methods remain underexplored,
a critical concern for budget-limited organizations. Our work addresses this
gap by studying the aforementioned approaches over the Banking77 financial
intent detection dataset, including the evaluation of cutting-edge LLMs by
OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We
complete the picture with two additional methods: first, a cost-effective
querying method for LLMs based on retrieval-augmented generation (RAG), able to
reduce operational costs multiple times compared to classic few-shot
approaches, and second, a data augmentation method using GPT-4, able to improve
performance in data-limited scenarios. Finally, to inspire future research, we
provide a human expert's curated subset of Banking77, along with extensive
error analysis.",None,-1
d7765c1a-b211-49a9-858e-f81434e779b3,Sentiment analysis with adaptive multi-head attention in Transformer,0.584371,5,"We propose a novel framework based on the attention mechanism to identify the
sentiment of a movie review document. Previous efforts on deep neural networks
with attention mechanisms focus on encoder and decoder with fixed numbers of
multi-head attention. Therefore, we need a mechanism to stop the attention
process automatically if no more useful information can be read from the
memory.In this paper, we propose an adaptive multi-head attention architecture
(AdaptAttn) which varies the number of attention heads based on length of
sentences. AdaptAttn has a data preprocessing step where each document is
classified into any one of the three bins small, medium or large based on
length of the sentence. The document classified as small goes through two heads
in each layer, the medium group passes four heads and the large group is
processed by eight heads. We examine the merit of our model on the Stanford
large movie review dataset. The experimental results show that the F1 score
from our model is on par with the baseline model.",None,-1
275a6b6e-01db-49ee-a57c-9248ae9a4ec7,Learning Language-Specific Layers for Multilingual Machine Translation,0.721236,13,"Multilingual Machine Translation promises to improve translation quality
between non-English languages. This is advantageous for several reasons, namely
lower latency (no need to translate twice), and reduced error cascades (e.g.,
avoiding losing gender and formality information when translating through
English). On the downside, adding more languages reduces model capacity per
language, which is usually countered by increasing the overall model size,
making training harder and inference slower. In this work, we introduce
Language-Specific Transformer Layers (LSLs), which allow us to increase model
capacity, while keeping the amount of computation and the number of parameters
used in the forward pass constant. The key idea is to have some layers of the
encoder be source or target language-specific, while keeping the remaining
layers shared. We study the best way to place these layers using a neural
architecture search inspired approach, and achieve an improvement of 1.3 chrF
(1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and
1.9 chrF (2.2 spBLEU) on a shared decoder one.",None,-1
c526d169-339e-44a6-b3eb-794369f62ae8,Back to Patterns: Efficient Japanese Morphological Analysis with Feature-Sequence Trie,0.585334,2,"Accurate neural models are much less efficient than non-neural models and are
useless for processing billions of social media posts or handling user queries
in real time with a limited budget. This study revisits the fastest
pattern-based NLP methods to make them as accurate as possible, thus yielding a
strikingly simple yet surprisingly accurate morphological analyzer for
Japanese. The proposed method induces reliable patterns from a morphological
dictionary and annotated data. Experimental results on two standard datasets
confirm that the method exhibits comparable accuracy to learning-based
baselines, while boasting a remarkable throughput of over 1,000,000 sentences
per second on a single modern CPU. The source code is available at
https://www.tkl.iis.u-tokyo.ac.jp/~ynaga/jagger/",None,-1
1ee46ff9-3490-45d3-8ce6-57b1e44f93ee,What Matters In The Structured Pruning of Generative Language Models?,0.365264,15,"Auto-regressive large language models such as GPT-3 require enormous
computational resources to use. Traditionally, structured pruning methods are
employed to reduce resource usage. However, their application to and efficacy
for generative language models is heavily under-explored. In this paper we
conduct an comprehensive evaluation of common structured pruning methods,
including magnitude, random, and movement pruning on the feed-forward layers in
GPT-type models. Unexpectedly, random pruning results in performance that is
comparable to the best established methods, across multiple natural language
generation tasks. To understand these results, we provide a framework for
measuring neuron-level redundancy of models pruned by different methods, and
discover that established structured pruning methods do not take into account
the distinctiveness of neurons, leaving behind excess redundancies. In view of
this, we introduce Globally Unique Movement (GUM) to improve the uniqueness of
neurons in pruned models. We then discuss the effects of our techniques on
different redundancy metrics to explain the improved performance.",None,-1
e3718841-4d72-47ad-b183-39430aea0956,AI-assisted coding: Experiments with GPT-4,0.715421,34,"Artificial intelligence (AI) tools based on large language models have
acheived human-level performance on some computer programming tasks. We report
several experiments using GPT-4 to generate computer code. These experiments
demonstrate that AI code generation using the current generation of tools,
while powerful, requires substantial human validation to ensure accurate
performance. We also demonstrate that GPT-4 refactoring of existing code can
significantly improve that code along several established metrics for code
quality, and we show that GPT-4 can generate tests with substantial coverage,
but that many of the tests fail when applied to the associated code. These
findings suggest that while AI coding tools are very powerful, they still
require humans in the loop to ensure validity and accuracy of the results.",None,-1
dd30c008-179b-41d8-8651-fee19cd1b472,Neural Spectro-polarimetric Fields,0.85507,3,"Modeling the spatial radiance distribution of light rays in a scene has been
extensively explored for applications, including view synthesis. Spectrum and
polarization, the wave properties of light, are often neglected due to their
integration into three RGB spectral bands and their non-perceptibility to human
vision. However, these properties are known to encompass substantial material
and geometric information about a scene. Here, we propose to model
spectro-polarimetric fields, the spatial Stokes-vector distribution of any
light ray at an arbitrary wavelength. We present Neural Spectro-polarimetric
Fields (NeSpoF), a neural representation that models the physically-valid
Stokes vector at given continuous variables of position, direction, and
wavelength. NeSpoF manages inherently noisy raw measurements, showcases memory
efficiency, and preserves physically vital signals - factors that are crucial
for representing the high-dimensional signal of a spectro-polarimetric field.
To validate NeSpoF, we introduce the first multi-view
hyperspectral-polarimetric image dataset, comprised of both synthetic and
real-world scenes. These were captured using our compact
hyperspectral-polarimetric imaging system, which has been calibrated for
robustness against system imperfections. We demonstrate the capabilities of
NeSpoF on diverse scenes.",None,-1
6fc246ee-d5b8-4b54-9c38-8cc7b13165bf,IC3: Image Captioning by Committee Consensus,0.335542,7,"If you ask a human to describe an image, they might do so in a thousand
different ways. Traditionally, image captioning models are trained to generate
a single ""best"" (most like a reference) image caption. Unfortunately, doing so
encourages captions that are ""informationally impoverished,"" and focus on only
a subset of the possible details, while ignoring other potentially useful
information in the scene. In this work, we introduce a simple, yet novel,
method: ""Image Captioning by Committee Consensus"" (IC3), designed to generate a
single caption that captures high-level details from several annotator
viewpoints. Humans rate captions produced by IC3 at least as helpful as
baseline SOTA models more than two thirds of the time, and IC3 can improve the
performance of SOTA automated recall systems by up to 84%, outperforming single
human-generated reference captions, and indicating significant improvements
over SOTA approaches for visual description. Code is available at
https://davidmchan.github.io/caption-by-committee/",None,-1
ff6a49af-edfe-4e3c-9165-c6bcbff37bcb,llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology,0.0381973,5,"This study constructed a Japanese chat dataset for tuning large language
models (LLMs), which consist of about 8.4 million records. Recently, LLMs have
been developed and gaining popularity. However, high-performing LLMs are
usually mainly for English. There are two ways to support languages other than
English by those LLMs: constructing LLMs from scratch or tuning existing
models. However, in both ways, datasets are necessary parts. In this study, we
focused on supporting Japanese in those LLMs and making a dataset for training
or tuning LLMs in Japanese. The dataset we constructed consisted of various
tasks, such as translation and knowledge tasks. In our experiment, we tuned an
existing LLM using our dataset and evaluated the performance qualitatively. The
results suggest that our dataset is possibly beneficial for LLMs. However, we
also revealed some difficulties in constructing LLMs in languages other than
English.",None,-1
28d36ef1-107e-499f-a2d3-3d5f7969a65e,Noisy Pair Corrector for Dense Retrieval,0.285776,4,"Most dense retrieval models contain an implicit assumption: the training
query-document pairs are exactly matched. Since it is expensive to annotate the
corpus manually, training pairs in real-world applications are usually
collected automatically, which inevitably introduces mismatched-pair noise. In
this paper, we explore an interesting and challenging problem in dense
retrieval, how to train an effective model with mismatched-pair noise. To solve
this problem, we propose a novel approach called Noisy Pair Corrector (NPC),
which consists of a detection module and a correction module. The detection
module estimates noise pairs by calculating the perplexity between annotated
positive and easy negative documents. The correction module utilizes an
exponential moving average (EMA) model to provide a soft supervised signal,
aiding in mitigating the effects of noise. We conduct experiments on
text-retrieval benchmarks Natural Question and TriviaQA, code-search benchmarks
StaQC and SO-DS. Experimental results show that NPC achieves excellent
performance in handling both synthetic and realistic noise.",None,-1
375a8e3b-17eb-4ad2-8f65-c11cff264c93,Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment,0.811864,11,"As a crucial extension of entity alignment (EA), multi-modal entity alignment
(MMEA) aims to identify identical entities across disparate knowledge graphs
(KGs) by exploiting associated visual information. However, existing MMEA
approaches primarily concentrate on the fusion paradigm of multi-modal entity
features, while neglecting the challenges presented by the pervasive phenomenon
of missing and intrinsic ambiguity of visual images. In this paper, we present
a further analysis of visual modality incompleteness, benchmarking latest MMEA
models on our proposed dataset MMEA-UMVM, where the types of alignment KGs
covering bilingual and monolingual, with standard (non-iterative) and iterative
training paradigms to evaluate the model performance. Our research indicates
that, in the face of modality incompleteness, models succumb to overfitting the
modality noise, and exhibit performance oscillations or declines at high rates
of missing modality. This proves that the inclusion of additional multi-modal
data can sometimes adversely affect EA. To address these challenges, we
introduce UMAEA , a robust multi-modal entity alignment approach designed to
tackle uncertainly missing and ambiguous visual modalities. It consistently
achieves SOTA performance across all 97 benchmark splits, significantly
surpassing existing baselines with limited parameters and time consumption,
while effectively alleviating the identified limitations of other models. Our
code and benchmark data are available at https://github.com/zjukg/UMAEA.",None,-1
23202304-37ab-41fe-b2d9-fda552579672,HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution,0.872119,22,"The rise of large language models (LLMs) had a transformative impact on
search, ushering in a new era of search engines that are capable of generating
search results in natural language text, imbued with citations for supporting
sources. Building generative information-seeking models demands openly
accessible datasets, which currently remain lacking. In this paper, we
introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative
Retrieval for Information-seeking Dataset) for building end-to-end generative
information-seeking models that are capable of retrieving candidate quotes and
generating attributed explanations. Unlike recent efforts that focus on human
evaluation of black-box proprietary search engines, we built our dataset atop
the English subset of MIRACL, a publicly available information retrieval
dataset. HAGRID is constructed based on human and LLM collaboration. We first
automatically collect attributed explanations that follow an in-context
citation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to
evaluate the LLM explanations based on two criteria: informativeness and
attributability. HAGRID serves as a catalyst for the development of
information-seeking models with better attribution capabilities.",None,-1
5afe47c4-eb5c-40ca-9a83-69cbb8fcc669,Flexible Techniques for Differentiable Rendering with 3D Gaussians,0.641828,22,"Fast, reliable shape reconstruction is an essential ingredient in many
computer vision applications. Neural Radiance Fields demonstrated that
photorealistic novel view synthesis is within reach, but was gated by
performance requirements for fast reconstruction of real scenes and objects.
Several recent approaches have built on alternative shape representations, in
particular, 3D Gaussians. We develop extensions to these renderers, such as
integrating differentiable optical flow, exporting watertight meshes and
rendering per-ray normals. Additionally, we show how two of the recent methods
are interoperable with each other. These reconstructions are quick, robust, and
easily performed on GPU or CPU. For code and visual examples, see
https://leonidk.github.io/fmb-plus",None,-1
0a3c342b-1f6c-4682-94ad-7cc3f81c4b0a,Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks,0.999999,141,"Intuitive psychology is a pillar of common-sense reasoning. The replication
of this reasoning in machine intelligence is an important stepping-stone on the
way to human-like artificial intelligence. Several recent tasks and benchmarks
for examining this reasoning in Large-Large Models have focused in particular
on belief attribution in Theory-of-Mind tasks. These tasks have shown both
successes and failures. We consider in particular a recent purported success
case, and show that small variations that maintain the principles of ToM turn
the results on their head. We argue that in general, the zero-hypothesis for
model evaluation in intuitive psychology should be skeptical, and that outlying
failure cases should outweigh average success rates. We also consider what
possible future successes on Theory-of-Mind tasks by more powerful LLMs would
mean for ToM tasks with people.",None,-1
e69f952d-b3c6-45b3-9c06-1218fb326109,Towards eXplainable AI for Mobility Data Science,0.277086,1,"This paper presents our ongoing work towards XAI for Mobility Data Science
applications, focusing on explainable models that can learn from dense
trajectory data, such as GPS tracks of vehicles and vessels using temporal
graph neural networks (GNNs) and counterfactuals. We review the existing GeoXAI
studies, argue the need for comprehensible explanations with human-centered
approaches, and outline a research path toward XAI for Mobility Data Science.",None,-1
12a0a136-83bb-4d98-9bf2-552c4a80fd45,CD-CTFM: A Lightweight CNN-Transformer Network for Remote Sensing Cloud Detection Fusing Multiscale Features,0.456315,2,"Clouds in remote sensing images inevitably affect information extraction,
which hinder the following analysis of satellite images. Hence, cloud detection
is a necessary preprocessing procedure. However, the existing methods have
numerous calculations and parameters. In this letter, a lightweight
CNN-Transformer network, CD-CTFM, is proposed to solve the problem. CD-CTFM is
based on encoder-decoder architecture and incorporates the attention mechanism.
In the decoder part, we utilize a lightweight network combing CNN and
Transformer as backbone, which is conducive to extract local and global
features simultaneously. Moreover, a lightweight feature pyramid module is
designed to fuse multiscale features with contextual information. In the
decoder part, we integrate a lightweight channel-spatial attention module into
each skip connection between encoder and decoder, extracting low-level features
while suppressing irrelevant information without introducing many parameters.
Finally, the proposed model is evaluated on two cloud datasets, 38-Cloud and
MODIS. The results demonstrate that CD-CTFM achieves comparable accuracy as the
state-of-art methods. At the same time, CD-CTFM outperforms state-of-art
methods in terms of efficiency.",None,-1
24211e47-bfd2-4c54-9f28-fe7688c114fa,From Model-Based to Data-Driven Simulation: Challenges and Trends in Autonomous Driving,0.752989,7,"Simulation is an integral part in the process of developing autonomous
vehicles and advantageous for training, validation, and verification of driving
functions. Even though simulations come with a series of benefits compared to
real-world experiments, various challenges still prevent virtual testing from
entirely replacing physical test-drives. Our work provides an overview of these
challenges with regard to different aspects and types of simulation and
subsumes current trends to overcome them. We cover aspects around perception-,
behavior- and content-realism as well as general hurdles in the domain of
simulation. Among others, we observe a trend of data-driven, generative
approaches and high-fidelity data synthesis to increasingly replace model-based
simulation.",None,-1
5ee2b8cb-da9a-4d98-870f-151699d6c65f,MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System,0.865593,7,"Multi-modal sarcasm detection has attracted much recent attention.
Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder
the development of reliable multi-modal sarcasm detection system: (1) There are
some spurious cues in MMSD, leading to the model bias learning; (2) The
negative samples in MMSD are not always reasonable. To solve the aforementioned
issues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings
of MMSD, by removing the spurious cues and re-annotating the unreasonable
samples. Meanwhile, we present a novel framework called multi-view CLIP that is
capable of leveraging multi-grained cues from multiple perspectives (i.e.,
text, image, and text-image interaction view) for multi-modal sarcasm
detection. Extensive experiments show that MMSD2.0 is a valuable benchmark for
building reliable multi-modal sarcasm detection systems and multi-view CLIP can
significantly outperform the previous best baselines.",None,-1
46075124-8e16-433c-a688-ee50b899af0f,Closing the Curious Case of Neural Text Degeneration,0.204046,7,"Despite their ubiquity in language generation, it remains unknown why
truncation sampling heuristics like nucleus sampling are so effective. We
provide a theoretical explanation for the effectiveness of the truncation
sampling by proving that truncation methods that discard tokens below some
probability threshold (the most common type of truncation) can guarantee that
all sampled tokens have nonzero true probability. However, thresholds are a
coarse heuristic, and necessarily discard some tokens with nonzero true
probability as well. In pursuit of a more precise sampling strategy, we show
that we can leverage a known source of model errors, the softmax bottleneck, to
prove that certain tokens have nonzero true probability, without relying on a
threshold. Based on our findings, we develop an experimental truncation
strategy and the present pilot studies demonstrating the promise of this type
of algorithm. Our evaluations show that our method outperforms its
threshold-based counterparts under automatic and human evaluation metrics for
low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical
findings and pilot experiments provide both insight into why truncation
sampling works, and make progress toward more expressive sampling algorithms
that better surface the generative capabilities of large language models.",None,-1
8129ca4d-87db-41b9-a5f1-ce1fed35efdc,Faithfulness Measurable Masked Language Models,0.066442,1,"A common approach to explaining NLP models is to use importance measures that
express which tokens are important for a prediction. Unfortunately, such
explanations are often wrong despite being persuasive. Therefore, it is
essential to measure their faithfulness. One such metric is if tokens are truly
important, then masking them should result in worse model performance. However,
token masking introduces out-of-distribution issues, and existing solutions
that address this are computationally expensive and employ proxy models.
Furthermore, other metrics are very limited in scope. This work proposes an
inherently faithfulness measurable model that addresses these challenges. This
is achieved using a novel fine-tuning method that incorporates masking, such
that masking tokens become in-distribution by design. This differs from
existing approaches, which are completely model-agnostic but are inapplicable
in practice. We demonstrate the generality of our approach by applying it to 16
different datasets and validate it using statistical in-distribution tests. The
faithfulness is then measured with 9 different importance measures. Because
masking is in-distribution, importance measures that themselves use masking
become consistently more faithful. Additionally, because the model makes
faithfulness cheap to measure, we can optimize explanations towards maximal
faithfulness; thus, our model becomes indirectly inherently explainable.",None,-1
4298f903-7e69-49a6-9424-28123403bf2e,LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models,0.926755,18,"Creating graphic layouts is a fundamental step in graphic designs. In this
work, we present a novel generative model named LayoutDiffusion for automatic
layout generation. As layout is typically represented as a sequence of discrete
tokens, LayoutDiffusion models layout generation as a discrete denoising
diffusion process. It learns to reverse a mild forward process, in which
layouts become increasingly chaotic with the growth of forward steps and
layouts in the neighboring steps do not differ too much. Designing such a mild
forward process is however very challenging as layout has both categorical
attributes and ordinal attributes. To tackle the challenge, we summarize three
critical factors for achieving a mild forward process for the layout, i.e.,
legality, coordinate proximity and type disruption. Based on the factors, we
propose a block-wise transition matrix coupled with a piece-wise linear noise
schedule. Experiments on RICO and PubLayNet datasets show that LayoutDiffusion
outperforms state-of-the-art approaches significantly. Moreover, it enables two
conditional layout generation tasks in a plug-and-play manner without
re-training and achieves better performance than existing methods.",None,-1
4eccdc7c-2750-4012-8d93-8d22807ef234,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,0.212715,40,"The scaling of large language models has greatly improved natural language
understanding, generation, and reasoning. In this work, we develop a system
that trained a trillion-parameter language model on a cluster of Ascend 910 AI
processors and MindSpore framework, and present the language model with 1.085T
parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha},
we extend the dense Transformer model to sparse one with Random Routed Experts
(RRE), and efficiently train the model over 329B tokens by using Expert
Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in
training throughput through heterogeneous computing. Our experimental findings
show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot
learning of various Chinese NLP downstream tasks. Moreover, it demonstrates
strong abilities when fine-tuned in application data of open-domain dialogue,
question answering, machine translation and code generation.",None,-1
e0a5f425-ea88-4c4a-aa2e-a883ecb93a41,Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting,0.233985,5,"Federated learning has exhibited vulnerabilities to Byzantine attacks, where
the Byzantine attackers can send arbitrary gradients to a central server to
destroy the convergence and performance of the global model. A wealth of robust
AGgregation Rules (AGRs) have been proposed to defend against Byzantine
attacks. However, Byzantine clients can still circumvent robust AGRs when data
is non-Identically and Independently Distributed (non-IID). In this paper, we
first reveal the root causes of performance degradation of current robust AGRs
in non-IID settings: the curse of dimensionality and gradient heterogeneity. In
order to address this issue, we propose GAS, a \shorten approach that can
successfully adapt existing robust AGRs to non-IID settings. We also provide a
detailed convergence analysis when the existing robust AGRs are combined with
GAS. Experiments on various real-world datasets verify the efficacy of our
proposed GAS. The implementation code is provided in
https://github.com/YuchenLiu-a/byzantine-gas.",None,-1
72066fff-7de7-4013-bab6-dd06075ce8d2,Tackling the Abstraction and Reasoning Corpus (ARC) with Object-centric Models and the MDL Principle,0.0857239,1,"The Abstraction and Reasoning Corpus (ARC) is a challenging benchmark,
introduced to foster AI research towards human-level intelligence. It is a
collection of unique tasks about generating colored grids, specified by a few
examples only. In contrast to the transformation-based programs of existing
work, we introduce object-centric models that are in line with the natural
programs produced by humans. Our models can not only perform predictions, but
also provide joint descriptions for input/output pairs. The Minimum Description
Length (MDL) principle is used to efficiently search the large model space. A
diverse range of tasks are solved, and the learned models are similar to the
natural programs. We demonstrate the generality of our approach by applying it
to a different domain.",None,-1
96da574c-610a-488e-977c-f5dcbaab0f0a,Diffusion Self-Guidance for Controllable Image Generation,0.999628,122,"Large-scale generative models are capable of producing high-quality images
from detailed text descriptions. However, many aspects of an image are
difficult or impossible to convey through text. We introduce self-guidance, a
method that provides greater control over generated images by guiding the
internal representations of diffusion models. We demonstrate that properties
such as the shape, location, and appearance of objects can be extracted from
these representations and used to steer sampling. Self-guidance works similarly
to classifier guidance, but uses signals present in the pretrained model
itself, requiring no additional models or training. We show how a simple set of
properties can be composed to perform challenging image manipulations, such as
modifying the position or size of objects, merging the appearance of objects in
one image with the layout of another, composing objects from many images into
one, and more. We also show that self-guidance can be used to edit real images.
For results and an interactive demo, see our project page at
https://dave.ml/selfguidance/",None,-1
34a8f7e0-c122-4bc6-bfb6-1e264dc27545,"AfriNames: Most ASR models ""butcher"" African Names",0.393849,2,"Useful conversational agents must accurately capture named entities to
minimize error for downstream tasks, for example, asking a voice assistant to
play a track from a certain artist, initiating navigation to a specific
location, or documenting a laboratory result for a patient. However, where
named entities such as ``Ukachukwu`` (Igbo), ``Lakicia`` (Swahili), or
``Ingabire`` (Rwandan) are spoken, automatic speech recognition (ASR) models'
performance degrades significantly, propagating errors to downstream systems.
We model this problem as a distribution shift and demonstrate that such model
bias can be mitigated through multilingual pre-training, intelligent data
augmentation strategies to increase the representation of African-named
entities, and fine-tuning multilingual ASR models on multiple African accents.
The resulting fine-tuned models show an 81.5\% relative WER improvement
compared with the baseline on samples with African-named entities.",None,-1
0ed7d332-8eb8-48e8-9bf0-625fa92d5d61,InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent,0.882771,8,"This research paper delves into the integration of OpenAI's ChatGPT into
embodied agent systems, evaluating its influence on interactive decision-making
benchmark. Drawing a parallel to the concept of people assuming roles according
to their unique strengths, we introduce InterAct. In this approach, we feed
ChatGPT with varied prompts, assigning it a numerous roles like a checker and a
sorter, then integrating them with the original language model. Our research
shows a remarkable success rate of 98% in AlfWorld, which consists of 6
different tasks in a simulated household environment, emphasizing the
significance of proficient prompt engineering. The results highlight ChatGPT's
competence in comprehending and performing intricate tasks effectively in
real-world settings, thus paving the way for further advancements in task
planning.",None,-1
f3ab8afc-9936-40f7-9c23-18e53e269e3e,MixTeacher: Mining Promising Labels with Mixed Scale Teacher for Semi-Supervised Object Detection,0.677103,8,"Scale variation across object instances remains a key challenge in object
detection task. Despite the remarkable progress made by modern detection
models, this challenge is particularly evident in the semi-supervised case.
While existing semi-supervised object detection methods rely on strict
conditions to filter high-quality pseudo labels from network predictions, we
observe that objects with extreme scale tend to have low confidence, resulting
in a lack of positive supervision for these objects. In this paper, we propose
a novel framework that addresses the scale variation problem by introducing a
mixed scale teacher to improve pseudo label generation and scale-invariant
learning. Additionally, we propose mining pseudo labels using score promotion
of predictions across scales, which benefits from better predictions from mixed
scale features. Our extensive experiments on MS COCO and PASCAL VOC benchmarks
under various semi-supervised settings demonstrate that our method achieves new
state-of-the-art performance. The code and models are available at
\url{https://github.com/lliuz/MixTeacher}.",None,-1
f97763b6-f5ad-4971-bc7f-73c7ceedefc3,Rotation-Scale Equivariant Steerable Filters,0.213176,1,"Incorporating either rotation equivariance or scale equivariance into CNNs
has proved to be effective in improving models' generalization performance.
However, jointly integrating rotation and scale equivariance into CNNs has not
been widely explored. Digital histology imaging of biopsy tissue can be
captured at arbitrary orientation and magnification and stored at different
resolutions, resulting in cells appearing in different scales. When
conventional CNNs are applied to histopathology image analysis, the
generalization performance of models is limited because 1) a part of the
parameters of filters are trained to fit rotation transformation, thus
decreasing the capability of learning other discriminative features; 2)
fixed-size filters trained on images at a given scale fail to generalize to
those at different scales. To deal with these issues, we propose the
Rotation-Scale Equivariant Steerable Filter (RSESF), which incorporates
steerable filters and scale-space theory. The RSESF contains copies of filters
that are linear combinations of Gaussian filters, whose direction is controlled
by directional derivatives and whose scale parameters are trainable but
constrained to span disjoint scales in successive layers of the network.
Extensive experiments on two gland segmentation datasets demonstrate that our
method outperforms other approaches, with much fewer trainable parameters and
fewer GPU resources required. The source code is available at:
https://github.com/ynulonger/RSESF.",None,-1
3d137da2-4122-4d12-b8a8-2fc5c3f96786,"Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search",0.501855,3,"In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual,
zero-shot event extraction system and accompanying user interface for event
visualization & search. Using only English training data, ISI-Clear makes
global events available on-demand, processing user-supplied text in 100
languages ranging from Afrikaans to Yiddish. We provide multiple event-centric
views of extracted events, including both a graphical representation and a
document-level summary. We also integrate existing cross-lingual search
algorithms with event extraction capabilities to provide cross-lingual
event-centric search, allowing English-speaking users to search over events
automatically extracted from a corpus of non-English documents, using either
English natural language queries (e.g. cholera outbreaks in Iran) or structured
queries (e.g. find all events of type Disease-Outbreak with agent cholera and
location Iran).",None,-1
8314e64d-4334-459b-af78-901689d44e93,Learning Cautiously in Federated Learning with Noisy and Heterogeneous Clients,0.174379,3,"Federated learning (FL) is a distributed framework for collaboratively
training with privacy guarantees. In real-world scenarios, clients may have
Non-IID data (local class imbalance) with poor annotation quality (label
noise). The co-existence of label noise and class imbalance in FL's small local
datasets renders conventional FL methods and noisy-label learning methods both
ineffective. To address the challenges, we propose FedCNI without using an
additional clean proxy dataset. It includes a noise-resilient local solver and
a robust global aggregator. For the local solver, we design a more robust
prototypical noise detector to distinguish noisy samples. Further to reduce the
negative impact brought by the noisy samples, we devise a curriculum pseudo
labeling method and a denoise Mixup training strategy. For the global
aggregator, we propose a switching re-weighted aggregation method tailored to
different learning periods. Extensive experiments demonstrate our method can
substantially outperform state-of-the-art solutions in mix-heterogeneous FL
environments.",None,-1
410b2df9-a3c3-4572-8942-48fbab07e4b0,Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling,0.664819,19,"We introduce Reprompting, an iterative sampling algorithm that automatically
learns the Chain-of-Thought (CoT) recipes for a given task without human
intervention. Through Gibbs sampling, Reprompting infers the CoT recipes that
work consistently well for a set of training samples by iteratively sampling
new recipes using previously sampled recipes as parent prompts to solve other
training problems. We conduct extensive experiments on 20 challenging reasoning
tasks. Results show that Reprompting outperforms human-written CoT prompts
substantially by +9.4 points on average. It also achieves consistently better
performance than the state-of-the-art prompt optimization and decoding
algorithms.",None,-1
22a60b51-6859-4ee7-8e7a-27225a194758,AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks,0.0944174,1,"Sparse training is emerging as a promising avenue for reducing the
computational cost of training neural networks. Several recent studies have
proposed pruning methods using learnable thresholds to efficiently explore the
non-uniform distribution of sparsity inherent within the models. In this paper,
we propose Gradient Annealing (GA), where gradients of masked weights are
scaled down in a non-linear manner. GA provides an elegant trade-off between
sparsity and accuracy without the need for additional sparsity-inducing
regularization. We integrated GA with the latest learnable pruning methods to
create an automated sparse training algorithm called AutoSparse, which achieves
better accuracy and/or training/inference FLOPS reduction than existing
learnable pruning methods for sparse ResNet50 and MobileNetV1 on ImageNet-1K:
AutoSparse achieves (2x, 7x) reduction in (training,inference) FLOPS for
ResNet50 on ImageNet at 80% sparsity. Finally, AutoSparse outperforms
sparse-to-sparse SotA method MEST (uniform sparsity) for 80% sparse ResNet50
with similar accuracy, where MEST uses 12% more training FLOPS and 50% more
inference FLOPS.",None,-1
6a5ff8f3-b15c-4c29-8deb-194f2a8c61fc,Attention Where It Matters: Rethinking Visual Document Understanding with Selective Region Concentration,0.318885,6,"We propose a novel end-to-end document understanding model called SeRum
(SElective Region Understanding Model) for extracting meaningful information
from document images, including document analysis, retrieval, and office
automation.
  Unlike state-of-the-art approaches that rely on multi-stage technical schemes
and are computationally expensive,
  SeRum converts document image understanding and recognition tasks into a
local decoding process of the visual tokens of interest, using a content-aware
token merge module.
  This mechanism enables the model to pay more attention to regions of interest
generated by the query decoder, improving the model's effectiveness and
speeding up the decoding speed of the generative scheme.
  We also designed several pre-training tasks to enhance the understanding and
local awareness of the model.
  Experimental results demonstrate that SeRum achieves state-of-the-art
performance on document understanding tasks and competitive results on text
spotting tasks.
  SeRum represents a substantial advancement towards enabling efficient and
effective end-to-end document understanding.",None,-1
02e1e3d4-ada3-4934-badb-14c7feb6a712,MDQE: Mining Discriminative Query Embeddings to Segment Occluded Instances on Challenging Videos,0.493433,8,"While impressive progress has been achieved, video instance segmentation
(VIS) methods with per-clip input often fail on challenging videos with
occluded objects and crowded scenes. This is mainly because instance queries in
these methods cannot encode well the discriminative embeddings of instances,
making the query-based segmenter difficult to distinguish those `hard'
instances. To address these issues, we propose to mine discriminative query
embeddings (MDQE) to segment occluded instances on challenging videos. First,
we initialize the positional embeddings and content features of object queries
by considering their spatial contextual information and the inter-frame object
motion. Second, we propose an inter-instance mask repulsion loss to distance
each instance from its nearby non-target instances. The proposed MDQE is the
first VIS method with per-clip input that achieves state-of-the-art results on
challenging videos and competitive performance on simple videos. In specific,
MDQE with ResNet50 achieves 33.0\% and 44.5\% mask AP on OVIS and YouTube-VIS
2021, respectively. Code of MDQE can be found at
\url{https://github.com/MinghanLi/MDQE_CVPR2023}.",None,-1
03915660-c7ee-4993-a48f-d10f3c262408,"Privacy in Large Language Models: Attacks, Defenses and Future Directions",0.850771,16,"The advancement of large language models (LLMs) has significantly enhanced
the ability to effectively tackle various downstream NLP tasks and unify these
tasks into generative pipelines. On the one hand, powerful language models,
trained on massive textual data, have brought unparalleled accessibility and
usability for both models and users. On the other hand, unrestricted access to
these models can also introduce potential malicious and unintentional privacy
risks. Despite ongoing efforts to address the safety and privacy concerns
associated with LLMs, the problem remains unresolved. In this paper, we provide
a comprehensive analysis of the current privacy attacks targeting LLMs and
categorize them according to the adversary's assumed capabilities to shed light
on the potential vulnerabilities present in LLMs. Then, we present a detailed
overview of prominent defense strategies that have been developed to counter
these privacy attacks. Beyond existing works, we identify upcoming privacy
concerns as LLMs evolve. Lastly, we point out several potential avenues for
future exploration.",None,-1
ea99f9c4-0873-40e7-929b-1e43a0f26c40,Navigating Fairness Measures and Trade-Offs,0.770508,5,"In order to monitor and prevent bias in AI systems we can use a wide range of
(statistical) fairness measures. However, it is mathematically impossible to
optimize for all of these measures at the same time. In addition, optimizing a
fairness measure often greatly reduces the accuracy of the system (Kozodoi et
al, 2022). As a result, we need a substantive theory that informs us how to
make these decisions and for what reasons. I show that by using Rawls' notion
of justice as fairness, we can create a basis for navigating fairness measures
and the accuracy trade-off. In particular, this leads to a principled choice
focusing on both the most vulnerable groups and the type of fairness measure
that has the biggest impact on that group. This also helps to close part of the
gap between philosophical accounts of distributive justice and the fairness
literature that has been observed (Kuppler et al, 2021) and to operationalise
the value of fairness.",None,-1
83c06d41-c31a-4461-b563-4873c61eb9d0,Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization,0.65134,6,"Multi-document summarization (MDS) aims to generate a summary for a number of
related documents. We propose HGSUM, an MDS model that extends an
encoder-decoder architecture, to incorporate a heterogeneous graph to represent
different semantic units (e.g., words and sentences) of the documents. This
contrasts with existing MDS models which do not consider different edge types
of graphs and as such do not capture the diversity of relationships in the
documents. To preserve only key information and relationships of the documents
in the heterogeneous graph, HGSUM uses graph pooling to compress the input
graph. And to guide HGSUM to learn compression, we introduce an additional
objective that maximizes the similarity between the compressed graph and the
graph constructed from the ground-truth summary during training. HGSUM is
trained end-to-end with graph similarity and standard cross-entropy objectives.
Experimental results over MULTI-NEWS, WCEP-100, and ARXIV show that HGSUM
outperforms state-of-the-art MDS models. The code for our model and experiments
is available at: https://github.com/oaimli/HGSum.",None,-1
cdf9ac3b-fb5a-46dd-adee-ccb2efef10d7,CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration,0.107933,2,"In recent years, large language models (LLMs) have shown remarkable
capabilities at scale, particularly at generating text conditioned on a prompt.
In our work, we investigate the use of LLMs to augment training data of small
language models~(SLMs) with automatically generated counterfactual~(CF)
instances -- i.e. minimally altered inputs -- in order to improve
out-of-domain~(OOD) performance of SLMs in the extractive question
answering~(QA) setup. We show that, across various LLM generators, such data
augmentation consistently enhances OOD performance and improves model
calibration for both confidence-based and rationale-augmented calibrator
models. Furthermore, these performance improvements correlate with higher
diversity of CF instances in terms of their surface form and semantic content.
Finally, we show that CF augmented models which are easier to calibrate also
exhibit much lower entropy when assigning importance, indicating that
rationale-augmented calibrators prefer concise explanations.",None,-1
968a6f49-8094-4981-9748-a0a9000bee9a,HiPool: Modeling Long Documents Using Graph Neural Networks,0.116831,2,"Encoding long sequences in Natural Language Processing (NLP) is a challenging
problem. Though recent pretraining language models achieve satisfying
performances in many NLP tasks, they are still restricted by a pre-defined
maximum length, making them challenging to be extended to longer sequences. So
some recent works utilize hierarchies to model long sequences. However, most of
them apply sequential models for upper hierarchies, suffering from long
dependency issues. In this paper, we alleviate these issues through a
graph-based method. We first chunk the sequence with a fixed length to model
the sentence-level information. We then leverage graphs to model intra- and
cross-sentence correlations with a new attention mechanism. Additionally, due
to limited standard benchmarks for long document classification (LDC), we
propose a new challenging benchmark, totaling six datasets with up to 53k
samples and 4034 average tokens' length. Evaluation shows our model surpasses
competitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence
dataset. Our method is shown to outperform hierarchical sequential models with
better performance and scalability, especially for longer sequences.",None,-1
725344c4-96b8-49a9-9513-71348e6d9f58,Active Learning for Multilingual Fingerspelling Corpora,0.548522,2,"We apply active learning to help with data scarcity problems in sign
languages. In particular, we perform a novel analysis of the effect of
pre-training. Since many sign languages are linguistic descendants of French
sign language, they share hand configurations, which pre-training can hopefully
exploit. We test this hypothesis on American, Chinese, German, and Irish
fingerspelling corpora. We do observe a benefit from pre-training, but this may
be due to visual rather than linguistic similarities",None,-1
48b4c516-8195-4369-9eef-47bab3efac51,TransWorldNG: Traffic Simulation via Foundation Model,0.773864,9,"Traffic simulation is a crucial tool for transportation decision-making and
policy development. However, achieving realistic simulations in the face of the
high dimensionality and heterogeneity of traffic environments is a longstanding
challenge. In this paper, we present TransWordNG, a traffic simulator that uses
Data-driven algorithms and Graph Computing techniques to learn traffic dynamics
from real data. The functionality and structure of TransWorldNG are introduced,
which utilize a foundation model for transportation management and control. The
results demonstrate that TransWorldNG can generate more realistic traffic
patterns compared to traditional simulators. Additionally, TransWorldNG
exhibits better scalability, as it shows linear growth in computation time as
the scenario scale increases. To the best of our knowledge, this is the first
traffic simulator that can automatically learn traffic patterns from real-world
data and efficiently generate accurate and realistic traffic environments.",None,-1
2e91d036-d76d-414c-900e-9b58193edde2,Agent-based Learning of Materials Datasets from Scientific Literature,0.863402,3,"Advancements in machine learning and artificial intelligence are transforming
materials discovery. Yet, the availability of structured experimental data
remains a bottleneck. The vast corpus of scientific literature presents a
valuable and rich resource of such data. However, manual dataset creation from
these resources is challenging due to issues in maintaining quality and
consistency, scalability limitations, and the risk of human error and bias.
Therefore, in this work, we develop a chemist AI agent, powered by large
language models (LLMs), to overcome these challenges by autonomously creating
structured datasets from natural language text, ranging from sentences and
paragraphs to extensive scientific research articles. Our chemist AI agent,
Eunomia, can plan and execute actions by leveraging the existing knowledge from
decades of scientific research articles, scientists, the Internet and other
tools altogether. We benchmark the performance of our approach in three
different information extraction tasks with various levels of complexity,
including solid-state impurity doping, metal-organic framework (MOF) chemical
formula, and property relations. Our results demonstrate that our zero-shot
agent, with the appropriate tools, is capable of attaining performance that is
either superior or comparable to the state-of-the-art fine-tuned materials
information extraction methods. This approach simplifies compilation of machine
learning-ready datasets for various materials discovery applications, and
significantly ease the accessibility of advanced natural language processing
tools for novice users in natural language. The methodology in this work is
developed as an open-source software on https://github.com/AI4ChemS/Eunomia.",None,-1
47c17399-658d-46bb-847d-f174410e5c8a,Class-Specific Distribution Alignment for Semi-Supervised Medical Image Classification,0.30621,3,"Despite the success of deep neural networks in medical image classification,
the problem remains challenging as data annotation is time-consuming, and the
class distribution is imbalanced due to the relative scarcity of diseases. To
address this problem, we propose Class-Specific Distribution Alignment (CSDA),
a semi-supervised learning framework based on self-training that is suitable to
learn from highly imbalanced datasets. Specifically, we first provide a new
perspective to distribution alignment by considering the process as a change of
basis in the vector space spanned by marginal predictions, and then derive CSDA
to capture class-dependent marginal predictions on both labeled and unlabeled
data, in order to avoid the bias towards majority classes. Furthermore, we
propose a Variable Condition Queue (VCQ) module to maintain a proportionately
balanced number of unlabeled samples for each class. Experiments on three
public datasets HAM10000, CheXpert and Kvasir show that our method provides
competitive performance on semi-supervised skin disease, thoracic disease, and
endoscopic image classification tasks.",None,-1
1906b87f-d3b0-45f0-95ea-24f5dd642451,Transformer-based World Models Are Happy With 100k Interactions,0.685862,39,"Deep neural networks have been successful in many reinforcement learning
settings. However, compared to human learners they are overly data hungry. To
build a sample-efficient world model, we apply a transformer to real-world
episodes in an autoregressive manner: not only the compact latent states and
the taken actions but also the experienced or predicted rewards are fed into
the transformer, so that it can attend flexibly to all three modalities at
different time steps. The transformer allows our world model to access previous
states directly, instead of viewing them through a compressed recurrent state.
By utilizing the Transformer-XL architecture, it is able to learn long-term
dependencies while staying computationally efficient. Our transformer-based
world model (TWM) generates meaningful, new experience, which is used to train
a policy that outperforms previous model-free and model-based reinforcement
learning algorithms on the Atari 100k benchmark.",None,-1
ddaf4c17-d284-44d0-931e-18aa55af6b86,Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation,0.999999,74,"ChatGPT, a large-scale language model based on the advanced GPT-3.5
architecture, has shown remarkable potential in various Natural Language
Processing (NLP) tasks. However, there is currently a dearth of comprehensive
study exploring its potential in the area of Grammatical Error Correction
(GEC). To showcase its capabilities in GEC, we design zero-shot
chain-of-thought (CoT) and few-shot CoT settings using in-context learning for
ChatGPT. Our evaluation involves assessing ChatGPT's performance on five
official test sets in three different languages, along with three
document-level GEC test sets in English. Our experimental results and human
evaluations demonstrate that ChatGPT has excellent error detection capabilities
and can freely correct errors to make the corrected sentences very fluent,
possibly due to its over-correction tendencies and not adhering to the
principle of minimal edits. Additionally, its performance in non-English and
low-resource settings highlights its potential in multilingual GEC tasks.
However, further analysis of various types of errors at the document-level has
shown that ChatGPT cannot effectively correct agreement, coreference, tense
errors across sentences, and cross-sentence boundary errors.",None,-1
cd196c9b-fc45-412b-a517-971305a7d657,Unsupervised Synthetic Image Refinement via Contrastive Learning and Consistent Semantic-Structural Constraints,0.377871,5,"Ensuring the realism of computer-generated synthetic images is crucial to
deep neural network (DNN) training. Due to different semantic distributions
between synthetic and real-world captured datasets, there exists semantic
mismatch between synthetic and refined images, which in turn results in the
semantic distortion. Recently, contrastive learning (CL) has been successfully
used to pull correlated patches together and push uncorrelated ones apart. In
this work, we exploit semantic and structural consistency between synthetic and
refined images and adopt CL to reduce the semantic distortion. Besides, we
incorporate hard negative mining to improve the performance furthermore. We
compare the performance of our method with several other benchmarking methods
using qualitative and quantitative measures and show that our method offers the
state-of-the-art performance.",None,-1
4318b605-04ec-4cd0-a7af-8114386d1a9a,Diving Deep into Modes of Fact Hallucinations in Dialogue Systems,0.525736,20,"Knowledge Graph(KG) grounded conversations often use large pre-trained models
and usually suffer from fact hallucination. Frequently entities with no
references in knowledge sources and conversation history are introduced into
responses, thus hindering the flow of the conversation -- existing work attempt
to overcome this issue by tweaking the training procedure or using a multi-step
refining method. However, minimal effort is put into constructing an
entity-level hallucination detection system, which would provide fine-grained
signals that control fallacious content while generating responses. As a first
step to address this issue, we dive deep to identify various modes of
hallucination in KG-grounded chatbots through human feedback analysis.
Secondly, we propose a series of perturbation strategies to create a synthetic
dataset named FADE (FActual Dialogue Hallucination DEtection Dataset). Finally,
we conduct comprehensive data analyses and create multiple baseline models for
hallucination detection to compare against human-verified data and already
established benchmarks.",None,-1
e59cf5b7-cdb9-47b6-90af-3f2f0a69550a,Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection,0.240986,3,"Knowledge distillation addresses the problem of transferring knowledge from a
teacher model to a student model. In this process, we typically have multiple
types of knowledge extracted from the teacher model. The problem is to make
full use of them to train the student model. Our preliminary study shows that:
(1) not all of the knowledge is necessary for learning a good student model,
and (2) knowledge distillation can benefit from certain knowledge at different
training steps. In response to these, we propose an actor-critic approach to
selecting appropriate knowledge to transfer during the process of knowledge
distillation. In addition, we offer a refinement of the training algorithm to
ease the computational burden. Experimental results on the GLUE datasets show
that our method outperforms several strong knowledge distillation baselines
significantly.",None,-1
cf439585-9193-4bd4-9859-a46dddbc61ac,A Wide Evaluation of ChatGPT on Affective Computing Tasks,0.832693,13,"With the rise of foundation models, a new artificial intelligence paradigm
has emerged, by simply using general purpose foundation models with prompting
to solve problems instead of training a separate machine learning model for
each problem. Such models have been shown to have emergent properties of
solving problems that they were not initially trained on. The studies for the
effectiveness of such models are still quite limited. In this work, we widely
study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13
affective computing problems, namely aspect extraction, aspect polarity
classification, opinion extraction, sentiment analysis, sentiment intensity
ranking, emotions intensity ranking, suicide tendency detection, toxicity
detection, well-being assessment, engagement measurement, personality
assessment, sarcasm detection, and subjectivity detection. We introduce a
framework to evaluate the ChatGPT models on regression-based problems, such as
intensity ranking problems, by modelling them as pairwise ranking
classification. We compare ChatGPT against more traditional NLP methods, such
as end-to-end recurrent neural networks and transformers. The results
demonstrate the emergent abilities of the ChatGPT models on a wide range of
affective computing problems, where GPT-3.5 and especially GPT-4 have shown
strong performance on many problems, particularly the ones related to
sentiment, emotions, or toxicity. The ChatGPT models fell short for problems
with implicit signals, such as engagement measurement and subjectivity
detection.",None,-1
48f0e101-f087-4b17-a2be-f17d75689585,Learning Visual Representations via Language-Guided Sampling,0.550505,19,"Although an object may appear in numerous contexts, we often describe it in a
limited number of ways. Language allows us to abstract away visual variation to
represent and communicate concepts. Building on this intuition, we propose an
alternative approach to visual representation learning: using language
similarity to sample semantically similar image pairs for contrastive learning.
Our approach diverges from image-based contrastive learning by sampling view
pairs using language similarity instead of hand-crafted augmentations or
learned clusters. Our approach also differs from image-text contrastive
learning by relying on pre-trained language models to guide the learning rather
than directly minimizing a cross-modal loss. Through a series of experiments,
we show that language-guided learning yields better features than image-based
and image-text representation learning approaches.",None,-1
1b4d14b4-bcc8-42d5-9de7-e48af3af0679,CARLA-BSP: a simulated dataset with pedestrians,0.33011,1,"We present a sample dataset featuring pedestrians generated using the ARCANE
framework, a new framework for generating datasets in CARLA (0.9.13). We
provide use cases for pedestrian detection, autoencoding, pose estimation, and
pose lifting. We also showcase baseline results. For more information, visit
https://project-arcane.eu/.",None,-1
9d73ccac-2f48-46f0-b9d9-e12bae95eb3d,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,0.768886,6,"Recently, deep learning models have shown the potential to predict breast
cancer risk and enable targeted screening strategies, but current models do not
consider the change in the breast over time. In this paper, we present a new
method, PRIME+, for breast cancer risk prediction that leverages prior
mammograms using a transformer decoder, outperforming a state-of-the-art risk
prediction method that only uses mammograms from a single time point. We
validate our approach on a dataset with 16,113 exams and further demonstrate
that it effectively captures patterns of changes from prior mammograms, such as
changes in breast density, resulting in improved short-term and long-term
breast cancer risk prediction. Experimental results show that our model
achieves a statistically significant improvement in performance over the
state-of-the-art based model, with a C-index increase from 0.68 to 0.73 (p <
0.05) on held-out test sets.",None,-1
80f394af-22cb-4571-90ef-98793ded3d43,JSEEGraph: Joint Structured Event Extraction as Graph Parsing,0.659386,4,"We propose a graph-based event extraction framework JSEEGraph that approaches
the task of event extraction as general graph parsing in the tradition of
Meaning Representation Parsing. It explicitly encodes entities and events in a
single semantic graph, and further has the flexibility to encode a wider range
of additional IE relations and jointly infer individual tasks. JSEEGraph
performs in an end-to-end manner via general graph parsing: (1) instead of flat
sequence labelling, nested structures between entities/triggers are efficiently
encoded as separate nodes in the graph, allowing for nested and overlapping
entities and triggers; (2) both entities, relations, and events can be encoded
in the same graph, where entities and event triggers are represented as nodes
and entity relations and event arguments are constructed via edges; (3) joint
inference avoids error propagation and enhances the interpolation of different
IE tasks. We experiment on two benchmark datasets of varying structural
complexities; ACE05 and Rich ERE, covering three languages: English, Chinese,
and Spanish. Experimental results show that JSEEGraph can handle nested event
structures, that it is beneficial to solve different IE tasks jointly, and that
event argument extraction in particular benefits from entity extraction. Our
code and models are released as open-source.",None,-1
2751f478-6f2b-4616-91c7-d55677251bbd,Localized Symbolic Knowledge Distillation for Visual Commonsense Models,0.191999,5,"Instruction following vision-language (VL) models offer a flexible interface
that supports a broad range of multimodal tasks in a zero-shot fashion.
However, interfaces that operate on full images do not directly enable the user
to ""point to"" and access specific regions within images. This capability is
important not only to support reference-grounded VL benchmarks, but also, for
practical applications that require precise within-image reasoning. We build
Localized Visual Commonsense models, which allow users to specify (multiple)
regions as input. We train our model by sampling localized commonsense
knowledge from a large language model (LLM): specifically, we prompt an LLM to
collect commonsense knowledge given a global literal image description and a
local literal region description automatically generated by a set of VL models.
With a separately trained critic model that selects high-quality examples, we
find that training on the localized commonsense corpus can successfully distill
existing VL models to support a reference-as-input interface. Empirical results
and human evaluations in a zero-shot setup demonstrate that our distillation
method results in more precise VL models of reasoning compared to a baseline of
passing a generated referring expression to an LLM.",None,-1
af0a8a6d-ddcd-4ab9-8a95-a55f0876a373,MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos,0.604357,15,"User-generated content (UGC) live videos are often bothered by various
distortions during capture procedures and thus exhibit diverse visual
qualities. Such source videos are further compressed and transcoded by media
server providers before being distributed to end-users. Because of the
flourishing of UGC live videos, effective video quality assessment (VQA) tools
are needed to monitor and perceptually optimize live streaming videos in the
distributing process. In this paper, we address \textbf{UGC Live VQA} problems
by constructing a first-of-a-kind subjective UGC Live VQA database and
developing an effective evaluation tool. Concretely, 418 source UGC videos are
collected in real live streaming scenarios and 3,762 compressed ones at
different bit rates are generated for the subsequent subjective VQA
experiments. Based on the built database, we develop a
\underline{M}ulti-\underline{D}imensional \underline{VQA} (\textbf{MD-VQA})
evaluator to measure the visual quality of UGC live videos from semantic,
distortion, and motion aspects respectively. Extensive experimental results
show that MD-VQA achieves state-of-the-art performance on both our UGC Live VQA
database and existing compressed UGC VQA databases.",None,-1
a43e0a62-d59e-4aaa-ab3c-902d6431b48d,Graph Agent: Explicit Reasoning Agent for Graphs,0.471165,3,"Graph embedding methods such as Graph Neural Networks (GNNs) and Graph
Transformers have contributed to the development of graph reasoning algorithms
for various tasks on knowledge graphs. However, the lack of interpretability
and explainability of graph embedding methods has limited their applicability
in scenarios requiring explicit reasoning. In this paper, we introduce the
Graph Agent (GA), an intelligent agent methodology of leveraging large language
models (LLMs), inductive-deductive reasoning modules, and long-term memory for
knowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning
and existing graph embedding methods to provide an innovative approach for
complex graph reasoning tasks. By converting graph structures into textual
data, GA enables LLMs to process, reason, and provide predictions alongside
human-interpretable explanations. The effectiveness of the GA was evaluated on
node classification and link prediction tasks. Results showed that GA reached
state-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and
89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to
existing GNN and transformer models, GA offered advantages of explicit
reasoning ability, free-of-training, easy adaption to various graph reasoning
tasks",None,-1
d93c7aa8-f097-4a32-9e6b-aa244e833e93,SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation,0.552713,2,"Detecting objects and estimating their 6D poses is essential for automated
systems to interact safely with the environment. Most 6D pose estimators,
however, rely on a single camera frame and suffer from occlusions and
ambiguities due to object symmetries. We overcome this issue by presenting a
novel symmetry-aware multi-view 6D pose estimator called SyMFM6D. Our approach
efficiently fuses the RGB-D frames from multiple perspectives in a deep
multi-directional fusion network and predicts predefined keypoints for all
objects in the scene simultaneously. Based on the keypoints and an instance
semantic segmentation, we efficiently compute the 6D poses by least-squares
fitting. To address the ambiguity issues for symmetric objects, we propose a
novel training procedure for symmetry-aware keypoint detection including a new
objective function. Our SyMFM6D network significantly outperforms the
state-of-the-art in both single-view and multi-view 6D pose estimation. We
furthermore show the effectiveness of our symmetry-aware training procedure and
demonstrate that our approach is robust towards inaccurate camera calibration
and dynamic camera setups.",None,-1
4e76c5f2-5554-45cf-a2ab-ab4703c6b914,Credit Assignment: Challenges and Opportunities in Developing Human-like AI Agents,0.333257,6,"Temporal credit assignment is crucial for learning and skill development in
natural and artificial intelligence. While computational methods like the TD
approach in reinforcement learning have been proposed, it's unclear if they
accurately represent how humans handle feedback delays. Cognitive models intend
to represent the mental steps by which humans solve problems and perform a
number of tasks, but limited research in cognitive science has addressed the
credit assignment problem in humans and cognitive models. Our research uses a
cognitive model based on a theory of decisions from experience, Instance-Based
Learning Theory (IBLT), to test different credit assignment mechanisms in a
goal-seeking navigation task with varying levels of decision complexity.
Instance-Based Learning (IBL) models simulate the process of making sequential
choices with different credit assignment mechanisms, including a new IBL-TD
model that combines the IBL decision mechanism with the TD approach. We found
that (1) An IBL model that gives equal credit assignment to all decisions is
able to match human performance better than other models, including IBL-TD and
Q-learning; (2) IBL-TD and Q-learning models underperform compared to humans
initially, but eventually, they outperform humans; (3) humans are influenced by
decision complexity, while models are not. Our study provides insights into the
challenges of capturing human behavior and the potential opportunities to use
these models in future AI systems to support human activities.",None,-1
992a7089-c274-4abd-acbf-bdfbbbdebb6e,Active Visual Exploration Based on Attention-Map Entropy,0.41033,4,"Active visual exploration addresses the issue of limited sensor capabilities
in real-world scenarios, where successive observations are actively chosen
based on the environment. To tackle this problem, we introduce a new technique
called Attention-Map Entropy (AME). It leverages the internal uncertainty of
the transformer-based model to determine the most informative observations. In
contrast to existing solutions, it does not require additional loss components,
which simplifies the training. Through experiments, which also mimic
retina-like sensors, we show that such simplified training significantly
improves the performance of reconstruction, segmentation and classification on
publicly available datasets.",None,-1
b0c610a0-386d-4d7c-b3ee-3326e51b0100,Optimal Interpretability-Performance Trade-off of Classification Trees with Black-Box Reinforcement Learning,0.10306,1,"Interpretability of AI models allows for user safety checks to build trust in
these models. In particular, decision trees (DTs) provide a global view on the
learned model and clearly outlines the role of the features that are critical
to classify a given data. However, interpretability is hindered if the DT is
too large. To learn compact trees, a Reinforcement Learning (RL) framework has
been recently proposed to explore the space of DTs. A given supervised
classification task is modeled as a Markov decision problem (MDP) and then
augmented with additional actions that gather information about the features,
equivalent to building a DT. By appropriately penalizing these actions, the RL
agent learns to optimally trade-off size and performance of a DT. However, to
do so, this RL agent has to solve a partially observable MDP. The main
contribution of this paper is to prove that it is sufficient to solve a fully
observable problem to learn a DT optimizing the interpretability-performance
trade-off. As such any planning or RL algorithm can be used. We demonstrate the
effectiveness of this approach on a set of classical supervised classification
datasets and compare our approach with other interpretability-performance
optimizing methods.",None,-1
0efa094b-238f-4eef-ac08-b8834f0c95a4,Gemini Pro Defeated by GPT-4V: Evidence from Education,0.893935,13,"This study compared the classification performance of Gemini Pro and GPT-4V
in educational settings. Employing visual question answering (VQA) techniques,
the study examined both models' abilities to read text-based rubrics and then
automatically score student-drawn models in science education. We employed both
quantitative and qualitative analyses using a dataset derived from
student-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics
for Image Feedback) prompting methods. The findings reveal that GPT-4V
significantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic
Weighted Kappa. The qualitative analysis reveals that the differences may be
due to the models' ability to process fine-grained texts in images and overall
image classification performance. Even adapting the NERIF approach by further
de-sizing the input images, Gemini Pro seems not able to perform as well as
GPT-4V. The findings suggest GPT-4V's superior capability in handling complex
multimodal educational tasks. The study concludes that while both models
represent advancements in AI, GPT-4V's higher performance makes it a more
suitable tool for educational applications involving multimodal data
interpretation.",None,-1
1b04fd21-fa18-421c-8cfb-596d419fc7c2,High-Fidelity Clothed Avatar Reconstruction from a Single Image,0.913186,24,"This paper presents a framework for efficient 3D clothed avatar
reconstruction. By combining the advantages of the high accuracy of
optimization-based methods and the efficiency of learning-based methods, we
propose a coarse-to-fine way to realize a high-fidelity clothed avatar
reconstruction (CAR) from a single image. At the first stage, we use an
implicit model to learn the general shape in the canonical space of a person in
a learning-based way, and at the second stage, we refine the surface detail by
estimating the non-rigid deformation in the posed space in an optimization way.
A hyper-network is utilized to generate a good initialization so that the
convergence o f the optimization process is greatly accelerated. Extensive
experiments on various datasets show that the proposed CAR successfully
produces high-fidelity avatars for arbitrarily clothed humans in real scenes.",None,-1
4e593a87-cdeb-44fd-bc26-1fe3c452b874,Automatic Photo Orientation Detection with Convolutional Neural Networks,0.486722,18,"We apply convolutional neural networks (CNN) to the problem of image
orientation detection in the context of determining the correct orientation
(from 0, 90, 180, and 270 degrees) of a consumer photo. The problem is
especially important for digitazing analog photographs. We substantially
improve on the published state of the art in terms of the performance on one of
the standard datasets, and test our system on a more difficult large dataset of
consumer photos. We use Guided Backpropagation to obtain insights into how our
CNN detects photo orientation, and to explain its mistakes.",None,-1
0c23e989-cac8-4668-86d7-75786c3fed22,Learning to Forecast Aleatoric and Epistemic Uncertainties over Long Horizon Trajectories,0.266892,6,"Giving autonomous agents the ability to forecast their own outcomes and
uncertainty will allow them to communicate their competencies and be used more
safely. We accomplish this by using a learned world model of the agent system
to forecast full agent trajectories over long time horizons. Real world systems
involve significant sources of both aleatoric and epistemic uncertainty that
compound and interact over time in the trajectory forecasts. We develop a deep
generative world model that quantifies aleatoric uncertainty while
incorporating the effects of epistemic uncertainty during the learning process.
We show on two reinforcement learning problems that our uncertainty model
produces calibrated outcome uncertainty estimates over the full trajectory
horizon.",None,-1
6e55f8bd-20f1-45d5-92a4-dd62d714feaf,Distribution-Aligned Diffusion for Human Mesh Recovery,0.696941,7,"Recovering a 3D human mesh from a single RGB image is a challenging task due
to depth ambiguity and self-occlusion, resulting in a high degree of
uncertainty. Meanwhile, diffusion models have recently seen much success in
generating high-quality outputs by progressively denoising noisy inputs.
Inspired by their capability, we explore a diffusion-based approach for human
mesh recovery, and propose a Human Mesh Diffusion (HMDiff) framework which
frames mesh recovery as a reverse diffusion process. We also propose a
Distribution Alignment Technique (DAT) that infuses prior distribution
information into the mesh distribution diffusion process, and provides useful
prior knowledge to facilitate the mesh recovery task. Our method achieves
state-of-the-art performance on three widely used datasets. Project page:
https://gongjia0208.github.io/HMDiff/.",None,-1
f1b095ee-1fe7-4c10-940a-69868c4959ef,Benchmarking Deepart Detection,0.158657,8,"Deepfake technologies have been blurring the boundaries between the real and
unreal, likely resulting in malicious events. By leveraging newly emerged
deepfake technologies, deepfake researchers have been making a great upending
to create deepfake artworks (deeparts), which are further closing the gap
between reality and fantasy. To address potentially appeared ethics questions,
this paper establishes a deepart detection database (DDDB) that consists of a
set of high-quality conventional art images (conarts) and five sets of deepart
images generated by five state-of-the-art deepfake models. This database
enables us to explore once-for-all deepart detection and continual deepart
detection. For the two new problems, we suggest four benchmark evaluations and
four families of solutions on the constructed DDDB. The comprehensive study
demonstrates the effectiveness of the proposed solutions on the established
benchmark dataset, which is capable of paving a way to more interesting
directions of deepart detection. The constructed benchmark dataset and the
source code will be made publicly available.",None,-1
d87316d0-bf0b-4440-bb2d-e67e41c966ae,GDBN: a Graph Neural Network Approach to Dynamic Bayesian Network,0.180357,1,"Identifying causal relations among multi-variate time series is one of the
most important elements towards understanding the complex mechanisms underlying
the dynamic system. It provides critical tools for forecasting, simulations and
interventions in science and business analytics. In this paper, we proposed a
graph neural network approach with score-based method aiming at learning a
sparse DAG that captures the causal dependencies in a discretized time temporal
graph. We demonstrate methods with graph neural network significantly
outperformed other state-of-the-art methods with dynamic bayesian networking
inference. In addition, from the experiments, the structural causal model can
be more accurate than a linear SCM discovered by the methods such as Notears.",None,-1
8b95ba62-95a6-4c5b-9de8-d7e0f37b71a2,Redrawing attendance boundaries to promote racial and ethnic diversity in elementary schools,0.486412,4,"Most US school districts draw ""attendance boundaries"" to define catchment
areas that assign students to schools near their homes, often recapitulating
neighborhood demographic segregation in schools. Focusing on elementary
schools, we ask: how much might we reduce school segregation by redrawing
attendance boundaries? Combining parent preference data with methods from
combinatorial optimization, we simulate alternative boundaries for 98 US school
districts serving over 3 million elementary-aged students, minimizing
White/non-White segregation while mitigating changes to travel times and school
sizes. Across districts, we observe a median 14% relative decrease in
segregation, which we estimate would require approximately 20\% of students to
switch schools and, surprisingly, a slight reduction in travel times. We
release a public dashboard depicting these alternative boundaries
(https://www.schooldiversity.org/) and invite both school boards and their
constituents to evaluate their viability. Our results show the possibility of
greater integration without significant disruptions for families.",None,-1
4cf676c3-4498-41dd-a4e4-0ec670595c71,Exploring the Benefits of Visual Prompting in Differential Privacy,0.755532,9,"Visual Prompting (VP) is an emerging and powerful technique that allows
sample-efficient adaptation to downstream tasks by engineering a well-trained
frozen source model. In this work, we explore the benefits of VP in
constructing compelling neural network classifiers with differential privacy
(DP). We explore and integrate VP into canonical DP training methods and
demonstrate its simplicity and efficiency. In particular, we discover that VP
in tandem with PATE, a state-of-the-art DP training method that leverages the
knowledge transfer from an ensemble of teachers, achieves the state-of-the-art
privacy-utility trade-off with minimum expenditure of privacy budget. Moreover,
we conduct additional experiments on cross-domain image classification with a
sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we
also conduct extensive ablation studies to validate the effectiveness and
contribution of VP under DP consideration. Our code is available at
(https://github.com/EzzzLi/Prompt-PATE).",None,-1
290ceeb7-cb28-4e63-8087-7688610d5c7d,Efficient Encoders for Streaming Sequence Tagging,0.0996314,4,"A naive application of state-of-the-art bidirectional encoders for streaming
sequence tagging would require encoding each token from scratch for each new
token in an incremental streaming input (like transcribed speech). The lack of
re-usability of previous computation leads to a higher number of Floating Point
Operations (or FLOPs) and higher number of unnecessary label flips. Increased
FLOPs consequently lead to higher wall-clock time and increased label flipping
leads to poorer streaming performance. In this work, we present a Hybrid
Encoder with Adaptive Restart (HEAR) that addresses these issues while
maintaining the performance of bidirectional encoders over the offline (or
complete) inputs while improving performance on streaming (or incomplete)
inputs. HEAR has a Hybrid unidirectional-bidirectional encoder architecture to
perform sequence tagging, along with an Adaptive Restart Module (ARM) to
selectively guide the restart of bidirectional portion of the encoder. Across
four sequence tagging tasks, HEAR offers FLOP savings in streaming settings
upto 71.1% and also outperforms bidirectional encoders for streaming
predictions by upto +10% streaming exact match.",None,-1
415b358b-bea8-4ad6-8f0f-f60ca8ea80b3,Knowledge Enhanced Semantic Communication Receiver,0.796956,9,"In recent years, with the rapid development of deep learning and natural
language processing technologies, semantic communication has become a topic of
great interest in the field of communication. Although existing deep
learning-based semantic communication approaches have shown many advantages,
they still do not make sufficient use of prior knowledge. Moreover, most
existing semantic communication methods focus on the semantic encoding at the
transmitter side, while we believe that the semantic decoding capability of the
receiver should also be concerned. In this paper, we propose a knowledge
enhanced semantic communication framework in which the receiver can more
actively utilize the facts in the knowledge base for semantic reasoning and
decoding, on the basis of only affecting the parameters rather than the
structure of the neural networks at the transmitter side. Specifically, we
design a transformer-based knowledge extractor to find relevant factual triples
for the received noisy signal. Extensive simulation results on the WebNLG
dataset demonstrate that the proposed receiver yields superior performance on
top of the knowledge graph enhanced decoding.",None,-1
ed20dd08-0faa-41c1-965b-eca7d02e66a8,Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning,0.637571,1,"To advance argumentative stance prediction as a multimodal problem, the First
Shared Task in Multimodal Argument Mining hosted stance prediction in crucial
social topics of gun control and abortion. Our exploratory study attempts to
evaluate the necessity of images for stance prediction in tweets and compare
out-of-the-box text-based large-language models (LLM) in few-shot settings
against fine-tuned unimodal and multimodal models. Our work suggests an
ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms
both the multimodal (0.677 F1-score) and text-based few-shot prediction using a
recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in
performance, our findings suggest that the multimodal models tend to perform
better when image content is summarized as natural language over their native
pixel structure and, using in-context examples improves few-shot performance of
LLMs.",None,-1
35a53513-a4c7-424e-b26d-f6f0aa209533,"ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition",0.278155,1,"Video Action Recognition (VAR) is a challenging task due to its inherent
complexities. Though different approaches have been explored in the literature,
designing a unified framework to recognize a large number of human actions is
still a challenging problem. Recently, Multi-Modal Learning (MML) has
demonstrated promising results in this domain. In literature, 2D skeleton or
pose modality has often been used for this task, either independently or in
conjunction with the visual information (RGB modality) present in videos.
However, the combination of pose, visual information, and text attributes has
not been explored yet, though text and pose attributes independently have been
proven to be effective in numerous computer vision tasks. In this paper, we
present the first pose augmented Vision-language model (VLM) for VAR. Notably,
our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video
action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even
without any video data pre-training, and an accuracy of 96.11% and 75.75% after
kinetics pre-training.",None,-1
9814a8a1-021b-44c9-af6f-a0f67970124e,PerturbScore: Connecting Discrete and Continuous Perturbations in NLP,0.0817187,2,"With the rapid development of neural network applications in NLP, model
robustness problem is gaining more attention. Different from computer vision,
the discrete nature of texts makes it more challenging to explore robustness in
NLP. Therefore, in this paper, we aim to connect discrete perturbations with
continuous perturbations, therefore we can use such connections as a bridge to
help understand discrete perturbations in NLP models. Specifically, we first
explore how to connect and measure the correlation between discrete
perturbations and continuous perturbations. Then we design a regression task as
a PerturbScore to learn the correlation automatically. Through experimental
results, we find that we can build a connection between discrete and continuous
perturbations and use the proposed PerturbScore to learn such correlation,
surpassing previous methods used in discrete perturbation measuring. Further,
the proposed PerturbScore can be well generalized to different datasets,
perturbation methods, indicating that we can use it as a powerful tool to study
model robustness in NLP.",None,-1
c5c4b7b7-4057-4bd3-89c2-672a3b0a0e6f,Mobile User Interface Element Detection Via Adaptively Prompt Tuning,0.908859,3,"Recent object detection approaches rely on pretrained vision-language models
for image-text alignment. However, they fail to detect the Mobile User
Interface (MUI) element since it contains additional OCR information, which
describes its content and function but is often ignored. In this paper, we
develop a new MUI element detection dataset named MUI-zh and propose an
Adaptively Prompt Tuning (APT) module to take advantage of discriminating OCR
information. APT is a lightweight and effective module to jointly optimize
category prompts across different modalities. For every element, APT uniformly
encodes its visual features and OCR descriptions to dynamically adjust the
representation of frozen category prompts. We evaluate the effectiveness of our
plug-and-play APT upon several existing CLIP-based detectors for both standard
and open-vocabulary MUI element detection. Extensive experiments show that our
method achieves considerable improvements on two datasets. The datasets is
available at \url{github.com/antmachineintelligence/MUI-zh}.",None,-1
5cf80162-4c2b-409e-9374-e2f8120464cf,Parameter Sharing with Network Pruning for Scalable Multi-Agent Deep Reinforcement Learning,0.346992,2,"Handling the problem of scalability is one of the essential issues for
multi-agent reinforcement learning (MARL) algorithms to be applied to
real-world problems typically involving massively many agents. For this,
parameter sharing across multiple agents has widely been used since it reduces
the training time by decreasing the number of parameters and increasing the
sample efficiency. However, using the same parameters across agents limits the
representational capacity of the joint policy and consequently, the performance
can be degraded in multi-agent tasks that require different behaviors for
different agents. In this paper, we propose a simple method that adopts
structured pruning for a deep neural network to increase the representational
capacity of the joint policy without introducing additional parameters. We
evaluate the proposed method on several benchmark tasks, and numerical results
show that the proposed method significantly outperforms other parameter-sharing
methods.",None,-1
d01e8b08-68e7-4eae-b82a-8e9c56903268,Prompted LLMs as Chatbot Modules for Long Open-domain Conversation,0.655466,34,"In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for
creating high-quality conversational agents without the need for fine-tuning.
Our method utilizes pre-trained large language models (LLMs) as individual
modules for long-term consistency and flexibility, by using techniques such as
few-shot prompting, chain-of-thought (CoT), and external memory. Our human
evaluation results show that MPC is on par with fine-tuned chatbot models in
open-domain conversations, making it an effective solution for creating
consistent and engaging chatbots.",None,-1
3879f7fc-2554-4751-aa94-e6e2cbd89488,Neural Invertible Variable-degree Optical Aberrations Correction,0.557554,2,"Optical aberrations of optical systems cause significant degradation of
imaging quality. Aberration correction by sophisticated lens designs and
special glass materials generally incurs high cost of manufacturing and the
increase in the weight of optical systems, thus recent work has shifted to
aberration correction with deep learning-based post-processing. Though
real-world optical aberrations vary in degree, existing methods cannot
eliminate variable-degree aberrations well, especially for the severe degrees
of degradation. Also, previous methods use a single feed-forward neural network
and suffer from information loss in the output. To address the issues, we
propose a novel aberration correction method with an invertible architecture by
leveraging its information-lossless property. Within the architecture, we
develop conditional invertible blocks to allow the processing of aberrations
with variable degrees. Our method is evaluated on both a synthetic dataset from
physics-based imaging simulation and a real captured dataset. Quantitative and
qualitative experimental results demonstrate that our method outperforms
compared methods in correcting variable-degree optical aberrations.",None,-1
d76662b7-056d-43d6-9cde-4017df46b85c,Layout-guided Indoor Panorama Inpainting with Plane-aware Normalization,0.455341,4,"We present an end-to-end deep learning framework for indoor panoramic image
inpainting. Although previous inpainting methods have shown impressive
performance on natural perspective images, most fail to handle panoramic
images, particularly indoor scenes, which usually contain complex structure and
texture content. To achieve better inpainting quality, we propose to exploit
both the global and local context of indoor panorama during the inpainting
process. Specifically, we take the low-level layout edges estimated from the
input panorama as a prior to guide the inpainting model for recovering the
global indoor structure. A plane-aware normalization module is employed to
embed plane-wise style features derived from the layout into the generator,
encouraging local texture restoration from adjacent room structures (i.e.,
ceiling, floor, and walls). Experimental results show that our work outperforms
the current state-of-the-art methods on a public panoramic dataset in both
qualitative and quantitative evaluations. Our code is available at
https://ericsujw.github.io/LGPN-net/",None,-1
25f23df1-469a-4b63-9bbb-a18ac54a9091,KBNet: Kernel Basis Network for Image Restoration,0.959461,28,"How to aggregate spatial information plays an essential role in
learning-based image restoration. Most existing CNN-based networks adopt static
convolutional kernels to encode spatial information, which cannot aggregate
spatial information adaptively. Recent transformer-based architectures achieve
adaptive spatial aggregation. But they lack desirable inductive biases of
convolutions and require heavy computational costs. In this paper, we propose a
kernel basis attention (KBA) module, which introduces learnable kernel bases to
model representative image patterns for spatial information aggregation.
Different kernel bases are trained to model different local structures. At each
spatial location, they are linearly and adaptively fused by predicted
pixel-wise coefficients to obtain aggregation weights. Based on the KBA module,
we further design a multi-axis feature fusion (MFF) block to encode and fuse
channel-wise, spatial-invariant, and pixel-adaptive features for image
restoration. Our model, named kernel basis network (KBNet), achieves
state-of-the-art performances on more than ten benchmarks over image denoising,
deraining, and deblurring tasks while requiring less computational cost than
previous SOTA methods.",None,-1
b0ca0a1c-0dec-47ad-b93a-9b98506cbbcf,"If consciousness is dynamically relevant, artificial intelligence isn't conscious",0.035565,2,"We demonstrate that if consciousness is relevant for the temporal evolution
of a system's states--that is, if it is dynamically relevant--then AI systems
cannot be conscious. That is because AI systems run on CPUs, GPUs, TPUs or
other processors which have been designed and verified to adhere to
computational dynamics that systematically preclude or suppress deviations. The
design and verification preclude or suppress, in particular, potential
consciousness-related dynamical effects, so that if consciousness is
dynamically relevant, AI systems cannot be conscious.",None,-1
8d07388e-1485-4847-bccb-d6f139e9ca0f,AI model GPT-3 (dis)informs us better than humans,0.831456,74,"Artificial intelligence is changing the way we create and evaluate
information, and this is happening during an infodemic, which has been having
dramatic effects on global health. In this paper we evaluate whether recruited
individuals can distinguish disinformation from accurate information,
structured in the form of tweets, and determine whether a tweet is organic or
synthetic, i.e., whether it has been written by a Twitter user or by the AI
model GPT-3. Our results show that GPT-3 is a double-edge sword, which, in
comparison with humans, can produce accurate information that is easier to
understand, but can also produce more compelling disinformation. We also show
that humans cannot distinguish tweets generated by GPT-3 from tweets written by
human users. Starting from our results, we reflect on the dangers of AI for
disinformation, and on how we can improve information campaigns to benefit
global health.",None,-1
8dd7d443-3e99-40df-93b6-75aa633a66be,Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers,0.756061,5,"Fine-grained classification is a challenging task that involves identifying
subtle differences between objects within the same category. This task is
particularly challenging in scenarios where data is scarce. Visual transformers
(ViT) have recently emerged as a powerful tool for image classification, due to
their ability to learn highly expressive representations of visual data using
self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine
tuned using semi-supervised learning techniques, suitable for situations where
we have lack of annotated data. This is particularly common in e-commerce,
where images are readily available but labels are noisy, nonexistent, or
expensive to obtain. Our results demonstrate that Semi-ViT outperforms
traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned
with limited annotated data. These findings indicate that Semi-ViTs hold
significant promise for applications that require precise and fine-grained
classification of visual data.",None,-1
764c6a9d-d262-4da5-a3a2-8d55a16861fa,Rendering Humans from Object-Occluded Monocular Videos,0.314786,6,"3D understanding and rendering of moving humans from monocular videos is a
challenging task. Despite recent progress, the task remains difficult in
real-world scenarios, where obstacles may block the camera view and cause
partial occlusions in the captured videos. Existing methods cannot handle such
defects due to two reasons. First, the standard rendering strategy relies on
point-point mapping, which could lead to dramatic disparities between the
visible and occluded areas of the body. Second, the naive direct regression
approach does not consider any feasibility criteria (ie, prior information) for
rendering under occlusions. To tackle the above drawbacks, we present OccNeRF,
a neural rendering method that achieves better rendering of humans in severely
occluded scenes. As direct solutions to the two drawbacks, we propose
surface-based rendering by integrating geometry and visibility priors. We
validate our method on both simulated and real-world occlusions and demonstrate
our method's superiority.",None,-1
43227d23-fa0b-4ec9-b3c3-76c35d741a6a,Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs,0.754654,11,"Large Language Models (LLMs) have demonstrated exceptional proficiency in
language-related tasks, but their deployment poses significant challenges due
to substantial memory and storage requirements. Weight-only quantization has
emerged as a promising solution to address these challenges. Previous research
suggests that fine-tuning through up and down rounding can enhance performance.
In this study, we introduce SignRound, a method that utilizes signed gradient
descent (SignSGD) to optimize rounding values and weight clipping within just
200 steps. SignRound integrates the advantages of Quantization-Aware Training
(QAT) and Post-Training Quantization (PTQ), achieving exceptional results
across 2 to 4 bits while maintaining low tuning costs and avoiding additional
inference overhead. For example, SignRound achieves absolute average accuracy
improvements ranging from 6.91\% to 33.22\% at 2 bits. It also demonstrates
robust generalization to recent models and achieves near-lossless quantization
in most scenarios at 4 bits. The source code is publicly available at
\url{https://github.com/intel/auto-round}.",None,-1
0a61e500-0c8e-4225-8cc5-68c6d9e99815,Visual-Language Prompt Tuning with Knowledge-guided Context Optimization,0.864916,71,"Prompt tuning is an effective way to adapt the pre-trained visual-language
model (VLM) to the downstream task using task-related textual tokens.
Representative CoOp-based work combines the learnable textual tokens with the
class tokens to obtain specific textual knowledge. However, the specific
textual knowledge is the worse generalization to the unseen classes because it
forgets the essential general textual knowledge having a strong generalization
ability. To tackle this issue, we introduce a novel Knowledge-guided Context
Optimization (KgCoOp) to enhance the generalization ability of the learnable
prompt for unseen classes. The key insight of KgCoOp is that forgetting about
essential knowledge can be alleviated by reducing the discrepancy between the
learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the
discrepancy between the textual embeddings generated by learned prompts and the
hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can
make a discriminative prompt for both seen and unseen tasks. Extensive
evaluation of several benchmarks demonstrates that the proposed
Knowledge-guided Context Optimization is an efficient method for prompt tuning,
\emph{i.e.,} achieves better performance with less training time.",None,-1
0d3d331e-8bfe-4219-8778-377f7181f254,Dual Learning for Large Vocabulary On-Device ASR,0.0783221,1,"Dual learning is a paradigm for semi-supervised machine learning that seeks
to leverage unsupervised data by solving two opposite tasks at once. In this
scheme, each model is used to generate pseudo-labels for unlabeled examples
that are used to train the other model. Dual learning has seen some use in
speech processing by pairing ASR and TTS as dual tasks. However, these results
mostly address only the case of using unpaired examples to compensate for very
small supervised datasets, and mostly on large, non-streaming models. Dual
learning has not yet been proven effective for using unsupervised data to
improve realistic on-device streaming models that are already trained on large
supervised corpora. We provide this missing piece though an analysis of an
on-device-sized streaming conformer trained on the entirety of Librispeech,
showing relative WER improvements of 10.7%/5.2% without an LM and 11.7%/16.4%
with an LM.",None,-1
ae28d68a-1e44-479f-8860-76decf8620fe,Multi-View Azimuth Stereo via Tangent Space Consistency,0.640484,5,"We present a method for 3D reconstruction only using calibrated multi-view
surface azimuth maps. Our method, multi-view azimuth stereo, is effective for
textureless or specular surfaces, which are difficult for conventional
multi-view stereo methods. We introduce the concept of tangent space
consistency: Multi-view azimuth observations of a surface point should be
lifted to the same tangent space. Leveraging this consistency, we recover the
shape by optimizing a neural implicit surface representation. Our method
harnesses the robust azimuth estimation capabilities of photometric stereo
methods or polarization imaging while bypassing potentially complex zenith
angle estimation. Experiments using azimuth maps from various sources validate
the accurate shape recovery with our method, even without zenith angles.",None,-1
79f8eac5-e3bf-4b6e-86a3-7f1cc275ee1b,Toucan: Token-Aware Character Level Language Modeling,0.0427266,1,"Character-level language models obviate the need for separately trained
tokenizers, but efficiency suffers from longer sequence lengths. Learning to
combine character representations into tokens has made training these models
more efficient, but they still require decoding characters individually. We
propose Toucan, an augmentation to character-level models to make them
""token-aware"". Comparing our method to prior work, we demonstrate significant
speed-ups in character generation without a loss in language modeling
performance. We then explore differences between our learned dynamic
tokenization of character sequences with popular fixed vocabulary solutions
such as Byte-Pair Encoding and WordPiece, finding our approach leads to a
greater amount of longer sequences tokenized as single items. Our project and
code are available at https://nlp.jhu.edu/nuggets/.",None,-1
55319c31-6b73-4104-b814-6f6f8796e56d,Diacritic Recognition Performance in Arabic ASR,0.457022,2,"We present an analysis of diacritic recognition performance in Arabic
Automatic Speech Recognition (ASR) systems. As most existing Arabic speech
corpora do not contain all diacritical marks, which represent short vowels and
other phonetic information in Arabic script, current state-of-the-art ASR
models do not produce full diacritization in their output. Automatic text-based
diacritization has previously been employed both as a pre-processing step to
train diacritized ASR, or as a post-processing step to diacritize the resulting
ASR hypotheses. It is generally believed that input diacritization degrades ASR
performance, but no systematic evaluation of ASR diacritization performance,
independent of ASR performance, has been conducted to date. In this paper, we
attempt to experimentally clarify whether input diacritiztation indeed degrades
ASR quality, and to compare the diacritic recognition performance against
text-based diacritization as a post-processing step. We start with pre-trained
Arabic ASR models and fine-tune them on transcribed speech data with different
diacritization conditions: manual, automatic, and no diacritization. We isolate
diacritic recognition performance from the overall ASR performance using
coverage and precision metrics. We find that ASR diacritization significantly
outperforms text-based diacritization in post-processing, particularly when the
ASR model is fine-tuned with manually diacritized transcripts.",None,-1
0f34d6f2-4e42-4de4-ac14-ad6987a253d4,Chasing Consistency in Text-to-3D Generation from a Single Image,0.500005,10,"Text-to-3D generation from a single-view image is a popular but challenging
task in 3D vision. Although numerous methods have been proposed, existing works
still suffer from the inconsistency issues, including 1) semantic
inconsistency, 2) geometric inconsistency, and 3) saturation inconsistency,
resulting in distorted, overfitted, and over-saturated generations. In light of
the above issues, we present Consist3D, a three-stage framework Chasing for
semantic-, geometric-, and saturation-Consistent Text-to-3D generation from a
single image, in which the first two stages aim to learn parameterized
consistency tokens, and the last stage is for optimization. Specifically, the
semantic encoding stage learns a token independent of views and estimations,
promoting semantic consistency and robustness. Meanwhile, the geometric
encoding stage learns another token with comprehensive geometry and
reconstruction constraints under novel-view estimations, reducing overfitting
and encouraging geometric consistency. Finally, the optimization stage benefits
from the semantic and geometric tokens, allowing a low classifier-free guidance
scale and therefore preventing oversaturation. Experimental results demonstrate
that Consist3D produces more consistent, faithful, and photo-realistic 3D
assets compared to previous state-of-the-art methods. Furthermore, Consist3D
also allows background and object editing through text prompts.",None,-1
ff2e8939-8cc0-4e45-8bfc-e205229ebdf5,Nearest Neighbors Meet Deep Neural Networks for Point Cloud Analysis,0.829318,8,"Performances on standard 3D point cloud benchmarks have plateaued, resulting
in oversized models and complex network design to make a fractional
improvement. We present an alternative to enhance existing deep neural networks
without any redesigning or extra parameters, termed as Spatial-Neighbor Adapter
(SN-Adapter). Building on any trained 3D network, we utilize its learned
encoding capability to extract features of the training dataset and summarize
them as prototypical spatial knowledge. For a test point cloud, the SN-Adapter
retrieves k nearest neighbors (k-NN) from the pre-constructed spatial
prototypes and linearly interpolates the k-NN prediction with that of the
original 3D network. By providing complementary characteristics, the proposed
SN-Adapter serves as a plug-and-play module to economically improve performance
in a non-parametric manner. More importantly, our SN-Adapter can be effectively
generalized to various 3D tasks, including shape classification, part
segmentation, and 3D object detection, demonstrating its superiority and
robustness. We hope our approach could show a new perspective for point cloud
analysis and facilitate future research.",None,-1
c97a3158-cd04-4c09-abc9-e5c14b0a6523,RSPT: Reconstruct Surroundings and Predict Trajectories for Generalizable Active Object Tracking,0.356836,4,"Active Object Tracking (AOT) aims to maintain a specific relation between the
tracker and object(s) by autonomously controlling the motion system of a
tracker given observations. AOT has wide-ranging applications, such as in
mobile robots and autonomous driving. However, building a generalizable active
tracker that works robustly across different scenarios remains a challenge,
especially in unstructured environments with cluttered obstacles and diverse
layouts. We argue that constructing a state representation capable of modeling
the geometry structure of the surroundings and the dynamics of the target is
crucial for achieving this goal. To address this challenge, we present RSPT, a
framework that forms a structure-aware motion representation by Reconstructing
the Surroundings and Predicting the target Trajectory. Additionally, we enhance
the generalization of the policy network by training in an asymmetric dueling
mechanism. We evaluate RSPT on various simulated scenarios and show that it
outperforms existing methods in unseen environments, particularly those with
complex obstacles and layouts. We also demonstrate the successful transfer of
RSPT to real-world settings. Project Website:
https://sites.google.com/view/aot-rspt.",None,-1
a852bcbc-98f4-426e-8020-9926a1e86a32,"Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models",0.782911,16,"The introduction of diffusion models in anomaly detection has paved the way
for more effective and accurate image reconstruction in pathologies. However,
the current limitations in controlling noise granularity hinder diffusion
models' ability to generalize across diverse anomaly types and compromise the
restoration of healthy tissues. To overcome these challenges, we propose
AutoDDPM, a novel approach that enhances the robustness of diffusion models.
AutoDDPM utilizes diffusion models to generate initial likelihood maps of
potential anomalies and seamlessly integrates them with the original image.
Through joint noised distribution re-sampling, AutoDDPM achieves harmonization
and in-painting effects. Our study demonstrates the efficacy of AutoDDPM in
replacing anomalous regions while preserving healthy tissues, considerably
surpassing diffusion models' limitations. It also contributes valuable insights
and analysis on the limitations of current diffusion models, promoting robust
and interpretable anomaly detection in medical imaging - an essential aspect of
building autonomous clinical decision systems with higher interpretability.",None,-1
be656241-4d06-4363-abcb-d071eb03c7cd,Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation,0.684019,8,"Emotion regulation is a crucial element in dealing with emotional events and
has positive effects on mental health. This paper aims to provide a more
comprehensive understanding of emotional events by introducing a new French
corpus of emotional narratives collected using a questionnaire for emotion
regulation. We follow the theoretical framework of the Component Process Model
which considers emotions as dynamic processes composed of four interrelated
components (behavior, feeling, thinking and territory). Each narrative is
related to a discrete emotion and is structured based on all emotion components
by the writers. We study the interaction of components and their impact on
emotion classification with machine learning methods and pre-trained language
models. Our results show that each component improves prediction performance,
and that the best results are achieved by jointly considering all components.
Our results also show the effectiveness of pre-trained language models in
predicting discrete emotion from certain components, which reveal differences
in how emotion components are expressed.",None,-1
ee64ff08-ac47-4cbc-a9cb-770b4245b996,Active hypothesis testing in unknown environments using recurrent neural networks and model free reinforcement learning,0.143515,1,"A combination of deep reinforcement learning and supervised learning is
proposed for the problem of active sequential hypothesis testing in completely
unknown environments. We make no assumptions about the prior probability, the
action and observation sets, and the observation generating process. Our method
can be used in any environment even if it has continuous observations or
actions, and performs competitively and sometimes better than the Chernoff
test, in both finite and infinite horizon problems, despite not having access
to the environment dynamics.",None,-1
fd73b0a9-e0a0-4645-952c-d58c017680b8,Rethinking Document-Level Relation Extraction: A Reality Check,0.909479,6,"Recently, numerous efforts have continued to push up performance boundaries
of document-level relation extraction (DocRE) and have claimed significant
progress in DocRE. In this paper, we do not aim at proposing a novel model for
DocRE. Instead, we take a closer look at the field to see if these performance
gains are actually true. By taking a comprehensive literature review and a
thorough examination of popular DocRE datasets, we find that these performance
gains are achieved upon a strong or even untenable assumption in common: all
named entities are perfectly localized, normalized, and typed in advance. Next,
we construct four types of entity mention attacks to examine the robustness of
typical DocRE models by behavioral probing. We also have a close check on model
usability in a more realistic setting. Our findings reveal that most of current
DocRE models are vulnerable to entity mention attacks and difficult to be
deployed in real-world end-user NLP applications. Our study calls more
attentions for future research to stop simplifying problem setups, and to model
DocRE in the wild rather than in an unrealistic Utopian world.",None,-1
40e13452-c7fa-4995-86b6-351b5ec3b807,Open-world Semi-supervised Novel Class Discovery,0.374813,3,"Traditional semi-supervised learning tasks assume that both labeled and
unlabeled data follow the same class distribution, but the realistic open-world
scenarios are of more complexity with unknown novel classes mixed in the
unlabeled set. Therefore, it is of great challenge to not only recognize
samples from known classes but also discover the unknown number of novel
classes within the unlabeled data. In this paper, we introduce a new open-world
semi-supervised novel class discovery approach named OpenNCD, a progressive
bi-level contrastive learning method over multiple prototypes. The proposed
method is composed of two reciprocally enhanced parts. First, a bi-level
contrastive learning method is introduced, which maintains the pair-wise
similarity of the prototypes and the prototype group levels for better
representation learning. Then, a reliable prototype similarity metric is
proposed based on the common representing instances. Prototypes with high
similarities will be grouped progressively for known class recognition and
novel class discovery. Extensive experiments on three image datasets are
conducted and the results show the effectiveness of the proposed method in
open-world scenarios, especially with scarce known classes and labels.",None,-1
32b51878-f147-493c-bd35-ad156bf0c491,Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation,0.614387,37,"Human evaluation is critical for validating the performance of text-to-image
generative models, as this highly cognitive process requires deep comprehension
of text and images. However, our survey of 37 recent papers reveals that many
works rely solely on automatic measures (e.g., FID) or perform poorly described
human evaluations that are not reliable or repeatable. This paper proposes a
standardized and well-defined human evaluation protocol to facilitate
verifiable and reproducible human evaluation in future works. In our pilot data
collection, we experimentally show that the current automatic measures are
incompatible with human perception in evaluating the performance of the
text-to-image generation results. Furthermore, we provide insights for
designing human evaluation experiments reliably and conclusively. Finally, we
make several resources publicly available to the community to facilitate easy
and fast implementations.",None,-1
aaa6ece8-0c9c-4f84-b2f0-feffeedf32a3,Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models,0.275378,7,"We present a novel method, the Chain of Empathy (CoE) prompting, that
utilizes insights from psychotherapy to induce Large Language Models (LLMs) to
reason about human emotional states. This method is inspired by various
psychotherapy approaches including Cognitive Behavioral Therapy (CBT),
Dialectical Behavior Therapy (DBT), Person Centered Therapy (PCT), and Reality
Therapy (RT), each leading to different patterns of interpreting clients'
mental states. LLMs without reasoning generated predominantly exploratory
responses. However, when LLMs used CoE reasoning, we found a more comprehensive
range of empathetic responses aligned with the different reasoning patterns of
each psychotherapy model. The CBT based CoE resulted in the most balanced
generation of empathetic responses. The findings underscore the importance of
understanding the emotional context and how it affects human and AI
communication. Our research contributes to understanding how psychotherapeutic
models can be incorporated into LLMs, facilitating the development of
context-specific, safer, and empathetic AI.",None,-1
408fd06f-c67c-4b64-ac35-001bcf96285a,Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation,0.566415,17,"Recent ubiquity and disruptive impacts of large language models (LLMs) have
raised concerns about their potential to be misused (.i.e, generating
large-scale harmful and misleading content). To combat this emerging risk of
LLMs, we propose a novel ""Fighting Fire with Fire"" (F3) strategy that harnesses
modern LLMs' generative and emergent reasoning capabilities to counter
human-written and LLM-generated disinformation. First, we leverage
GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content
through paraphrase-based and perturbation-based prefix-style prompts,
respectively. Second, we apply zero-shot in-context semantic reasoning
techniques with cloze-style prompts to discern genuine from deceptive posts and
news articles. In our extensive experiments, we observe GPT-3.5-turbo's
zero-shot superiority for both in-distribution and out-of-distribution
datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike
the decline observed in previous customized and fine-tuned disinformation
detectors. Our codebase and dataset are available at
https://github.com/mickeymst/F3.",None,-1
e1b108fe-4943-45c8-bf80-ccf7096c8f40,Optimizing delegation between human and AI collaborative agents,0.594539,4,"In the context of humans operating with artificial or autonomous agents in a
hybrid team, it is essential to accurately identify when to authorize those
team members to perform actions. Given past examples where humans and
autonomous systems can either succeed or fail at tasks, we seek to train a
delegating manager agent to make delegation decisions with respect to these
potential performance deficiencies. Additionally, we cannot always expect the
various agents to operate within the same underlying model of the environment.
It is possible to encounter cases where the actions and transitions would vary
between agents. Therefore, our framework provides a manager model which learns
through observations of team performance without restricting agents to matching
dynamics. Our results show our manager learns to perform delegation decisions
with teams of agents operating under differing representations of the
environment, significantly outperforming alternative methods to manage the
team.",None,-1
9b446596-afaa-4a72-8bd5-6f8a97d1d65e,Lexi: Self-Supervised Learning of the UI Language,0.25152,7,"Humans can learn to operate the user interface (UI) of an application by
reading an instruction manual or how-to guide. Along with text, these resources
include visual content such as UI screenshots and images of application icons
referenced in the text. We explore how to leverage this data to learn generic
visio-linguistic representations of UI screens and their components. These
representations are useful in many real applications, such as accessibility,
voice navigation, and task automation. Prior UI representation models rely on
UI metadata (UI trees and accessibility labels), which is often missing,
incompletely defined, or not accessible. We avoid such a dependency, and
propose Lexi, a pre-trained vision and language model designed to handle the
unique features of UI screens, including their text richness and context
sensitivity. To train Lexi we curate the UICaption dataset consisting of 114k
UI images paired with descriptions of their functionality. We evaluate Lexi on
four tasks: UI action entailment, instruction-based UI image retrieval,
grounding referring expressions, and UI entity recognition.",None,-1
67bfca9c-e8ec-4d43-81b5-51f502c0e409,Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems,0.945103,18,"When learning task-oriented dialogue (ToD) agents, reinforcement learning
(RL) techniques can naturally be utilized to train dialogue strategies to
achieve user-specific goals. Prior works mainly focus on adopting advanced RL
techniques to train the ToD agents, while the design of the reward function is
not well studied. This paper aims at answering the question of how to
efficiently learn and leverage a reward function for training end-to-end (E2E)
ToD agents. Specifically, we introduce two generalized objectives for
reward-function learning, inspired by the classical learning-to-rank
literature. Further, we utilize the learned reward function to guide the
training of the E2E ToD agent. With the proposed techniques, we achieve
competitive results on the E2E response-generation task on the Multiwoz 2.0
dataset. Source code and checkpoints are publicly released at
https://github.com/Shentao-YANG/Fantastic_Reward_ICLR2023.",None,-1
fe0b1af2-7dc1-41ab-b69a-e2d829fee36f,Iterative Geometry Encoding Volume for Stereo Matching,0.99678,63,"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in
matching tasks. However, all-pairs correlations lack non-local geometry
knowledge and have difficulties tackling local ambiguities in ill-posed
regions. In this paper, we propose Iterative Geometry Encoding Volume
(IGEV-Stereo), a new deep network architecture for stereo matching. The
proposed IGEV-Stereo builds a combined geometry encoding volume that encodes
geometry and context information as well as local matching details, and
iteratively indexes it to update the disparity map. To speed up the
convergence, we exploit GEV to regress an accurate starting point for ConvGRUs
iterations. Our IGEV-Stereo ranks $1^{st}$ on KITTI 2015 and 2012 (Reflective)
among all published methods and is the fastest among the top 10 methods. In
addition, IGEV-Stereo has strong cross-dataset generalization as well as high
inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e.
IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is
available at https://github.com/gangweiX/IGEV.",None,-1
62e63047-1773-43a8-967f-206eec444352,Can Generative Large Language Models Perform ASR Error Correction?,0.925919,13,"ASR error correction is an interesting option for post processing speech
recognition system outputs. These error correction models are usually trained
in a supervised fashion using the decoding results of a target ASR system. This
approach can be computationally intensive and the model is tuned to a specific
ASR system. Recently generative large language models (LLMs) have been applied
to a wide range of natural language processing tasks, as they can operate in a
zero-shot or few shot fashion. In this paper we investigate using ChatGPT, a
generative LLM, for ASR error correction. Based on the ASR N-best output, we
propose both unconstrained and constrained, where a member of the N-best list
is selected, approaches. Additionally, zero and 1-shot settings are evaluated.
Experiments show that this generative LLM approach can yield performance gains
for two different state-of-the-art ASR architectures, transducer and
attention-encoder-decoder based, and multiple test sets.",None,-1
2fa5d557-7797-4499-a74d-a95bac6af931,Fused Classification For Differential Face Morphing Detection,0.232718,1,"Face morphing, a sophisticated presentation attack technique, poses
significant security risks to face recognition systems. Traditional methods
struggle to detect morphing attacks, which involve blending multiple face
images to create a synthetic image that can match different individuals. In
this paper, we focus on the differential detection of face morphing and propose
an extended approach based on fused classification method for no-reference
scenario. We introduce a public face morphing detection benchmark for the
differential scenario and utilize a specific data mining technique to enhance
the performance of our approach. Experimental results demonstrate the
effectiveness of our method in detecting morphing attacks.",None,-1
3907b192-b092-4185-b7c7-643ca14e5255,PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment,0.986745,30,"Camera pose estimation is a long-standing computer vision problem that to
date often relies on classical methods, such as handcrafted keypoint matching,
RANSAC and bundle adjustment. In this paper, we propose to formulate the
Structure from Motion (SfM) problem inside a probabilistic diffusion framework,
modelling the conditional distribution of camera poses given input images. This
novel view of an old problem has several advantages. (i) The nature of the
diffusion framework mirrors the iterative procedure of bundle adjustment. (ii)
The formulation allows a seamless integration of geometric constraints from
epipolar geometry. (iii) It excels in typically difficult scenarios such as
sparse views with wide baselines. (iv) The method can predict intrinsics and
extrinsics for an arbitrary amount of images. We demonstrate that our method
PoseDiffusion significantly improves over the classic SfM pipelines and the
learned approaches on two real-world datasets. Finally, it is observed that our
method can generalize across datasets without further training. Project page:
https://posediffusion.github.io/",None,-1
c00a8fbf-0db4-4c72-8116-5ba51b036bb3,NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes,0.274832,20,"Complex reasoning ability is one of the most important features of current
LLMs, which has also been leveraged to play an integral role in complex
decision-making tasks. Therefore, the investigation into the reasoning
capabilities of Large Language Models (LLMs) is critical: numerous benchmarks
have been established to assess the reasoning abilities of LLMs. However,
current benchmarks are inadequate in offering a rigorous evaluation of the full
extent of reasoning abilities that LLMs are capable of achieving. They are also
prone to the risk of overfitting, as these benchmarks, being publicly
accessible and static, allow models to potentially tailor their responses to
specific benchmark metrics, thereby inflating their performance. Addressing
these limitations, our research introduces a new benchmark, named NPHardEval.
This benchmark is designed to evaluate the reasoning abilities of LLMs across a
broad spectrum of 900 algorithmic questions, extending up to the NP-Hard
complexity class. These questions are meticulously chosen to represent a wide
range of complexity class below the NP-hard complexity class, offering a
rigorous measure of the reasoning ability of LLMs. Through this study, we shed
light on the current state of reasoning in LLMs, providing an objective and
rigorous perspective through the comparison of LLMs' performance across complex
classes. Moreover, this benchmark is designed with a dynamic update mechanism,
where the datapoints are refreshed on a monthly basis. Such regular updates
play a crucial role in mitigating the risk of LLMs overfitting to the
benchmark, promoting a more accurate and reliable assessment of their reasoning
capabilities. The benchmark dataset and code of NPHardEval are available at
https://github.com/casmlab/NPHardEval.",None,-1
e91fd438-7dff-4f4c-9c5a-b59df6749ba1,Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors,0.375919,3,"A major challenge in the practical use of Machine Translation (MT) is that
users lack guidance to make informed decisions about when to rely on outputs.
Progress in quality estimation research provides techniques to automatically
assess MT quality, but these techniques have primarily been evaluated in vitro
by comparison against human judgments outside of a specific context of use.
This paper evaluates quality estimation feedback in vivo with a human study
simulating decision-making in high-stakes medical settings. Using Emergency
Department discharge instructions, we study how interventions based on quality
estimation versus backtranslation assist physicians in deciding whether to show
MT outputs to a patient. We find that quality estimation improves appropriate
reliance on MT, but backtranslation helps physicians detect more clinically
harmful errors that QE alone often misses.",None,-1
4c5d6f6d-0d93-4084-8a7c-b8f55ed72493,nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla Sentiment Analysis,0.559276,4,"In this paper, we discuss the nlpBDpatriots entry to the shared task on
Sentiment Analysis of Bangla Social Media Posts organized at the first workshop
on Bangla Language Processing (BLP) co-located with EMNLP. The main objective
of this task is to identify the polarity of social media content using a Bangla
dataset annotated with positive, neutral, and negative labels provided by the
shared task organizers. Our best system for this task is a transfer learning
approach with data augmentation which achieved a micro F1 score of 0.71. Our
best system ranked 12th among 30 teams that participated in the competition.",None,-1
efacce17-b848-4ced-ba8b-06fd5ebca033,The Secret of Metaphor on Expressing Stronger Emotion,0.864665,8,"Metaphors are proven to have stronger emotional impact than literal
expressions. Although this conclusion is shown to be promising in benefiting
various NLP applications, the reasons behind this phenomenon are not well
studied. This paper conducts the first study in exploring how metaphors convey
stronger emotion than their literal counterparts. We find that metaphors are
generally more specific than literal expressions. The more specific property of
metaphor can be one of the reasons for metaphors' superiority in emotion
expression. When we compare metaphors with literal expressions with the same
specificity level, the gap of emotion expressing ability between both reduces
significantly. In addition, we observe specificity is crucial in literal
language as well, as literal language can express stronger emotion by making it
more specific.",None,-1
883d9d25-7c2b-40f2-9039-c6c9f75af543,Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering,0.076607,1,"Large language models (LLMs) enable zero-shot approaches in open-domain
question answering (ODQA), yet with limited advancements as the reader is
compared to the retriever. This study aims at the feasibility of a zero-shot
reader that addresses the challenges of computational cost and the need for
labeled data. We find that LLMs are distracted due to irrelevant documents in
the retrieved set and the overconfidence of the generated answers when they are
exploited as zero-shot readers. To tackle these problems, we mitigate the
impact of such documents via Distraction-aware Answer Selection (DAS) with a
negation-based instruction and score adjustment for proper answer selection.
Experimental results show that our approach successfully handles distraction
across diverse scenarios, enhancing the performance of zero-shot readers.
Furthermore, unlike supervised readers struggling with unseen data, zero-shot
readers demonstrate outstanding transferability without any training.",None,-1
16bcd429-0b40-4c0c-a4fa-db16fc49b359,On the Importance of Noise Scheduling for Diffusion Models,0.69145,83,"We empirically study the effect of noise scheduling strategies for denoising
diffusion generative models. There are three findings: (1) the noise scheduling
is crucial for the performance, and the optimal one depends on the task (e.g.,
image sizes), (2) when increasing the image size, the optimal noise scheduling
shifts towards a noisier one (due to increased redundancy in pixels), and (3)
simply scaling the input data by a factor of $b$ while keeping the noise
schedule function fixed (equivalent to shifting the logSNR by $\log b$) is a
good strategy across image sizes. This simple recipe, when combined with
recently proposed Recurrent Interface Network (RIN), yields state-of-the-art
pixel-based diffusion models for high-resolution images on ImageNet, enabling
single-stage, end-to-end generation of diverse and high-fidelity images at
1024$\times$1024 resolution (without upsampling/cascades).",None,-1
0ebf67c5-848b-4030-bc7b-a284f8a8ae3d,Weakly Supervised Human Skin Segmentation using Guidance Attention Mechanisms,0.113731,1,"Human skin segmentation is a crucial task in computer vision and biometric
systems, yet it poses several challenges such as variability in skin color,
pose, and illumination. This paper presents a robust data-driven skin
segmentation method for a single image that addresses these challenges through
the integration of contextual information and efficient network design. In
addition to robustness and accuracy, the integration into real-time systems
requires a careful balance between computational power, speed, and performance.
The proposed method incorporates two attention modules, Body Attention and Skin
Attention, that utilize contextual information to improve segmentation results.
These modules draw attention to the desired areas, focusing on the body
boundaries and skin pixels, respectively. Additionally, an efficient network
architecture is employed in the encoder part to minimize computational power
while retaining high performance. To handle the issue of noisy labels in skin
datasets, the proposed method uses a weakly supervised training strategy,
relying on the Skin Attention module. The results of this study demonstrate
that the proposed method is comparable to, or outperforms, state-of-the-art
methods on benchmark datasets.",None,-1
f96f1af9-4cce-4d3d-8736-b8e9c84eba4b,Hierarchical Vision Transformers for Cardiac Ejection Fraction Estimation,0.833826,6,"The left ventricular of ejection fraction is one of the most important metric
of cardiac function. It is used by cardiologist to identify patients who are
eligible for lifeprolonging therapies. However, the assessment of ejection
fraction suffers from inter-observer variability. To overcome this challenge,
we propose a deep learning approach, based on hierarchical vision Transformers,
to estimate the ejection fraction from echocardiogram videos. The proposed
method can estimate ejection fraction without the need for left ventrice
segmentation first, make it more efficient than other methods. We evaluated our
method on EchoNet-Dynamic dataset resulting 5.59, 7.59 and 0.59 for MAE, RMSE
and R2 respectivelly. This results are better compared to the state-of-the-art
method, Ultrasound Video Transformer (UVT). The source code is available on
https://github.com/lhfazry/UltraSwin.",None,-1
3ecbf59d-f8ad-4e71-b7e6-c12ab3cda1ad,Construction of Knowledge Graphs: State and Challenges,0.925324,12,"With knowledge graphs (KGs) at the center of numerous applications such as
recommender systems and question answering, the need for generalized pipelines
to construct and continuously update such KGs is increasing. While the
individual steps that are necessary to create KGs from unstructured (e.g. text)
and structured data sources (e.g. databases) are mostly well-researched for
their one-shot execution, their adoption for incremental KG updates and the
interplay of the individual steps have hardly been investigated in a systematic
manner so far. In this work, we first discuss the main graph models for KGs and
introduce the major requirement for future KG construction pipelines. Next, we
provide an overview of the necessary steps to build high-quality KGs, including
cross-cutting topics such as metadata management, ontology development, and
quality assurance. We then evaluate the state of the art of KG construction
w.r.t the introduced requirements for specific popular KGs as well as some
recent tools and strategies for KG construction. Finally, we identify areas in
need of further research and improvement.",None,-1
49586f63-0878-40a1-9032-a7206385bb61,SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data,0.853472,7,"Text-to-SQL aims to automate the process of generating SQL queries on a
database from natural language text. In this work, we propose ""SQLPrompt"",
tailored to improve the few-shot prompting capabilities of Text-to-SQL for
Large Language Models (LLMs). Our methods include innovative prompt design,
execution-based consistency decoding strategy which selects the SQL with the
most consistent execution outcome among other SQL proposals, and a method that
aims to improve performance by diversifying the SQL proposals during
consistency selection with different prompt designs (""MixPrompt"") and
foundation models (""MixLLMs""). We show that \emph{SQLPrompt} outperforms
previous approaches for in-context learning with few labeled data by a large
margin, closing the gap with finetuning state-of-the-art with thousands of
labeled data.",None,-1
3bdc2956-7f66-4a4e-b5ec-455a8b954788,Efficient and Explicit Modelling of Image Hierarchies for Image Restoration,0.999876,86,"The aim of this paper is to propose a mechanism to efficiently and explicitly
model image hierarchies in the global, regional, and local range for image
restoration. To achieve that, we start by analyzing two important properties of
natural images including cross-scale similarity and anisotropic image features.
Inspired by that, we propose the anchored stripe self-attention which achieves
a good balance between the space and time complexity of self-attention and the
modelling capacity beyond the regional range. Then we propose a new network
architecture dubbed GRL to explicitly model image hierarchies in the Global,
Regional, and Local range via anchored stripe self-attention, window
self-attention, and channel attention enhanced convolution. Finally, the
proposed network is applied to 7 image restoration types, covering both real
and synthetic settings. The proposed method sets the new state-of-the-art for
several of those. Code will be available at
https://github.com/ofsoundof/GRL-Image-Restoration.git.",None,-1
05930784-8204-4107-b710-53a03902c038,Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation,0.843233,15,"Understanding and manipulating deformable objects (e.g., ropes and fabrics)
is an essential yet challenging task with broad applications. Difficulties come
from complex states and dynamics, diverse configurations and high-dimensional
action space of deformable objects. Besides, the manipulation tasks usually
require multiple steps to accomplish, and greedy policies may easily lead to
local optimal states. Existing studies usually tackle this problem using
reinforcement learning or imitating expert demonstrations, with limitations in
modeling complex states or requiring hand-crafted expert policies. In this
paper, we study deformable object manipulation using dense visual affordance,
with generalization towards diverse states, and propose a novel kind of
foresightful dense affordance, which avoids local optima by estimating states'
values for long-term manipulation. We propose a framework for learning this
representation, with novel designs such as multi-stage stable learning and
efficient self-supervised data collection without experts. Experiments
demonstrate the superiority of our proposed foresightful dense affordance.
Project page: https://hyperplane-lab.github.io/DeformableAffordance",None,-1
4b298157-9b0b-4dfa-840d-e2956fac8fb4,Algebra Error Classification with Large Language Models,0.599918,5,"Automated feedback as students answer open-ended math questions has
significant potential in improving learning outcomes at large scale. A key part
of automated feedback systems is an error classification component, which
identifies student errors and enables appropriate, predefined feedback to be
deployed. Most existing approaches to error classification use a rule-based
method, which has limited capacity to generalize. Existing data-driven methods
avoid these limitations but specifically require mathematical expressions in
student responses to be parsed into syntax trees. This requirement is itself a
limitation, since student responses are not always syntactically valid and
cannot be converted into trees. In this work, we introduce a flexible method
for error classification using pre-trained large language models. We
demonstrate that our method can outperform existing methods in algebra error
classification, and is able to classify a larger set of student responses.
Additionally, we analyze common classification errors made by our method and
discuss limitations of automated error classification.",None,-1
a7e72f2f-147b-4607-b923-231d4d3d62cc,EXnet: Efficient In-context Learning for Data-less Text classification,0.0647183,1,"Large pre-trained language models (PLMs) have made significant progress in
encoding world knowledge and spawned a new set of learning paradigms including
zero-shot, few-shot, and in-context learning. Many language tasks can be
modeled as a set of prompts (for example, is this text about geography?) and
language models can provide binary answers, i.e., Yes or No. There is evidence
to suggest that the next-word prediction used by many PLMs does not align well
with zero-shot paradigms. Therefore, PLMs are fine-tuned as a
question-answering system. In-context learning extends zero-shot learning by
incorporating prompts and examples, resulting in increased task accuracy. Our
paper presents EXnet, a model specifically designed to perform in-context
learning without any limitations on the number of examples. We argue that
in-context learning is an effective method to increase task accuracy, and
providing examples facilitates cross-task generalization, especially when it
comes to text classification tasks. With extensive experiments, we show that
even our smallest model (15M parameters) generalizes to several unseen
classification tasks and domains.",None,-1
5295b821-05b4-4c44-b172-2af26fd32999,AU-aware graph convolutional network for Macro- and Micro-expression spotting,0.766672,4,"Automatic Micro-Expression (ME) spotting in long videos is a crucial step in
ME analysis but also a challenging task due to the short duration and low
intensity of MEs. When solving this problem, previous works generally lack in
considering the structures of human faces and the correspondence between
expressions and relevant facial muscles. To address this issue for better
performance of ME spotting, this paper seeks to extract finer spatial features
by modeling the relationships between facial Regions of Interest (ROIs).
Specifically, we propose a graph convolutional-based network, called
Action-Unit-aWare Graph Convolutional Network (AUW-GCN). Furthermore, to inject
prior information and to cope with the problem of small datasets, AU-related
statistics are encoded into the network. Comprehensive experiments show that
our results outperform baseline methods consistently and achieve new SOTA
performance in two benchmark datasets,CAS(ME)^2 and SAMM-LV. Our code is
available at https://github.com/xjtupanda/AUW-GCN.",None,-1
be763b55-61ba-4289-8560-c2339ea68b73,Object Discovery from Motion-Guided Tokens,0.212293,10,"Object discovery -- separating objects from the background without manual
labels -- is a fundamental open challenge in computer vision. Previous methods
struggle to go beyond clustering of low-level cues, whether handcrafted (e.g.,
color, texture) or learned (e.g., from auto-encoders). In this work, we augment
the auto-encoder representation learning framework with two key components:
motion-guidance and mid-level feature tokenization. Although both have been
separately investigated, we introduce a new transformer decoder showing that
their benefits can compound thanks to motion-guided vector quantization. We
show that our architecture effectively leverages the synergy between motion and
tokenization, improving upon the state of the art on both synthetic and real
datasets. Our approach enables the emergence of interpretable object-specific
mid-level features, demonstrating the benefits of motion-guidance (no labeling)
and quantization (interpretability, memory efficiency).",None,-1
ccfcdd86-ddb2-45a6-ad02-d63a80a9307c,A Diffusion Model for Event Skeleton Generation,0.522847,2,"Event skeleton generation, aiming to induce an event schema skeleton graph
with abstracted event nodes and their temporal relations from a set of event
instance graphs, is a critical step in the temporal complex event schema
induction task. Existing methods effectively address this task from a graph
generation perspective but suffer from noise-sensitive and error accumulation,
e.g., the inability to correct errors while generating schema. We, therefore,
propose a novel Diffusion Event Graph Model~(DEGM) to address these issues. Our
DEGM is the first workable diffusion model for event skeleton generation, where
the embedding and rounding techniques with a custom edge-based loss are
introduced to transform a discrete event graph into learnable latent
representation. Furthermore, we propose a denoising training process to
maintain the model's robustness. Consequently, DEGM derives the final schema,
where error correction is guaranteed by iteratively refining the latent
representation during the schema generation process. Experimental results on
three IED bombing datasets demonstrate that our DEGM achieves better results
than other state-of-the-art baselines. Our code and data are available at
https://github.com/zhufq00/EventSkeletonGeneration.",None,-1
3091ad21-879f-46e3-bd7c-6543db273bb8,Company2Vec -- German Company Embeddings based on Corporate Websites,0.079021,2,"With Company2Vec, the paper proposes a novel application in representation
learning. The model analyzes business activities from unstructured company
website data using Word2Vec and dimensionality reduction. Company2Vec maintains
semantic language structures and thus creates efficient company embeddings in
fine-granular industries. These semantic embeddings can be used for various
applications in banking. Direct relations between companies and words allow
semantic business analytics (e.g. top-n words for a company). Furthermore,
industry prediction is presented as a supervised learning application and
evaluation method. The vectorized structure of the embeddings allows measuring
companies similarities with the cosine distance. Company2Vec hence offers a
more fine-grained comparison of companies than the standard industry labels
(NACE). This property is relevant for unsupervised learning tasks, such as
clustering. An alternative industry segmentation is shown with k-means
clustering on the company embeddings. Finally, this paper proposes three
algorithms for (1) firm-centric, (2) industry-centric and (3) portfolio-centric
peer-firm identification.",None,-1
525c2a0f-eb20-4f9c-a82a-4a287b626dbe,What A Situated Language-Using Agent Must be Able to Do: A Top-Down Analysis,0.360856,7,"Even in our increasingly text-intensive times, the primary site of language
use is situated, co-present interaction. It is primary ontogenetically and
phylogenetically, and it is arguably also still primary in negotiating everyday
social situations. Situated interaction is also the final frontier of Natural
Language Processing, where, compared to the area of text processing, very
little progress has been made in the past decade, and where a myriad of
practical applications is waiting to be unlocked. While the usual approach in
the field is to reach, bottom-up, for the ever next ""adjacent possible"", in
this paper I attempt a top-down analysis of what the demands are that
unrestricted situated interaction makes on the participating agent, and suggest
ways in which this analysis can structure computational models and research on
them. Specifically, I discuss representational demands (the building up and
application of world model, language model, situation model, discourse model,
and agent model) and what I call anchoring processes (incremental processing,
incremental learning, conversational grounding, multimodal grounding) that bind
the agent to the here, now, and us.",None,-1
27fd49b6-5605-40c4-af41-8581ca30ebe5,HunSum-1: an Abstractive Summarization Dataset for Hungarian,0.0839327,1,"We introduce HunSum-1: a dataset for Hungarian abstractive summarization,
consisting of 1.14M news articles. The dataset is built by collecting, cleaning
and deduplicating data from 9 major Hungarian news sites through CommonCrawl.
Using this dataset, we build abstractive summarizer models based on huBERT and
mT5. We demonstrate the value of the created dataset by performing a
quantitative and qualitative analysis on the models' results. The HunSum-1
dataset, all models used in our experiments and our code are available open
source.",None,-1
3a5dc4ca-ff8d-4536-aac3-961da1566238,Evaluating GPT's Programming Capability through CodeWars' Katas,0.0901561,2,"In the burgeoning field of artificial intelligence (AI), understanding the
capabilities and limitations of programming-oriented models is crucial. This
paper presents a novel evaluation of the programming proficiency of Generative
Pretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against
coding problems of varying difficulty levels drawn from Codewars. The
experiments reveal a distinct boundary at the 3kyu level, beyond which these
GPT models struggle to provide solutions. These findings led to the proposal of
a measure for coding problem complexity that incorporates both problem
difficulty and the time required for solution. The research emphasizes the need
for validation and creative thinking capabilities in AI models to better
emulate human problem-solving techniques. Future work aims to refine this
proposed complexity measure, enhance AI models with these suggested
capabilities, and develop an objective measure for programming problem
difficulty. The results of this research offer invaluable insights for
improving AI programming capabilities and advancing the frontier of AI
problem-solving abilities.",None,-1
518e56c2-a5c4-4013-80a8-2f31bce667ec,Spatio-Temporal AU Relational Graph Representation Learning For Facial Action Units Detection,0.933374,9,"This paper presents our Facial Action Units (AUs) detection submission to the
fifth Affective Behavior Analysis in-the-wild Competition (ABAW). Our approach
consists of three main modules: (i) a pre-trained facial representation encoder
which produce a strong facial representation from each input face image in the
input sequence; (ii) an AU-specific feature generator that specifically learns
a set of AU features from each facial representation; and (iii) a
spatio-temporal graph learning module that constructs a spatio-temporal graph
representation. This graph representation describes AUs contained in all frames
and predicts the occurrence of each AU based on both the modeled spatial
information within the corresponding face and the learned temporal dynamics
among frames. The experimental results show that our approach outperformed the
baseline and the spatio-temporal graph representation learning allows our model
to generate the best results among all ablated systems. Our model ranks at the
4th place in the AU recognition track at the 5th ABAW Competition. Our code is
publicly available at https://github.com/wzh125/ABAW-5.",None,-1
956f5dab-2da0-4722-b8b4-dc61859948ed,Moral Uncertainty and the Problem of Fanaticism,0.163057,1,"While there is universal agreement that agents ought to act ethically, there
is no agreement as to what constitutes ethical behaviour. To address this
problem, recent philosophical approaches to `moral uncertainty' propose
aggregation of multiple ethical theories to guide agent behaviour. However, one
of the foundational proposals for aggregation - Maximising Expected
Choiceworthiness (MEC) - has been criticised as being vulnerable to fanaticism;
the problem of an ethical theory dominating agent behaviour despite low
credence (confidence) in said theory. Fanaticism thus undermines the
`democratic' motivation for accommodating multiple ethical perspectives. The
problem of fanaticism has not yet been mathematically defined. Representing
moral uncertainty as an instance of social welfare aggregation, this paper
contributes to the field of moral uncertainty by 1) formalising the problem of
fanaticism as a property of social welfare functionals and 2) providing
non-fanatical alternatives to MEC, i.e. Highest k-trimmed Mean and Highest
Median.",None,-1
5e6c34cd-9b06-452f-9566-1389fc70ec4c,Automatic Answerability Evaluation for Question Generation,0.126658,2,"Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed
for natural language generation (NLG) tasks, are based on measuring the n-gram
overlap between the generated and reference text. These simple metrics may be
insufficient for more complex tasks, such as question generation (QG), which
requires generating questions that are answerable by the reference answers.
Developing a more sophisticated automatic evaluation metric, thus, remains an
urgent problem in QG research. This work proposes PMAN (Prompting-based Metric
on ANswerability), a novel automatic evaluation metric to assess whether the
generated questions are answerable by the reference answers for the QG tasks.
Extensive experiments demonstrate that its evaluation results are reliable and
align with human evaluations. We further apply our metric to evaluate the
performance of QG models, which shows that our metric complements conventional
metrics. Our implementation of a GPT-based QG model achieves state-of-the-art
performance in generating answerable questions.",None,-1
88c1eba6-008c-4a4e-b0a5-5eda92bd722d,Text-Conditional Contextualized Avatars For Zero-Shot Personalization,0.0743544,3,"Recent large-scale text-to-image generation models have made significant
improvements in the quality, realism, and diversity of the synthesized images
and enable users to control the created content through language. However, the
personalization aspect of these generative models is still challenging and
under-explored. In this work, we propose a pipeline that enables
personalization of image generation with avatars capturing a user's identity in
a delightful way. Our pipeline is zero-shot, avatar texture and style agnostic,
and does not require training on the avatar at all - it is scalable to millions
of users who can generate a scene with their avatar. To render the avatar in a
pose faithful to the given text prompt, we propose a novel text-to-3D pose
diffusion model trained on a curated large-scale dataset of in-the-wild human
poses improving the performance of the SOTA text-to-motion models
significantly. We show, for the first time, how to leverage large-scale image
datasets to learn human 3D pose parameters and overcome the limitations of
motion capture datasets.",None,-1
5ed76f81-812c-47c6-9b9c-81913a362f77,DiffProtect: Generate Adversarial Examples with Diffusion Models for Facial Privacy Protection,0.770375,9,"The increasingly pervasive facial recognition (FR) systems raise serious
concerns about personal privacy, especially for billions of users who have
publicly shared their photos on social media. Several attempts have been made
to protect individuals from being identified by unauthorized FR systems
utilizing adversarial attacks to generate encrypted face images. However,
existing methods suffer from poor visual quality or low attack success rates,
which limit their utility. Recently, diffusion models have achieved tremendous
success in image generation. In this work, we ask: can diffusion models be used
to generate adversarial examples to improve both visual quality and attack
performance? We propose DiffProtect, which utilizes a diffusion autoencoder to
generate semantically meaningful perturbations on FR systems. Extensive
experiments demonstrate that DiffProtect produces more natural-looking
encrypted images than state-of-the-art methods while achieving significantly
higher attack success rates, e.g., 24.5% and 25.1% absolute improvements on the
CelebA-HQ and FFHQ datasets.",None,-1
2f3ed641-9132-4c0c-840f-00a5ec0887fc,Is Knowledge All Large Language Models Needed for Causal Reasoning?,0.182083,4,"This paper explores the causal reasoning of large language models (LLMs) to
enhance their interpretability and reliability in advancing artificial
intelligence. Despite the proficiency of LLMs in a range of tasks, their
potential for understanding causality requires further exploration. We propose
a novel causal attribution model that utilizes ``do-operators"" for constructing
counterfactual scenarios, allowing us to systematically quantify the influence
of input numerical data and LLMs' pre-existing knowledge on their causal
reasoning processes. Our newly developed experimental setup assesses LLMs'
reliance on contextual information and inherent knowledge across various
domains. Our evaluation reveals that LLMs' causal reasoning ability mainly
depends on the context and domain-specific knowledge provided. In the absence
of such knowledge, LLMs can still maintain a degree of causal reasoning using
the available numerical data, albeit with limitations in the calculations. This
motivates the proposed fine-tuned LLM for pairwise causal discovery,
effectively leveraging both knowledge and numerical information.",None,-1
3d08c173-7c43-49fa-9f7d-3e3e53ad6ac9,SHAP-IQ: Unified Approximation of any-order Shapley Interactions,0.379623,9,"Predominately in explainable artificial intelligence (XAI) research, the
Shapley value (SV) is applied to determine feature attributions for any black
box model. Shapley interaction indices extend the SV to define any-order
feature interactions. Defining a unique Shapley interaction index is an open
research question and, so far, three definitions have been proposed, which
differ by their choice of axioms. Moreover, each definition requires a specific
approximation technique. Here, we propose SHAPley Interaction Quantification
(SHAP-IQ), an efficient sampling-based approximator to compute Shapley
interactions for arbitrary cardinal interaction indices (CII), i.e. interaction
indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based
on a novel representation and, in contrast to existing methods, we provide
theoretical guarantees for its approximation quality, as well as estimates for
the variance of the point estimates. For the special case of SV, our approach
reveals a novel representation of the SV and corresponds to Unbiased KernelSHAP
with a greatly simplified calculation. We illustrate the computational
efficiency and effectiveness by explaining language, image classification and
high-dimensional synthetic models.",None,-1
962494c6-26e6-40e1-aebb-ad3add97cdd8,Are Language Models Worse than Humans at Following Prompts? It's Complicated,0.0628261,11,"Prompts have been the center of progress in advancing language models'
zero-shot and few-shot performance. However, recent work finds that models can
perform surprisingly well when given intentionally irrelevant or misleading
prompts. Such results may be interpreted as evidence that model behavior is not
""human like"". In this study, we challenge a central assumption in such work:
that humans would perform badly when given pathological instructions. We find
that humans are able to reliably ignore irrelevant instructions and thus, like
models, perform well on the underlying task despite an apparent lack of signal
regarding the task they are being asked to do. However, when given deliberately
misleading instructions, humans follow the instructions faithfully, whereas
models do not. Our findings caution that future research should not idealize
human behaviors as a monolith and should not train or evaluate models to mimic
assumptions about these behaviors without first validating humans' behaviors
empirically.",None,-1
9486623a-317c-465e-86a1-4b614d467929,Anthropomorphization of AI: Opportunities and Risks,0.899339,7,"Anthropomorphization is the tendency to attribute human-like traits to
non-human entities. It is prevalent in many social contexts -- children
anthropomorphize toys, adults do so with brands, and it is a literary device.
It is also a versatile tool in science, with behavioral psychology and
evolutionary biology meticulously documenting its consequences. With widespread
adoption of AI systems, and the push from stakeholders to make it human-like
through alignment techniques, human voice, and pictorial avatars, the tendency
for users to anthropomorphize it increases significantly. We take a dyadic
approach to understanding this phenomenon with large language models (LLMs) by
studying (1) the objective legal implications, as analyzed through the lens of
the recent blueprint of AI bill of rights and the (2) subtle psychological
aspects customization and anthropomorphization. We find that anthropomorphized
LLMs customized for different user bases violate multiple provisions in the
legislative blueprint. In addition, we point out that anthropomorphization of
LLMs affects the influence they can have on their users, thus having the
potential to fundamentally change the nature of human-AI interaction, with
potential for manipulation and negative influence. With LLMs being
hyper-personalized for vulnerable groups like children and patients among
others, our work is a timely and important contribution. We propose a
conservative strategy for the cautious use of anthropomorphization to improve
trustworthiness of AI systems.",None,-1
d6d672f7-38ab-4ee3-aab4-2b40a8a435b2,Creating Large Language Model Resistant Exams: Guidelines and Strategies,0.0216847,3,"The proliferation of Large Language Models (LLMs), such as ChatGPT, has
raised concerns about their potential impact on academic integrity, prompting
the need for LLM-resistant exam designs. This article investigates the
performance of LLMs on exams and their implications for assessment, focusing on
ChatGPT's abilities and limitations. We propose guidelines for creating
LLM-resistant exams, including content moderation, deliberate inaccuracies,
real-world scenarios beyond the model's knowledge base, effective distractor
options, evaluating soft skills, and incorporating non-textual information. The
article also highlights the significance of adapting assessments to modern
tools and promoting essential skills development in students. By adopting these
strategies, educators can maintain academic integrity while ensuring that
assessments accurately reflect contemporary professional settings and address
the challenges and opportunities posed by artificial intelligence in education.",None,-1
0e3a5f25-69c7-4f8d-ae8e-c7ff5c4bb31f,"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",0.621143,17,"Human feedback is increasingly used to steer the behaviours of Large Language
Models (LLMs). However, it is unclear how to collect and incorporate feedback
in a way that is efficient, effective and unbiased, especially for highly
subjective human preferences and values. In this paper, we survey existing
approaches for learning from human feedback, drawing on 95 papers primarily
from the ACL and arXiv repositories.First, we summarise the past, pre-LLM
trends for integrating human feedback into language models. Second, we give an
overview of present techniques and practices, as well as the motivations for
using feedback; conceptual frameworks for defining values and preferences; and
how feedback is collected and from whom. Finally, we encourage a better future
of feedback learning in LLMs by raising five unresolved conceptual and
practical challenges.",None,-1
f86d11ad-7121-4535-8b55-0817b9ef05cd,High-fidelity Interpretable Inverse Rig: An Accurate and Sparse Solution Optimizing the Quartic Blendshape Model,0.485557,3,"We propose a method to fit arbitrarily accurate blendshape rig models by
solving the inverse rig problem in realistic human face animation. The method
considers blendshape models with different levels of added corrections and
solves the regularized least-squares problem using coordinate descent, i.e.,
iteratively estimating blendshape weights. Besides making the optimization
easier to solve, this approach ensures that mutually exclusive controllers will
not be activated simultaneously and improves the goodness of fit after each
iteration. We show experimentally that the proposed method yields solutions
with mesh error comparable to or lower than the state-of-the-art approaches
while significantly reducing the cardinality of the weight vector (over 20
percent), hence giving a high-fidelity reconstruction of the reference
expression that is easier to manipulate in the post-production manually. Python
scripts for the algorithm will be publicly available upon acceptance of the
paper.",None,-1
9745b14b-17a6-4e46-980f-5e55b39899a9,AutoRecon: Automated 3D Object Discovery and Reconstruction,0.920092,7,"A fully automated object reconstruction pipeline is crucial for digital
content creation. While the area of 3D reconstruction has witnessed profound
developments, the removal of background to obtain a clean object model still
relies on different forms of manual labor, such as bounding box labeling, mask
annotations, and mesh manipulations. In this paper, we propose a novel
framework named AutoRecon for the automated discovery and reconstruction of an
object from multi-view images. We demonstrate that foreground objects can be
robustly located and segmented from SfM point clouds by leveraging
self-supervised 2D vision transformer features. Then, we reconstruct decomposed
neural scene representations with dense supervision provided by the decomposed
point clouds, resulting in accurate object reconstruction and segmentation.
Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the
effectiveness and robustness of AutoRecon.",None,-1
73759fe6-3afa-4738-bcb1-1d508b01db10,Accelerating Diffusion Models for Inverse Problems through Shortcut Sampling,0.145055,9,"Diffusion models have recently demonstrated an impressive ability to address
inverse problems in an unsupervised manner. While existing methods primarily
focus on modifying the posterior sampling process, the potential of the forward
process remains largely unexplored. In this work, we propose Shortcut Sampling
for Diffusion(SSD), a novel approach for solving inverse problems in a
zero-shot manner. Instead of initiating from random noise, the core concept of
SSD is to find a specific transitional state that bridges the measurement image
y and the restored image x. By utilizing the shortcut path of ""input -
transitional state - output"", SSD can achieve precise restoration with fewer
steps. To derive the transitional state during the forward process, we
introduce Distortion Adaptive Inversion. Moreover, we apply back projection as
additional consistency constraints during the generation process.
Experimentally, we demonstrate SSD's effectiveness on multiple representative
IR tasks. Our method achieves competitive results with only 30 NFEs compared to
state-of-the-art zero-shot methods(100 NFEs) and outperforms them with 100 NFEs
in certain tasks. Code is available at https://github.com/GongyeLiu/SSD",None,-1
40c67412-1733-4e06-98e0-253d02bdc241,"Neural Wavelet-domain Diffusion for 3D Shape Generation, Inversion, and Manipulation",0.236779,7,"This paper presents a new approach for 3D shape generation, inversion, and
manipulation, through a direct generative modeling on a continuous implicit
representation in wavelet domain. Specifically, we propose a compact wavelet
representation with a pair of coarse and detail coefficient volumes to
implicitly represent 3D shapes via truncated signed distance functions and
multi-scale biorthogonal wavelets. Then, we design a pair of neural networks: a
diffusion-based generator to produce diverse shapes in the form of the coarse
coefficient volumes and a detail predictor to produce compatible detail
coefficient volumes for introducing fine structures and details. Further, we
may jointly train an encoder network to learn a latent space for inverting
shapes, allowing us to enable a rich variety of whole-shape and region-aware
shape manipulations. Both quantitative and qualitative experimental results
manifest the compelling shape generation, inversion, and manipulation
capabilities of our approach over the state-of-the-art methods.",None,-1
6014002f-4dbf-47c1-883a-13f9b7c7ccaf,Zero-touch realization of Pervasive Artificial Intelligence-as-a-service in 6G networks,0.813266,13,"The vision of the upcoming 6G technologies, characterized by ultra-dense
network, low latency, and fast data rate is to support Pervasive AI (PAI) using
zero-touch solutions enabling self-X (e.g., self-configuration,
self-monitoring, and self-healing) services. However, the research on 6G is
still in its infancy, and only the first steps have been taken to conceptualize
its design, investigate its implementation, and plan for use cases. Toward this
end, academia and industry communities have gradually shifted from theoretical
studies of AI distribution to real-world deployment and standardization. Still,
designing an end-to-end framework that systematizes the AI distribution by
allowing easier access to the service using a third-party application assisted
by a zero-touch service provisioning has not been well explored. In this
context, we introduce a novel platform architecture to deploy a zero-touch
PAI-as-a-Service (PAIaaS) in 6G networks supported by a blockchain-based smart
system. This platform aims to standardize the pervasive AI at all levels of the
architecture and unify the interfaces in order to facilitate the service
deployment across application and infrastructure domains, relieve the users
worries about cost, security, and resource allocation, and at the same time,
respect the 6G stringent performance requirements. As a proof of concept, we
present a Federated Learning-as-a-service use case where we evaluate the
ability of our proposed system to self-optimize and self-adapt to the dynamics
of 6G networks in addition to minimizing the users' perceived costs.",None,-1
5b20edc4-85de-45b5-9869-4e40f431c019,ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation,0.360195,11,"This paper presents a new mechanism to facilitate the training of mask
transformers for efficient panoptic segmentation, democratizing its deployment.
We observe that due to its high complexity, the training objective of panoptic
segmentation will inevitably lead to much higher false positive penalization.
Such unbalanced loss makes the training process of the end-to-end
mask-transformer based architectures difficult, especially for efficient
models. In this paper, we present ReMaX that adds relaxation to mask
predictions and class predictions during training for panoptic segmentation. We
demonstrate that via these simple relaxation techniques during training, our
model can be consistently improved by a clear margin \textbf{without} any extra
computational cost on inference. By combining our method with efficient
backbones like MobileNetV3-Small, our method achieves new state-of-the-art
results for efficient panoptic segmentation on COCO, ADE20K and Cityscapes.
Code and pre-trained checkpoints will be available at
\url{https://github.com/google-research/deeplab2}.",None,-1
daf17e00-a7dc-4db4-a39c-60032c5a47c6,DANES: Deep Neural Network Ensemble Architecture for Social and Textual Context-aware Fake News Detection,0.567826,6,"The growing popularity of social media platforms has simplified the creation
and distribution of news articles but also creates a conduit for spreading fake
news. In consequence, the need arises for effective context-aware fake news
detection mechanisms, where the contextual information can be built either from
the textual content of posts or from available social data (e.g., information
about the users, reactions to posts, or the social network). In this paper, we
propose DANES, a Deep Neural Network Ensemble Architecture for Social and
Textual Context-aware Fake News Detection. DANES comprises a Text Branch for a
textual content-based context and a Social Branch for the social context. These
two branches are used to create a novel Network Embedding. Preliminary ablation
results on 3 real-world datasets, i.e., BuzzFace, Twitter15, and Twitter16, are
promising, with an accuracy that outperforms state-of-the-art solutions when
employing both social and textual content features.",None,-1
e61b86f8-6f5e-4e3e-9e7e-7df1a4dea82d,What makes a good data augmentation for few-shot unsupervised image anomaly detection?,0.559202,4,"Data augmentation is a promising technique for unsupervised anomaly detection
in industrial applications, where the availability of positive samples is often
limited due to factors such as commercial competition and sample collection
difficulties. In this paper, how to effectively select and apply data
augmentation methods for unsupervised anomaly detection is studied. The impact
of various data augmentation methods on different anomaly detection algorithms
is systematically investigated through experiments. The experimental results
show that the performance of different industrial image anomaly detection
(termed as IAD) algorithms is not significantly affected by the specific data
augmentation method employed and that combining multiple data augmentation
methods does not necessarily yield further improvements in the accuracy of
anomaly detection, although it can achieve excellent results on specific
methods. These findings provide useful guidance on selecting appropriate data
augmentation methods for different requirements in IAD.",None,-1
67e5ce40-096a-4625-9d62-5e2d4fb78744,Adaptive Policy with Wait-$k$ Model for Simultaneous Translation,0.314506,5,"Simultaneous machine translation (SiMT) requires a robust read/write policy
in conjunction with a high-quality translation model. Traditional methods rely
on either a fixed wait-$k$ policy coupled with a standalone wait-$k$
translation model, or an adaptive policy jointly trained with the translation
model. In this study, we propose a more flexible approach by decoupling the
adaptive policy model from the translation model. Our motivation stems from the
observation that a standalone multi-path wait-$k$ model performs competitively
with adaptive policies utilized in state-of-the-art SiMT approaches.
Specifically, we introduce DaP, a divergence-based adaptive policy, that makes
read/write decisions for any translation model based on the potential
divergence in translation distributions resulting from future information. DaP
extends a frozen wait-$k$ model with lightweight parameters, and is both memory
and computation efficient. Experimental results across various benchmarks
demonstrate that our approach offers an improved trade-off between translation
accuracy and latency, outperforming strong baselines.",None,-1
20743533-c35b-4e6b-b975-f61b9c07ea1a,Thompson Sampling with Diffusion Generative Prior,0.665959,5,"In this work, we initiate the idea of using denoising diffusion models to
learn priors for online decision making problems. Our special focus is on the
meta-learning for bandit framework, with the goal of learning a strategy that
performs well across bandit tasks of a same class. To this end, we train a
diffusion model that learns the underlying task distribution and combine
Thompson sampling with the learned prior to deal with new tasks at test time.
Our posterior sampling algorithm is designed to carefully balance between the
learned prior and the noisy observations that come from the learner's
interaction with the environment. To capture realistic bandit scenarios, we
also propose a novel diffusion model training procedure that trains even from
incomplete and/or noisy data, which could be of independent interest. Finally,
our extensive experimental evaluations clearly demonstrate the potential of the
proposed approach.",None,-1
ad9bdbab-d6bd-494f-8aae-f7932864e10b,Towards Edge-Cloud Architectures for Personal Protective Equipment Detection,0.282103,1,"Detecting Personal Protective Equipment in images and video streams is a
relevant problem in ensuring the safety of construction workers. In this
contribution, an architecture enabling live image recognition of such equipment
is proposed. The solution is deployable in two settings -- edge-cloud and
edge-only. The system was tested on an active construction site, as a part of a
larger scenario, within the scope of the ASSIST-IoT H2020 project. To determine
the feasibility of the edge-only variant, a model for counting people wearing
safety helmets was developed using the YOLOX method. It was found that an
edge-only deployment is possible for this use case, given the hardware
infrastructure available on site. In the preliminary evaluation, several
important observations were made, that are crucial to the further development
and deployment of the system. Future work will include an in-depth
investigation of performance aspects of the two architecture variants.",None,-1
2a0585fb-936e-4255-9296-c971d7807052,Pushing the Limits of Machine Design: Automated CPU Design with AI,0.3609,3,"Design activity -- constructing an artifact description satisfying given
goals and constraints -- distinguishes humanity from other animals and
traditional machines, and endowing machines with design abilities at the human
level or beyond has been a long-term pursuit. Though machines have already
demonstrated their abilities in designing new materials, proteins, and computer
programs with advanced artificial intelligence (AI) techniques, the search
space for designing such objects is relatively small, and thus, ""Can machines
design like humans?"" remains an open question. To explore the boundary of
machine design, here we present a new AI approach to automatically design a
central processing unit (CPU), the brain of a computer, and one of the world's
most intricate devices humanity have ever designed. This approach generates the
circuit logic, which is represented by a graph structure called Binary
Speculation Diagram (BSD), of the CPU design from only external input-output
observations instead of formal program code. During the generation of BSD,
Monte Carlo-based expansion and the distance of Boolean functions are used to
guarantee accuracy and efficiency, respectively. By efficiently exploring a
search space of unprecedented size 10^{10^{540}}, which is the largest one of
all machine-designed objects to our best knowledge, and thus pushing the limits
of machine design, our approach generates an industrial-scale RISC-V CPU within
only 5 hours. The taped-out CPU successfully runs the Linux operating system
and performs comparably against the human-designed Intel 80486SX CPU. In
addition to learning the world's first CPU only from input-output observations,
which may reform the semiconductor industry by significantly reducing the
design cycle, our approach even autonomously discovers human knowledge of the
von Neumann architecture.",None,-1
b030573c-a36d-4b77-a1b8-24dc6ccd6e7b,Joint Problems in Learning Multiple Dynamical Systems,0.120168,1,"Clustering of time series is a well-studied problem, with applications
ranging from quantitative, personalized models of metabolism obtained from
metabolite concentrations to state discrimination in quantum information
theory. We consider a variant, where given a set of trajectories and a number
of parts, we jointly partition the set of trajectories and learn linear
dynamical system (LDS) models for each part, so as to minimize the maximum
error across all the models. We present globally convergent methods and EM
heuristics, accompanied by promising computational results.",None,-1
17b257a2-83d0-42a0-b732-67e5dfcc99c9,API-Assisted Code Generation for Question Answering on Varied Table Structures,0.223073,6,"A persistent challenge to table question answering (TableQA) by generating
executable programs has been adapting to varied table structures, typically
requiring domain-specific logical forms. In response, this paper introduces a
unified TableQA framework that: (1) provides a unified representation for
structured tables as multi-index Pandas data frames, (2) uses Python as a
powerful querying language, and (3) uses few-shot prompting to translate NL
questions into Python programs, which are executable on Pandas data frames.
Furthermore, to answer complex relational questions with extended program
functionality and external knowledge, our framework allows customized APIs that
Python programs can call. We experiment with four TableQA datasets that involve
tables of different structures -- relational, multi-table, and hierarchical
matrix shapes -- and achieve prominent improvements over past state-of-the-art
systems. In ablation studies, we (1) show benefits from our multi-index
representation and APIs over baselines that use only an LLM, and (2)
demonstrate that our approach is modular and can incorporate additional APIs.",None,-1
0aa22d87-268e-4a70-b257-cf25e4317362,Cracking the Code of Negative Transfer: A Cooperative Game Theoretic Approach for Cross-Domain Sequential Recommendation,0.521157,2,"This paper investigates Cross-Domain Sequential Recommendation (CDSR), a
promising method that uses information from multiple domains (more than three)
to generate accurate and diverse recommendations, and takes into account the
sequential nature of user interactions. The effectiveness of these systems
often depends on the complex interplay among the multiple domains. In this
dynamic landscape, the problem of negative transfer arises, where heterogeneous
knowledge between dissimilar domains leads to performance degradation due to
differences in user preferences across these domains. As a remedy, we propose a
new CDSR framework that addresses the problem of negative transfer by assessing
the extent of negative transfer from one domain to another and adaptively
assigning low weight values to the corresponding prediction losses. To this
end, the amount of negative transfer is estimated by measuring the marginal
contribution of each domain to model performance based on a cooperative game
theory. In addition, a hierarchical contrastive learning approach that
incorporates information from the sequence of coarse-level categories into that
of fine-level categories (e.g., item level) when implementing contrastive
learning was developed to mitigate negative transfer. Despite the potentially
low relevance between domains at the fine-level, there may be higher relevance
at the category level due to its generalised and broader preferences. We show
that our model is superior to prior works in terms of model performance on two
real-world datasets across ten different domains.",None,-1
33409e22-55cb-4804-b2a1-4bd3277d6327,Multi-modal Facial Action Unit Detection with Large Pre-trained Models for the 5th Competition on Affective Behavior Analysis in-the-wild,0.933374,9,"Facial action unit detection has emerged as an important task within facial
expression analysis, aimed at detecting specific pre-defined, objective facial
expressions, such as lip tightening and cheek raising. This paper presents our
submission to the Affective Behavior Analysis in-the-wild (ABAW) 2023
Competition for AU detection. We propose a multi-modal method for facial action
unit detection with visual, acoustic, and lexical features extracted from the
large pre-trained models. To provide high-quality details for visual feature
extraction, we apply super-resolution and face alignment to the training data
and show potential performance gain. Our approach achieves the F1 score of
52.3% on the official validation set of the 5th ABAW Challenge.",None,-1
c664c994-cd7b-46d2-a966-079f4121c793,Weighted First Order Model Counting with Directed Acyclic Graph Axioms,0.142891,2,"Statistical Relational Learning (SRL) integrates First-Order Logic (FOL) and
probability theory for learning and inference over relational data.
Probabilistic inference and learning in many SRL models can be reduced to
Weighted First Order Model Counting (WFOMC). However, WFOMC is known to be
intractable ($\mathrm{\#P_1-}$ complete). Hence, logical fragments that admit
polynomial time WFOMC are of significant interest. Such fragments are called
domain liftable. Recent line of works have shown the two-variable fragment of
FOL, extended with counting quantifiers ($\mathrm{C^2}$) to be domain-liftable.
However, many properties of real-world data can not be modelled in
$\mathrm{C^2}$. In fact many ubiquitous properties of real-world data are
inexressible in FOL. Acyclicity is one such property, found in citation
networks, genealogy data, temporal data e.t.c. In this paper we aim to address
this problem by investigating the domain liftability of directed acyclicity
constraints. We show that the fragment $\mathrm{C^2}$ with a Directed Acyclic
Graph (DAG) axiom, i.e., a predicate in the language is axiomatized to
represent a DAG, is domain-liftable. We present a method based on principle of
inclusion-exclusion for WFOMC of $\mathrm{C^2}$ formulas extended with DAG
axioms.",None,-1
43be8dab-88e1-4ca0-9b9a-562d551137b5,Real-time Object Detection: YOLOv1 Re-Implementation in PyTorch,0.042681,1,"Real-time object detection is a crucial problem to solve when in comes to
computer vision systems that needs to make appropriate decision based on
detection in a timely manner. I have chosen the YOLO v1 architecture to
implement it using PyTorch framework, with goal to familiarize with entire
object detection pipeline I attempted different techniques to modify the
original architecture to improve the results. Finally, I compare the metrics of
my implementation to the original.",None,-1
e7036f6f-fd17-4616-a987-5a9834ccded1,Do large language models solve verbal analogies like children do?,0.585665,2,"Analogy-making lies at the heart of human cognition. Adults solve analogies
such as \textit{Horse belongs to stable like chicken belongs to ...?} by
mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In
contrast, children often use association, e.g., answering \textit{egg}. This
paper investigates whether large language models (LLMs) solve verbal analogies
in A:B::C:? form using associations, similar to what children do. We use verbal
analogies extracted from an online adaptive learning environment, where 14,002
7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six
tested Dutch monolingual and multilingual LLMs performed around the same level
as children, with MGPT performing worst, around the 7-year-old level, and XLM-V
and GPT-3 the best, slightly above the 11-year-old level. However, when we
control for associative processes this picture changes and each model's
performance level drops 1-2 years. Further experiments demonstrate that
associative processes often underlie correctly solved analogies. We conclude
that the LLMs we tested indeed tend to solve verbal analogies by association
with C like children do.",None,-1
ed7e9af1-5cad-4ddf-b6e9-60d653491bb7,Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models,0.108623,5,"There have been wide spread claims in the literature about the emergent
reasoning capabilities of Pretrained Large Language Models. However, recent
studies, have found that their ability to plan remains questionable. Through
our experiments using GPT-2, we empirically demonstrate that the performance of
a finetuned baseline remains poor because it violates pre-conditions of actions
in the plans that it generates. To improve the planning capabilities of a
finetuned LLM, we train a verifier, which can classify actions as being valid
or invalid in a particular state. By randomly sampling actions from the same
dataset, we generate examples of invalid actions which are then used to train a
verifier which can check for action applicability. In the presence of diverse
sampling from a generator and a verifier which can prune invalid trajectories,
we show significant gains in the success rate on the Blocksworld domain.
Additionally, we show that finetuning the GPT-2 generator itself to create the
verifier generalizes better than finetuning the base GPT-2. Lastly, we
investigate the role of the sampling temperature which can be used to control
the exploration-exploitation tradeoff.",None,-1
25abc3ad-7c7b-45ca-886a-012fb0c8c239,Forgetting-aware Linear Bias for Attentive Knowledge Tracing,0.439188,2,"Knowledge Tracing (KT) aims to track proficiency based on a question-solving
history, allowing us to offer a streamlined curriculum. Recent studies actively
utilize attention-based mechanisms to capture the correlation between questions
and combine it with the learner's characteristics for responses. However, our
empirical study shows that existing attention-based KT models neglect the
learner's forgetting behavior, especially as the interaction history becomes
longer. This problem arises from the bias that overprioritizes the correlation
of questions while inadvertently ignoring the impact of forgetting behavior.
This paper proposes a simple-yet-effective solution, namely Forgetting-aware
Linear Bias (FoLiBi), to reflect forgetting behavior as a linear bias. Despite
its simplicity, FoLiBi is readily equipped with existing attentive KT models by
effectively decomposing question correlations with forgetting behavior. FoLiBi
plugged with several KT models yields a consistent improvement of up to 2.58%
in AUC over state-of-the-art KT models on four benchmark datasets.",None,-1
cbe5c563-5e5f-4c27-b155-4d74d33a5848,System 2 Attention (is something you might need too),0.710487,27,"Soft attention in Transformer-based Large Language Models (LLMs) is
susceptible to incorporating irrelevant information from the context into its
latent representations, which adversely affects next token generations. To help
rectify these issues, we introduce System 2 Attention (S2A), which leverages
the ability of LLMs to reason in natural language and follow instructions in
order to decide what to attend to. S2A regenerates the input context to only
include the relevant portions, before attending to the regenerated context to
elicit the final response. In experiments, S2A outperforms standard
attention-based LLMs on three tasks containing opinion or irrelevant
information, QA, math word problems and longform generation, where S2A
increases factuality and objectivity, and decreases sycophancy.",None,-1
35be039a-2560-4016-a2a6-09e9a14b3855,Can Contextual Biasing Remain Effective with Whisper and GPT-2?,0.848173,10,"End-to-end automatic speech recognition (ASR) and large language models, such
as Whisper and GPT-2, have recently been scaled to use vast amounts of training
data. Despite the large amount of training data, infrequent content words that
occur in a particular task may still exhibit poor ASR performance, with
contextual biasing a possible remedy. This paper investigates the effectiveness
of neural contextual biasing for Whisper combined with GPT-2. Specifically,
this paper proposes integrating an adapted tree-constrained pointer generator
(TCPGen) component for Whisper and a dedicated training scheme to dynamically
adjust the final output without modifying any Whisper model parameters.
Experiments across three datasets show a considerable reduction in errors on
biasing words with a biasing list of 1000 words. Contextual biasing was more
effective when applied to domain-specific data and can boost the performance of
Whisper and GPT-2 without losing their generality.",None,-1
1ed66e8a-919d-4329-8e1c-6dcf27b21c11,Empower Your Model with Longer and Better Context Comprehension,0.0167347,2,"Recently, with the emergence of numerous Large Language Models (LLMs), the
implementation of AI has entered a new era. Irrespective of these models' own
capacity and structure, there is a growing demand for LLMs to possess enhanced
comprehension of longer and more complex contexts with relatively smaller
sizes. Models often encounter an upper limit when processing sequences of
sentences that extend beyond their comprehension capacity and result in
off-topic or even chaotic responses. While several recent works attempt to
address this issue in various ways, they rarely focus on ""why models are unable
to compensate or strengthen their capabilities on their own"". In this paper, we
thoroughly investigate the nature of information transfer within LLMs and
propose a novel technique called Attention Transition. This technique empowers
models to achieve longer and better context comprehension with minimal
additional training or impact on generation fluency. Our experiments are
conducted on the challenging XSum dataset using LLaMa-7b model with context
token length ranging from 800 to 1900. Results demonstrate that we achieve
substantial improvements compared with the original generation results
evaluated by GPT4.",None,-1
4a3bd71a-d941-4c44-bf1c-12b7069e7b75,State Spaces Aren't Enough: Machine Translation Needs Attention,0.293608,4,"Structured State Spaces for Sequences (S4) is a recently proposed sequence
model with successful applications in various tasks, e.g. vision, language
modeling, and audio. Thanks to its mathematical formulation, it compresses its
input to a single hidden state, and is able to capture long range dependencies
while avoiding the need for an attention mechanism. In this work, we apply S4
to Machine Translation (MT), and evaluate several encoder-decoder variants on
WMT'14 and WMT'16. In contrast with the success in language modeling, we find
that S4 lags behind the Transformer by approximately 4 BLEU points, and that it
counter-intuitively struggles with long sentences. Finally, we show that this
gap is caused by S4's inability to summarize the full source sentence in a
single hidden state, and show that we can close the gap by introducing an
attention mechanism.",None,-1
7d949707-aafc-438d-abce-f9e60cfa1639,shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation,0.628541,9,"Instruction-tuned generative Large language models (LLMs) like ChatGPT and
Bloomz possess excellent generalization abilities, but they face limitations in
understanding radiology reports, particularly in the task of generating the
IMPRESSIONS section from the FINDINGS section. They tend to generate either
verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to
medical text data during training. We present a system which leverages
large-scale medical text data for domain-adaptive pre-training of
instruction-tuned LLMs to enhance its medical knowledge and performance on
specific medical tasks. We show that this system performs better in a zero-shot
setting than a number of pretrain-and-finetune adaptation methods on the
IMPRESSIONS generation task, and ranks 1st among participating systems in Task
1B: Radiology Report Summarization at the BioNLP 2023 workshop.",None,-1
67281b7a-f728-4d79-9114-04a9e266f246,Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics,0.183641,2,"Automatic assessment of reading fluency using automatic speech recognition
(ASR) holds great potential for early detection of reading difficulties and
subsequent timely intervention. Precise assessment tools are required,
especially for languages other than English. In this study, we evaluate six
state-of-the-art ASR-based systems for automatically assessing Dutch oral
reading accuracy using Kaldi and Whisper. Results show our most successful
system reached substantial agreement with human evaluations (MCC = .63). The
same system reached the highest correlation between forced decoding confidence
scores and word correctness (r = .45). This system's language model (LM)
consisted of manual orthographic transcriptions and reading prompts of the test
data, which shows that including reading errors in the LM improves assessment
performance. We discuss the implications for developing automatic assessment
systems and identify possible avenues of future research.",None,-1
a06499af-345f-4550-b7e4-d71bfa7f4a83,Adapting a Language Model While Preserving its General Knowledge,0.467276,14,"Domain-adaptive pre-training (or DA-training for short), also known as
post-training, aims to train a pre-trained general-purpose language model (LM)
using an unlabeled corpus of a particular domain to adapt the LM so that
end-tasks in the domain can give improved performances. However, existing
DA-training methods are in some sense blind as they do not explicitly identify
what knowledge in the LM should be preserved and what should be changed by the
domain corpus. This paper shows that the existing methods are suboptimal and
proposes a novel method to perform a more informed adaptation of the knowledge
in the LM by (1) soft-masking the attention heads based on their importance to
best preserve the general knowledge in the LM and (2) contrasting the
representations of the general and the full (both general and domain knowledge)
to learn an integrated representation with both general and domain-specific
knowledge. Experimental results will demonstrate the effectiveness of the
proposed approach.",None,-1
e5666935-f7c4-4acb-8181-2652d52e6ef0,A Global Transport Capacity Risk Prediction Method for Rail Transit Based on Gaussian Bayesian Network,0.583634,1,"Aiming at the prediction problem of transport capacity risk caused by the
mismatch between the carrying capacity of rail transit network and passenger
flow demand, this paper proposes an explainable prediction method of rail
transit network transport capacity risk based on linear Gaussian Bayesian
network. This method obtains the training data of the prediction model based on
the simulation model of the rail transit system with a three-layer structure
including rail transit network, train flow and passenger flow. A Bayesian
network structure construction method based on the topology of the rail transit
network is proposed, and the MLE (Maximum Likelihood Estimation) method is used
to realize the parameter learning of the Bayesian network. Finally, the
effectiveness of the proposed method is verified by simulation examples.",None,-1
8b01c0b7-46d3-475a-873a-ba599ab4a22b,QCQP-Tunneling: Ellipsoidal Constrained Agent Navigation,0.238172,3,"This paper presents a convex-QCQP based novel path planning algorithm named
ellipsoidal constrained agent navigation (ECAN), for a challenging problem of
online path planning in completely unknown and unseen continuous environments.
ECAN plans path for the agent by making a tunnel of overlapping ellipsoids, in
an online fashion, through the environment. Convex constraints in the
ellipsoid-formation step circumvent collision with the obstacles. The problem
of online-tunneling is solved as a convex-QCQP. This paper assumes no
constraints on shape of the agent and the obstacles. However, to make the
approach clearer, this paper first introduces the framework for a point-mass
agent with point-size obstacles. After explaining the underlying principle in
drawing an ellipsoid tunnel, the framework is extended to the agent and
obstacles having finite area (2d space) and finite-volume (3d-space).",None,-1
ded32d16-ea2b-48e0-bfbb-6b7920f134e7,End-to-End 3D Dense Captioning with Vote2Cap-DETR,0.948967,25,"3D dense captioning aims to generate multiple captions localized with their
associated object regions. Existing methods follow a sophisticated
``detect-then-describe'' pipeline equipped with numerous hand-crafted
components. However, these hand-crafted components would yield suboptimal
performance given cluttered object spatial and class distributions among
different scenes. In this paper, we propose a simple-yet-effective transformer
framework Vote2Cap-DETR based on recent popular \textbf{DE}tection
\textbf{TR}ansformer (DETR). Compared with prior arts, our framework has
several appealing advantages: 1) Without resorting to numerous hand-crafted
components, our method is based on a full transformer encoder-decoder
architecture with a learnable vote query driven object decoder, and a caption
decoder that produces the dense captions in a set-prediction manner. 2) In
contrast to the two-stage scheme, our method can perform detection and
captioning in one-stage. 3) Without bells and whistles, extensive experiments
on two commonly used datasets, ScanRefer and Nr3D, demonstrate that our
Vote2Cap-DETR surpasses current state-of-the-arts by 11.13\% and 7.11\% in
CIDEr@0.5IoU, respectively. Codes will be released soon.",None,-1
162ce5af-85f8-4101-83a5-a4889d86b993,OpenGDA: Graph Domain Adaptation Benchmark for Cross-network Learning,0.094982,1,"Graph domain adaptation models are widely adopted in cross-network learning
tasks, with the aim of transferring labeling or structural knowledge.
Currently, there mainly exist two limitations in evaluating graph domain
adaptation models. On one side, they are primarily tested for the specific
cross-network node classification task, leaving tasks at edge-level and
graph-level largely under-explored. Moreover, they are primarily tested in
limited scenarios, such as social networks or citation networks, lacking
validation of model's capability in richer scenarios. As comprehensively
assessing models could enhance model practicality in real-world applications,
we propose a benchmark, known as OpenGDA. It provides abundant pre-processed
and unified datasets for different types of tasks (node, edge, graph). They
originate from diverse scenarios, covering web information systems, urban
systems and natural systems. Furthermore, it integrates state-of-the-art models
with standardized and end-to-end pipelines. Overall, OpenGDA provides a
user-friendly, scalable and reproducible benchmark for evaluating graph domain
adaptation models. The benchmark experiments highlight the challenges of
applying GDA models to real-world applications with consistent good
performance, and potentially provide insights to future research. As an
emerging project, OpenGDA will be regularly updated with new datasets and
models. It could be accessed from https://github.com/Skyorca/OpenGDA.",None,-1
d251f801-0c55-405d-8caa-cb63817386b4,MAP: Domain Generalization via Meta-Learning on Anatomy-Consistent Pseudo-Modalities,0.746767,2,"Deep models suffer from limited generalization capability to unseen domains,
which has severely hindered their clinical applicability. Specifically for the
retinal vessel segmentation task, although the model is supposed to learn the
anatomy of the target, it can be distracted by confounding factors like
intensity and contrast. We propose Meta learning on Anatomy-consistent
Pseudo-modalities (MAP), a method that improves model generalizability by
learning structural features. We first leverage a feature extraction network to
generate three distinct pseudo-modalities that share the vessel structure of
the original image. Next, we use the episodic learning paradigm by selecting
one of the pseudo-modalities as the meta-train dataset, and perform
meta-testing on a continuous augmented image space generated through Dirichlet
mixup of the remaining pseudo-modalities. Further, we introduce two loss
functions that facilitate the model's focus on shape information by clustering
the latent vectors obtained from images featuring identical vasculature. We
evaluate our model on seven public datasets of various retinal imaging
modalities and we conclude that MAP has substantially better generalizability.
Our code is publically available at https://github.com/DeweiHu/MAP.",None,-1
1e06cd41-ed2b-449f-a69c-3f57e058547e,Deep Image Fingerprint: Towards Low Budget Synthetic Image Detection and Model Lineage Analysis,0.721556,7,"The generation of high-quality images has become widely accessible and is a
rapidly evolving process. As a result, anyone can generate images that are
indistinguishable from real ones. This leads to a wide range of applications,
including malicious usage with deceptive intentions. Despite advances in
detection techniques for generated images, a robust detection method still
eludes us. Furthermore, model personalization techniques might affect the
detection capabilities of existing methods. In this work, we utilize the
architectural properties of convolutional neural networks (CNNs) to develop a
new detection method. Our method can detect images from a known generative
model and enable us to establish relationships between fine-tuned generative
models. We tested the method on images produced by both Generative Adversarial
Networks (GANs) and recent large text-to-image models (LTIMs) that rely on
Diffusion Models. Our approach outperforms others trained under identical
conditions and achieves comparable performance to state-of-the-art pre-trained
detection methods on images generated by Stable Diffusion and MidJourney, with
significantly fewer required train samples.",None,-1
5abc0e56-ad25-49b6-9f5f-0c8e9879b55e,"Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia",0.505754,11,"Agent-based modeling has been around for decades, and applied widely across
the social and natural sciences. The scope of this research method is now
poised to grow dramatically as it absorbs the new affordances provided by Large
Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just
classic Agent-Based Models (ABM)s where the agents talk to one another. Rather,
GABMs are constructed using an LLM to apply common sense to situations, act
""reasonably"", recall common semantic knowledge, produce API calls to control
digital technologies like apps, and communicate both within the simulation and
to researchers viewing it from the outside. Here we present Concordia, a
library to facilitate constructing and working with GABMs. Concordia makes it
easy to construct language-mediated simulations of physically- or
digitally-grounded environments. Concordia agents produce their behavior using
a flexible component system which mediates between two fundamental operations:
LLM calls and associative memory retrieval. A special agent called the Game
Master (GM), which was inspired by tabletop role-playing games, is responsible
for simulating the environment where the agents interact. Agents take actions
by describing what they want to do in natural language. The GM then translates
their actions into appropriate implementations. In a simulated physical world,
the GM checks the physical plausibility of agent actions and describes their
effects. In digital environments simulating technologies such as apps and
services, the GM may handle API calls to integrate with external tools such as
general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar,
Email, Search, etc.). Concordia was designed to support a wide array of
applications both in scientific research and for evaluating performance of real
digital services by simulating users and/or generating synthetic data.",None,-1
63bb0c61-7b0f-44c3-8dbb-d4d818a96ece,Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models,0.535015,11,"Answering multi-hop reasoning questions requires retrieving and synthesizing
information from diverse sources. Large Language Models (LLMs) struggle to
perform such reasoning consistently. Here we propose an approach to pinpoint
and rectify multi-hop reasoning failures through targeted memory injections on
LLM attention heads. First, we analyze the per-layer activations of GPT-2
models in response to single and multi-hop prompts. We then propose a mechanism
that allows users to inject pertinent prompt-specific information, which we
refer to as ""memories,"" at critical LLM locations during inference. By thus
enabling the LLM to incorporate additional relevant information during
inference, we enhance the quality of multi-hop prompt completions. We show
empirically that a simple, efficient, and targeted memory injection into a key
attention layer can often increase the probability of the desired next token in
multi-hop tasks, by up to 424%.",None,-1
12c201ed-6ee8-441a-9331-6d9eaa070f4e,Explaining Legal Concepts with Augmented Large Language Models (GPT-4),0.999837,24,"Interpreting the meaning of legal open-textured terms is a key task of legal
professionals. An important source for this interpretation is how the term was
applied in previous court cases. In this paper, we evaluate the performance of
GPT-4 in generating factually accurate, clear and relevant explanations of
terms in legislation. We compare the performance of a baseline setup, where
GPT-4 is directly asked to explain a legal term, to an augmented approach,
where a legal information retrieval module is used to provide relevant context
to the model, in the form of sentences from case law. We found that the direct
application of GPT-4 yields explanations that appear to be of very high quality
on their surface. However, detailed analysis uncovered limitations in terms of
the factual accuracy of the explanations. Further, we found that the
augmentation leads to improved quality, and appears to eliminate the issue of
hallucination, where models invent incorrect statements. These findings open
the door to the building of systems that can autonomously retrieve relevant
sentences from case law and condense them into a useful explanation for legal
scholars, educators or practicing lawyers alike.",None,-1
2bafb246-7256-4cf2-91a4-41c1b94a8fbc,Reference-based Image Composition with Sketch via Structure-aware Diffusion Model,0.302267,5,"Recent remarkable improvements in large-scale text-to-image generative models
have shown promising results in generating high-fidelity images. To further
enhance editability and enable fine-grained generation, we introduce a
multi-input-conditioned image composition model that incorporates a sketch as a
novel modal, alongside a reference image. Thanks to the edge-level
controllability using sketches, our method enables a user to edit or complete
an image sub-part with a desired structure (i.e., sketch) and content (i.e.,
reference image). Our framework fine-tunes a pre-trained diffusion model to
complete missing regions using the reference image while maintaining sketch
guidance. Albeit simple, this leads to wide opportunities to fulfill user needs
for obtaining the in-demand images. Through extensive experiments, we
demonstrate that our proposed method offers unique use cases for image
manipulation, enabling user-driven modifications of arbitrary scenes.",None,-1
32242a9e-5b64-4d5a-8e3e-18200d3c4dca,Bayesian Deep Learning for Affordance Segmentation in images,0.192997,7,"Affordances are a fundamental concept in robotics since they relate available
actions for an agent depending on its sensory-motor capabilities and the
environment. We present a novel Bayesian deep network to detect affordances in
images, at the same time that we quantify the distribution of the aleatoric and
epistemic variance at the spatial level. We adapt the Mask-RCNN architecture to
learn a probabilistic representation using Monte Carlo dropout. Our results
outperform the state-of-the-art of deterministic networks. We attribute this
improvement to a better probabilistic feature space representation on the
encoder and the Bayesian variability induced at the mask generation, which
adapts better to the object contours. We also introduce the new
Probability-based Mask Quality measure that reveals the semantic and spatial
differences on a probabilistic instance segmentation model. We modify the
existing Probabilistic Detection Quality metric by comparing the binary masks
rather than the predicted bounding boxes, achieving a finer-grained evaluation
of the probabilistic segmentation. We find aleatoric variance in the contours
of the objects due to the camera noise, while epistemic variance appears in
visual challenging pixels.",None,-1
1efd5f21-051f-48f1-859a-9b463bcb442c,Machine Love,0.323229,3,"While ML generates much economic value, many of us have problematic
relationships with social media and other ML-powered applications. One reason
is that ML often optimizes for what we want in the moment, which is easy to
quantify but at odds with what is known scientifically about human flourishing.
Thus, through its impoverished models of us, ML currently falls far short of
its exciting potential, which is for it to help us to reach ours. While there
is no consensus on defining human flourishing, from diverse perspectives across
psychology, philosophy, and spiritual traditions, love is understood to be one
of its primary catalysts. Motivated by this view, this paper explores whether
there is a useful conception of love fitting for machines to embody, as
historically it has been generative to explore whether a nebulous concept, such
as life or intelligence, can be thoughtfully abstracted and reimagined, as in
the fields of machine intelligence or artificial life. This paper forwards a
candidate conception of machine love, inspired in particular by work in
positive psychology and psychotherapy: to provide unconditional support
enabling humans to autonomously pursue their own growth and development.
Through proof of concept experiments, this paper aims to highlight the need for
richer models of human flourishing in ML, provide an example framework through
which positive psychology can be combined with ML to realize a rough conception
of machine love, and demonstrate that current language models begin to enable
embodying qualitative humanistic principles. The conclusion is that though at
present ML may often serve to addict, distract, or divide us, an alternative
path may be opening up: We may align ML to support our growth, through it
helping us to align ourselves towards our highest aspirations.",None,-1
6dfd82e2-02c9-4437-a324-37acb489e243,LLaMA-E: Empowering E-commerce Authoring with Object-Interleaved Instruction Following,0.661069,4,"E-commerce authoring entails creating engaging, diverse, and targeted content
to enhance preference elicitation and retrieval experience. While Large
Language Models (LLMs) have revolutionized content generation, they often fall
short in e-commerce applications due to their limited memorization of
domain-specific features. This paper proposes LLaMA-E, the unified e-commerce
authoring models that address the contextual preferences of customers, sellers,
and platforms, the essential objects in e-commerce operation. We design the
instruction set derived from tasks of ads generation, query-enhanced product
title rewriting, product classification, purchase intent speculation, and
general e-commerce Q&A. The instruction formulation ensures the interleaved
cover of the presented and required object features, allowing the alignment of
base models to parameterise e-commerce knowledge comprehensively. The proposed
LLaMA-E models achieve state-of-the-art evaluation performance and exhibit the
advantage in zero-shot practical applications. To our knowledge, this is the
first LLM tailored to empower authoring applications with comprehensive
scenario understanding by integrating features focused on participated objects.",None,-1
ff0ed106-cc2e-42ad-8556-b4f9ab1b7d81,Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning,0.223263,1,"Objective: The artificial pancreas (AP) has shown promising potential in
achieving closed-loop glucose control for individuals with type 1 diabetes
mellitus (T1DM). However, designing an effective control policy for the AP
remains challenging due to the complex physiological processes, delayed insulin
response, and inaccurate glucose measurements. While model predictive control
(MPC) offers safety and stability through the dynamic model and safety
constraints, it lacks individualization and is adversely affected by
unannounced meals. Conversely, deep reinforcement learning (DRL) provides
personalized and adaptive strategies but faces challenges with distribution
shifts and substantial data requirements. Methods: We propose a hybrid control
policy for the artificial pancreas (HyCPAP) to address the above challenges.
HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the
strengths of both policies while compensating for their respective limitations.
To facilitate faster deployment of AP systems in real-world settings, we
further incorporate meta-learning techniques into HyCPAP, leveraging previous
experience and patient-shared knowledge to enable fast adaptation to new
patients with limited available data. Results: We conduct extensive experiments
using the FDA-accepted UVA/Padova T1DM simulator across three scenarios. Our
approaches achieve the highest percentage of time spent in the desired
euglycemic range and the lowest occurrences of hypoglycemia. Conclusion: The
results clearly demonstrate the superiority of our methods for closed-loop
glucose management in individuals with T1DM. Significance: The study presents
novel control policies for AP systems, affirming the great potential of
proposed methods for efficient closed-loop glucose control.",None,-1
17725416-2576-42d6-84f6-be3f559bf958,A Systematic Analysis of Vocabulary and BPE Settings for Optimal Fine-tuning of NMT: A Case Study of In-domain Translation,0.575362,3,"The effectiveness of Neural Machine Translation (NMT) models largely depends
on the vocabulary used at training; small vocabularies can lead to
out-of-vocabulary problems -- large ones, to memory issues. Subword (SW)
tokenization has been successfully employed to mitigate these issues. The
choice of vocabulary and SW tokenization has a significant impact on both
training and fine-tuning an NMT model. Fine-tuning is a common practice in
optimizing an MT model with respect to new data. However, new data potentially
introduces new words (or tokens), which, if not taken into consideration, may
lead to suboptimal performance. In addition, the distribution of tokens in the
new data can differ from the distribution of the original data. As such, the
original SW tokenization model could be less suitable for the new data. Through
a systematic empirical evaluation, in this work we compare different strategies
for SW tokenization and vocabulary generation with the ultimate goal to uncover
an optimal setting for fine-tuning a domain-specific model. Furthermore, we
developed several (in-domain) models, the best of which achieves 6 BLEU points
improvement over the baseline.",None,-1
b11d6bd5-c050-4758-9e85-8beb8abf4589,Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems,0.361499,4,"This paper aims to serve as a comprehensive guide for researchers and
practitioners, offering insights into the current state, potential
applications, and future research directions for generative artificial
intelligence and foundation models within the context of intelligent vehicles.
As the automotive industry progressively integrates AI, generative artificial
intelligence technologies hold the potential to revolutionize user
interactions, delivering more immersive, intuitive, and personalised in-car
experiences. We provide an overview of current applications of generative
artificial intelligence in the automotive domain, emphasizing speech, audio,
vision, and multimodal interactions. We subsequently outline critical future
research areas, including domain adaptability, alignment, multimodal
integration and others, as well as, address the challenges and risks associated
with ethics. By fostering collaboration and addressing these research areas,
generative artificial intelligence can unlock its full potential, transforming
the driving experience and shaping the future of intelligent vehicles.",None,-1
1f6b4854-cd32-4468-9c3d-caf8bf0eab6d,ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data,0.571867,5,"Compared with traditional RGB-only visual tracking, few datasets have been
constructed for RGB-D tracking. In this paper, we propose ARKitTrack, a new
RGB-D tracking dataset for both static and dynamic scenes captured by
consumer-grade LiDAR scanners equipped on Apple's iPhone and iPad. ARKitTrack
contains 300 RGB-D sequences, 455 targets, and 229.7K video frames in total.
Along with the bounding box annotations and frame-level attributes, we also
annotate this dataset with 123.9K pixel-level target masks. Besides, the camera
intrinsic and camera pose of each frame are provided for future developments.
To demonstrate the potential usefulness of this dataset, we further present a
unified baseline for both box-level and pixel-level tracking, which integrates
RGB features with bird's-eye-view representations to better explore
cross-modality 3D geometry. In-depth empirical analysis has verified that the
ARKitTrack dataset can significantly facilitate RGB-D tracking and that the
proposed baseline method compares favorably against the state of the arts. The
code and dataset is available at https://arkittrack.github.io.",None,-1
64411a18-1590-4de8-a778-ef9593cd563b,Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces,0.441223,5,"Self-supervised speech representations are known to encode both speaker and
phonetic information, but how they are distributed in the high-dimensional
space remains largely unexplored. We hypothesize that they are encoded in
orthogonal subspaces, a property that lends itself to simple disentanglement.
Applying principal component analysis to representations of two predictive
coding models, we identify two subspaces that capture speaker and phonetic
variances, and confirm that they are nearly orthogonal. Based on this property,
we propose a new speaker normalization method which collapses the subspace that
encodes speaker information, without requiring transcriptions. Probing
experiments show that our method effectively eliminates speaker information and
outperforms a previous baseline in phone discrimination tasks. Moreover, the
approach generalizes and can be used to remove information of unseen speakers.",None,-1
8d6d3879-5e20-4c80-b74a-504e9fd7bac2,On Modeling Network Slicing Communication Resources with SARSA Optimization,0.318434,2,"Network slicing is a crucial enabler to support the composition and
deployment of virtual network infrastructures required by the dynamic behavior
of networks like 5G/6G mobile networks, IoT-aware networks, e-health systems,
and industry verticals like the internet of vehicles (IoV) and industry 4.0.
The communication slices and their allocated communication resources are
essential in slicing architectures for resource orchestration and allocation,
virtual network function (VNF) deployment, and slice operation functionalities.
The communication slices provide the communications capabilities required to
support slice operation, SLA guarantees, and QoS/ QoE application requirements.
Therefore, this contribution proposes a networking slicing conceptual model to
formulate the optimization problem related to the sharing of communication
resources among communication slices. First, we present a conceptual model of
network slicing, we then formulate analytically some aspects of the model and
the optimization problem to address. Next, we proposed to use a SARSA agent to
solve the problem and implement a proof of concept prototype. Finally, we
present the obtained results and discuss them.",None,-1
8a0266de-c65f-48a8-a65a-b72769a8bce1,Distribution-Aware Prompt Tuning for Vision-Language Models,0.135272,5,"Pre-trained vision-language models (VLMs) have shown impressive performance
on various downstream tasks by utilizing knowledge learned from large data. In
general, the performance of VLMs on target tasks can be further improved by
prompt tuning, which adds context to the input image or text. By leveraging
data from target tasks, various prompt-tuning methods have been studied in the
literature. A key to prompt tuning is the feature space alignment between two
modalities via learnable vectors with model parameters fixed. We observed that
the alignment becomes more effective when embeddings of each modality are
`well-arranged' in the latent space. Inspired by this observation, we proposed
distribution-aware prompt tuning (DAPT) for vision-language models, which is
simple yet effective. Specifically, the prompts are learned by maximizing
inter-dispersion, the distance between classes, as well as minimizing the
intra-dispersion measured by the distance between embeddings from the same
class. Our extensive experiments on 11 benchmark datasets demonstrate that our
method significantly improves generalizability. The code is available at
https://github.com/mlvlab/DAPT.",None,-1
4f861d91-5e1d-4dd7-bc26-62536db024cc,Persistent Nature: A Generative Model of Unbounded 3D Worlds,0.320234,16,"Despite increasingly realistic image quality, recent 3D image generative
models often operate on 3D volumes of fixed extent with limited camera motions.
We investigate the task of unconditionally synthesizing unbounded nature
scenes, enabling arbitrarily large camera motion while maintaining a persistent
3D world model. Our scene representation consists of an extendable, planar
scene layout grid, which can be rendered from arbitrary camera poses via a 3D
decoder and volume rendering, and a panoramic skydome. Based on this
representation, we learn a generative world model solely from single-view
internet photos. Our method enables simulating long flights through 3D
landscapes, while maintaining global scene consistency--for instance, returning
to the starting point yields the same view of the scene. Our approach enables
scene extrapolation beyond the fixed bounds of current 3D generative models,
while also supporting a persistent, camera-independent world representation
that stands in contrast to auto-regressive 3D prediction models. Our project
page: https://chail.github.io/persistent-nature/.",None,-1
f8c96fe4-615b-47ad-895b-c3bbdab06974,Video-Mined Task Graphs for Keystep Recognition in Instructional Videos,0.390777,8,"Procedural activity understanding requires perceiving human actions in terms
of a broader task, where multiple keysteps are performed in sequence across a
long video to reach a final goal state -- such as the steps of a recipe or a
DIY fix-it task. Prior work largely treats keystep recognition in isolation of
this broader structure, or else rigidly confines keysteps to align with a
predefined sequential script. We propose discovering a task graph automatically
from how-to videos to represent probabilistically how people tend to execute
keysteps, and then leverage this graph to regularize keystep recognition in
novel videos. On multiple datasets of real-world instructional videos, we show
the impact: more reliable zero-shot keystep localization and improved video
representation learning, exceeding the state of the art.",None,-1
13273539-dc72-4acd-8d22-80b00ec8019b,Synthetically generated text for supervised text analysis,0.101369,6,"Supervised text models are a valuable tool for political scientists but
present several obstacles to their use, including the expense of hand-labeling
documents, the difficulty of retrieving rare relevant documents for annotation,
and copyright and privacy concerns involved in sharing annotated documents.
This article proposes a partial solution to these three issues, in the form of
controlled generation of synthetic text with large language models. I provide a
conceptual overview of text generation, guidance on when researchers should
prefer different techniques for generating synthetic text, a discussion of
ethics, and a simple technique for improving the quality of synthetic text. I
demonstrate the usefulness of synthetic text with three applications:
generating synthetic tweets describing the fighting in Ukraine, synthetic news
articles describing specified political events for training an event detection
system, and a multilingual corpus of populist manifesto statements for training
a sentence-level populism classifier.",None,-1
22b79ce0-8e7c-400a-96af-0deeb429fc47,Artificial General Intelligence for Medical Imaging,0.727174,23,"In this review, we explore the potential applications of Artificial General
Intelligence (AGI) models in healthcare, focusing on foundational Large
Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We
emphasize the importance of integrating clinical expertise, domain knowledge,
and multimodal capabilities into AGI models. In addition, we lay out key
roadmaps that guide the development and deployment of healthcare AGI models.
Throughout the review, we provide critical perspectives on the potential
challenges and pitfalls associated with deploying large-scale AGI models in the
medical field. This comprehensive review aims to offer insights into the future
implications of AGI in medical imaging, healthcare and beyond.",None,-1
4cae3e94-481e-49f8-905d-01c8f2f0b7f1,EriBERTa: A Bilingual Pre-Trained Language Model for Clinical Natural Language Processing,0.0998327,1,"The utilization of clinical reports for various secondary purposes, including
health research and treatment monitoring, is crucial for enhancing patient
care. Natural Language Processing (NLP) tools have emerged as valuable assets
for extracting and processing relevant information from these reports. However,
the availability of specialized language models for the clinical domain in
Spanish has been limited.
  In this paper, we introduce EriBERTa, a bilingual domain-specific language
model pre-trained on extensive medical and clinical corpora. We demonstrate
that EriBERTa outperforms previous Spanish language models in the clinical
domain, showcasing its superior capabilities in understanding medical texts and
extracting meaningful information. Moreover, EriBERTa exhibits promising
transfer learning abilities, allowing for knowledge transfer from one language
to another. This aspect is particularly beneficial given the scarcity of
Spanish clinical data.",None,-1
ea8f8173-9878-4ec2-a83c-eeb874627d06,A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,0.667166,7,"Recent instruction fine-tuned models can solve multiple NLP tasks when
prompted to do so, with machine translation (MT) being a prominent use case.
However, current research often focuses on standard performance benchmarks,
leaving compelling fairness and ethical considerations behind. In MT, this
might lead to misgendered translations, resulting, among other harms, in the
perpetuation of stereotypes and prejudices. In this work, we address this gap
by investigating whether and to what extent such models exhibit gender bias in
machine translation and how we can mitigate it. Concretely, we compute
established gender bias metrics on the WinoMT corpus from English to German and
Spanish. We discover that IFT models default to male-inflected translations,
even disregarding female occupational stereotypes. Next, using interpretability
methods, we unveil that models systematically overlook the pronoun indicating
the gender of a target occupation in misgendered translations. Finally, based
on this finding, we propose an easy-to-implement and effective bias mitigation
solution based on few-shot learning that leads to significantly fairer
translations.",None,-1
e2718da9-9d8f-427e-8ea3-e444f7bd9f4c,Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning,0.874446,13,"The surge in Reinforcement Learning (RL) applications in Intelligent
Transportation Systems (ITS) has contributed to its growth as well as
highlighted key challenges. However, defining objectives of RL agents in
traffic control and management tasks, as well as aligning policies with these
goals through an effective formulation of Markov Decision Process (MDP), can be
challenging and often require domain experts in both RL and ITS. Recent
advancements in Large Language Models (LLMs) such as GPT-4 highlight their
broad general knowledge, reasoning capabilities, and commonsense priors across
various domains. In this work, we conduct a large-scale user study involving 70
participants to investigate whether novices can leverage ChatGPT to solve
complex mixed traffic control problems. Three environments are tested,
including ring road, bottleneck, and intersection. We find ChatGPT has mixed
results. For intersection and bottleneck, ChatGPT increases number of
successful policies by 150% and 136% compared to solely beginner capabilities,
with some of them even outperforming experts. However, ChatGPT does not provide
consistent improvements across all scenarios.",None,-1
7a07e8d8-4cde-44e6-9022-40a1b4898445,MobileVidFactory: Automatic Diffusion-Based Social Media Video Generation for Mobile Devices from Text,0.865314,5,"Videos for mobile devices become the most popular access to share and acquire
information recently. For the convenience of users' creation, in this paper, we
present a system, namely MobileVidFactory, to automatically generate vertical
mobile videos where users only need to give simple texts mainly. Our system
consists of two parts: basic and customized generation. In the basic
generation, we take advantage of the pretrained image diffusion model, and
adapt it to a high-quality open-domain vertical video generator for mobile
devices. As for the audio, by retrieving from our big database, our system
matches a suitable background sound for the video. Additionally to produce
customized content, our system allows users to add specified screen texts to
the video for enriching visual expression, and specify texts for automatic
reading with optional voices as they like.",None,-1
1b11d985-206d-4410-808d-11f13022e593,Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata,0.497441,5,"In this work, we explore the use of Large Language Models (LLMs) for
knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge.
For this task, given subject and relation pairs sourced from Wikidata, we
utilize pre-trained LLMs to produce the relevant objects in string format and
link them to their respective Wikidata QIDs. We developed a pipeline using LLMs
for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata
entity mapping. The method achieved a macro-averaged F1-score of 0.701 across
the properties, with the scores varying from 1.00 to 0.328. These results
demonstrate that the knowledge of LLMs varies significantly depending on the
domain and that further experimentation is required to determine the
circumstances under which LLMs can be used for automatic Knowledge Base (e.g.,
Wikidata) completion and correction. The investigation of the results also
suggests the promising contribution of LLMs in collaborative knowledge
engineering. LLMKE won Track 2 of the challenge. The implementation is
available at https://github.com/bohuizhang/LLMKE.",None,-1
b4db68a7-f083-4e6e-94a5-086dca9d8181,Llemma: An Open Language Model For Mathematics,0.999993,119,"We present Llemma, a large language model for mathematics. We continue
pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web
data containing mathematics, and mathematical code, yielding Llemma. On the
MATH benchmark Llemma outperforms all known open base models, as well as the
unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is
capable of tool use and formal theorem proving without any further finetuning.
We openly release all artifacts, including 7 billion and 34 billion parameter
models, the Proof-Pile-2, and code to replicate our experiments.",None,-1
d6e278e1-af78-4023-9717-0ac9f13d2a76,ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing,0.0805024,2,"The StyleGAN family succeed in high-fidelity image generation and allow for
flexible and plausible editing of generated images by manipulating the
semantic-rich latent style space.However, projecting a real image into its
latent space encounters an inherent trade-off between inversion quality and
editability. Existing encoder-based or optimization-based StyleGAN inversion
methods attempt to mitigate the trade-off but see limited performance. To
fundamentally resolve this problem, we propose a novel two-phase framework by
designating two separate networks to tackle editing and reconstruction
respectively, instead of balancing the two. Specifically, in Phase I, a
W-space-oriented StyleGAN inversion network is trained and used to perform
image inversion and editing, which assures the editability but sacrifices
reconstruction quality. In Phase II, a carefully designed rectifying network is
utilized to rectify the inversion errors and perform ideal reconstruction.
Experimental results show that our approach yields near-perfect reconstructions
without sacrificing the editability, thus allowing accurate manipulation of
real images. Further, we evaluate the performance of our rectifying network,
and see great generalizability towards unseen manipulation types and
out-of-domain images.",None,-1
6e4437f6-b08a-4f92-a326-d9f3ee46bddc,A novel efficient Multi-view traffic-related object detection framework,0.684739,7,"With the rapid development of intelligent transportation system applications,
a tremendous amount of multi-view video data has emerged to enhance vehicle
perception. However, performing video analytics efficiently by exploiting the
spatial-temporal redundancy from video data remains challenging. Accordingly,
we propose a novel traffic-related framework named CEVAS to achieve efficient
object detection using multi-view video data. Briefly, a fine-grained input
filtering policy is introduced to produce a reasonable region of interest from
the captured images. Also, we design a sharing object manager to manage the
information of objects with spatial redundancy and share their results with
other vehicles. We further derive a content-aware model selection policy to
select detection methods adaptively. Experimental results show that our
framework significantly reduces response latency while achieving the same
detection accuracy as the state-of-the-art methods.",None,-1
39e5ccbc-ce03-4d96-a6ca-a94436962840,Mining for Unknown Unknowns,0.12126,1,"Unknown unknowns are future relevant contingencies that lack an ex ante
description. While there are numerous retrospective accounts showing that
significant gains or losses might have been achieved or avoided had such
contingencies been previously uncovered, getting hold of unknown unknowns still
remains elusive, both in practice and conceptually. Using Formal Concept
Analysis (FCA) - a subfield of lattice theory which is increasingly applied for
mining and organizing data - this paper introduces a simple framework to
systematically think out of the box and direct the search for unknown unknowns.",None,-1
6353b1e4-5c25-47af-9692-6c823fb3d558,AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of Autism Spectrum Disorder,0.841606,3,"To easily obtain the knowledge about autism spectrum disorder and help its
early screening and diagnosis, we create AsdKB, a Chinese knowledge base on
autism spectrum disorder. The knowledge base is built on top of various
sources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical
descriptions on mental and behavioural disorders, 2) the diagnostic knowledge
from DSM-5 and different screening tools recommended by social organizations
and medical institutes, and 3) the expert knowledge on professional physicians
and hospitals from the Web. AsdKB contains both ontological and factual
knowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The
potential applications of AsdKB are question answering, auxiliary diagnosis,
and expert recommendation, and we illustrate them with a prototype which can be
accessed at http://asdkb.org.cn/.",None,-1
025c6f6d-68cf-4bd0-9607-8aff0ba92b61,Zero-shot Transfer of Article-aware Legal Outcome Classification for European Court of Human Rights Cases,0.911355,13,"In this paper, we cast Legal Judgment Prediction on European Court of Human
Rights cases into an article-aware classification task, where the case outcome
is classified from a combined input of case facts and convention articles. This
configuration facilitates the model learning some legal reasoning ability in
mapping article text to specific case fact text. It also provides an
opportunity to evaluate the model's ability to generalize to zero-shot settings
when asked to classify the case outcome with respect to articles not seen
during training. We devise zero-shot experiments and apply domain adaptation
methods based on domain discrimination and Wasserstein distance. Our results
demonstrate that the article-aware architecture outperforms straightforward
fact classification. We also find that domain adaptation methods improve
zero-shot transfer performance, with article relatedness and encoder
pre-training influencing the effect.",None,-1
a084458f-03ec-4854-a7aa-9f7c6c74eb2e,How Can Large Language Models Help Humans in Design and Manufacturing?,0.591619,16,"The advancement of Large Language Models (LLMs), including GPT-4, provides
exciting new opportunities for generative design. We investigate the
application of this tool across the entire design and manufacturing workflow.
Specifically, we scrutinize the utility of LLMs in tasks such as: converting a
text-based prompt into a design specification, transforming a design into
manufacturing instructions, producing a design space and design variations,
computing the performance of a design, and searching for designs predicated on
performance. Through a series of examples, we highlight both the benefits and
the limitations of the current LLMs. By exposing these limitations, we aspire
to catalyze the continued improvement and progression of these models.",None,-1
3c576720-5005-436c-85c4-826ae7bb1e0d,Generative Semantic Segmentation,0.602743,18,"We present Generative Semantic Segmentation (GSS), a generative learning
approach for semantic segmentation. Uniquely, we cast semantic segmentation as
an image-conditioned mask generation problem. This is achieved by replacing the
conventional per-pixel discriminative learning with a latent prior learning
process. Specifically, we model the variational posterior distribution of
latent variables given the segmentation mask. To that end, the segmentation
mask is expressed with a special type of image (dubbed as maskige). This
posterior distribution allows to generate segmentation masks unconditionally.
To achieve semantic segmentation on a given image, we further introduce a
conditioning network. It is optimized by minimizing the divergence between the
posterior distribution of maskige (i.e., segmentation masks) and the latent
prior distribution of input training images. Extensive experiments on standard
benchmarks show that our GSS can perform competitively to prior art
alternatives in the standard semantic segmentation setting, whilst achieving a
new state of the art in the more challenging cross-domain setting.",None,-1
72bbd099-c68c-4124-9287-a77d473fc11f,Exploring Opinion-unaware Video Quality Assessment with Semantic Affinity Criterion,0.624026,16,"Recent learning-based video quality assessment (VQA) algorithms are expensive
to implement due to the cost of data collection of human quality opinions, and
are less robust across various scenarios due to the biases of these opinions.
This motivates our exploration on opinion-unaware (a.k.a zero-shot) VQA
approaches. Existing approaches only considers low-level naturalness in spatial
or temporal domain, without considering impacts from high-level semantics. In
this work, we introduce an explicit semantic affinity index for opinion-unaware
VQA using text-prompts in the contrastive language-image pre-training (CLIP)
model. We also aggregate it with different traditional low-level naturalness
indexes through gaussian normalization and sigmoid rescaling strategies.
Composed of aggregated semantic and technical metrics, the proposed Blind
Unified Opinion-Unaware Video Quality Index via Semantic and Technical Metric
Aggregation (BUONA-VISTA) outperforms existing opinion-unaware VQA methods by
at least 20% improvements, and is more robust than opinion-aware approaches.",None,-1
42f13c6a-70c6-4dc5-b9a9-63882d7e4066,Large Language Models Cannot Self-Correct Reasoning Yet,0.86057,179,"Large Language Models (LLMs) have emerged as a groundbreaking technology with
their unparalleled text generation capabilities across various applications.
Nevertheless, concerns persist regarding the accuracy and appropriateness of
their generated content. A contemporary methodology, self-correction, has been
proposed as a remedy to these issues. Building upon this premise, this paper
critically examines the role and efficacy of self-correction within LLMs,
shedding light on its true potential and limitations. Central to our
investigation is the notion of intrinsic self-correction, whereby an LLM
attempts to correct its initial responses based solely on its inherent
capabilities, without the crutch of external feedback. In the context of
reasoning, our research indicates that LLMs struggle to self-correct their
responses without external feedback, and at times, their performance even
degrades after self-correction. Drawing from these insights, we offer
suggestions for future research and practical applications in this field.",None,-1
19a57d44-a28b-45bc-9750-18d8c81f8635,BiSVP: Building Footprint Extraction via Bidirectional Serialized Vertex Prediction,0.250688,1,"Extracting building footprints from remote sensing images has been attracting
extensive attention recently. Dominant approaches address this challenging
problem by generating vectorized building masks with cumbersome refinement
stages, which limits the application of such methods. In this paper, we
introduce a new refinement-free and end-to-end building footprint extraction
method, which is conceptually intuitive, simple, and effective. Our method,
termed as BiSVP, represents a building instance with ordered vertices and
formulates the building footprint extraction as predicting the serialized
vertices directly in a bidirectional fashion. Moreover, we propose a
cross-scale feature fusion (CSFF) module to facilitate high resolution and rich
semantic feature learning, which is essential for the dense building vertex
prediction task. Without bells and whistles, our BiSVP outperforms
state-of-the-art methods by considerable margins on three building instance
segmentation benchmarks, clearly demonstrating its superiority. The code and
datasets will be made public available.",None,-1
49a91aa3-7f67-4b5d-a842-b445515369bc,Data Contamination Through the Lens of Time,0.17065,17,"Recent claims about the impressive abilities of large language models (LLMs)
are often supported by evaluating publicly available benchmarks. Since LLMs
train on wide swaths of the internet, this practice raises concerns of data
contamination, i.e., evaluating on examples that are explicitly or implicitly
included in the training data. Data contamination remains notoriously
challenging to measure and mitigate, even with partial attempts like controlled
experimentation of training data, canary strings, or embedding similarities. In
this work, we conduct the first thorough longitudinal analysis of data
contamination in LLMs by using the natural experiment of training cutoffs in
GPT models to look at benchmarks released over time. Specifically, we consider
two code/mathematical problem-solving datasets, Codeforces and Project Euler,
and find statistically significant trends among LLM pass rate vs. GitHub
popularity and release date that provide strong evidence of contamination. By
open-sourcing our dataset, raw results, and evaluation framework, our work
paves the way for rigorous analyses of data contamination in modern models. We
conclude with a discussion of best practices and future steps for publicly
releasing benchmarks in the age of LLMs that train on webscale data.",None,-1
843e4816-6ef2-4df8-a99e-1527ef62b4a6,UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction,0.133424,2,"In recent years, many video tasks have achieved breakthroughs by utilizing
the vision transformer and establishing spatial-temporal decoupling for feature
extraction. Although multi-view 3D reconstruction also faces multiple images as
input, it cannot immediately inherit their success due to completely ambiguous
associations between unstructured views. There is not usable prior
relationship, which is similar to the temporally-coherence property in a video.
To solve this problem, we propose a novel transformer network for Unstructured
Multiple Images (UMIFormer). It exploits transformer blocks for decoupled
intra-view encoding and designed blocks for token rectification that mine the
correlation between similar tokens from different views to achieve decoupled
inter-view encoding. Afterward, all tokens acquired from various branches are
compressed into a fixed-size compact representation while preserving rich
information for reconstruction by leveraging the similarities between tokens.
We empirically demonstrate on ShapeNet and confirm that our decoupled learning
method is adaptable for unstructured multiple images. Meanwhile, the
experiments also verify our model outperforms existing SOTA methods by a large
margin. Code will be available at https://github.com/GaryZhu1996/UMIFormer.",None,-1
b0322439-f9f3-4da2-a90b-a9588251dd1c,What's the Meaning of Superhuman Performance in Today's NLU?,0.344702,16,"In the last five years, there has been a significant focus in Natural
Language Processing (NLP) on developing larger Pretrained Language Models
(PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their
abilities in language understanding, reasoning, and reading comprehension.
These PLMs have achieved impressive results on these benchmarks, even
surpassing human performance in some cases. This has led to claims of
superhuman capabilities and the provocative idea that certain tasks have been
solved. In this position paper, we take a critical look at these claims and ask
whether PLMs truly have superhuman abilities and what the current benchmarks
are really evaluating. We show that these benchmarks have serious limitations
affecting the comparison between humans and PLMs and provide recommendations
for fairer and more transparent benchmarks.",None,-1
c26adc6e-c7ba-4776-8872-82094f12502a,A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings,0.423898,12,"Predicting where a person is looking is a complex task, requiring to
understand not only the person's gaze and scene content, but also the 3D scene
structure and the person's situation (are they manipulating? interacting or
observing others? attentive?) to detect obstructions in the line of sight or
apply attention priors that humans typically have when observing others. In
this paper, we hypothesize that identifying and leveraging such priors can be
better achieved through the exploitation of explicitly derived multimodal cues
such as depth and pose. We thus propose a modular multimodal architecture
allowing to combine these cues using an attention mechanism. The architecture
can naturally be exploited in privacy-sensitive situations such as surveillance
and health, where personally identifiable information cannot be released. We
perform extensive experiments on the GazeFollow and VideoAttentionTarget public
datasets, obtaining state-of-the-art performance and demonstrating very
competitive results in the privacy setting case.",None,-1
f6af3f9c-f516-47d0-9c44-0aeeeb7aadd5,Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity,0.02051,1,"This paper presents novel benchmarks for evaluating vision-language models
(VLMs) in zero-shot recognition, focusing on granularity and specificity.
Although VLMs excel in tasks like image captioning, they face challenges in
open-world settings. Our benchmarks test VLMs' consistency in understanding
concepts across semantic granularity levels and their response to varying text
specificity. Findings show that VLMs favor moderately fine-grained concepts and
struggle with specificity, often misjudging texts that differ from their
training data. Extensive evaluations reveal limitations in current VLMs,
particularly in distinguishing between correct and subtly incorrect
descriptions. While fine-tuning offers some improvements, it doesn't fully
address these issues, highlighting the need for VLMs with enhanced
generalization capabilities for real-world applications. This study provides
insights into VLM limitations and suggests directions for developing more
robust models.",None,-1
8d92ec9c-13ce-4876-9436-c29dbad5ffbb,Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models,0.503423,41,"Text-to-image (T2I) personalization allows users to guide the creative image
generation process by combining their own visual concepts in natural language
prompts. Recently, encoder-based techniques have emerged as a new effective
approach for T2I personalization, reducing the need for multiple images and
long training times. However, most existing encoders are limited to a
single-class domain, which hinders their ability to handle diverse concepts. In
this work, we propose a domain-agnostic method that does not require any
specialized dataset or prior information about the personalized concepts. We
introduce a novel contrastive-based regularization technique to maintain high
fidelity to the target concept characteristics while keeping the predicted
embeddings close to editable regions of the latent space, by pushing the
predicted tokens toward their nearest existing CLIP tokens. Our experimental
results demonstrate the effectiveness of our approach and show how the learned
tokens are more semantic than tokens predicted by unregularized models. This
leads to a better representation that achieves state-of-the-art performance
while being more flexible than previous methods.",None,-1
c368e4a5-8c73-4821-bec4-1ba883e486e8,An Overview about Emerging Technologies of Autonomous Driving,0.127643,2,"Since DARPA started Grand Challenges in 2004 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. This
paper gives an overview about technical aspects of autonomous driving
technologies and open problems. We investigate the major fields of self-driving
systems, such as perception, mapping and localization, prediction, planning and
control, simulation, V2X and safety etc. Especially we elaborate on all these
issues in a framework of data closed loop, a popular platform to solve the long
tailed autonomous driving problems.",None,-1
082f5975-8e72-438e-941b-c7247b115104,Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge,0.546039,8,"In empathetic conversations, individuals express their empathy towards
others. Previous work has mainly focused on generating empathetic responses by
utilizing the speaker's emotion. Besides, external commonsense knowledge has
been applied to enhance the system's understandings of the speaker's situation.
However, given an event, commonsense knowledge base contains various relations,
potentially leading to confusion for the dialogue system. Consequently,
inconsistencies arise among the emotion, generated response and speaker's
contextual information. To this end, we propose a novel approach for empathetic
response generation, which incorporates an adaptive module for commonsense
knowledge selection to ensure consistency between the generated empathetic
responses and the speaker's situation. This selected knowledge is used to
refine the commonsense cognition and empathy expression for generated
responses. Experimental results show that our approach significantly
outperforms baseline models in both automatic and human evaluations, exhibiting
the generation of more coherent and empathetic responses. Moreover, case
studies highlight the interpretability of knowledge selection in the responses
and the effectiveness of adaptive module in our model. Code:
https://github.com/Hanscal/DCKS.",None,-1
8c5b9d7a-19d2-4cb4-96f1-a70125bd1271,FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation,0.329644,6,"Humans ask follow-up questions driven by curiosity, which reflects a creative
human cognitive process. We introduce the task of real-world
information-seeking follow-up question generation (FQG), which aims to generate
follow-up questions seeking a more in-depth understanding of an initial
question and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world
(initial question, answer, follow-up question) tuples collected from a Reddit
forum providing layman-friendly explanations for open-ended questions. In
contrast to existing datasets, questions in FOLLOWUPQG use more diverse
pragmatic strategies to seek information, and they also show higher-order
cognitive skills (such as applying and relating). We evaluate current question
generation models on their efficacy for generating follow-up questions,
exploring how to generate specific types of follow-up questions based on
step-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging
benchmark, as model-generated questions are adequate but far from human-raised
questions in terms of informativeness and complexity.",None,-1
4e996435-debb-4178-8a3b-17666efb17bf,On Evaluating and Mitigating Gender Biases in Multilingual Settings,0.707932,8,"While understanding and removing gender biases in language models has been a
long-standing problem in Natural Language Processing, prior research work has
primarily been limited to English. In this work, we investigate some of the
challenges with evaluating and mitigating biases in multilingual settings which
stem from a lack of existing benchmarks and resources for bias evaluation
beyond English especially for non-western context. In this paper, we first
create a benchmark for evaluating gender biases in pre-trained masked language
models by extending DisCo to different Indian languages using human
annotations. We extend various debiasing methods to work beyond English and
evaluate their effectiveness for SOTA massively multilingual models on our
proposed metric. Overall, our work highlights the challenges that arise while
studying social biases in multilingual settings and provides resources as well
as mitigation techniques to take a step toward scaling to more languages.",None,-1
f76fbd64-2591-41ad-986b-dbcccaf01630,Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation,0.374643,3,"We study the problem of few-shot physically-aware articulated mesh
generation. By observing an articulated object dataset containing only a few
examples, we wish to learn a model that can generate diverse meshes with high
visual fidelity and physical validity. Previous mesh generative models either
have difficulties in depicting a diverse data space from only a few examples or
fail to ensure physical validity of their samples. Regarding the above
challenges, we propose two key innovations, including 1) a hierarchical mesh
deformation-based generative model based upon the divide-and-conquer philosophy
to alleviate the few-shot challenge by borrowing transferrable deformation
patterns from large scale rigid meshes and 2) a physics-aware deformation
correction scheme to encourage physically plausible generations. We conduct
extensive experiments on 6 articulated categories to demonstrate the
superiority of our method in generating articulated meshes with better
diversity, higher visual fidelity, and better physical validity over previous
methods in the few-shot setting. Further, we validate solid contributions of
our two innovations in the ablation study. Project page with code is available
at https://meowuu7.github.io/few-arti-obj-gen.",None,-1
e79fc755-5172-4f09-bc80-8810e50a9d4b,RBSR: Efficient and Flexible Recurrent Network for Burst Super-Resolution,0.414735,4,"Burst super-resolution (BurstSR) aims at reconstructing a high-resolution
(HR) image from a sequence of low-resolution (LR) and noisy images, which is
conducive to enhancing the imaging effects of smartphones with limited sensors.
The main challenge of BurstSR is to effectively combine the complementary
information from input frames, while existing methods still struggle with it.
In this paper, we suggest fusing cues frame-by-frame with an efficient and
flexible recurrent network. In particular, we emphasize the role of the
base-frame and utilize it as a key prompt to guide the knowledge acquisition
from other frames in every recurrence. Moreover, we introduce an implicit
weighting loss to improve the model's flexibility in facing input frames with
variable numbers. Extensive experiments on both synthetic and real-world
datasets demonstrate that our method achieves better results than
state-of-the-art ones. Codes and pre-trained models are available at
https://github.com/ZcsrenlongZ/RBSR.",None,-1
2d2b5d2f-9378-43cd-899e-4ba528610e11,Markerless Motion Capture and Biomechanical Analysis Pipeline,0.605595,6,"Markerless motion capture using computer vision and human pose estimation
(HPE) has the potential to expand access to precise movement analysis. This
could greatly benefit rehabilitation by enabling more accurate tracking of
outcomes and providing more sensitive tools for research. There are numerous
steps between obtaining videos to extracting accurate biomechanical results and
limited research to guide many critical design decisions in these pipelines. In
this work, we analyze several of these steps including the algorithm used to
detect keypoints and the keypoint set, the approach to reconstructing
trajectories for biomechanical inverse kinematics and optimizing the IK
process. Several features we find important are: 1) using a recent algorithm
trained on many datasets that produces a dense set of biomechanically-motivated
keypoints, 2) using an implicit representation to reconstruct smooth,
anatomically constrained marker trajectories for IK, 3) iteratively optimizing
the biomechanical model to match the dense markers, 4) appropriate
regularization of the IK process. Our pipeline makes it easy to obtain accurate
biomechanical estimates of movement in a rehabilitation hospital.",None,-1
fda1c487-b865-44a4-9e5d-971230caa275,Hierarchical Interactive Reconstruction Network For Video Compressive Sensing,0.178846,1,"Deep network-based image and video Compressive Sensing(CS) has attracted
increasing attentions in recent years. However, in the existing deep
network-based CS methods, a simple stacked convolutional network is usually
adopted, which not only weakens the perception of rich contextual prior
knowledge, but also limits the exploration of the correlations between temporal
video frames. In this paper, we propose a novel Hierarchical InTeractive Video
CS Reconstruction Network(HIT-VCSNet), which can cooperatively exploit the deep
priors in both spatial and temporal domains to improve the reconstruction
quality. Specifically, in the spatial domain, a novel hierarchical structure is
designed, which can hierarchically extract deep features from keyframes and
non-keyframes. In the temporal domain, a novel hierarchical interaction
mechanism is proposed, which can cooperatively learn the correlations among
different frames in the multiscale space. Extensive experiments manifest that
the proposed HIT-VCSNet outperforms the existing state-of-the-art video and
image CS methods in a large margin.",None,-1
2781e734-e9b2-4fc3-b732-22dea65beb14,Don't worry about mistakes! Glass Segmentation Network via Mistake Correction,0.0336317,1,"Recall one time when we were in an unfamiliar mall. We might mistakenly think
that there exists or does not exist a piece of glass in front of us. Such
mistakes will remind us to walk more safely and freely at the same or a similar
place next time. To absorb the human mistake correction wisdom, we propose a
novel glass segmentation network to detect transparent glass, dubbed
GlassSegNet. Motivated by this human behavior, GlassSegNet utilizes two key
stages: the identification stage (IS) and the correction stage (CS). The IS is
designed to simulate the detection procedure of human recognition for
identifying transparent glass by global context and edge information. The CS
then progressively refines the coarse prediction by correcting mistake regions
based on gained experience. Extensive experiments show clear improvements of
our GlassSegNet over thirty-four state-of-the-art methods on three benchmark
datasets.",None,-1
9c29e030-c199-48db-b5af-8fda867ad177,The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings,0.5704,7,"Numerous works use word embedding-based metrics to quantify societal biases
and stereotypes in texts. Recent studies have found that word embeddings can
capture semantic similarity but may be affected by word frequency. In this work
we study the effect of frequency when measuring female vs. male gender bias
with word embedding-based bias quantification methods. We find that Skip-gram
with negative sampling and GloVe tend to detect male bias in high frequency
words, while GloVe tends to return female bias in low frequency words. We show
these behaviors still exist when words are randomly shuffled. This proves that
the frequency-based effect observed in unshuffled corpora stems from properties
of the metric rather than from word associations. The effect is spurious and
problematic since bias metrics should depend exclusively on word co-occurrences
and not individual word frequencies. Finally, we compare these results with the
ones obtained with an alternative metric based on Pointwise Mutual Information.
We find that this metric does not show a clear dependence on frequency, even
though it is slightly skewed towards male bias across all frequencies.",None,-1
1a42a218-9cfc-4f46-b66b-2033ff227b62,Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models,0.0154914,2,"This project focuses on enhancing open-source large language models through
instruction-tuning and providing comprehensive evaluations of their
performance. We explore how various training data factors, such as quantity,
quality, and linguistic distribution, influence the performance of
instruction-tuned models trained on publicly accessible high-quality
instruction datasets for both English and Chinese languages. Our goal is to
supplement evaluation with quantitative analyses, providing valuable insights
for the continued advancement of open-source chat models. Our model, data, and
code are publicly available for others to use and build upon.",None,-1
45ef27f1-66b4-4dda-b044-8d54611e1666,Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models,0.97742,13,"Existing dialogue models may encounter scenarios which are not
well-represented in the training data, and as a result generate responses that
are unnatural, inappropriate, or unhelpful. We propose the ""Ask an Expert""
framework in which the model is trained with access to an ""expert"" which it can
consult at each turn. Advice is solicited via a structured dialogue with the
expert, and the model is optimized to selectively utilize (or ignore) it given
the context and dialogue history. In this work the expert takes the form of an
LLM. We evaluate this framework in a mental health support domain, where the
structure of the expert conversation is outlined by pre-specified prompts which
reflect a reasoning strategy taught to practitioners in the field. Blenderbot
models utilizing ""Ask an Expert"" show quality improvements across all expert
sizes, including those with fewer parameters than the dialogue model itself.
Our best model provides a $\sim 10\%$ improvement over baselines, approaching
human-level scores on ""engingingness"" and ""helpfulness"" metrics.",None,-1
878a5534-93c5-45f5-bdff-c7ffe6ff8738,3D-aware Image Generation using 2D Diffusion Models,0.914092,43,"In this paper, we introduce a novel 3D-aware image generation method that
leverages 2D diffusion models. We formulate the 3D-aware image generation task
as multiview 2D image set generation, and further to a sequential
unconditional-conditional multiview image generation process. This allows us to
utilize 2D diffusion models to boost the generative modeling power of the
method. Additionally, we incorporate depth information from monocular depth
estimators to construct the training data for the conditional diffusion model
using only still images. We train our method on a large-scale dataset, i.e.,
ImageNet, which is not addressed by previous methods. It produces high-quality
images that significantly outperform prior methods. Furthermore, our approach
showcases its capability to generate instances with large view angles, even
though the training images are diverse and unaligned, gathered from
""in-the-wild"" real-world environments.",None,-1
6860e7ff-3310-4d6a-902f-d2a94e225f82,Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition,0.610264,4,"The development of robust, generalized models in human activity recognition
(HAR) has been hindered by the scarcity of large-scale, labeled data sets.
Recent work has shown that virtual IMU data extracted from videos using
computer vision techniques can lead to substantial performance improvements
when training HAR models combined with small portions of real IMU data.
Inspired by recent advances in motion synthesis from textual descriptions and
connecting Large Language Models (LLMs) to various AI models, we introduce an
automated pipeline that first uses ChatGPT to generate diverse textual
descriptions of activities. These textual descriptions are then used to
generate 3D human motion sequences via a motion synthesis model, T2M-GPT, and
later converted to streams of virtual IMU data. We benchmarked our approach on
three HAR datasets (RealWorld, PAMAP2, and USC-HAD) and demonstrate that the
use of virtual IMU training data generated using our new approach leads to
significantly improved HAR model performance compared to only using real IMU
data. Our approach contributes to the growing field of cross-modality transfer
methods and illustrate how HAR models can be improved through the generation of
virtual training data that do not require any manual effort.",None,-1
1ea90055-1227-41ed-b1cd-b1116f901dd9,AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator,0.884432,3,"Designing robust machine learning systems remains an open problem, and there
is a need for benchmark problems that cover both environmental changes and
evaluation on a downstream task. In this work, we introduce AVOIDDS, a
realistic object detection benchmark for the vision-based aircraft
detect-and-avoid problem. We provide a labeled dataset consisting of 72,000
photorealistic images of intruder aircraft with various lighting conditions,
weather conditions, relative geometries, and geographic locations. We also
provide an interface that evaluates trained models on slices of this dataset to
identify changes in performance with respect to changing environmental
conditions. Finally, we implement a fully-integrated, closed-loop simulator of
the vision-based detect-and-avoid problem to evaluate trained models with
respect to the downstream collision avoidance task. This benchmark will enable
further research in the design of robust machine learning systems for use in
safety-critical applications. The AVOIDDS dataset and code are publicly
available at https://purl.stanford.edu/hj293cv5980 and
https://github.com/sisl/VisionBasedAircraftDAA respectively.",None,-1
46c7d706-f164-4986-bae7-fb4a27be18d8,Deception Detection with Feature-Augmentation by soft Domain Transfer,0.107104,4,"In this era of information explosion, deceivers use different domains or
mediums of information to exploit the users, such as News, Emails, and Tweets.
Although numerous research has been done to detect deception in all these
domains, information shortage in a new event necessitates these domains to
associate with each other to battle deception. To form this association, we
propose a feature augmentation method by harnessing the intermediate layer
representation of neural models. Our approaches provide an improvement over the
self-domain baseline models by up to 6.60%. We find Tweets to be the most
helpful information provider for Fake News and Phishing Email detection,
whereas News helps most in Tweet Rumor detection. Our analysis provides a
useful insight for domain knowledge transfer which can help build a stronger
deception detection system than the existing literature.",None,-1
28ff21b0-3dae-487f-a9cf-c8c1ef3a04df,Optimization-Based Eye Tracking using Deflectometric Information,0.766444,5,"Eye tracking is an important tool with a wide range of applications in
Virtual, Augmented, and Mixed Reality (VR/AR/MR) technologies. State-of-the-art
eye tracking methods are either reflection-based and track reflections of
sparse point light sources, or image-based and exploit 2D features of the
acquired eye image. In this work, we attempt to significantly improve
reflection-based methods by utilizing pixel-dense deflectometric surface
measurements in combination with optimization-based inverse rendering
algorithms. Utilizing the known geometry of our deflectometric setup, we
develop a differentiable rendering pipeline based on PyTorch3D that simulates a
virtual eye under screen illumination. Eventually, we exploit the
image-screen-correspondence information from the captured measurements to find
the eye's rotation, translation, and shape parameters with our renderer via
gradient descent. In general, our method does not require a specific pattern
and can work with ordinary video frames of the main VR/AR/MR screen itself. We
demonstrate real-world experiments with evaluated mean relative gaze errors
below 0.45 degrees at a precision better than 0.11 degrees. Moreover, we show
an improvement of 6X over a representative reflection-based state-of-the-art
method in simulation.",None,-1
eb7600fc-8328-4c95-b720-4ae2d70e6b30,LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery,0.643008,12,"Large Language Models (LLMs) have transformed the landscape of artificial
intelligence, while their enormous size presents significant challenges in
terms of computational costs. We introduce LoRAShear, a novel efficient
approach to structurally prune LLMs and recover knowledge. Given general LLMs,
LoRAShear at first creates the dependency graphs over LoRA modules to discover
minimally removal structures and analyze the knowledge distribution. It then
proceeds progressive structured pruning on LoRA adaptors and enables inherent
knowledge transfer to better preserve the information in the redundant
structures. To recover the lost knowledge during pruning, LoRAShear
meticulously studies and proposes a dynamic fine-tuning schemes with dynamic
data adaptors to effectively narrow down the performance gap to the full
models. Numerical results demonstrate that by only using one GPU within a
couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with
only 1.0% performance degradation and significantly outperforms
state-of-the-arts. The source code will be available at
https://github.com/microsoft/lorashear.",None,-1
32e56e46-0169-4edc-b8b1-1b2753826221,Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal of Raindrops and Rain Streaks,0.76322,7,"In the real world, image degradations caused by rain often exhibit a
combination of rain streaks and raindrops, thereby increasing the challenges of
recovering the underlying clean image. Note that the rain streaks and raindrops
have diverse shapes, sizes, and locations in the captured image, and thus
modeling the correlation relationship between irregular degradations caused by
rain artifacts is a necessary prerequisite for image deraining. This paper aims
to present an efficient and flexible mechanism to learn and model degradation
relationships in a global view, thereby achieving a unified removal of
intricate rain scenes. To do so, we propose a Sparse Sampling Transformer based
on Uncertainty-Driven Ranking, dubbed UDR-S2Former. Compared to previous
methods, our UDR-S2Former has three merits. First, it can adaptively sample
relevant image degradation information to model underlying degradation
relationships. Second, explicit application of the uncertainty-driven ranking
strategy can facilitate the network to attend to degradation features and
understand the reconstruction process. Finally, experimental results show that
our UDR-S2Former clearly outperforms state-of-the-art methods for all
benchmarks.",None,-1
10b0403a-11f7-4635-b6e6-ba57e04f1380,Continual Detection Transformer for Incremental Object Detection,0.706437,26,"Incremental object detection (IOD) aims to train an object detector in
phases, each with annotations for new object categories. As other incremental
settings, IOD is subject to catastrophic forgetting, which is often addressed
by techniques such as knowledge distillation (KD) and exemplar replay (ER).
However, KD and ER do not work well if applied directly to state-of-the-art
transformer-based object detectors such as Deformable DETR and UP-DETR. In this
paper, we solve these issues by proposing a ContinuaL DEtection TRansformer
(CL-DETR), a new method for transformer-based IOD which enables effective usage
of KD and ER in this context. First, we introduce a Detector Knowledge
Distillation (DKD) loss, focusing on the most informative and reliable
predictions from old versions of the model, ignoring redundant background
predictions, and ensuring compatibility with the available ground-truth labels.
We also improve ER by proposing a calibration strategy to preserve the label
distribution of the training set, therefore better matching training and
testing statistics. We conduct extensive experiments on COCO 2017 and
demonstrate that CL-DETR achieves state-of-the-art results in the IOD setting.",None,-1
250114e6-1926-44a5-94d2-61ff65e77c2e,"Contrast, Stylize and Adapt: Unsupervised Contrastive Learning Framework for Domain Adaptive Semantic Segmentation",0.563396,6,"To overcome the domain gap between synthetic and real-world datasets,
unsupervised domain adaptation methods have been proposed for semantic
segmentation. Majority of the previous approaches have attempted to reduce the
gap either at the pixel or feature level, disregarding the fact that the two
components interact positively. To address this, we present CONtrastive FEaTure
and pIxel alignment (CONFETI) for bridging the domain gap at both the pixel and
feature levels using a unique contrastive formulation. We introduce
well-estimated prototypes by including category-wise cross-domain information
to link the two alignments: the pixel-level alignment is achieved using the
jointly trained style transfer module with the prototypical semantic
consistency, while the feature-level alignment is enforced to cross-domain
features with the \textbf{pixel-to-prototype contrast}. Our extensive
experiments demonstrate that our method outperforms existing state-of-the-art
methods using DeepLabV2. Our code is available at
https://github.com/cxa9264/CONFETI",None,-1
977e776b-20df-4810-a1aa-928884e552f8,Prompt-Guided Retrieval Augmentation for Non-Knowledge-Intensive Tasks,0.364702,7,"Retrieval-augmented methods have received increasing attention to support
downstream tasks by leveraging useful information from external resources.
Recent studies mainly focus on exploring retrieval to solve knowledge-intensive
(KI) tasks. However, the potential of retrieval for most
non-knowledge-intensive (NKI) tasks remains under-explored. There are two main
challenges to leveraging retrieval-augmented methods for NKI tasks: 1) the
demand for diverse relevance score functions and 2) the dilemma between
training cost and task performance. To address these challenges, we propose a
two-stage framework for NKI tasks, named PGRA. In the first stage, we adopt a
task-agnostic retriever to build a shared static index and select candidate
evidence efficiently. In the second stage, we design a prompt-guided reranker
to rerank the nearest evidence according to task-specific relevance for the
reader. Experimental results show that PGRA outperforms other state-of-the-art
retrieval-augmented methods. Our analyses further investigate the influence
factors to model performance and demonstrate the generality of PGRA. Codes are
available at https://github.com/THUNLP-MT/PGRA.",None,-1
053617b5-4e95-4d65-bcd2-cb44d2de2507,Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning,0.339732,6,"Dialogue state tracking (DST) is an important step in dialogue management to
keep track of users' beliefs. Existing works fine-tune all language model (LM)
parameters to tackle the DST task, which requires significant data and
computing resources for training and hosting. The cost grows exponentially in
the real-world deployment where dozens of fine-tuned LM are used for different
domains and tasks. To reduce parameter size and better utilize cross-task
shared information, we propose to use soft prompt token embeddings to learn
task properties. Without tuning LM parameters, our method drastically reduces
the number of parameters needed to less than 0.5% of prior works while achieves
better low-resource DST performance.",None,-1
056800a5-aefc-4d81-be6a-b7a2de7bb8a4,The Flan Collection: Designing Data and Methods for Effective Instruction Tuning,0.985041,381,"We study the design decisions of publicly available instruction tuning
methods, and break down the development of Flan 2022 (Chung et al., 2022).
Through careful ablation studies on the Flan Collection of tasks and methods,
we tease apart the effect of design decisions which enable Flan-T5 to
outperform prior work by 3-17%+ across evaluation settings. We find task
balancing and enrichment techniques are overlooked but critical to effective
instruction tuning, and in particular, training with mixed prompt settings
(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)
performance in all settings. In further experiments, we show Flan-T5 requires
less finetuning to converge higher and faster than T5 on single downstream
tasks, motivating instruction-tuned models as more computationally-efficient
starting checkpoints for new tasks. Finally, to accelerate research on
instruction tuning, we make the Flan 2022 collection of datasets, templates,
and methods publicly available at
https://github.com/google-research/FLAN/tree/main/flan/v2.",None,-1
17f85a9b-2def-4c94-a058-d60efb1914fd,What makes a good pause? Investigating the turn-holding effects of fillers,0.442971,3,"Filled pauses (or fillers), such as ""uh"" and ""um"", are frequent in
spontaneous speech and can serve as a turn-holding cue for the listener,
indicating that the current speaker is not done yet. In this paper, we use the
recently proposed Voice Activity Projection (VAP) model, which is a deep
learning model trained to predict the dynamics of conversation, to analyse the
effects of filled pauses on the expected turn-hold probability. The results
show that, while filled pauses do indeed have a turn-holding effect, it is
perhaps not as strong as could be expected, probably due to the redundancy of
other cues. We also find that the prosodic properties and position of the
filler has a significant effect on the turn-hold probability. However, contrary
to what has been suggested in previous work, there is no difference between
""uh"" and ""um"" in this regard.",None,-1
a90e7561-52e0-4713-9ede-e48267cfc457,Time Series Analysis of Urban Liveability,0.148162,1,"In this paper we explore deep learning models to monitor longitudinal
liveability changes in Dutch cities at the neighbourhood level. Our liveability
reference data is defined by a country-wise yearly survey based on a set of
indicators combined into a liveability score, the Leefbaarometer. We pair this
reference data with yearly-available high-resolution aerial images, which
creates yearly timesteps at which liveability can be monitored. We deploy a
convolutional neural network trained on an aerial image from 2016 and the
Leefbaarometer score to predict liveability at new timesteps 2012 and 2020. The
results in a city used for training (Amsterdam) and one never seen during
training (Eindhoven) show some trends which are difficult to interpret,
especially in light of the differences in image acquisitions at the different
time steps. This demonstrates the complexity of liveability monitoring across
time periods and the necessity for more sophisticated methods compensating for
changes unrelated to liveability dynamics.",None,-1
6acc5b5b-6207-4321-8ebe-d8f2cb96c181,Align and Attend: Multimodal Summarization with Dual Contrastive Losses,0.449563,25,"The goal of multimodal summarization is to extract the most important
information from different modalities to form output summaries. Unlike the
unimodal summarization, the multimodal summarization task explicitly leverages
cross-modal information to help generate more reliable and high-quality
summaries. However, existing methods fail to leverage the temporal
correspondence between different modalities and ignore the intrinsic
correlation between different samples. To address this issue, we introduce
Align and Attend Multimodal Summarization (A2Summ), a unified multimodal
transformer-based model which can effectively align and attend the multimodal
input. In addition, we propose two novel contrastive losses to model both
inter-sample and intra-sample correlations. Extensive experiments on two
standard video summarization datasets (TVSum and SumMe) and two multimodal
summarization datasets (Daily Mail and CNN) demonstrate the superiority of
A2Summ, achieving state-of-the-art performances on all datasets. Moreover, we
collected a large-scale multimodal summarization dataset BLiSS, which contains
livestream videos and transcribed texts with annotated summaries. Our code and
dataset are publicly available at ~\url{https://boheumd.github.io/A2Summ/}.",None,-1
4b4b67ee-0d63-46b7-99d5-740dd3f5a884,GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations,0.420522,4,"Diverse explainability methods of graph neural networks (GNN) have recently
been developed to highlight the edges and nodes in the graph that contribute
the most to the model predictions. However, it is not clear yet how to evaluate
the correctness of those explanations, whether it is from a human or a model
perspective. One unaddressed bottleneck in the current evaluation procedure is
the problem of out-of-distribution explanations, whose distribution differs
from those of the training data. This important issue affects existing
evaluation metrics such as the popular faithfulness or fidelity score. In this
paper, we show the limitations of faithfulness metrics. We propose GInX-Eval
(Graph In-distribution eXplanation Evaluation), an evaluation procedure of
graph explanations that overcomes the pitfalls of faithfulness and offers new
insights on explainability methods. Using a fine-tuning strategy, the GInX
score measures how informative removed edges are for the model and the EdgeRank
score evaluates if explanatory edges are correctly ordered by their importance.
GInX-Eval verifies if ground-truth explanations are instructive to the GNN
model. In addition, it shows that many popular methods, including
gradient-based methods, produce explanations that are not better than a random
designation of edges as important subgraphs, challenging the findings of
current works in the area. Results with GInX-Eval are consistent across
multiple datasets and align with human evaluation.",None,-1
6c2a8f3e-08b0-407a-a99a-41c05bbfb128,Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,0.535441,5,"Disagreement in natural language annotation has mostly been studied from a
perspective of biases introduced by the annotators and the annotation
frameworks. Here, we propose to analyze another source of bias: task design
bias, which has a particularly strong impact on crowdsourced linguistic
annotations where natural language is used to elicit the interpretation of
laymen annotators. For this purpose we look at implicit discourse relation
annotation, a task that has repeatedly been shown to be difficult due to the
relations' ambiguity. We compare the annotations of 1,200 discourse relations
obtained using two distinct annotation tasks and quantify the biases of both
methods across four different domains. Both methods are natural language
annotation tasks designed for crowdsourcing. We show that the task design can
push annotators towards certain relations and that some discourse relations
senses can be better elicited with one or the other annotation approach. We
also conclude that this type of bias should be taken into account when training
and testing models.",None,-1
b034f7f9-5793-47d7-8ae7-15990bd4ec2e,Instant Continual Learning of Neural Radiance Fields,0.349591,4,"Neural radiance fields (NeRFs) have emerged as an effective method for
novel-view synthesis and 3D scene reconstruction. However, conventional
training methods require access to all training views during scene
optimization. This assumption may be prohibitive in continual learning
scenarios, where new data is acquired in a sequential manner and a continuous
update of the NeRF is desired, as in automotive or remote sensing applications.
When naively trained in such a continual setting, traditional scene
representation frameworks suffer from catastrophic forgetting, where previously
learned knowledge is corrupted after training on new data. Prior works in
alleviating forgetting with NeRFs suffer from low reconstruction quality and
high latency, making them impractical for real-world application. We propose a
continual learning framework for training NeRFs that leverages replay-based
methods combined with a hybrid explicit--implicit scene representation. Our
method outperforms previous methods in reconstruction quality when trained in a
continual setting, while having the additional benefit of being an order of
magnitude faster.",None,-1
439b36ca-c2d7-417c-a628-f99845bfc799,Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean,0.439939,2,"We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire
modeling in the Mediterranean. Mesogeos integrates variables representing
wildfire drivers (meteorology, vegetation, human activity) and historical
records of wildfire ignitions and burned areas for 17 years (2006-2022). It is
designed as a cloud-friendly spatio-temporal dataset, namely a datacube,
harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The
datacube structure offers opportunities to assess machine learning (ML) usage
in various wildfire modeling tasks. We extract two ML-ready datasets that
establish distinct tracks to demonstrate this potential: (1) short-term
wildfire danger forecasting and (2) final burned area estimation given the
point of ignition. We define appropriate metrics and baselines to evaluate the
performance of models in each track. By publishing the datacube, along with the
code to create the ML datasets and models, we encourage the community to foster
the implementation of additional tracks for mitigating the increasing threat of
wildfires in the Mediterranean.",None,-1
280f9192-c370-4c39-8808-41f07f7578b9,Accurate prediction of international trade flows: Leveraging knowledge graphs and their embeddings,0.976039,4,"Knowledge representation (KR) is vital in designing symbolic notations to
represent real-world facts and facilitate automated decision-making tasks.
Knowledge graphs (KGs) have emerged so far as a popular form of KR, offering a
contextual and human-like representation of knowledge. In international
economics, KGs have proven valuable in capturing complex interactions between
commodities, companies, and countries. By putting the gravity model, which is a
common economic framework, into the process of building KGs, important factors
that affect trade relationships can be taken into account, making it possible
to predict international trade patterns. This paper proposes an approach that
leverages Knowledge Graph embeddings for modeling international trade, focusing
on link prediction using embeddings. Thus, valuable insights are offered to
policymakers, businesses, and economists, enabling them to anticipate the
effects of changes in the international trade system. Moreover, the integration
of traditional machine learning methods with KG embeddings, such as decision
trees and graph neural networks are also explored. The research findings
demonstrate the potential for improving prediction accuracy and provide
insights into embedding explainability in knowledge representation. The paper
also presents a comprehensive analysis of the influence of embedding methods on
other intelligent algorithms.",None,-1
277c58ac-f44c-4531-b5f4-0fb613d9ad6b,Feature Mixing for Writer Retrieval and Identification on Papyri Fragments,0.614179,1,"This paper proposes a deep-learning-based approach to writer retrieval and
identification for papyri, with a focus on identifying fragments associated
with a specific writer and those corresponding to the same image. We present a
novel neural network architecture that combines a residual backbone with a
feature mixing stage to improve retrieval performance, and the final descriptor
is derived from a projection layer. The methodology is evaluated on two
benchmarks: PapyRow, where we achieve a mAP of 26.6 % and 24.9 % on writer and
page retrieval, and HisFragIR20, showing state-of-the-art performance (44.0 %
and 29.3 % mAP). Furthermore, our network has an accuracy of 28.7 % for writer
identification. Additionally, we conduct experiments on the influence of two
binarization techniques on fragments and show that binarizing does not enhance
performance. Our code and models are available to the community.",None,-1
36c0fb15-90b5-4e82-a7d6-66c8a39a0050,Inspecting and Editing Knowledge Representations in Language Models,0.977738,45,"Neural language models (LMs) represent facts about the world described by
text. Sometimes these facts derive from training data (in most LMs, a
representation of the word ""banana"" encodes the fact that bananas are fruits).
Sometimes facts derive from input text itself (a representation of the sentence
""I poured out the bottle"" encodes the fact that the bottle became empty). We
describe REMEDI, a method for learning to map statements in natural language to
fact encodings in an LM's internal representation system. REMEDI encodings can
be used as knowledge editors: when added to LM hidden representations, they
modify downstream generation to be consistent with new facts. REMEDI encodings
may also be used as probes: when compared to LM representations, they reveal
which properties LMs already attribute to mentioned entities, in some cases
making it possible to predict when LMs will generate outputs that conflict with
background knowledge or input text. REMEDI thus links work on probing,
prompting, and LM editing, and offers steps toward general tools for
fine-grained inspection and control of knowledge in LMs.",None,-1
e184f202-723b-4478-ba5c-4f1b694a169c,Deep Learning-based Eye-Tracking Analysis for Diagnosis of Alzheimer's Disease Using 3D Comprehensive Visual Stimuli,0.307483,3,"Alzheimer's Disease (AD) causes a continuous decline in memory, thinking, and
judgment. Traditional diagnoses are usually based on clinical experience, which
is limited by some realistic factors. In this paper, we focus on exploiting
deep learning techniques to diagnose AD based on eye-tracking behaviors. Visual
attention, as typical eye-tracking behavior, is of great clinical value to
detect cognitive abnormalities in AD patients. To better analyze the
differences in visual attention between AD patients and normals, we first
conduct a 3D comprehensive visual task on a non-invasive eye-tracking system to
collect visual attention heatmaps. We then propose a multi-layered comparison
convolution neural network (MC-CNN) to distinguish the visual attention
differences between AD patients and normals. In MC-CNN, the multi-layered
representations of heatmaps are obtained by hierarchical convolution to better
encode eye-movement behaviors, which are further integrated into a distance
vector to benefit the comprehensive visual task. Extensive experimental results
on the collected dataset demonstrate that MC-CNN achieves consistent validity
in classifying AD patients and normals with eye-tracking data.",None,-1
e2bab82d-7699-4e6d-84b6-df8052b128a9,Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss,0.0903678,1,"Recently, denoising methods based on supervised learning have exhibited
promising performance. However, their reliance on external datasets containing
noisy-clean image pairs restricts their applicability. To address this
limitation, researchers have focused on training denoising networks using
solely a set of noisy inputs. To improve the feasibility of denoising
procedures, in this study, we proposed a single-image self-supervised learning
method in which only the noisy input image is used for network training. Gated
convolution was used for feature extraction and no-reference image quality
assessment was used for guiding the training process. Moreover, the proposed
method sampled instances from the input image dataset using Bernoulli sampling
with a certain dropout rate for training. The corresponding result was produced
by averaging the generated predictions from various instances of the trained
network with dropouts. The experimental results indicated that the proposed
method achieved state-of-the-art denoising performance on both synthetic and
real-world datasets. This highlights the effectiveness and practicality of our
method as a potential solution for various noise removal tasks.",None,-1
41807d98-e3fa-443d-a883-5e636934e6b2,DDRF: Denoising Diffusion Model for Remote Sensing Image Fusion,0.770817,9,"Denosing diffusion model, as a generative model, has received a lot of
attention in the field of image generation recently, thanks to its powerful
generation capability. However, diffusion models have not yet received
sufficient research in the field of image fusion. In this article, we introduce
diffusion model to the image fusion field, treating the image fusion task as
image-to-image translation and designing two different conditional injection
modulation modules (i.e., style transfer modulation and wavelet modulation) to
inject coarse-grained style information and fine-grained high-frequency and
low-frequency information into the diffusion UNet, thereby generating fused
images. In addition, we also discussed the residual learning and the selection
of training objectives of the diffusion model in the image fusion task.
Extensive experimental results based on quantitative and qualitative
assessments compared with benchmarks demonstrates state-of-the-art results and
good generalization performance in image fusion tasks. Finally, it is hoped
that our method can inspire other works and gain insight into this field to
better apply the diffusion model to image fusion tasks. Code shall be released
for better reproducibility.",None,-1
f535710c-5332-49ac-b25f-906302e935bc,Can Language Models Laugh at YouTube Short-form Videos?,0.288684,3,"As short-form funny videos on social networks are gaining popularity, it
becomes demanding for AI models to understand them for better communication
with humans. Unfortunately, previous video humor datasets target specific
domains, such as speeches or sitcoms, and mostly focus on verbal cues. We
curate a user-generated dataset of 10K multimodal funny videos from YouTube,
called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both
verbal and visual elements contributing to humor. After filtering, we annotate
each video with timestamps and text explanations for funny moments. Our
ExFunTube is unique over existing datasets in that our videos cover a wide
range of domains with various types of humor that necessitate a multimodal
understanding of the content. Also, we develop a zero-shot video-to-text
prompting to maximize video humor understanding of large language models
(LLMs). With three different evaluation methods using automatic scores,
rationale quality experiments, and human evaluations, we show that our
prompting significantly improves LLMs' ability for humor explanation.",None,-1
a4cb1b41-7c0e-4ffe-b635-8abbbcb43837,Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition,0.974076,18,"We introduce a new cross-modal fusion technique designed for generative error
correction in automatic speech recognition (ASR). Our methodology leverages
both acoustic information and external linguistic representations to generate
accurate speech transcription contexts. This marks a step towards a fresh
paradigm in generative error correction within the realm of n-best hypotheses.
Unlike the existing ranking-based rescoring methods, our approach adeptly uses
distinct initialization techniques and parameter-efficient algorithms to boost
ASR performance derived from pre-trained speech and text models. Through
evaluation across diverse ASR datasets, we evaluate the stability and
reproducibility of our fusion technique, demonstrating its improved word error
rate relative (WERR) performance in comparison to n-best hypotheses by
relatively 37.66%. To encourage future research, we have made our code and
pre-trained models open source at
https://github.com/Srijith-rkr/Whispering-LLaMA.",None,-1
3ee626dd-efcb-4422-be39-da0d68561f40,LayoutDM: Discrete Diffusion Model for Controllable Layout Generation,0.489803,45,"Controllable layout generation aims at synthesizing plausible arrangement of
element bounding boxes with optional constraints, such as type or position of a
specific element. In this work, we try to solve a broad range of layout
generation tasks in a single model that is based on discrete state-space
diffusion models. Our model, named LayoutDM, naturally handles the structured
layout data in the discrete representation and learns to progressively infer a
noiseless layout from the initial input, where we model the layout corruption
process by modality-wise discrete diffusion. For conditional generation, we
propose to inject layout constraints in the form of masking or logit adjustment
during inference. We show in the experiments that our LayoutDM successfully
generates high-quality layouts and outperforms both task-specific and
task-agnostic baselines on several layout tasks.",None,-1
c541c37d-008e-4a6f-bcd6-8b9012181b8e,Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse,0.503415,1,"Online violence against children has increased globally recently, demanding
urgent attention. Competent authorities manually analyze abuse complaints to
comprehend crime dynamics and identify patterns. However, the manual analysis
of these complaints presents a challenge because it exposes analysts to harmful
content during the review process. Given these challenges, we present a novel
solution, an automated tool designed to analyze children's sexual abuse reports
comprehensively. By automating the analysis process, our tool significantly
reduces the risk of exposure to harmful content by categorizing the reports on
three dimensions: Subject, Degree of Criminality, and Damage. Furthermore,
leveraging our multidisciplinary team's expertise, we introduce a novel
approach to annotate the collected data, enabling a more in-depth analysis of
the reports. This approach improves the comprehension of fundamental patterns
and trends, enabling law enforcement agencies and policymakers to create
focused strategies in the fight against children's violence.",None,-1
8118a535-0710-4fa9-8c35-e4050a8cb19d,ZeroPose: CAD-Model-based Zero-Shot Pose Estimation,0.540853,7,"In this paper, we present a CAD model-based zero-shot pose estimation
pipeline called ZeroPose. Existing pose estimation methods remain to require
expensive training when applied to an unseen object, which greatly hinders
their scalability in the practical application of industry. In contrast, the
proposed method enables the accurate estimation of pose parameters for
previously unseen objects without the need for training. Specifically, we
design a two-step pipeline consisting of CAD model-based zero-shot instance
segmentation and a zero-shot pose estimator. For the first step, there is a
simple but effective way to leverage CAD models and visual foundation models
SAM and Imagebind to segment the interest unseen object at the instance level.
For the second step, we based on the intensive geometric information in the CAD
model of the rigid object to propose a lightweight hierarchical geometric
structure matching mechanism achieving zero-shot pose estimation. Extensive
experimental results on the seven core datasets on the BOP challenge show that
the proposed zero-shot instance segmentation methods achieve comparable
performance with supervised MaskRCNN and the zero-shot pose estimation results
outperform the SOTA pose estimators with better efficiency.",None,-1
740d9031-1297-4b8a-9baf-a039aa842aad,Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems,0.748593,3,"In this article, a benchmark for real-world bin packing problems is proposed.
This dataset consists of 12 instances of varying levels of complexity regarding
size (with the number of packages ranging from 38 to 53) and user-defined
requirements. In fact, several real-world-oriented restrictions were taken into
account to build these instances: i) item and bin dimensions, ii) weight
restrictions, iii) affinities among package categories iv) preferences for
package ordering and v) load balancing. Besides the data, we also offer an own
developed Python script for the dataset generation, coined Q4RealBPP-DataGen.
The benchmark was initially proposed to evaluate the performance of quantum
solvers. Therefore, the characteristics of this set of instances were designed
according to the current limitations of quantum devices. Additionally, the
dataset generator is included to allow the construction of general-purpose
benchmarks. The data introduced in this article provides a baseline that will
encourage quantum computing researchers to work on real-world bin packing
problems.",None,-1
7b018027-b0cf-4397-98f7-f1be30bef9f5,I3D: Transformer architectures with input-dependent dynamic depth for speech recognition,0.539909,9,"Transformer-based end-to-end speech recognition has achieved great success.
However, the large footprint and computational overhead make it difficult to
deploy these models in some real-world applications. Model compression
techniques can reduce the model size and speed up inference, but the compressed
model has a fixed architecture which might be suboptimal. We propose a novel
Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong
performance-efficiency trade-offs. With a similar number of layers at inference
time, I3D-based models outperform the vanilla Transformer and the static pruned
model via iterative layer pruning. We also present interesting analysis on the
gate probabilities and the input-dependency, which helps us better understand
deep encoders.",None,-1
aa2e9983-d371-4462-907c-b9626473c35a,GCRE-GPT: A Generative Model for Comparative Relation Extraction,0.182755,1,"Given comparative text, comparative relation extraction aims to extract two
targets (\eg two cameras) in comparison and the aspect they are compared for
(\eg image quality). The extracted comparative relations form the basis of
further opinion analysis.Existing solutions formulate this task as a sequence
labeling task, to extract targets and aspects. However, they cannot directly
extract comparative relation(s) from text. In this paper, we show that
comparative relations can be directly extracted with high accuracy, by
generative model. Based on GPT-2, we propose a Generation-based Comparative
Relation Extractor (GCRE-GPT). Experiment results show that \modelname achieves
state-of-the-art accuracy on two datasets.",None,-1
a06aa00b-bb86-418d-b53e-7087d4e7d493,Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding,0.678847,27,"To tackle the high inference latency exhibited by autoregressive language
models, previous studies have proposed an early-exiting framework that
allocates adaptive computation paths for each token based on the complexity of
generating the subsequent token. However, we observed several shortcomings,
including performance degradation caused by a state copying mechanism or
numerous exit paths, and sensitivity to exit confidence thresholds.
Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework,
which incorporates a shallow-deep module and a synchronized parallel decoding.
Our framework enables faster inference by synchronizing the decoding process of
the current token with previously stacked early-exited tokens. Furthermore, as
parallel decoding allows us to observe predictions from both shallow and deep
models, we present a novel adaptive threshold estimator that exploits a Beta
mixture model to determine suitable confidence thresholds. We empirically
demonstrated the superiority of our proposed framework on extensive generation
tasks.",None,-1
f9547b29-7c0f-43be-ba8d-7bdf81e02054,CoRL: Environment Creation and Management Focused on System Integration,0.0292539,1,"Existing reinforcement learning environment libraries use monolithic
environment classes, provide shallow methods for altering agent observation and
action spaces, and/or are tied to a specific simulation environment. The Core
Reinforcement Learning library (CoRL) is a modular, composable, and
hyper-configurable environment creation tool. It allows minute control over
agent observations, rewards, and done conditions through the use of
easy-to-read configuration files, pydantic validators, and a functor design
pattern. Using integration pathways allows agents to be quickly implemented in
new simulation environments, encourages rapid exploration, and enables
transition of knowledge from low-fidelity to high-fidelity simulations.
Natively multi-agent design and integration with Ray/RLLib (Liang et al., 2018)
at release allow for easy scalability of agent complexity and computing power.
The code is publicly released and available at
https://github.com/act3-ace/CoRL.",None,-1
d57df2ca-6452-43f4-8105-de1c5deecd17,RuSentNE-2023: Evaluating Entity-Oriented Sentiment Analysis on Russian News Texts,0.507972,3,"The paper describes the RuSentNE-2023 evaluation devoted to targeted
sentiment analysis in Russian news texts. The task is to predict sentiment
towards a named entity in a single sentence. The dataset for RuSentNE-2023
evaluation is based on the Russian news corpus RuSentNE having rich
sentiment-related annotation. The corpus is annotated with named entities and
sentiments towards these entities, along with related effects and emotional
states. The evaluation was organized using the CodaLab competition framework.
The main evaluation measure was macro-averaged measure of positive and negative
classes. The best results achieved were of 66% Macro F-measure
(Positive+Negative classes). We also tested ChatGPT on the test set from our
evaluation and found that the zero-shot answers provided by ChatGPT reached 60%
of the F-measure, which corresponds to 4th place in the evaluation. ChatGPT
also provided detailed explanations of its conclusion. This can be considered
as quite high for zero-shot application.",None,-1
4c6fae79-c604-4fb3-a90d-5a9d58e21794,Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance,0.966006,14,"Category-level articulated object pose estimation aims to estimate a
hierarchy of articulation-aware object poses of an unseen articulated object
from a known category. To reduce the heavy annotations needed for supervised
learning methods, we present a novel self-supervised strategy that solves this
problem without any human labels. Our key idea is to factorize canonical shapes
and articulated object poses from input articulated shapes through part-level
equivariant shape analysis. Specifically, we first introduce the concept of
part-level SE(3) equivariance and devise a network to learn features of such
property. Then, through a carefully designed fine-grained pose-shape
disentanglement strategy, we expect that canonical spaces to support pose
estimation could be induced automatically. Thus, we could further predict
articulated object poses as per-part rigid transformations describing how parts
transform from their canonical part spaces to the camera space. Extensive
experiments demonstrate the effectiveness of our method on both complete and
partial point clouds from synthetic and real articulated object datasets.",None,-1
fc2e9f72-01af-43d4-8c0f-956c320a5d5e,UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity,0.738807,17,"Image reconstruction and captioning from brain activity evoked by visual
stimuli allow researchers to further understand the connection between the
human brain and the visual perception system. While deep generative models have
recently been employed in this field, reconstructing realistic captions and
images with both low-level details and high semantic fidelity is still a
challenging problem. In this work, we propose UniBrain: Unify Image
Reconstruction and Captioning All in One Diffusion Model from Human Brain
Activity. For the first time, we unify image reconstruction and captioning from
visual-evoked functional magnetic resonance imaging (fMRI) through a latent
diffusion model termed Versatile Diffusion. Specifically, we transform fMRI
voxels into text and image latent for low-level information and guide the
backward diffusion process through fMRI-based image and text conditions derived
from CLIP to generate realistic captions and images. UniBrain outperforms
current methods both qualitatively and quantitatively in terms of image
reconstruction and reports image captioning results for the first time on the
Natural Scenes Dataset (NSD) dataset. Moreover, the ablation experiments and
functional region-of-interest (ROI) analysis further exhibit the superiority of
UniBrain and provide comprehensive insight for visual-evoked brain decoding.",None,-1
263d513e-ae3e-4745-9e48-1307d89194b9,A Laplace-inspired Distribution on SO(3) for Probabilistic Rotation Estimation,0.773099,5,"Estimating the 3DoF rotation from a single RGB image is an important yet
challenging problem. Probabilistic rotation regression has raised more and more
attention with the benefit of expressing uncertainty information along with the
prediction. Though modeling noise using Gaussian-resembling Bingham
distribution and matrix Fisher distribution is natural, they are shown to be
sensitive to outliers for the nature of quadratic punishment to deviations. In
this paper, we draw inspiration from multivariate Laplace distribution and
propose a novel Rotation Laplace distribution on SO(3). Rotation Laplace
distribution is robust to the disturbance of outliers and enforces much
gradient to the low-error region, resulting in a better convergence. Our
extensive experiments show that our proposed distribution achieves
state-of-the-art performance for rotation regression tasks over both
probabilistic and non-probabilistic baselines. Our project page is at
https://pku-epic.github.io/RotationLaplace.",None,-1
11f2f03d-fb07-469b-beb9-afeefdedb32d,Thistle: A Vector Database in Rust,0.222669,1,"We present Thistle, a fully functional vector database. Thistle is an entry
into the domain of latent knowledge use in answering search queries, an ongoing
research topic at both start-ups and search engine companies. We implement
Thistle with several well-known algorithms, and benchmark results on the MS
MARCO dataset. Results help clarify the latent knowledge domain as well as the
growing Rust ML ecosystem.",None,-1
deee4bd0-68a1-4270-b56c-94b127bb7538,Internal-External Boundary Attention Fusion for Glass Surface Segmentation,0.623397,2,"Glass surfaces of transparent objects and mirrors are not able to be uniquely
and explicitly characterized by their visual appearances because they contain
the visual appearance of other reflected or transmitted surfaces as well.
Detecting glass regions from a single-color image is a challenging task. Recent
deep-learning approaches have paid attention to the description of glass
surface boundary where the transition of visual appearances between glass and
non-glass surfaces are observed. In this work, we analytically investigate how
glass surface boundary helps to characterize glass objects. Inspired by prior
semantic segmentation approaches with challenging image types such as X-ray or
CT scans, we propose separated internal-external boundary attention modules
that individually learn and selectively integrate visual characteristics of the
inside and outside region of glass surface from a single color image. Our
proposed method is evaluated on six public benchmarks comparing with
state-of-the-art methods showing promising results.",None,-1
981d7071-1399-417d-9c19-96383dd34cab,Contrastive Multi-Task Dense Prediction,0.371477,5,"This paper targets the problem of multi-task dense prediction which aims to
achieve simultaneous learning and inference on a bunch of multiple dense
prediction tasks in a single framework. A core objective in design is how to
effectively model cross-task interactions to achieve a comprehensive
improvement on different tasks based on their inherent complementarity and
consistency. Existing works typically design extra expensive distillation
modules to perform explicit interaction computations among different
task-specific features in both training and inference, bringing difficulty in
adaptation for different task sets, and reducing efficiency due to clearly
increased size of multi-task models. In contrast, we introduce feature-wise
contrastive consistency into modeling the cross-task interactions for
multi-task dense prediction. We propose a novel multi-task contrastive
regularization method based on the consistency to effectively boost the
representation learning of the different sub-tasks, which can also be easily
generalized to different multi-task dense prediction frameworks, and costs no
additional computation in the inference. Extensive experiments on two
challenging datasets (i.e. NYUD-v2 and Pascal-Context) clearly demonstrate the
superiority of the proposed multi-task contrastive learning approach for dense
predictions, establishing new state-of-the-art performances.",None,-1
a620f597-cbea-4cdc-9cc5-ef40faf5edf0,Grandma Karl is 27 years old -- research agenda for pseudonymization of research data,0.825978,4,"Accessibility of research data is critical for advances in many research
fields, but textual data often cannot be shared due to the personal and
sensitive information which it contains, e.g names or political opinions.
General Data Protection Regulation (GDPR) suggests pseudonymization as a
solution to secure open access to research data, but we need to learn more
about pseudonymization as an approach before adopting it for manipulation of
research data. This paper outlines a research agenda within pseudonymization,
namely need of studies into the effects of pseudonymization on unstructured
data in relation to e.g. readability and language assessment, as well as the
effectiveness of pseudonymization as a way of protecting writer identity, while
also exploring different ways of developing context-sensitive algorithms for
detection, labelling and replacement of personal information in unstructured
data. The recently granted project on pseudonymization Grandma Karl is 27 years
old addresses exactly those challenges.",None,-1
2f423118-f5b3-4922-8edc-6910cd858257,Cascading Hierarchical Networks with Multi-task Balanced Loss for Fine-grained hashing,0.05346,1,"With the explosive growth in the number of fine-grained images in the
Internet era, it has become a challenging problem to perform fast and efficient
retrieval from large-scale fine-grained images. Among the many retrieval
methods, hashing methods are widely used due to their high efficiency and small
storage space occupation. Fine-grained hashing is more challenging than
traditional hashing problems due to the difficulties such as low inter-class
variances and high intra-class variances caused by the characteristics of
fine-grained images. To improve the retrieval accuracy of fine-grained hashing,
we propose a cascaded network to learn compact and highly semantic hash codes,
and introduce an attention-guided data augmentation method. We refer to this
network as a cascaded hierarchical data augmentation network. We also propose a
novel approach to coordinately balance the loss of multi-task learning. We do
extensive experiments on some common fine-grained visual classification
datasets. The experimental results demonstrate that our proposed method
outperforms several state-of-art hashing methods and can effectively improve
the accuracy of fine-grained retrieval. The source code is publicly available:
https://github.com/kaiba007/FG-CNET.",None,-1
db066154-7e22-4a1b-b20e-302dcde3e0ae,UniCal: a Single-Branch Transformer-Based Model for Camera-to-LiDAR Calibration and Validation,0.631092,3,"We introduce a novel architecture, UniCal, for Camera-to-LiDAR (C2L)
extrinsic calibration which leverages self-attention mechanisms through a
Transformer-based backbone network to infer the 6-degree of freedom (DoF)
relative transformation between the sensors. Unlike previous methods, UniCal
performs an early fusion of the input camera and LiDAR data by aggregating
camera image channels and LiDAR mappings into a multi-channel unified
representation before extracting their features jointly with a single-branch
architecture. This single-branch architecture makes UniCal lightweight, which
is desirable in applications with restrained resources such as autonomous
driving. Through experiments, we show that UniCal achieves state-of-the-art
results compared to existing methods. We also show that through transfer
learning, weights learned on the calibration task can be applied to a
calibration validation task without re-training the backbone.",None,-1
8df75c13-0327-4bf7-adac-5824fa4fbaca,Towards Mitigating Hallucination in Large Language Models via Self-Reflection,0.940237,17,"Large language models (LLMs) have shown promise for generative and
knowledge-intensive tasks including question-answering (QA) tasks. However, the
practical deployment still faces challenges, notably the issue of
""hallucination"", where models generate plausible-sounding but unfaithful or
nonsensical information. This issue becomes particularly critical in the
medical domain due to the uncommon professional concepts and potential social
risks involved. This paper analyses the phenomenon of hallucination in medical
generative QA systems using widely adopted LLMs and datasets. Our investigation
centers on the identification and comprehension of common problematic answers,
with a specific emphasis on hallucination. To tackle this challenge, we present
an interactive self-reflection methodology that incorporates knowledge
acquisition and answer generation. Through this feedback process, our approach
steadily enhances the factuality, consistency, and entailment of the generated
answers. Consequently, we harness the interactivity and multitasking ability of
LLMs and produce progressively more precise and accurate answers. Experimental
results on both automatic and human evaluation demonstrate the superiority of
our approach in hallucination reduction compared to baselines.",None,-1
0a7364a1-578a-4ec2-9e3e-8bc0cf127302,AISYN: AI-driven Reinforcement Learning-Based Logic Synthesis Framework,0.145876,3,"Logic synthesis is one of the most important steps in design and
implementation of digital chips with a big impact on final Quality of Results
(QoR). For a most general input circuit modeled by a Directed Acyclic Graph
(DAG), many logic synthesis problems such as delay or area minimization are
NP-Complete, hence, no optimal solution is available. This is why many
classical logic optimization functions tend to follow greedy approaches that
are easily trapped in local minima that does not allow improving QoR as much as
needed. We believe that Artificial Intelligence (AI) and more specifically
Reinforcement Learning (RL) algorithms can help in solving this problem. This
is because AI and RL can help minimizing QoR further by exiting from local
minima. Our experiments on both open source and industrial benchmark circuits
show that significant improvements on important metrics such as area, delay,
and power can be achieved by making logic synthesis optimization functions
AI-driven. For example, our RL-based rewriting algorithm could improve total
cell area post-synthesis by up to 69.3% when compared to a classical rewriting
algorithm with no AI awareness.",None,-1
8215d7cf-8c15-4391-bdce-ea41c1c6f3ea,Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance,0.893414,3,"Robust obstacle avoidance is one of the critical steps for successful
goal-driven indoor navigation tasks.Due to the obstacle missing in the visual
image and the possible missed detection issue, visual image-based obstacle
avoidance techniques still suffer from unsatisfactory robustness. To mitigate
it, in this paper, we propose a novel implicit obstacle map-driven indoor
navigation framework for robust obstacle avoidance, where an implicit obstacle
map is learned based on the historical trial-and-error experience rather than
the visual image. In order to further improve the navigation efficiency, a
non-local target memory aggregation module is designed to leverage a non-local
network to model the intrinsic relationship between the target semantic and the
target orientation clues during the navigation process so as to mine the most
target-correlated object clues for the navigation decision. Extensive
experimental results on AI2-Thor and RoboTHOR benchmarks verify the excellent
obstacle avoidance and navigation efficiency of our proposed method. The core
source code is available at https://github.com/xwaiyy123/object-navigation.",None,-1
1b9b674f-d293-4721-b2ca-e13cf88f169f,The First Proven Performance Guarantees for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) on a Combinatorial Optimization Problem,0.989897,19,"The Non-dominated Sorting Genetic Algorithm-II (NSGA-II) is one of the most
prominent algorithms to solve multi-objective optimization problems. Recently,
the first mathematical runtime guarantees have been obtained for this
algorithm, however only for synthetic benchmark problems.
  In this work, we give the first proven performance guarantees for a classic
optimization problem, the NP-complete bi-objective minimum spanning tree
problem. More specifically, we show that the NSGA-II with population size $N
\ge 4((n-1) w_{\max} + 1)$ computes all extremal points of the Pareto front in
an expected number of $O(m^2 n w_{\max} \log(n w_{\max}))$ iterations, where
$n$ is the number of vertices, $m$ the number of edges, and $w_{\max}$ is the
maximum edge weight in the problem instance. This result confirms, via
mathematical means, the good performance of the NSGA-II observed empirically.
It also shows that mathematical analyses of this algorithm are not only
possible for synthetic benchmark problems, but also for more complex
combinatorial optimization problems.
  As a side result, we also obtain a new analysis of the performance of the
global SEMO algorithm on the bi-objective minimum spanning tree problem, which
improves the previous best result by a factor of $|F|$, the number of extremal
points of the Pareto front, a set that can be as large as $n w_{\max}$. The
main reason for this improvement is our observation that both multi-objective
evolutionary algorithms find the different extremal points in parallel rather
than sequentially, as assumed in the previous proofs.",None,-1
ce30cc5f-3e95-4a6f-b249-ad9080590435,BEVScope: Enhancing Self-Supervised Depth Estimation Leveraging Bird's-Eye-View in Dynamic Scenarios,0.431551,2,"Depth estimation is a cornerstone of perception in autonomous driving and
robotic systems. The considerable cost and relatively sparse data acquisition
of LiDAR systems have led to the exploration of cost-effective alternatives,
notably, self-supervised depth estimation. Nevertheless, current
self-supervised depth estimation methods grapple with several limitations: (1)
the failure to adequately leverage informative multi-camera views. (2) the
limited capacity to handle dynamic objects effectively. To address these
challenges, we present BEVScope, an innovative approach to self-supervised
depth estimation that harnesses Bird's-Eye-View (BEV) features. Concurrently,
we propose an adaptive loss function, specifically designed to mitigate the
complexities associated with moving objects. Empirical evaluations conducted on
the Nuscenes dataset validate our approach, demonstrating competitive
performance. Code will be released at https://github.com/myc634/BEVScope.",None,-1
0b3be4a4-49ff-45ea-aae6-1583f45b3e81,RQAT-INR: Improved Implicit Neural Image Compression,0.620297,6,"Deep variational autoencoders for image and video compression have gained
significant attraction in the recent years, due to their potential to offer
competitive or better compression rates compared to the decades long
traditional codecs such as AVC, HEVC or VVC. However, because of complexity and
energy consumption, these approaches are still far away from practical usage in
industry. More recently, implicit neural representation (INR) based codecs have
emerged, and have lower complexity and energy usage to classical approaches at
decoding. However, their performances are not in par at the moment with
state-of-the-art methods. In this research, we first show that INR based image
codec has a lower complexity than VAE based approaches, then we propose several
improvements for INR-based image codec and outperformed baseline model by a
large margin.",None,-1
5ad6f2e4-0575-445e-897c-4c767afa12ea,SDOH-NLI: a Dataset for Inferring Social Determinants of Health from Clinical Notes,0.476595,1,"Social and behavioral determinants of health (SDOH) play a significant role
in shaping health outcomes, and extracting these determinants from clinical
notes is a first step to help healthcare providers systematically identify
opportunities to provide appropriate care and address disparities. Progress on
using NLP methods for this task has been hindered by the lack of high-quality
publicly available labeled data, largely due to the privacy and regulatory
constraints on the use of real patients' information. This paper introduces a
new dataset, SDOH-NLI, that is based on publicly available notes and which we
release publicly. We formulate SDOH extraction as a natural language inference
(NLI) task, and provide binary textual entailment labels obtained from human
raters for a cross product of a set of social history snippets as premises and
SDOH factors as hypotheses. Our dataset differs from standard NLI benchmarks in
that our premises and hypotheses are obtained independently. We evaluate both
""off-the-shelf"" entailment models as well as models fine-tuned on our data, and
highlight the ways in which our dataset appears more challenging than commonly
used NLI datasets.",None,-1
3112a03b-3b5c-49bb-8973-fa027d679bb5,"Mephisto: A Framework for Portable, Reproducible, and Iterative Crowdsourcing",0.7561,5,"We introduce Mephisto, a framework to make crowdsourcing for research more
reproducible, transparent, and collaborative. Mephisto provides abstractions
that cover a broad set of task designs and data collection workflows, and
provides a simple user experience to make best-practices easy defaults. In this
whitepaper we discuss the current state of data collection and annotation in ML
research, establish the motivation for building a shared framework to enable
researchers to create and open-source data collection and annotation tools as
part of their publication, and outline a set of suggested requirements for a
system to facilitate these goals. We then step through our resolution in
Mephisto, explaining the abstractions we use, our design decisions around the
user experience, and share implementation details and where they align with the
original motivations. We also discuss current limitations, as well as future
work towards continuing to deliver on the framework's initial goals. Mephisto
is available as an open source project, and its documentation can be found at
www.mephisto.ai.",None,-1
e5e1b435-f4ce-452e-a052-94ed6cd55607,Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding,0.959829,21,"3D GAN inversion aims to achieve high reconstruction fidelity and reasonable
3D geometry simultaneously from a single image input. However, existing 3D GAN
inversion methods rely on time-consuming optimization for each individual case.
In this work, we introduce a novel encoder-based inversion framework based on
EG3D, one of the most widely-used 3D GAN models. We leverage the inherent
properties of EG3D's latent space to design a discriminator and a background
depth regularization. This enables us to train a geometry-aware encoder capable
of converting the input image into corresponding latent code. Additionally, we
explore the feature space of EG3D and develop an adaptive refinement stage that
improves the representation ability of features in EG3D to enhance the recovery
of fine-grained textural details. Finally, we propose an occlusion-aware fusion
operation to prevent distortion in unobserved regions. Our method achieves
impressive results comparable to optimization-based methods while operating up
to 500 times faster. Our framework is well-suited for applications such as
semantic editing.",None,-1
fee2d9bf-d5c6-421a-b0ec-cd38a7f684ee,Self-supervised Multi-view Disentanglement for Expansion of Visual Collections,0.283784,2,"Image search engines enable the retrieval of images relevant to a query
image. In this work, we consider the setting where a query for similar images
is derived from a collection of images. For visual search, the similarity
measurements may be made along multiple axes, or views, such as style and
color. We assume access to a set of feature extractors, each of which computes
representations for a specific view. Our objective is to design a retrieval
algorithm that effectively combines similarities computed over representations
from multiple views. To this end, we propose a self-supervised learning method
for extracting disentangled view-specific representations for images such that
the inter-view overlap is minimized. We show how this allows us to compute the
intent of a collection as a distribution over views. We show how effective
retrieval can be performed by prioritizing candidate expansion images that
match the intent of a query collection. Finally, we present a new querying
mechanism for image search enabled by composing multiple collections and
perform retrieval under this setting using the techniques presented in this
paper.",None,-1
e0e4ad1a-2c84-4a3d-9c5d-daa29f2924e1,Harvesting Event Schemas from Large Language Models,0.463936,2,"Event schema provides a conceptual, structural and formal language to
represent events and model the world event knowledge. Unfortunately, it is
challenging to automatically induce high-quality and high-coverage event
schemas due to the open nature of real-world events, the diversity of event
expressions, and the sparsity of event knowledge. In this paper, we propose a
new paradigm for event schema induction -- knowledge harvesting from
large-scale pre-trained language models, which can effectively resolve the
above challenges by discovering, conceptualizing and structuralizing event
schemas from PLMs. And an Event Schema Harvester (ESHer) is designed to
automatically induce high-quality event schemas via in-context generation-based
conceptualization, confidence-aware schema structuralization and graph-based
schema aggregation. Empirical results show that ESHer can induce high-quality
and high-coverage event schemas on varying domains.",None,-1
1e760575-1898-4055-8ca3-853b7003431b,EdgeYOLO: An Edge-Real-Time Object Detector,0.474289,5,"This paper proposes an efficient, low-complexity and anchor-free object
detector based on the state-of-the-art YOLO framework, which can be implemented
in real time on edge computing platforms. We develop an enhanced data
augmentation method to effectively suppress overfitting during training, and
design a hybrid random loss function to improve the detection accuracy of small
objects. Inspired by FCOS, a lighter and more efficient decoupled head is
proposed, and its inference speed can be improved with little loss of
precision. Our baseline model can reach the accuracy of 50.6% AP50:95 and 69.8%
AP50 in MS COCO2017 dataset, 26.4% AP50:95 and 44.8% AP50 in VisDrone2019-DET
dataset, and it meets real-time requirements (FPS>=30) on edge-computing device
Nvidia Jetson AGX Xavier. We also designed lighter models with less parameters
for edge computing devices with lower computing power, which also show better
performances. Our source code, hyper-parameters and model weights are all
available at https://github.com/LSH9832/edgeyolo.",None,-1
4f45a587-6819-4363-8eeb-9d193b769ffd,HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining,0.734559,17,"Human-centric perceptions include a variety of vision tasks, which have
widespread industrial applications, including surveillance, autonomous driving,
and the metaverse. It is desirable to have a general pretrain model for
versatile human-centric downstream tasks. This paper forges ahead along this
path from the aspects of both benchmark and pretraining methods. Specifically,
we propose a \textbf{HumanBench} based on existing datasets to comprehensively
evaluate on the common ground the generalization abilities of different
pretraining methods on 19 datasets from 6 diverse downstream tasks, including
person ReID, pose estimation, human parsing, pedestrian attribute recognition,
pedestrian detection, and crowd counting. To learn both coarse-grained and
fine-grained knowledge in human bodies, we further propose a \textbf{P}rojector
\textbf{A}ssis\textbf{T}ed \textbf{H}ierarchical pretraining method
(\textbf{PATH}) to learn diverse knowledge at different granularity levels.
Comprehensive evaluations on HumanBench show that our PATH achieves new
state-of-the-art results on 17 downstream datasets and on-par results on the
other 2 datasets. The code will be publicly at
\href{https://github.com/OpenGVLab/HumanBench}{https://github.com/OpenGVLab/HumanBench}.",None,-1
e3a44528-11d6-47bb-b232-8d5158bf629d,ClothCombo: Modeling Inter-Cloth Interaction for Draping Multi-Layered Clothes,0.172302,2,"We present ClothCombo, a pipeline to drape arbitrary combinations of clothes
on 3D human models with varying body shapes and poses. While existing
learning-based approaches for draping clothes have shown promising results,
multi-layered clothing remains challenging as it is non-trivial to model
inter-cloth interaction. To this end, our method utilizes a GNN-based network
to efficiently model the interaction between clothes in different layers, thus
enabling multi-layered clothing. Specifically, we first create feature
embedding for each cloth using a topology-agnostic network. Then, the draping
network deforms all clothes to fit the target body shape and pose without
considering inter-cloth interaction. Lastly, the untangling network predicts
the per-vertex displacements in a way that resolves interpenetration between
clothes. In experiments, the proposed model demonstrates strong performance in
complex multi-layered scenarios. Being agnostic to cloth topology, our method
can be readily used for layered virtual try-on of real clothes in diverse poses
and combinations of clothes.",None,-1
f049a8b3-0904-43d6-b82a-f6772505375b,Region-Aware Portrait Retouching with Sparse Interactive Guidance,0.277925,1,"Portrait retouching aims to improve the aesthetic quality of input portrait
photos and especially requires human-region priority. The deep learning-based
methods largely elevate the retouching efficiency and provide promising
retouched results. However, existing portrait retouching methods focus on
automatic retouching, which treats all human-regions equally and ignores users'
preferences for specific individuals, thus suffering from limited flexibility
in interactive scenarios. In this work, we emphasize the importance of users'
intents and explore the interactive portrait retouching task. Specifically, we
propose a region-aware retouching framework with two branches: an automatic
branch and an interactive branch. The automatic branch involves an
encoding-decoding process, which searches region candidates and performs
automatic region-aware retouching without user guidance. The interactive branch
encodes sparse user guidance into a priority condition vector and modulates
latent features with a region selection module to further emphasize the
user-specified regions. Experimental results show that our interactive branch
effectively captures users' intents and generalizes well to unseen scenes with
sparse user guidance, while our automatic branch also outperforms the
state-of-the-art retouching methods due to improved region-awareness.",None,-1
2dad8b81-ad33-454a-8792-09dfb3f0b3ce,Zero-Shot Multi-Label Topic Inference with Sentence Encoders,0.0542019,1,"Sentence encoders have indeed been shown to achieve superior performances for
many downstream text-mining tasks and, thus, claimed to be fairly general.
Inspired by this, we performed a detailed study on how to leverage these
sentence encoders for the ""zero-shot topic inference"" task, where the topics
are defined/provided by the users in real-time. Extensive experiments on seven
different datasets demonstrate that Sentence-BERT demonstrates superior
generality compared to other encoders, while Universal Sentence Encoder can be
preferred when efficiency is a top priority.",None,-1
19465124-1e23-4b6c-95c4-df2672ad8d6e,Understanding Expressivity of GNN in Rule Learning,0.318264,5,"Rule learning is critical to improving knowledge graph (KG) reasoning due to
their ability to provide logical and interpretable explanations. Recently,
Graph Neural Networks (GNNs) with tail entity scoring achieve the
state-of-the-art performance on KG reasoning. However, the theoretical
understandings for these GNNs are either lacking or focusing on
single-relational graphs, leaving what the kind of rules these GNNs can learn
an open problem. We propose to fill the above gap in this paper. Specifically,
GNNs with tail entity scoring are unified into a common framework. Then, we
analyze their expressivity by formally describing the rule structures they can
learn and theoretically demonstrating their superiority. These results further
inspire us to propose a novel labeling strategy to learn more rules in KG
reasoning. Experimental results are consistent with our theoretical findings
and verify the effectiveness of our proposed method. The code is publicly
available at https://github.com/LARS-research/Rule-learning-expressivity.",None,-1
1f8cb769-3cf9-4a6d-9f82-d929e76ae91b,Inferring Preferences from Demonstrations in Multi-objective Reinforcement Learning: A Dynamic Weight-based Approach,0.198318,2,"Many decision-making problems feature multiple objectives. In such problems,
it is not always possible to know the preferences of a decision-maker for
different objectives. However, it is often possible to observe the behavior of
decision-makers. In multi-objective decision-making, preference inference is
the process of inferring the preferences of a decision-maker for different
objectives. This research proposes a Dynamic Weight-based Preference Inference
(DWPI) algorithm that can infer the preferences of agents acting in
multi-objective decision-making problems, based on observed behavior
trajectories in the environment. The proposed method is evaluated on three
multi-objective Markov decision processes: Deep Sea Treasure, Traffic, and Item
Gathering. The performance of the proposed DWPI approach is compared to two
existing preference inference methods from the literature, and empirical
results demonstrate significant improvements compared to the baseline
algorithms, in terms of both time requirements and accuracy of the inferred
preferences. The Dynamic Weight-based Preference Inference algorithm also
maintains its performance when inferring preferences for sub-optimal behavior
demonstrations. In addition to its impressive performance, the Dynamic
Weight-based Preference Inference algorithm does not require any interactions
during training with the agent whose preferences are inferred, all that is
required is a trajectory of observed behavior.",None,-1
ab612278-eb80-4c79-9ae5-0835ca2f2a3c,Leveraging GPT-4 for Automatic Translation Post-Editing,0.947425,32,"While Neural Machine Translation (NMT) represents the leading approach to
Machine Translation (MT), the outputs of NMT models still require translation
post-editing to rectify errors and enhance quality under critical settings. In
this work, we formalize the task of direct translation post-editing with Large
Language Models (LLMs) and explore the use of GPT-4 to automatically post-edit
NMT outputs across several language pairs. Our results demonstrate that GPT-4
is adept at translation post-editing, producing meaningful and trustworthy
edits to translations that help improve its general quality as well as remove
different classes of major errors in translations. In particular, human
evaluations on assessing edit trustworthiness show that GPT-4 exhibits a large
improvement over the prior state-of-the-art LLM. Notably, we improve upon
state-of-the-art performance on WMT-22 English-Chinese, English-German,
Chinese-English and German-English language pairs using GPT-4 based
post-editing, as evaluated by state-of-the-art MT quality metrics. However, we
also show that GPT-4 could produce hallucinated edits, thereby urging caution
in its use as an expert translation post-editor.",None,-1
a8c1eb4b-511f-4ab7-9b78-667158958fb5,Learning to Evaluate the Artness of AI-generated Images,0.0801638,4,"Assessing the artness of AI-generated images continues to be a challenge
within the realm of image generation. Most existing metrics cannot be used to
perform instance-level and reference-free artness evaluation. This paper
presents ArtScore, a metric designed to evaluate the degree to which an image
resembles authentic artworks by artists (or conversely photographs), thereby
offering a novel approach to artness assessment. We first blend pre-trained
models for photo and artwork generation, resulting in a series of mixed models.
Subsequently, we utilize these mixed models to generate images exhibiting
varying degrees of artness with pseudo-annotations. Each photorealistic image
has a corresponding artistic counterpart and a series of interpolated images
that range from realistic to artistic. This dataset is then employed to train a
neural network that learns to estimate quantized artness levels of arbitrary
images. Extensive experiments reveal that the artness levels predicted by
ArtScore align more closely with human artistic evaluation than existing
evaluation metrics, such as Gram loss and ArtFID.",None,-1
8c4da732-8024-4351-88b7-25125cb5e3b9,AutoSTL: Automated Spatio-Temporal Multi-Task Learning,0.785725,7,"Spatio-Temporal prediction plays a critical role in smart city construction.
Jointly modeling multiple spatio-temporal tasks can further promote an
intelligent city life by integrating their inseparable relationship. However,
existing studies fail to address this joint learning problem well, which
generally solve tasks individually or a fixed task combination. The challenges
lie in the tangled relation between different properties, the demand for
supporting flexible combinations of tasks and the complex spatio-temporal
dependency. To cope with the problems above, we propose an Automated
Spatio-Temporal multi-task Learning (AutoSTL) method to handle multiple
spatio-temporal tasks jointly. Firstly, we propose a scalable architecture
consisting of advanced spatio-temporal operations to exploit the complicated
dependency. Shared modules and feature fusion mechanism are incorporated to
further capture the intrinsic relationship between tasks. Furthermore, our
model automatically allocates the operations and fusion weight. Extensive
experiments on benchmark datasets verified that our model achieves
state-of-the-art performance. As we can know, AutoSTL is the first automated
spatio-temporal multi-task learning method.",None,-1
2659ceb9-2513-4328-8407-f0856dc3af95,"QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models",0.767023,7,"Misinformation spreading in mainstream and social media has been misleading
users in different ways. Manual detection and verification efforts by
journalists and fact-checkers can no longer cope with the great scale and quick
spread of misleading information. This motivated research and industry efforts
to develop systems for analyzing and verifying news spreading online. The
SemEval-2023 Task 3 is an attempt to address several subtasks under this
overarching problem, targeting writing techniques used in news articles to
affect readers' opinions. The task addressed three subtasks with six languages,
in addition to three ``surprise'' test languages, resulting in 27 different
test setups. This paper describes our participating system to this task. Our
team is one of the 6 teams that successfully submitted runs for all setups. The
official results show that our system is ranked among the top 3 systems for 10
out of the 27 setups.",None,-1
86e960e6-2228-40c6-8715-0e4518647f05,No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference,0.0664817,1,"Natural Language Inference (NLI) has been a cornerstone task in evaluating
language models' inferential reasoning capabilities. However, the standard
three-way classification scheme used in NLI has well-known shortcomings in
evaluating models' ability to capture the nuances of natural human reasoning.
In this paper, we argue that the operationalization of the neutral label in
current NLI datasets has low validity, is interpreted inconsistently, and that
at least one important sense of neutrality is often ignored. We uncover the
detrimental impact of these shortcomings, which in some cases leads to
annotation datasets that actually decrease performance on downstream tasks. We
compare approaches of handling annotator disagreement and identify flaws in a
recent NLI dataset that designs an annotator study based on a problematic
operationalization. Our findings highlight the need for a more refined
evaluation framework for NLI, and we hope to spark further discussion and
action in the NLP community.",None,-1
087c6a6d-eb45-4001-8fef-90cf76ad96b8,Diffusion Model for Generative Image Denoising,0.444449,22,"In supervised learning for image denoising, usually the paired clean images
and noisy images are collected or synthesised to train a denoising model. L2
norm loss or other distance functions are used as the objective function for
training. It often leads to an over-smooth result with less image details. In
this paper, we regard the denoising task as a problem of estimating the
posterior distribution of clean images conditioned on noisy images. We apply
the idea of diffusion model to realize generative image denoising. According to
the noise model in denoising tasks, we redefine the diffusion process such that
it is different from the original one. Hence, the sampling of the posterior
distribution is a reverse process of dozens of steps from the noisy image. We
consider three types of noise model, Gaussian, Gamma and Poisson noise. With
the guarantee of theory, we derive a unified strategy for model training. Our
method is verified through experiments on three types of noise models and
achieves excellent performance.",None,-1
a0a7b6ed-93e3-40c6-97e6-f218b6265eab,An Investigation of Noise in Morphological Inflection,0.265067,1,"With a growing focus on morphological inflection systems for languages where
high-quality data is scarce, training data noise is a serious but so far
largely ignored concern. We aim at closing this gap by investigating the types
of noise encountered within a pipeline for truly unsupervised morphological
paradigm completion and its impact on morphological inflection systems: First,
we propose an error taxonomy and annotation pipeline for inflection training
data. Then, we compare the effect of different types of noise on multiple
state-of-the-art inflection models. Finally, we propose a novel character-level
masked language modeling (CMLM) pretraining objective and explore its impact on
the models' resistance to noise. Our experiments show that various
architectures are impacted differently by separate types of noise, but
encoder-decoders tend to be more robust to noise than models trained with a
copy bias. CMLM pretraining helps transformers, but has lower impact on LSTMs.",None,-1
e4bc4dee-6aad-46e3-a50f-598dea2f2383,Aligning Language Models to User Opinions,0.978395,26,"An important aspect of developing LLMs that interact with humans is to align
models' behavior to their users. It is possible to prompt an LLM into behaving
as a certain persona, especially a user group or ideological persona the model
captured during its pertaining stage. But, how to best align an LLM with a
specific user and not a demographic or ideological group remains an open
question. Mining public opinion surveys (by Pew Research), we find that the
opinions of a user and their demographics and ideologies are not mutual
predictors. We use this insight to align LLMs by modeling both user opinions as
well as user demographics and ideology, achieving up to 7 points accuracy gains
in predicting public opinions from survey questions across a broad set of
topics. In addition to the typical approach of prompting LLMs with demographics
and ideology, we discover that utilizing the most relevant past opinions from
individual users enables the model to predict user opinions more accurately.",None,-1
11a96912-3143-4c1c-a6cf-e8f82445a917,Guided Focal Stack Refinement Network for Light Field Salient Object Detection,0.655784,3,"Light field salient object detection (SOD) is an emerging research direction
attributed to the richness of light field data. However, most existing methods
lack effective handling of focal stacks, therefore making the latter involved
in a lot of interfering information and degrade the performance of SOD. To
address this limitation, we propose to utilize multi-modal features to refine
focal stacks in a guided manner, resulting in a novel guided focal stack
refinement network called GFRNet. To this end, we propose a guided refinement
and fusion module (GRFM) to refine focal stacks and aggregate multi-modal
features. In GRFM, all-in-focus (AiF) and depth modalities are utilized to
refine focal stacks separately, leading to two novel sub-modules for different
modalities, namely AiF-based refinement module (ARM) and depth-based refinement
module (DRM). Such refinement modules enhance structural and positional
information of salient objects in focal stacks, and are able to improve SOD
accuracy. Experimental results on four benchmark datasets demonstrate the
superiority of our GFRNet model against 12 state-of-the-art models.",None,-1
b36fed3f-ed3a-4795-9f69-d84e9d85babf,FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain,0.398526,15,"This paper introduces FrenchMedMCQA, the first publicly available
Multiple-Choice Question Answering (MCQA) dataset in French for medical domain.
It is composed of 3,105 questions taken from real exams of the French medical
specialization diploma in pharmacy, mixing single and multiple answers. Each
instance of the dataset contains an identifier, a question, five possible
answers and their manual correction(s). We also propose first baseline models
to automatically process this MCQA task in order to report on the current
performances and to highlight the difficulty of the task. A detailed analysis
of the results showed that it is necessary to have representations adapted to
the medical domain or to the MCQA task: in our case, English specialized models
yielded better results than generic French ones, even though FrenchMedMCQA is
in French. Corpus, models and tools are available online.",None,-1
251b186c-01d9-43ee-8eaa-33c182adee87,Input Reconstruction Attack against Vertical Federated Large Language Models,0.335759,3,"Recently, large language models (LLMs) have drawn extensive attention from
academia and the public, due to the advent of the ChatGPT. While LLMs show
their astonishing ability in text generation for various tasks, privacy
concerns limit their usage in real-life businesses. More specifically, either
the user's inputs (the user sends the query to the model-hosting server) or the
model (the user downloads the complete model) itself will be revealed during
the usage. Vertical federated learning (VFL) is a promising solution to this
kind of problem. It protects both the user's input and the knowledge of the
model by splitting the model into a bottom part and a top part, which is
maintained by the user and the model provider, respectively. However, in this
paper, we demonstrate that in LLMs, VFL fails to protect the user input since
it is simple and cheap to reconstruct the input from the intermediate
embeddings. Experiments show that even with a commercial GPU, the input
sentence can be reconstructed in only one second. We also discuss several
possible solutions to enhance the privacy of vertical federated LLMs.",None,-1
440b3b18-6b7b-4183-afcc-b7a97d9552a7,Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models,0.34148,14,"Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP). Although convenient for research and practical applications, open-source
LLMs with fewer parameters often suffer from severe hallucinations compared to
their larger counterparts. This paper focuses on measuring and reducing
hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs
that are publicly available for research and commercial applications. We
introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed
to quantify the severity of hallucinations in LLMs. Additionally, we explore
techniques like knowledge injection and teacher-student approaches to alleviate
hallucinations in low-parameter LLMs. Our experiments effectively demonstrate
the reduction of hallucinations in challenging domains for these LLMs.",None,-1
ea0788e0-eb06-4914-a0f4-fc285097964a,Retentive Network: A Successor to Transformer for Large Language Models,0.674462,133,"In this work, we propose Retentive Network (RetNet) as a foundation
architecture for large language models, simultaneously achieving training
parallelism, low-cost inference, and good performance. We theoretically derive
the connection between recurrence and attention. Then we propose the retention
mechanism for sequence modeling, which supports three computation paradigms,
i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel
representation allows for training parallelism. The recurrent representation
enables low-cost $O(1)$ inference, which improves decoding throughput, latency,
and GPU memory without sacrificing performance. The chunkwise recurrent
representation facilitates efficient long-sequence modeling with linear
complexity, where each chunk is encoded parallelly while recurrently
summarizing the chunks. Experimental results on language modeling show that
RetNet achieves favorable scaling results, parallel training, low-cost
deployment, and efficient inference. The intriguing properties make RetNet a
strong successor to Transformer for large language models. Code will be
available at https://aka.ms/retnet.",None,-1
a276fd3a-9ab8-4e4a-aa96-cd96aeec6427,Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science,0.986305,16,"Large Language Models (LLMs) have democratized synthetic data generation,
which in turn has the potential to simplify and broaden a wide gamut of NLP
tasks. Here, we tackle a pervasive problem in synthetic data generation: its
generative distribution often differs from the distribution of real-world data
researchers care about (in other words, it is unfaithful). In a case study on
sarcasm detection, we study three strategies to increase the faithfulness of
synthetic data: grounding, filtering, and taxonomy-based generation. We
evaluate these strategies using the performance of classifiers trained with
generated synthetic data on real-world data. While all three strategies improve
the performance of classifiers, we find that grounding works best for the task
at hand. As synthetic data generation plays an ever-increasing role in NLP
research, we expect this work to be a stepping stone in improving its utility.
We conclude this paper with some recommendations on how to generate
high(er)-fidelity synthetic data for specific tasks.",None,-1
a47ea070-f163-4df8-b6e9-0cb602a9c256,ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics,0.844938,30,"We introduce ProofNet, a benchmark for autoformalization and formal proving
of undergraduate-level mathematics. The ProofNet benchmarks consists of 371
examples, each consisting of a formal theorem statement in Lean 3, a natural
language theorem statement, and a natural language proof. The problems are
primarily drawn from popular undergraduate pure mathematics textbooks and cover
topics such as real and complex analysis, linear algebra, abstract algebra, and
topology. We intend for ProofNet to be a challenging benchmark that will drive
progress in autoformalization and automatic theorem proving. We report baseline
results on statement autoformalization via in-context learning. Moreover, we
introduce two novel statement autoformalization methods: prompt retrieval and
distilled backtranslation.",None,-1
8583ad9d-a435-47c1-9ee3-4400776ef7da,Instructed Language Models with Retrievers Are Powerful Entity Linkers,0.784527,6,"Generative approaches powered by large language models (LLMs) have
demonstrated emergent abilities in tasks that require complex reasoning
abilities. Yet the generative nature still makes the generated content suffer
from hallucinations, thus unsuitable for entity-centric tasks like entity
linking (EL) requiring precise entity predictions over a large knowledge base.
We present Instructed Generative Entity Linker (INSGENEL), the first approach
that enables casual language models to perform entity linking over knowledge
bases. Several methods to equip language models with EL capability were
proposed in this work, including (i) a sequence-to-sequence training EL
objective with instruction-tuning, (ii) a novel generative EL framework based
on a light-weight potential mention retriever that frees the model from heavy
and non-parallelizable decoding, achieving 4$\times$ speedup without compromise
on linking metrics. INSGENEL outperforms previous generative alternatives with
+6.8 F1 points gain on average, also with a huge advantage in training data
efficiency and training compute consumption. In addition, our skillfully
engineered in-context learning (ICL) framework for EL still lags behind
INSGENEL significantly, reaffirming that the EL task remains a persistent
hurdle for general LLMs.",None,-1
d44a1ca6-8922-4e09-afa6-1e58cfc27b25,Logic programming for deliberative robotic task planning,0.367839,4,"Over the last decade, the use of robots in production and daily life has
increased. With increasingly complex tasks and interaction in different
environments including humans, robots are required a higher level of autonomy
for efficient deliberation. Task planning is a key element of deliberation. It
combines elementary operations into a structured plan to satisfy a prescribed
goal, given specifications on the robot and the environment. In this
manuscript, we present a survey on recent advances in the application of logic
programming to the problem of task planning. Logic programming offers several
advantages compared to other approaches, including greater expressivity and
interpretability which may aid in the development of safe and reliable robots.
We analyze different planners and their suitability for specific robotic
applications, based on expressivity in domain representation, computational
efficiency and software implementation. In this way, we support the robotic
designer in choosing the best tool for his application.",None,-1
11f4d957-904e-49d2-be01-f11c6c9a8183,Learning Discriminative Visual-Text Representation for Polyp Re-Identification,0.326388,1,"Colonoscopic Polyp Re-Identification aims to match a specific polyp in a
large gallery with different cameras and views, which plays a key role for the
prevention and treatment of colorectal cancer in the computer-aided diagnosis.
However, traditional methods mainly focus on the visual representation
learning, while neglect to explore the potential of semantic features during
training, which may easily leads to poor generalization capability when adapted
the pretrained model into the new scenarios. To relieve this dilemma, we
propose a simple but effective training method named VT-ReID, which can
remarkably enrich the representation of polyp videos with the interchange of
high-level semantic information. Moreover, we elaborately design a novel
clustering mechanism to introduce prior knowledge from textual data, which
leverages contrastive learning to promote better separation from abundant
unlabeled text data. To the best of our knowledge, this is the first attempt to
employ the visual-text feature with clustering mechanism for the colonoscopic
polyp re-identification. Empirical results show that our method significantly
outperforms current state-of-the art methods with a clear margin.",None,-1
3f4ddbf6-1650-4654-bea3-78337325e1a1,SeqXGPT: Sentence-Level AI-Generated Text Detection,0.871646,20,"Widely applied large language models (LLMs) can generate human-like content,
raising concerns about the abuse of LLMs. Therefore, it is important to build
strong AI-generated text (AIGT) detectors. Current works only consider
document-level AIGT detection, therefore, in this paper, we first introduce a
sentence-level detection challenge by synthesizing a dataset that contains
documents that are polished with LLMs, that is, the documents contain sentences
written by humans and sentences modified by LLMs. Then we propose
\textbf{Seq}uence \textbf{X} (Check) \textbf{GPT}, a novel method that utilizes
log probability lists from white-box LLMs as features for sentence-level AIGT
detection. These features are composed like \textit{waves} in speech processing
and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution
and self-attention networks. We test it in both sentence and document-level
detection challenges. Experimental results show that previous methods struggle
in solving sentence-level AIGT detection, while our method not only
significantly surpasses baseline methods in both sentence and document-level
detection challenges but also exhibits strong generalization capabilities.",None,-1
0efddf23-eb89-4283-8a07-914744f5c9d2,Applying Large Language Models for Causal Structure Learning in Non Small Cell Lung Cancer,0.656434,2,"Causal discovery is becoming a key part in medical AI research. These methods
can enhance healthcare by identifying causal links between biomarkers,
demographics, treatments and outcomes. They can aid medical professionals in
choosing more impactful treatments and strategies. In parallel, Large Language
Models (LLMs) have shown great potential in identifying patterns and generating
insights from text data. In this paper we investigate applying LLMs to the
problem of determining the directionality of edges in causal discovery.
Specifically, we test our approach on a deidentified set of Non Small Cell Lung
Cancer(NSCLC) patients that have both electronic health record and genomic
panel data. Graphs are validated using Bayesian Dirichlet estimators using
tabular data. Our result shows that LLMs can accurately predict the
directionality of edges in causal graphs, outperforming existing
state-of-the-art methods. These findings suggests that LLMs can play a
significant role in advancing causal discovery and help us better understand
complex systems.",None,-1
0138cbd1-e50c-4af9-99f4-49e0e27a4919,Tokenization and the Noiseless Channel,0.293852,15,"Subword tokenization is a key part of many NLP pipelines. However, little is
known about why some tokenizer and hyperparameter combinations lead to better
downstream model performance than others. We propose that good tokenizers lead
to \emph{efficient} channel usage, where the channel is the means by which some
input is conveyed to the model and efficiency can be quantified in
information-theoretic terms as the ratio of the Shannon entropy to the maximum
possible entropy of the token distribution. Yet, an optimal encoding according
to Shannon entropy assigns extremely long codes to low-frequency tokens and
very short codes to high-frequency tokens. Defining efficiency in terms of
R\'enyi entropy, on the other hand, penalizes distributions with either very
high or very low-frequency tokens. In machine translation, we find that across
multiple tokenizers, the R\'enyi entropy with $\alpha = 2.5$ has a very strong
correlation with \textsc{Bleu}: $0.78$ in comparison to just $-0.32$ for
compressed length.",None,-1
11716e5a-f021-4ece-9813-ad0e94c78e9d,Generative AI in the Construction Industry: Opportunities & Challenges,0.996744,7,"In the last decade, despite rapid advancements in artificial intelligence
(AI) transforming many industry practices, construction largely lags in
adoption. Recently, the emergence and rapid adoption of advanced large language
models (LLM) like OpenAI's GPT, Google's PaLM, and Meta's Llama have shown
great potential and sparked considerable global interest. However, the current
surge lacks a study investigating the opportunities and challenges of
implementing Generative AI (GenAI) in the construction sector, creating a
critical knowledge gap for researchers and practitioners. This underlines the
necessity to explore the prospects and complexities of GenAI integration.
Bridging this gap is fundamental to optimizing GenAI's early-stage adoption
within the construction sector. Given GenAI's unprecedented capabilities to
generate human-like content based on learning from existing content, we reflect
on two guiding questions: What will the future bring for GenAI in the
construction industry? What are the potential opportunities and challenges in
implementing GenAI in the construction industry? This study delves into
reflected perception in literature, analyzes the industry perception using
programming-based word cloud and frequency analysis, and integrates authors'
opinions to answer these questions. This paper recommends a conceptual GenAI
implementation framework, provides practical recommendations, summarizes future
research questions, and builds foundational literature to foster subsequent
research expansion in GenAI within the construction and its allied architecture
& engineering domains.",None,-1
550f95ed-ae9e-487f-9f6f-1eb031838c72,Egocentric Auditory Attention Localization in Conversations,0.801655,7,"In a noisy conversation environment such as a dinner party, people often
exhibit selective auditory attention, or the ability to focus on a particular
speaker while tuning out others. Recognizing who somebody is listening to in a
conversation is essential for developing technologies that can understand
social behavior and devices that can augment human hearing by amplifying
particular sound sources. The computer vision and audio research communities
have made great strides towards recognizing sound sources and speakers in
scenes. In this work, we take a step further by focusing on the problem of
localizing auditory attention targets in egocentric video, or detecting who in
a camera wearer's field of view they are listening to. To tackle the new and
challenging Selective Auditory Attention Localization problem, we propose an
end-to-end deep learning approach that uses egocentric video and multichannel
audio to predict the heatmap of the camera wearer's auditory attention. Our
approach leverages spatiotemporal audiovisual features and holistic reasoning
about the scene to make predictions, and outperforms a set of baselines on a
challenging multi-speaker conversation dataset. Project page:
https://fkryan.github.io/saal",None,-1
4dba7994-908d-4b56-b7ac-8fc624713cf7,Gradient-Free Structured Pruning with Unlabeled Data,0.613165,10,"Large Language Models (LLMs) have achieved great success in solving difficult
tasks across many domains, but such success comes with a high computation cost,
and inference latency. As developers and third parties customize these models,
the need to provide efficient inference has increased. Many efforts have
attempted to reduce inference cost through model compression techniques such as
pruning and distillation. However, these techniques either require labeled
data, or are time-consuming as they require the compressed model to be
retrained to regain accuracy. In this paper, we propose a gradient-free
structured pruning framework that uses only unlabeled data. An evaluation on
the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates
the effectiveness of the proposed approach. By only using the weights of the
pre-trained model and unlabeled data, in a matter of a few minutes on a single
GPU, up to 40% of the original FLOP count can be reduced with less than a 4%
accuracy loss across all tasks considered.",None,-1
c464a40a-15ea-43fb-8593-2886ec2d7db2,Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking,0.325169,1,"Tracking dialogue states is an essential topic in task-oriented dialogue
systems, which involve filling in the necessary information in pre-defined
slots corresponding to a schema. While general pre-trained language models have
been shown effective in slot-filling, their performance is limited when applied
to specific domains. We propose a graph-based framework that learns
domain-specific prompts by incorporating the dialogue schema. Specifically, we
embed domain-specific schema encoded by a graph neural network into the
pre-trained language model, which allows for relations in the schema to guide
the model for better adaptation to the specific domain. Our experiments
demonstrate that the proposed graph-based method outperforms other multi-domain
DST approaches while using similar or fewer trainable parameters. We also
conduct a comprehensive study of schema graph architectures, parameter usage,
and module ablation that demonstrate the effectiveness of our model on
multi-domain dialogue state tracking.",None,-1
f2445962-4a8d-4eb4-9dbc-5700ddf7730a,Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational AutoEncoders,0.580866,2,"The injection of syntactic information in Variational AutoEncoders (VAEs) has
been shown to result in an overall improvement of performances and
generalisation. An effective strategy to achieve such a goal is to separate the
encoding of distributional semantic features and syntactic structures into
heterogeneous latent spaces via multi-task learning or dual encoder
architectures. However, existing works employing such techniques are limited to
LSTM-based VAEs. In this paper, we investigate latent space separation methods
for structural syntactic injection in Transformer-based VAE architectures
(i.e., Optimus). Specifically, we explore how syntactic structures can be
leveraged in the encoding stage through the integration of graph-based and
sequential models, and how multiple, specialised latent representations can be
injected into the decoder's attention mechanism via low-rank operators. Our
empirical evaluation, carried out on natural language sentences and
mathematical expressions, reveals that the proposed end-to-end VAE architecture
can result in a better overall organisation of the latent space, alleviating
the information loss occurring in standard VAE setups, resulting in enhanced
performances on language modelling and downstream generation tasks.",None,-1
53402105-b075-48db-aecd-fdae1b6a5e1a,A Global Model Approach to Robust Few-Shot SAR Automatic Target Recognition,0.875866,7,"In real-world scenarios, it may not always be possible to collect hundreds of
labeled samples per class for training deep learning-based SAR Automatic Target
Recognition (ATR) models. This work specifically tackles the few-shot SAR ATR
problem, where only a handful of labeled samples may be available to support
the task of interest. Our approach is composed of two stages. In the first, a
global representation model is trained via self-supervised learning on a large
pool of diverse and unlabeled SAR data. In the second stage, the global model
is used as a fixed feature extractor and a classifier is trained to partition
the feature space given the few-shot support samples, while simultaneously
being calibrated to detect anomalous inputs. Unlike competing approaches which
require a pristine labeled dataset for pretraining via meta-learning, our
approach learns highly transferable features from unlabeled data that have
little-to-no relation to the downstream task. We evaluate our method in
standard and extended MSTAR operating conditions and find it to achieve high
accuracy and robust out-of-distribution detection in many different few-shot
settings. Our results are particularly significant because they show the merit
of a global model approach to SAR ATR, which makes minimal assumptions, and
provides many axes for extendability.",None,-1
05abc18f-0ee6-4cf5-922f-d41bb43bdea2,Local-Global Pseudo-label Correction for Source-free Domain Adaptive Medical Image Segmentation,0.118997,1,"Domain shift is a commonly encountered issue in medical imaging solutions,
primarily caused by variations in imaging devices and data sources. To mitigate
this problem, unsupervised domain adaptation techniques have been employed.
However, concerns regarding patient privacy and potential degradation of image
quality have led to an increased focus on source-free domain adaptation. In
this study, we address the issue of false labels in self-training based
source-free domain adaptive medical image segmentation methods. To correct
erroneous pseudo-labels, we propose a novel approach called the local-global
pseudo-label correction (LGDA) method for source-free domain adaptive medical
image segmentation. Our method consists of two components: An offline local
context-based pseudo-label correction method that utilizes local context
similarity in image space. And an online global pseudo-label correction method
based on class prototypes, which corrects erroneously predicted pseudo-labels
by considering the relative distance between pixel-wise feature vectors and
prototype vectors. We evaluate the performance of our method on three benchmark
fundus image datasets for optic disc and cup segmentation. Our method achieves
superior performance compared to the state-of-the-art approaches, even without
using of any source data.",None,-1
c9974553-1aaa-4c42-b776-ab71f41d136d,Explainable Multi-Agent Reinforcement Learning for Temporal Queries,0.577713,5,"As multi-agent reinforcement learning (MARL) systems are increasingly
deployed throughout society, it is imperative yet challenging for users to
understand the emergent behaviors of MARL agents in complex environments. This
work presents an approach for generating policy-level contrastive explanations
for MARL to answer a temporal user query, which specifies a sequence of tasks
completed by agents with possible cooperation. The proposed approach encodes
the temporal query as a PCTL logic formula and checks if the query is feasible
under a given MARL policy via probabilistic model checking. Such explanations
can help reconcile discrepancies between the actual and anticipated multi-agent
behaviors. The proposed approach also generates correct and complete
explanations to pinpoint reasons that make a user query infeasible. We have
successfully applied the proposed approach to four benchmark MARL domains (up
to 9 agents in one domain). Moreover, the results of a user study show that the
generated explanations significantly improve user performance and satisfaction.",None,-1
57548ac5-490c-4050-8fac-f112cacdb791,Context-Aware Transformer Pre-Training for Answer Sentence Selection,0.0481635,2,"Answer Sentence Selection (AS2) is a core component for building an accurate
Question Answering pipeline. AS2 models rank a set of candidate sentences based
on how likely they answer a given question. The state of the art in AS2
exploits pre-trained transformers by transferring them on large annotated
datasets, while using local contextual information around the candidate
sentence. In this paper, we propose three pre-training objectives designed to
mimic the downstream fine-tuning task of contextual AS2. This allows for
specializing LMs when fine-tuning for contextual AS2. Our experiments on three
public and two large-scale industrial datasets show that our pre-training
approaches (applied to RoBERTa and ELECTRA) can improve baseline contextual AS2
accuracy by up to 8% on some datasets.",None,-1
49ff3b86-c16b-4ed0-9e01-80fd83c26485,Mastering the Task of Open Information Extraction with Large Language Models and Consistent Reasoning Environment,0.612607,2,"Open Information Extraction (OIE) aims to extract objective structured
knowledge from natural texts, which has attracted growing attention to build
dedicated models with human experience. As the large language models (LLMs)
have exhibited remarkable in-context learning capabilities, a question arises
as to whether the task of OIE can be effectively tackled with this paradigm? In
this paper, we explore solving the OIE problem by constructing an appropriate
reasoning environment for LLMs. Specifically, we first propose a method to
effectively estimate the discrepancy of syntactic distribution between a LLM
and test samples, which can serve as correlation evidence for preparing
positive demonstrations. Upon the evidence, we introduce a simple yet effective
mechanism to establish the reasoning environment for LLMs on specific tasks.
Without bells and whistles, experimental results on the standard CaRB benchmark
demonstrate that our $6$-shot approach outperforms state-of-the-art supervised
method, achieving an $55.3$ $F_1$ score. Further experiments on TACRED and
ACE05 show that our method can naturally generalize to other information
extraction tasks, resulting in improvements of $5.7$ and $6.8$ $F_1$ scores,
respectively.",None,-1
e3691610-d54d-4d48-a684-ed9a0123556b,DiffuSum: Generation Enhanced Extractive Summarization with Diffusion,0.998015,25,"Extractive summarization aims to form a summary by directly extracting
sentences from the source document. Existing works mostly formulate it as a
sequence labeling problem by making individual sentence label predictions. This
paper proposes DiffuSum, a novel paradigm for extractive summarization, by
directly generating the desired summary sentence representations with diffusion
models and extracting sentences based on sentence representation matching. In
addition, DiffuSum jointly optimizes a contrastive sentence encoder with a
matching loss for sentence representation alignment and a multi-class
contrastive loss for representation diversity. Experimental results show that
DiffuSum achieves the new state-of-the-art extractive results on CNN/DailyMail
with ROUGE scores of $44.83/22.56/40.56$. Experiments on the other two datasets
with different summary lengths also demonstrate the effectiveness of DiffuSum.
The strong performance of our framework shows the great potential of adapting
generative models for extractive summarization. To encourage more following
work in the future, we have released our codes at
\url{https://github.com/hpzhang94/DiffuSum}",None,-1
10651531-6291-407d-97ee-7c0f0953f6ac,Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs,0.517899,18,"In any system that uses structured knowledge graph (KG) data as its
underlying knowledge representation, KG-to-text generation is a useful tool for
turning parts of the graph data into text that can be understood by humans.
Recent work has shown that models that make use of pretraining on large amounts
of text data can perform well on the KG-to-text task even with relatively small
sets of training data on the specific graph-to-text task. In this paper, we
build on this concept by using large language models to perform zero-shot
generation based on nothing but the model's understanding of the triple
structure from what it can read. We show that ChatGPT achieves near
state-of-the-art performance on some measures of the WebNLG 2020 challenge, but
falls behind on others. Additionally, we compare factual, counter-factual and
fictional statements, and show that there is a significant connection between
what the LLM already knows about the data it is parsing and the quality of the
output text.",None,-1
4762281e-b654-4b31-a9d4-a661c8daf648,Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline,0.628824,7,"Brain network analysis is essential for diagnosing and intervention for
Alzheimer's disease (AD). However, previous research relied primarily on
specific time-consuming and subjective toolkits. Only few tools can obtain the
structural brain networks from brain diffusion tensor images (DTI). In this
paper, we propose a diffusion based end-to-end brain network generative model
Brain Diffuser that directly shapes the structural brain networks from DTI.
Compared to existing toolkits, Brain Diffuser exploits more structural
connectivity features and disease-related information by analyzing disparities
in structural brain networks across subjects. For the case of Alzheimer's
disease, the proposed model performs better than the results from existing
toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.",None,-1
c60a3518-662a-42e2-9621-1e6f55f72560,Revisiting Event-based Video Frame Interpolation,0.295783,1,"Dynamic vision sensors or event cameras provide rich complementary
information for video frame interpolation. Existing state-of-the-art methods
follow the paradigm of combining both synthesis-based and warping networks.
However, few of those methods fully respect the intrinsic characteristics of
events streams. Given that event cameras only encode intensity changes and
polarity rather than color intensities, estimating optical flow from events is
arguably more difficult than from RGB information. We therefore propose to
incorporate RGB information in an event-guided optical flow refinement
strategy. Moreover, in light of the quasi-continuous nature of the time signals
provided by event cameras, we propose a divide-and-conquer strategy in which
event-based intermediate frame synthesis happens incrementally in multiple
simplified stages rather than in a single, long stage. Extensive experiments on
both synthetic and real-world datasets show that these modifications lead to
more reliable and realistic intermediate frame results than previous video
frame interpolation methods. Our findings underline that a careful
consideration of event characteristics such as high temporal density and
elevated noise benefits interpolation accuracy.",None,-1
e3e54e61-4122-4b5f-bda5-b3a33d8fc455,TryOnDiffusion: A Tale of Two UNets,0.859621,44,"Given two images depicting a person and a garment worn by another person, our
goal is to generate a visualization of how the garment might look on the input
person. A key challenge is to synthesize a photorealistic detail-preserving
visualization of the garment, while warping the garment to accommodate a
significant body pose and shape change across the subjects. Previous methods
either focus on garment detail preservation without effective pose and shape
variation, or allow try-on with the desired shape and pose but lack garment
details. In this paper, we propose a diffusion-based architecture that unifies
two UNets (referred to as Parallel-UNet), which allows us to preserve garment
details and warp the garment for significant pose and body change in a single
network. The key ideas behind Parallel-UNet include: 1) garment is warped
implicitly via a cross attention mechanism, 2) garment warp and person blend
happen as part of a unified process as opposed to a sequence of two separate
tasks. Experimental results indicate that TryOnDiffusion achieves
state-of-the-art performance both qualitatively and quantitatively.",None,-1
a23c9fa0-8f64-4e82-8215-ca0444fa0249,Fairness for Workers Who Pull the Arms: An Index Based Policy for Allocation of Restless Bandit Tasks,0.649635,3,"Motivated by applications such as machine repair, project monitoring, and
anti-poaching patrol scheduling, we study intervention planning of stochastic
processes under resource constraints. This planning problem has previously been
modeled as restless multi-armed bandits (RMAB), where each arm is an
intervention-dependent Markov Decision Process. However, the existing
literature assumes all intervention resources belong to a single uniform pool,
limiting their applicability to real-world settings where interventions are
carried out by a set of workers, each with their own costs, budgets, and
intervention effects. In this work, we consider a novel RMAB setting, called
multi-worker restless bandits (MWRMAB) with heterogeneous workers. The goal is
to plan an intervention schedule that maximizes the expected reward while
satisfying budget constraints on each worker as well as fairness in terms of
the load assigned to each worker. Our contributions are two-fold: (1) we
provide a multi-worker extension of the Whittle index to tackle heterogeneous
costs and per-worker budget and (2) we develop an index-based scheduling policy
to achieve fairness. Further, we evaluate our method on various cost structures
and show that our method significantly outperforms other baselines in terms of
fairness without sacrificing much in reward accumulated.",None,-1
c4a54e18-5d6c-481f-96d5-2888e2d97f35,Learning from Multi-Perception Features for Real-Word Image Super-resolution,0.397103,4,"Currently, there are two popular approaches for addressing real-world image
super-resolution problems: degradation-estimation-based and blind-based
methods. However, degradation-estimation-based methods may be inaccurate in
estimating the degradation, making them less applicable to real-world LR
images. On the other hand, blind-based methods are often limited by their fixed
single perception information, which hinders their ability to handle diverse
perceptual characteristics. To overcome this limitation, we propose a novel SR
method called MPF-Net that leverages multiple perceptual features of input
images. Our method incorporates a Multi-Perception Feature Extraction (MPFE)
module to extract diverse perceptual information and a series of newly-designed
Cross-Perception Blocks (CPB) to combine this information for effective
super-resolution reconstruction. Additionally, we introduce a contrastive
regularization term (CR) that improves the model's learning capability by using
newly generated HR and LR images as positive and negative samples for ground
truth HR. Experimental results on challenging real-world SR datasets
demonstrate that our approach significantly outperforms existing
state-of-the-art methods in both qualitative and quantitative measures.",None,-1
2504bd63-288e-4d05-ad61-e249f4657f0c,Hallucination Detection for Grounded Instruction Generation,0.211527,4,"We investigate the problem of generating instructions to guide humans to
navigate in simulated residential environments. A major issue with current
models is hallucination: they generate references to actions or objects that
are inconsistent with what a human follower would perform or encounter along
the described path. We develop a model that detects these hallucinated
references by adopting a model pre-trained on a large corpus of image-text
pairs, and fine-tuning it with a contrastive loss that separates correct
instructions from instructions containing synthesized hallucinations. Our final
model outperforms several baselines, including using word probability estimated
by the instruction-generation model, and supervised models based on LSTM and
Transformer.",None,-1
efcd6de4-6869-4326-951b-6239b1894669,Expanding Scope: Adapting English Adversarial Attacks to Chinese,0.248114,3,"Recent studies have revealed that NLP predictive models are vulnerable to
adversarial attacks. Most existing studies focused on designing attacks to
evaluate the robustness of NLP models in the English language alone. Literature
has seen an increasing need for NLP solutions for other languages. We,
therefore, ask one natural question: whether state-of-the-art (SOTA) attack
methods generalize to other languages. This paper investigates how to adapt
SOTA adversarial attack algorithms in English to the Chinese language. Our
experiments show that attack methods previously applied to English NLP can
generate high-quality adversarial examples in Chinese when combined with proper
text segmentation and linguistic constraints. In addition, we demonstrate that
the generated adversarial examples can achieve high fluency and semantic
consistency by focusing on the Chinese language's morphology and phonology,
which in turn can be used to improve the adversarial robustness of Chinese NLP
models.",None,-1
de74be41-ea61-4451-8a58-4b60cd12a6cb,Why Oatmeal is Cheap: Kolmogorov Complexity and Procedural Generation,0.0382688,3,"Although procedural generation is popular among game developers, academic
research on the topic has primarily focused on new applications, with some
research into empirical analysis. In this paper we relate theoretical work in
information theory to the generation of content for games. We prove that there
is a relationship between the Kolomogorov complexity of the most complex
artifact a generator can produce, and the size of that generator's possibility
space. In doing so, we identify the limiting relationship between the knowledge
encoded in a generator, the density of its output space, and the intricacy of
the artifacts it produces. We relate our result to the experience of expert
procedural generator designers, and illustrate it with some examples.",None,-1
d2f76edf-38ac-4bdd-86b7-8e16cf31b627,A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram,0.395877,14,"Geometry problem solving (GPS) is a high-level mathematical reasoning
requiring the capacities of multi-modal fusion and geometric knowledge
application. Recently, neural solvers have shown great potential in GPS but
still be short in diagram presentation and modal fusion. In this work, we
convert diagrams into basic textual clauses to describe diagram features
effectively, and propose a new neural solver called PGPSNet to fuse multi-modal
information efficiently. Combining structural and semantic pre-training, data
augmentation and self-limited decoding, PGPSNet is endowed with rich knowledge
of geometry theorems and geometric representation, and therefore promotes
geometric understanding and reasoning. In addition, to facilitate the research
of GPS, we build a new large-scale and fine-annotated GPS dataset named PGPS9K,
labeled with both fine-grained diagram annotation and interpretable solution
program. Experiments on PGPS9K and an existing dataset Geometry3K validate the
superiority of our method over the state-of-the-art neural solvers. Our code,
dataset and appendix material are available at
\url{https://github.com/mingliangzhang2018/PGPS}.",None,-1
4f3577c9-11a6-4ab2-a7c5-dbf65bc29050,Multi-view Cross-Modality MR Image Translation for Vestibular Schwannoma and Cochlea Segmentation,0.552344,5,"In this work, we propose a multi-view image translation framework, which can
translate contrast-enhanced T1 (ceT1) MR imaging to high-resolution T2 (hrT2)
MR imaging for unsupervised vestibular schwannoma and cochlea segmentation. We
adopt two image translation models in parallel that use a pixel-level
consistent constraint and a patch-level contrastive constraint, respectively.
Thereby, we can augment pseudo-hrT2 images reflecting different perspectives,
which eventually lead to a high-performing segmentation model. Our experimental
results on the CrossMoDA challenge show that the proposed method achieved
enhanced performance on the vestibular schwannoma and cochlea segmentation.",None,-1
04d2ccc3-7d04-4087-8b5f-303f57953aa9,Learning Residual Elastic Warps for Image Stitching under Dirichlet Boundary Condition,0.207069,1,"Trendy suggestions for learning-based elastic warps enable the deep image
stitchings to align images exposed to large parallax errors. Despite the
remarkable alignments, the methods struggle with occasional holes or
discontinuity between overlapping and non-overlapping regions of a target image
as the applied training strategy mostly focuses on overlap region alignment. As
a result, they require additional modules such as seam finder and image
inpainting for hiding discontinuity and filling holes, respectively. In this
work, we suggest Recurrent Elastic Warps (REwarp) that address the problem with
Dirichlet boundary condition and boost performances by residual learning for
recurrent misalign correction. Specifically, REwarp predicts a homography and a
Thin-plate Spline (TPS) under the boundary constraint for discontinuity and
hole-free image stitching. Our experiments show the favorable aligns and the
competitive computational costs of REwarp compared to the existing stitching
methods. Our source code is available at https://github.com/minshu-kim/REwarp.",None,-1
ab672c10-a25a-4563-9c46-d7602f10c4c0,Online POMDP Planning with Anytime Deterministic Guarantees,0.814804,3,"Autonomous agents operating in real-world scenarios frequently encounter
uncertainty and make decisions based on incomplete information. Planning under
uncertainty can be mathematically formalized using partially observable Markov
decision processes (POMDPs). However, finding an optimal plan for POMDPs can be
computationally expensive and is feasible only for small tasks. In recent
years, approximate algorithms, such as tree search and sample-based
methodologies, have emerged as state-of-the-art POMDP solvers for larger
problems. Despite their effectiveness, these algorithms offer only
probabilistic and often asymptotic guarantees toward the optimal solution due
to their dependence on sampling. To address these limitations, we derive a
deterministic relationship between a simplified solution that is easier to
obtain and the theoretically optimal one. First, we derive bounds for selecting
a subset of the observations to branch from while computing a complete belief
at each posterior node. Then, since a complete belief update may be
computationally demanding, we extend the bounds to support reduction of both
the state and the observation spaces. We demonstrate how our guarantees can be
integrated with existing state-of-the-art solvers that sample a subset of
states and observations. As a result, the returned solution holds deterministic
bounds relative to the optimal policy. Lastly, we substantiate our findings
with supporting experimental results.",None,-1
08ef63ea-d2b0-4668-9642-0b59edf04cdc,LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models,0.25709,27,"Large language models (LLMs) have been applied in various applications due to
their astonishing capabilities. With advancements in technologies such as
chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed
to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of
tokens. To accelerate model inference and reduce cost, this paper presents
LLMLingua, a coarse-to-fine prompt compression method that involves a budget
controller to maintain semantic integrity under high compression ratios, a
token-level iterative compression algorithm to better model the interdependence
between compressed contents, and an instruction tuning based method for
distribution alignment between language models. We conduct experiments and
analysis over four datasets from different scenarios, i.e., GSM8K, BBH,
ShareGPT, and Arxiv-March23; showing that the proposed approach yields
state-of-the-art performance and allows for up to 20x compression with little
performance loss. Our code is available at https://aka.ms/LLMLingua.",None,-1
5cb3b6fe-052f-42d6-adb8-8627aecf80ff,USTC-NELSLIP at SemEval-2023 Task 2: Statistical Construction and Dual Adaptation of Gazetteer for Multilingual Complex NER,0.376754,2,"This paper describes the system developed by the USTC-NELSLIP team for
SemEval-2023 Task 2 Multilingual Complex Named Entity Recognition (MultiCoNER
II). A method named Statistical Construction and Dual Adaptation of Gazetteer
(SCDAG) is proposed for Multilingual Complex NER. The method first utilizes a
statistics-based approach to construct a gazetteer. Secondly, the
representations of gazetteer networks and language models are adapted by
minimizing the KL divergence between them at both the sentence-level and
entity-level. Finally, these two networks are then integrated for supervised
named entity recognition (NER) training. The proposed method is applied to
XLM-R with a gazetteer built from Wikidata, and shows great generalization
ability across different tracks. Experimental results and detailed analysis
verify the effectiveness of the proposed method. The official results show that
our system ranked 1st on one track (Hindi) in this task.",None,-1
e9ea385a-6d0f-4d50-b4ac-2764f308fa85,Question Decomposition Tree for Answering Complex Questions over Knowledge Bases,0.633472,9,"Knowledge base question answering (KBQA) has attracted a lot of interest in
recent years, especially for complex questions which require multiple facts to
answer. Question decomposition is a promising way to answer complex questions.
Existing decomposition methods split the question into sub-questions according
to a single compositionality type, which is not sufficient for questions
involving multiple compositionality types. In this paper, we propose Question
Decomposition Tree (QDT) to represent the structure of complex questions.
Inspired by recent advances in natural language generation (NLG), we present a
two-staged method called Clue-Decipher to generate QDT. It can leverage the
strong ability of NLG model and simultaneously preserve the original questions.
To verify that QDT can enhance KBQA task, we design a decomposition-based KBQA
system called QDTQA. Extensive experiments show that QDTQA outperforms previous
state-of-the-art methods on ComplexWebQuestions dataset. Besides, our
decomposition method improves an existing KBQA system by 12% and sets a new
state-of-the-art on LC-QuAD 1.0.",None,-1
7d39ba0b-f53e-4951-8f44-e9137dad45a6,To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation,0.294893,5,"The goal of Online Domain Adaptation for semantic segmentation is to handle
unforeseeable domain changes that occur during deployment, like sudden weather
events. However, the high computational costs associated with brute-force
adaptation make this paradigm unfeasible for real-world applications. In this
paper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training
framework for real-time domain adaptation. Our approach includes a
hardware-aware back-propagation orchestration agent (HAMT) and a dedicated
domain-shift detector that enables active control over when and how the model
is adapted (LT). Thanks to these advancements, our approach is capable of
performing semantic segmentation while simultaneously adapting at more than
29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy and
speed trade-off is demonstrated on OnDA and SHIFT benchmarks through
experimental results.",None,-1
4152ffe3-e47e-45c5-b3b0-683b26fa616e,Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning,0.156944,8,"Language models still struggle on moral reasoning, despite their impressive
performance in many other tasks. In particular, the Moral Scenarios task in
MMLU (Multi-task Language Understanding) is among the worst performing tasks
for many language models, including GPT-3. In this work, we propose a new
prompting framework, Thought Experiments, to teach language models to do better
moral reasoning using counterfactuals. Experiment results show that our
framework elicits counterfactual questions and answers from the model, which in
turn helps improve the accuracy on Moral Scenarios task by 9-16% compared to
other zero-shot baselines. Interestingly, unlike math reasoning tasks,
zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and
even reduces accuracy by around 4% compared to direct zero-shot. We further
observed that with minimal human supervision in the form of 5 few-shot
examples, the accuracy of the task can be improved to as much as 80%.",None,-1
38854433-579c-422f-b512-61f3b6bd3004,The Unfairness of Fair Machine Learning: Levelling down and strict egalitarianism by default,0.922064,23,"In recent years fairness in machine learning (ML) has emerged as a highly
active area of research and development. Most define fairness in simple terms,
where fairness means reducing gaps in performance or outcomes between
demographic groups while preserving as much of the accuracy of the original
system as possible. This oversimplification of equality through fairness
measures is troubling. Many current fairness measures suffer from both fairness
and performance degradation, or ""levelling down,"" where fairness is achieved by
making every group worse off, or by bringing better performing groups down to
the level of the worst off. When fairness can only be achieved by making
everyone worse off in material or relational terms through injuries of stigma,
loss of solidarity, unequal concern, and missed opportunities for substantive
equality, something would appear to have gone wrong in translating the vague
concept of 'fairness' into practice. This paper examines the causes and
prevalence of levelling down across fairML, and explore possible justifications
and criticisms based on philosophical and legal theories of equality and
distributive justice, as well as equality law jurisprudence. We find that
fairML does not currently engage in the type of measurement, reporting, or
analysis necessary to justify levelling down in practice. We propose a first
step towards substantive equality in fairML: ""levelling up"" systems by design
through enforcement of minimum acceptable harm thresholds, or ""minimum rate
constraints,"" as fairness constraints. We likewise propose an alternative
harms-based framework to counter the oversimplified egalitarian framing
currently dominant in the field and push future discussion more towards
substantive equality opportunities and away from strict egalitarianism by
default. N.B. Shortened abstract, see paper for full abstract.",None,-1
2ac9b985-1c74-4a40-82f7-f1a978d6cc29,HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding,0.289983,2,"Understanding comprehensive assembly knowledge from videos is critical for
futuristic ultra-intelligent industry. To enable technological breakthrough, we
present HA-ViD - the first human assembly video dataset that features
representative industrial assembly scenarios, natural procedural knowledge
acquisition process, and consistent human-robot shared annotations.
Specifically, HA-ViD captures diverse collaboration patterns of real-world
assembly, natural human behaviors and learning progression during assembly, and
granulate action annotations to subject, action verb, manipulated object,
target object, and tool. We provide 3222 multi-view, multi-modality videos
(each video contains one assembly task), 1.5M frames, 96K temporal labels and
2M spatial labels. We benchmark four foundational video understanding tasks:
action recognition, action segmentation, object detection and multi-object
tracking. Importantly, we analyze their performance for comprehending knowledge
in assembly progress, process efficiency, task collaboration, skill parameters
and human intention. Details of HA-ViD is available at:
https://iai-hrc.github.io/ha-vid.",None,-1
dadb7ed3-f800-4598-b2db-05a16e7f68e9,Sampling-based Uncertainty Estimation for an Instance Segmentation Network,0.121466,1,"The examination of uncertainty in the predictions of machine learning (ML)
models is receiving increasing attention. One uncertainty modeling technique
used for this purpose is Monte-Carlo (MC)-Dropout, where repeated predictions
are generated for a single input. Therefore, clustering is required to describe
the resulting uncertainty, but only through efficient clustering is it possible
to describe the uncertainty from the model attached to each object. This
article uses Bayesian Gaussian Mixture (BGM) to solve this problem. In
addition, we investigate different values for the dropout rate and other
techniques, such as focal loss and calibration, which we integrate into the
Mask-RCNN model to obtain the most accurate uncertainty approximation of each
instance and showcase it graphically.",None,-1
53e058c8-f07e-419a-af10-5cfc73566c04,An Adaptive Kernel Approach to Federated Learning of Heterogeneous Causal Effects,0.399205,10,"We propose a new causal inference framework to learn causal effects from
multiple, decentralized data sources in a federated setting. We introduce an
adaptive transfer algorithm that learns the similarities among the data sources
by utilizing Random Fourier Features to disentangle the loss function into
multiple components, each of which is associated with a data source. The data
sources may have different distributions; the causal effects are independently
and systematically incorporated. The proposed method estimates the similarities
among the sources through transfer coefficients, and hence requiring no prior
information about the similarity measures. The heterogeneous causal effects can
be estimated with no sharing of the raw training data among the sources, thus
minimizing the risk of privacy leak. We also provide minimax lower bounds to
assess the quality of the parameters learned from the disparate sources. The
proposed method is empirically shown to outperform the baselines on
decentralized data sources with dissimilar distributions.",None,-1
90155e7c-10f4-4082-be4b-09db747662d7,Arabic Dialect Identification under Scrutiny: Limitations of Single-label Classification,0.972463,5,"Automatic Arabic Dialect Identification (ADI) of text has gained great
popularity since it was introduced in the early 2010s. Multiple datasets were
developed, and yearly shared tasks have been running since 2018. However, ADI
systems are reported to fail in distinguishing between the micro-dialects of
Arabic. We argue that the currently adopted framing of the ADI task as a
single-label classification problem is one of the main reasons for that. We
highlight the limitation of the incompleteness of the Dialect labels and
demonstrate how it impacts the evaluation of ADI systems. A manual error
analysis for the predictions of an ADI, performed by 7 native speakers of
different Arabic dialects, revealed that $\approx$ 66% of the validated errors
are not true errors. Consequently, we propose framing ADI as a multi-label
classification task and give recommendations for designing new ADI datasets.",None,-1
d0b1ce27-15aa-4f5a-ba38-818408fc6f25,To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer,0.195499,1,"Choosing an appropriate tokenization scheme is often a bottleneck in
low-resource cross-lingual transfer. To understand the downstream implications
of text representation choices, we perform a comparative analysis on language
models having diverse text representation modalities including 2
segmentation-based models (\texttt{BERT}, \texttt{mBERT}), 1 image-based model
(\texttt{PIXEL}), and 1 character-level model (\texttt{CANINE}). First, we
propose a scoring Language Quotient (LQ) metric capable of providing a weighted
representation of both zero-shot and few-shot evaluation combined. Utilizing
this metric, we perform experiments comprising 19 source languages and 133
target languages on three tasks (POS tagging, Dependency parsing, and NER). Our
analysis reveals that image-based models excel in cross-lingual transfer when
languages are closely related and share visually similar scripts. However, for
tasks biased toward word meaning (POS, NER), segmentation-based models prove to
be superior. Furthermore, in dependency parsing tasks where word relationships
play a crucial role, models with their character-level focus, outperform
others. Finally, we propose a recommendation scheme based on our findings to
guide model selection according to task and language requirements.",None,-1
5c227547-e55d-4293-89c7-6fe6b4d30333,DoE2Vec: Deep-learning Based Features for Exploratory Landscape Analysis,0.797644,9,"We propose DoE2Vec, a variational autoencoder (VAE)-based methodology to
learn optimization landscape characteristics for downstream meta-learning
tasks, e.g., automated selection of optimization algorithms. Principally, using
large training data sets generated with a random function generator, DoE2Vec
self-learns an informative latent representation for any design of experiments
(DoE). Unlike the classical exploratory landscape analysis (ELA) method, our
approach does not require any feature engineering and is easily applicable for
high dimensional search spaces. For validation, we inspect the quality of
latent reconstructions and analyze the latent representations using different
experiments. The latent representations not only show promising potentials in
identifying similar (cheap-to-evaluate) surrogate functions, but also can
significantly boost performances when being used complementary to the classical
ELA features in classification tasks.",None,-1
dc83a895-168f-448f-b3b3-60bbb8bc3717,The Music Note Ontology,0.266187,3,"In this paper we propose the Music Note Ontology, an ontology for modelling
music notes and their realisation. The ontology addresses the relation between
a note represented in a symbolic representation system, and its realisation,
i.e. a musical performance. This work therefore aims to solve the modelling and
representation issues that arise when analysing the relationships between
abstract symbolic features and the corresponding physical features of an audio
signal. The ontology is composed of three different Ontology Design Patterns
(ODP), which model the structure of the score (Score Part Pattern), the note in
the symbolic notation (Music Note Pattern) and its realisation (Musical Object
Pattern).",None,-1
a8f70d21-ddc1-4a84-b45a-614e3f65eed0,Visually Grounded Keyword Detection and Localisation for Low-Resource Languages,0.14658,1,"This study investigates the use of Visually Grounded Speech (VGS) models for
keyword localisation in speech. The study focusses on two main research
questions: (1) Is keyword localisation possible with VGS models and (2) Can
keyword localisation be done cross-lingually in a real low-resource setting?
Four methods for localisation are proposed and evaluated on an English dataset,
with the best-performing method achieving an accuracy of 57%. A new dataset
containing spoken captions in Yoruba language is also collected and released
for cross-lingual keyword localisation. The cross-lingual model obtains a
precision of 16% in actual keyword localisation and this performance can be
improved by initialising from a model pretrained on English data. The study
presents a detailed analysis of the model's success and failure modes and
highlights the challenges of using VGS models for keyword localisation in
low-resource settings.",None,-1
4eb79b8c-93cb-4900-a6d2-41743ec6f059,Few-Shot Domain Adaptation for Charge Prediction on Unprofessional Descriptions,0.0546637,1,"Recent works considering professional legal-linguistic style (PLLS) texts
have shown promising results on the charge prediction task. However,
unprofessional users also show an increasing demand on such a prediction
service. There is a clear domain discrepancy between PLLS texts and non-PLLS
texts expressed by those laypersons, which degrades the current SOTA models'
performance on non-PLLS texts. A key challenge is the scarcity of non-PLLS data
for most charge classes. This paper proposes a novel few-shot domain adaptation
(FSDA) method named Disentangled Legal Content for Charge Prediction (DLCCP).
Compared with existing FSDA works, which solely perform instance-level
alignment without considering the negative impact of text style information
existing in latent features, DLCCP (1) disentangles the content and style
representations for better domain-invariant legal content learning with
carefully designed optimization goals for content and style spaces and, (2)
employs the constitutive elements knowledge of charges to extract and align
element-level and instance-level content representations simultaneously. We
contribute the first publicly available non-PLLS dataset named NCCP for
developing layperson-friendly charge prediction models. Experiments on NCCP
show the superiority of our methods over competitive baselines.",None,-1
20e41568-d2cc-412b-889a-3b7c5760060b,ConKI: Contrastive Knowledge Injection for Multimodal Sentiment Analysis,0.0919453,1,"Multimodal Sentiment Analysis leverages multimodal signals to detect the
sentiment of a speaker. Previous approaches concentrate on performing
multimodal fusion and representation learning based on general knowledge
obtained from pretrained models, which neglects the effect of domain-specific
knowledge. In this paper, we propose Contrastive Knowledge Injection (ConKI)
for multimodal sentiment analysis, where specific-knowledge representations for
each modality can be learned together with general knowledge representations
via knowledge injection based on an adapter architecture. In addition, ConKI
uses a hierarchical contrastive learning procedure performed between knowledge
types within every single modality, across modalities within each sample, and
across samples to facilitate the effective learning of the proposed
representations, hence improving multimodal sentiment predictions. The
experiments on three popular multimodal sentiment analysis benchmarks show that
ConKI outperforms all prior methods on a variety of performance metrics.",None,-1
1a3f6fec-0be8-463d-a70f-a9dd8de7ba37,Hybrid Modeling Design Patterns,0.111027,1,"Design patterns provide a systematic way to convey solutions to recurring
modeling challenges. This paper introduces design patterns for hybrid modeling,
an approach that combines modeling based on first principles with data-driven
modeling techniques. While both approaches have complementary advantages there
are often multiple ways to combine them into a hybrid model, and the
appropriate solution will depend on the problem at hand. In this paper, we
provide four base patterns that can serve as blueprints for combining
data-driven components with domain knowledge into a hybrid approach. In
addition, we also present two composition patterns that govern the combination
of the base patterns into more complex hybrid models. Each design pattern is
illustrated by typical use cases from application areas such as climate
modeling, engineering, and physics.",None,-1
c00f2395-8dcd-43c7-a861-a8bfe27af073,GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution,0.175792,7,"Augmenting large language models (LLM) to use external tools enhances their
performance across a variety of tasks. However, prior works over-rely on
task-specific demonstration of tool use that limits their generalizability and
computational cost due to making many calls to large-scale LLMs. We introduce
GEAR, a computationally efficient query-tool grounding algorithm that is
generalizable to various tasks that require tool use while not relying on
task-specific demonstrations. GEAR achieves better efficiency by delegating
tool grounding and execution to small language models (SLM) and LLM,
respectively; while leveraging semantic and pattern-based evaluation at both
question and answer levels for generalizable tool grounding. We evaluate GEAR
on 14 datasets across 6 downstream tasks, demonstrating its strong
generalizability to novel tasks, tools and different SLMs. Despite offering
more efficiency, GEAR achieves higher precision in tool grounding compared to
prior strategies using LLM prompting, thus improving downstream accuracy at a
reduced computational cost. For example, we demonstrate that GEAR-augmented
GPT-J and GPT-3 outperform counterpart tool-augmented baselines because of
better tool use.",None,-1
8ef08ff5-3151-4e51-83b4-46bc7a25530d,ScribbleVC: Scribble-supervised Medical Image Segmentation with Vision-Class Embedding,0.569771,8,"Medical image segmentation plays a critical role in clinical decision-making,
treatment planning, and disease monitoring. However, accurate segmentation of
medical images is challenging due to several factors, such as the lack of
high-quality annotation, imaging noise, and anatomical differences across
patients. In addition, there is still a considerable gap in performance between
the existing label-efficient methods and fully-supervised methods. To address
the above challenges, we propose ScribbleVC, a novel framework for
scribble-supervised medical image segmentation that leverages vision and class
embeddings via the multimodal information enhancement mechanism. In addition,
ScribbleVC uniformly utilizes the CNN features and Transformer features to
achieve better visual feature extraction. The proposed method combines a
scribble-based approach with a segmentation network and a class-embedding
module to produce accurate segmentation masks. We evaluate ScribbleVC on three
benchmark datasets and compare it with state-of-the-art methods. The
experimental results demonstrate that our method outperforms existing
approaches in terms of accuracy, robustness, and efficiency. The datasets and
code are released on GitHub.",None,-1
041bf4c9-22ea-45d3-847c-ac51e9526cb3,Watch out Venomous Snake Species: A Solution to SnakeCLEF2023,0.779217,3,"The SnakeCLEF2023 competition aims to the development of advanced algorithms
for snake species identification through the analysis of images and
accompanying metadata. This paper presents a method leveraging utilization of
both images and metadata. Modern CNN models and strong data augmentation are
utilized to learn better representation of images. To relieve the challenge of
long-tailed distribution, seesaw loss is utilized in our method. We also design
a light model to calculate prior probabilities using metadata features
extracted from CLIP in post processing stage. Besides, we attach more
importance to venomous species by assigning venomous species labels to some
examples that model is uncertain about. Our method achieves 91.31% score of the
final metric combined of F1 and other metrics on private leaderboard, which is
the 1st place among the participators. The code is available at
https://github.com/xiaoxsparraw/CLEF2023.",None,-1
40e660db-50ba-4201-b78f-707648ec000f,Heterogeneous Neuronal and Synaptic Dynamics for Spike-Efficient Unsupervised Learning: Theory and Design Principles,0.788563,7,"This paper shows that the heterogeneity in neuronal and synaptic dynamics
reduces the spiking activity of a Recurrent Spiking Neural Network (RSNN) while
improving prediction performance, enabling spike-efficient (unsupervised)
learning. We analytically show that the diversity in neurons'
integration/relaxation dynamics improves an RSNN's ability to learn more
distinct input patterns (higher memory capacity), leading to improved
classification and prediction performance. We further prove that heterogeneous
Spike-Timing-Dependent-Plasticity (STDP) dynamics of synapses reduce spiking
activity but preserve memory capacity. The analytical results motivate
Heterogeneous RSNN design using Bayesian optimization to determine
heterogeneity in neurons and synapses to improve $\mathcal{E}$, defined as the
ratio of spiking activity and memory capacity. The empirical results on time
series classification and prediction tasks show that optimized HRSNN increases
performance and reduces spiking activity compared to a homogeneous RSNN.",None,-1
e0e4ed31-4f71-4e19-80e1-9f9064952b08,Hierarchical Evaluation Framework: Best Practices for Human Evaluation,0.219142,3,"Human evaluation plays a crucial role in Natural Language Processing (NLP) as
it assesses the quality and relevance of developed systems, thereby
facilitating their enhancement. However, the absence of widely accepted human
evaluation metrics in NLP hampers fair comparisons among different systems and
the establishment of universal assessment standards. Through an extensive
analysis of existing literature on human evaluation metrics, we identified
several gaps in NLP evaluation methodologies. These gaps served as motivation
for developing our own hierarchical evaluation framework. The proposed
framework offers notable advantages, particularly in providing a more
comprehensive representation of the NLP system's performance. We applied this
framework to evaluate the developed Machine Reading Comprehension system, which
was utilized within a human-AI symbiosis model. The results highlighted the
associations between the quality of inputs and outputs, underscoring the
necessity to evaluate both components rather than solely focusing on outputs.
In future work, we will investigate the potential time-saving benefits of our
proposed framework for evaluators assessing NLP systems.",None,-1
2034b00a-eb79-4131-9ab2-6c3a4c0762de,Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features,0.704495,3,"A challenge towards developing NLP systems for the world's languages is
understanding how they generalize to typological differences relevant for
real-world applications. To this end, we propose M2C, a morphologically-aware
framework for behavioral testing of NLP models. We use M2C to generate tests
that probe models' behavior in light of specific linguistic features in 12
typologically diverse languages. We evaluate state-of-the-art language models
on the generated tests. While models excel at most tests in English, we
highlight generalization failures to specific typological characteristics such
as temporal expressions in Swahili and compounding possessives in Finish. Our
findings motivate the development of models that address these blind spots.",None,-1
f608a136-9e3c-4df5-9d7f-08e72497a520,Analyzing Multiple-Choice Reading and Listening Comprehension Tests,0.234017,2,"Multiple-choice reading and listening comprehension tests are an important
part of language assessment. Content creators for standard educational tests
need to carefully curate questions that assess the comprehension abilities of
candidates taking the tests. However, recent work has shown that a large number
of questions in general multiple-choice reading comprehension datasets can be
answered without comprehension, by leveraging world knowledge instead. This
work investigates how much of a contextual passage needs to be read in
multiple-choice reading based on conversation transcriptions and listening
comprehension tests to be able to work out the correct answer. We find that
automated reading comprehension systems can perform significantly better than
random with partial or even no access to the context passage. These findings
offer an approach for content creators to automatically capture the trade-off
between comprehension and world knowledge required for their proposed
questions.",None,-1
4a24f2b2-13db-4a57-a435-27613e83ebed,Improving Language Models via Plug-and-Play Retrieval Feedback,0.725941,49,"Large language models (LLMs) exhibit remarkable performance across various
NLP tasks. However, they often generate incorrect or hallucinated information,
which hinders their practical applicability in real-world scenarios. Human
feedback has been shown to effectively enhance the factuality and quality of
generated content, addressing some of these limitations. However, this approach
is resource-intensive, involving manual input and supervision, which can be
time-consuming and expensive. Moreover, it cannot be provided during inference,
further limiting its practical utility in dynamic and interactive applications.
In this paper, we introduce ReFeed, a novel pipeline designed to enhance LLMs
by providing automatic retrieval feedback in a plug-and-play framework without
the need for expensive fine-tuning. ReFeed first generates initial outputs,
then utilizes a retrieval model to acquire relevant information from large
document collections, and finally incorporates the retrieved information into
the in-context demonstration for output refinement, thereby addressing the
limitations of LLMs in a more efficient and cost-effective manner. Experiments
on four knowledge-intensive benchmark datasets demonstrate our proposed ReFeed
could improve over +6.0% under zero-shot setting and +2.5% under few-shot
setting, compared to baselines without using retrieval feedback.",None,-1
34d994c1-2347-4b6b-aec6-d0ad2f20277d,Concept Learning for Interpretable Multi-Agent Reinforcement Learning,0.724004,9,"Multi-agent robotic systems are increasingly operating in real-world
environments in close proximity to humans, yet are largely controlled by policy
models with inscrutable deep neural network representations. We introduce a
method for incorporating interpretable concepts from a domain expert into
models trained through multi-agent reinforcement learning, by requiring the
model to first predict such concepts then utilize them for decision making.
This allows an expert to both reason about the resulting concept policy models
in terms of these high-level concepts at run-time, as well as intervene and
correct mispredictions to improve performance. We show that this yields
improved interpretability and training stability, with benefits to policy
performance and sample efficiency in a simulated and real-world
cooperative-competitive multi-agent game.",None,-1
f2c1ea27-2e77-4c47-babd-3e3cfa8e059f,DartsReNet: Exploring new RNN cells in ReNet architectures,0.0486543,4,"We present new Recurrent Neural Network (RNN) cells for image classification
using a Neural Architecture Search (NAS) approach called DARTS. We are
interested in the ReNet architecture, which is a RNN based approach presented
as an alternative for convolutional and pooling steps. ReNet can be defined
using any standard RNN cells, such as LSTM and GRU. One limitation is that
standard RNN cells were designed for one dimensional sequential data and not
for two dimensions like it is the case for image classification. We overcome
this limitation by using DARTS to find new cell designs. We compare our results
with ReNet that uses GRU and LSTM cells. Our found cells outperform the
standard RNN cells on CIFAR-10 and SVHN. The improvements on SVHN indicate
generalizability, as we derived the RNN cell designs from CIFAR-10 without
performing a new cell search for SVHN.",None,-1
0ad3c8da-ce77-4151-937c-6a80b1623a81,NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes,0.857539,36,"With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis
has recently made a big leap forward. At the core, NeRF proposes that each 3D
point can emit radiance, allowing to conduct view synthesis using
differentiable volumetric rendering. While neural radiance fields can
accurately represent 3D scenes for computing the image rendering, 3D meshes are
still the main scene representation supported by most computer graphics and
simulation pipelines, enabling tasks such as real time rendering and
physics-based simulations. Obtaining 3D meshes from neural radiance fields
still remains an open challenge since NeRFs are optimized for view synthesis,
not enforcing an accurate underlying geometry on the radiance field. We thus
propose a novel compact and flexible architecture that enables easy 3D surface
reconstruction from any NeRF-driven approach. Upon having trained the radiance
field, we distill the volumetric 3D representation into a Signed Surface
Approximation Network, allowing easy extraction of the 3D mesh and appearance.
Our final 3D mesh is physically accurate and can be rendered in real time on an
array of devices.",None,-1
e35377f6-fc01-45d1-8609-cb406aca524e,Feature Collapse,0.054159,2,"We formalize and study a phenomenon called feature collapse that makes
precise the intuitive idea that entities playing a similar role in a learning
task receive similar representations. As feature collapse requires a notion of
task, we leverage a simple but prototypical NLP task to study it. We start by
showing experimentally that feature collapse goes hand in hand with
generalization. We then prove that, in the large sample limit, distinct words
that play identical roles in this NLP task receive identical local feature
representations in a neural network. This analysis reveals the crucial role
that normalization mechanisms, such as LayerNorm, play in feature collapse and
in generalization.",None,-1
cead4ab6-ea02-4376-ab97-bba910ab823c,Fine-Tuning BERT with Character-Level Noise for Zero-Shot Transfer to Dialects and Closely-Related Languages,0.46841,7,"In this work, we induce character-level noise in various forms when
fine-tuning BERT to enable zero-shot cross-lingual transfer to unseen dialects
and languages. We fine-tune BERT on three sentence-level classification tasks
and evaluate our approach on an assortment of unseen dialects and languages. We
find that character-level noise can be an extremely effective agent of
cross-lingual transfer under certain conditions, while it is not as helpful in
others. Specifically, we explore these differences in terms of the nature of
the task and the relationships between source and target languages, finding
that introduction of character-level noise during fine-tuning is particularly
helpful when a task draws on surface level cues and the source-target
cross-lingual pair has a relatively high lexical overlap with shorter (i.e.,
less meaningful) unseen tokens on average.",None,-1
cb8b4ced-7510-4863-8908-e7da435ab31c,Extending Multilingual Machine Translation through Imitation Learning,0.311811,2,"Despite the growing variety of languages supported by existing multilingual
neural machine translation (MNMT) models, most of the world's languages are
still being left behind. We aim to extend large-scale MNMT models to a new
language, allowing for translation between the newly added and all of the
already supported languages in a challenging scenario: using only a parallel
corpus between the new language and English. Previous approaches, such as
continued training on parallel data including the new language, suffer from
catastrophic forgetting (i.e., performance on other languages is reduced). Our
novel approach Imit-MNMT treats the task as an imitation learning process,
which mimicks the behavior of an expert, a technique widely used in the
computer vision area, but not well explored in NLP. More specifically, we
construct a pseudo multi-parallel corpus of the new and the original languages
by pivoting through English, and imitate the output distribution of the
original MNMT model. Extensive experiments show that our approach significantly
improves the translation performance between the new and the original
languages, without severe catastrophic forgetting. We also demonstrate that our
approach is capable of solving copy and off-target problems, which are two
common issues existence in current large-scale MNMT models.",None,-1
cc29f938-f803-4cf2-8292-3b17aa178137,GlotScript: A Resource and Tool for Low Resource Writing System Identification,0.984707,5,"We present GlotScript, an open resource and tool for low resource writing
system identification. GlotScript-R is a resource that provides the attested
writing systems for more than 7,000 languages. It is compiled by aggregating
information from existing writing system resources. GlotScript-T is a writing
system identification tool that covers all 161 Unicode 15.0 scripts. For an
input text, it returns its script distribution where scripts are identified by
ISO 15924 codes. We also present two use cases for GlotScript. First, we
demonstrate that GlotScript can help cleaning multilingual corpora such as mC4
and OSCAR. Second, we analyze the tokenization of a number of language models
such as GPT-4 using GlotScript and provide insights on the coverage of low
resource scripts and languages by each language model. We hope that GlotScript
will become a useful resource for work on low resource languages in the NLP
community. GlotScript-R and GlotScript-T are available at
https://github.com/cisnlp/GlotScript.",None,-1
a9a9fb57-dcad-4694-821e-89217b5c5d9e,RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks,0.949104,10,"Spiking Neural Networks (SNNs) as one of the biology-inspired models have
received much attention recently. It can significantly reduce energy
consumption since they quantize the real-valued membrane potentials to 0/1
spikes to transmit information thus the multiplications of activations and
weights can be replaced by additions when implemented on hardware. However,
this quantization mechanism will inevitably introduce quantization error, thus
causing catastrophic information loss. To address the quantization error
problem, we propose a regularizing membrane potential loss (RMP-Loss) to adjust
the distribution which is directly related to quantization error to a range
close to the spikes. Our method is extremely simple to implement and
straightforward to train an SNN. Furthermore, it is shown to consistently
outperform previous state-of-the-art methods over different network
architectures and datasets.",None,-1
63c5920c-54f3-4044-94b7-1ee3843f7028,Short Answer Grading Using One-shot Prompting and Text Similarity Scoring Model,0.796142,5,"In this study, we developed an automated short answer grading (ASAG) model
that provided both analytic scores and final holistic scores. Short answer
items typically consist of multiple sub-questions, and providing an analytic
score and the text span relevant to each sub-question can increase the
interpretability of the automated scores. Furthermore, they can be used to
generate actionable feedback for students. Despite these advantages, most
studies have focused on predicting only holistic scores due to the difficulty
in constructing dataset with manual annotations. To address this difficulty, we
used large language model (LLM)-based one-shot prompting and a text similarity
scoring model with domain adaptation using small manually annotated dataset.
The accuracy and quadratic weighted kappa of our model were 0.67 and 0.71 on a
subset of the publicly available ASAG dataset. The model achieved a substantial
improvement over the majority baseline.",None,-1
744d7cfc-8291-4531-a1f6-f7fbd91b7f3c,Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts,0.59686,4,"Whisper is a multitask and multilingual speech model covering 99 languages.
It yields commendable automatic speech recognition (ASR) results in a subset of
its covered languages, but the model still underperforms on a non-negligible
number of under-represented languages, a problem exacerbated in smaller model
versions. In this work, we propose DistilWhisper, an approach able to bridge
the performance gap in ASR for these languages while retaining the advantages
of multitask and multilingual capabilities. Our approach involves two key
strategies: lightweight modular ASR fine-tuning of whisper-small using
language-specific experts, and knowledge distillation from whisper-large-v2.
This dual approach allows us to effectively boost ASR performance while keeping
the robustness inherited from the multitask and multilingual pre-training.
Results demonstrate that our approach is more effective than standard
fine-tuning or LoRA adapters, boosting performance in the targeted languages
for both in- and out-of-domain test sets, while introducing only a negligible
parameter overhead at inference.",None,-1
9b511877-c20f-462f-bde0-5599d4b33c27,A Frustratingly Easy Improvement for Position Embeddings via Random Padding,0.761132,5,"Position embeddings, encoding the positional relationships among tokens in
text sequences, make great contributions to modeling local context features in
Transformer-based pre-trained language models. However, in Extractive Question
Answering, position embeddings trained with instances of varied context lengths
may not perform well as we expect. Since the embeddings of rear positions are
updated fewer times than the front position embeddings, the rear ones may not
be properly trained. In this paper, we propose a simple but effective strategy,
Random Padding, without any modifications to architectures of existing
pre-trained language models. We adjust the token order of input sequences when
fine-tuning, to balance the number of updating times of every position
embedding. Experiments show that Random Padding can significantly improve model
performance on the instances whose answers are located at rear positions,
especially when models are trained on short contexts but evaluated on long
contexts. Our code and data will be released for future research.",None,-1
8f8b16e0-5d83-46a1-8928-bc30b8599390,SepVAE: a contrastive VAE to separate pathological patterns from healthy ones,0.360889,4,"Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders
(VAEs) that aims at separating the common factors of variation between a
background dataset (BG) (i.e., healthy subjects) and a target dataset (TG)
(i.e., patients) from the ones that only exist in the target dataset. To do so,
these methods separate the latent space into a set of salient features (i.e.,
proper to the target dataset) and a set of common features (i.e., exist in both
datasets). Currently, all models fail to prevent the sharing of information
between latent spaces effectively and to capture all salient factors of
variation. To this end, we introduce two crucial regularization losses: a
disentangling term between common and salient representations and a
classification term between background and target samples in the salient space.
We show a better performance than previous CA-VAEs methods on three medical
applications and a natural images dataset (CelebA). Code and datasets are
available on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.",None,-1
64312b62-af73-4426-943f-5a55edc62415,Multimodal Machine Unlearning,0.731273,4,"Machine Unlearning is the process of removing specific training data samples
and their corresponding effects from an already trained model. It has
significant practical benefits, such as purging private, inaccurate, or
outdated information from trained models without the need for complete
re-training. Unlearning within a multimodal setting presents unique challenges
due to the intrinsic dependencies between different data modalities and the
expensive cost of training on large multimodal datasets and architectures.
Current approaches to machine unlearning have not fully addressed these
challenges. To bridge this gap, we introduce MMUL, a machine unlearning
approach specifically designed for multimodal data and models. MMUL formulates
the multimodal unlearning task by focusing on three key properties: (a):
modality decoupling, which effectively decouples the association between
individual unimodal data points within multimodal inputs marked for deletion,
rendering them as unrelated data points within the model's context, (b):
unimodal knowledge retention, which retains the unimodal representation
capability of the model post-unlearning, and (c): multimodal knowledge
retention, which retains the multimodal representation capability of the model
post-unlearning. MMUL is efficient to train and is not constrained by the
requirement of using a strongly convex loss. Experiments on two multimodal
models and four multimodal benchmark datasets, including vision-language and
graph-language datasets, show that MMUL outperforms existing baselines, gaining
an average improvement of +17.6 points against the best-performing unimodal
baseline in distinguishing between deleted and remaining data. In addition,
MMUL can largely maintain pre-existing knowledge of the original model post
unlearning, with a performance gap of only 0.3 points compared to retraining a
new model from scratch.",None,-1
2c3ffb9e-3614-4592-9615-706255401e01,Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play,0.999662,27,"Teaching dexterity to multi-fingered robots has been a longstanding challenge
in robotics. Most prominent work in this area focuses on learning controllers
or policies that either operate on visual observations or state estimates
derived from vision. However, such methods perform poorly on fine-grained
manipulation tasks that require reasoning about contact forces or about objects
occluded by the hand itself. In this work, we present T-Dex, a new approach for
tactile-based dexterity, that operates in two phases. In the first phase, we
collect 2.5 hours of play data, which is used to train self-supervised tactile
encoders. This is necessary to bring high-dimensional tactile readings to a
lower-dimensional embedding. In the second phase, given a handful of
demonstrations for a dexterous task, we learn non-parametric policies that
combine the tactile observations with visual ones. Across five challenging
dexterous tasks, we show that our tactile-based dexterity models outperform
purely vision and torque-based models by an average of 1.7X. Finally, we
provide a detailed analysis on factors critical to T-Dex including the
importance of play data, architectures, and representation learning.",None,-1
43acfabf-3225-427f-9197-94dd5d53f0e9,Local Region Perception and Relationship Learning Combined with Feature Fusion for Facial Action Unit Detection,0.93233,9,"Human affective behavior analysis plays a vital role in human-computer
interaction (HCI) systems. In this paper, we introduce our submission to the
CVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW). We
propose a single-stage trained AU detection framework. Specifically, in order
to effectively extract facial local region features related to AU detection, we
use a local region perception module to effectively extract features of
different AUs. Meanwhile, we use a graph neural network-based relational
learning module to capture the relationship between AUs. In addition,
considering the role of the overall feature of the target face on AU detection,
we also use the feature fusion module to fuse the feature information extracted
by the backbone network and the AU feature information extracted by the
relationship learning module. We also adopted some sampling methods, data
augmentation techniques and post-processing strategies to further improve the
performance of the model.",None,-1
5e938ceb-2c74-4873-80ed-91dc58c4085d,OBJECT 3DIT: Language-guided 3D-aware Image Editing,0.843393,16,"Existing image editing tools, while powerful, typically disregard the
underlying 3D geometry from which the image is projected. As a result, edits
made using these tools may become detached from the geometry and lighting
conditions that are at the foundation of the image formation process. In this
work, we formulate the newt ask of language-guided 3D-aware editing, where
objects in an image should be edited according to a language instruction in
context of the underlying 3D scene. To promote progress towards this goal, we
release OBJECT: a dataset consisting of 400K editing examples created from
procedurally generated 3D scenes. Each example consists of an input image,
editing instruction in language, and the edited image. We also introduce 3DIT :
single and multi-task models for four editing tasks. Our models show impressive
abilities to understand the 3D composition of entire scenes, factoring in
surrounding objects, surfaces, lighting conditions, shadows, and
physically-plausible object configurations. Surprisingly, training on only
synthetic scenes from OBJECT, editing capabilities of 3DIT generalize to
real-world images.",None,-1
e4a6a3ab-8332-4232-a0a4-2b2c7be14868,Enhancing Trust in LLM-Based AI Automation Agents: New Considerations and Future Challenges,0.969803,7,"Trust in AI agents has been extensively studied in the literature, resulting
in significant advancements in our understanding of this field. However, the
rapid advancements in Large Language Models (LLMs) and the emergence of
LLM-based AI agent frameworks pose new challenges and opportunities for further
research. In the field of process automation, a new generation of AI-based
agents has emerged, enabling the execution of complex tasks. At the same time,
the process of building automation has become more accessible to business users
via user-friendly no-code tools and training mechanisms. This paper explores
these new challenges and opportunities, analyzes the main aspects of trust in
AI agents discussed in existing literature, and identifies specific
considerations and challenges relevant to this new generation of automation
agents. We also evaluate how nascent products in this category address these
considerations. Finally, we highlight several challenges that the research
community should address in this evolving landscape.",None,-1
c5e33c28-82fb-4a69-97a1-0500578e5c87,Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations,0.77426,16,"The language ability of Large Language Models (LLMs) is often unbalanced
towards English because of the imbalance in the distribution of the
pre-training data. This disparity is demanded in further fine-tuning and
affecting the cross-lingual abilities of LLMs. In this paper, we propose to
empower Instructiontuned LLMs (It-LLMs) in languages other than English by
building semantic alignment between them. Hence, we propose CrossAlpaca, an
It-LLM with cross-lingual instruction-following and Translation-following
demonstrations to improve semantic alignment between languages. We validate our
approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA
and adapted versions of MMLU and BBH. Our models, tested over six different
languages, outperform the It-LLMs tuned on monolingual data. The final results
show that instruction tuning on non-English data is not enough and that
semantic alignment can be further improved by Translation-following
demonstrations.",None,-1
06b59f20-056a-4950-ab0d-6b31b86d3914,Songs Across Borders: Singable and Controllable Neural Lyric Translation,0.484255,4,"The development of general-domain neural machine translation (NMT) methods
has advanced significantly in recent years, but the lack of naturalness and
musical constraints in the outputs makes them unable to produce singable lyric
translations. This paper bridges the singability quality gap by formalizing
lyric translation into a constrained translation problem, converting
theoretical guidance and practical techniques from translatology literature to
prompt-driven NMT approaches, exploring better adaptation methods, and
instantiating them to an English-Chinese lyric translation system. Our model
achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and
word boundary recall. In our subjective evaluation, our model shows a 75%
relative enhancement on overall quality, compared against naive fine-tuning
(Code available at https://github.com/Sonata165/ControllableLyricTranslation).",None,-1
5ca1d7cf-5106-478c-80c9-3227d07b926d,Event Causality Extraction with Event Argument Correlations,0.720348,5,"Event Causality Identification (ECI), which aims to detect whether a
causality relation exists between two given textual events, is an important
task for event causality understanding. However, the ECI task ignores crucial
event structure and cause-effect causality component information, making it
struggle for downstream applications. In this paper, we explore a novel task,
namely Event Causality Extraction (ECE), aiming to extract the cause-effect
event causality pairs with their structured event information from plain texts.
The ECE task is more challenging since each event can contain multiple event
arguments, posing fine-grained correlations between events to decide the
causeeffect event pair. Hence, we propose a method with a dual grid tagging
scheme to capture the intra- and inter-event argument correlations for ECE.
Further, we devise a event type-enhanced model architecture to realize the dual
grid tagging scheme. Experiments demonstrate the effectiveness of our method,
and extensive analyses point out several future directions for ECE.",None,-1
9a9cb5a5-3f6a-48ab-b479-9a3548a38437,Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking,0.890582,26,"This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT)
framework. It emphasizes spatio-temporal continuity and integrates both past
and future reasoning for tracked objects. Thus, we name it ""Past-and-Future
reasoning for Tracking"" (PF-Track). Specifically, our method adapts the
""tracking by attention"" framework and represents tracked instances coherently
over time with object queries. To explicitly use historical cues, our ""Past
Reasoning"" module learns to refine the tracks and enhance the object features
by cross-attending to queries from previous frames and other objects. The
""Future Reasoning"" module digests historical information and predicts robust
future trajectories. In the case of long-term occlusions, our method maintains
the object positions and enables re-association by integrating motion
predictions. On the nuScenes dataset, our method improves AMOTA by a large
margin and remarkably reduces ID-Switches by 90% compared to prior approaches,
which is an order of magnitude less. The code and models are made available at
https://github.com/TRI-ML/PF-Track.",None,-1
691b151f-4eca-4b1a-b2f8-1bd0d72ff12d,SimHaze: game engine simulated data for real-world dehazing,0.137924,1,"Deep models have demonstrated recent success in single-image dehazing. Most
prior methods consider fully supervised training and learn from paired clean
and hazy images, where a hazy image is synthesized based on a clean image and
its estimated depth map. This paradigm, however, can produce low-quality hazy
images due to inaccurate depth estimation, resulting in poor generalization of
the trained models. In this paper, we explore an alternative approach for
generating paired clean-hazy images by leveraging computer graphics. Using a
modern game engine, our approach renders crisp clean images and their precise
depth maps, based on which high-quality hazy images can be synthesized for
training dehazing models. To this end, we present SimHaze: a new synthetic haze
dataset. More importantly, we show that training with SimHaze alone allows the
latest dehazing models to achieve significantly better performance in
comparison to previous dehazing datasets. Our dataset and code will be made
publicly available.",None,-1
e359c51d-c515-462c-9fa0-0d3e867573f3,Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports,0.907538,14,"With the increasing number of clinical trial reports generated every day, it
is becoming hard to keep up with novel discoveries that inform evidence-based
healthcare recommendations. To help automate this process and assist medical
experts, NLP solutions are being developed. This motivated the SemEval-2023
Task 7, where the goal was to develop an NLP system for two tasks: evidence
retrieval and natural language inference from clinical trial data. In this
paper, we describe our two developed systems. The first one is a pipeline
system that models the two tasks separately, while the second one is a joint
system that learns the two tasks simultaneously with a shared representation
and a multi-task learning approach. The final system combines their outputs in
an ensemble system. We formalize the models, present their characteristics and
challenges, and provide an analysis of achieved results. Our system ranked 3rd
out of 40 participants with a final submission.",None,-1
67490e2d-21c6-4bf7-81b1-b60eefc6a3be,Large-Scale Traffic Signal Control Using Constrained Network Partition and Adaptive Deep Reinforcement Learning,0.376614,2,"Multi-agent Deep Reinforcement Learning (MADRL) based traffic signal control
becomes a popular research topic in recent years. To alleviate the scalability
issue of completely centralized RL techniques and the non-stationarity issue of
completely decentralized RL techniques on large-scale traffic networks, some
literature utilizes a regional control approach where the whole network is
firstly partitioned into multiple disjoint regions, followed by applying the
centralized RL approach to each region. However, the existing partitioning
rules either have no constraints on the topology of regions or require the same
topology for all regions. Meanwhile, no existing regional control approach
explores the performance of optimal joint action in an exponentially growing
regional action space when intersections are controlled by 4-phase traffic
signals (EW, EWL, NS, NSL). In this paper, we propose a novel RL training
framework named RegionLight to tackle the above limitations. Specifically, the
topology of regions is firstly constrained to a star network which comprises
one center and an arbitrary number of leaves. Next, the network partitioning
problem is modeled as an optimization problem to minimize the number of
regions. Then, an Adaptive Branching Dueling Q-Network (ABDQ) model is proposed
to decompose the regional control task into several joint signal control
sub-tasks corresponding to particular intersections. Subsequently, these
sub-tasks maximize the regional benefits cooperatively. Finally, the global
control strategy for the whole network is obtained by concatenating the optimal
joint actions of all regions. Experimental results demonstrate the superiority
of our proposed framework over all baselines under both real and synthetic
datasets in all evaluation metrics.",None,-1
32ab892c-96c4-4be2-b69a-5c755e7e5ca4,Incremental 3D Semantic Scene Graph Prediction from RGB Sequences,0.736691,11,"3D semantic scene graphs are a powerful holistic representation as they
describe the individual objects and depict the relation between them. They are
compact high-level graphs that enable many tasks requiring scene reasoning. In
real-world settings, existing 3D estimation methods produce robust predictions
that mostly rely on dense inputs. In this work, we propose a real-time
framework that incrementally builds a consistent 3D semantic scene graph of a
scene given an RGB image sequence. Our method consists of a novel incremental
entity estimation pipeline and a scene graph prediction network. The proposed
pipeline simultaneously reconstructs a sparse point map and fuses entity
estimation from the input images. The proposed network estimates 3D semantic
scene graphs with iterative message passing using multi-view and geometric
features extracted from the scene entities. Extensive experiments on the 3RScan
dataset show the effectiveness of the proposed method in this challenging task,
outperforming state-of-the-art approaches.",None,-1
a193db28-3691-4c42-89ad-692c1f53d4d8,Multi-lingual and Multi-cultural Figurative Language Understanding,0.468803,25,"Figurative language permeates human communication, but at the same time is
relatively understudied in NLP. Datasets have been created in English to
accelerate progress towards measuring and improving figurative language
processing in language models (LMs). However, the use of figurative language is
an expression of our cultural and societal experiences, making it difficult for
these phrases to be universally applicable. In this work, we create a
figurative language inference dataset, \datasetname, for seven diverse
languages associated with a variety of cultures: Hindi, Indonesian, Javanese,
Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language
relies on cultural and regional concepts for figurative expressions, with the
highest overlap between languages originating from the same region. We assess
multilingual LMs' abilities to interpret figurative language in zero-shot and
few-shot settings. All languages exhibit a significant deficiency compared to
English, with variations in performance reflecting the availability of
pre-training and fine-tuning data, emphasizing the need for LMs to be exposed
to a broader range of linguistic and cultural variation during training.",None,-1
8bcf5043-d302-488e-83f4-67b8f2130031,Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty,0.888327,6,"Open Information Extraction (OIE) task aims at extracting structured facts
from unstructured text, typically in the form of (subject, relation, object)
triples. Despite the potential of large language models (LLMs) like ChatGPT as
a general task solver, they lag behind state-of-the-art (supervised) methods in
OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant
context from relevant relations and generate structured output due to the
restrictions on fine-tuning the model. Second, LLMs generates responses
autoregressively based on probability, which makes the predicted relations lack
confidence. In this paper, we assess the capabilities of LLMs in improving the
OIE task. Particularly, we propose various in-context learning strategies to
enhance LLM's instruction-following ability and a demonstration uncertainty
quantification module to enhance the confidence of the generated relations. Our
experiments on three OIE benchmark datasets show that our approach holds its
own against established supervised methods, both quantitatively and
qualitatively.",None,-1
cb5234eb-bc35-49dc-a031-6f5e40770e4b,Self-Knowledge Guided Retrieval Augmentation for Large Language Models,0.104226,10,"Large language models (LLMs) have shown superior performance without
task-specific fine-tuning. Despite the success, the knowledge stored in the
parameters of LLMs could still be incomplete and difficult to update due to the
computational costs. As complementary, retrieval-based methods can offer
non-parametric world knowledge and improve the performance on tasks such as
question answering. However, we find that the retrieved knowledge does not
always help and even has a negative impact on original responses occasionally.
To better make use of both internal knowledge and external world knowledge, we
investigate eliciting the model's ability to recognize what they know and do
not know (which is also called self-knowledge) and propose Self-Knowledge
guided Retrieval augmentation (SKR), a simple yet effective method which can
let LLMs refer to the questions they have previously encountered and adaptively
call for external resources when dealing with new questions. We evaluate SKR on
multiple datasets and demonstrate that it outperforms chain-of-thought based
and fully retrieval-based methods by using either InstructGPT or ChatGPT.",None,-1
c215ba21-c892-4c22-8414-d9470153826a,Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines,0.613259,1,"Polarization and the marketplace for impressions have conspired to make
navigating information online difficult for users, and while there has been a
significant effort to detect false or misleading text, multimodal datasets have
received considerably less attention. To complement existing resources, we
present multimodal Video Misleading Headline (VMH), a dataset that consists of
videos and whether annotators believe the headline is representative of the
video's contents. After collecting and annotating this dataset, we analyze
multimodal baselines for detecting misleading headlines. Our annotation process
also focuses on why annotators view a video as misleading, allowing us to
better understand the interplay of annotators' background and the content of
the videos.",None,-1
0a180e38-e87d-4f34-a11b-6841faeb1260,Understanding Gaussian Attention Bias of Vision Transformers Using Effective Receptive Fields,0.0463243,1,"Vision transformers (ViTs) that model an image as a sequence of partitioned
patches have shown notable performance in diverse vision tasks. Because
partitioning patches eliminates the image structure, to reflect the order of
patches, ViTs utilize an explicit component called positional embedding.
However, we claim that the use of positional embedding does not simply
guarantee the order-awareness of ViT. To support this claim, we analyze the
actual behavior of ViTs using an effective receptive field. We demonstrate that
during training, ViT acquires an understanding of patch order from the
positional embedding that is trained to be a specific pattern. Based on this
observation, we propose explicitly adding a Gaussian attention bias that guides
the positional embedding to have the corresponding pattern from the beginning
of training. We evaluated the influence of Gaussian attention bias on the
performance of ViTs in several image classification, object detection, and
semantic segmentation experiments. The results showed that proposed method not
only facilitates ViTs to understand images but also boosts their performance on
various datasets, including ImageNet, COCO 2017, and ADE20K.",None,-1
7551623a-6942-43c5-9ab8-5e06681c91b9,Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection,0.827955,11,"Large Language Models (LLMs) can adapt to new tasks via in-context learning
(ICL). ICL is efficient as it does not require any parameter updates to the
trained LLM, but only few annotated examples as input for the LLM. In this
work, we investigate an active learning approach for ICL, where there is a
limited budget for annotating examples. We propose a model-adaptive
optimization-free algorithm, termed AdaICL, which identifies examples that the
model is uncertain about, and performs semantic diversity-based example
selection. Diversity-based sampling improves overall effectiveness, while
uncertainty sampling improves budget efficiency and helps the LLM learn new
information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage
problem, that dynamically adapts based on the model's feedback and can be
approximately solved via greedy algorithms. Extensive experiments on nine
datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy
points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient
than performing annotations uniformly at random, while it outperforms SOTA with
2x fewer ICL examples.",None,-1
9a621611-f1f6-4d57-982e-306d67fc85d1,Unlabeled Imperfect Demonstrations in Adversarial Imitation Learning,0.214062,4,"Adversarial imitation learning has become a widely used imitation learning
framework. The discriminator is often trained by taking expert demonstrations
and policy trajectories as examples respectively from two categories (positive
vs. negative) and the policy is then expected to produce trajectories that are
indistinguishable from the expert demonstrations. But in the real world, the
collected expert demonstrations are more likely to be imperfect, where only an
unknown fraction of the demonstrations are optimal. Instead of treating
imperfect expert demonstrations as absolutely positive or negative, we
investigate unlabeled imperfect expert demonstrations as they are. A
positive-unlabeled adversarial imitation learning algorithm is developed to
dynamically sample expert demonstrations that can well match the trajectories
from the constantly optimized agent policy. The trajectories of an initial
agent policy could be closer to those non-optimal expert demonstrations, but
within the framework of adversarial imitation learning, agent policy will be
optimized to cheat the discriminator and produce trajectories that are similar
to those optimal expert demonstrations. Theoretical analysis shows that our
method learns from the imperfect demonstrations via a self-paced way.
Experimental results on MuJoCo and RoboSuite platforms demonstrate the
effectiveness of our method from different aspects.",None,-1
6dafd940-0294-497b-b821-1f3f934c1e74,FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views,0.727817,9,"We present FlexNeRF, a method for photorealistic freeviewpoint rendering of
humans in motion from monocular videos. Our approach works well with sparse
views, which is a challenging scenario when the subject is exhibiting
fast/complex motions. We propose a novel approach which jointly optimizes a
canonical time and pose configuration, with a pose-dependent motion field and
pose-independent temporal deformations complementing each other. Thanks to our
novel temporal and cyclic consistency constraints along with additional losses
on intermediate representation such as segmentation, our approach provides high
quality outputs as the observed views become sparser. We empirically
demonstrate that our method significantly outperforms the state-of-the-art on
public benchmark datasets as well as a self-captured fashion dataset. The
project page is available at: https://flex-nerf.github.io/",None,-1
ba1d1f60-5bf9-46a4-873c-c87de1cf5250,MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation,0.710772,10,"Weakly-supervised semantic segmentation aims to reduce labeling costs by
training semantic segmentation models using weak supervision, such as
image-level class labels. However, most approaches struggle to produce accurate
localization maps and suffer from false predictions in class-related
backgrounds (i.e., biased objects), such as detecting a railroad with the train
class. Recent methods that remove biased objects require additional supervision
for manually identifying biased objects for each problematic class and
collecting their datasets by reviewing predictions, limiting their
applicability to the real-world dataset with multiple labels and complex
relationships for biasing. Following the first observation that biased features
can be separated and eliminated by matching biased objects with backgrounds in
the same dataset, we propose a fully-automatic/model-agnostic biased removal
framework called MARS (Model-Agnostic biased object Removal without additional
Supervision), which utilizes semantically consistent features of an
unsupervised technique to eliminate biased objects in pseudo labels.
Surprisingly, we show that MARS achieves new state-of-the-art results on two
popular benchmarks, PASCAL VOC 2012 (val: 77.7%, test: 77.2%) and MS COCO 2014
(val: 49.4%), by consistently improving the performance of various WSSS models
by at least 30% without additional supervision.",None,-1
73dc9175-14cd-43a0-91a7-5063264a9e7c,Open-Set Face Identification on Few-Shot Gallery by Fine-Tuning,0.177517,1,"In this paper, we focus on addressing the open-set face identification
problem on a few-shot gallery by fine-tuning. The problem assumes a realistic
scenario for face identification, where only a small number of face images is
given for enrollment and any unknown identity must be rejected during
identification. We observe that face recognition models pretrained on a large
dataset and naively fine-tuned models perform poorly for this task. Motivated
by this issue, we propose an effective fine-tuning scheme with classifier
weight imprinting and exclusive BatchNorm layer tuning. For further improvement
of rejection accuracy on unknown identities, we propose a novel matcher called
Neighborhood Aware Cosine (NAC) that computes similarity based on neighborhood
information. We validate the effectiveness of the proposed schemes thoroughly
on large-scale face benchmarks across different convolutional neural network
architectures. The source code for this project is available at:
https://github.com/1ho0jin1/OSFI-by-FineTuning",None,-1
413ed3c4-5c00-40ce-8111-5da674ac7a0e,InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning,0.979533,151,"Recent advances in personalized image generation allow a pre-trained
text-to-image model to learn a new concept from a set of images. However,
existing personalization approaches usually require heavy test-time finetuning
for each concept, which is time-consuming and difficult to scale. We propose
InstantBooth, a novel approach built upon pre-trained text-to-image models that
enables instant text-guided image personalization without any test-time
finetuning. We achieve this with several major components. First, we learn the
general concept of the input images by converting them to a textual token with
a learnable image encoder. Second, to keep the fine details of the identity, we
learn rich visual feature representation by introducing a few adapter layers to
the pre-trained model. We train our components only on text-image pairs without
using paired images of the same concept. Compared to test-time finetuning-based
methods like DreamBooth and Textual-Inversion, our model can generate
competitive results on unseen concepts concerning language-image alignment,
image fidelity, and identity preservation while being 100 times faster.",None,-1
69c5afcd-c916-48e7-aeb8-310979723aa5,Real-time Multi-Class Helmet Violation Detection Using Few-Shot Data Sampling Technique and YOLOv8,1.0,52,"Traffic safety is a major global concern. Helmet usage is a key factor in
preventing head injuries and fatalities caused by motorcycle accidents.
However, helmet usage violations continue to be a significant problem. To
identify such violations, automatic helmet detection systems have been proposed
and implemented using computer vision techniques. Real-time implementation of
such systems is crucial for traffic surveillance and enforcement, however, most
of these systems are not real-time. This study proposes a robust real-time
helmet violation detection system. The proposed system utilizes a unique data
processing strategy, referred to as few-shot data sampling, to develop a robust
model with fewer annotations, and a single-stage object detection model, YOLOv8
(You Only Look Once Version 8), for detecting helmet violations in real-time
from video frames. Our proposed method won 7th place in the 2023 AI City
Challenge, Track 5, with an mAP score of 0.5861 on experimental validation
data. The experimental results demonstrate the effectiveness, efficiency, and
robustness of the proposed system.",None,-1
75e051f8-5bb0-47f0-a890-bf4cffe701ce,An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback,0.479051,4,"While search is the predominant method of accessing information, formulating
effective queries remains a challenging task, especially for situations where
the users are not familiar with a domain, or searching for documents in other
languages, or looking for complex information such as events, which are not
easily expressible as queries. Providing example documents or passages of
interest, might be easier for a user, however, such query-by-example scenarios
are prone to concept drift, and are highly sensitive to the query generation
method. This demo illustrates complementary approaches of using LLMs
interactively, assisting and enabling the user to provide edits and feedback at
all stages of the query formulation process. The proposed Query Generation
Assistant is a novel search interface which supports automatic and interactive
query generation over a mono-linguial or multi-lingual document collection.
Specifically, the proposed assistive interface enables the users to refine the
queries generated by different LLMs, to provide feedback on the retrieved
documents or passages, and is able to incorporate the users' feedback as
prompts to generate more effective queries. The proposed interface is a
valuable experimental tool for exploring fine-tuning and prompting of LLMs for
query generation to qualitatively evaluate the effectiveness of retrieval and
ranking models, and for conducting Human-in-the-Loop (HITL) experiments for
complex search tasks where users struggle to formulate queries without such
assistance.",None,-1
f6076724-3164-4003-bdc5-8a1086c72b95,SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation,0.499833,18,"We present a cascaded diffusion model based on a part-level implicit 3D
representation. Our model achieves state-of-the-art generation quality and also
enables part-level shape editing and manipulation without any additional
training in conditional setup. Diffusion models have demonstrated impressive
capabilities in data generation as well as zero-shot completion and editing via
a guided reverse process. Recent research on 3D diffusion models has focused on
improving their generation capabilities with various data representations,
while the absence of structural information has limited their capability in
completion and editing tasks. We thus propose our novel diffusion model using a
part-level implicit representation. To effectively learn diffusion with
high-dimensional embedding vectors of parts, we propose a cascaded framework,
learning diffusion first on a low-dimensional subspace encoding extrinsic
parameters of parts and then on the other high-dimensional subspace encoding
intrinsic attributes. In the experiments, we demonstrate the outperformance of
our method compared with the previous ones both in generation and part-level
completion and manipulation tasks.",None,-1
fd1103ec-286c-4f23-9e82-758992450ce0,NExT-GPT: Any-to-Any Multimodal LLM,0.986175,183,"While recently Multimodal Large Language Models (MM-LLMs) have made exciting
strides, they mostly fall prey to the limitation of only input-side multimodal
understanding, without the ability to produce content in multiple modalities.
As we humans always perceive the world and communicate with people through
various modalities, developing any-to-any MM-LLMs capable of accepting and
delivering content in any modality becomes essential to human-level AI. To fill
the gap, we present an end-to-end general-purpose any-to-any MM-LLM system,
NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion
decoders, enabling NExT-GPT to perceive inputs and generate outputs in
arbitrary combinations of text, images, videos, and audio. By leveraging the
existing well-trained highly-performing encoders and decoders, NExT-GPT is
tuned with only a small amount of parameter (1%) of certain projection layers,
which not only benefits low-cost training and also facilitates convenient
expansion to more potential modalities. Moreover, we introduce a
modality-switching instruction tuning (MosIT) and manually curate a
high-quality dataset for MosIT, based on which NExT-GPT is empowered with
complex cross-modal semantic understanding and content generation. Overall, our
research showcases the promising possibility of building an AI agent capable of
modeling universal modalities, paving the way for more human-like AI research
in the community. Project page: https://next-gpt.github.io/",None,-1
af15cfdb-4aad-4699-b87f-7e6631675b38,MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies,0.133785,7,"Autoregressive language models are trained by minimizing the cross-entropy of
the model distribution Q relative to the data distribution P -- that is,
minimizing the forward cross-entropy, which is equivalent to maximum likelihood
estimation (MLE). We have observed that models trained in this way may
""over-generalize"", in the sense that they produce non-human-like text.
Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P
relative to Q, is a better reflection of how a human would evaluate text
generated by a model. Hence, we propose learning with MixCE, an objective that
mixes the forward and reverse cross-entropies. We evaluate models trained with
this objective on synthetic data settings (where P is known) and real data, and
show that the resulting models yield better generated text without complex
decoding strategies. Our code and models are publicly available at
https://github.com/bloomberg/mixce-acl2023",None,-1
272e5e2e-f014-41a1-b751-1d063c911b8e,Bi-level Dynamic Learning for Jointly Multi-modality Image Fusion and Beyond,0.855567,16,"Recently, multi-modality scene perception tasks, e.g., image fusion and scene
understanding, have attracted widespread attention for intelligent vision
systems. However, early efforts always consider boosting a single task
unilaterally and neglecting others, seldom investigating their underlying
connections for joint promotion. To overcome these limitations, we establish
the hierarchical dual tasks-driven deep model to bridge these tasks.
Concretely, we firstly construct an image fusion module to fuse complementary
characteristics and cascade dual task-related modules, including a
discriminator for visual effects and a semantic network for feature
measurement. We provide a bi-level perspective to formulate image fusion and
follow-up downstream tasks. To incorporate distinct task-related responses for
image fusion, we consider image fusion as a primary goal and dual modules as
learnable constraints. Furthermore, we develop an efficient first-order
approximation to compute corresponding gradients and present dynamic weighted
aggregation to balance the gradients for fusion learning. Extensive experiments
demonstrate the superiority of our method, which not only produces visually
pleasant fused results but also realizes significant promotion for detection
and segmentation than the state-of-the-art approaches.",None,-1
22a261b4-bb10-4656-95b9-d2d67e7db4db,Echoes of Biases: How Stigmatizing Language Affects AI Performance,0.11341,2,"Electronic health records (EHRs) serve as an essential data source for the
envisioned artificial intelligence (AI)-driven transformation in healthcare.
However, clinician biases reflected in EHR notes can lead to AI models
inheriting and amplifying these biases, perpetuating health disparities. This
study investigates the impact of stigmatizing language (SL) in EHR notes on
mortality prediction using a Transformer-based deep learning model and
explainable AI (XAI) techniques. Our findings demonstrate that SL written by
clinicians adversely affects AI performance, particularly so for black
patients, highlighting SL as a source of racial disparity in AI model
development. To explore an operationally efficient way to mitigate SL's impact,
we investigate patterns in the generation of SL through a clinicians'
collaborative network, identifying central clinicians as having a stronger
impact on racial disparity in the AI model. We find that removing SL written by
central clinicians is a more efficient bias reduction strategy than eliminating
all SL in the entire corpus of data. This study provides actionable insights
for responsible AI development and contributes to understanding clinician
behavior and EHR note writing in healthcare.",None,-1
227657e1-f4ba-4ab5-936d-e6cdb6e7f60c,"""You might think about slightly revising the title"": identifying hedges in peer-tutoring interactions",0.991125,10,"Hedges play an important role in the management of conversational
interaction. In peer tutoring, they are notably used by tutors in dyads (pairs
of interlocutors) experiencing low rapport to tone down the impact of
instructions and negative feedback. Pursuing the objective of building a
tutoring agent that manages rapport with students in order to improve learning,
we used a multimodal peer-tutoring dataset to construct a computational
framework for identifying hedges. We compared approaches relying on pre-trained
resources with others that integrate insights from the social science
literature. Our best performance involved a hybrid approach that outperforms
the existing baseline while being easier to interpret. We employ a model
explainability tool to explore the features that characterize hedges in
peer-tutoring conversations, and we identify some novel features, and the
benefits of such a hybrid model approach.",None,-1
47b377b2-70e2-461d-bad6-2f818c0d5821,War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars,0.999996,36,"Can we avoid wars at the crossroads of history? This question has been
pursued by individuals, scholars, policymakers, and organizations throughout
human history. In this research, we attempt to answer the question based on the
recent advances of Artificial Intelligence (AI) and Large Language Models
(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to
simulate the participating countries, their decisions, and the consequences, in
historical international conflicts, including the World War I (WWI), the World
War II (WWII), and the Warring States Period (WSP) in Ancient China. By
evaluating the simulation effectiveness, we examine the advancements and
limitations of cutting-edge AI systems' abilities in studying complex
collective human behaviors such as international conflicts under diverse
settings. In these simulations, the emergent interactions among agents also
offer a novel perspective for examining the triggers and conditions that lead
to war. Our findings offer data-driven and AI-augmented insights that can
redefine how we approach conflict resolution and peacekeeping strategies. The
implications stretch beyond historical analysis, offering a blueprint for using
AI to understand human history and possibly prevent future international
conflicts. Code and data are available at
\url{https://github.com/agiresearch/WarAgent}.",None,-1
d00b2689-2beb-45f7-817e-01deecc3c1b7,Enhancing Programming eTextbooks with ChatGPT Generated Counterfactual-Thinking-Inspired Questions,0.152213,1,"Digital textbooks have become an integral part of everyday learning tasks. In
this work, we consider the use of digital textbooks for programming classes.
Generally, students struggle with utilizing textbooks on programming to the
maximum, with a possible reason being that the example programs provided as
illustration of concepts in these textbooks don't offer sufficient
interactivity for students, and thereby not sufficiently motivating to explore
or understand these programming examples better. In our work, we explore the
idea of enhancing the navigability of intelligent textbooks with the use of
``counterfactual'' questions, to make students think critically about these
programs and enhance possible program comprehension. Inspired from previous
works on nudging students on counter factual thinking, we present the
possibility to enhance digital textbooks with questions generated using GPT.",None,-1
1c8f44d6-329b-4864-8007-10b40c8d1455,SoccerNet 2023 Tracking Challenge -- 3rd place MOT4MOT Team Technical Report,0.705425,2,"The SoccerNet 2023 tracking challenge requires the detection and tracking of
soccer players and the ball. In this work, we present our approach to tackle
these tasks separately. We employ a state-of-the-art online multi-object
tracker and a contemporary object detector for player tracking. To overcome the
limitations of our online approach, we incorporate a post-processing stage
using interpolation and appearance-free track merging. Additionally, an
appearance-based track merging technique is used to handle the termination and
creation of tracks far from the image boundaries. Ball tracking is formulated
as single object detection, and a fine-tuned YOLOv8l detector with proprietary
filtering improves the detection precision. Our method achieves 3rd place on
the SoccerNet 2023 tracking challenge with a HOTA score of 66.27.",None,-1
1d22b76d-a501-4eeb-873e-e59114aab16a,Depth-NeuS: Neural Implicit Surfaces Learning for Multi-view Reconstruction Based on Depth Information Optimization,0.228761,4,"Recently, methods for neural surface representation and rendering, for
example NeuS, have shown that learning neural implicit surfaces through volume
rendering is becoming increasingly popular and making good progress. However,
these methods still face some challenges. Existing methods lack a direct
representation of depth information, which makes object reconstruction
unrestricted by geometric features, resulting in poor reconstruction of objects
with texture and color features. This is because existing methods only use
surface normals to represent implicit surfaces without using depth information.
Therefore, these methods cannot model the detailed surface features of objects
well. To address this problem, we propose a neural implicit surface learning
method called Depth-NeuS based on depth information optimization for multi-view
reconstruction. In this paper, we introduce depth loss to explicitly constrain
SDF regression and introduce geometric consistency loss to optimize for
low-texture areas. Specific experiments show that Depth-NeuS outperforms
existing technologies in multiple scenarios and achieves high-quality surface
reconstruction in multiple scenarios.",None,-1
77b94abc-0121-4448-b0aa-26e56badc9d7,Enhancing Translation for Indigenous Languages: Experiments with Multilingual Models,0.239799,3,"This paper describes CIC NLP's submission to the AmericasNLP 2023 Shared Task
on machine translation systems for indigenous languages of the Americas. We
present the system descriptions for three methods. We used two multilingual
models, namely M2M-100 and mBART50, and one bilingual (one-to-one) -- Helsinki
NLP Spanish-English translation model, and experimented with different transfer
learning setups. We experimented with 11 languages from America and report the
setups we used as well as the results we achieved. Overall, the mBART setup was
able to improve upon the baseline for three out of the eleven languages.",None,-1
6ab9c6d1-0338-4c35-86b6-1b6d536bc193,GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding,0.697726,12,"Humans subconsciously engage in geospatial reasoning when reading articles.
We recognize place names and their spatial relations in text and mentally
associate them with their physical locations on Earth. Although pretrained
language models can mimic this cognitive process using linguistic context, they
do not utilize valuable geospatial information in large, widely available
geographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a
geospatially grounded language model that enhances the understanding of
geo-entities in natural language. GeoLM leverages geo-entity mentions as
anchors to connect linguistic information in text corpora with geospatial
information extracted from geographical databases. GeoLM connects the two types
of context through contrastive learning and masked language modeling. It also
incorporates a spatial coordinate embedding mechanism to encode distance and
direction relations to capture geospatial context. In the experiment, we
demonstrate that GeoLM exhibits promising capabilities in supporting toponym
recognition, toponym linking, relation extraction, and geo-entity typing, which
bridge the gap between natural language processing and geospatial sciences. The
code is publicly available at https://github.com/knowledge-computing/geolm.",None,-1
c71a6e2d-17bc-4cbb-b765-cbad7492f368,Using Large Language Models for Cybersecurity Capture-The-Flag Challenges and Certification Questions,0.999704,13,"The assessment of cybersecurity Capture-The-Flag (CTF) exercises involves
participants finding text strings or ``flags'' by exploiting system
vulnerabilities. Large Language Models (LLMs) are natural-language models
trained on vast amounts of words to understand and generate text; they can
perform well on many CTF challenges. Such LLMs are freely available to
students. In the context of CTF exercises in the classroom, this raises
concerns about academic integrity. Educators must understand LLMs' capabilities
to modify their teaching to accommodate generative AI assistance. This research
investigates the effectiveness of LLMs, particularly in the realm of CTF
challenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT,
Google Bard, and Microsoft Bing. First, we assess the LLMs' question-answering
performance on five Cisco certifications with varying difficulty levels. Next,
we qualitatively study the LLMs' abilities in solving CTF challenges to
understand their limitations. We report on the experience of using the LLMs for
seven test cases in all five types of CTF challenges. In addition, we
demonstrate how jailbreak prompts can bypass and break LLMs' ethical
safeguards. The paper concludes by discussing LLM's impact on CTF exercises and
its implications.",None,-1
47d7601b-6e6c-447d-9cfa-79de6cdb7a8b,Towards equilibrium molecular conformation generation with GFlowNets,0.9611,7,"Sampling diverse, thermodynamically feasible molecular conformations plays a
crucial role in predicting properties of a molecule. In this paper we propose
to use GFlowNet for sampling conformations of small molecules from the
Boltzmann distribution, as determined by the molecule's energy. The proposed
approach can be used in combination with energy estimation methods of different
fidelity and discovers a diverse set of low-energy conformations for highly
flexible drug-like molecules. We demonstrate that GFlowNet can reproduce
molecular potential energy surfaces by sampling proportionally to the Boltzmann
distribution.",None,-1
8ef3a02d-a468-4dc2-90ff-9374dbc44cfa,Bipartite Graph Diffusion Model for Human Interaction Generation,0.601755,7,"The generation of natural human motion interactions is a hot topic in
computer vision and computer animation. It is a challenging task due to the
diversity of possible human motion interactions. Diffusion models, which have
already shown remarkable generative capabilities in other domains, are a good
candidate for this task. In this paper, we introduce a novel bipartite graph
diffusion method (BiGraphDiff) to generate human motion interactions between
two persons. Specifically, bipartite node sets are constructed to model the
inherent geometric constraints between skeleton nodes during interactions. The
interaction graph diffusion model is transformer-based, combining some
state-of-the-art motion methods. We show that the proposed achieves new
state-of-the-art results on leading benchmarks for the human interaction
generation task.",None,-1
a660ec22-69d0-4444-9e68-99f5328efb54,Text Classification of Cancer Clinical Trial Eligibility Criteria,0.80928,3,"Automatic identification of clinical trials for which a patient is eligible
is complicated by the fact that trial eligibility is stated in natural
language. A potential solution to this problem is to employ text classification
methods for common types of eligibility criteria. In this study, we focus on
seven common exclusion criteria in cancer trials: prior malignancy, human
immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness,
drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase
III cancer trials with these exclusions annotated at the trial level. We
experiment with common transformer models as well as a new pre-trained clinical
trial BERT model. Our results demonstrate the feasibility of automatically
classifying common exclusion criteria. Additionally, we demonstrate the value
of a pre-trained language model specifically for clinical trials, which yields
the highest average performance across all criteria.",None,-1
1901855c-04b6-4cb5-8b00-76f23079b38f,Video Dehazing via a Multi-Range Temporal Alignment Network with Physical Prior,0.298778,11,"Video dehazing aims to recover haze-free frames with high visibility and
contrast. This paper presents a novel framework to effectively explore the
physical haze priors and aggregate temporal information. Specifically, we
design a memory-based physical prior guidance module to encode the
prior-related features into long-range memory. Besides, we formulate a
multi-range scene radiance recovery module to capture space-time dependencies
in multiple space-time ranges, which helps to effectively aggregate temporal
information from adjacent frames. Moreover, we construct the first large-scale
outdoor video dehazing benchmark dataset, which contains videos in various
real-world scenarios. Experimental results on both synthetic and real
conditions show the superiority of our proposed method.",None,-1
068a2260-1388-43ed-8aac-1a95e58b88a2,Advances and Challenges in Multimodal Remote Sensing Image Registration,0.988702,19,"Over the past few decades, with the rapid development of global aerospace and
aerial remote sensing technology, the types of sensors have evolved from the
traditional monomodal sensors (e.g., optical sensors) to the new generation of
multimodal sensors [e.g., multispectral, hyperspectral, light detection and
ranging (LiDAR) and synthetic aperture radar (SAR) sensors]. These advanced
devices can dynamically provide various and abundant multimodal remote sensing
images with different spatial, temporal, and spectral resolutions according to
different application requirements. Since then, it is of great scientific
significance to carry out the research of multimodal remote sensing image
registration, which is a crucial step for integrating the complementary
information among multimodal data and making comprehensive observations and
analysis of the Earths surface. In this work, we will present our own
contributions to the field of multimodal image registration, summarize the
advantages and limitations of existing multimodal image registration methods,
and then discuss the remaining challenges and make a forward-looking prospect
for the future development of the field.",None,-1
1e915dd3-8d28-4cae-9b8d-9a569d7dac3d,AugDiff: Diffusion based Feature Augmentation for Multiple Instance Learning in Whole Slide Image,0.75864,8,"Multiple Instance Learning (MIL), a powerful strategy for weakly supervised
learning, is able to perform various prediction tasks on gigapixel Whole Slide
Images (WSIs). However, the tens of thousands of patches in WSIs usually incur
a vast computational burden for image augmentation, limiting the MIL model's
improvement in performance. Currently, the feature augmentation-based MIL
framework is a promising solution, while existing methods such as Mixup often
produce unrealistic features. To explore a more efficient and practical
augmentation method, we introduce the Diffusion Model (DM) into MIL for the
first time and propose a feature augmentation framework called AugDiff.
Specifically, we employ the generation diversity of DM to improve the quality
of feature augmentation and the step-by-step generation property to control the
retention of semantic information. We conduct extensive experiments over three
distinct cancer datasets, two different feature extractors, and three prevalent
MIL algorithms to evaluate the performance of AugDiff. Ablation study and
visualization further verify the effectiveness. Moreover, we highlight
AugDiff's higher-quality augmented feature over image augmentation and its
superiority over self-supervised learning. The generalization over external
datasets indicates its broader applications.",None,-1
9abbe065-9188-4c43-ac30-aa521b0d87cf,Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning,0.733599,12,"We propose ADCLR: A ccurate and D ense Contrastive Representation Learning, a
novel self-supervised learning framework for learning accurate and dense vision
representation. To extract spatial-sensitive information, ADCLR introduces
query patches for contrasting in addition with global contrasting. Compared
with previous dense contrasting methods, ADCLR mainly enjoys three merits: i)
achieving both global-discriminative and spatial-sensitive representation, ii)
model-efficient (no extra parameters in addition to the global contrasting
baseline), and iii) correspondence-free and thus simpler to implement. Our
approach achieves new state-of-the-art performance for contrastive methods. On
classification tasks, for ViT-S, ADCLR achieves 77.5% top-1 accuracy on
ImageNet with linear probing, outperforming our baseline (DINO) without our
devised techniques as plug-in, by 0.5%. For ViT-B, ADCLR achieves 79.8%, 84.0%
accuracy on ImageNet by linear probing and finetune, outperforming iBOT by
0.3%, 0.2% accuracy. For dense tasks, on MS-COCO, ADCLR achieves significant
improvements of 44.3% AP on object detection, 39.7% AP on instance
segmentation, outperforming previous SOTA method SelfPatch by 2.2% and 1.2%,
respectively. On ADE20K, ADCLR outperforms SelfPatch by 1.0% mIoU, 1.2% mAcc on
the segme",None,-1
148ad6fb-e132-4126-a161-860166f3b5a0,Affect as a proxy for literary mood,0.0952894,1,"We propose to use affect as a proxy for mood in literary texts. In this
study, we explore the differences in computationally detecting tone versus
detecting mood. Methodologically we utilize affective word embeddings to look
at the affective distribution in different text segments. We also present a
simple yet efficient and effective method of enhancing emotion lexicons to take
both semantic shift and the domain of the text into account producing
real-world congruent results closely matching both contemporary and modern
qualitative analyses.",None,-1
4a6865af-972c-40e8-a1a3-cd6a0e2dcb3c,Kinematics and Dynamics Modeling of 7 Degrees of Freedom Human Lower Limb Using Dual Quaternions Algebra,0.234718,1,"Denavit and Hartenberg based methods as Cardan, Fick and Euler angles
describe the position and orientation of an end-effector in Three Dimensional
(3D) space. However, the generation of unrealistic human posture in joint space
constitutes the weak point to these methods because they impose a well-defined
rotations order. A method to handle the transformation homogeneous performance
uses the dual quaternions. Quaternions have proven themselves in many fields as
providing a computational efficient method to represent a rotation, and yet,
they can not deal with the translations in 3D-space. The dual numbers can
extend quaternions to dual quaternions. This paper exploits dual quaternions
theory to provide a fast and accurate solution to the forward, inverse
kinematics and recursive Newton-Euler dynamics algorithm for 7 Degree of
Freedom (DOF) human lower limb in 3D-space.",None,-1
aa9ee45d-2bc6-4264-9a98-1ad428344dfe,Understanding Self-Supervised Features for Learning Unsupervised Instance Segmentation,0.420911,2,"Self-supervised learning (SSL) can be used to solve complex visual tasks
without human labels. Self-supervised representations encode useful semantic
information about images, and as a result, they have already been used for
tasks such as unsupervised semantic segmentation. In this paper, we investigate
self-supervised representations for instance segmentation without any manual
annotations. We find that the features of different SSL methods vary in their
level of instance-awareness. In particular, DINO features, which are known to
be excellent semantic descriptors, lack behind MAE features in their
sensitivity for separating instances.",None,-1
8dbd40f2-18f3-4ba8-8b8c-2b9e5ec273e1,Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds,0.536311,6,"Urban modeling from LiDAR point clouds is an important topic in computer
vision, computer graphics, photogrammetry and remote sensing. 3D city models
have found a wide range of applications in smart cities, autonomous navigation,
urban planning and mapping etc. However, existing datasets for 3D modeling
mainly focus on common objects such as furniture or cars. Lack of building
datasets has become a major obstacle for applying deep learning technology to
specific domains such as urban modeling. In this paper, we present a
urban-scale dataset consisting of more than 160 thousands buildings along with
corresponding point clouds, mesh and wire-frame models, covering 16 cities in
Estonia about 998 Km2. We extensively evaluate performance of state-of-the-art
algorithms including handcrafted and deep feature based methods. Experimental
results indicate that Building3D has challenges of high intra-class variance,
data imbalance and large-scale noises. The Building3D is the first and largest
urban-scale building modeling benchmark, allowing a comparison of supervised
and self-supervised learning methods. We believe that our Building3D will
facilitate future research on urban modeling, aerial path planning, mesh
simplification, and semantic/part segmentation etc.",None,-1
52b1ae27-5116-4630-b342-b18eb88d82ee,Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency,0.0656669,7,"Large language models (LLMs) have exhibited remarkable ability in code
generation. However, generating the correct solution in a single attempt still
remains a challenge. Prior works utilize verification properties in software
engineering to verify and re-rank solutions in a majority voting manner. But
the assumption behind them that generated verification properties have better
qualities than solutions may not always hold. In this paper, we treat them
equally as different perspectives of LLMs' reasoning processes. We propose the
Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter-
and intra-consistency across outputs from multiple perspectives. Specifically,
we prompt LLMs to generate diverse outputs from three perspectives, Solution,
Specification and Test case, constructing a 3-partite graph. With two measure
functions of consistency, we embed both inter- and intra-consistency
information into the graph. The optimal choice of solutions is then determined
based on analysis in the graph. MPSC significantly boosts performance of
foundation models (ChatGPT in this paper) on various benchmarks, including
HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing
GPT-4.",None,-1
39e3af23-40b6-4e85-aec4-24757c7e26ea,InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators,0.704249,7,"Instruction-based language modeling has received significant attention in
pretrained language models. However, the efficiency of instruction engineering
remains low and hinders the development of instruction studies. Recent studies
have focused on automating instruction generation, but they primarily aim to
improve performance without considering other crucial objectives that impact
instruction quality, such as instruction length and perplexity. Therefore, we
propose a novel approach (i.e., InstOptima) that treats instruction generation
as an evolutionary multi-objective optimization problem. In contrast to text
edition-based methods, our approach utilizes a large language model (LLM) to
simulate instruction operators, including mutation and crossover. Furthermore,
we introduce an objective-guided mechanism for these operators, allowing the
LLM to comprehend the objectives and enhance the quality of the generated
instructions. Experimental results demonstrate improved fine-tuning performance
and the generation of a diverse set of high-quality instructions.",None,-1
f240740a-bad7-4ec1-a6c4-415a047d312b,PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter,0.592891,12,"The Retrieval Question Answering (ReQA) task employs the retrieval-augmented
framework, composed of a retriever and generator. The generator formulates the
answer based on the documents retrieved by the retriever. Incorporating Large
Language Models (LLMs) as generators is beneficial due to their advanced QA
capabilities, but they are typically too large to be fine-tuned with budget
constraints while some of them are only accessible via APIs. To tackle this
issue and further improve ReQA performance, we propose a trainable Pluggable
Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box.
Positioned between the retriever and generator in a Pluggable manner, PRCA
refines the retrieved information by operating in a token-autoregressive
strategy via maximizing rewards of the reinforcement learning phase. Our
experiments validate PRCA's effectiveness in enhancing ReQA performance on
three datasets by up to 20% improvement to fit black-box LLMs into existing
frameworks, demonstrating its considerable potential in the LLMs era.",None,-1
2314c1c4-c4cf-42bd-bd44-7d38e2137ff6,On ML-Based Program Translation: Perils and Promises,0.150171,2,"With the advent of new and advanced programming languages, it becomes
imperative to migrate legacy software to new programming languages.
Unsupervised Machine Learning-based Program Translation could play an essential
role in such migration, even without a sufficiently sizeable reliable corpus of
parallel source code. However, these translators are far from perfect due to
their statistical nature. This work investigates unsupervised program
translators and where and why they fail. With in-depth error analysis of such
failures, we have identified that the cases where such translators fail follow
a few particular patterns. With this insight, we develop a rule-based program
mutation engine, which pre-processes the input code if the input follows
specific patterns and post-process the output if the output follows certain
patterns. We show that our code processing tool, in conjunction with the
program translator, can form a hybrid program translator and significantly
improve the state-of-the-art. In the future, we envision an end-to-end program
translation tool where programming domain knowledge can be embedded into an
ML-based translation pipeline using pre- and post-processing steps.",None,-1
051caa97-0f58-46c0-8a71-436f68902803,Memory augment is All You Need for image restoration,0.163482,1,"Image restoration is a low-level vision task, most CNN methods are designed
as a black box, lacking transparency and internal aesthetics. Although some
methods combining traditional optimization algorithms with DNNs have been
proposed, they all have some limitations. In this paper, we propose a
three-granularity memory layer and contrast learning named MemoryNet,
specifically, dividing the samples into positive, negative, and actual three
samples for contrastive learning, where the memory layer is able to preserve
the deep features of the image and the contrastive learning converges the
learned features to balance. Experiments on Derain/Deshadow/Deblur task
demonstrate that these methods are effective in improving restoration
performance. In addition, this paper's model obtains significant PSNR, SSIM
gain on three datasets with different degradation types, which is a strong
proof that the recovered images are perceptually realistic. The source code of
MemoryNet can be obtained from https://github.com/zhangbaijin/MemoryNet",None,-1
82568cbe-9c37-4a7e-a233-c1d39af48c2f,Stance Detection: A Practical Guide to Classifying Political Beliefs in Text,0.0816853,3,"Stance detection is identifying expressed beliefs in a document. While
researchers widely use sentiment analysis for this, recent research
demonstrates that sentiment and stance are distinct. This paper advances text
analysis methods by precisely defining stance detection and presenting three
distinct approaches: supervised classification, natural language inference, and
in-context learning with generative language models. I discuss how document
context and trade-offs between resources and workload should inform your
methods. For all three approaches I provide guidance on application and
validation techniques, as well as coding tutorials for implementation. Finally,
I demonstrate how newer classification approaches can replicate supervised
classifiers.",None,-1
03764a62-e430-40f6-b73b-dca0d9201883,UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View,0.894304,18,"In the field of 3D object detection for autonomous driving, the sensor
portfolio including multi-modality and single-modality is diverse and complex.
Since the multi-modal methods have system complexity while the accuracy of
single-modal ones is relatively low, how to make a tradeoff between them is
difficult. In this work, we propose a universal cross-modality knowledge
distillation framework (UniDistill) to improve the performance of
single-modality detectors. Specifically, during training, UniDistill projects
the features of both the teacher and the student detector into Bird's-Eye-View
(BEV), which is a friendly representation for different modalities. Then, three
distillation losses are calculated to sparsely align the foreground features,
helping the student learn from the teacher without introducing additional cost
during inference. Taking advantage of the similar detection paradigm of
different detectors in BEV, UniDistill easily supports LiDAR-to-camera,
camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths.
Furthermore, the three distillation losses can filter the effect of misaligned
background information and balance between objects of different sizes,
improving the distillation effectiveness. Extensive experiments on nuScenes
demonstrate that UniDistill effectively improves the mAP and NDS of student
detectors by 2.0%~3.2%.",None,-1
7dcb6620-292d-40ab-934e-4ffe95699041,ACTOR: Active Learning with Annotator-specific Classification Heads to Embrace Human Label Variation,0.380552,3,"Label aggregation such as majority voting is commonly used to resolve
annotator disagreement in dataset creation. However, this may disregard
minority values and opinions. Recent studies indicate that learning from
individual annotations outperforms learning from aggregated labels, though they
require a considerable amount of annotation. Active learning, as an annotation
cost-saving strategy, has not been fully explored in the context of learning
from disagreement. We show that in the active learning setting, a multi-head
model performs significantly better than a single-head model in terms of
uncertainty estimation. By designing and evaluating acquisition functions with
annotator-specific heads on two datasets, we show that group-level entropy
works generally well on both datasets. Importantly, it achieves performance in
terms of both prediction and uncertainty estimation comparable to full-scale
training from disagreement, while saving up to 70% of the annotation budget.",None,-1
191e2233-1db4-4d73-a88a-52a74e4709a4,POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery,0.920481,20,"Transformer architectures have achieved SOTA performance on the human mesh
recovery (HMR) from monocular images. However, the performance gain has come at
the cost of substantial memory and computational overhead. A lightweight and
efficient model to reconstruct accurate human mesh is needed for real-world
applications. In this paper, we propose a pure transformer architecture named
POoling aTtention TransformER (POTTER) for the HMR task from single images.
Observing that the conventional attention module is memory and computationally
expensive, we propose an efficient pooling attention module, which
significantly reduces the memory and computational cost without sacrificing
performance. Furthermore, we design a new transformer architecture by
integrating a High-Resolution (HR) stream for the HMR task. The high-resolution
local and global features from the HR stream can be utilized for recovering
more accurate human mesh. Our POTTER outperforms the SOTA method METRO by only
requiring 7% of total parameters and 14% of the Multiply-Accumulate Operations
on the Human3.6M (PA-MPJPE metric) and 3DPW (all three metrics) datasets. The
project webpage is https://zczcwh.github.io/potter_page.",None,-1
1f88ea5e-1359-459d-9398-390226afc49a,ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition,0.538763,4,"Class imbalance is a common challenge in real-world recognition tasks, where
the majority of classes have few samples, also known as tail classes. We
address this challenge with the perspective of generalization and empirically
find that the promising Sharpness-Aware Minimization (SAM) fails to address
generalization issues under the class-imbalanced setting. Through investigating
this specific type of task, we identify that its generalization bottleneck
primarily lies in the severe overfitting for tail classes with limited training
data. To overcome this bottleneck, we leverage class priors to restrict the
generalization scope of the class-agnostic SAM and propose a class-aware
smoothness optimization algorithm named Imbalanced-SAM (ImbSAM). With the
guidance of class priors, our ImbSAM specifically improves generalization
targeting tail classes. We also verify the efficacy of ImbSAM on two
prototypical applications of class-imbalanced recognition: long-tailed
classification and semi-supervised anomaly detection, where our ImbSAM
demonstrates remarkable performance improvements for tail classes and anomaly.
Our code implementation is available at
https://github.com/cool-xuan/Imbalanced_SAM.",None,-1
6821162f-da9b-4893-a416-39d65c4a68e8,Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum,0.487255,17,"Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to extending the capability of LLMs. Although some works
employ open-source LLMs for the tool learning task, most of them are trained in
a controlled environment in which LLMs only learn to execute the human-provided
tools. However, selecting proper tools from the large toolset is also a crucial
ability for the tool learning model to be applied in real-world applications.
Existing methods usually directly employ self-instruction methods to train the
model, which ignores differences in tool complexity. In this paper, we propose
the Confucius, a novel tool learning framework to train LLM to use complicated
tools in real-world scenarios, which contains two main phases: (1) We first
propose a multi-stage learning method to teach the LLM to use various tools
from an easy-to-difficult curriculum; (2) thenceforth, we propose the Iterative
Self-instruct from Introspective Feedback (ISIF) to dynamically construct the
dataset to improve the ability to use the complicated tool. Extensive
experiments conducted on both controlled and real-world settings demonstrate
the superiority of our tool learning framework in the real-world application
scenarios compared to both tuning-free (e.g. ChatGPT, Claude) and tuning-based
baselines (e.g. GPT4Tools).",None,-1
646792d1-c238-41e6-bc98-b59a12c5811c,Frustrated with Code Quality Issues? LLMs can Help!,0.167693,1,"As software projects progress, quality of code assumes paramount importance
as it affects reliability, maintainability and security of software. For this
reason, static analysis tools are used in developer workflows to flag code
quality issues. However, developers need to spend extra efforts to revise their
code to improve code quality based on the tool findings. In this work, we
investigate the use of (instruction-following) large language models (LLMs) to
assist developers in revising code to resolve code quality issues. We present a
tool, CORE (short for COde REvisions), architected using a pair of LLMs
organized as a duo comprised of a proposer and a ranker. Providers of static
analysis tools recommend ways to mitigate the tool warnings and developers
follow them to revise their code. The \emph{proposer LLM} of CORE takes the
same set of recommendations and applies them to generate candidate code
revisions. The candidates which pass the static quality checks are retained.
However, the LLM may introduce subtle, unintended functionality changes which
may go un-detected by the static analysis. The \emph{ranker LLM} evaluates the
changes made by the proposer using a rubric that closely follows the acceptance
criteria that a developer would enforce. CORE uses the scores assigned by the
ranker LLM to rank the candidate revisions before presenting them to the
developer. CORE could revise 59.2% Python files (across 52 quality checks) so
that they pass scrutiny by both a tool and a human reviewer. The ranker LLM is
able to reduce false positives by 25.8% in these cases. CORE produced revisions
that passed the static analysis tool in 76.8% Java files (across 10 quality
checks) comparable to 78.3% of a specialized program repair tool, with
significantly much less engineering efforts.",None,-1
aa93a1e5-4cb9-40fa-aaae-baa178f5b744,Prompt-based Extraction of Social Determinants of Health Using Few-shot Learning,0.930517,8,"Social determinants of health (SDOH) documented in the electronic health
record through unstructured text are increasingly being studied to understand
how SDOH impacts patient health outcomes. In this work, we utilize the Social
History Annotation Corpus (SHAC), a multi-institutional corpus of de-identified
social history sections annotated for SDOH, including substance use,
employment, and living status information. We explore the automatic extraction
of SDOH information with SHAC in both standoff and inline annotation formats
using GPT-4 in a one-shot prompting setting. We compare GPT-4 extraction
performance with a high-performing supervised approach and perform thorough
error analyses. Our prompt-based GPT-4 method achieved an overall 0.652 F1 on
the SHAC test set, similar to the 7th best-performing system among all teams in
the n2c2 challenge with SHAC.",None,-1
aecb44e7-ab46-4b89-96dd-0bcf6c264ebc,Gradient Domain Diffusion Models for Image Synthesis,0.119047,2,"Diffusion models are getting popular in generative image and video synthesis.
However, due to the diffusion process, they require a large number of steps to
converge. To tackle this issue, in this paper, we propose to perform the
diffusion process in the gradient domain, where the convergence becomes faster.
There are two reasons. First, thanks to the Poisson equation, the gradient
domain is mathematically equivalent to the original image domain. Therefore,
each diffusion step in the image domain has a unique corresponding gradient
domain representation. Second, the gradient domain is much sparser than the
image domain. As a result, gradient domain diffusion models converge faster.
Several numerical experiments confirm that the gradient domain diffusion models
are more efficient than the original diffusion models. The proposed method can
be applied in a wide range of applications such as image processing, computer
vision and machine learning tasks.",None,-1
3b2b7717-3316-4160-aa7a-a1377a087f8a,MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer Adapters,0.143335,5,"State-sponsored trolls are the main actors of influence campaigns on social
media and automatic troll detection is important to combat misinformation at
scale. Existing troll detection models are developed based on training data for
known campaigns (e.g.\ the influence campaign by Russia's Internet Research
Agency on the 2016 US Election), and they fall short when dealing with {\em
novel} campaigns with new targets. We propose MetaTroll, a text-based troll
detection model based on the meta-learning framework that enables high
portability and parameter-efficient adaptation to new campaigns using only a
handful of labelled samples for few-shot transfer. We introduce
\textit{campaign-specific} transformer adapters to MetaTroll to ``memorise''
campaign-specific knowledge so as to tackle catastrophic forgetting, where a
model ``forgets'' how to detect trolls from older campaigns due to continual
adaptation. Our experiments demonstrate that MetaTroll substantially
outperforms baselines and state-of-the-art few-shot text classification models.
Lastly, we explore simple approaches to extend MetaTroll to multilingual and
multimodal detection. Source code for MetaTroll is available at:
https://github.com/ltian678/metatroll-code.git.",None,-1
d7c3c46b-81a5-4343-8f92-676debd88973,milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing,0.387641,3,"Human motion sensing plays a crucial role in smart systems for
decision-making, user interaction, and personalized services. Extensive
research that has been conducted is predominantly based on cameras, whose
intrusive nature limits their use in smart home applications. To address this,
mmWave radars have gained popularity due to their privacy-friendly features. In
this work, we propose milliFlow, a novel deep learning approach to estimate
scene flow as complementary motion information for mmWave point cloud, serving
as an intermediate level of features and directly benefiting downstream human
motion sensing tasks. Experimental results demonstrate the superior performance
of our method when compared with the competing approaches. Furthermore, by
incorporating scene flow information, we achieve remarkable improvements in
human activity recognition and human parsing and support human body part
tracking. To foster further research in this area, we will provide our codebase
and dataset for open access.",None,-1
c73356a8-f1b3-4a5d-be17-34ec95f3ed3e,Empirical study of the modulus as activation function in computer vision applications,0.509669,6,"In this work we propose a new non-monotonic activation function: the modulus.
The majority of the reported research on nonlinearities is focused on monotonic
functions. We empirically demonstrate how by using the modulus activation
function on computer vision tasks the models generalize better than with other
nonlinearities - up to a 15% accuracy increase in CIFAR100 and 4% in CIFAR10,
relative to the best of the benchmark activations tested. With the proposed
activation function the vanishing gradient and dying neurons problems
disappear, because the derivative of the activation function is always 1 or -1.
The simplicity of the proposed function and its derivative make this solution
specially suitable for TinyML and hardware applications.",None,-1
dbc0aff5-26ed-4f2a-b6bb-1282577177b6,Gaussian Prior Reinforcement Learning for Nested Named Entity Recognition,0.697305,5,"Named Entity Recognition (NER) is a well and widely studied task in natural
language processing. Recently, the nested NER has attracted more attention
since its practicality and difficulty. Existing works for nested NER ignore the
recognition order and boundary position relation of nested entities. To address
these issues, we propose a novel seq2seq model named GPRL, which formulates the
nested NER task as an entity triplet sequence generation process. GPRL adopts
the reinforcement learning method to generate entity triplets decoupling the
entity order in gold labels and expects to learn a reasonable recognition order
of entities via trial and error. Based on statistics of boundary distance for
nested entities, GPRL designs a Gaussian prior to represent the boundary
distance distribution between nested entities and adjust the output probability
distribution of nested boundary tokens. Experiments on three nested NER
datasets demonstrate that GPRL outperforms previous nested NER models.",None,-1
bc993636-9c57-40db-84bd-e7235235fbec,CCGen: Explainable Complementary Concept Generation in E-Commerce,0.565221,5,"We propose and study Complementary Concept Generation (CCGen): given a
concept of interest, e.g., ""Digital Cameras"", generating a list of
complementary concepts, e.g., 1) Camera Lenses 2) Batteries 3) Camera Cases 4)
Memory Cards 5) Battery Chargers. CCGen is beneficial for various applications
like query suggestion and item recommendation, especially in the e-commerce
domain. To solve CCGen, we propose to train language models to generate ranked
lists of concepts with a two-step training strategy. We also teach the models
to generate explanations by incorporating explanations distilled from large
teacher models. Extensive experiments and analysis demonstrate that our model
can generate high-quality concepts complementary to the input concept while
producing explanations to justify the predictions.",None,-1
72f74f85-f119-4a3f-833d-6c88f1acf276,"MLTEing Models: Negotiating, Evaluating, and Documenting Model and System Qualities",0.317386,4,"Many organizations seek to ensure that machine learning (ML) and artificial
intelligence (AI) systems work as intended in production but currently do not
have a cohesive methodology in place to do so. To fill this gap, we propose
MLTE (Machine Learning Test and Evaluation, colloquially referred to as
""melt""), a framework and implementation to evaluate ML models and systems. The
framework compiles state-of-the-art evaluation techniques into an
organizational process for interdisciplinary teams, including model developers,
software engineers, system owners, and other stakeholders. MLTE tooling
supports this process by providing a domain-specific language that teams can
use to express model requirements, an infrastructure to define, generate, and
collect ML evaluation metrics, and the means to communicate results.",None,-1
bd4825a3-ceb1-47fe-a740-16b0b46b8c5b,Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking,0.968152,8,"The biomedical field relies heavily on concept linking in various areas such
as literature mining, graph alignment, information retrieval,
question-answering, data, and knowledge integration. Although large language
models (LLMs) have made significant strides in many natural language processing
tasks, their effectiveness in biomedical concept mapping is yet to be fully
explored. This research investigates a method that exploits the in-context
learning (ICL) capabilities of large models for biomedical concept linking. The
proposed approach adopts a two-stage retrieve-and-rank framework. Initially,
biomedical concepts are embedded using language models, and then embedding
similarity is utilized to retrieve the top candidates. These candidates'
contextual information is subsequently incorporated into the prompt and
processed by a large language model to re-rank the concepts. This approach
achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7%
in chemical entity normalization, exhibiting a competitive performance relative
to supervised learning methods. Further, it showed a significant improvement,
with an over 20-point absolute increase in F1 score on an oncology matching
dataset. Extensive qualitative assessments were conducted, and the benefits and
potential shortcomings of using large language models within the biomedical
domain were discussed. were discussed.",None,-1
9f13e428-a51d-4c12-9fe6-f7dba3b19f90,ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection,0.464425,2,"The task of multi-author writing style detection aims at finding any
positions of writing style change in a given text document. We formulate the
task as a natural language inference problem where two consecutive paragraphs
are paired. Our approach focuses on transitions between paragraphs while
truncating input tokens for the task. As backbone models, we employ different
Transformer-based encoders with warmup phase during training. We submit the
model version that outperforms baselines and other proposed model versions in
our experiments. For the easy and medium setups, we submit transition-focused
natural language inference based on DeBERTa with warmup training, and the same
model without transition for the hard setup.",None,-1
515b4dba-bac7-4f0c-bdb7-20963dd69582,Pushing the Boundaries of Tractable Multiperspective Reasoning: A Deduction Calculus for Standpoint EL+,0.0919349,1,"Standpoint EL is a multi-modal extension of the popular description logic EL
that allows for the integrated representation of domain knowledge relative to
diverse standpoints or perspectives. Advantageously, its satisfiability problem
has recently been shown to be in PTime, making it a promising framework for
large-scale knowledge integration.
  In this paper, we show that we can further push the expressivity of this
formalism, arriving at an extended logic, called Standpoint EL+, which allows
for axiom negation, role chain axioms, self-loops, and other features, while
maintaining tractability. This is achieved by designing a
satisfiability-checking deduction calculus, which at the same time addresses
the need for practical algorithms. We demonstrate the feasibility of our
calculus by presenting a prototypical Datalog implementation of its deduction
rules.",None,-1
89af46bb-a440-4bb4-8324-00de80a76eed,TAA-GCN: A Temporally Aware Adaptive Graph Convolutional Network for Age Estimation,0.41454,11,"This paper proposes a novel age estimation algorithm, the Temporally-Aware
Adaptive Graph Convolutional Network (TAA-GCN). Using a new representation
based on graphs, the TAA-GCN utilizes skeletal, posture, clothing, and facial
information to enrich the feature set associated with various ages. Such a
novel graph representation has several advantages: First, reduced sensitivity
to facial expression and other appearance variances; Second, robustness to
partial occlusion and non-frontal-planar viewpoint, which is commonplace in
real-world applications such as video surveillance. The TAA-GCN employs two
novel components, (1) the Temporal Memory Module (TMM) to compute temporal
dependencies in age; (2) Adaptive Graph Convolutional Layer (AGCL) to refine
the graphs and accommodate the variance in appearance. The TAA-GCN outperforms
the state-of-the-art methods on four public benchmarks, UTKFace, MORPHII, CACD,
and FG-NET. Moreover, the TAA-GCN showed reliability in different camera
viewpoints and reduced quality images.",None,-1
3280b031-108f-4d17-9f97-7c4ed509a652,Counterfactual Explanations and Predictive Models to Enhance Clinical Decision-Making in Schizophrenia using Digital Phenotyping,0.317187,1,"Clinical practice in psychiatry is burdened with the increased demand for
healthcare services and the scarce resources available. New paradigms of health
data powered with machine learning techniques could open the possibility to
improve clinical workflow in critical stages of clinical assessment and
treatment in psychiatry. In this work, we propose a machine learning system
capable of predicting, detecting, and explaining individual changes in symptoms
of patients with Schizophrenia by using behavioral digital phenotyping data. We
forecast symptoms of patients with an error rate below 10%. The system detects
decreases in symptoms using changepoint algorithms and uses counterfactual
explanations as a recourse in a simulated continuous monitoring scenario in
healthcare. Overall, this study offers valuable insights into the performance
and potential of counterfactual explanations, predictive models, and
change-point detection within a simulated clinical workflow. These findings lay
the foundation for further research to explore additional facets of the
workflow, aiming to enhance its effectiveness and applicability in real-world
healthcare settings. By leveraging these components, the goal is to develop an
actionable, interpretable, and trustworthy integrative decision support system
that combines real-time clinical assessments with sensor-based inputs.",None,-1
3709de25-07ec-4e78-b73b-b6eb7af2d7c5,DreamEditor: Text-Driven 3D Scene Editing with Neural Fields,0.976824,70,"Neural fields have achieved impressive advancements in view synthesis and
scene reconstruction. However, editing these neural fields remains challenging
due to the implicit encoding of geometry and texture information. In this
paper, we propose DreamEditor, a novel framework that enables users to perform
controlled editing of neural fields using text prompts. By representing scenes
as mesh-based neural fields, DreamEditor allows localized editing within
specific regions. DreamEditor utilizes the text encoder of a pretrained
text-to-Image diffusion model to automatically identify the regions to be
edited based on the semantics of the text prompts. Subsequently, DreamEditor
optimizes the editing region and aligns its geometry and texture with the text
prompts through score distillation sampling [29]. Extensive experiments have
demonstrated that DreamEditor can accurately edit neural fields of real-world
scenes according to the given text prompts while ensuring consistency in
irrelevant areas. DreamEditor generates highly realistic textures and geometry,
significantly surpassing previous works in both quantitative and qualitative
evaluations.",None,-1
59a83893-3985-4445-85e8-8e90151b2d5f,Sat2Density: Faithful Density Learning from Satellite-Ground Image Pairs,0.410907,4,"This paper aims to develop an accurate 3D geometry representation of
satellite images using satellite-ground image pairs. Our focus is on the
challenging problem of 3D-aware ground-views synthesis from a satellite image.
We draw inspiration from the density field representation used in volumetric
neural rendering and propose a new approach, called Sat2Density. Our method
utilizes the properties of ground-view panoramas for the sky and non-sky
regions to learn faithful density fields of 3D scenes in a geometric
perspective. Unlike other methods that require extra depth information during
training, our Sat2Density can automatically learn accurate and faithful 3D
geometry via density representation without depth supervision. This advancement
significantly improves the ground-view panorama synthesis task. Additionally,
our study provides a new geometric perspective to understand the relationship
between satellite and ground-view images in 3D space.",None,-1
6be41e21-c646-47d5-bfb9-d0118f5c49ed,Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction,0.839892,3,"Accurate prediction of what types of patents that companies will apply for in
the next period of time can figure out their development strategies and help
them discover potential partners or competitors in advance. Although important,
this problem has been rarely studied in previous research due to the challenges
in modelling companies' continuously evolving preferences and capturing the
semantic correlations of classification codes. To fill in this gap, we propose
an event-based dynamic graph learning framework for patent application trend
prediction. In particular, our method is founded on the memorable
representations of both companies and patent classification codes. When a new
patent is observed, the representations of the related companies and
classification codes are updated according to the historical memories and the
currently encoded messages. Moreover, a hierarchical message passing mechanism
is provided to capture the semantic proximities of patent classification codes
by updating their representations along the hierarchical taxonomy. Finally, the
patent application trend is predicted by aggregating the representations of the
target company and classification codes from static, dynamic, and hierarchical
perspectives. Experiments on real-world data demonstrate the effectiveness of
our approach under various experimental conditions, and also reveal the
abilities of our method in learning semantics of classification codes and
tracking technology developing trajectories of companies.",None,-1
e3c4e4be-0280-4bbc-a727-bde8176de5dd,DomainAdaptor: A Novel Approach to Test-time Adaptation,0.661406,10,"To deal with the domain shift between training and test samples, current
methods have primarily focused on learning generalizable features during
training and ignore the specificity of unseen samples that are also critical
during the test. In this paper, we investigate a more challenging task that
aims to adapt a trained CNN model to unseen domains during the test. To
maximumly mine the information in the test data, we propose a unified method
called DomainAdaptor for the test-time adaptation, which consists of an
AdaMixBN module and a Generalized Entropy Minimization (GEM) loss.
Specifically, AdaMixBN addresses the domain shift by adaptively fusing training
and test statistics in the normalization layer via a dynamic mixture
coefficient and a statistic transformation operation. To further enhance the
adaptation ability of AdaMixBN, we design a GEM loss that extends the Entropy
Minimization loss to better exploit the information in the test data. Extensive
experiments show that DomainAdaptor consistently outperforms the
state-of-the-art methods on four benchmarks. Furthermore, our method brings
more remarkable improvement against existing methods on the few-data unseen
domain. The code is available at https://github.com/koncle/DomainAdaptor.",None,-1
2868206e-01d3-4cee-88c8-44446ab31850,Exploring Large-scale Unlabeled Faces to Enhance Facial Expression Recognition,0.770541,6,"Facial Expression Recognition (FER) is an important task in computer vision
and has wide applications in human-computer interaction, intelligent security,
emotion analysis, and other fields. However, the limited size of FER datasets
limits the generalization ability of expression recognition models, resulting
in ineffective model performance. To address this problem, we propose a
semi-supervised learning framework that utilizes unlabeled face data to train
expression recognition models effectively. Our method uses a dynamic threshold
module (\textbf{DTM}) that can adaptively adjust the confidence threshold to
fully utilize the face recognition (FR) data to generate pseudo-labels, thus
improving the model's ability to model facial expressions. In the ABAW5 EXPR
task, our method achieved excellent results on the official validation set.",None,-1
3fdc265c-4bbe-4bb2-b094-3d6ec301058e,System Combination via Quality Estimation for Grammatical Error Correction,0.289983,1,"Quality estimation models have been developed to assess the corrections made
by grammatical error correction (GEC) models when the reference or
gold-standard corrections are not available. An ideal quality estimator can be
utilized to combine the outputs of multiple GEC systems by choosing the best
subset of edits from the union of all edits proposed by the GEC base systems.
However, we found that existing GEC quality estimation models are not good
enough in differentiating good corrections from bad ones, resulting in a low
F0.5 score when used for system combination. In this paper, we propose GRECO, a
new state-of-the-art quality estimation model that gives a better estimate of
the quality of a corrected sentence, as indicated by having a higher
correlation to the F0.5 score of a corrected sentence. It results in a combined
GEC system with a higher F0.5 score. We also propose three methods for
utilizing GEC quality estimation models for system combination with varying
generality: model-agnostic, model-agnostic with voting bias, and
model-dependent method. The combined GEC system outperforms the state of the
art on the CoNLL-2014 test set and the BEA-2019 test set, achieving the highest
F0.5 scores published to date.",None,-1
04a4e465-4892-448b-ae45-c0540c30f8dc,Predictive auxiliary objectives in deep RL mimic learning in the brain,0.28918,3,"The ability to predict upcoming events has been hypothesized to comprise a
key aspect of natural and machine cognition. This is supported by trends in
deep reinforcement learning (RL), where self-supervised auxiliary objectives
such as prediction are widely used to support representation learning and
improve task performance. Here, we study the effects predictive auxiliary
objectives have on representation learning across different modules of an RL
system and how these mimic representational changes observed in the brain. We
find that predictive objectives improve and stabilize learning particularly in
resource-limited architectures, and we identify settings where longer
predictive horizons better support representational transfer. Furthermore, we
find that representational changes in this RL system bear a striking
resemblance to changes in neural activity observed in the brain across various
experiments. Specifically, we draw a connection between the auxiliary
predictive model of the RL system and hippocampus, an area thought to learn a
predictive model to support memory-guided behavior. We also connect the encoder
network and the value learning network of the RL system to visual cortex and
striatum in the brain, respectively. This work demonstrates how representation
learning in deep RL systems can provide an interpretable framework for modeling
multi-region interactions in the brain. The deep RL perspective taken here also
suggests an additional role of the hippocampus in the brain -- that of an
auxiliary learning system that benefits representation learning in other
regions.",None,-1
80ffc32b-a00e-4c7a-8e8d-b852a978846b,NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models,0.983609,40,"Automatically generating high-quality real world 3D scenes is of enormous
interest for applications such as virtual reality and robotics simulation.
Towards this goal, we introduce NeuralField-LDM, a generative model capable of
synthesizing complex 3D environments. We leverage Latent Diffusion Models that
have been successfully utilized for efficient high-quality 2D content creation.
We first train a scene auto-encoder to express a set of image and pose pairs as
a neural field, represented as density and feature voxel grids that can be
projected to produce novel views of the scene. To further compress this
representation, we train a latent-autoencoder that maps the voxel grids to a
set of latent representations. A hierarchical diffusion model is then fit to
the latents to complete the scene generation pipeline. We achieve a substantial
improvement over existing state-of-the-art scene generation models.
Additionally, we show how NeuralField-LDM can be used for a variety of 3D
content creation applications, including conditional scene generation, scene
inpainting and scene style manipulation.",None,-1
1fe7c249-5417-41c2-83cb-19c216b66c23,TransCAR: Transformer-based Camera-And-Radar Fusion for 3D Object Detection,0.620433,7,"Despite radar's popularity in the automotive industry, for fusion-based 3D
object detection, most existing works focus on LiDAR and camera fusion. In this
paper, we propose TransCAR, a Transformer-based Camera-And-Radar fusion
solution for 3D object detection. Our TransCAR consists of two modules. The
first module learns 2D features from surround-view camera images and then uses
a sparse set of 3D object queries to index into these 2D features. The
vision-updated queries then interact with each other via transformer
self-attention layer. The second module learns radar features from multiple
radar scans and then applies transformer decoder to learn the interactions
between radar features and vision-updated queries. The cross-attention layer
within the transformer decoder can adaptively learn the soft-association
between the radar features and vision-updated queries instead of
hard-association based on sensor calibration only. Finally, our model estimates
a bounding box per query using set-to-set Hungarian loss, which enables the
method to avoid non-maximum suppression. TransCAR improves the velocity
estimation using the radar scans without temporal information. The superior
experimental results of our TransCAR on the challenging nuScenes datasets
illustrate that our TransCAR outperforms state-of-the-art Camera-Radar
fusion-based 3D object detection approaches.",None,-1
65627fe2-d4a2-4d12-8562-955cbcf47b72,K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging,0.414721,6,"In this paper, we consider the problem of disease diagnosis. Unlike the
conventional learning paradigm that treats labels independently, we propose a
knowledge-enhanced framework, that enables training visual representation with
the guidance of medical domain knowledge. In particular, we make the following
contributions: First, to explicitly incorporate experts' knowledge, we propose
to learn a neural representation for the medical knowledge graph via
contrastive learning, implicitly establishing relations between different
medical concepts. Second, while training the visual encoder, we keep the
parameters of the knowledge encoder frozen and propose to learn a set of prompt
vectors for efficient adaptation. Third, we adopt a Transformer-based
disease-query module for cross-model fusion, which naturally enables
explainable diagnosis results via cross attention. To validate the
effectiveness of our proposed framework, we conduct thorough experiments on
three x-ray imaging datasets across different anatomy structures, showing our
model is able to exploit the implicit relations between diseases/findings, thus
is beneficial to the commonly encountered problem in the medical domain,
namely, long-tailed and zero-shot recognition, which conventional methods
either struggle or completely fail to realize.",None,-1
fc82aeff-bcd3-460d-9ba9-dee65dea0086,Hidden Citations Obscure True Impact in Science,0.439,2,"References, the mechanism scientists rely on to signal previous knowledge,
lately have turned into widely used and misused measures of scientific impact.
Yet, when a discovery becomes common knowledge, citations suffer from
obliteration by incorporation. This leads to the concept of hidden citation,
representing a clear textual credit to a discovery without a reference to the
publication embodying it. Here, we rely on unsupervised interpretable machine
learning applied to the full text of each paper to systematically identify
hidden citations. We find that for influential discoveries hidden citations
outnumber citation counts, emerging regardless of publishing venue and
discipline. We show that the prevalence of hidden citations is not driven by
citation counts, but rather by the degree of the discourse on the topic within
the text of the manuscripts, indicating that the more discussed is a discovery,
the less visible it is to standard bibliometric analysis. Hidden citations
indicate that bibliometric measures offer a limited perspective on quantifying
the true impact of a discovery, raising the need to extract knowledge from the
full text of the scientific corpus.",None,-1
c123c69a-fc49-435b-bd6b-aa04c106a9f0,Using Large Language Models for Hyperparameter Optimization,0.992909,16,"This paper studies using foundational large language models (LLMs) to make
decisions during hyperparameter optimization (HPO). Empirical evaluations
demonstrate that in settings with constrained search budgets, LLMs can perform
comparably or better than traditional HPO methods like random search and
Bayesian optimization on standard benchmarks. Furthermore, we propose to treat
the code specifying our model as a hyperparameter, which the LLM outputs, going
beyond the capabilities of existing HPO approaches. Our findings suggest that
LLMs are a promising tool for improving efficiency in the traditional
decision-making problem of hyperparameter optimization.",None,-1
befb16f6-4ba0-429f-8b93-508938588e03,Re-thinking Federated Active Learning based on Inter-class Diversity,0.290416,5,"Although federated learning has made awe-inspiring advances, most studies
have assumed that the client's data are fully labeled. However, in a real-world
scenario, every client may have a significant amount of unlabeled instances.
Among the various approaches to utilizing unlabeled data, a federated active
learning framework has emerged as a promising solution. In the decentralized
setting, there are two types of available query selector models, namely
'global' and 'local-only' models, but little literature discusses their
performance dominance and its causes. In this work, we first demonstrate that
the superiority of two selector models depends on the global and local
inter-class diversity. Furthermore, we observe that the global and local-only
models are the keys to resolving the imbalance of each side. Based on our
findings, we propose LoGo, a FAL sampling strategy robust to varying local
heterogeneity levels and global imbalance ratio, that integrates both models by
two steps of active selection scheme. LoGo consistently outperforms six active
learning strategies in the total number of 38 experimental settings.",None,-1
bee6581a-fd0a-4bc1-ad65-fc55f0a0e38d,USA-Net: Unified Semantic and Affordance Representations for Robot Memory,0.597776,9,"In order for robots to follow open-ended instructions like ""go open the brown
cabinet over the sink"", they require an understanding of both the scene
geometry and the semantics of their environment. Robotic systems often handle
these through separate pipelines, sometimes using very different representation
spaces, which can be suboptimal when the two objectives conflict. In this work,
we present USA-Net, a simple method for constructing a world representation
that encodes both the semantics and spatial affordances of a scene in a
differentiable map. This allows us to build a gradient-based planner which can
navigate to locations in the scene specified using open-ended vocabulary. We
use this planner to consistently generate trajectories which are both shorter
5-10% shorter and 10-30% closer to our goal query in CLIP embedding space than
paths from comparable grid-based planners which don't leverage gradient
information. To our knowledge, this is the first end-to-end differentiable
planner optimizes for both semantics and affordance in a single implicit map.
Code and visuals are available at our website: https://usa.bolte.cc/",None,-1
2b5f917f-7fdd-4e24-bd1a-fe43da75cd0a,Semantic Embedded Deep Neural Network: A Generic Approach to Boost Multi-Label Image Classification Performance,0.586783,6,"Fine-grained multi-label classification models have broad applications in
e-commerce, such as visual based label predictions ranging from fashion
attribute detection to brand recognition. One challenge to achieve satisfactory
performance for those classification tasks in real world is the wild visual
background signal that contains irrelevant pixels which confuses model to focus
onto the region of interest and make prediction upon the specific region. In
this paper, we introduce a generic semantic-embedding deep neural network to
apply the spatial awareness semantic feature incorporating a channel-wise
attention based model to leverage the localization guidance to boost model
performance for multi-label prediction. We observed an Avg.relative improvement
of 15.27% in terms of AUC score across all labels compared to the baseline
approach. Core experiment and ablation studies involve multi-label fashion
attribute classification performed on Instagram fashion apparels' image. We
compared the model performances among our approach, baseline approach, and 3
alternative approaches to leverage semantic features. Results show favorable
performance for our approach.",None,-1
42e70fe5-622d-45e6-bb40-04b72f981478,Can Diffusion Model Achieve Better Performance in Text Generation? Bridging the Gap between Training and Inference!,0.16228,9,"Diffusion models have been successfully adapted to text generation tasks by
mapping the discrete text into the continuous space. However, there exist
nonnegligible gaps between training and inference, owing to the absence of the
forward process during inference. Thus, the model only predicts based on the
previously generated reverse noise rather than the noise computed by the
forward process. Besides, the widely-used downsampling strategy in speeding up
the inference will cause the mismatch of diffusion trajectories between
training and inference. To understand and mitigate the above two types of
training-inference discrepancies, we launch a thorough preliminary study. Based
on our observations, we propose two simple yet effective methods to bridge the
gaps mentioned above, named Distance Penalty and Adaptive Decay Sampling.
Extensive experiments on \textbf{6} generation tasks confirm the superiority of
our methods, which can achieve $100\times \rightarrow 200\times$ speedup with
better performance.",None,-1
4578bac1-e07f-4111-b022-4a8c72e4227c,A Diachronic Perspective on User Trust in AI under Uncertainty,0.534309,6,"In a human-AI collaboration, users build a mental model of the AI system
based on its reliability and how it presents its decision, e.g. its
presentation of system confidence and an explanation of the output. Modern NLP
systems are often uncalibrated, resulting in confidently incorrect predictions
that undermine user trust. In order to build trustworthy AI, we must understand
how user trust is developed and how it can be regained after potential
trust-eroding events. We study the evolution of user trust in response to these
trust-eroding events using a betting game. We find that even a few incorrect
instances with inaccurate confidence estimates damage user trust and
performance, with very slow recovery. We also show that this degradation in
trust reduces the success of human-AI collaboration and that different types of
miscalibration -- unconfidently correct and confidently incorrect -- have
different negative effects on user trust. Our findings highlight the importance
of calibration in user-facing AI applications and shed light on what aspects
help users decide whether to trust the AI system.",None,-1
c208c0f6-9a34-4fdf-9384-b4d8cf0bc54b,Semiconductor Fab Scheduling with Self-Supervised and Reinforcement Learning,0.506828,1,"Semiconductor manufacturing is a notoriously complex and costly multi-step
process involving a long sequence of operations on expensive and
quantity-limited equipment. Recent chip shortages and their impacts have
highlighted the importance of semiconductors in the global supply chains and
how reliant on those our daily lives are. Due to the investment cost,
environmental impact, and time scale needed to build new factories, it is
difficult to ramp up production when demand spikes.
  This work introduces a method to successfully learn to schedule a
semiconductor manufacturing facility more efficiently using deep reinforcement
and self-supervised learning. We propose the first adaptive scheduling approach
to handle complex, continuous, stochastic, dynamic, modern semiconductor
manufacturing models. Our method outperforms the traditional hierarchical
dispatching strategies typically used in semiconductor manufacturing plants,
substantially reducing each order's tardiness and time until completion. As a
result, our method yields a better allocation of resources in the semiconductor
manufacturing process.",None,-1
8e3b57f5-d92f-46cc-ae21-276e641332ef,PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction,0.36636,3,"Speech-to-text errors made by automatic speech recognition (ASR) systems
negatively impact downstream models. Error correction models as a
post-processing text editing method have been recently developed for refining
the ASR outputs. However, efficient models that meet the low latency
requirements of industrial grade production systems have not been well studied.
We propose PATCorrect-a novel non-autoregressive (NAR) approach based on
multi-modal fusion leveraging representations from both text and phoneme
modalities, to reduce word error rate (WER) and perform robustly with varying
input transcription quality. We demonstrate that PATCorrect consistently
outperforms state-of-the-art NAR method on English corpus across different
upstream ASR systems, with an overall 11.62% WER reduction (WERR) compared to
9.46% WERR achieved by other methods using text only modality. Besides, its
inference latency is at tens of milliseconds, making it ideal for systems with
low latency requirements.",None,-1
1d9ccf00-e9a9-456f-b809-32c38d3c1854,Monocular Visual-Inertial Depth Estimation,0.836452,6,"We present a visual-inertial depth estimation pipeline that integrates
monocular depth estimation and visual-inertial odometry to produce dense depth
estimates with metric scale. Our approach performs global scale and shift
alignment against sparse metric depth, followed by learning-based dense
alignment. We evaluate on the TartanAir and VOID datasets, observing up to 30%
reduction in inverse RMSE with dense scale alignment relative to performing
just global alignment alone. Our approach is especially competitive at low
density; with just 150 sparse metric depth points, our dense-to-dense depth
alignment method achieves over 50% lower iRMSE over sparse-to-dense depth
completion by KBNet, currently the state of the art on VOID. We demonstrate
successful zero-shot transfer from synthetic TartanAir to real-world VOID data
and perform generalization tests on NYUv2 and VCU-RVI. Our approach is modular
and is compatible with a variety of monocular depth estimation models. Video:
https://youtu.be/IMwiKwSpshQ Code: https://github.com/isl-org/VI-Depth",None,-1
9d461b13-e887-4361-ae05-67eeabb476d3,Regret Bounds for Markov Decision Processes with Recursive Optimized Certainty Equivalents,0.78911,8,"The optimized certainty equivalent (OCE) is a family of risk measures that
cover important examples such as entropic risk, conditional value-at-risk and
mean-variance models. In this paper, we propose a new episodic risk-sensitive
reinforcement learning formulation based on tabular Markov decision processes
with recursive OCEs. We design an efficient learning algorithm for this problem
based on value iteration and upper confidence bound. We derive an upper bound
on the regret of the proposed algorithm, and also establish a minimax lower
bound. Our bounds show that the regret rate achieved by our proposed algorithm
has optimal dependence on the number of episodes and the number of actions.",None,-1
61337538-52ad-466e-ae55-57f9eceec9cd,Working Memory Capacity of ChatGPT: An Empirical Study,0.515585,3,"Working memory is a critical aspect of both human intelligence and artificial
intelligence, serving as a workspace for the temporary storage and manipulation
of information. In this paper, we systematically assess the working memory
capacity of ChatGPT, a large language model developed by OpenAI, by examining
its performance in verbal and spatial n-back tasks under various conditions.
Our experiments reveal that ChatGPT has a working memory capacity limit
strikingly similar to that of humans. Furthermore, we investigate the impact of
different instruction strategies on ChatGPT's performance and observe that the
fundamental patterns of a capacity limit persist. From our empirical findings,
we propose that n-back tasks may serve as tools for benchmarking the working
memory capacity of large language models and hold potential for informing
future efforts aimed at enhancing AI working memory.",None,-1
2ee32681-358b-4927-a44d-0ad34c759412,"Joint Person Identity, Gender and Age Estimation from Hand Images using Deep Multi-Task Representation Learning",0.158921,2,"In this paper, we propose a multi-task representation learning framework to
jointly estimate the identity, gender and age of individuals from their hand
images for the purpose of criminal investigations since the hand images are
often the only available information in cases of serious crime such as sexual
abuse. We investigate different up-to-date deep learning architectures and
compare their performance for joint estimation of identity, gender and age from
hand images of perpetrators of serious crime. To simplify the age prediction,
we create age groups for the age estimation. We make extensive evaluations and
comparisons of both convolution-based and transformer-based deep learning
architectures on a publicly available 11k hands dataset. Our experimental
analysis shows that it is possible to efficiently estimate not only identity
but also other attributes such as gender and age of suspects jointly from hand
images for criminal investigations, which is crucial in assisting international
police forces in the court to identify and convict abusers.",None,-1
5054dd8f-f13e-4a8c-87cc-0e02787a897f,Interactive Learning of Hierarchical Tasks from Dialog with GPT,0.0582038,1,"We present a system for interpretable, symbolic, interactive task learning
from dialog using a GPT model as a conversational front-end. The learned tasks
are represented as hierarchical decompositions of predicate-argument structures
with scoped variable arguments. By using a GPT model to convert interactive
dialog into a semantic representation, and then recursively asking for
definitions of unknown steps, we show that hierarchical task knowledge can be
acquired and re-used in a natural and unrestrained conversational environment.
We compare our system to a similar architecture using a more conventional
parser and show that our system tolerates a much wider variety of linguistic
variance.",None,-1
150008c5-ae3d-4fb0-aa75-f29b453b07b7,Attention! Dynamic Epistemic Logic Models of (In)attentive Agents,0.522978,2,"Attention is the crucial cognitive ability that limits and selects what
information we observe. Previous work by Bolander et al. (2016) proposes a
model of attention based on dynamic epistemic logic (DEL) where agents are
either fully attentive or not attentive at all. While introducing the realistic
feature that inattentive agents believe nothing happens, the model does not
represent the most essential aspect of attention: its selectivity. Here, we
propose a generalization that allows for paying attention to subsets of atomic
formulas. We introduce the corresponding logic for propositional attention, and
show its axiomatization to be sound and complete. We then extend the framework
to account for inattentive agents that, instead of assuming nothing happens,
may default to a specific truth-value of what they failed to attend to (a sort
of prior concerning the unattended atoms). This feature allows for a more
cognitively plausible representation of the inattentional blindness phenomenon,
where agents end up with false beliefs due to their failure to attend to
conspicuous but unexpected events. Both versions of the model define
attention-based learning through appropriate DEL event models based on a few
and clear edge principles. While the size of such event models grow
exponentially both with the number of agents and the number of atoms, we
introduce a new logical language for describing event models syntactically and
show that using this language our event models can be represented linearly in
the number of agents and atoms. Furthermore, representing our event models
using this language is achieved by a straightforward formalisation of the
aforementioned edge principles.",None,-1
d91dd95a-cee8-41ee-a139-bcee9c6f3721,Controllable Mind Visual Diffusion Model,0.827786,12,"Brain signal visualization has emerged as an active research area, serving as
a critical interface between the human visual system and computer vision
models. Although diffusion models have shown promise in analyzing functional
magnetic resonance imaging (fMRI) data, including reconstructing high-quality
images consistent with original visual stimuli, their accuracy in extracting
semantic and silhouette information from brain signals remains limited. In this
regard, we propose a novel approach, referred to as Controllable Mind Visual
Diffusion Model (CMVDM). CMVDM extracts semantic and silhouette information
from fMRI data using attribute alignment and assistant networks. Additionally,
a residual block is incorporated to capture information beyond semantic and
silhouette features. We then leverage a control model to fully exploit the
extracted information for image synthesis, resulting in generated images that
closely resemble the visual stimuli in terms of semantics and silhouette.
Through extensive experimentation, we demonstrate that CMVDM outperforms
existing state-of-the-art methods both qualitatively and quantitatively.",None,-1
e62eb71b-e987-4bb0-a0e1-6e6c4881c79c,Prompt2Model: Generating Deployable Models from Natural Language Instructions,0.379631,16,"Large language models (LLMs) enable system builders today to create competent
NLP systems through prompting, where they only need to describe the task in
natural language and provide a few examples. However, in other ways, LLMs are a
step backward from traditional special-purpose NLP models; they require
extensive computational resources for deployment and can be gated behind APIs.
In this paper, we propose Prompt2Model, a general-purpose method that takes a
natural language task description like the prompts provided to LLMs, and uses
it to train a special-purpose model that is conducive to deployment. This is
done through a multi-step process of retrieval of existing datasets and
pretrained models, dataset generation using LLMs, and supervised fine-tuning on
these retrieved and generated datasets. Over three tasks, we demonstrate that
given the same few-shot prompt as input, Prompt2Model trains models that
outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20%
while being up to 700 times smaller. We also show that this data can be used to
obtain reliable performance estimates of model performance, enabling model
developers to assess model reliability before deployment. Prompt2Model is
available open-source at https://github.com/neulab/prompt2model.",None,-1
c14a8e82-5b86-4d1e-9967-996f878378f3,Triplet Loss-less Center Loss Sampling Strategies in Facial Expression Recognition Scenarios,0.749447,6,"Facial expressions convey massive information and play a crucial role in
emotional expression. Deep neural network (DNN) accompanied by deep metric
learning (DML) techniques boost the discriminative ability of the model in
facial expression recognition (FER) applications. DNN, equipped with only
classification loss functions such as Cross-Entropy cannot compact intra-class
feature variation or separate inter-class feature distance as well as when it
gets fortified by a DML supporting loss item. The triplet center loss (TCL)
function is applied on all dimensions of the sample's embedding in the
embedding space. In our work, we developed three strategies: fully-synthesized,
semi-synthesized, and prediction-based negative sample selection strategies. To
achieve better results, we introduce a selective attention module that provides
a combination of pixel-wise and element-wise attention coefficients using
high-semantic deep features of input samples. We evaluated the proposed method
on the RAF-DB, a highly imbalanced dataset. The experimental results reveal
significant improvements in comparison to the baseline for all three negative
sample selection strategies.",None,-1
5870170d-f42a-4cb1-a4dd-08bccf12388a,AutoScrum: Automating Project Planning Using Large Language Models,0.622142,4,"Recent advancements in the field of large language models have made it
possible to use language models for advanced reasoning. In this paper we
leverage this ability for designing complex project plans based only on knowing
the current state and the desired state. Two approaches are demonstrated - a
scrum based approach and a shortcut plan approach. The scrum based approach
executes an automated process of requirements gathering, user story mapping,
feature identification, task decomposition and finally generates questions and
search terms for seeking out domain specific information to assist with task
completion. The shortcut approach looks at most recent snapshot of the current
and desired state and generates the next most reasonable task to do in order to
get to the desired state as quickly as possible. In this paper we automate
everything using a novel concept of ""Language Programs"". These are programs
written in natural language designed to process input data through the language
model. Guidance language is used for all LLM programs. All demo source code for
this paper is available at https://github.com/autoscrum/autoscrum",None,-1
dc2b10e3-a4c4-4a10-99bd-f4280626bd41,Learning Attention as Disentangler for Compositional Zero-shot Learning,0.309626,10,"Compositional zero-shot learning (CZSL) aims at learning visual concepts
(i.e., attributes and objects) from seen compositions and combining concept
knowledge into unseen compositions. The key to CZSL is learning the
disentanglement of the attribute-object composition. To this end, we propose to
exploit cross-attentions as compositional disentanglers to learn disentangled
concept embeddings. For example, if we want to recognize an unseen composition
""yellow flower"", we can learn the attribute concept ""yellow"" and object concept
""flower"" from different yellow objects and different flowers respectively. To
further constrain the disentanglers to learn the concept of interest, we employ
a regularization at the attention level. Specifically, we adapt the earth
mover's distance (EMD) as a feature similarity metric in the cross-attention
module. Moreover, benefiting from concept disentanglement, we improve the
inference process and tune the prediction score by combining multiple concept
probabilities. Comprehensive experiments on three CZSL benchmark datasets
demonstrate that our method significantly outperforms previous works in both
closed- and open-world settings, establishing a new state-of-the-art.",None,-1
664491e9-f54d-4e0a-aaba-b64d773f2f4d,Conditioning Diffusion Models via Attributes and Semantic Masks for Face Generation,0.198671,2,"Deep generative models have shown impressive results in generating realistic
images of faces. GANs managed to generate high-quality, high-fidelity images
when conditioned on semantic masks, but they still lack the ability to
diversify their output. Diffusion models partially solve this problem and are
able to generate diverse samples given the same condition. In this paper, we
propose a multi-conditioning approach for diffusion models via cross-attention
exploiting both attributes and semantic masks to generate high-quality and
controllable face images. We also studied the impact of applying
perceptual-focused loss weighting into the latent space instead of the pixel
space. Our method extends the previous approaches by introducing conditioning
on more than one set of features, guaranteeing a more fine-grained control over
the generated face images. We evaluate our approach on the CelebA-HQ dataset,
and we show that it can generate realistic and diverse samples while allowing
for fine-grained control over multiple attributes and semantic regions.
Additionally, we perform an ablation study to evaluate the impact of different
conditioning strategies on the quality and diversity of the generated images.",None,-1
97d22a2d-f3aa-42fa-850e-5c416325a13c,Machine-Generated Text Detection using Deep Learning,0.104165,1,"Our research focuses on the crucial challenge of discerning text produced by
Large Language Models (LLMs) from human-generated text, which holds
significance for various applications. With ongoing discussions about attaining
a model with such functionality, we present supporting evidence regarding the
feasibility of such models. We evaluated our models on multiple datasets,
including Twitter Sentiment, Football Commentary, Project Gutenberg, PubMedQA,
and SQuAD, confirming the efficacy of the enhanced detection approaches. These
datasets were sampled with intricate constraints encompassing every
possibility, laying the foundation for future research. We evaluate
GPT-3.5-Turbo against various detectors such as SVM, RoBERTa-base, and
RoBERTa-large. Based on the research findings, the results predominantly relied
on the sequence length of the sentence.",None,-1
2b053d90-a550-41c2-b9c2-2cdacb89b55c,"SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation",0.797587,21,"Reliable automatic evaluation of summarization systems is challenging due to
the multifaceted and subjective nature of the task. This is especially the case
for languages other than English, where human evaluations are scarce. In this
work, we introduce SEAHORSE, a dataset for multilingual, multifaceted
summarization evaluation. SEAHORSE consists of 96K summaries with human ratings
along 6 dimensions of text quality: comprehensibility, repetition, grammar,
attribution, main ideas, and conciseness, covering 6 languages, 9 systems and 4
datasets. As a result of its size and scope, SEAHORSE can serve both as a
benchmark to evaluate learnt metrics, as well as a large-scale resource for
training such metrics. We show that metrics trained with SEAHORSE achieve
strong performance on the out-of-domain meta-evaluation benchmarks TRUE
(Honovich et al., 2022) and mFACE (Aharoni et al., 2022). We make the SEAHORSE
dataset and metrics publicly available for future research on multilingual and
multifaceted summarization evaluation.",None,-1
781dce0e-1ad9-42f6-a109-5417ef4ca4df,Benchmarking Stroke Forecasting with Stroke-Level Badminton Dataset,0.388675,4,"In recent years, badminton analytics has drawn attention due to the
advancement of artificial intelligence and the efficiency of data collection.
While there is a line of effective applications to improve and investigate
player performance, there are only a few public badminton datasets that can be
used by researchers outside the badminton domain. Existing badminton singles
datasets focus on specific matchups; however, they cannot provide comprehensive
studies on different players and various matchups. In this paper, we provide a
badminton singles dataset, ShuttleSet22, which is collected from high-ranking
matches in 2022. ShuttleSet22 consists of 30,172 strokes in 2,888 rallies in
the training set, 1,400 strokes in 450 rallies in the validation set, and 2,040
strokes in 654 rallies in the testing set, with detailed stroke-level metadata
within a rally. To benchmark existing work with ShuttleSet22, we hold a
challenge, Track 2: Forecasting Future Turn-Based Strokes in Badminton Rallies,
at CoachAI Badminton Challenge @ IJCAI 2023, to encourage researchers to tackle
this real-world problem through innovative approaches and to summarize insights
between the state-of-the-art baseline and improved techniques, exchanging
inspiring ideas. The baseline codes and the dataset are made available at
https://github.com/wywyWang/CoachAI-Projects/tree/main/CoachAI-Challenge-IJCAI2023.",None,-1
64dd4102-b2a3-4a3e-a15b-d1bb7d450a90,TextMesh: Generation of Realistic 3D Meshes From Text Prompts,0.986285,100,"The ability to generate highly realistic 2D images from mere text prompts has
recently made huge progress in terms of speed and quality, thanks to the advent
of image diffusion models. Naturally, the question arises if this can be also
achieved in the generation of 3D content from such text prompts. To this end, a
new line of methods recently emerged trying to harness diffusion models,
trained on 2D images, for supervision of 3D model generation using view
dependent prompts. While achieving impressive results, these methods, however,
have two major drawbacks. First, rather than commonly used 3D meshes, they
instead generate neural radiance fields (NeRFs), making them impractical for
most real applications. Second, these approaches tend to produce over-saturated
models, giving the output a cartoonish looking effect. Therefore, in this work
we propose a novel method for generation of highly realistic-looking 3D meshes.
To this end, we extend NeRF to employ an SDF backbone, leading to improved 3D
mesh extraction. In addition, we propose a novel way to finetune the mesh
texture, removing the effect of high saturation and improving the details of
the output 3D mesh.",None,-1
abb16332-1cc7-4f1c-9e17-4d42669f094a,ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain,0.146684,6,"The increasing number of benchmarks for Natural Language Processing (NLP)
tasks in the computational job market domain highlights the demand for methods
that can handle job-related tasks such as skill extraction, skill
classification, job title classification, and de-identification. While some
approaches have been developed that are specific to the job market domain,
there is a lack of generalized, multilingual models and benchmarks for these
tasks. In this study, we introduce a language model called ESCOXLM-R, based on
XLM-R, which uses domain-adaptive pre-training on the European Skills,
Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27
languages. The pre-training objectives for ESCOXLM-R include dynamic masked
language modeling and a novel additional objective for inducing multilingual
taxonomical ESCO relations. We comprehensively evaluate the performance of
ESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and
find that it achieves state-of-the-art results on 6 out of 9 datasets. Our
analysis reveals that ESCOXLM-R performs better on short spans and outperforms
XLM-R on entity-level and surface-level span-F1, likely due to ESCO containing
short skill and occupation titles, and encoding information on the
entity-level.",None,-1
c68bef1b-7af2-4059-b78f-b83e6f8c8c17,No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function,0.122464,4,"Large language models (LLMs) demonstrate impressive language understanding
and contextual learning abilities, making them suitable for natural language
processing (NLP) tasks and complex mathematical reasoning. However, when
applied to mathematical reasoning tasks, LLMs often struggle to generate
correct reasoning steps and answers despite having high probabilities for the
solutions. To overcome this limitation and enhance the mathematical reasoning
capabilities of fine-tuned LLMs without additional fine-tuning steps, we
propose a method that incorporates Monte Carlo Tree Search (MCTS) and a
lightweight energy function to rank decision steps and enable immediate
reaction and precise reasoning. Specifically, we re-formulate the fine-tuned
LLMs into a Residual-based Energy Model (Residual-EBM) and employ noise
contrastive estimation to estimate the energy function's parameters. We then
utilize MCTS with the energy function as a path verifier to search the output
space and evaluate the reasoning path. Through extensive experiments on two
mathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the
exceptional capabilities of our method, which significantly improves the pass@1
metric of the fine-tuned model without requiring additional fine-tuning or
reinforcement learning with human feedback alignment.",None,-1
6484a738-ac6a-4fa5-81c2-f45cbb4c25a2,Noise-to-Norm Reconstruction for Industrial Anomaly Detection and Localization,0.0979567,1,"Anomaly detection has a wide range of applications and is especially
important in industrial quality inspection. Currently, many top-performing
anomaly-detection models rely on feature-embedding methods. However, these
methods do not perform well on datasets with large variations in object
locations. Reconstruction-based methods use reconstruction errors to detect
anomalies without considering positional differences between samples. In this
study, a reconstruction-based method using the noise-to-norm paradigm is
proposed, which avoids the invariant reconstruction of anomalous regions. Our
reconstruction network is based on M-net and incorporates multiscale fusion and
residual attention modules to enable end-to-end anomaly detection and
localization. Experiments demonstrate that the method is effective in
reconstructing anomalous regions into normal patterns and achieving accurate
anomaly detection and localization. On the MPDD and VisA datasets, our proposed
method achieved more competitive results than the latest methods, and it set a
new state-of-the-art standard on the MPDD dataset.",None,-1
39df10f0-9853-42e7-8714-c0c4d21a04bc,"Generate, Transform, Answer: Question Specific Tool Synthesis for Tabular Data",0.194939,8,"Tabular question answering (TQA) presents a challenging setting for neural
systems by requiring joint reasoning of natural language with large amounts of
semi-structured data. Unlike humans who use programmatic tools like filters to
transform data before processing, language models in TQA process tables
directly, resulting in information loss as table size increases. In this paper
we propose ToolWriter to generate query specific programs and detect when to
apply them to transform tables and align them with the TQA model's
capabilities. Focusing ToolWriter to generate row-filtering tools improves the
state-of-the-art for WikiTableQuestions and WikiSQL with the most performance
gained on long tables. By investigating headroom, our work highlights the
broader potential for programmatic tools combined with neural components to
manipulate large amounts of structured data.",None,-1
8acdbdbb-5576-4885-ba69-4c15c54c3727,On the Generalization of Multi-modal Contrastive Learning,0.542192,11,"Multi-modal contrastive learning (MMCL) has recently garnered considerable
interest due to its superior performance in visual tasks, achieved by embedding
multi-modal data, such as visual-language pairs. However, there still lack
theoretical understandings of how MMCL extracts useful visual representation
from multi-modal pairs, and particularly, how MMCL outperforms previous
approaches like self-supervised contrastive learning (SSCL). In this paper, by
drawing an intrinsic connection between MMCL and asymmetric matrix
factorization, we establish the first generalization guarantees of MMCL for
visual downstream tasks. Based on this framework, we further unify MMCL and
SSCL by showing that MMCL implicitly performs SSCL with (pseudo) positive pairs
induced by text pairs. Through this unified perspective, we characterize the
advantage of MMCL by showing that text pairs induce more semantically
consistent and diverse positive pairs, which, according to our analysis,
provably benefit downstream generalization. Inspired by this finding, we
propose CLIP-guided resampling methods to significantly improve the downstream
performance of SSCL on ImageNet by leveraging multi-modal information. Code is
available at https://github.com/PKU-ML/CLIP-Help-SimCLR.",None,-1
4d6246f1-c8fd-4977-a7b6-93fe466323f7,Structured Pruning for Multi-Task Deep Neural Networks,0.166413,1,"Although multi-task deep neural network (DNN) models have computation and
storage benefits over individual single-task DNN models, they can be further
optimized via model compression. Numerous structured pruning methods are
already developed that can readily achieve speedups in single-task models, but
the pruning of multi-task networks has not yet been extensively studied. In
this work, we investigate the effectiveness of structured pruning on multi-task
models. We use an existing single-task filter pruning criterion and also
introduce an MTL-based filter pruning criterion for estimating the filter
importance scores. We prune the model using an iterative pruning strategy with
both pruning methods. We show that, with careful hyper-parameter tuning,
architectures obtained from different pruning methods do not have significant
differences in their performances across tasks when the number of parameters is
similar. We also show that iterative structure pruning may not be the best way
to achieve a well-performing pruned model because, at extreme pruning levels,
there is a high drop in performance across all tasks. But when the same models
are randomly initialized and re-trained, they show better results.",None,-1
b7170c89-d4f8-4a31-a35f-6ff2400759dd,Learning Conditional Attributes for Compositional Zero-Shot Learning,0.187117,5,"Compositional Zero-Shot Learning (CZSL) aims to train models to recognize
novel compositional concepts based on learned concepts such as attribute-object
combinations. One of the challenges is to model attributes interacted with
different objects, e.g., the attribute ``wet"" in ``wet apple"" and ``wet cat"" is
different. As a solution, we provide analysis and argue that attributes are
conditioned on the recognized object and input image and explore learning
conditional attribute embeddings by a proposed attribute learning framework
containing an attribute hyper learner and an attribute base learner. By
encoding conditional attributes, our model enables to generate flexible
attribute embeddings for generalization from seen to unseen compositions.
Experiments on CZSL benchmarks, including the more challenging C-GQA dataset,
demonstrate better performances compared with other state-of-the-art approaches
and validate the importance of learning conditional attributes. Code is
available at https://github.com/wqshmzh/CANet-CZSL",None,-1
3f2d1c5f-b5b0-470b-979e-727f641bb1ab,CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains,0.144509,4,"Generative chat models, such as ChatGPT and GPT-4, have revolutionized
natural language generation (NLG) by incorporating instructions and human
feedback to achieve significant performance improvements. However, the lack of
standardized evaluation benchmarks for chat models, particularly for Chinese
and domain-specific models, hinders their assessment and progress. To address
this gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark,
focusing on general and financial domains. The CGCE benchmark encompasses
diverse tasks, including 200 questions in the general domain and 150 specific
professional questions in the financial domain. Manual scoring evaluates
factors such as accuracy, coherence, expression clarity, and completeness. The
CGCE benchmark provides researchers with a standardized framework to assess and
compare Chinese generative chat models, fostering advancements in NLG research.",None,-1
7fe929f7-c9a6-4412-9256-7a2caa5ba3af,Implicit Neural Head Synthesis via Controllable Local Deformation Fields,0.524946,6,"High-quality reconstruction of controllable 3D head avatars from 2D videos is
highly desirable for virtual human applications in movies, games, and
telepresence. Neural implicit fields provide a powerful representation to model
3D head avatars with personalized shape, expressions, and facial parts, e.g.,
hair and mouth interior, that go beyond the linear 3D morphable model (3DMM).
However, existing methods do not model faces with fine-scale facial features,
or local control of facial parts that extrapolate asymmetric expressions from
monocular videos. Further, most condition only on 3DMM parameters with poor(er)
locality, and resolve local features with a global neural field. We build on
part-based implicit shape models that decompose a global deformation field into
local ones. Our novel formulation models multiple implicit deformation fields
with local semantic rig-like control via 3DMM-based parameters, and
representative facial landmarks. Further, we propose a local control loss and
attention mask mechanism that promote sparsity of each learned deformation
field. Our formulation renders sharper locally controllable nonlinear
deformations than previous implicit monocular approaches, especially mouth
interior, asymmetric expressions, and facial details.",None,-1
f6bc9289-9abf-4b69-a345-76a78743a268,Leveraging BERT Language Models for Multi-Lingual ESG Issue Identification,0.953649,7,"Environmental, Social, and Governance (ESG) has been used as a metric to
measure the negative impacts and enhance positive outcomes of companies in
areas such as the environment, society, and governance. Recently, investors
have increasingly recognized the significance of ESG criteria in their
investment choices, leading businesses to integrate ESG principles into their
operations and strategies. The Multi-Lingual ESG Issue Identification (ML-ESG)
shared task encompasses the classification of news documents into 35 distinct
ESG issue labels. In this study, we explored multiple strategies harnessing
BERT language models to achieve accurate classification of news documents
across these labels. Our analysis revealed that the RoBERTa classifier emerged
as one of the most successful approaches, securing the second-place position
for the English test dataset, and sharing the fifth-place position for the
French test dataset. Furthermore, our SVM-based binary model tailored for the
Chinese language exhibited exceptional performance, earning the second-place
rank on the test dataset.",None,-1
512491da-e707-4bf9-b383-46b88b361168,Deception Abilities Emerged in Large Language Models,0.463849,23,"Large language models (LLMs) are currently at the forefront of intertwining
artificial intelligence (AI) systems with human communication and everyday
life. Thus, aligning them with human values is of great importance. However,
given the steady increase in reasoning abilities, future LLMs are under
suspicion of becoming able to deceive human operators and utilizing this
ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to
possess a conceptual understanding of deception strategies. This study reveals
that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were
non-existent in earlier LLMs. We conduct a series of experiments showing that
state-of-the-art LLMs are able to understand and induce false beliefs in other
agents, that their performance in complex deception scenarios can be amplified
utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in
LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown
machine behavior in LLMs, our study contributes to the nascent field of machine
psychology.",None,-1
35482194-3c62-46f7-a8a5-b095d5308926,Exploiting Unlabeled Data for Feedback Efficient Human Preference based Reinforcement Learning,0.0575029,2,"Preference Based Reinforcement Learning has shown much promise for utilizing
human binary feedback on queried trajectory pairs to recover the underlying
reward model of the Human in the Loop (HiL). While works have attempted to
better utilize the queries made to the human, in this work we make two
observations about the unlabeled trajectories collected by the agent and
propose two corresponding loss functions that ensure participation of unlabeled
trajectories in the reward learning process, and structure the embedding space
of the reward model such that it reflects the structure of state space with
respect to action distances. We validate the proposed method on one locomotion
domain and one robotic manipulation task and compare with the state-of-the-art
baseline PEBBLE. We further present an ablation of the proposed loss components
across both the domains and find that not only each of the loss components
perform better than the baseline, but the synergic combination of the two has
much better reward recovery and human feedback sample efficiency.",None,-1
33070bce-9179-47cd-ae23-386b69a32ee9,Prompting Large Language Models for Topic Modeling,0.613611,8,"Topic modeling is a widely used technique for revealing underlying thematic
structures within textual data. However, existing models have certain
limitations, particularly when dealing with short text datasets that lack
co-occurring words. Moreover, these models often neglect sentence-level
semantics, focusing primarily on token-level semantics. In this paper, we
propose PromptTopic, a novel topic modeling approach that harnesses the
advanced language understanding of large language models (LLMs) to address
these challenges. It involves extracting topics at the sentence level from
individual documents, then aggregating and condensing these topics into a
predefined quantity, ultimately providing coherent topics for texts of varying
lengths. This approach eliminates the need for manual parameter tuning and
improves the quality of extracted topics. We benchmark PromptTopic against the
state-of-the-art baselines on three vastly diverse datasets, establishing its
proficiency in discovering meaningful topics. Furthermore, qualitative analysis
showcases PromptTopic's ability to uncover relevant topics in multiple
datasets.",None,-1
4feb4e2f-ce60-4d5d-acbd-8cc250b2c2c4,OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?,0.069188,5,"This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale
dataset for humour generation and understanding. Humour is an abstract,
subjective, and context-dependent cognitive construct involving several
cognitive factors, making it a challenging task to generate and interpret.
Hence, humour generation and understanding can serve as a new task for
evaluating the ability of deep-learning methods to process abstract and
subjective information. Due to the scarcity of data, humour-related generation
tasks such as captioning remain under-explored. To address this gap,
OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to
train a generalizable humour captioning model. Contrary to existing captioning
datasets, OxfordTVG-HIC features a wide range of emotional and semantic
diversity resulting in out-of-context examples that are particularly conducive
to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive
content. We also show how OxfordTVG-HIC can be leveraged for evaluating the
humour of a generated text. Through explainability analysis of the trained
models, we identify the visual and linguistic cues influential for evoking
humour prediction (and generation). We observe qualitatively that these cues
are aligned with the benign violation theory of humour in cognitive psychology.",None,-1
219b41b3-3d64-485b-ab55-dc531b4f8f57,How Fragile is Relation Extraction under Entity Replacements?,0.246303,2,"Relation extraction (RE) aims to extract the relations between entity names
from the textual context. In principle, textual context determines the
ground-truth relation and the RE models should be able to correctly identify
the relations reflected by the textual context. However, existing work has
found that the RE models memorize the entity name patterns to make RE
predictions while ignoring the textual context. This motivates us to raise the
question: ``are RE models robust to the entity replacements?'' In this work, we
operate the random and type-constrained entity replacements over the RE
instances in TACRED and evaluate the state-of-the-art RE models under the
entity replacements. We observe the 30\% - 50\% F1 score drops on the
state-of-the-art RE models under entity replacements. These results suggest
that we need more efforts to develop effective RE models robust to entity
replacements. We release the source code at
https://github.com/wangywUST/RobustRE.",None,-1
276857af-1b29-4894-a81e-84cb306f2aaf,"Philosophical Foundations of GeoAI: Exploring Sustainability, Diversity, and Bias in GeoAI and Spatial Data Science",0.726076,7,"This chapter presents some of the fundamental assumptions and principles that
could form the philosophical foundation of GeoAI and spatial data science.
Instead of reviewing the well-established characteristics of spatial data
(analysis), including interaction, neighborhoods, and autocorrelation, the
chapter highlights themes such as sustainability, bias in training data,
diversity in schema knowledge, and the (potential lack of) neutrality of GeoAI
systems from a unifying ethical perspective. Reflecting on our profession's
ethical implications will assist us in conducting potentially disruptive
research more responsibly, identifying pitfalls in designing, training, and
deploying GeoAI-based systems, and developing a shared understanding of the
benefits but also potential dangers of artificial intelligence and machine
learning research across academic fields, all while sharing our unique
(geo)spatial perspective with others.",None,-1
51fb2d3f-92f7-4aed-8272-4695cc3e2fb1,A Neural Separation Algorithm for the Rounded Capacity Inequalities,0.322748,1,"The cutting plane method is a key technique for successful branch-and-cut and
branch-price-and-cut algorithms that find the exact optimal solutions for
various vehicle routing problems (VRPs). Among various cuts, the rounded
capacity inequalities (RCIs) are the most fundamental. To generate RCIs, we
need to solve the separation problem, whose exact solution takes a long time to
obtain; therefore, heuristic methods are widely used. We design a
learning-based separation heuristic algorithm with graph coarsening that learns
the solutions of the exact separation problem with a graph neural network
(GNN), which is trained with small instances of 50 to 100 customers. We embed
our separation algorithm within the cutting plane method to find a lower bound
for the capacitated VRP (CVRP) with up to 1,000 customers. We compare the
performance of our approach with CVRPSEP, a popular separation software package
for various cuts used in solving VRPs. Our computational results show that our
approach finds better lower bounds than CVRPSEP for large-scale problems with
400 or more customers, while CVRPSEP shows strong competency for problems with
less than 400 customers.",None,-1
755ac1f1-3e40-4882-a016-f38458129faa,The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment Quadruples: A Comparative Analysis,0.289069,1,"Recently, ChatGPT has attracted great attention from both industry and
academia due to its surprising abilities in natural language understanding and
generation. We are particularly curious about whether it can achieve promising
performance on one of the most complex tasks in aspect-based sentiment
analysis, i.e., extracting aspect-category-opinion-sentiment quadruples from
texts. To this end, in this paper we develop a specialized prompt template that
enables ChatGPT to effectively tackle this complex quadruple extraction task.
Further, we propose a selection method on few-shot examples to fully exploit
the in-context learning ability of ChatGPT and uplift its effectiveness on this
complex task. Finally, we provide a comparative evaluation on ChatGPT against
existing state-of-the-art quadruple extraction models based on four public
datasets and highlight some important findings regarding the capability
boundaries of ChatGPT in the quadruple extraction.",None,-1
ea8b9b8f-21b9-4d9b-b8b6-8b16538f0a4d,Hallucination is the last thing you need,0.647517,6,"The legal profession necessitates a multidimensional approach that involves
synthesizing an in-depth comprehension of a legal issue with insightful
commentary based on personal experience, combined with a comprehensive
understanding of pertinent legislation, regulation, and case law, in order to
deliver an informed legal solution. The present offering with generative AI
presents major obstacles in replicating this, as current models struggle to
integrate and navigate such a complex interplay of understanding, experience,
and fact-checking procedures. It is noteworthy that where generative AI outputs
understanding and experience, which reflect the aggregate of various subjective
views on similar topics, this often deflects the model's attention from the
crucial legal facts, thereby resulting in hallucination. Hence, this paper
delves into the feasibility of three independent LLMs, each focused on
understanding, experience, and facts, synthesising as one single ensemble model
to effectively counteract the current challenges posed by the existing
monolithic generative AI models. We introduce an idea of mutli-length
tokenisation to protect key information assets like common law judgements, and
finally we interrogate the most advanced publicly available models for legal
hallucination, with some interesting results.",None,-1
2dddb45d-3fbf-4a0a-aeec-572e2fb7e2b8,RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition,0.542516,4,"Emotion recognition in conversation (ERC) has received increasing attention
from researchers due to its wide range of applications.As conversation has a
natural graph structure,numerous approaches used to model ERC based on graph
convolutional networks (GCNs) have yielded significant results.However,the
aggregation approach of traditional GCNs suffers from the node information
redundancy problem,leading to node discriminant information
loss.Additionally,single-layer GCNs lack the capacity to capture long-range
contextual information from the graph. Furthermore,the majority of approaches
are based on textual modality or stitching together different modalities,
resulting in a weak ability to capture interactions between modalities. To
address these problems, we present the relational bilevel aggregation graph
convolutional network (RBA-GCN), which consists of three modules: the graph
generation module (GGM), similarity-based cluster building module (SCBM) and
bilevel aggregation module (BiAM). First, GGM constructs a novel graph to
reduce the redundancy of target node information.Then,SCBM calculates the node
similarity in the target node and its structural neighborhood, where noisy
information with low similarity is filtered out to preserve the discriminant
information of the node. Meanwhile, BiAM is a novel aggregation method that can
preserve the information of nodes during the aggregation process. This module
can construct the interaction between different modalities and capture
long-range contextual information based on similarity clusters. On both the
IEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a
2.17$\sim$5.21\% improvement over that of the most advanced method.Our code is
available at https://github.com/luftmenscher/RBA-GCN and our article with the
same name has been published in IEEE/ACM Transactions on Audio,Speech,and
Language Processing,vol.31,2023",None,-1
8e0d1e97-d368-49e1-aae0-6e268320bc90,Probability-based Global Cross-modal Upsampling for Pansharpening,0.712391,6,"Pansharpening is an essential preprocessing step for remote sensing image
processing. Although deep learning (DL) approaches performed well on this task,
current upsampling methods used in these approaches only utilize the local
information of each pixel in the low-resolution multispectral (LRMS) image
while neglecting to exploit its global information as well as the cross-modal
information of the guiding panchromatic (PAN) image, which limits their
performance improvement. To address this issue, this paper develops a novel
probability-based global cross-modal upsampling (PGCU) method for
pan-sharpening. Precisely, we first formulate the PGCU method from a
probabilistic perspective and then design an efficient network module to
implement it by fully utilizing the information mentioned above while
simultaneously considering the channel specificity. The PGCU module consists of
three blocks, i.e., information extraction (IE), distribution and expectation
estimation (DEE), and fine adjustment (FA). Extensive experiments verify the
superiority of the PGCU method compared with other popular upsampling methods.
Additionally, experiments also show that the PGCU module can help improve the
performance of existing SOTA deep learning pansharpening methods. The codes are
available at https://github.com/Zeyu-Zhu/PGCU.",None,-1
189a2e30-d59c-4e8a-b6a0-265bf0afff86,Playing the Werewolf game with artificial intelligence for language understanding,0.932801,7,"The Werewolf game is a social deduction game based on free natural language
communication, in which players try to deceive others in order to survive. An
important feature of this game is that a large portion of the conversations are
false information, and the behavior of artificial intelligence (AI) in such a
situation has not been widely investigated. The purpose of this study is to
develop an AI agent that can play Werewolf through natural language
conversations. First, we collected game logs from 15 human players. Next, we
fine-tuned a Transformer-based pretrained language model to construct a value
network that can predict a posterior probability of winning a game at any given
phase of the game and given a candidate for the next action. We then developed
an AI agent that can interact with humans and choose the best voting target on
the basis of its probability from the value network. Lastly, we evaluated the
performance of the agent by having it actually play the game with human
players. We found that our AI agent, Deep Wolf, could play Werewolf as
competitively as average human players in a villager or a betrayer role,
whereas Deep Wolf was inferior to human players in a werewolf or a seer role.
These results suggest that current language models have the capability to
suspect what others are saying, tell a lie, or detect lies in conversations.",None,-1
26e52a8d-6e6e-4d5f-b6c7-da909ec8fb8d,Preference Transformer: Modeling Human Preferences using Transformers for RL,0.790777,29,"Preference-based reinforcement learning (RL) provides a framework to train
agents using human preferences between two behaviors. However, preference-based
RL has been challenging to scale since it requires a large amount of human
feedback to learn a reward function aligned with human intent. In this paper,
we present Preference Transformer, a neural architecture that models human
preferences using transformers. Unlike prior approaches assuming human judgment
is based on the Markovian rewards which contribute to the decision equally, we
introduce a new preference model based on the weighted sum of non-Markovian
rewards. We then design the proposed preference model using a transformer
architecture that stacks causal and bidirectional self-attention layers. We
demonstrate that Preference Transformer can solve a variety of control tasks
using real human preferences, while prior approaches fail to work. We also show
that Preference Transformer can induce a well-specified reward and attend to
critical events in the trajectory by automatically capturing the temporal
dependencies in human decision-making. Code is available on the project
website: https://sites.google.com/view/preference-transformer.",None,-1
b5365a04-9430-4ab8-92c0-bd0ae43ab2b8,"The Ontology for Agents, Systems and Integration of Services: OASIS version 2",0.560412,6,"Semantic representation is a key enabler for several application domains, and
the multi-agent systems realm makes no exception. Among the methods for
semantically representing agents, one has been essentially achieved by taking a
behaviouristic vision, through which one can describe how they operate and
engage with their peers. The approach essentially aims at defining the
operational capabilities of agents through the mental states related with the
achievement of tasks. The OASIS ontology -- An Ontology for Agent, Systems, and
Integration of Services, presented in 2019 -- pursues the behaviouristic
approach to deliver a semantic representation system and a communication
protocol for agents and their commitments. This paper reports on the main
modeling choices concerning the representation of agents in OASIS 2, the latest
major upgrade of OASIS, and the achievement reached by the ontology since it
was first introduced, in particular in the context of ontologies for
blockchains.",None,-1
1dc942bc-2167-4221-b713-6fdc5d3536c2,Facial Affective Behavior Analysis Method for 5th ABAW Competition,0.986902,12,"Facial affective behavior analysis is important for human-computer
interaction. 5th ABAW competition includes three challenges from Aff-Wild2
database. Three common facial affective analysis tasks are involved, i.e.
valence-arousal estimation, expression classification, action unit recognition.
For the three challenges, we construct three different models to solve the
corresponding problems to improve the results, such as data unbalance and data
noise. For the experiments of three challenges, we train the models on the
provided training data and validate the models on the validation data.",None,-1
e047e2e1-5655-41a6-8b96-8b37bafd1f11,Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts,0.50325,4,"Recent studies have demonstrated that natural-language prompts can help to
leverage the knowledge learned by pre-trained language models for the binary
sentence-level sentiment classification task. Specifically, these methods
utilize few-shot learning settings to fine-tune the sentiment classification
model using manual or automatically generated prompts. However, the performance
of these methods is sensitive to the perturbations of the utilized prompts.
Furthermore, these methods depend on a few labeled instances for automatic
prompt generation and prompt ranking. This study aims to find high-quality
prompts for the given task in a zero-shot setting. Given a base prompt, our
proposed approach automatically generates multiple prompts similar to the base
prompt employing positional, reasoning, and paraphrasing techniques and then
ranks the prompts using a novel metric. We empirically demonstrate that the
top-ranked prompts are high-quality and significantly outperform the base
prompt and the prompts generated using few-shot learning for the binary
sentence-level sentiment classification task.",None,-1
4d827d46-820c-4a3a-ac34-e3d3c8ad7a25,Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity,0.736049,41,"A widespread view is that Artificial Intelligence cannot be creative. We
tested this assumption by comparing human-generated ideas with those generated
by six Generative Artificial Intelligence (GAI) chatbots: $alpa.\!ai$,
$Copy.\!ai$, ChatGPT (versions 3 and 4), $Studio.\!ai$, and YouChat. Humans and
a specifically trained AI independently assessed the quality and quantity of
ideas. We found no qualitative difference between AI and human-generated
creativity, although there are differences in how ideas are generated.
Interestingly, 9.4 percent of humans were more creative than the most creative
GAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the
creative process. Continued research and development of GAI in creative tasks
is crucial to fully understand this technology's potential benefits and
drawbacks in shaping the future of creativity. Finally, we discuss the question
of whether GAIs are capable of being truly creative.",None,-1
c2ae0a79-fde6-4dec-a805-14f43a7a25b6,Integrating Large Pre-trained Models into Multimodal Named Entity Recognition with Evidential Fusion,0.411278,2,"Multimodal Named Entity Recognition (MNER) is a crucial task for information
extraction from social media platforms such as Twitter. Most current methods
rely on attention weights to extract information from both text and images but
are often unreliable and lack interpretability. To address this problem, we
propose incorporating uncertainty estimation into the MNER task, producing
trustworthy predictions. Our proposed algorithm models the distribution of each
modality as a Normal-inverse Gamma distribution, and fuses them into a unified
distribution with an evidential fusion mechanism, enabling hierarchical
characterization of uncertainties and promotion of prediction accuracy and
trustworthiness. Additionally, we explore the potential of pre-trained large
foundation models in MNER and propose an efficient fusion approach that
leverages their robust feature representations. Experiments on two datasets
demonstrate that our proposed method outperforms the baselines and achieves new
state-of-the-art performance.",None,-1
46b93eda-3992-4de0-820d-6576a89966b8,Nemo: First Glimpse of a New Rule Engine,0.667434,3,"This system demonstration presents Nemo, a new logic programming engine with
a focus on reliability and performance. Nemo is built for data-centric analytic
computations, modelled in a fully declarative Datalog dialect. Its scalability
for these tasks matches or exceeds that of leading Datalog systems. We
demonstrate uses in reasoning with knowledge graphs and ontologies with 10^5 to
10^8 input facts, all on a laptop. Nemo is written in Rust and available as a
free and open source tool.",None,-1
a39a0063-758e-45ac-bf0b-8b5c95fd5a84,IDA: Informed Domain Adaptive Semantic Segmentation,0.449823,5,"Mixup-based data augmentation has been validated to be a critical stage in
the self-training framework for unsupervised domain adaptive semantic
segmentation (UDA-SS), which aims to transfer knowledge from a well-annotated
(source) domain to an unlabeled (target) domain. Existing self-training methods
usually adopt the popular region-based mixup techniques with a random sampling
strategy, which unfortunately ignores the dynamic evolution of different
semantics across various domains as training proceeds. To improve the UDA-SS
performance, we propose an Informed Domain Adaptation (IDA) model, a
self-training framework that mixes the data based on class-level segmentation
performance, which aims to emphasize small-region semantics during mixup. In
our IDA model, the class-level performance is tracked by an expected confidence
score (ECS). We then use a dynamic schedule to determine the mixing ratio for
data in different domains. Extensive experimental results reveal that our
proposed method is able to outperform the state-of-the-art UDA-SS method by a
margin of 1.1 mIoU in the adaptation of GTA-V to Cityscapes and of 0.9 mIoU in
the adaptation of SYNTHIA to Cityscapes.",None,-1
5c4f2fc3-2eab-42eb-99f3-c703224a6eb4,Regulatory Markets: The Future of AI Governance,0.940501,13,"Appropriately regulating artificial intelligence is an increasingly urgent
policy challenge. Legislatures and regulators lack the specialized knowledge
required to best translate public demands into legal requirements. Overreliance
on industry self-regulation fails to hold producers and users of AI systems
accountable to democratic demands. Regulatory markets, in which governments
require the targets of regulation to purchase regulatory services from a
private regulator, are proposed. This approach to AI regulation could overcome
the limitations of both command-and-control regulation and self-regulation.
Regulatory market could enable governments to establish policy priorities for
the regulation of AI, whilst relying on market forces and industry R&D efforts
to pioneer the methods of regulation that best achieve policymakers' stated
objectives.",None,-1
6e7a6d76-f0a6-496c-a82c-799a2cf65671,GANHead: Towards Generative Animatable Neural Head Avatars,0.180686,5,"To bring digital avatars into people's lives, it is highly demanded to
efficiently generate complete, realistic, and animatable head avatars. This
task is challenging, and it is difficult for existing methods to satisfy all
the requirements at once. To achieve these goals, we propose GANHead
(Generative Animatable Neural Head Avatar), a novel generative head model that
takes advantages of both the fine-grained control over the explicit expression
parameters and the realistic rendering results of implicit representations.
Specifically, GANHead represents coarse geometry, fine-gained details and
texture via three networks in canonical space to obtain the ability to generate
complete and realistic head avatars. To achieve flexible animation, we define
the deformation filed by standard linear blend skinning (LBS), with the learned
continuous pose and expression bases and LBS weights. This allows the avatars
to be directly animated by FLAME parameters and generalize well to unseen poses
and expressions. Compared to state-of-the-art (SOTA) methods, GANHead achieves
superior performance on head avatar generation and raw scan fitting.",None,-1
2725f6bd-18ec-4489-84ca-cc0be8321c1c,Hubs and Hyperspheres: Reducing Hubness and Improving Transductive Few-shot Learning with Hyperspherical Embeddings,0.296712,7,"Distance-based classification is frequently used in transductive few-shot
learning (FSL). However, due to the high-dimensionality of image
representations, FSL classifiers are prone to suffer from the hubness problem,
where a few points (hubs) occur frequently in multiple nearest neighbour lists
of other points. Hubness negatively impacts distance-based classification when
hubs from one class appear often among the nearest neighbors of points from
another class, degrading the classifier's performance. To address the hubness
problem in FSL, we first prove that hubness can be eliminated by distributing
representations uniformly on the hypersphere. We then propose two new
approaches to embed representations on the hypersphere, which we prove optimize
a tradeoff between uniformity and local similarity preservation -- reducing
hubness while retaining class structure. Our experiments show that the proposed
methods reduce hubness, and significantly improves transductive FSL accuracy
for a wide range of classifiers.",None,-1
fc87669a-aafb-4a71-aadc-1f175e24d493,Super-Resolving Face Image by Facial Parsing Information,0.268028,3,"Face super-resolution is a technology that transforms a low-resolution face
image into the corresponding high-resolution one. In this paper, we build a
novel parsing map guided face super-resolution network which extracts the face
prior (i.e., parsing map) directly from low-resolution face image for the
following utilization. To exploit the extracted prior fully, a parsing map
attention fusion block is carefully designed, which can not only effectively
explore the information of parsing map, but also combines powerful attention
mechanism. Moreover, in light of that high-resolution features contain more
precise spatial information while low-resolution features provide strong
contextual information, we hope to maintain and utilize these complementary
information. To achieve this goal, we develop a multi-scale refine block to
maintain spatial and contextual information and take advantage of multi-scale
features to refine the feature representations. Experimental results
demonstrate that our method outperforms the state-of-the-arts in terms of
quantitative metrics and visual quality. The source codes will be available at
https://github.com/wcy-cs/FishFSRNet.",None,-1
6a5d27d1-850a-4947-b481-1eb64204a8c3,FRACAS: A FRench Annotated Corpus of Attribution relations in newS,0.0909096,1,"Quotation extraction is a widely useful task both from a sociological and
from a Natural Language Processing perspective. However, very little data is
available to study this task in languages other than English. In this paper, we
present a manually annotated corpus of 1676 newswire texts in French for
quotation extraction and source attribution. We first describe the composition
of our corpus and the choices that were made in selecting the data. We then
detail the annotation guidelines and annotation process, as well as a few
statistics about the final corpus and the obtained balance between quote types
(direct, indirect and mixed, which are particularly challenging). We end by
detailing our inter-annotator agreement between the 8 annotators who worked on
manual labelling, which is substantially high for such a difficult linguistic
phenomenon.",None,-1
54064476-8b40-413d-ab3c-c67e52c74c19,Experience and Evidence are the eyes of an excellent summarizer! Towards Knowledge Infused Multi-modal Clinical Conversation Summarization,0.229691,1,"With the advancement of telemedicine, both researchers and medical
practitioners are working hand-in-hand to develop various techniques to
automate various medical operations, such as diagnosis report generation. In
this paper, we first present a multi-modal clinical conversation summary
generation task that takes a clinician-patient interaction (both textual and
visual information) and generates a succinct synopsis of the conversation. We
propose a knowledge-infused, multi-modal, multi-tasking medical domain
identification and clinical conversation summary generation
(MM-CliConSummation) framework. It leverages an adapter to infuse knowledge and
visual features and unify the fused feature vector using a gated mechanism.
Furthermore, we developed a multi-modal, multi-intent clinical conversation
summarization corpus annotated with intent, symptom, and summary. The extensive
set of experiments, both quantitatively and qualitatively, led to the following
findings: (a) critical significance of visuals, (b) more precise and medical
entity preserving summary with additional knowledge infusion, and (c) a
correlation between medical department identification and clinical synopsis
generation. Furthermore, the dataset and source code are available at
https://github.com/NLP-RL/MM-CliConSummation.",None,-1
411a5c73-1888-4cd5-86a2-624b09e09196,Polynomial Implicit Neural Representations For Large Diverse Datasets,0.450356,9,"Implicit neural representations (INR) have gained significant popularity for
signal and image representation for many end-tasks, such as superresolution, 3D
modeling, and more. Most INR architectures rely on sinusoidal positional
encoding, which accounts for high-frequency information in data. However, the
finite encoding size restricts the model's representational power. Higher
representational power is needed to go from representing a single given image
to representing large and diverse datasets. Our approach addresses this gap by
representing an image with a polynomial function and eliminates the need for
positional encodings. Therefore, to achieve a progressively higher degree of
polynomial representation, we use element-wise multiplications between features
and affine-transformed coordinate locations after every ReLU layer. The
proposed method is evaluated qualitatively and quantitatively on large datasets
like ImageNet. The proposed Poly-INR model performs comparably to
state-of-the-art generative models without any convolution, normalization, or
self-attention layers, and with far fewer trainable parameters. With much fewer
training parameters and higher representative power, our approach paves the way
for broader adoption of INR models for generative modeling tasks in complex
domains. The code is available at \url{https://github.com/Rajhans0/Poly_INR}",None,-1
8fc96ac1-57ce-43b7-a628-bc46333a74eb,Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors,0.26586,3,"Reasoning on knowledge graphs is a challenging task because it utilizes
observed information to predict the missing one. Particularly, answering
complex queries based on first-order logic is one of the crucial tasks to
verify learning to reason abilities for generalization and composition.
Recently, the prevailing method is query embedding which learns the embedding
of a set of entities and treats logic operations as set operations and has
shown great empirical success. Though there has been much research following
the same formulation, many of its claims lack a formal and systematic
inspection. In this paper, we rethink this formulation and justify many of the
previous claims by characterizing the scope of queries investigated previously
and precisely identifying the gap between its formulation and its goal, as well
as providing complexity analysis for the currently investigated queries.
Moreover, we develop a new dataset containing ten new types of queries with
features that have never been considered and therefore can provide a thorough
investigation of complex queries. Finally, we propose a new neural-symbolic
method, Fuzzy Inference with Truth value (FIT), where we equip the neural link
predictors with fuzzy logic theory to support end-to-end learning using complex
queries with provable reasoning capability. Empirical results show that our
method outperforms previous methods significantly in the new dataset and also
surpasses previous methods in the existing dataset at the same time.",None,-1
602d636e-1c9a-4570-85f7-707379b369b9,BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset,0.709024,145,"In this paper, we introduce the BeaverTails dataset, aimed at fostering
research on safety alignment in large language models (LLMs). This dataset
uniquely separates annotations of helpfulness and harmlessness for
question-answering pairs, thus offering distinct perspectives on these crucial
attributes. In total, we have gathered safety meta-labels for 333,963
question-answer (QA) pairs and 361,903 pairs of expert comparison data for both
the helpfulness and harmlessness metrics. We further showcase applications of
BeaverTails in content moderation and reinforcement learning with human
feedback (RLHF), emphasizing its potential for practical safety measures in
LLMs. We believe this dataset provides vital resources for the community,
contributing towards the safe development and deployment of LLMs. Our project
page is available at the following URL:
https://sites.google.com/view/pku-beavertails.",None,-1
bf773c82-4296-4e2f-9b90-f43167ad1022,Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives,0.999981,185,"Intelligent vehicles (IVs) have gained worldwide attention due to their
increased convenience, safety advantages, and potential commercial value.
Despite predictions of commercial deployment by 2025, implementation remains
limited to small-scale validation, with precise tracking controllers and motion
planners being essential prerequisites for IVs. This paper reviews
state-of-the-art motion planning methods for IVs, including pipeline planning
and end-to-end planning methods. The study examines the selection, expansion,
and optimization operations in a pipeline method, while it investigates
training approaches and validation scenarios for driving tasks in end-to-end
methods. Experimental platforms are reviewed to assist readers in choosing
suitable training and validation strategies. A side-by-side comparison of the
methods is provided to highlight their strengths and limitations, aiding
system-level design choices. Current challenges and future perspectives are
also discussed in this survey.",None,-1
57e2881a-6876-4a92-876c-f554e1c4d4d2,Improving Medical Dialogue Generation with Abstract Meaning Representations,0.618325,9,"Medical Dialogue Generation serves a critical role in telemedicine by
facilitating the dissemination of medical expertise to patients. Existing
studies focus on incorporating textual representations, which have limited
their ability to represent the semantics of text, such as ignoring important
medical entities. To enhance the model's understanding of the textual semantics
and the medical knowledge including entities and relations, we introduce the
use of Abstract Meaning Representations (AMR) to construct graphical
representations that delineate the roles of language constituents and medical
entities within the dialogues. In this paper, We propose a novel framework that
models dialogues between patients and healthcare professionals using AMR
graphs, where the neural networks incorporate textual and graphical knowledge
with a dual attention mechanism. Experimental results show that our framework
outperforms strong baseline models in medical dialogue generation,
demonstrating the effectiveness of AMR graphs in enhancing the representations
of medical knowledge and logical relationships. Furthermore, to support future
research in this domain, we provide the corresponding source code at
https://github.com/Bernard-Yang/MedDiaAMR.",None,-1
d322a4da-7df4-4133-99b0-8d2215eb2158,Classification of Primitive Manufacturing Tasks from Filtered Event Data,0.573349,3,"Collaborative robots are increasingly present in industry to support human
activities. However, to make the human-robot collaborative process more
effective, there are several challenges to be addressed. Collaborative robotic
systems need to be aware of the human activities to (1) anticipate
collaborative/assistive actions, (2) learn by demonstration, and (3) activate
safety procedures in shared workspace. This study proposes an action
classification system to recognize primitive assembly tasks from human motion
events data captured by a Dynamic and Active-pixel Vision Sensor (DAVIS).
Several filters are compared and combined to remove event data noise. Task
patterns are classified from a continuous stream of event data using advanced
deep learning and recurrent networks to classify spatial and temporal features.
Experiments were conducted on a novel dataset, the dataset of manufacturing
tasks (DMT22), featuring 5 classes of representative manufacturing primitives
(PickUp, Place, Screw, Hold, Idle) from 5 participants. Results show that the
proposed filters remove about 65\% of all events (noise) per recording,
conducting to a classification accuracy up to 99,37\% for subjects that trained
the system and 97.08\% for new subjects. Data from a left-handed subject were
successfully classified using only right-handed training data. These results
are object independent.",None,-1
8a8407b6-d7e6-40fc-86c7-d452d47e3392,Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions,0.483054,68,"Training large language models to follow instructions makes them perform
better on a wide range of tasks and generally become more helpful. However, a
perfectly helpful model will follow even the most malicious instructions and
readily generate harmful content. In this paper, we raise concerns over the
safety of models that only emphasize helpfulness, not harmlessness, in their
instruction-tuning. We show that several popular instruction-tuned models are
highly unsafe. Moreover, we show that adding just 3% safety examples (a few
hundred demonstrations) when fine-tuning a model like LLaMA can substantially
improve its safety. Our safety-tuning does not make models significantly less
capable or helpful as measured by standard benchmarks. However, we do find
exaggerated safety behaviours, where too much safety-tuning makes models refuse
perfectly safe prompts if they superficially resemble unsafe ones. As a whole,
our results illustrate trade-offs in training LLMs to be helpful and training
them to be safe.",None,-1
0ada3f56-ac25-40ed-bab6-a6c37ee52838,UUKG: Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal Prediction,0.625422,8,"Accurate Urban SpatioTemporal Prediction (USTP) is of great importance to the
development and operation of the smart city. As an emerging building block,
multi-sourced urban data are usually integrated as urban knowledge graphs
(UrbanKGs) to provide critical knowledge for urban spatiotemporal prediction
models. However, existing UrbanKGs are often tailored for specific downstream
prediction tasks and are not publicly available, which limits the potential
advancement. This paper presents UUKG, the unified urban knowledge graph
dataset for knowledge-enhanced urban spatiotemporal predictions. Specifically,
we first construct UrbanKGs consisting of millions of triplets for two
metropolises by connecting heterogeneous urban entities such as administrative
boroughs, POIs, and road segments. Moreover, we conduct qualitative and
quantitative analysis on constructed UrbanKGs and uncover diverse high-order
structural patterns, such as hierarchies and cycles, that can be leveraged to
benefit downstream USTP tasks. To validate and facilitate the use of UrbanKGs,
we implement and evaluate 15 KG embedding methods on the KG completion task and
integrate the learned KG embeddings into 9 spatiotemporal models for five
different USTP tasks. The extensive experimental results not only provide
benchmarks of knowledge-enhanced USTP models under different task settings but
also highlight the potential of state-of-the-art high-order structure-aware
UrbanKG embedding methods. We hope the proposed UUKG fosters research on urban
knowledge graphs and broad smart city applications. The dataset and source code
are available at https://github.com/usail-hkust/UUKG/.",None,-1
33b1c8e9-9c8d-44b0-a0fc-6789ab711a1b,Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation,0.419153,7,"Recent approaches to empathetic response generation try to incorporate
commonsense knowledge or reasoning about the causes of emotions to better
understand the user's experiences and feelings. However, these approaches
mainly focus on understanding the causalities of context from the user's
perspective, ignoring the system's perspective. In this paper, we propose a
commonsense-based causality explanation approach for diverse empathetic
response generation that considers both the user's perspective (user's desires
and reactions) and the system's perspective (system's intentions and
reactions). We enhance ChatGPT's ability to reason for the system's perspective
by integrating in-context learning with commonsense knowledge. Then, we
integrate the commonsense-based causality explanation with both ChatGPT and a
T5-based model. Experimental evaluations demonstrate that our method
outperforms other comparable methods on both automatic and human evaluations.",None,-1
1a372e6e-eca0-4594-8276-001de39cb736,A computational framework of human values for ethical AI,0.21032,4,"In the diverse array of work investigating the nature of human values from
psychology, philosophy and social sciences, there is a clear consensus that
values guide behaviour. More recently, a recognition that values provide a
means to engineer ethical AI has emerged. Indeed, Stuart Russell proposed
shifting AI's focus away from simply ``intelligence'' towards intelligence
``provably aligned with human values''. This challenge -- the value alignment
problem -- with others including an AI's learning of human values, aggregating
individual values to groups, and designing computational mechanisms to reason
over values, has energised a sustained research effort. Despite this, no
formal, computational definition of values has yet been proposed. We address
this through a formal conceptual framework rooted in the social sciences, that
provides a foundation for the systematic, integrated and interdisciplinary
investigation into how human values can support designing ethical AI.",None,-1
50f1a2e7-80cf-4e93-baa2-2d2e9d4de2a1,Zero-shot Temporal Relation Extraction with ChatGPT,0.999708,48,"The goal of temporal relation extraction is to infer the temporal relation
between two events in the document. Supervised models are dominant in this
task. In this work, we investigate ChatGPT's ability on zero-shot temporal
relation extraction. We designed three different prompt techniques to break
down the task and evaluate ChatGPT. Our experiments show that ChatGPT's
performance has a large gap with that of supervised methods and can heavily
rely on the design of prompts. We further demonstrate that ChatGPT can infer
more small relation classes correctly than supervised methods. The current
shortcomings of ChatGPT on temporal relation extraction are also discussed in
this paper. We found that ChatGPT cannot keep consistency during temporal
inference and it fails in actively long-dependency temporal inference.",None,-1
300d717d-2f32-448b-a811-491886e706f8,PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback,0.714947,46,"Large Language Models for Code (Code LLM) are flourishing. New and powerful
models are released on a weekly basis, demonstrating remarkable performance on
the code generation task. Various approaches have been proposed to boost the
code generation performance of pre-trained Code LLMs, such as supervised
fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we
propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework,
which can effectively and efficiently boost pre-trained large language models
for code generation. Under this framework, we present PanGu-Coder2, which
achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through
an extensive evaluation on CoderEval and LeetCode benchmarks, we show that
PanGu-Coder2 consistently outperforms all previous Code LLMs.",None,-1
2d6b23e5-2e7a-4388-b6a4-356deb699a0b,Unveiling the Potential of Counterfactuals Explanations in Employability,0.402957,1,"In eXplainable Artificial Intelligence (XAI), counterfactual explanations are
known to give simple, short, and comprehensible justifications for complex
model decisions. However, we are yet to see more applied studies in which they
are applied in real-world cases. To fill this gap, this study focuses on
showing how counterfactuals are applied to employability-related problems which
involve complex machine learning algorithms. For these use cases, we use real
data obtained from a public Belgian employment institution (VDAB). The use
cases presented go beyond the mere application of counterfactuals as
explanations, showing how they can enhance decision support, comply with legal
requirements, guide controlled changes, and analyze novel insights.",None,-1
17ff3fad-216d-413a-9278-446ac2374d58,Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation,0.127334,1,"We study the effect of tokenization on gender bias in machine translation, an
aspect that has been largely overlooked in previous works. Specifically, we
focus on the interactions between the frequency of gendered profession names in
training data, their representation in the subword tokenizer's vocabulary, and
gender bias. We observe that female and non-stereotypical gender inflections of
profession names (e.g., Spanish ""doctora"" for ""female doctor"") tend to be split
into multiple subword tokens. Our results indicate that the imbalance of gender
forms in the model's training corpus is a major factor contributing to gender
bias and has a greater impact than subword splitting. We show that analyzing
subword splits provides good estimates of gender-form imbalance in the training
data and can be used even when the corpus is not publicly available. We also
demonstrate that fine-tuning just the token embedding layer can decrease the
gap in gender prediction accuracy between female and male forms without
impairing the translation quality.",None,-1
00b3f47e-e64e-41a0-9022-3ba6b9d788e4,Policy Reuse for Communication Load Balancing in Unseen Traffic Scenarios,0.328185,2,"With the continuous growth in communication network complexity and traffic
volume, communication load balancing solutions are receiving increasing
attention. Specifically, reinforcement learning (RL)-based methods have shown
impressive performance compared with traditional rule-based methods. However,
standard RL methods generally require an enormous amount of data to train, and
generalize poorly to scenarios that are not encountered during training. We
propose a policy reuse framework in which a policy selector chooses the most
suitable pre-trained RL policy to execute based on the current traffic
condition. Our method hinges on a policy bank composed of policies trained on a
diverse set of traffic scenarios. When deploying to an unknown traffic
scenario, we select a policy from the policy bank based on the similarity
between the previous-day traffic of the current scenario and the traffic
observed during training. Experiments demonstrate that this framework can
outperform classical and adaptive rule-based methods by a large margin.",None,-1
43b98696-b0cc-497e-9a9d-e582d9c8e6f3,VDialogUE: A Unified Evaluation Benchmark for Visually-grounded Dialogue,0.240596,1,"Visually-grounded dialog systems, which integrate multiple modes of
communication such as text and visual inputs, have become an increasingly
popular area of investigation. However, the absence of a standardized
evaluation framework poses a challenge in assessing the development of this
field. To this end, we propose \textbf{VDialogUE}, a \textbf{V}isually-grounded
\textbf{Dialog}ue benchmark for \textbf{U}nified \textbf{E}valuation. It
defines five core multi-modal dialogue tasks and covers six datasets.
Furthermore, in order to provide a comprehensive assessment of the model's
performance across all tasks, we developed a novel evaluation metric called
VDscore, which is based on the Analytic Hierarchy Process~(AHP) method.
Additionally, we present a straightforward yet efficient baseline model, named
\textbf{VISIT}~(\textbf{VIS}ually-grounded d\textbf{I}alog
\textbf{T}ransformer), to promote the advancement of general multi-modal
dialogue systems. It progressively builds its multi-modal foundation and
dialogue capability via a two-stage pre-training strategy.
  We believe that the VDialogUE benchmark, along with the evaluation scripts
and our baseline models, will accelerate the development of visually-grounded
dialog systems and lead to the development of more sophisticated and effective
pre-trained models.",None,-1
31354773-393d-4173-8ddc-f898e569d342,Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels,0.0925661,1,"Numerous large-scale chest x-ray datasets have spearheaded expert-level
detection of abnormalities using deep learning. However, these datasets focus
on detecting a subset of disease labels that could be present, thus making them
distributed and non-iid with partial labels. Recent literature has indicated
the impact of batch normalization layers on the convergence of federated
learning due to domain shift associated with non-iid data with partial labels.
To that end, we propose FedFBN, a federated learning framework that draws
inspiration from transfer learning by using pretrained networks as the model
backend and freezing the batch normalization layers throughout the training
process. We evaluate FedFBN with current FL strategies using synthetic iid toy
datasets and large-scale non-iid datasets across scenarios with partial and
complete labels. Our results demonstrate that FedFBN outperforms current
aggregation strategies for training global models using distributed and non-iid
data with partial labels.",None,-1
77b94ae2-4d64-420d-9df4-140f3440ac5d,Consistency Regularization for Generalizable Source-free Domain Adaptation,0.787252,7,"Source-free domain adaptation (SFDA) aims to adapt a well-trained source
model to an unlabelled target domain without accessing the source dataset,
making it applicable in a variety of real-world scenarios. Existing SFDA
methods ONLY assess their adapted models on the target training set, neglecting
the data from unseen but identically distributed testing sets. This oversight
leads to overfitting issues and constrains the model's generalization ability.
In this paper, we propose a consistency regularization framework to develop a
more generalizable SFDA method, which simultaneously boosts model performance
on both target training and testing datasets. Our method leverages soft
pseudo-labels generated from weakly augmented images to supervise strongly
augmented images, facilitating the model training process and enhancing the
generalization ability of the adapted model. To leverage more potentially
useful supervision, we present a sampling-based pseudo-label selection
strategy, taking samples with severer domain shift into consideration.
Moreover, global-oriented calibration methods are introduced to exploit global
class distribution and feature cluster information, further improving the
adaptation process. Extensive experiments demonstrate our method achieves
state-of-the-art performance on several SFDA benchmarks, and exhibits
robustness on unseen testing datasets.",None,-1
c2bbf860-6e0f-45c5-95a0-0c6567d840a7,K-ESConv: Knowledge Injection for Emotional Support Dialogue Systems via Prompt Learning,0.611104,1,"Automatic psychological counseling requires mass of professional knowledge
that can be found in online counseling forums. Motivated by this, we propose
K-ESConv, a novel prompt learning based knowledge injection method for
emotional support dialogue system, transferring forum knowledge to response
generation. We evaluate our model on an emotional support dataset ESConv, where
the model retrieves and incorporates knowledge from external professional
emotional Q\&A forum. Experiment results show that the proposed method
outperforms existing baselines on both automatic evaluation and human
evaluation, which shows that our approach significantly improves the
correlation and diversity of responses and provides more comfort and better
suggestion for the seeker.",None,-1
ed9be15b-450f-4bad-b6ff-f60b98cf6829,Exploiting Contextual Structure to Generate Useful Auxiliary Tasks,0.0573759,2,"Reinforcement learning requires interaction with an environment, which is
expensive for robots. This constraint necessitates approaches that work with
limited environmental interaction by maximizing the reuse of previous
experiences. We propose an approach that maximizes experience reuse while
learning to solve a given task by generating and simultaneously learning useful
auxiliary tasks. To generate these tasks, we construct an abstract temporal
logic representation of the given task and leverage large language models to
generate context-aware object embeddings that facilitate object replacements.
Counterfactual reasoning and off-policy methods allow us to simultaneously
learn these auxiliary tasks while solving the given target task. We combine
these insights into a novel framework for multitask reinforcement learning and
experimentally show that our generated auxiliary tasks share similar underlying
exploration requirements as the given task, thereby maximizing the utility of
directed exploration. Our approach allows agents to automatically learn
additional useful policies without extra environment interaction.",None,-1
0d31f764-9559-4372-b8b4-02ae09295b64,Anytime Approximate Formal Feature Attribution,0.631019,2,"Widespread use of artificial intelligence (AI) algorithms and machine
learning (ML) models on the one hand and a number of crucial issues pertaining
to them warrant the need for explainable artificial intelligence (XAI). A key
explainability question is: given this decision was made, what are the input
features which contributed to the decision? Although a range of XAI approaches
exist to tackle this problem, most of them have significant limitations.
Heuristic XAI approaches suffer from the lack of quality guarantees, and often
try to approximate Shapley values, which is not the same as explaining which
features contribute to a decision. A recent alternative is so-called formal
feature attribution (FFA), which defines feature importance as the fraction of
formal abductive explanations (AXp's) containing the given feature. This
measures feature importance from the view of formally reasoning about the
model's behavior. It is challenging to compute FFA using its definition because
that involves counting AXp's, although one can approximate it. Based on these
results, this paper makes several contributions. First, it gives compelling
evidence that computing FFA is intractable, even if the set of contrastive
formal explanations (CXp's) is provided, by proving that the problem is
#P-hard. Second, by using the duality between AXp's and CXp's, it proposes an
efficient heuristic to switch from CXp enumeration to AXp enumeration
on-the-fly resulting in an adaptive explanation enumeration algorithm
effectively approximating FFA in an anytime fashion. Finally, experimental
results obtained on a range of widely used datasets demonstrate the
effectiveness of the proposed FFA approximation approach in terms of the error
of FFA approximation as well as the number of explanations computed and their
diversity given a fixed time limit.",None,-1
dafb59b8-e8a4-42c0-bce2-4e8094c954f6,Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation,0.563514,11,"Previous studies have shown that leveraging domain index can significantly
boost domain adaptation performance (arXiv:2007.01807, arXiv:2202.03628).
However, such domain indices are not always available. To address this
challenge, we first provide a formal definition of domain index from the
probabilistic perspective, and then propose an adversarial variational Bayesian
framework that infers domain indices from multi-domain data, thereby providing
additional insight on domain relations and improving domain adaptation
performance. Our theoretical analysis shows that our adversarial variational
Bayesian framework finds the optimal domain index at equilibrium. Empirical
results on both synthetic and real data verify that our model can produce
interpretable domain indices which enable us to achieve superior performance
compared to state-of-the-art domain adaptation methods. Code is available at
https://github.com/Wang-ML-Lab/VDI.",None,-1
4fe2e60b-c59a-4575-b534-1442a81b11d1,An Extended Sequence Tagging Vocabulary for Grammatical Error Correction,0.381122,3,"We extend a current sequence-tagging approach to Grammatical Error Correction
(GEC) by introducing specialised tags for spelling correction and morphological
inflection using the SymSpell and LemmInflect algorithms. Our approach improves
generalisation: the proposed new tagset allows a smaller number of tags to
correct a larger range of errors. Our results show a performance improvement
both overall and in the targeted error categories. We further show that
ensembles trained with our new tagset outperform those trained with the
baseline tagset on the public BEA benchmark.",None,-1
cabcab32-5395-44c4-9b9b-91af57cb1786,Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception,0.827748,21,"Multi-agent collaborative perception as a potential application for
vehicle-to-everything communication could significantly improve the perception
performance of autonomous vehicles over single-agent perception. However,
several challenges remain in achieving pragmatic information sharing in this
emerging research. In this paper, we propose SCOPE, a novel collaborative
perception framework that aggregates the spatio-temporal awareness
characteristics across on-road agents in an end-to-end manner. Specifically,
SCOPE has three distinct strengths: i) it considers effective semantic cues of
the temporal context to enhance current representations of the target agent;
ii) it aggregates perceptually critical spatial information from heterogeneous
agents and overcomes localization errors via multi-scale feature interactions;
iii) it integrates multi-source representations of the target agent based on
their complementary contributions by an adaptive fusion paradigm. To thoroughly
evaluate SCOPE, we consider both real-world and simulated scenarios of
collaborative 3D object detection tasks on three datasets. Extensive
experiments demonstrate the superiority of our approach and the necessity of
the proposed components.",None,-1
51ee54de-d753-4d36-8f59-14d3bed669b1,Preference Ranking Optimization for Human Alignment,0.995773,120,"Large language models (LLMs) often contain misleading content, emphasizing
the need to align them with human values to ensure secure AI systems.
Reinforcement learning from human feedback (RLHF) has been employed to achieve
this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits
complexity, instability, and sensitivity to hyperparameters in contrast to SFT.
(2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise
contrast, thus lacking contrasts from a macro perspective. In this paper, we
propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to
directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast
to accommodate preference rankings of any length. By iteratively contrasting
candidates, PRO instructs the LLM to prioritize the best response while
progressively ranking the rest responses. In this manner, PRO effectively
transforms human alignment into aligning the probability ranking of n responses
generated by LLM with the preference ranking of humans towards these responses.
Experiments have shown that PRO outperforms baseline algorithms, achieving
comparable results to ChatGPT and human responses through automatic-based,
reward-based, GPT-4, and human evaluations.",None,-1
a39d283b-641a-4c58-9b05-9b380a543b12,Hypernetworks build Implicit Neural Representations of Sounds,0.384282,3,"Implicit Neural Representations (INRs) are nowadays used to represent
multimedia signals across various real-life applications, including image
super-resolution, image compression, or 3D rendering. Existing methods that
leverage INRs are predominantly focused on visual data, as their application to
other modalities, such as audio, is nontrivial due to the inductive biases
present in architectural attributes of image-based INR models. To address this
limitation, we introduce HyperSound, the first meta-learning approach to
produce INRs for audio samples that leverages hypernetworks to generalize
beyond samples observed in training. Our approach reconstructs audio samples
with quality comparable to other state-of-the-art models and provides a viable
alternative to contemporary sound representations used in deep neural networks
for audio processing, such as spectrograms.",None,-1
b732dcdd-0c5a-4c94-8813-50e113c6bdec,Transformer-based model for monocular visual odometry: a video understanding approach,0.717169,3,"Estimating the camera's pose given images of a single camera is a traditional
task in mobile robots and autonomous vehicles. This problem is called monocular
visual odometry and it often relies on geometric approaches that require
considerable engineering effort for a specific scenario. Deep learning methods
have shown to be generalizable after proper training and a large amount of
available data. Transformer-based architectures have dominated the
state-of-the-art in natural language processing and computer vision tasks, such
as image and video understanding. In this work, we deal with the monocular
visual odometry as a video understanding task to estimate the 6-DoF camera's
pose. We contribute by presenting the TSformer-VO model based on
spatio-temporal self-attention mechanisms to extract features from clips and
estimate the motions in an end-to-end manner. Our approach achieved competitive
state-of-the-art performance compared with geometry-based and deep
learning-based methods on the KITTI visual odometry dataset, outperforming the
DeepVO implementation highly accepted in the visual odometry community.",None,-1
0fd0dd9f-2231-4944-85fc-6846c67f4e02,Accurate Use of Label Dependency in Multi-Label Text Classification Through the Lens of Causality,0.276655,3,"Multi-Label Text Classification (MLTC) aims to assign the most relevant
labels to each given text. Existing methods demonstrate that label dependency
can help to improve the model's performance. However, the introduction of label
dependency may cause the model to suffer from unwanted prediction bias. In this
study, we attribute the bias to the model's misuse of label dependency, i.e.,
the model tends to utilize the correlation shortcut in label dependency rather
than fusing text information and label dependency for prediction. Motivated by
causal inference, we propose a CounterFactual Text Classifier (CFTC) to
eliminate the correlation bias, and make causality-based predictions.
Specifically, our CFTC first adopts the predict-then-modify backbone to extract
precise label information embedded in label dependency, then blocks the
correlation shortcut through the counterfactual de-bias technique with the help
of the human causal graph. Experimental results on three datasets demonstrate
that our CFTC significantly outperforms the baselines and effectively
eliminates the correlation bias in datasets.",None,-1
03230859-7d90-48d7-ab3d-218f18a8a0b8,Thermal Spread Functions (TSF): Physics-guided Material Classification,0.596666,3,"Robust and non-destructive material classification is a challenging but
crucial first-step in numerous vision applications. We propose a physics-guided
material classification framework that relies on thermal properties of the
object. Our key observation is that the rate of heating and cooling of an
object depends on the unique intrinsic properties of the material, namely the
emissivity and diffusivity. We leverage this observation by gently heating the
objects in the scene with a low-power laser for a fixed duration and then
turning it off, while a thermal camera captures measurements during the heating
and cooling process. We then take this spatial and temporal ""thermal spread
function"" (TSF) to solve an inverse heat equation using the finite-differences
approach, resulting in a spatially varying estimate of diffusivity and
emissivity. These tuples are then used to train a classifier that produces a
fine-grained material label at each spatial pixel. Our approach is extremely
simple requiring only a small light source (low power laser) and a thermal
camera, and produces robust classification results with 86% accuracy over 16
classes.",None,-1
be8d77d4-ad4d-4c02-940f-e9d9ad675a9a,Computer Vision for Construction Progress Monitoring: A Real-Time Object Detection Approach,0.442304,2,"Construction progress monitoring (CPM) is essential for effective project
management, ensuring on-time and on-budget delivery. Traditional CPM methods
often rely on manual inspection and reporting, which are time-consuming and
prone to errors. This paper proposes a novel approach for automated CPM using
state-of-the-art object detection algorithms. The proposed method leverages
e.g. YOLOv8's real-time capabilities and high accuracy to identify and track
construction elements within site images and videos. A dataset was created,
consisting of various building elements and annotated with relevant objects for
training and validation. The performance of the proposed approach was evaluated
using standard metrics, such as precision, recall, and F1-score, demonstrating
significant improvement over existing methods. The integration of Computer
Vision into CPM provides stakeholders with reliable, efficient, and
cost-effective means to monitor project progress, facilitating timely
decision-making and ultimately contributing to the successful completion of
construction projects.",None,-1
59bd1ecf-ce64-4214-932b-a1e91f05988f,STEPS: A Benchmark for Order Reasoning in Sequential Tasks,0.0236742,1,"Various human activities can be abstracted into a sequence of actions in
natural text, i.e. cooking, repairing, manufacturing, etc. Such action
sequences heavily depend on the executing order, while disorder in action
sequences leads to failure of further task execution by robots or AI agents.
Therefore, to verify the order reasoning capability of current neural models in
sequential tasks, we propose a challenging benchmark , named STEPS. STEPS
involves two subtask settings, focusing on determining the rationality of given
next step in recipes and selecting the reasonable step from the multi-choice
question, respectively. We describe the data construction and task
formulations, and benchmark most of significant Large Language Models (LLMs).
The experimental results demonstrate 1) The commonsense reasoning of action
orders in sequential tasks are challenging to resolve via zero-shot prompting
or few-shot in-context learning for LLMs; 2) Prompting method still
significantly lags behind tuning-based method on STEPS.",None,-1
9dd3315c-7a43-49c6-9fd4-81b5896c1095,Hypothesis Testing and Machine Learning: Interpreting Variable Effects in Deep Artificial Neural Networks using Cohen's f2,0.108701,4,"Deep artificial neural networks show high predictive performance in many
fields, but they do not afford statistical inferences and their black-box
operations are too complicated for humans to comprehend. Because positing that
a relationship exists is often more important than prediction in scientific
experiments and research models, machine learning is far less frequently used
than inferential statistics. Additionally, statistics calls for improving the
test of theory by showing the magnitude of the phenomena being studied. This
article extends current XAI methods and develops a model agnostic hypothesis
testing framework for machine learning. First, Fisher's variable permutation
algorithm is tweaked to compute an effect size measure equivalent to Cohen's f2
for OLS regression models. Second, the Mann-Kendall test of monotonicity and
the Theil-Sen estimator is applied to Apley's accumulated local effect plots to
specify a variable's direction of influence and statistical significance. The
usefulness of this approach is demonstrated on an artificial data set and a
social survey with a Python sandbox implementation.",None,-1
f41118f2-0bd5-41ac-8252-f2cdb8af6a53,Patched Line Segment Learning for Vector Road Mapping,0.454504,1,"This paper presents a novel approach to computing vector road maps from
satellite remotely sensed images, building upon a well-defined Patched Line
Segment (PaLiS) representation for road graphs that holds geometric
significance. Unlike prevailing methods that derive road vector representations
from satellite images using binary masks or keypoints, our method employs line
segments. These segments not only convey road locations but also capture their
orientations, making them a robust choice for representation. More precisely,
given an input image, we divide it into non-overlapping patches and predict a
suitable line segment within each patch. This strategy enables us to capture
spatial and structural cues from these patch-based line segments, simplifying
the process of constructing the road network graph without the necessity of
additional neural networks for connectivity. In our experiments, we demonstrate
how an effective representation of a road graph significantly enhances the
performance of vector road mapping on established benchmarks, without requiring
extensive modifications to the neural network architecture. Furthermore, our
method achieves state-of-the-art performance with just 6 GPU hours of training,
leading to a substantial 32-fold reduction in training costs in terms of GPU
hours.",None,-1
2af95135-4570-4e87-a4da-63195519f53a,CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion,0.825314,21,"This paper proposes a novel diffusion-based model, CompoDiff, for solving
zero-shot Composed Image Retrieval (ZS-CIR) with latent diffusion. This paper
also introduces a new synthetic dataset, named SynthTriplets18M, with 18.8
million reference images, conditions, and corresponding target image triplets
to train CIR models. CompoDiff and SynthTriplets18M tackle the shortages of the
previous CIR approaches, such as poor generalizability due to the small dataset
scale and the limited types of conditions. CompoDiff not only achieves a new
state-of-the-art on four ZS-CIR benchmarks, including FashionIQ, CIRR, CIRCO,
and GeneCIS, but also enables a more versatile and controllable CIR by
accepting various conditions, such as negative text, and image mask conditions.
CompoDiff also shows the controllability of the condition strength between text
and image queries and the trade-off between inference speed and performance,
which are unavailable with existing CIR methods. The code and dataset are
available at https://github.com/navervision/CompoDiff",None,-1
b40e089b-fffb-441f-93f7-49b40ed5f10a,DUMB: A Benchmark for Smart Evaluation of Dutch Models,0.100275,4,"We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a
diverse set of datasets for low-, medium- and high-resource tasks. The total
set of nine tasks includes four tasks that were previously not available in
Dutch. Instead of relying on a mean score across tasks, we propose Relative
Error Reduction (RER), which compares the DUMB performance of language models
to a strong baseline which can be referred to in the future even when assessing
different sets of language models. Through a comparison of 14 pre-trained
language models (mono- and multi-lingual, of varying sizes), we assess the
internal consistency of the benchmark tasks, as well as the factors that likely
enable high performance. Our results indicate that current Dutch monolingual
models under-perform and suggest training larger Dutch models with other
architectures and pre-training objectives. At present, the highest performance
is achieved by DeBERTaV3 (large), XLM-R (large) and mDeBERTaV3 (base). In
addition to highlighting best strategies for training larger Dutch models, DUMB
will foster further research on Dutch. A public leaderboard is available at
https://dumbench.nl.",None,-1
10211934-c266-4a85-a702-71e4a25e9585,Delayed Feedback in Kernel Bandits,0.282992,3,"Black box optimisation of an unknown function from expensive and noisy
evaluations is a ubiquitous problem in machine learning, academic research and
industrial production. An abstraction of the problem can be formulated as a
kernel based bandit problem (also known as Bayesian optimisation), where a
learner aims at optimising a kernelized function through sequential noisy
observations. The existing work predominantly assumes feedback is immediately
available; an assumption which fails in many real world situations, including
recommendation systems, clinical trials and hyperparameter tuning. We consider
a kernel bandit problem under stochastically delayed feedback, and propose an
algorithm with $\tilde{\mathcal{O}}(\sqrt{\Gamma_k(T)T}+\mathbb{E}[\tau])$
regret, where $T$ is the number of time steps, $\Gamma_k(T)$ is the maximum
information gain of the kernel with $T$ observations, and $\tau$ is the delay
random variable. This represents a significant improvement over the state of
the art regret bound of
$\tilde{\mathcal{O}}(\Gamma_k(T)\sqrt{T}+\mathbb{E}[\tau]\Gamma_k(T))$ reported
in Verma et al. (2022). In particular, for very non-smooth kernels, the
information gain grows almost linearly in time, trivializing the existing
results. We also validate our theoretical results with simulations.",None,-1
6f7b5fcf-8c5d-4faf-b46c-04ce30413a3c,I2SRM: Intra- and Inter-Sample Relationship Modeling for Multimodal Information Extraction,0.78707,2,"Multimodal information extraction is attracting research attention nowadays,
which requires aggregating representations from different modalities. In this
paper, we present the Intra- and Inter-Sample Relationship Modeling (I2SRM)
method for this task, which contains two modules. Firstly, the intra-sample
relationship modeling module operates on a single sample and aims to learn
effective representations. Embeddings from textual and visual modalities are
shifted to bridge the modality gap caused by distinct pre-trained language and
image models. Secondly, the inter-sample relationship modeling module considers
relationships among multiple samples and focuses on capturing the interactions.
An AttnMixup strategy is proposed, which not only enables collaboration among
samples but also augments data to improve generalization. We conduct extensive
experiments on the multimodal named entity recognition datasets Twitter-2015
and Twitter-2017, and the multimodal relation extraction dataset MNRE. Our
proposed method I2SRM achieves competitive results, 77.12% F1-score on
Twitter-2015, 88.40% F1-score on Twitter-2017, and 84.12% F1-score on MNRE.",None,-1
ec0f760f-1d2e-48e4-9055-3432a09acae2,Theta sequences as eligibility traces: a biological solution to credit assignment,0.196686,1,"Credit assignment problems, for example policy evaluation in RL, often
require bootstrapping prediction errors through preceding states \textit{or}
maintaining temporally extended memory traces; solutions which are unfavourable
or implausible for biological networks of neurons. We propose theta sequences
-- chains of neural activity during theta oscillations in the hippocampus,
thought to represent rapid playthroughs of awake behaviour -- as a solution. By
analysing and simulating a model for theta sequences we show they compress
behaviour such that existing but short $\mathsf{O}(10)$ ms neuronal memory
traces are effectively extended allowing for bootstrap-free credit assignment
without long memory traces, equivalent to the use of eligibility traces in
TD($\lambda$).",None,-1
86182eb8-5364-4321-8de7-497ad438596a,Tracking without Label: Unsupervised Multiple Object Tracking via Contrastive Similarity Learning,0.221418,2,"Unsupervised learning is a challenging task due to the lack of labels.
Multiple Object Tracking (MOT), which inevitably suffers from mutual object
interference, occlusion, etc., is even more difficult without label
supervision. In this paper, we explore the latent consistency of sample
features across video frames and propose an Unsupervised Contrastive Similarity
Learning method, named UCSL, including three contrast modules: self-contrast,
cross-contrast, and ambiguity contrast. Specifically, i) self-contrast uses
intra-frame direct and inter-frame indirect contrast to obtain discriminative
representations by maximizing self-similarity. ii) Cross-contrast aligns cross-
and continuous-frame matching results, mitigating the persistent negative
effect caused by object occlusion. And iii) ambiguity contrast matches
ambiguous objects with each other to further increase the certainty of
subsequent object association through an implicit manner. On existing
benchmarks, our method outperforms the existing unsupervised methods using only
limited help from ReID head, and even provides higher accuracy than lots of
fully supervised methods.",None,-1
44b6f04a-e9bb-4741-bed2-9e95ddda2bba,Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers,0.378468,8,"This paper introduces Content-aware Token Sharing (CTS), a token reduction
approach that improves the computational efficiency of semantic segmentation
networks that use Vision Transformers (ViTs). Existing works have proposed
token reduction approaches to improve the efficiency of ViT-based image
classification networks, but these methods are not directly applicable to
semantic segmentation, which we address in this work. We observe that, for
semantic segmentation, multiple image patches can share a token if they contain
the same semantic class, as they contain redundant information. Our approach
leverages this by employing an efficient, class-agnostic policy network that
predicts if image patches contain the same semantic class, and lets them share
a token if they do. With experiments, we explore the critical design choices of
CTS and show its effectiveness on the ADE20K, Pascal Context and Cityscapes
datasets, various ViT backbones, and different segmentation decoders. With
Content-aware Token Sharing, we are able to reduce the number of processed
tokens by up to 44%, without diminishing the segmentation quality.",None,-1
0d7528e5-88e9-4903-ac0f-16b329e178af,Unsupervised Cross-domain Pulmonary Nodule Detection without Source Data,0.332567,1,"Cross domain pulmonary nodule detection suffers from performance degradation
due to large shift of data distributions between the source and target domain.
Besides, considering the high cost of medical data annotation, it is often
assumed that the target images are unlabeled. Existing approaches have made
much progress for this unsupervised domain adaptation setting. However, this
setting is still rarely plausible in the medical application since the source
medical data are often not accessible due to the privacy concerns. This
motivates us to propose a Source-free Unsupervised cross-domain method for
Pulmonary nodule detection (SUP). It first adapts the source model to the
target domain by utilizing instance-level contrastive learning. Then the
adapted model is trained in a teacher-student interaction manner, and a
weighted entropy loss is incorporated to further improve the accuracy.
Extensive experiments by adapting a pre-trained source model to three popular
pulmonary nodule datasets demonstrate the effectiveness of our method.",None,-1
040554ef-0438-4a50-b1e8-56d93ec3022d,Pedestrian detection with high-resolution event camera,0.383415,1,"Despite the dynamic development of computer vision algorithms, the
implementation of perception and control systems for autonomous vehicles such
as drones and self-driving cars still poses many challenges. A video stream
captured by traditional cameras is often prone to problems such as motion blur
or degraded image quality due to challenging lighting conditions. In addition,
the frame rate - typically 30 or 60 frames per second - can be a limiting
factor in certain scenarios. Event cameras (DVS -- Dynamic Vision Sensor) are a
potentially interesting technology to address the above mentioned problems. In
this paper, we compare two methods of processing event data by means of deep
learning for the task of pedestrian detection. We used a representation in the
form of video frames, convolutional neural networks and asynchronous sparse
convolutional neural networks. The results obtained illustrate the potential of
event cameras and allow the evaluation of the accuracy and efficiency of the
methods used for high-resolution (1280 x 720 pixels) footage.",None,-1
27c34d7b-a7b2-4013-860d-74b6890607fd,What does CLIP know about a red circle? Visual prompt engineering for VLMs,0.723542,58,"Large-scale Vision-Language Models, such as CLIP, learn powerful image-text
representations that have found numerous applications, from zero-shot
classification to text-to-image generation. Despite that, their capabilities
for solving novel discriminative tasks via prompting fall behind those of large
language models, such as GPT-3. Here we explore the idea of visual prompt
engineering for solving computer vision tasks beyond classification by editing
in image space instead of text. In particular, we discover an emergent ability
of CLIP, where, by simply drawing a red circle around an object, we can direct
the model's attention to that region, while also maintaining global
information. We show the power of this simple approach by achieving
state-of-the-art in zero-shot referring expressions comprehension and strong
performance in keypoint localization tasks. Finally, we draw attention to some
potential ethical concerns of large language-vision models.",None,-1
13d7ffb3-5163-4378-87b2-6398151a0312,MBPTrack: Improving 3D Point Cloud Tracking with Memory Networks and Box Priors,0.470477,8,"3D single object tracking has been a crucial problem for decades with
numerous applications such as autonomous driving. Despite its wide-ranging use,
this task remains challenging due to the significant appearance variation
caused by occlusion and size differences among tracked targets. To address
these issues, we present MBPTrack, which adopts a Memory mechanism to utilize
past information and formulates localization in a coarse-to-fine scheme using
Box Priors given in the first frame. Specifically, past frames with targetness
masks serve as an external memory, and a transformer-based module propagates
tracked target cues from the memory to the current frame. To precisely localize
objects of all sizes, MBPTrack first predicts the target center via Hough
voting. By leveraging box priors given in the first frame, we adaptively sample
reference points around the target center that roughly cover the target of
different sizes. Then, we obtain dense feature maps by aggregating point
features into the reference points, where localization can be performed more
effectively. Extensive experiments demonstrate that MBPTrack achieves
state-of-the-art performance on KITTI, nuScenes and Waymo Open Dataset, while
running at 50 FPS on a single RTX3090 GPU.",None,-1
096d9f96-d376-474c-bc11-0597f620adbb,HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation,0.841917,8,"Panoptic Scene Graph generation (PSG) is a recently proposed task in image
scene understanding that aims to segment the image and extract triplets of
subjects, objects and their relations to build a scene graph. This task is
particularly challenging for two reasons. First, it suffers from a long-tail
problem in its relation categories, making naive biased methods more inclined
to high-frequency relations. Existing unbiased methods tackle the long-tail
problem by data/loss rebalancing to favor low-frequency relations. Second, a
subject-object pair can have two or more semantically overlapping relations.
While existing methods favor one over the other, our proposed HiLo framework
lets different network branches specialize on low and high frequency relations,
enforce their consistency and fuse the results. To the best of our knowledge we
are the first to propose an explicitly unbiased PSG method. In extensive
experiments we show that our HiLo framework achieves state-of-the-art results
on the PSG task. We also apply our method to the Scene Graph Generation task
that predicts boxes instead of masks and see improvements over all baseline
methods. Code is available at https://github.com/franciszzj/HiLo.",None,-1
602358fb-2e02-4517-aa62-e42cd59f3ffe,Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Visual Bootstrapping,0.556607,2,"This paper presents a novel visual-LiDAR odometry and mapping method with
low-drift characteristics. The proposed method is based on two popular
approaches, ORB-SLAM and A-LOAM, with monocular scale correction and
visual-bootstrapped LiDAR poses initialization modifications. The scale
corrector calculates the proportion between the depth of image keypoints
recovered by triangulation and that provided by LiDAR, using an outlier
rejection process for accuracy improvement. Concerning LiDAR poses
initialization, the visual odometry approach gives the initial guesses of LiDAR
motions for better performance. This methodology is not only applicable to
high-resolution LiDAR but can also adapt to low-resolution LiDAR. To evaluate
the proposed SLAM system's robustness and accuracy, we conducted experiments on
the KITTI Odometry and S3E datasets. Experimental results illustrate that our
method significantly outperforms standalone ORB-SLAM2 and A-LOAM. Furthermore,
regarding the accuracy of visual odometry with scale correction, our method
performs similarly to the stereo-mode ORB-SLAM2.",None,-1
5794df41-2a33-466c-90ac-5ac674ac7e4e,Fast Diffusion EM: a diffusion model for blind inverse problems with application to deconvolution,0.919068,8,"Using diffusion models to solve inverse problems is a growing field of
research. Current methods assume the degradation to be known and provide
impressive results in terms of restoration quality and diversity. In this work,
we leverage the efficiency of those models to jointly estimate the restored
image and unknown parameters of the degradation model such as blur kernel. In
particular, we designed an algorithm based on the well-known
Expectation-Minimization (EM) estimation method and diffusion models. Our
method alternates between approximating the expected log-likelihood of the
inverse problem using samples drawn from a diffusion model and a maximization
step to estimate unknown model parameters. For the maximization step, we also
introduce a novel blur kernel regularization based on a Plug \& Play denoiser.
Diffusion models are long to run, thus we provide a fast version of our
algorithm. Extensive experiments on blind image deblurring demonstrate the
effectiveness of our method when compared to other state-of-the-art approaches.",None,-1
c9f89b66-0dc8-42cd-83f4-0a515e0a6574,MVKT-ECG: Efficient Single-lead ECG Classification on Multi-Label Arrhythmia by Multi-View Knowledge Transferring,0.655995,4,"The widespread emergence of smart devices for ECG has sparked demand for
intelligent single-lead ECG-based diagnostic systems. However, it is
challenging to develop a single-lead-based ECG interpretation model for
multiple diseases diagnosis due to the lack of some key disease information. In
this work, we propose inter-lead Multi-View Knowledge Transferring of ECG
(MVKT-ECG) to boost single-lead ECG's ability for multi-label disease
diagnosis. This training strategy can transfer superior disease knowledge from
multiple different views of ECG (e.g. 12-lead ECG) to single-lead-based ECG
interpretation model to mine details in single-lead ECG signals that are easily
overlooked by neural networks. MVKT-ECG allows this lead variety as a
supervision signal within a teacher-student paradigm, where the teacher
observes multi-lead ECG educates a student who observes only single-lead ECG.
Since the mutual disease information between the single-lead ECG and muli-lead
ECG plays a key role in knowledge transferring, we present a new disease-aware
Contrastive Lead-information Transferring(CLT) to improve the mutual disease
information between the single-lead ECG and muli-lead ECG. Moreover, We modify
traditional Knowledge Distillation to multi-label disease Knowledge
Distillation (MKD) to make it applicable for multi-label disease diagnosis. The
comprehensive experiments verify that MVKT-ECG has an excellent performance in
improving the diagnostic effect of single-lead ECG.",None,-1
7dd16155-b22e-4ede-be01-c8560b6f2f07,GROOT: Learning to Follow Instructions by Watching Gameplay Videos,0.57947,12,"We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. The project page is
available at https://craftjarvis-groot.github.io.",None,-1
2dabf96a-bc2f-4973-a839-2496d2973394,Video Waterdrop Removal via Spatio-Temporal Fusion in Driving Scenes,0.524935,2,"The waterdrops on windshields during driving can cause severe visual
obstructions, which may lead to car accidents. Meanwhile, the waterdrops can
also degrade the performance of a computer vision system in autonomous driving.
To address these issues, we propose an attention-based framework that fuses the
spatio-temporal representations from multiple frames to restore visual
information occluded by waterdrops. Due to the lack of training data for video
waterdrop removal, we propose a large-scale synthetic dataset with simulated
waterdrops in complex driving scenes on rainy days. To improve the generality
of our proposed method, we adopt a cross-modality training strategy that
combines synthetic videos and real-world images. Extensive experiments show
that our proposed method can generalize well and achieve the best waterdrop
removal performance in complex real-world driving scenes.",None,-1
1f9f64fd-4d60-41ff-8aa8-c35e176d75e8,Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models,0.55578,13,"In this study, we present an efficient and effective approach for achieving
temporally consistent synthetic-to-real video translation in videos of varying
lengths. Our method leverages off-the-shelf conditional image diffusion models,
allowing us to perform multiple synthetic-to-real image generations in
parallel. By utilizing the available optical flow information from the
synthetic videos, our approach seamlessly enforces temporal consistency among
corresponding pixels across frames. This is achieved through joint noise
optimization, effectively minimizing spatial and temporal discrepancies. To the
best of our knowledge, our proposed method is the first to accomplish diverse
and temporally consistent synthetic-to-real video translation using conditional
image diffusion models. Furthermore, our approach does not require any training
or fine-tuning of the diffusion models. Extensive experiments conducted on
various benchmarks for synthetic-to-real video translation demonstrate the
effectiveness of our approach, both quantitatively and qualitatively. Finally,
we show that our method outperforms other baseline methods in terms of both
temporal consistency and visual quality.",None,-1
3026aba9-4633-4f44-ba43-da22311cbb7f,A new perspective on building efficient and expressive 3D equivariant graph neural networks,0.972468,19,"Geometric deep learning enables the encoding of physical symmetries in
modeling 3D objects. Despite rapid progress in encoding 3D symmetries into
Graph Neural Networks (GNNs), a comprehensive evaluation of the expressiveness
of these networks through a local-to-global analysis lacks today. In this
paper, we propose a local hierarchy of 3D isomorphism to evaluate the
expressive power of equivariant GNNs and investigate the process of
representing global geometric information from local patches. Our work leads to
two crucial modules for designing expressive and efficient geometric GNNs;
namely local substructure encoding (LSE) and frame transition encoding (FTE).
To demonstrate the applicability of our theory, we propose LEFTNet which
effectively implements these modules and achieves state-of-the-art performance
on both scalar-valued and vector-valued molecular property prediction tasks. We
further point out the design space for future developments of equivariant graph
neural networks. Our codes are available at
\url{https://github.com/yuanqidu/LeftNet}.",None,-1
3e369ec2-3e50-4fec-83dd-28b6b44374b9,DADFNet: Dual Attention and Dual Frequency-Guided Dehazing Network for Video-Empowered Intelligent Transportation,0.556478,2,"Visual surveillance technology is an indispensable functional component of
advanced traffic management systems. It has been applied to perform traffic
supervision tasks, such as object detection, tracking and recognition. However,
adverse weather conditions, e.g., fog, haze and mist, pose severe challenges
for video-based transportation surveillance. To eliminate the influences of
adverse weather conditions, we propose a dual attention and dual
frequency-guided dehazing network (termed DADFNet) for real-time visibility
enhancement. It consists of a dual attention module (DAM) and a high-low
frequency-guided sub-net (HLFN) to jointly consider the attention and frequency
mapping to guide haze-free scene reconstruction. Extensive experiments on both
synthetic and real-world images demonstrate the superiority of DADFNet over
state-of-the-art methods in terms of visibility enhancement and improvement in
detection accuracy. Furthermore, DADFNet only takes $6.3$ ms to process a 1,920
* 1,080 image on the 2080 Ti GPU, making it highly efficient for deployment in
intelligent transportation systems.",None,-1
e862aba5-a339-49ae-89b1-bcf16e391262,"Gaze-based intention estimation: principles, methodologies, and applications in HRI",0.964875,8,"Intention prediction has become a relevant field of research in Human-Machine
and Human-Robot Interaction. Indeed, any artificial system (co)-operating with
and along humans, designed to assist and coordinate its actions with a human
partner, would benefit from first inferring the human's current intention. To
spare the user the cognitive burden of explicitly uttering their goals, this
inference relies mostly on behavioral cues deemed indicative of the current
action. It has been long known that eye movements are highly anticipatory of
the single steps unfolding during a task, hence they can serve as a very early
and reliable behavioural cue for intention recognition. This review aims to
draw a line between insights in the psychological literature on visuomotor
control and relevant applications of gaze-based intention recognition in
technical domains, with a focus on teleoperated and assistive robotic systems.
Starting from the cognitive principles underlying the relationship between
intentions, eye movements, and action, the use of eye tracking and gaze-based
models for intent recognition in Human-Robot Interaction is considered, with
prevalent methodologies and their diverse applications. Finally, special
consideration is given to relevant human factors issues and current limitations
to be factored in when designing such systems.",None,-1
3e63011f-857a-4a56-8dd7-fd8fa55b5ebb,Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction,0.830708,8,"Multi-Modal Relation Extraction (MMRE) aims at identifying the relation
between two entities in texts that contain visual clues. Rich visual content is
valuable for the MMRE task, but existing works cannot well model finer
associations among different modalities, failing to capture the truly helpful
visual information and thus limiting relation extraction performance. In this
paper, we propose a novel MMRE framework to better capture the deeper
correlations of text, entity pair, and image/objects, so as to mine more
helpful information for the task, termed as DGF-PT. We first propose a
prompt-based autoregressive encoder, which builds the associations of
intra-modal and inter-modal features related to the task, respectively by
entity-oriented and object-oriented prefixes. To better integrate helpful
visual information, we design a dual-gated fusion module to distinguish the
importance of image/objects and further enrich text representations. In
addition, a generative decoder is introduced with entity type restriction on
relations, better filtering out candidates. Extensive experiments conducted on
the benchmark dataset show that our approach achieves excellent performance
compared to strong competitors, even in the few-shot situation.",None,-1
c53acaf8-9af4-4899-983d-d156efaa111d,Benchmarking bias: Expanding clinical AI model card to incorporate bias reporting of social and non-social factors,0.303575,1,"Clinical AI model reporting cards should be expanded to incorporate a broad
bias reporting of both social and non-social factors. Non-social factors
consider the role of other factors, such as disease dependent, anatomic, or
instrument factors on AI model bias, which are essential to ensure safe
deployment.",None,-1
5ffe325c-7c6a-4a07-83b1-f1a888af310c,Score-balanced Loss for Multi-aspect Pronunciation Assessment,0.283453,5,"With rapid technological growth, automatic pronunciation assessment has
transitioned toward systems that evaluate pronunciation in various aspects,
such as fluency and stress. However, despite the highly imbalanced score labels
within each aspect, existing studies have rarely tackled the data imbalance
problem. In this paper, we suggest a novel loss function, score-balanced loss,
to address the problem caused by uneven data, such as bias toward the majority
scores. As a re-weighting approach, we assign higher costs when the predicted
score is of the minority class, thus, guiding the model to gain positive
feedback for sparse score prediction. Specifically, we design two weighting
factors by leveraging the concept of an effective number of samples and using
the ranks of scores. We evaluate our method on the speechocean762 dataset,
which has noticeably imbalanced scores for several aspects. Improved results
particularly on such uneven aspects prove the effectiveness of our method.",None,-1
b804cb98-3167-437e-8f1d-49dfa083e7b1,Large Language Models as Topological Structure Enhancers for Text-Attributed Graphs,0.607936,8,"The latest advancements in large language models (LLMs) have revolutionized
the field of natural language processing (NLP). Inspired by the success of LLMs
in NLP tasks, some recent work has begun investigating the potential of
applying LLMs in graph learning tasks. However, most of the existing work
focuses on utilizing LLMs as powerful node feature augmenters, leaving
employing LLMs to enhance graph topological structures an understudied problem.
In this work, we explore how to leverage the information retrieval and text
generation capabilities of LLMs to refine/enhance the topological structure of
text-attributed graphs (TAGs) under the node classification setting. First, we
propose using LLMs to help remove unreliable edges and add reliable ones in the
TAG. Specifically, we first let the LLM output the semantic similarity between
node attributes through delicate prompt designs, and then perform edge deletion
and edge addition based on the similarity. Second, we propose using
pseudo-labels generated by the LLM to improve graph topology, that is, we
introduce the pseudo-label propagation as a regularization to guide the graph
neural network (GNN) in learning proper edge weights. Finally, we incorporate
the two aforementioned LLM-based methods for graph topological refinement into
the process of GNN training, and perform extensive experiments on four
real-world datasets. The experimental results demonstrate the effectiveness of
LLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain
on public benchmarks).",None,-1
cbf9794e-d0a2-4aa5-90a0-c8807dbd85ee,Double A3C: Deep Reinforcement Learning on OpenAI Gym Games,0.0548555,1,"Reinforcement Learning (RL) is an area of machine learning figuring out how
agents take actions in an unknown environment to maximize its rewards. Unlike
classical Markov Decision Process (MDP) in which agent has full knowledge of
its state, rewards, and transitional probability, reinforcement learning
utilizes exploration and exploitation for the model uncertainty. Under the
condition that the model usually has a large state space, a neural network (NN)
can be used to correlate its input state to its output actions to maximize the
agent's rewards. However, building and training an efficient neural network is
challenging. Inspired by Double Q-learning and Asynchronous Advantage
Actor-Critic (A3C) algorithm, we will propose and implement an improved version
of Double A3C algorithm which utilizing the strength of both algorithms to play
OpenAI Gym Atari 2600 games to beat its benchmarks for our project.",None,-1
84b13cca-402d-415b-88c7-52e684a09f53,Unsupervised Domain Adaptation for Training Event-Based Networks Using Contrastive Learning and Uncorrelated Conditioning,0.626462,10,"Event-based cameras offer reliable measurements for preforming computer
vision tasks in high-dynamic range environments and during fast motion
maneuvers. However, adopting deep learning in event-based vision faces the
challenge of annotated data scarcity due to recency of event cameras.
Transferring the knowledge that can be obtained from conventional camera
annotated data offers a practical solution to this challenge. We develop an
unsupervised domain adaptation algorithm for training a deep network for
event-based data image classification using contrastive learning and
uncorrelated conditioning of data. Our solution outperforms the existing
algorithms for this purpose.",None,-1
edc79c98-b0bf-48e4-b432-224ecbd631ef,Agent-based Collaborative Random Search for Hyper-parameter Tuning and Global Function Optimization,0.580747,8,"Hyper-parameter optimization is one of the most tedious yet crucial steps in
training machine learning models. There are numerous methods for this vital
model-building stage, ranging from domain-specific manual tuning guidelines
suggested by the oracles to the utilization of general-purpose black-box
optimization techniques. This paper proposes an agent-based collaborative
technique for finding near-optimal values for any arbitrary set of
hyper-parameters (or decision variables) in a machine learning model (or
general function optimization problem). The developed method forms a
hierarchical agent-based architecture for the distribution of the searching
operations at different dimensions and employs a cooperative searching
procedure based on an adaptive width-based random sampling technique to locate
the optima. The behavior of the presented model, specifically against the
changes in its design parameters, is investigated in both machine learning and
global function optimization applications, and its performance is compared with
that of two randomized tuning strategies that are commonly used in practice.
According to the empirical results, the proposed model outperformed the
compared methods in the experimented classification, regression, and
multi-dimensional function optimization tasks, notably in a higher number of
dimensions and in the presence of limited on-device computational resources.",None,-1
49fbcf9e-276b-4f9f-9c30-871fce104f41,FashionTex: Controllable Virtual Try-on with Text and Texture,0.968663,11,"Virtual try-on attracts increasing research attention as a promising way for
enhancing the user experience for online cloth shopping. Though existing
methods can generate impressive results, users need to provide a well-designed
reference image containing the target fashion clothes that often do not exist.
To support user-friendly fashion customization in full-body portraits, we
propose a multi-modal interactive setting by combining the advantages of both
text and texture for multi-level fashion manipulation. With the carefully
designed fashion editing module and loss functions, FashionTex framework can
semantically control cloth types and local texture patterns without annotated
pairwise training data. We further introduce an ID recovery module to maintain
the identity of input portrait. Extensive experiments have demonstrated the
effectiveness of our proposed pipeline.",None,-1
bbcc4835-ed16-45b7-bbb4-fefa2047910e,AVSegFormer: Audio-Visual Segmentation with Transformer,0.772234,15,"The combination of audio and vision has long been a topic of interest in the
multi-modal community. Recently, a new audio-visual segmentation (AVS) task has
been introduced, aiming to locate and segment the sounding objects in a given
video. This task demands audio-driven pixel-level scene understanding for the
first time, posing significant challenges. In this paper, we propose
AVSegFormer, a novel framework for AVS tasks that leverages the transformer
architecture. Specifically, we introduce audio queries and learnable queries
into the transformer decoder, enabling the network to selectively attend to
interested visual features. Besides, we present an audio-visual mixer, which
can dynamically adjust visual features by amplifying relevant and suppressing
irrelevant spatial channels. Additionally, we devise an intermediate mask loss
to enhance the supervision of the decoder, encouraging the network to produce
more accurate intermediate predictions. Extensive experiments demonstrate that
AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is
available at https://github.com/vvvb-github/AVSegFormer.",None,-1
17a2797d-400f-4beb-a0d1-ba41cffcd07a,Temporal Action Localization with Enhanced Instant Discriminability,0.615406,4,"Temporal action detection (TAD) aims to detect all action boundaries and
their corresponding categories in an untrimmed video. The unclear boundaries of
actions in videos often result in imprecise predictions of action boundaries by
existing methods. To resolve this issue, we propose a one-stage framework named
TriDet. First, we propose a Trident-head to model the action boundary via an
estimated relative probability distribution around the boundary. Then, we
analyze the rank-loss problem (i.e. instant discriminability deterioration) in
transformer-based methods and propose an efficient scalable-granularity
perception (SGP) layer to mitigate this issue. To further push the limit of
instant discriminability in the video backbone, we leverage the strong
representation capability of pretrained large models and investigate their
performance on TAD. Last, considering the adequate spatial-temporal context for
classification, we design a decoupled feature pyramid network with separate
feature pyramids to incorporate rich spatial context from the large model for
localization. Experimental results demonstrate the robustness of TriDet and its
state-of-the-art performance on multiple TAD datasets, including hierarchical
(multilabel) TAD datasets.",None,-1
83c613d1-7687-477a-b1be-8172f58065c8,Spatially and Spectrally Consistent Deep Functional Maps,0.937201,9,"Cycle consistency has long been exploited as a powerful prior for jointly
optimizing maps within a collection of shapes. In this paper, we investigate
its utility in the approaches of Deep Functional Maps, which are considered
state-of-the-art in non-rigid shape matching. We first justify that under
certain conditions, the learned maps, when represented in the spectral domain,
are already cycle consistent. Furthermore, we identify the discrepancy that
spectrally consistent maps are not necessarily spatially, or point-wise,
consistent. In light of this, we present a novel design of unsupervised Deep
Functional Maps, which effectively enforces the harmony of learned maps under
the spectral and the point-wise representation. By taking advantage of cycle
consistency, our framework produces state-of-the-art results in mapping shapes
even under significant distortions. Beyond that, by independently estimating
maps in both spectral and spatial domains, our method naturally alleviates
over-fitting in network training, yielding superior generalization performance
and accuracy within an array of challenging tests for both near-isometric and
non-isometric datasets. Codes are available at
https://github.com/rqhuang88/Spatiallyand-Spectrally-Consistent-Deep-Functional-Maps.",None,-1
324e413c-74a0-4cd5-825d-3e3274adb05a,Investigating Strategies for Clause Recommendation,0.0165559,1,"Clause recommendation is the problem of recommending a clause to a legal
contract, given the context of the contract in question and the clause type to
which the clause should belong. With not much prior work being done toward the
generation of legal contracts, this problem was proposed as a first step toward
the bigger problem of contract generation. As an open-ended text generation
problem, the distinguishing characteristics of this problem lie in the nature
of legal language as a sublanguage and the considerable similarity of textual
content within the clauses of a specific type. This similarity aspect in legal
clauses drives us to investigate the importance of similar contracts'
representation for recommending clauses. In our work, we experiment with
generating clauses for 15 commonly occurring clause types in contracts
expanding upon the previous work on this problem and analyzing clause
recommendations in varying settings using information derived from similar
contracts.",None,-1
4fa2f58f-dac9-4fc0-8678-c7cfafa5c9f8,CLIP-KD: An Empirical Study of CLIP Model Distillation,0.483363,5,"Contrastive Language-Image Pre-training (CLIP) has become a promising
language-supervised visual pre-training framework. This paper aims to distill
small CLIP models supervised by a large teacher CLIP model. We propose several
distillation strategies, including relation, feature, gradient and contrastive
paradigms, to examine the effectiveness of CLIP-Knowledge Distillation (KD). We
show that a simple feature mimicry with Mean Squared Error loss works
surprisingly well. Moreover, interactive contrastive learning across teacher
and student encoders is also effective in performance improvement. We explain
that the success of CLIP-KD can be attributed to maximizing the feature
similarity between teacher and student. The unified method is applied to
distill several student models trained on CC3M+12M. CLIP-KD improves student
CLIP models consistently over zero-shot ImageNet classification and cross-modal
retrieval benchmarks. When using ViT-L/14 pretrained on Laion-400M as the
teacher, CLIP-KD achieves 57.5\% and 55.4\% zero-shot top-1 ImageNet accuracy
over ViT-B/16 and ResNet-50, surpassing the original CLIP without KD by 20.5\%
and 20.1\% margins, respectively. Our code is released on
https://github.com/winycg/CLIP-KD.",None,-1
300cf31d-a6d9-4dde-a129-01271316b71a,Thread of Thought Unraveling Chaotic Contexts,0.474146,14,"Large Language Models (LLMs) have ushered in a transformative era in the
field of natural language processing, excelling in tasks related to text
comprehension and generation. Nevertheless, they encounter difficulties when
confronted with chaotic contexts (e.g., distractors rather than long irrelevant
context), leading to the inadvertent omission of certain details within the
chaotic context. In response to these challenges, we introduce the ""Thread of
Thought"" (ThoT) strategy, which draws inspiration from human cognitive
processes. ThoT systematically segments and analyzes extended contexts while
adeptly selecting pertinent information. This strategy serves as a versatile
""plug-and-play"" module, seamlessly integrating with various LLMs and prompting
techniques. In the experiments, we utilize the PopQA and EntityQ datasets, as
well as a Multi-Turn Conversation Response dataset (MTCR) we collected, to
illustrate that ThoT significantly improves reasoning performance compared to
other prompting techniques.",None,-1
5c7f1279-fb2d-4697-a75e-6c4607f40eb6,THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech,0.598444,2,"Detecting harmful content on social media, such as Twitter, is made difficult
by the fact that the seemingly simple yes/no classification conceals a
significant amount of complexity. Unfortunately, while several datasets have
been collected for training classifiers in hate and offensive speech, there is
a scarcity of datasets labeled with a finer granularity of target classes and
specific targets. In this paper, we introduce THOS, a dataset of 8.3k tweets
manually labeled with fine-grained annotations about the target of the message.
We demonstrate that this dataset makes it feasible to train classifiers, based
on Large Language Models, to perform classification at this level of
granularity.",None,-1
4ca4860d-5cde-41f8-ae6b-a365534b405a,Single Domain Generalization via Normalised Cross-correlation Based Convolutions,0.0687525,1,"Deep learning techniques often perform poorly in the presence of domain
shift, where the test data follows a different distribution than the training
data. The most practically desirable approach to address this issue is Single
Domain Generalization (S-DG), which aims to train robust models using data from
a single source. Prior work on S-DG has primarily focused on using data
augmentation techniques to generate diverse training data. In this paper, we
explore an alternative approach by investigating the robustness of linear
operators, such as convolution and dense layers commonly used in deep learning.
We propose a novel operator called XCNorm that computes the normalized
cross-correlation between weights and an input feature patch. This approach is
invariant to both affine shifts and changes in energy within a local feature
patch and eliminates the need for commonly used non-linear activation
functions. We show that deep neural networks composed of this operator are
robust to common semantic distribution shifts. Furthermore, our empirical
results on single-domain generalization benchmarks demonstrate that our
proposed technique performs comparably to the state-of-the-art methods.",None,-1
1757cf8e-1e1b-40f6-9777-d5ced37a41b0,Collective Privacy Recovery: Data-sharing Coordination via Decentralized Artificial Intelligence,0.66407,7,"Collective privacy loss becomes a colossal problem, an emergency for personal
freedoms and democracy. But, are we prepared to handle personal data as scarce
resource and collectively share data under the doctrine: as little as possible,
as much as necessary? We hypothesize a significant privacy recovery if a
population of individuals, the data collective, coordinates to share minimum
data for running online services with the required quality. Here we show how to
automate and scale-up complex collective arrangements for privacy recovery
using decentralized artificial intelligence. For this, we compare for first
time attitudinal, intrinsic, rewarded and coordinated data sharing in a
rigorous living-lab experiment of high realism involving >27,000 real data
disclosures. Using causal inference and cluster analysis, we differentiate
criteria predicting privacy and five key data-sharing behaviors. Strikingly,
data-sharing coordination proves to be a win-win for all: remarkable privacy
recovery for people with evident costs reduction for service providers.",None,-1
85d1bbb5-5d4a-40b2-b701-114a67e2b0d6,Control Risk for Potential Misuse of Artificial Intelligence in Science,0.885085,5,"The expanding application of Artificial Intelligence (AI) in scientific
fields presents unprecedented opportunities for discovery and innovation.
However, this growth is not without risks. AI models in science, if misused,
can amplify risks like creation of harmful substances, or circumvention of
established regulations. In this study, we aim to raise awareness of the
dangers of AI misuse in science, and call for responsible AI development and
use in this domain. We first itemize the risks posed by AI in scientific
contexts, then demonstrate the risks by highlighting real-world examples of
misuse in chemical science. These instances underscore the need for effective
risk management strategies. In response, we propose a system called SciGuard to
control misuse risks for AI models in science. We also propose a red-teaming
benchmark SciMT-Safety to assess the safety of different systems. Our proposed
SciGuard shows the least harmful impact in the assessment without compromising
performance in benign tests. Finally, we highlight the need for a
multidisciplinary and collaborative effort to ensure the safe and ethical use
of AI models in science. We hope that our study can spark productive
discussions on using AI ethically in science among researchers, practitioners,
policymakers, and the public, to maximize benefits and minimize the risks of
misuse.",None,-1
5580348a-6ae5-4d17-a564-d99c6c440416,Scale-Equivariant UNet for Histopathology Image Segmentation,0.451235,7,"Digital histopathology slides are scanned and viewed under different
magnifications and stored as images at different resolutions. Convolutional
Neural Networks (CNNs) trained on such images at a given scale fail to
generalise to those at different scales. This inability is often addressed by
augmenting training data with re-scaled images, allowing a model with
sufficient capacity to learn the requisite patterns. Alternatively, designing
CNN filters to be scale-equivariant frees up model capacity to learn
discriminative features. In this paper, we propose the Scale-Equivariant UNet
(SEUNet) for image segmentation by building on scale-space theory. The SEUNet
contains groups of filters that are linear combinations of Gaussian basis
filters, whose scale parameters are trainable but constrained to span disjoint
scales through the layers of the network. Extensive experiments on a nuclei
segmentation dataset and a tissue type segmentation dataset demonstrate that
our method outperforms other approaches, with much fewer trainable parameters.",None,-1
e8e9e04a-b730-4bc6-a92a-b99f976f187e,"What Should Be Balanced in a ""Balanced"" Face Recognition Dataset?",0.663357,6,"The issue of demographic disparities in face recognition accuracy has
attracted increasing attention in recent years. Various face image datasets
have been proposed as 'fair' or 'balanced' to assess the accuracy of face
recognition algorithms across demographics. These datasets typically balance
the number of identities and images across demographics. It is important to
note that the number of identities and images in an evaluation dataset are {\em
not} driving factors for 1-to-1 face matching accuracy. Moreover, balancing the
number of identities and images does not ensure balance in other factors known
to impact accuracy, such as head pose, brightness, and image quality. We
demonstrate these issues using several recently proposed datasets. To improve
the ability to perform less biased evaluations, we propose a bias-aware toolkit
that facilitates creation of cross-demographic evaluation datasets balanced on
factors mentioned in this paper.",None,-1
0611936f-ef8a-42bc-8707-bad91255893e,Faithful Question Answering with Monte-Carlo Planning,0.218711,10,"Although large language models demonstrate remarkable question-answering
performances, revealing the intermediate reasoning steps that the models
faithfully follow remains challenging. In this paper, we propose FAME (FAithful
question answering with MontE-carlo planning) to answer questions based on
faithful reasoning steps. The reasoning steps are organized as a structured
entailment tree, which shows how premises are used to produce intermediate
conclusions that can prove the correctness of the answer. We formulate the task
as a discrete decision-making problem and solve it through the interaction of a
reasoning environment and a controller. The environment is modular and contains
several basic task-oriented modules, while the controller proposes actions to
assemble the modules. Since the search space could be large, we introduce a
Monte-Carlo planning algorithm to do a look-ahead search and select actions
that will eventually lead to high-quality steps. FAME achieves state-of-the-art
performance on the standard benchmark. It can produce valid and faithful
reasoning steps compared with large language models with a much smaller model
size.",None,-1
dbb7a5dd-9f01-4519-ace0-a5a54ade475b,Multi-Modality Deep Network for JPEG Artifacts Reduction,0.608941,2,"In recent years, many convolutional neural network-based models are designed
for JPEG artifacts reduction, and have achieved notable progress. However, few
methods are suitable for extreme low-bitrate image compression artifacts
reduction. The main challenge is that the highly compressed image loses too
much information, resulting in reconstructing high-quality image difficultly.
To address this issue, we propose a multimodal fusion learning method for
text-guided JPEG artifacts reduction, in which the corresponding text
description not only provides the potential prior information of the highly
compressed image, but also serves as supplementary information to assist in
image deblocking. We fuse image features and text semantic features from the
global and local perspectives respectively, and design a contrastive loss built
upon contrastive learning to produce visually pleasing results. Extensive
experiments, including a user study, prove that our method can obtain better
deblocking results compared to the state-of-the-art methods.",None,-1
9dd79f3e-487a-40c8-8b32-36858bb7088c,"TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT",0.723464,25,"Tables are prevalent in real-world databases, requiring significant time and
effort for humans to analyze and manipulate. The advancements in large language
models (LLMs) have made it possible to interact with tables using natural
language input, bringing this capability closer to reality. In this paper, we
present TableGPT, a unified fine-tuned framework that enables LLMs to
understand and operate on tables using external functional commands. It
introduces the capability to seamlessly interact with tables, enabling a wide
range of functionalities such as question answering, data manipulation (e.g.,
insert, delete, query, and modify operations), data visualization, analysis
report generation, and automated prediction. TableGPT aims to provide
convenience and accessibility to users by empowering them to effortlessly
leverage tabular data. At the core of TableGPT lies the novel concept of global
tabular representations, which empowers LLMs to gain a comprehensive
understanding of the entire table beyond meta-information. By jointly training
LLMs on both table and text modalities, TableGPT achieves a deep understanding
of tabular data and the ability to perform complex operations on tables through
chain-of-command instructions. Importantly, TableGPT offers the advantage of
being a self-contained system rather than relying on external API interfaces.
Moreover, it supports efficient data process flow, query rejection (when
appropriate) and private deployment, enabling faster domain data fine-tuning
and ensuring data privacy, which enhances the framework's adaptability to
specific use cases.",None,-1
c83accc0-0233-4c11-9112-e5528138c5c9,What does BERT learn about prosody?,0.51502,2,"Language models have become nearly ubiquitous in natural language processing
applications achieving state-of-the-art results in many tasks including
prosody. As the model design does not define predetermined linguistic targets
during training but rather aims at learning generalized representations of the
language, analyzing and interpreting the representations that models implicitly
capture is important in bridging the gap between interpretability and model
performance. Several studies have explored the linguistic information that
models capture providing some insights on their representational capacity.
However, the current studies have not explored whether prosody is part of the
structural information of the language that models learn. In this work, we
perform a series of experiments on BERT probing the representations captured at
different layers. Our results show that information about prosodic prominence
spans across many layers but is mostly focused in middle layers suggesting that
BERT relies mostly on syntactic and semantic information.",None,-1
90ee18cb-3ca2-45f6-a758-b38618fe5a6d,kNN-BOX: A Unified Framework for Nearest Neighbor Generation,0.358282,4,"Augmenting the base neural model with a token-level symbolic datastore is a
novel generation paradigm and has achieved promising results in machine
translation (MT). In this paper, we introduce a unified framework kNN-BOX,
which enables quick development and interactive analysis for this novel
paradigm. kNN-BOX decomposes the datastore-augmentation approach into three
modules: datastore, retriever and combiner, thus putting diverse kNN generation
methods into a unified way. Currently, kNN-BOX has provided implementation of
seven popular kNN-MT variants, covering research from performance enhancement
to efficiency optimization. It is easy for users to reproduce these existing
works or customize their own models. Besides, users can interact with their kNN
generation systems with kNN-BOX to better understand the underlying inference
process in a visualized way. In the experiment section, we apply kNN-BOX for
machine translation and three other seq2seq generation tasks, namely, text
simplification, paraphrase generation and question generation. Experiment
results show that augmenting the base neural model with kNN-BOX leads to a
large performance improvement in all these tasks. The code and document of
kNN-BOX is available at https://github.com/NJUNLP/knn-box.",None,-1
2dab3e4b-6ef7-4c7f-9da2-e048e4e088e9,Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided by Self-presentation Theory,0.398691,3,"Having the ability to empathize is crucial for accurately representing human
behavior during conversations. Despite numerous research aim to improve the
cognitive capability of models by incorporating external knowledge, there has
been limited attention on the sensible and rational expression of the
conversation itself, which are crucial components of the cognitive empathy.
Guided by self-presentation theory in sociology, we have designed an innovative
categorical approach that segregates historical dialogues into sensible and
rational sentences and subsequently elucidate the context through the designed
attention mechanism. However, the rational information within the conversation
is restricted and the external knowledge used in previous methods have
limitations of semantic contradiction and narrow vision field. Considering the
impressive performance of LLM in the domain of intelligent agent. We employ
LLaMA2-70b as a rational brain to analyze the profound logical information
maintained in conversations, which assists the model assessing the balance of
sensibility and rationality to produce quality empathetic responses.
Experimental evaluations demonstrate that our method outperforms other
comparable methods on both automatic and human evaluations.",None,-1
ddd70078-c18a-4f35-9fd1-6d35052402f6,A multi-functional simulation platform for on-demand ride service operations,0.139344,1,"On-demand ride services or ride-sourcing services have been experiencing fast
development in the past decade. Various mathematical models and optimization
algorithms have been developed to help ride-sourcing platforms design
operational strategies with higher efficiency. However, due to cost and
reliability issues (implementing an immature algorithm for real operations may
result in system turbulence), it is commonly infeasible to validate these
models and train/test these optimization algorithms within real-world ride
sourcing platforms. Acting as a useful test bed, a simulation platform for
ride-sourcing systems will be very important to conduct algorithm
training/testing or model validation through trails and errors. While previous
studies have established a variety of simulators for their own tasks, it lacks
a fair and public platform for comparing the models or algorithms proposed by
different researchers. In addition, the existing simulators still face many
challenges, ranging from their closeness to real environments of ride-sourcing
systems, to the completeness of different tasks they can implement. To address
the challenges, we propose a novel multi-functional and open-sourced simulation
platform for ride-sourcing systems, which can simulate the behaviors and
movements of various agents on a real transportation network. It provides a few
accessible portals for users to train and test various optimization algorithms,
especially reinforcement learning algorithms, for a variety of tasks, including
on-demand matching, idle vehicle repositioning, and dynamic pricing. In
addition, it can be used to test how well the theoretical models approximate
the simulated outcomes. Evaluated on real-world data based experiments, the
simulator is demonstrated to be an efficient and effective test bed for various
tasks related to on-demand ride service operations.",None,-1
3030806c-03af-4e0a-8401-fe13cbd8ef35,BEVHeight: A Robust Framework for Vision-based Roadside 3D Object Detection,0.971284,27,"While most recent autonomous driving system focuses on developing perception
methods on ego-vehicle sensors, people tend to overlook an alternative approach
to leverage intelligent roadside cameras to extend the perception ability
beyond the visual range. We discover that the state-of-the-art vision-centric
bird's eye view detection methods have inferior performances on roadside
cameras. This is because these methods mainly focus on recovering the depth
regarding the camera center, where the depth difference between the car and the
ground quickly shrinks while the distance increases. In this paper, we propose
a simple yet effective approach, dubbed BEVHeight, to address this issue. In
essence, instead of predicting the pixel-wise depth, we regress the height to
the ground to achieve a distance-agnostic formulation to ease the optimization
process of camera-only perception methods. On popular 3D detection benchmarks
of roadside cameras, our method surpasses all previous vision-centric methods
by a significant margin. The code is available at
{\url{https://github.com/ADLab-AutoDrive/BEVHeight}}.",None,-1
98d763d3-0cc8-411c-bf32-058aad7bdbf2,Worst-Case Control and Learning Using Partial Observations Over an Infinite Time-Horizon,0.452928,6,"Safety-critical cyber-physical systems require control strategies whose
worst-case performance is robust against adversarial disturbances and modeling
uncertainties. In this paper, we present a framework for approximate control
and learning in partially observed systems to minimize the worst-case
discounted cost over an infinite time horizon. We model disturbances to the
system as finite-valued uncertain variables with unknown probability
distributions. For problems with known system dynamics, we construct a dynamic
programming (DP) decomposition to compute the optimal control strategy. Our
first contribution is to define information states that improve the
computational tractability of this DP without loss of optimality. Then, we
describe a simplification for a class of problems where the incurred cost is
observable at each time instance. Our second contribution is defining an
approximate information state that can be constructed or learned directly from
observed data for problems with observable costs. We derive bounds on the
performance loss of the resulting approximate control strategy and illustrate
the effectiveness of our approach in partially observed decision-making
problems with a numerical example.",None,-1
fb66689b-12fd-4c6f-bf0d-af77be872825,Seeing in Flowing: Adapting CLIP for Action Recognition with Motion Prompts Learning,0.368393,3,"The Contrastive Language-Image Pre-training (CLIP) has recently shown
remarkable generalization on ""zero-shot"" training and has applied to many
downstream tasks. We explore the adaptation of CLIP to achieve a more efficient
and generalized action recognition method. We propose that the key lies in
explicitly modeling the motion cues flowing in video frames. To that end, we
design a two-stream motion modeling block to capture motion and spatial
information at the same time. And then, the obtained motion cues are utilized
to drive a dynamic prompts learner to generate motion-aware prompts, which
contain much semantic information concerning human actions. In addition, we
propose a multimodal communication block to achieve a collaborative learning
and further improve the performance. We conduct extensive experiments on
HMDB-51, UCF-101, and Kinetics-400 datasets. Our method outperforms most
existing state-of-the-art methods by a significant margin on ""few-shot"" and
""zero-shot"" training. We also achieve competitive performance on ""closed-set""
training with extremely few trainable parameters and additional computational
costs.",None,-1
8fc6a82a-082d-4876-92bd-6f486b6ddeac,InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models,0.227927,32,"Large language models~(LLMs) are instruction followers, but it can be
challenging to find the best instruction for different situations, especially
for black-box LLMs on which backpropagation is forbidden. Instead of directly
optimizing the discrete instruction, we optimize a low-dimensional soft prompt
applied to an open-source LLM to generate the instruction for the black-box
LLM. On each iteration of the proposed method, which we call InstructZero, a
soft prompt is converted into an instruction using the open-source LLM, which
is then submitted to the black-box LLM for zero-shot evaluation, and the
performance is sent to Bayesian optimization to produce new soft prompts
improving the zero-shot performance. We evaluate InstructZero on different
combinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our
results show that InstructZero outperforms SOTA auto-instruction methods across
a variety of downstream tasks. Our code and data are publicly available at
https://github.com/Lichang-Chen/InstructZero.",None,-1
090f6a24-5732-44ae-b16f-516f7930f849,CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation Detection in Online Communities,0.609976,2,"Detecting norm violations in online communities is critical to maintaining
healthy and safe spaces for online discussions. Existing machine learning
approaches often struggle to adapt to the diverse rules and interpretations
across different communities due to the inherent challenges of fine-tuning
models for such context-specific tasks. In this paper, we introduce
Context-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a
novel method that employs prompt-based learning to detect norm violations
across various types of rules. CPL-NoViD outperforms the baseline by
incorporating context through natural language prompts and demonstrates
improved performance across different rule types. Significantly, it not only
excels in cross-rule-type and cross-community norm violation detection but also
exhibits adaptability in few-shot learning scenarios. Most notably, it
establishes a new state-of-the-art in norm violation detection, surpassing
existing benchmarks. Our work highlights the potential of prompt-based learning
for context-sensitive norm violation detection and paves the way for future
research on more adaptable, context-aware models to better support online
community moderators.",None,-1
2c772aa5-ed26-41e4-98b0-3f288f42a22f,Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation,0.742214,3,"Conversational Recommendation System (CRS) is a rapidly growing research area
that has gained significant attention alongside advancements in language
modelling techniques. However, the current state of conversational
recommendation faces numerous challenges due to its relative novelty and
limited existing contributions. In this study, we delve into benchmark datasets
for developing CRS models and address potential biases arising from the
feedback loop inherent in multi-turn interactions, including selection bias and
multiple popularity bias variants. Drawing inspiration from the success of
generative data via using language models and data augmentation techniques, we
present two novel strategies, 'Once-Aug' and 'PopNudge', to enhance model
performance while mitigating biases. Through extensive experiments on ReDial
and TG-ReDial benchmark datasets, we show a consistent improvement of CRS
techniques with our data augmentation approaches and offer additional insights
on addressing multiple newly formulated biases.",None,-1
4f800a83-e438-40d3-99ce-bd13e0745a48,Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport,0.508184,14,"Answering complex queries on knowledge graphs is important but particularly
challenging because of the data incompleteness. Query embedding methods address
this issue by learning-based models and simulating logical reasoning with set
operators. Previous works focus on specific forms of embeddings, but scoring
functions between embeddings are underexplored. In contrast to existing scoring
functions motivated by local comparison or global transport, this work
investigates the local and global trade-off with unbalanced optimal transport
theory. Specifically, we embed sets as bounded measures in $\real$ endowed with
a scoring function motivated by the Wasserstein-Fisher-Rao metric. Such a
design also facilitates closed-form set operators in the embedding space.
Moreover, we introduce a convolution-based algorithm for linear time
computation and a block-diagonal kernel to enforce the trade-off. Results show
that WFRE can outperform existing query embedding methods on standard datasets,
evaluation sets with combinatorially complex queries, and hierarchical
knowledge graphs. Ablation study shows that finding a better local and global
trade-off is essential for performance improvement.",None,-1
a4478a41-514d-4ed4-9f9d-51742cd9e6e7,MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations,0.0324808,3,"Humans possess a remarkable ability to assign novel interpretations to
linguistic expressions, enabling them to learn new words and understand
community-specific connotations. However, Large Language Models (LLMs) have a
knowledge cutoff and are costly to finetune repeatedly. Therefore, it is
crucial for LLMs to learn novel interpretations in-context. In this paper, we
systematically analyse the ability of LLMs to acquire novel interpretations
using in-context learning. To facilitate our study, we introduce MAGNIFICo, an
evaluation suite implemented within a text-to-SQL semantic parsing framework
that incorporates diverse tokens and prompt settings to simulate real-world
complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a
surprisingly robust capacity for comprehending novel interpretations from
natural language descriptions as well as from discussions within long
conversations. Nevertheless, our findings also highlight the need for further
improvements, particularly when interpreting unfamiliar words or when composing
multiple novel interpretations simultaneously in the same example.
Additionally, our analysis uncovers the semantic predispositions in LLMs and
reveals the impact of recency bias for information presented in long contexts.",None,-1
03933926-306b-49a1-a85d-f1e60cc845b5,OneShotSTL: One-Shot Seasonal-Trend Decomposition For Online Time Series Anomaly Detection And Forecasting,0.767675,9,"Seasonal-trend decomposition is one of the most fundamental concepts in time
series analysis that supports various downstream tasks, including time series
anomaly detection and forecasting. However, existing decomposition methods rely
on batch processing with a time complexity of O(W), where W is the number of
data points within a time window. Therefore, they cannot always efficiently
support real-time analysis that demands low processing delay. To address this
challenge, we propose OneShotSTL, an efficient and accurate algorithm that can
decompose time series online with an update time complexity of O(1). OneShotSTL
is more than $1,000$ times faster than the batch methods, with accuracy
comparable to the best counterparts. Extensive experiments on real-world
benchmark datasets for downstream time series anomaly detection and forecasting
tasks demonstrate that OneShotSTL is from 10 to over 1,000 times faster than
the state-of-the-art methods, while still providing comparable or even better
accuracy.",None,-1
47cf79a3-6844-44e0-bf69-64c5e8fb1908,Robust Point Cloud Processing through Positional Embedding,0.696031,4,"End-to-end trained per-point embeddings are an essential ingredient of any
state-of-the-art 3D point cloud processing such as detection or alignment.
Methods like PointNet, or the more recent point cloud transformer -- and its
variants -- all employ learned per-point embeddings. Despite impressive
performance, such approaches are sensitive to out-of-distribution (OOD) noise
and outliers. In this paper, we explore the role of an analytical per-point
embedding based on the criterion of bandwidth. The concept of bandwidth enables
us to draw connections with an alternate per-point embedding -- positional
embedding, particularly random Fourier features. We present compelling robust
results across downstream tasks such as point cloud classification and
registration with several categories of OOD noise.",None,-1
26f81b21-5b0c-46fa-a0c5-3a28ce15beac,Cordyceps@LT-EDI: Patching Language-Specific Homophobia/Transphobia Classifiers with a Multilingual Understanding,0.290066,1,"Detecting transphobia, homophobia, and various other forms of hate speech is
difficult. Signals can vary depending on factors such as language, culture,
geographical region, and the particular online platform. Here, we present a
joint multilingual (M-L) and language-specific (L-S) approach to homophobia and
transphobic hate speech detection (HSD). M-L models are needed to catch words,
phrases, and concepts that are less common or missing in a particular language
and subsequently overlooked by L-S models. Nonetheless, L-S models are better
situated to understand the cultural and linguistic context of the users who
typically write in a particular language. Here we construct a simple and
successful way to merge the M-L and L-S approaches through simple weight
interpolation in such a way that is interpretable and data-driven. We
demonstrate our system on task A of the 'Shared Task on Homophobia/Transphobia
Detection in social media comments' dataset for homophobia and transphobic HSD.
Our system achieves the best results in three of five languages and achieves a
0.997 macro average F1-score on Malayalam texts.",None,-1
871eb78a-ed8b-4e10-a732-7ce4548f8241,Language Model Tokenizers Introduce Unfairness Between Languages,0.718888,48,"Recent language models have shown impressive multilingual performance, even
when not explicitly trained for it. Despite this, there are concerns about the
quality of their outputs across different languages. In this paper, we show how
disparity in the treatment of different languages arises at the tokenization
stage, well before a model is even invoked. The same text translated into
different languages can have drastically different tokenization lengths, with
differences up to 15 times in some cases. These disparities persist even for
tokenizers that are intentionally trained for multilingual support.
Character-level and byte-level models also exhibit over 4 times the difference
in the encoding length for some language pairs. This induces unfair treatment
for some language communities in regard to the cost of accessing commercial
language services, the processing time and latency, as well as the amount of
content that can be provided as context to the models. Therefore, we make the
case that we should train future language models using multilingually fair
subword tokenizers.",None,-1
834d45ca-f0df-418c-a22c-89cf54a24083,Explain-then-Translate: An Analysis on Improving Program Translation with Self-generated Explanations,0.646464,4,"This work explores the use of self-generated natural language explanations as
an intermediate step for code-to-code translation with language models. Across
three types of explanations and 19 programming languages constructed from the
MultiPL-E dataset, we find the explanations to be particularly effective in the
zero-shot case, improving performance by 12% on average. Improvements with
natural language explanations are particularly pronounced on difficult
programs. We release our dataset, code, and canonical solutions in all 19
languages.",None,-1
fe8d22b4-d3e4-44c7-bdff-60a5adc4beee,TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement,0.686279,8,"Speech enhancement models have greatly progressed in recent years, but still
show limits in perceptual quality of their speech outputs. We propose an
objective for perceptual quality based on temporal acoustic parameters. These
are fundamental speech features that play an essential role in various
applications, including speaker recognition and paralinguistic analysis. We
provide a differentiable estimator for four categories of low-level acoustic
descriptors involving: frequency-related parameters, energy or
amplitude-related parameters, spectral balance parameters, and temporal
features. Unlike prior work that looks at aggregated acoustic parameters or a
few categories of acoustic parameters, our temporal acoustic parameter (TAP)
loss enables auxiliary optimization and improvement of many fine-grain speech
characteristics in enhancement workflows. We show that adding TAPLoss as an
auxiliary objective in speech enhancement produces speech with improved
perceptual quality and intelligibility. We use data from the Deep Noise
Suppression 2020 Challenge to demonstrate that both time-domain models and
time-frequency domain models can benefit from our method.",None,-1
1cbf15c7-d71e-44d7-be5a-d3d3975c2731,FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt,0.982352,10,"Currently, the construction of large language models in specific domains is
done by fine-tuning on a base model. Some models also incorporate knowledge
bases without the need for pre-training. This is because the base model already
contains domain-specific knowledge during the pre-training process. We build a
large language model for food testing. Unlike the above approach, a significant
amount of data in this domain exists in Scanning format for domain standard
documents. In addition, there is a large amount of untrained structured
knowledge. Therefore, we introduce an incremental pre-training step to inject
this knowledge into a large language model. In this paper, we propose a method
for handling structured knowledge and scanned documents in incremental
pre-training. To overcome the problem of machine hallucination, we constructe a
knowledge graph to serve as an external knowledge base for supporting retrieval
in the large language model. It is worth mentioning that this paper is a
technical report of our pre-release version, and we will report our specific
experimental data in future versions.",None,-1
8abb2d15-b21b-4db6-8066-2b1b24d6e7bf,Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information,0.639458,4,"Multi-target multi-camera tracking (MTMCT) of vehicles, i.e. tracking
vehicles across multiple cameras, is a crucial application for the development
of smart city and intelligent traffic system. The main challenges of MTMCT of
vehicles include the intra-class variability of the same vehicle and
inter-class similarity between different vehicles and how to associate the same
vehicle accurately across different cameras under large search space. Previous
methods for MTMCT usually use hierarchical clustering of trajectories to
conduct cross camera association. However, the search space can be large and
does not take spatial and temporal information into consideration. In this
paper, we proposed a transformer-based camera link model with spatial and
temporal filtering to conduct cross camera tracking. Achieving 73.68% IDF1 on
the Nvidia Cityflow V2 dataset test set, showing the effectiveness of our
camera link model on multi-target multi-camera tracking.",None,-1
32fe5634-3f1c-45f6-a881-53023e8197f7,Refining 3D Human Texture Estimation from a Single Image,0.158128,1,"Estimating 3D human texture from a single image is essential in graphics and
vision. It requires learning a mapping function from input images of humans
with diverse poses into the parametric (UV) space and reasonably hallucinating
invisible parts. To achieve a high-quality 3D human texture estimation, we
propose a framework that adaptively samples the input by a deformable
convolution where offsets are learned via a deep neural network. Additionally,
we describe a novel cycle consistency loss that improves view generalization.
We further propose to train our framework with an uncertainty-based pixel-level
image reconstruction loss, which enhances color fidelity. We compare our method
against the state-of-the-art approaches and show significant qualitative and
quantitative improvements.",None,-1
161ea8b3-7c9f-48fe-81e3-ff2f586744c4,arXiv4TGC: Large-Scale Datasets for Temporal Graph Clustering,0.330936,3,"Temporal graph clustering (TGC) is a crucial task in temporal graph learning.
Its focus is on node clustering on temporal graphs, and it offers greater
flexibility for large-scale graph structures due to the mechanism of temporal
graph methods. However, the development of TGC is currently constrained by a
significant problem: the lack of suitable and reliable large-scale temporal
graph datasets to evaluate clustering performance. In other words, most
existing temporal graph datasets are in small sizes, and even large-scale
datasets contain only a limited number of available node labels. It makes
evaluating models for large-scale temporal graph clustering challenging. To
address this challenge, we build arXiv4TGC, a set of novel academic datasets
(including arXivAI, arXivCS, arXivMath, arXivPhy, and arXivLarge) for
large-scale temporal graph clustering. In particular, the largest dataset,
arXivLarge, contains 1.3 million labeled available nodes and 10 million
temporal edges. We further compare the clustering performance with typical
temporal graph learning models on both previous classic temporal graph datasets
and the new datasets proposed in this paper. The clustering performance on
arXiv4TGC can be more apparent for evaluating different models, resulting in
higher clustering confidence and more suitable for large-scale temporal graph
clustering. The arXiv4TGC datasets are publicly available at:
https://github.com/MGitHubL/arXiv4TGC.",None,-1
202ae4d4-bfb7-4114-bf8d-7254bddf06b5,Online Safety Property Collection and Refinement for Safe Deep Reinforcement Learning in Mapless Navigation,0.315506,5,"Safety is essential for deploying Deep Reinforcement Learning (DRL)
algorithms in real-world scenarios. Recently, verification approaches have been
proposed to allow quantifying the number of violations of a DRL policy over
input-output relationships, called properties. However, such properties are
hard-coded and require task-level knowledge, making their application
intractable in challenging safety-critical tasks. To this end, we introduce the
Collection and Refinement of Online Properties (CROP) framework to design
properties at training time. CROP employs a cost signal to identify unsafe
interactions and use them to shape safety properties. Hence, we propose a
refinement strategy to combine properties that model similar unsafe
interactions. Our evaluation compares the benefits of computing the number of
violations using standard hard-coded properties and the ones generated with
CROP. We evaluate our approach in several robotic mapless navigation tasks and
demonstrate that the violation metric computed with CROP allows higher returns
and lower violations over previous Safe DRL approaches.",None,-1
ce5fc126-fbc8-4be2-9707-6bdf9a6a863a,Towards a Praxis for Intercultural Ethics in Explainable AI,0.0728369,1,"Explainable AI (XAI) is often promoted with the idea of helping users
understand how machine learning models function and produce predictions. Still,
most of these benefits are reserved for those with specialized domain
knowledge, such as machine learning developers. Recent research has argued that
making AI explainable can be a viable way of making AI more useful in
real-world contexts, especially within low-resource domains in the Global
South. While AI has transcended borders, a limited amount of work focuses on
democratizing the concept of explainable AI to the ""majority world"", leaving
much room to explore and develop new approaches within this space that cater to
the distinct needs of users within culturally and socially-diverse regions.
This article introduces the concept of an intercultural ethics approach to AI
explainability. It examines how cultural nuances impact the adoption and use of
technology, the factors that impede how technical concepts such as AI are
explained, and how integrating an intercultural ethics approach in the
development of XAI can improve user understanding and facilitate efficient
usage of these methods.",None,-1
5be47d08-a770-4897-8f84-97d207fd6c93,NumHG: A Dataset for Number-Focused Headline Generation,0.192214,12,"Headline generation, a key task in abstractive summarization, strives to
condense a full-length article into a succinct, single line of text. Notably,
while contemporary encoder-decoder models excel based on the ROUGE metric, they
often falter when it comes to the precise generation of numerals in headlines.
We identify the lack of datasets providing fine-grained annotations for
accurate numeral generation as a major roadblock. To address this, we introduce
a new dataset, the NumHG, and provide over 27,000 annotated numeral-rich news
articles for detailed investigation. Further, we evaluate five well-performing
models from previous headline generation tasks using human evaluation in terms
of numerical accuracy, reasonableness, and readability. Our study reveals a
need for improvement in numerical accuracy, demonstrating the potential of the
NumHG dataset to drive progress in number-focused headline generation and
stimulate further discussions in numeral-focused text generation.",None,-1
941032eb-3691-4991-9b76-eb5fe58354ad,A Semantic Approach to Negation Detection and Word Disambiguation with Natural Language Processing,0.0465908,2,"This study aims to demonstrate the methods for detecting negations in a
sentence by uniquely evaluating the lexical structure of the text via
word-sense disambiguation. The proposed framework examines all the unique
features in the various expressions within a text to resolve the contextual
usage of all tokens and decipher the effect of negation on sentiment analysis.
The application of popular expression detectors skips this important step,
thereby neglecting the root words caught in the web of negation and making text
classification difficult for machine learning and sentiment analysis. This
study adopts the Natural Language Processing (NLP) approach to discover and
antonimize words that were negated for better accuracy in text classification
using a knowledge base provided by an NLP library called WordHoard. Early
results show that our initial analysis improved on traditional sentiment
analysis, which sometimes neglects negations or assigns an inverse polarity
score. The SentiWordNet analyzer was improved by 35%, the Vader analyzer by 20%
and the TextBlob by 6%.",None,-1
d6e9d065-f475-4534-b9b6-f8ffc6d7a72c,DiffColor: Toward High Fidelity Text-Guided Image Colorization with Diffusion Models,0.115417,1,"Recent data-driven image colorization methods have enabled automatic or
reference-based colorization, while still suffering from unsatisfactory and
inaccurate object-level color control. To address these issues, we propose a
new method called DiffColor that leverages the power of pre-trained diffusion
models to recover vivid colors conditioned on a prompt text, without any
additional inputs. DiffColor mainly contains two stages: colorization with
generative color prior and in-context controllable colorization. Specifically,
we first fine-tune a pre-trained text-to-image model to generate colorized
images using a CLIP-based contrastive loss. Then we try to obtain an optimized
text embedding aligning the colorized image and the text prompt, and a
fine-tuned diffusion model enabling high-quality image reconstruction. Our
method can produce vivid and diverse colors with a few iterations, and keep the
structure and background intact while having colors well-aligned with the
target language guidance. Moreover, our method allows for in-context
colorization, i.e., producing different colorization results by modifying
prompt texts without any fine-tuning, and can achieve object-level controllable
colorization results. Extensive experiments and user studies demonstrate that
DiffColor outperforms previous works in terms of visual quality, color
fidelity, and diversity of colorization options.",None,-1
653317ee-54d3-4107-aa04-2a35583fbcbd,ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought,0.857491,13,"Recently Large Language Models (LLMs) have been proven to have strong
abilities in various domains and tasks. We study the problem of prompt
designing in the text-to-SQL task and attempt to improve the LLMs' reasoning
ability when generating SQL queries. Besides the trivial few-shot in-context
learning setting, we design our chain-of-thought (CoT) prompt with a similar
method to schema linking. We provide a method named ACT-SQL to automatically
generate auto-CoT exemplars and thus the whole process doesn't need manual
labeling. Our approach is cost-saving since we only use the LLMs' API call once
when generating one SQL query. Furthermore, we extend our in-context learning
method to the multi-turn text-to-SQL task. The experiment results show that the
LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves
SOTA performance on the Spider dev set among existing in-context learning
approaches.",None,-1
67b09660-08f0-4eae-bc54-80feb21b631d,Distributed Trust Through the Lens of Software Architecture,0.394839,2,"Distributed trust is a nebulous concept that has evolved from different
perspectives in recent years. While one can attribute its current prominence to
blockchain and cryptocurrency, the distributed trust concept has been
cultivating progress in federated learning, trustworthy and responsible AI in
an ecosystem setting, data sharing, privacy issues across organizational
boundaries, and zero trust cybersecurity. This paper will survey the concept of
distributed trust in multiple disciplines. It will take a system/software
architecture point of view to look at trust redistribution/shift and the
associated tradeoffs in systems and applications enabled by distributed trust
technologies.",None,-1
3f66c5bc-4d45-4114-a18a-e462da9de5bc,My Actions Speak Louder Than Your Words: When User Behavior Predicts Their Beliefs about Agents' Attributes,0.142962,1,"An implicit expectation of asking users to rate agents, such as an AI
decision-aid, is that they will use only relevant information -- ask them about
an agent's benevolence, and they should consider whether or not it was kind.
Behavioral science, however, suggests that people sometimes use irrelevant
information. We identify an instance of this phenomenon, where users who
experience better outcomes in a human-agent interaction systematically rated
the agent as having better abilities, being more benevolent, and exhibiting
greater integrity in a post hoc assessment than users who experienced worse
outcome -- which were the result of their own behavior -- with the same agent.
Our analyses suggest the need for augmentation of models so that they account
for such biased perceptions as well as mechanisms so that agents can detect and
even actively work to correct this and similar biases of users.",None,-1
a2d9c51b-39b9-4766-90f2-e1d4172f269f,Joint Optimization of Class-Specific Training- and Test-Time Data Augmentation in Segmentation,0.327231,4,"This paper presents an effective and general data augmentation framework for
medical image segmentation. We adopt a computationally efficient and
data-efficient gradient-based meta-learning scheme to explicitly align the
distribution of training and validation data which is used as a proxy for
unseen test data. We improve the current data augmentation strategies with two
core designs. First, we learn class-specific training-time data augmentation
(TRA) effectively increasing the heterogeneity within the training subsets and
tackling the class imbalance common in segmentation. Second, we jointly
optimize TRA and test-time data augmentation (TEA), which are closely connected
as both aim to align the training and test data distribution but were so far
considered separately in previous works. We demonstrate the effectiveness of
our method on four medical image segmentation tasks across different scenarios
with two state-of-the-art segmentation models, DeepMedic and nnU-Net. Extensive
experimentation shows that the proposed data augmentation framework can
significantly and consistently improve the segmentation performance when
compared to existing solutions. Code is publicly available.",None,-1
a5b37004-49c0-4c47-aac6-8eaea6aadb8e,Grammar Prompting for Domain-Specific Language Generation with Large Language Models,0.484226,16,"Large language models (LLMs) can learn to perform a wide range of natural
language tasks from just a handful of in-context examples. However, for
generating strings from highly structured languages (e.g., semantic parsing to
complex domain-specific languages), it is challenging for the LLM to generalize
from just a few exemplars. We propose \emph{grammar prompting}, a simple
approach to enable LLMs to use external knowledge and domain-specific
constraints, expressed through a grammar in Backus--Naur Form (BNF), during
in-context learning. Grammar prompting augments each demonstration example with
a specialized grammar that is minimally sufficient for generating the
particular output example, where the specialized grammar is a subset of the
full DSL grammar. For inference, the LLM first predicts a BNF grammar given a
test input, and then generates the output according to the rules of the
grammar. Experiments demonstrate that grammar prompting can enable LLMs to
perform competitively on a diverse set of DSL generation tasks, including
semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and
SMILES-based molecule generation.",None,-1
7bbce0af-4295-49e3-b1cf-61c1019c2fd8,Single Frame Semantic Segmentation Using Multi-Modal Spherical Images,0.147475,2,"In recent years, the research community has shown a lot of interest to
panoramic images that offer a 360-degree directional perspective. Multiple data
modalities can be fed, and complimentary characteristics can be utilized for
more robust and rich scene interpretation based on semantic segmentation, to
fully realize the potential. Existing research, however, mostly concentrated on
pinhole RGB-X semantic segmentation. In this study, we propose a
transformer-based cross-modal fusion architecture to bridge the gap between
multi-modal fusion and omnidirectional scene perception. We employ
distortion-aware modules to address extreme object deformations and panorama
distortions that result from equirectangular representation. Additionally, we
conduct cross-modal interactions for feature rectification and information
exchange before merging the features in order to communicate long-range
contexts for bi-modal and tri-modal feature streams. In thorough tests using
combinations of four different modality types in three indoor panoramic-view
datasets, our technique achieved state-of-the-art mIoU performance: 60.60% on
Stanford2D3DS (RGB-HHA), 71.97% Structured3D (RGB-D-N), and 35.92% Matterport3D
(RGB-D). We plan to release all codes and trained models soon.",None,-1
67c0a7b1-af9d-4cb6-9d08-e70141a99012,CrossKD: Cross-Head Knowledge Distillation for Object Detection,0.174006,4,"Knowledge Distillation (KD) has been validated as an effective model
compression technique for learning compact object detectors. Existing
state-of-the-art KD methods for object detection are mostly based on feature
imitation. In this paper, we present a general and effective prediction
mimicking distillation scheme, called CrossKD, which delivers the intermediate
features of the student's detection head to the teacher's detection head. The
resulting cross-head predictions are then forced to mimic the teacher's
predictions. This manner relieves the student's head from receiving
contradictory supervision signals from the annotations and the teacher's
predictions, greatly improving the student's detection performance. Moreover,
as mimicking the teacher's predictions is the target of KD, CrossKD offers more
task-oriented information in contrast with feature imitation. On MS COCO, with
only prediction mimicking losses applied, our CrossKD boosts the average
precision of GFL ResNet-50 with 1x training schedule from 40.2 to 43.7,
outperforming all existing KD methods. In addition, our method also works well
when distilling detectors with heterogeneous backbones. Code is available at
https://github.com/jbwang1997/CrossKD.",None,-1
28bedaf3-7364-4d46-8218-bf4ad9f43d91,What happens before and after: Multi-Event Commonsense in Event Coreference Resolution,0.932627,10,"Event coreference models cluster event mentions pertaining to the same
real-world event. Recent models rely on contextualized representations to
recognize coreference among lexically or contextually similar mentions.
However, models typically fail to leverage commonsense inferences, which is
particularly limiting for resolving lexically-divergent mentions. We propose a
model that extends event mentions with temporal commonsense inferences. Given a
complex sentence with multiple events, e.g., ""The man killed his wife and got
arrested"", with the target event ""arrested"", our model generates plausible
events that happen before the target event - such as ""the police arrived"", and
after it, such as ""he was sentenced"". We show that incorporating such
inferences into an existing event coreference model improves its performance,
and we analyze the coreferences in which such temporal knowledge is required.",None,-1
5e158563-8e6b-4626-907f-a8bb2cfbd0e8,Chebyshev Particles,0.515897,1,"Markov chain Monte Carlo (MCMC) provides a feasible method for inferring
Hidden Markov models, however, it is often computationally prohibitive,
especially constrained by the curse of dimensionality, as the Monte Carlo
sampler traverses randomly taking small steps within uncertain regions in the
parameter space. We are the first to consider the posterior distribution of the
objective as a mapping of samples in an infinite-dimensional Euclidean space
where deterministic submanifolds are embedded and propose a new criterion by
maximizing the weighted Riesz polarization quantity, to discretize rectifiable
submanifolds via pairwise interaction. We study the characteristics of
Chebyshev particles and embed them into sequential MCMC, a novel sampler with a
high acceptance ratio that proposes only a few evaluations. We have achieved
high performance from the experiments for parameter inference in a linear
Gaussian state-space model with synthetic data and a non-linear stochastic
volatility model with real-world data.",None,-1
2590fc82-2dd3-4372-9031-5611f6ab187b,DAG Learning on the Permutahedron,0.890169,9,"We propose a continuous optimization framework for discovering a latent
directed acyclic graph (DAG) from observational data. Our approach optimizes
over the polytope of permutation vectors, the so-called Permutahedron, to learn
a topological ordering. Edges can be optimized jointly, or learned conditional
on the ordering via a non-differentiable subroutine. Compared to existing
continuous optimization approaches our formulation has a number of advantages
including: 1. validity: optimizes over exact DAGs as opposed to other
relaxations optimizing approximate DAGs; 2. modularity: accommodates any
edge-optimization procedure, edge structural parameterization, and optimization
loss; 3. end-to-end: either alternately iterates between node-ordering and
edge-optimization, or optimizes them jointly. We demonstrate, on real-world
data problems in protein-signaling and transcriptional network discovery, that
our approach lies on the Pareto frontier of two key metrics, the SID and SHD.",None,-1
f717b78c-d351-4d76-9d92-d00369029fc4,Learning Topology-Preserving Data Representations,0.841901,8,"We propose a method for learning topology-preserving data representations
(dimensionality reduction). The method aims to provide topological similarity
between the data manifold and its latent representation via enforcing the
similarity in topological features (clusters, loops, 2D voids, etc.) and their
localization. The core of the method is the minimization of the Representation
Topology Divergence (RTD) between original high-dimensional data and
low-dimensional representation in latent space. RTD minimization provides
closeness in topological features with strong theoretical guarantees. We
develop a scheme for RTD differentiation and apply it as a loss term for the
autoencoder. The proposed method ""RTD-AE"" better preserves the global structure
and topology of the data manifold than state-of-the-art competitors as measured
by linear correlation, triplet distance ranking accuracy, and Wasserstein
distance between persistence barcodes.",None,-1
f50c414a-922d-4668-9ee6-27eac91d4839,Revisiting Large Language Models as Zero-shot Relation Extractors,0.882441,9,"Relation extraction (RE) consistently involves a certain degree of labeled or
unlabeled data even if under zero-shot setting. Recent studies have shown that
large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt, which provides the possibility of extracting
relations from text without any data and parameter tuning. This work focuses on
the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.
On the one hand, we analyze the drawbacks of existing RE prompts and attempt to
incorporate recent prompt techniques such as chain-of-thought (CoT) to improve
zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a
simple prompt recursively using LLMs to transform RE inputs to the effective
question answering (QA) format. On the other hand, we conduct comprehensive
experiments on various benchmarks and settings to investigate the capabilities
of LLMs on zero-shot RE. Specifically, we have the following findings: (i)
\textsc{SumAsk} consistently and significantly improves LLMs performance on
different model sizes, benchmarks and settings; (ii) Zero-shot prompting with
ChatGPT achieves competitive or superior results compared with zero-shot and
fully supervised methods; (iii) LLMs deliver promising performance in
extracting overlapping relations; (iv) The performance varies greatly regarding
different relations. Different from small language models, LLMs are effective
in handling challenge none-of-the-above (NoTA) relation.",None,-1
2eb31c42-c179-446c-bf77-2d388058da7f,History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System,0.74007,10,"With the evolution of pre-trained language models, current open-domain
dialogue systems have achieved great progress in conducting one-session
conversations. In contrast, Multi-Session Conversation (MSC), which consists of
multiple sessions over a long term with the same user, is under-investigated.
In this paper, we propose History-Aware Hierarchical Transformer (HAHT) for
multi-session open-domain dialogue. HAHT maintains a long-term memory of
history conversations and utilizes history information to understand current
conversation context and generate well-informed and context-relevant responses.
Specifically, HAHT first encodes history conversation sessions hierarchically
into a history memory. Then, HAHT leverages historical information to
facilitate the understanding of the current conversation context by encoding
the history memory together with the current context with attention-based
mechanisms. Finally, to explicitly utilize historical information, HAHT uses a
history-aware response generator that switches between a generic vocabulary and
a history-aware vocabulary. Experimental results on a large-scale MSC dataset
suggest that the proposed HAHT model consistently outperforms baseline models.
Human evaluation results support that HAHT generates more human-like,
context-relevant and history-relevant responses than baseline models.",None,-1
d3c40a43-db00-40aa-9b1e-5fc4fe75b494,A request for clarity over the End of Sequence token in the Self-Critical Sequence Training,0.0643003,3,"The Image Captioning research field is currently compromised by the lack of
transparency and awareness over the End-of-Sequence token (<Eos>) in the
Self-Critical Sequence Training. If the <Eos> token is omitted, a model can
boost its performance up to +4.1 CIDEr-D using trivial sentence fragments.
While this phenomenon poses an obstacle to a fair evaluation and comparison of
established works, people involved in new projects are given the arduous choice
between lower scores and unsatisfactory descriptions due to the competitive
nature of the research. This work proposes to solve the problem by spreading
awareness of the issue itself. In particular, we invite future works to share a
simple and informative signature with the help of a library called SacreEOS.
Code available at
\emph{\href{https://github.com/jchenghu/sacreeos}{https://github.com/jchenghu/sacreeos}}",None,-1
3cef9d68-b846-44b4-b3cf-e593becc458c,Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation,0.592189,9,"Cross-modal contrastive learning in vision language pretraining (VLP) faces
the challenge of (partial) false negatives. In this paper, we study this
problem from the perspective of Mutual Information (MI) optimization. It is
common sense that InfoNCE loss used in contrastive learning will maximize the
lower bound of MI between anchors and their positives, while we theoretically
prove that MI involving negatives also matters when noises commonly exist.
Guided by a more general lower bound form for optimization, we propose a
contrastive learning strategy regulated by progressively refined cross-modal
similarity, to more accurately optimize MI between an image/text anchor and its
negative texts/images instead of improperly minimizing it. Our method performs
competitively on four downstream cross-modal tasks and systematically balances
the beneficial and harmful effects of (partial) false negative samples under
theoretical guidance.",None,-1
e7f1cacc-ae55-402e-9beb-639b8a939d59,PINNs-Based Uncertainty Quantification for Transient Stability Analysis,0.28507,1,"This paper addresses the challenge of transient stability in power systems
with missing parameters and uncertainty propagation in swing equations. We
introduce a novel application of Physics-Informed Neural Networks (PINNs),
specifically an Ensemble of PINNs (E-PINNs), to estimate critical parameters
like rotor angle and inertia coefficient with enhanced accuracy and reduced
computational load. E-PINNs capitalize on the underlying physical principles of
swing equations to provide a robust solution. Our approach not only facilitates
efficient parameter estimation but also quantifies uncertainties, delivering
probabilistic insights into the system behavior. The efficacy of E-PINNs is
demonstrated through the analysis of $1$-bus and $2$-bus systems, highlighting
the model's ability to handle parameter variability and data scarcity. The
study advances the application of machine learning in power system stability,
paving the way for reliable and computationally efficient transient stability
analysis.",None,-1
e4ca6a93-2bef-490f-9dd1-d3680396b23c,Fine-grained Affective Processing Capabilities Emerging from Large Language Models,0.364653,3,"Large language models, in particular generative pre-trained transformers
(GPTs), show impressive results on a wide variety of language-related tasks. In
this paper, we explore ChatGPT's zero-shot ability to perform affective
computing tasks using prompting alone. We show that ChatGPT a) performs
meaningful sentiment analysis in the Valence, Arousal and Dominance dimensions,
b) has meaningful emotion representations in terms of emotion categories and
these affective dimensions, and c) can perform basic appraisal-based emotion
elicitation of situations based on a prompt-based computational implementation
of the OCC appraisal model. These findings are highly relevant: First, they
show that the ability to solve complex affect processing tasks emerges from
language-based token prediction trained on extensive data sets. Second, they
show the potential of large language models for simulating, processing and
analyzing human emotions, which has important implications for various
applications such as sentiment analysis, socially interactive agents, and
social robotics.",None,-1
b74ccc49-0c08-4448-9df1-587a3f55c740,Adaptive Sparse Pairwise Loss for Object Re-Identification,0.874729,15,"Object re-identification (ReID) aims to find instances with the same identity
as the given probe from a large gallery. Pairwise losses play an important role
in training a strong ReID network. Existing pairwise losses densely exploit
each instance as an anchor and sample its triplets in a mini-batch. This dense
sampling mechanism inevitably introduces positive pairs that share few visual
similarities, which can be harmful to the training. To address this problem, we
propose a novel loss paradigm termed Sparse Pairwise (SP) loss that only
leverages few appropriate pairs for each class in a mini-batch, and empirically
demonstrate that it is sufficient for the ReID tasks. Based on the proposed
loss framework, we propose an adaptive positive mining strategy that can
dynamically adapt to diverse intra-class variations. Extensive experiments show
that SP loss and its adaptive variant AdaSP loss outperform other pairwise
losses, and achieve state-of-the-art performance across several ReID
benchmarks. Code is available at https://github.com/Astaxanthin/AdaSP.",None,-1
a62f2d49-7c2d-4f6a-8098-23b49f12eea6,"RGB-D-Based Categorical Object Pose and Shape Estimation: Methods, Datasets, and Evaluation",0.341006,3,"Recently, various methods for 6D pose and shape estimation of objects at a
per-category level have been proposed. This work provides an overview of the
field in terms of methods, datasets, and evaluation protocols. First, an
overview of existing works and their commonalities and differences is provided.
Second, we take a critical look at the predominant evaluation protocol,
including metrics and datasets. Based on the findings, we propose a new set of
metrics, contribute new annotations for the Redwood dataset, and evaluate
state-of-the-art methods in a fair comparison. The results indicate that
existing methods do not generalize well to unconstrained orientations and are
actually heavily biased towards objects being upright. We provide an
easy-to-use evaluation toolbox with well-defined metrics, methods, and dataset
interfaces, which allows evaluation and comparison with various
state-of-the-art approaches
(https://github.com/roym899/pose_and_shape_evaluation).",None,-1
b5578b37-206b-4253-bd0a-71c1c895e7fc,Leveraging Large Language Model for Automatic Evolving of Industrial Data-Centric R&D Cycle,0.573803,1,"In the wake of relentless digital transformation, data-driven solutions are
emerging as powerful tools to address multifarious industrial tasks such as
forecasting, anomaly detection, planning, and even complex decision-making.
Although data-centric R&D has been pivotal in harnessing these solutions, it
often comes with significant costs in terms of human, computational, and time
resources. This paper delves into the potential of large language models (LLMs)
to expedite the evolution cycle of data-centric R&D. Assessing the foundational
elements of data-centric R&D, including heterogeneous task-related data,
multi-facet domain knowledge, and diverse computing-functional tools, we
explore how well LLMs can understand domain-specific requirements, generate
professional ideas, utilize domain-specific tools to conduct experiments,
interpret results, and incorporate knowledge from past endeavors to tackle new
challenges. We take quantitative investment research as a typical example of
industrial data-centric R&D scenario and verified our proposed framework upon
our full-stack open-sourced quantitative research platform Qlib and obtained
promising results which shed light on our vision of automatic evolving of
industrial data-centric R&D cycle.",None,-1
1020c957-d403-4c82-abc0-aeca107d14b3,Image To Tree with Recursive Prompting,0.0770791,1,"Extracting complex structures from grid-based data is a common key step in
automated medical image analysis. The conventional solution to recovering
tree-structured geometries typically involves computing the minimal cost path
through intermediate representations derived from segmentation masks. However,
this methodology has significant limitations in the context of projective
imaging of tree-structured 3D anatomical data such as coronary arteries, since
there are often overlapping branches in the 2D projection. In this work, we
propose a novel approach to predicting tree connectivity structure which
reformulates the task as an optimization problem over individual steps of a
recursive process. We design and train a two-stage model which leverages the
UNet and Transformer architectures and introduces an image-based prompting
technique. Our proposed method achieves compelling results on a pair of
synthetic datasets, and outperforms a shortest-path baseline.",None,-1
33915ddc-36a4-42fc-91ae-12cd27899556,MedChatZH: a Better Medical Adviser Learns from Better Instructions,0.753541,4,"Generative large language models (LLMs) have shown great success in various
applications, including question-answering (QA) and dialogue systems. However,
in specialized domains like traditional Chinese medical QA, these models may
perform unsatisfactorily without fine-tuning on domain-specific datasets. To
address this, we introduce MedChatZH, a dialogue model designed specifically
for traditional Chinese medical QA. Our model is pre-trained on Chinese
traditional medical books and fine-tuned with a carefully curated medical
instruction dataset. It outperforms several solid baselines on a real-world
medical dialogue dataset. We release our model, code, and dataset on
https://github.com/tyang816/MedChatZH to facilitate further research in the
domain of traditional Chinese medicine and LLMs.",None,-1
45ced2ce-6725-41c0-9732-63fada95b4c2,SwinDocSegmenter: An End-to-End Unified Domain Adaptive Transformer for Document Instance Segmentation,0.734345,10,"Instance-level segmentation of documents consists in assigning a class-aware
and instance-aware label to each pixel of the image. It is a key step in
document parsing for their understanding. In this paper, we present a unified
transformer encoder-decoder architecture for en-to-end instance segmentation of
complex layouts in document images. The method adapts a contrastive training
with a mixed query selection for anchor initialization in the decoder. Later
on, it performs a dot product between the obtained query embeddings and the
pixel embedding map (coming from the encoder) for semantic reasoning. Extensive
experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical
Japanese (HJ), and TableBank demonstrate that our model with SwinL backbone
achieves better segmentation performance than the existing state-of-the-art
approaches with the average precision of \textbf{93.72}, \textbf{54.39},
\textbf{84.65} and \textbf{98.04} respectively under one billion parameters.
The code is made publicly available at:
\href{https://github.com/ayanban011/SwinDocSegmenter}{github.com/ayanban011/SwinDocSegmenter}",None,-1
68213494-5317-420c-9767-b561bf2488a5,Watch Your Steps: Local Image and Scene Editing by Text Instructions,0.612898,15,"Denoising diffusion models have enabled high-quality image generation and
editing. We present a method to localize the desired edit region implicit in a
text instruction. We leverage InstructPix2Pix (IP2P) and identify the
discrepancy between IP2P predictions with and without the instruction. This
discrepancy is referred to as the relevance map. The relevance map conveys the
importance of changing each pixel to achieve the edits, and is used to to guide
the modifications. This guidance ensures that the irrelevant pixels remain
unchanged. Relevance maps are further used to enhance the quality of
text-guided editing of 3D scenes in the form of neural radiance fields. A field
is trained on relevance maps of training views, denoted as the relevance field,
defining the 3D region within which modifications should be made. We perform
iterative updates on the training views guided by rendered relevance maps from
the relevance field. Our method achieves state-of-the-art performance on both
image and NeRF editing tasks. Project page:
https://ashmrz.github.io/WatchYourSteps/",None,-1
8a851c61-cd29-49a2-af61-0ca411479a84,Give and Take: Federated Transfer Learning for Industrial IoT Network Intrusion Detection,0.958766,6,"The rapid growth in Internet of Things (IoT) technology has become an
integral part of today's industries forming the Industrial IoT (IIoT)
initiative, where industries are leveraging IoT to improve communication and
connectivity via emerging solutions like data analytics and cloud computing.
Unfortunately, the rapid use of IoT has made it an attractive target for
cybercriminals. Therefore, protecting these systems is of utmost importance. In
this paper, we propose a federated transfer learning (FTL) approach to perform
IIoT network intrusion detection. As part of the research, we also propose a
combinational neural network as the centerpiece for performing FTL. The
proposed technique splits IoT data between the client and server devices to
generate corresponding models, and the weights of the client models are
combined to update the server model. Results showcase high performance for the
FTL setup between iterations on both the IIoT clients and the server.
Additionally, the proposed FTL setup achieves better overall performance than
contemporary machine learning algorithms at performing network intrusion
detection.",None,-1
fc961fcd-a44e-4740-bbfb-8dd5d95c218f,On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection,0.29365,2,"Detecting adversarial samples that are carefully crafted to fool the model is
a critical step to socially-secure applications. However, existing adversarial
detection methods require access to sufficient training data, which brings
noteworthy concerns regarding privacy leakage and generalizability. In this
work, we validate that the adversarial sample generated by attack algorithms is
strongly related to a specific vector in the high-dimensional inputs. Such
vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated
without original training data. Based on this discovery, we propose a
data-agnostic adversarial detection framework, which induces different
responses between normal and adversarial samples to UAPs. Experimental results
show that our method achieves competitive detection performance on various text
classification tasks, and maintains an equivalent time consumption to normal
inference.",None,-1
5b2c99ad-54a8-439e-a617-19e34fab44da,Improved Visual Story Generation with Adaptive Context Modeling,0.307118,7,"Diffusion models developed on top of powerful text-to-image generation models
like Stable Diffusion achieve remarkable success in visual story generation.
However, the best-performing approach considers historically generated results
as flattened memory cells, ignoring the fact that not all preceding images
contribute equally to the generation of the characters and scenes at the
current stage. To address this, we present a simple method that improves the
leading system with adaptive context modeling, which is not only incorporated
in the encoder but also adopted as additional guidance in the sampling stage to
boost the global consistency of the generated story. We evaluate our model on
PororoSV and FlintstonesSV datasets and show that our approach achieves
state-of-the-art FID scores on both story visualization and continuation
scenarios. We conduct detailed model analysis and show that our model excels at
generating semantically consistent images for stories.",None,-1
6b84f318-7a41-4990-bedd-ad95192f18da,BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?,0.0899171,2,"Language models have seen significant growth in the size of their corpus,
leading to notable performance improvements. Yet, there has been limited
progress in developing models that handle smaller, more human-like datasets. As
part of the BabyLM shared task, this study explores the impact of reinforcement
learning from human feedback (RLHF) on language models pretrained from scratch
with a limited training corpus. Comparing two GPT-2 variants, the larger model
performs better in storytelling tasks after RLHF fine-tuning. These findings
suggest that RLHF techniques may be more advantageous for larger models due to
their higher learning and adaptation capacity, though more experiments are
needed to confirm this finding. These insights highlight the potential benefits
of RLHF fine-tuning for language models within limited data, enhancing their
ability to maintain narrative focus and coherence while adhering better to
initial instructions in storytelling tasks. The code for this work is publicly
at https://github.com/Zephyr1022/BabyStories-UTSA.",None,-1
3ab7eb7e-6033-4b5f-9f6b-9fe2e9f7ec3c,Camoscio: an Italian Instruction-tuned LLaMA,0.277632,12,"In recent years Large Language Models (LLMs) have increased the state of the
art on several natural language processing tasks. However, their accessibility
is often limited to paid API services, posing challenges for researchers in
conducting extensive investigations. On the other hand, while some open-source
models have been proposed by the community, they are typically English-centric
or multilingual without a specific adaptation for the Italian language. In an
effort to democratize the available and open resources for the Italian
language, in this paper we introduce Camoscio: a language model specifically
tuned to follow users' prompts in Italian. Specifically, we finetuned the
smallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts
translated to Italian via ChatGPT. Results indicate that the model's zero-shot
performance on various downstream tasks in Italian competes favorably with
existing models specifically finetuned for those tasks. All the artifacts
(code, dataset, model) are released to the community at the following url:
https://github.com/teelinsan/camoscio",None,-1
1a076b3b-6af7-4371-832e-b5eb40cb4851,Multi-Scale Prototypical Transformer for Whole Slide Image Classification,0.837679,6,"Whole slide image (WSI) classification is an essential task in computational
pathology. Despite the recent advances in multiple instance learning (MIL) for
WSI classification, accurate classification of WSIs remains challenging due to
the extreme imbalance between the positive and negative instances in bags, and
the complicated pre-processing to fuse multi-scale information of WSI. To this
end, we propose a novel multi-scale prototypical Transformer (MSPT) for WSI
classification, which includes a prototypical Transformer (PT) module and a
multi-scale feature fusion module (MFFM). The PT is developed to reduce
redundant instances in bags by integrating prototypical learning into the
Transformer architecture. It substitutes all instances with cluster prototypes,
which are then re-calibrated through the self-attention mechanism of the
Trans-former. Thereafter, an MFFM is proposed to fuse the clustered prototypes
of different scales, which employs MLP-Mixer to enhance the information
communication between prototypes. The experimental results on two public WSI
datasets demonstrate that the proposed MSPT outperforms all the compared
algorithms, suggesting its potential applications.",None,-1
35d51693-927a-4972-9608-641df083ceba,Audio Visual Language Maps for Robot Navigation,0.78493,17,"While interacting in the world is a multi-sensory experience, many robots
continue to predominantly rely on visual perception to map and navigate in
their environments. In this work, we propose Audio-Visual-Language Maps
(AVLMaps), a unified 3D spatial map representation for storing cross-modal
information from audio, visual, and language cues. AVLMaps integrate the
open-vocabulary capabilities of multimodal foundation models pre-trained on
Internet-scale data by fusing their features into a centralized 3D voxel grid.
In the context of navigation, we show that AVLMaps enable robot systems to
index goals in the map based on multimodal queries, e.g., textual descriptions,
images, or audio snippets of landmarks. In particular, the addition of audio
information enables robots to more reliably disambiguate goal locations.
Extensive experiments in simulation show that AVLMaps enable zero-shot
multimodal goal navigation from multimodal prompts and provide 50% better
recall in ambiguous scenarios. These capabilities extend to mobile robots in
the real world - navigating to landmarks referring to visual, audio, and
spatial concepts. Videos and code are available at: https://avlmaps.github.io.",None,-1
778789cc-3f6b-4e6a-aad6-2698dae7590b,Toward Open-ended Embodied Tasks Solving,0.640492,2,"Empowering embodied agents, such as robots, with Artificial Intelligence (AI)
has become increasingly important in recent years. A major challenge is task
open-endedness. In practice, robots often need to perform tasks with novel
goals that are multifaceted, dynamic, lack a definitive ""end-state"", and were
not encountered during training. To tackle this problem, this paper introduces
\textit{Diffusion for Open-ended Goals} (DOG), a novel framework designed to
enable embodied AI to plan and act flexibly and dynamically for open-ended task
goals. DOG synergizes the generative prowess of diffusion models with
state-of-the-art, training-free guidance techniques to adaptively perform
online planning and control. Our evaluations demonstrate that DOG can handle
various kinds of novel task goals not seen during training, in both maze
navigation and robot control problems. Our work sheds light on enhancing
embodied AI's adaptability and competency in tackling open-ended goals.",None,-1
7dfbc9bd-9d35-408b-9afe-348086c89f7c,Exploring the Constructicon: Linguistic Analysis of a Computational CxG,0.474478,3,"Recent work has formulated the task for computational construction grammar as
producing a constructicon given a corpus of usage. Previous work has evaluated
these unsupervised grammars using both internal metrics (for example, Minimum
Description Length) and external metrics (for example, performance on a
dialectology task). This paper instead takes a linguistic approach to
evaluation, first learning a constructicon and then analyzing its contents from
a linguistic perspective. This analysis shows that a learned constructicon can
be divided into nine major types of constructions, of which Verbal and Nominal
are the most common. The paper also shows that both the token and type
frequency of constructions can be used to model variation across registers and
dialects.",None,-1
1aeaa12d-7982-4763-9396-7f48ae25b1ce,ACC-UNet: A Completely Convolutional UNet model for the 2020s,0.726884,10,"This decade is marked by the introduction of Vision Transformer, a radical
paradigm shift in broad computer vision. A similar trend is followed in medical
imaging, UNet, one of the most influential architectures, has been redesigned
with transformers. Recently, the efficacy of convolutional models in vision is
being reinvestigated by seminal works such as ConvNext, which elevates a ResNet
to Swin Transformer level. Deriving inspiration from this, we aim to improve a
purely convolutional UNet model so that it can be on par with the
transformer-based models, e.g, Swin-Unet or UCTransNet. We examined several
advantages of the transformer-based UNet models, primarily long-range
dependencies and cross-level skip connections. We attempted to emulate them
through convolution operations and thus propose, ACC-UNet, a completely
convolutional UNet model that brings the best of both worlds, the inherent
inductive biases of convnets with the design decisions of transformers.
ACC-UNet was evaluated on 5 different medical image segmentation benchmarks and
consistently outperformed convnets, transformers, and their hybrids. Notably,
ACC-UNet outperforms state-of-the-art models Swin-Unet and UCTransNet by $2.64
\pm 2.54\%$ and $0.45 \pm 1.61\%$ in terms of dice score, respectively, while
using a fraction of their parameters ($59.26\%$ and $24.24\%$). Our codes are
available at https://github.com/kiharalab/ACC-UNet.",None,-1
f48d87d9-adad-4def-ac54-add49ff90c75,INO at Factify 2: Structure Coherence based Multi-Modal Fact Verification,0.586227,4,"This paper describes our approach to the multi-modal fact verification
(FACTIFY) challenge at AAAI2023. In recent years, with the widespread use of
social media, fake news can spread rapidly and negatively impact social
security. Automatic claim verification becomes more and more crucial to combat
fake news. In fact verification involving multiple modal data, there should be
a structural coherence between claim and document. Therefore, we proposed a
structure coherence-based multi-modal fact verification scheme to classify fake
news. Our structure coherence includes the following four aspects: sentence
length, vocabulary similarity, semantic similarity, and image similarity.
Specifically, CLIP and Sentence BERT are combined to extract text features, and
ResNet50 is used to extract image features. In addition, we also extract the
length of the text as well as the lexical similarity. Then the features were
concatenated and passed through the random forest classifier. Finally, our
weighted average F1 score has reached 0.8079, achieving 2nd place in FACTIFY2.",None,-1
db60f15d-a654-4627-8c23-49e5f5fce5ad,Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport,0.230818,14,"Optimal Transport (OT) problem investigates a transport map that bridges two
distributions while minimizing a given cost function. In this regard, OT
between tractable prior distribution and data has been utilized for generative
modeling tasks. However, OT-based methods are susceptible to outliers and face
optimization challenges during training. In this paper, we propose a novel
generative model based on the semi-dual formulation of Unbalanced Optimal
Transport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution
matching. This approach provides better robustness against outliers, stability
during training, and faster convergence. We validate these properties
empirically through experiments. Moreover, we study the theoretical upper-bound
of divergence between distributions in UOT. Our model outperforms existing
OT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 6.36
on CelebA-HQ-256. The code is available at
\url{https://github.com/Jae-Moo/UOTM}.",None,-1
bf39bf12-668c-4c6a-90ee-6a20dcdc7f40,Prefix Propagation: Parameter-Efficient Tuning for Long Sequences,0.326133,7,"Parameter-efficient tuning aims to mitigate the large memory requirements of
adapting pretrained language models for downstream tasks. For example, one
popular method, prefix-tuning, prepends trainable tokens to sequences while
freezing the rest of the model's parameters. Although such models attain
comparable performance with fine-tuning when applied to sequences with short to
moderate lengths, we show their inferior performance when modelling long
sequences. To bridge this gap, we propose prefix-propagation, a simple but
effective approach that conditions prefixes on previous hidden states. We
empirically demonstrate that prefix-propagation outperforms prefix-tuning
across long-document tasks, while using 50% fewer parameters. To further
investigate the proposed architecture, we also show its advantage in
calibration, and perform additional study on its relationship with kernel
attention. To the best of our knowledge, this work is the first to focus on
parameter-efficient learning for long-sequence language tasks.",None,-1
66e1b778-d1b5-43a5-9981-ae45b81afd49,CollabKG: A Learnable Human-Machine-Cooperative Information Extraction Toolkit for (Event) Knowledge Graph Construction,0.294292,1,"In order to construct or extend entity-centric and event-centric knowledge
graphs (KG and EKG), the information extraction (IE) annotation toolkit is
essential. However, existing IE toolkits have several non-trivial problems,
such as not supporting multi-tasks, not supporting automatic updates. In this
work, we present CollabKG, a learnable human-machine-cooperative IE toolkit for
KG and EKG construction. Specifically, for the multi-task issue, CollabKG
unifies different IE subtasks, including named entity recognition (NER),
entity-relation triple extraction (RE), and event extraction (EE), and supports
both KG and EKG. Then, combining advanced prompting-based IE technology, the
human-machine-cooperation mechanism with LLMs as the assistant machine is
presented which can provide a lower cost as well as a higher performance.
Lastly, owing to the two-way interaction between the human and machine,
CollabKG with learning ability allows self-renewal. Besides, CollabKG has
several appealing features (e.g., customization, training-free, propagation,
etc.) that make the system powerful, easy-to-use, and high-productivity. We
holistically compare our toolkit with other existing tools on these features.
Human evaluation quantitatively illustrates that CollabKG significantly
improves annotation quality, efficiency, and stability simultaneously.",None,-1
54eeb0fb-0efe-4c18-93cf-c89783ea3afd,The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions,0.333486,11,"Recent progress in Large Language Models (LLMs) has produced models that
exhibit remarkable performance across a variety of NLP tasks. However, it
remains unclear whether the existing focus of NLP research accurately captures
the genuine requirements of human users. This paper provides a comprehensive
analysis of the divergence between current NLP research and the needs of
real-world NLP applications via a large-scale collection of user-GPT
conversations. We analyze a large-scale collection of real user queries to GPT.
We compare these queries against existing NLP benchmark tasks and identify a
significant gap between the tasks that users frequently request from LLMs and
the tasks that are commonly studied in academic research. For example, we find
that tasks such as ``design'' and ``planning'' are prevalent in user
interactions but are largely neglected or different from traditional NLP
benchmarks. We investigate these overlooked tasks, dissect the practical
challenges they pose, and provide insights toward a roadmap to make LLMs better
aligned with user needs.",None,-1
e5795310-d0f2-4553-9ddf-3ef67954f5d1,Drafting Event Schemas using Language Models,0.819449,5,"Past work has studied event prediction and event language modeling, sometimes
mediated through structured representations of knowledge in the form of event
schemas. Such schemas can lead to explainable predictions and forecasting of
unseen events given incomplete information. In this work, we look at the
process of creating such schemas to describe complex events. We use large
language models (LLMs) to draft schemas directly in natural language, which can
be further refined by human curators as necessary. Our focus is on whether we
can achieve sufficient diversity and recall of key events and whether we can
produce the schemas in a sufficiently descriptive style. We show that large
language models are able to achieve moderate recall against schemas taken from
two different datasets, with even better results when multiple prompts and
multiple samples are combined. Moreover, we show that textual entailment
methods can be used for both matching schemas to instances of events as well as
evaluating overlap between gold and predicted schemas. Our method paves the way
for easier distillation of event knowledge from large language model into
schemas.",None,-1
70753f64-def9-4184-a717-d7b1ae48cd71,To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support,0.757846,6,"Optimizing the phrasing of argumentative text is crucial in higher education
and professional development. However, assessing whether and how the different
claims in a text should be revised is a hard task, especially for novice
writers. In this work, we explore the main challenges to identifying
argumentative claims in need of specific revisions. By learning from
collaborative editing behaviors in online debates, we seek to capture implicit
revision patterns in order to develop approaches aimed at guiding writers in
how to further improve their arguments. We systematically compare the ability
of common word embedding models to capture the differences between different
versions of the same text, and we analyze their impact on various types of
writing issues. To deal with the noisy nature of revision-based corpora, we
propose a new sampling strategy based on revision distance. Opposed to
approaches from prior work, such sampling can be done without employing
additional annotations and judgments. Moreover, we provide evidence that using
contextual information and domain knowledge can further improve prediction
results. How useful a certain type of context is, depends on the issue the
claim is suffering from, though.",None,-1
5532f357-9e57-4aeb-86c0-a2ee5d22f2f1,ALAN: Autonomously Exploring Robotic Agents in the Real World,0.781307,14,"Robotic agents that operate autonomously in the real world need to
continuously explore their environment and learn from the data collected, with
minimal human supervision. While it is possible to build agents that can learn
in such a manner without supervision, current methods struggle to scale to the
real world. Thus, we propose ALAN, an autonomously exploring robotic agent,
that can perform tasks in the real world with little training and interaction
time. This is enabled by measuring environment change, which reflects object
movement and ignores changes in the robot position. We use this metric directly
as an environment-centric signal, and also maximize the uncertainty of
predicted environment change, which provides agent-centric exploration signal.
We evaluate our approach on two different real-world play kitchen settings,
enabling a robot to efficiently explore and discover manipulation skills, and
perform tasks specified via goal images. Website at
https://robo-explorer.github.io/",None,-1
9e92a83b-bc18-42f3-b012-c3f3dd2ab39c,Control invariant set enhanced reinforcement learning for process control: improved sampling efficiency and guaranteed stability,0.0535544,1,"Reinforcement learning (RL) is an area of significant research interest, and
safe RL in particular is attracting attention due to its ability to handle
safety-driven constraints that are crucial for real-world applications of RL
algorithms. This work proposes a novel approach to RL training, called control
invariant set (CIS) enhanced RL, which leverages the benefits of CIS to improve
stability guarantees and sampling efficiency. The approach consists of two
learning stages: offline and online. In the offline stage, CIS is incorporated
into the reward design, initial state sampling, and state reset procedures. In
the online stage, RL is retrained whenever the state is outside of CIS, which
serves as a stability criterion. A backup table that utilizes the explicit form
of CIS is obtained to ensure the online stability. To evaluate the proposed
approach, we apply it to a simulated chemical reactor. The results show a
significant improvement in sampling efficiency during offline training and
closed-loop stability in the online implementation.",None,-1
98625448-c1a0-4ef4-b4ad-c2f029c12733,An End-to-End Multi-Task Learning Model for Image-based Table Recognition,0.522754,7,"Image-based table recognition is a challenging task due to the diversity of
table styles and the complexity of table structures. Most of the previous
methods focus on a non-end-to-end approach which divides the problem into two
separate sub-problems: table structure recognition; and cell-content
recognition and then attempts to solve each sub-problem independently using two
separate systems. In this paper, we propose an end-to-end multi-task learning
model for image-based table recognition. The proposed model consists of one
shared encoder, one shared decoder, and three separate decoders which are used
for learning three sub-tasks of table recognition: table structure recognition,
cell detection, and cell-content recognition. The whole system can be easily
trained and inferred in an end-to-end approach. In the experiments, we evaluate
the performance of the proposed model on two large-scale datasets: FinTabNet
and PubTabNet. The experiment results show that the proposed model outperforms
the state-of-the-art methods in all benchmark datasets.",None,-1
f03d2369-bd4c-4927-9649-26132dd9b485,Crosslingual Retrieval Augmented In-context Learning for Bangla,0.625468,4,"The promise of Large Language Models (LLMs) in Natural Language Processing
has often been overshadowed by their limited performance in low-resource
languages such as Bangla. To address this, our paper presents a pioneering
approach that utilizes cross-lingual retrieval augmented in-context learning.
By strategically sourcing semantically similar prompts from high-resource
language, we enable multilingual pretrained language models (MPLMs), especially
the generative model BLOOMZ, to successfully boost performance on Bangla tasks.
Our extensive evaluation highlights that the cross-lingual retrieval augmented
prompts bring steady improvements to MPLMs over the zero-shot performance.",None,-1
8252faae-51c0-4e6e-9cb6-f704fcaf999e,Data-Efficient Protein 3D Geometric Pretraining via Refinement of Diffused Protein Structure Decoy,0.381381,11,"Learning meaningful protein representation is important for a variety of
biological downstream tasks such as structure-based drug design. Having
witnessed the success of protein sequence pretraining, pretraining for
structural data which is more informative has become a promising research
topic. However, there are three major challenges facing protein structure
pretraining: insufficient sample diversity, physically unrealistic modeling,
and the lack of protein-specific pretext tasks. To try to address these
challenges, we present the 3D Geometric Pretraining. In this paper, we propose
a unified framework for protein pretraining and a 3D geometric-based,
data-efficient, and protein-specific pretext task: RefineDiff (Refine the
Diffused Protein Structure Decoy). After pretraining our geometric-aware model
with this task on limited data(less than 1% of SOTA models), we obtained
informative protein representations that can achieve comparable performance for
various downstream tasks.",None,-1
bd606608-4810-4c42-8caf-a68ccfde1706,Learning Expressive Prompting With Residuals for Vision Transformers,0.303767,9,"Prompt learning is an efficient approach to adapt transformers by inserting
learnable set of parameters into the input and intermediate representations of
a pre-trained model. In this work, we present Expressive Prompts with Residuals
(EXPRES) which modifies the prompt learning paradigm specifically for effective
adaptation of vision transformers (ViT). Out method constructs downstream
representations via learnable ``output'' tokens, that are akin to the learned
class tokens of the ViT. Further for better steering of the downstream
representation processed by the frozen transformer, we introduce residual
learnable tokens that are added to the output of various computations. We apply
EXPRES for image classification, few shot learning, and semantic segmentation,
and show our method is capable of achieving state of the art prompt tuning on
3/3 categories of the VTAB benchmark. In addition to strong performance, we
observe that our approach is an order of magnitude more prompt efficient than
existing visual prompting baselines. We analytically show the computational
benefits of our approach over weight space adaptation techniques like
finetuning. Lastly we systematically corroborate the architectural design of
our method via a series of ablation experiments.",None,-1
7cda2ccf-8ca4-4353-8c47-68cc2c85ab82,Action Capsules: Human Skeleton Action Recognition,0.292164,3,"Due to the compact and rich high-level representations offered,
skeleton-based human action recognition has recently become a highly active
research topic. Previous studies have demonstrated that investigating joint
relationships in spatial and temporal dimensions provides effective information
critical to action recognition. However, effectively encoding global
dependencies of joints during spatio-temporal feature extraction is still
challenging. In this paper, we introduce Action Capsule which identifies
action-related key joints by considering the latent correlation of joints in a
skeleton sequence. We show that, during inference, our end-to-end network pays
attention to a set of joints specific to each action, whose encoded
spatio-temporal features are aggregated to recognize the action. Additionally,
the use of multiple stages of action capsules enhances the ability of the
network to classify similar actions. Consequently, our network outperforms the
state-of-the-art approaches on the N-UCLA dataset and obtains competitive
results on the NTURGBD dataset. This is while our approach has significantly
lower computational requirements based on GFLOPs measurements.",None,-1
e723a4ad-3ddb-4576-8221-ab1c1d82e774,"The Past, Present, and Future of Typological Databases in NLP",0.221102,2,"Typological information has the potential to be beneficial in the development
of NLP models, particularly for low-resource languages. Unfortunately, current
large-scale typological databases, notably WALS and Grambank, are inconsistent
both with each other and with other sources of typological information, such as
linguistic grammars. Some of these inconsistencies stem from coding errors or
linguistic variation, but many of the disagreements are due to the discrete
categorical nature of these databases. We shed light on this issue by
systematically exploring disagreements across typological databases and
resources, and their uses in NLP, covering the past and present. We next
investigate the future of such work, offering an argument that a continuous
view of typological features is clearly beneficial, echoing recommendations
from linguistics. We propose that such a view of typology has significant
potential in the future, including in language modeling in low-resource
scenarios.",None,-1
54644710-1976-45c9-afbd-35c7294ba92a,LightGlue: Local Feature Matching at Light Speed,1.0,121,"We introduce LightGlue, a deep neural network that learns to match local
features across images. We revisit multiple design decisions of SuperGlue, the
state of the art in sparse matching, and derive simple but effective
improvements. Cumulatively, they make LightGlue more efficient - in terms of
both memory and computation, more accurate, and much easier to train. One key
property is that LightGlue is adaptive to the difficulty of the problem: the
inference is much faster on image pairs that are intuitively easy to match, for
example because of a larger visual overlap or limited appearance change. This
opens up exciting prospects for deploying deep matchers in latency-sensitive
applications like 3D reconstruction. The code and trained models are publicly
available at https://github.com/cvg/LightGlue.",None,-1
29206abf-00d5-40f2-9017-e1524d52cdd6,"Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",0.919375,20,"Large language models (LLMs) demonstrate impressive reasoning abilities, but
translating reasoning into actions in the real world remains challenging. In
particular, it remains unclear how to complete a given task provably within a
minimum number of interactions with the external environment, e.g., through an
internal mechanism of reasoning. To this end, we propose a principled framework
with provable regret guarantees to orchestrate reasoning and acting, which we
call ""reason for future, act for now"" (\texttt{RAFA}). Specifically, we design
a prompt template for reasoning that learns from the memory buffer and plans a
future trajectory over a long horizon (""reason for future""). At each step, the
LLM agent takes the initial action of the planned trajectory (""act for now""),
stores the collected feedback in the memory buffer, and reinvokes the reasoning
routine to replan the future trajectory from the new state.
  The key idea is to cast reasoning in LLMs as learning and planning in
Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt
LLMs to form an updated posterior of the unknown environment from the memory
buffer (learning) and generate an optimal trajectory for multiple future steps
that maximizes a value function (planning). The learning and planning
subroutines are performed in an ""in-context"" manner to emulate the actor-critic
update for MDPs. Our theoretical analysis proves that the novel combination of
long-term reasoning and short-term acting achieves a $\sqrt{T}$ regret. Here,
$T$ denotes the number of online interactions. In particular, the regret bound
highlights an intriguing interplay between the prior knowledge obtained through
pretraining and the uncertainty reduction achieved by reasoning and acting. Our
empirical validation shows that it outperforms various existing frameworks and
achieves nearly perfect scores on a few benchmarks.",None,-1
3cafdfdc-8d76-492d-8507-dccb1ed59437,Better Sampling of Negatives for Distantly Supervised Named Entity Recognition,0.21054,1,"Distantly supervised named entity recognition (DS-NER) has been proposed to
exploit the automatically labeled training data instead of human annotations.
The distantly annotated datasets are often noisy and contain a considerable
number of false negatives. The recent approach uses a weighted sampling
approach to select a subset of negative samples for training. However, it
requires a good classifier to assign weights to the negative samples. In this
paper, we propose a simple and straightforward approach for selecting the top
negative samples that have high similarities with all the positive samples for
training. Our method achieves consistent performance improvements on four
distantly supervised NER datasets. Our analysis also shows that it is critical
to differentiate the true negatives from the false negatives.",None,-1
41bd2e84-c5cb-4052-b912-1b38de07bd3a,Evaluation of AI Chatbots for Patient-Specific EHR Questions,0.361319,10,"This paper investigates the use of artificial intelligence chatbots for
patient-specific question answering (QA) from clinical notes using several
large language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google
Bard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and
coherence of the answers generated by each model using a 5-point Likert scale
on a set of patient-specific questions.",None,-1
c97a808b-f669-452c-b66c-d701cde68706,Renate: A Library for Real-World Continual Learning,0.15517,2,"Continual learning enables the incremental training of machine learning
models on non-stationary data streams.While academic interest in the topic is
high, there is little indication of the use of state-of-the-art continual
learning algorithms in practical machine learning deployment. This paper
presents Renate, a continual learning library designed to build real-world
updating pipelines for PyTorch models. We discuss requirements for the use of
continual learning algorithms in practice, from which we derive design
principles for Renate. We give a high-level description of the library
components and interfaces. Finally, we showcase the strengths of the library by
presenting experimental results. Renate may be found at
https://github.com/awslabs/renate.",None,-1
284106a6-069e-48d4-aa35-884043ce2eb8,"Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models",0.840644,25,"While adversarial training has been extensively studied for ResNet
architectures and low resolution datasets like CIFAR, much less is known for
ImageNet. Given the recent debate about whether transformers are more robust
than convnets, we revisit adversarial training on ImageNet comparing ViTs and
ConvNeXts. Extensive experiments show that minor changes in architecture, most
notably replacing PatchStem with ConvStem, and training scheme have a
significant impact on the achieved robustness. These changes not only increase
robustness in the seen $\ell_\infty$-threat model, but even more so improve
generalization to unseen $\ell_1/\ell_2$-attacks. Our modified ConvNeXt,
ConvNeXt + ConvStem, yields the most robust $\ell_\infty$-models across
different ranges of model parameters and FLOPs, while our ViT + ConvStem yields
the best generalization to unseen threat models.",None,-1
03d77a96-2db5-4f11-997e-7ad707b0ef10,Self-Attention Based Generative Adversarial Networks For Unsupervised Video Summarization,0.174248,2,"In this paper, we study the problem of producing a comprehensive video
summary following an unsupervised approach that relies on adversarial learning.
We build on a popular method where a Generative Adversarial Network (GAN) is
trained to create representative summaries, indistinguishable from the
originals. The introduction of the attention mechanism into the architecture
for the selection, encoding and decoding of video frames, shows the efficacy of
self-attention and transformer in modeling temporal relationships for video
summarization. We propose the SUM-GAN-AED model that uses a self-attention
mechanism for frame selection, combined with LSTMs for encoding and decoding.
We evaluate the performance of the SUM-GAN-AED model on the SumMe, TVSum and
COGNIMUSE datasets. Experimental results indicate that using a self-attention
mechanism as the frame selection mechanism outperforms the state-of-the-art on
SumMe and leads to comparable to state-of-the-art performance on TVSum and
COGNIMUSE.",None,-1
c8d78c08-174e-48d6-a2eb-0844ffd373b5,Text-Video Retrieval with Disentangled Conceptualization and Set-to-Set Alignment,0.47402,16,"Text-video retrieval is a challenging cross-modal task, which aims to align
visual entities with natural language descriptions. Current methods either fail
to leverage the local details or are computationally expensive. What's worse,
they fail to leverage the heterogeneous concepts in data. In this paper, we
propose the Disentangled Conceptualization and Set-to-set Alignment (DiCoSA) to
simulate the conceptualizing and reasoning process of human beings. For
disentangled conceptualization, we divide the coarse feature into multiple
latent factors related to semantic concepts. For set-to-set alignment, where a
set of visual concepts correspond to a set of textual concepts, we propose an
adaptive pooling method to aggregate semantic concepts to address the partial
matching. In particular, since we encode concepts independently in only a few
dimensions, DiCoSA is superior at efficiency and granularity, ensuring
fine-grained interactions using a similar computational complexity as
coarse-grained alignment. Extensive experiments on five datasets, including
MSR-VTT, LSMDC, MSVD, ActivityNet, and DiDeMo, demonstrate that our method
outperforms the existing state-of-the-art methods.",None,-1
6a64b6f7-6f27-4c5f-ab62-070dd6fee3e8,Solar Irradiance Anticipative Transformer,0.522922,2,"This paper proposes an anticipative transformer-based model for short-term
solar irradiance forecasting. Given a sequence of sky images, our proposed
vision transformer encodes features of consecutive images, feeding into a
transformer decoder to predict irradiance values associated with future unseen
sky images. We show that our model effectively learns to attend only to
relevant features in images in order to forecast irradiance. Moreover, the
proposed anticipative transformer captures long-range dependencies between sky
images to achieve a forecasting skill of 21.45 % on a 15 minute ahead
prediction for a newly introduced dataset of all-sky images when compared to a
smart persistence model.",None,-1
eb6617e8-5e5b-4ce9-8800-f13489a59211,Algorithm Selection for Deep Active Learning with Imbalanced Datasets,0.475277,6,"Label efficiency has become an increasingly important objective in deep
learning applications. Active learning aims to reduce the number of labeled
examples needed to train deep networks, but the empirical performance of active
learning algorithms can vary dramatically across datasets and applications. It
is difficult to know in advance which active learning strategy will perform
well or best in a given application. To address this, we propose the first
adaptive algorithm selection strategy for deep active learning. For any
unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning
algORithm selection) iteratively and adaptively chooses among a set of
candidate active learning algorithms. TAILOR uses novel reward functions aimed
at gathering class-balanced examples. Extensive experiments in multi-class and
multi-label applications demonstrate TAILOR's effectiveness in achieving
accuracy comparable or better than that of the best of the candidate
algorithms. Our implementation of TAILOR is open-sourced at
https://github.com/jifanz/TAILOR.",None,-1
eacf53a2-52a8-4f61-9e17-387692649fea,D-Separation for Causal Self-Explanation,0.412553,2,"Rationalization is a self-explaining framework for NLP models. Conventional
work typically uses the maximum mutual information (MMI) criterion to find the
rationale that is most indicative of the target label. However, this criterion
can be influenced by spurious features that correlate with the causal rationale
or the target label. Instead of attempting to rectify the issues of the MMI
criterion, we propose a novel criterion to uncover the causal rationale, termed
the Minimum Conditional Dependence (MCD) criterion, which is grounded on our
finding that the non-causal features and the target label are
\emph{d-separated} by the causal rationale. By minimizing the dependence
between the unselected parts of the input and the target label conditioned on
the selected rationale candidate, all the causes of the label are compelled to
be selected. In this study, we employ a simple and practical measure of
dependence, specifically the KL-divergence, to validate our proposed MCD
criterion. Empirically, we demonstrate that MCD improves the F1 score by up to
$13.7\%$ compared to previous state-of-the-art MMI-based methods. Our code is
available at: \url{https://github.com/jugechengzi/Rationalization-MCD}.",None,-1
b20aa458-ce48-4874-b4ec-481623ea2a94,Differentiable Tree Operations Promote Compositional Generalization,0.132272,3,"In the context of structure-to-structure transformation tasks, learning
sequences of discrete symbolic operations poses significant challenges due to
their non-differentiability. To facilitate the learning of these symbolic
sequences, we introduce a differentiable tree interpreter that compiles
high-level symbolic tree operations into subsymbolic matrix operations on
tensors. We present a novel Differentiable Tree Machine (DTM) architecture that
integrates our interpreter with an external memory and an agent that learns to
sequentially select tree operations to execute the target transformation in an
end-to-end manner. With respect to out-of-distribution compositional
generalization on synthetic semantic parsing and language generation tasks, DTM
achieves 100% while existing baselines such as Transformer, Tree Transformer,
LSTM, and Tree2Tree LSTM achieve less than 30%. DTM remains highly
interpretable in addition to its perfect performance.",None,-1
0a6ea6b3-6147-46a8-a1d0-bbcfcf737131,Knowledge Acquisition and Completion for Long-Term Human-Robot Interactions using Knowledge Graph Embedding,0.25404,2,"In Human-Robot Interaction (HRI) systems, a challenging task is sharing the
representation of the operational environment, fusing symbolic knowledge and
perceptions, between users and robots. With the existing HRI pipelines, users
can teach the robots some concepts to increase their knowledge base.
Unfortunately, the data coming from the users are usually not enough dense for
building a consistent representation. Furthermore, the existing approaches are
not able to incrementally build up their knowledge base, which is very
important when robots have to deal with dynamic contexts. To this end, we
propose an architecture to gather data from users and environments in long-runs
of continual learning. We adopt Knowledge Graph Embedding techniques to
generalize the acquired information with the goal of incrementally extending
the robot's inner representation of the environment. We evaluate the
performance of the overall continual learning architecture by measuring the
capabilities of the robot of learning entities and relations coming from
unknown contexts through a series of incremental learning sessions.",None,-1
16c801aa-181f-4a6c-9109-015e952beea1,Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models,0.169119,7,"Large Language Models (LLMs) are trained primarily on minimally processed web
text, which exhibits the same wide range of social biases held by the humans
who created that content. Consequently, text generated by LLMs can
inadvertently perpetuate stereotypes towards marginalized groups, like the
LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs
generate text describing people with different sexual identities. Analyzing
bias in the text generated by an LLM using regard score shows measurable bias
against queer people. We then show that a post-hoc method based on
chain-of-thought prompting using SHAP analysis can increase the regard of the
sentence, representing a promising approach towards debiasing the output of
LLMs in this setting.",None,-1
f1a4f323-7972-4bdc-87f9-922bd7bcb3a7,Garment Recovery with Shape and Deformation Priors,0.581501,2,"While modeling people wearing tight-fitting clothing has made great strides
in recent years, loose-fitting clothing remains a challenge. We propose a
method that delivers realistic garment models from real-world images,
regardless of garment shape or deformation. To this end, we introduce a fitting
approach that utilizes shape and deformation priors learned from synthetic data
to accurately capture garment shapes and deformations, including large ones.
Not only does our approach recover the garment geometry accurately, it also
yields models that can be directly used by downstream applications such as
animation and simulation.",None,-1
64d843a9-7454-4eaf-b60e-a027792a8ebf,NeMF: Inverse Volume Rendering with Neural Microflake Field,0.750486,11,"Recovering the physical attributes of an object's appearance from its images
captured under an unknown illumination is challenging yet essential for
photo-realistic rendering. Recent approaches adopt the emerging implicit scene
representations and have shown impressive results.However, they unanimously
adopt a surface-based representation,and hence can not well handle scenes with
very complex geometry, translucent object and etc. In this paper, we propose to
conduct inverse volume rendering, in contrast to surface-based, by representing
a scene using microflake volume, which assumes the space is filled with
infinite small flakes and light reflects or scatters at each spatial location
according to microflake distributions. We further adopt the coordinate networks
to implicitly encode the microflake volume, and develop a differentiable
microflake volume renderer to train the network in an end-to-end way in
principle.Our NeMF enables effective recovery of appearance attributes for
highly complex geometry and scattering object, enables high-quality relighting,
material editing, and especially simulates volume rendering effects, such as
scattering, which is infeasible for surface-based approaches.",None,-1
5b5f1cf8-8645-4a0c-9873-248e783e7056,Deep Learning-Enabled Sleep Staging From Vital Signs and Activity Measured Using a Near-Infrared Video Camera,0.713048,3,"Conventional sleep monitoring is time-consuming, expensive and uncomfortable,
requiring a large number of contact sensors to be attached to the patient.
Video data is commonly recorded as part of a sleep laboratory assessment. If
accurate sleep staging could be achieved solely from video, this would overcome
many of the problems of traditional methods. In this work we use heart rate,
breathing rate and activity measures, all derived from a near-infrared video
camera, to perform sleep stage classification. We use a deep transfer learning
approach to overcome data scarcity, by using an existing contact-sensor dataset
to learn effective representations from the heart and breathing rate time
series. Using a dataset of 50 healthy volunteers, we achieve an accuracy of
73.4\% and a Cohen's kappa of 0.61 in four-class sleep stage classification,
establishing a new state-of-the-art for video-based sleep staging.",None,-1
b9a76a28-2e3b-4c60-8529-88847a064cc3,Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images,0.309948,6,"Self-driving vehicles rely on urban street maps for autonomous navigation. In
this paper, we introduce Pix2Map, a method for inferring urban street map
topology directly from ego-view images, as needed to continually update and
expand existing maps. This is a challenging task, as we need to infer a complex
urban road topology directly from raw image data. The main insight of this
paper is that this problem can be posed as cross-modal retrieval by learning a
joint, cross-modal embedding space for images and existing maps, represented as
discrete graphs that encode the topological layout of the visual surroundings.
We conduct our experimental evaluation using the Argoverse dataset and show
that it is indeed possible to accurately retrieve street maps corresponding to
both seen and unseen roads solely from image data. Moreover, we show that our
retrieved maps can be used to update or expand existing maps and even show
proof-of-concept results for visual localization and image retrieval from
spatial graphs.",None,-1
a2c585e0-540b-4cdc-9f15-cb3fe0fd684e,DReg-NeRF: Deep Registration for Neural Radiance Fields,0.436607,6,"Although Neural Radiance Fields (NeRF) is popular in the computer vision
community recently, registering multiple NeRFs has yet to gain much attention.
Unlike the existing work, NeRF2NeRF, which is based on traditional optimization
methods and needs human annotated keypoints, we propose DReg-NeRF to solve the
NeRF registration problem on object-centric scenes without human intervention.
After training NeRF models, our DReg-NeRF first extracts features from the
occupancy grid in NeRF. Subsequently, our DReg-NeRF utilizes a transformer
architecture with self-attention and cross-attention layers to learn the
relations between pairwise NeRF blocks. In contrast to state-of-the-art (SOTA)
point cloud registration methods, the decoupled correspondences are supervised
by surface fields without any ground truth overlapping labels. We construct a
novel view synthesis dataset with 1,700+ 3D objects obtained from Objaverse to
train our network. When evaluated on the test set, our proposed method beats
the SOTA point cloud registration methods by a large margin, with a mean
$\text{RPE}=9.67^{\circ}$ and a mean $\text{RTE}=0.038$.
  Our code is available at https://github.com/AIBluefisher/DReg-NeRF.",None,-1
53d22f47-2f92-4a88-9bfa-356eceff48d0,A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets,0.562199,3,"Low-quality data can cause downstream problems in high-stakes applications.
Data-centric approach emphasizes on improving dataset quality to enhance model
performance. High-quality datasets are needed for general-purpose Large
Language Models (LLMs) training, as well as for domain-specific models, which
are usually small in size as it is costly to engage a large number of domain
experts for their creation. Thus, it is vital to ensure high-quality
domain-specific training data. In this paper, we propose a framework for
enhancing the data quality of original datasets. We applied the proposed
framework to four biomedical datasets and showed relative improvement of up to
33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when
using back translation to enhance the original dataset quality.",None,-1
ddf30d03-6964-4f03-b5cc-cea043f82971,Think Rationally about What You See: Continuous Rationale Extraction for Relation Extraction,0.782246,7,"Relation extraction (RE) aims to extract potential relations according to the
context of two entities, thus, deriving rational contexts from sentences plays
an important role. Previous works either focus on how to leverage the entity
information (e.g., entity types, entity verbalization) to inference relations,
but ignore context-focused content, or use counterfactual thinking to remove
the model's bias of potential relations in entities, but the relation reasoning
process will still be hindered by irrelevant content. Therefore, how to
preserve relevant content and remove noisy segments from sentences is a crucial
task. In addition, retained content needs to be fluent enough to maintain
semantic coherence and interpretability. In this work, we propose a novel
rationale extraction framework named RE2, which leverages two continuity and
sparsity factors to obtain relevant and coherent rationales from sentences. To
solve the problem that the gold rationales are not labeled, RE2 applies an
optimizable binary mask to each token in the sentence, and adjust the
rationales that need to be selected according to the relation label.
Experiments on four datasets show that RE2 surpasses baselines.",None,-1
ef2d9b32-73c8-4c65-af94-4e8041b545ac,SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks,0.989537,33,"Prompt tuning is a technology that tunes a small set of parameters to steer a
pre-trained language model (LM) to directly generate the output for downstream
tasks. Recently, prompt tuning has demonstrated its storage and computation
efficiency in both natural language processing (NLP) and speech processing
fields. These advantages have also revealed prompt tuning as a candidate
approach to serving pre-trained LM for multiple tasks in a unified manner. For
speech processing, SpeechPrompt shows its high parameter efficiency and
competitive performance on a few speech classification tasks. However, whether
SpeechPrompt is capable of serving a large number of tasks is unanswered. In
this work, we propose SpeechPrompt v2, a prompt tuning framework capable of
performing a wide variety of speech classification tasks, covering multiple
languages and prosody-related tasks. The experiment result shows that
SpeechPrompt v2 achieves performance on par with prior works with less than
0.15M trainable parameters in a unified framework.",None,-1
f6341162-926b-45a5-86d3-417d8c722f19,Masked and Adaptive Transformer for Exemplar Based Image Translation,0.44924,7,"We present a novel framework for exemplar based image translation. Recent
advanced methods for this task mainly focus on establishing cross-domain
semantic correspondence, which sequentially dominates image generation in the
manner of local style control. Unfortunately, cross-domain semantic matching is
challenging; and matching errors ultimately degrade the quality of generated
images. To overcome this challenge, we improve the accuracy of matching on the
one hand, and diminish the role of matching in image generation on the other
hand. To achieve the former, we propose a masked and adaptive transformer (MAT)
for learning accurate cross-domain correspondence, and executing context-aware
feature augmentation. To achieve the latter, we use source features of the
input and global style codes of the exemplar, as supplementary information, for
decoding an image. Besides, we devise a novel contrastive style learning
method, for acquire quality-discriminative style representations, which in turn
benefit high-quality image generation. Experimental results show that our
method, dubbed MATEBIT, performs considerably better than state-of-the-art
methods, in diverse image translation tasks. The codes are available at
\url{https://github.com/AiArt-HDU/MATEBIT}.",None,-1
7f425f3a-ff67-483d-a38c-656a47a7c181,Discovering Causality for Efficient Cooperation in Multi-Agent Environments,0.166109,1,"In cooperative Multi-Agent Reinforcement Learning (MARL) agents are required
to learn behaviours as a team to achieve a common goal. However, while learning
a task, some agents may end up learning sub-optimal policies, not contributing
to the objective of the team. Such agents are called lazy agents due to their
non-cooperative behaviours that may arise from failing to understand whether
they caused the rewards. As a consequence, we observe that the emergence of
cooperative behaviours is not necessarily a byproduct of being able to solve a
task as a team. In this paper, we investigate the applications of causality in
MARL and how it can be applied in MARL to penalise these lazy agents. We
observe that causality estimations can be used to improve the credit assignment
to the agents and show how it can be leveraged to improve independent learning
in MARL. Furthermore, we investigate how Amortized Causal Discovery can be used
to automate causality detection within MARL environments. The results
demonstrate that causality relations between individual observations and the
team reward can be used to detect and punish lazy agents, making them develop
more intelligent behaviours. This results in improvements not only in the
overall performances of the team but also in their individual capabilities. In
addition, results show that Amortized Causal Discovery can be used efficiently
to find causal relations in MARL.",None,-1
eed814f5-ab4d-44a7-9bb4-189702546f33,An empirical study of using radiology reports and images to improve ICU mortality prediction,0.472384,8,"Background: The predictive Intensive Care Unit (ICU) scoring system plays an
important role in ICU management because it predicts important outcomes,
especially mortality. Many scoring systems have been developed and used in the
ICU. These scoring systems are primarily based on the structured clinical data
in the electronic health record (EHR), which may suffer the loss of important
clinical information in the narratives and images. Methods: In this work, we
build a deep learning based survival prediction model with multi-modality data
to predict ICU mortality. Four sets of features are investigated: (1)
physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2)
common thorax diseases pre-defined by radiologists, (3) BERT-based text
representations, and (4) chest X-ray image features. We use the Medical
Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the
proposed model. Results: Our model achieves the average C-index of 0.7829 (95%
confidence interval, 0.7620-0.8038), which substantially exceeds that of the
baseline with SAPS-II features (0.7470 (0.7263-0.7676)). Ablation studies
further demonstrate the contributions of pre-defined labels (2.00%), text
features (2.44%), and image features (2.82%).",None,-1
26edaf45-92c0-4a41-9daf-a2bb813bc6a7,Correcting Semantic Parses with Natural Language through Dynamic Schema Encoding,0.128833,2,"In addressing the task of converting natural language to SQL queries, there
are several semantic and syntactic challenges. It becomes increasingly
important to understand and remedy the points of failure as the performance of
semantic parsing systems improve. We explore semantic parse correction with
natural language feedback, proposing a new solution built on the success of
autoregressive decoders in text-to-SQL tasks. By separating the semantic and
syntactic difficulties of the task, we show that the accuracy of text-to-SQL
parsers can be boosted by up to 26% with only one turn of correction with
natural language. Additionally, we show that a T5-base model is capable of
correcting the errors of a T5-large model in a zero-shot, cross-parser setting.",None,-1
0c06b186-f9e8-458a-bfa5-ab83e1b2911e,Self-NeRF: A Self-Training Pipeline for Few-Shot Neural Radiance Fields,0.0528052,1,"Recently, Neural Radiance Fields (NeRF) have emerged as a potent method for
synthesizing novel views from a dense set of images. Despite its impressive
performance, NeRF is plagued by its necessity for numerous calibrated views and
its accuracy diminishes significantly in a few-shot setting. To address this
challenge, we propose Self-NeRF, a self-evolved NeRF that iteratively refines
the radiance fields with very few number of input views, without incorporating
additional priors. Basically, we train our model under the supervision of
reference and unseen views simultaneously in an iterative procedure. In each
iteration, we label unseen views with the predicted colors or warped pixels
generated by the model from the preceding iteration. However, these expanded
pseudo-views are afflicted by imprecision in color and warping artifacts, which
degrades the performance of NeRF. To alleviate this issue, we construct an
uncertainty-aware NeRF with specialized embeddings. Some techniques such as
cone entropy regularization are further utilized to leverage the pseudo-views
in the most efficient manner. Through experiments under various settings, we
verified that our Self-NeRF is robust to input with uncertainty and surpasses
existing methods when trained on limited training data.",None,-1
3085cc2f-b6db-4b72-b1a6-70b005bd8fee,EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education,0.959388,43,"EduChat (https://www.educhat.top/) is a large-scale language model
(LLM)-based chatbot system in the education domain. Its goal is to support
personalized, fair, and compassionate intelligent education, serving teachers,
students, and parents. Guided by theories from psychology and education, it
further strengthens educational functions such as open question answering,
essay assessment, Socratic teaching, and emotional support based on the
existing basic LLMs. Particularly, we learn domain-specific knowledge by
pre-training on the educational corpus and stimulate various skills with tool
use by fine-tuning on designed system prompts and instructions. Currently,
EduChat is available online as an open-source project, with its code, data, and
model parameters available on platforms (e.g., GitHub
https://github.com/icalk-nlp/EduChat, Hugging Face
https://huggingface.co/ecnu-icalk ). We also prepare a demonstration of its
capabilities online (https://vimeo.com/851004454). This initiative aims to
promote research and applications of LLMs for intelligent education.",None,-1
1957652e-96fd-4d5d-8736-73533fe060e0,Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning,0.310007,26,"Recent works on instruction tuning (IT) have achieved great performance with
zero-shot generalizability to unseen tasks. With additional context (e.g., task
definition, examples) provided to models for fine-tuning, they achieved much
higher performance than untuned models. Despite impressive performance gains,
what models learn from IT remains understudied. In this work, we analyze how
models utilize instructions during IT by comparing model training with altered
vs. original instructions. Specifically, we create simplified task definitions
by removing all semantic components and only leaving the output space
information, and delusive examples that contain incorrect input-output mapping.
Our experiments show that models trained on simplified task definition or
delusive examples can achieve comparable performance to the ones trained on the
original instructions and examples. Furthermore, we introduce a random baseline
to perform zeroshot classification tasks, and find it achieves similar
performance (42.6% exact-match) as IT does (43% exact-match) in low resource
setting, while both methods outperform naive T5 significantly (30% per
exact-match). Our analysis provides evidence that the impressive performance
gain of current IT models can come from picking up superficial patterns, such
as learning the output format and guessing. Our study highlights the urgent
need for more reliable IT methods and evaluation.",None,-1
dd071e98-54cf-460f-a778-201e772c462c,CertViT: Certified Robustness of Pre-Trained Vision Transformers,0.0756084,2,"Lipschitz bounded neural networks are certifiably robust and have a good
trade-off between clean and certified accuracy. Existing Lipschitz bounding
methods train from scratch and are limited to moderately sized networks (< 6M
parameters). They require a fair amount of hyper-parameter tuning and are
computationally prohibitive for large networks like Vision Transformers (5M to
660M parameters). Obtaining certified robustness of transformers is not
feasible due to the non-scalability and inflexibility of the current methods.
This work presents CertViT, a two-step proximal-projection method to achieve
certified robustness from pre-trained weights. The proximal step tries to lower
the Lipschitz bound and the projection step tries to maintain the clean
accuracy of pre-trained weights. We show that CertViT networks have better
certified accuracy than state-of-the-art Lipschitz trained networks. We apply
CertViT on several variants of pre-trained vision transformers and show
adversarial robustness using standard attacks. Code :
https://github.com/sagarverma/transformer-lipschitz",None,-1
25470270-5855-4b70-99dc-55b3efe41e33,Exploring Large Language Models for Human Mobility Prediction under Public Events,0.773698,11,"Public events, such as concerts and sports games, can be major attractors for
large crowds, leading to irregular surges in travel demand. Accurate human
mobility prediction for public events is thus crucial for event planning as
well as traffic or crowd management. While rich textual descriptions about
public events are commonly available from online sources, it is challenging to
encode such information in statistical or machine learning models. Existing
methods are generally limited in incorporating textual information, handling
data sparsity, or providing rationales for their predictions. To address these
challenges, we introduce a framework for human mobility prediction under public
events (LLM-MPE) based on Large Language Models (LLMs), leveraging their
unprecedented ability to process textual data, learn from minimal examples, and
generate human-readable explanations. Specifically, LLM-MPE first transforms
raw, unstructured event descriptions from online sources into a standardized
format, and then segments historical mobility data into regular and
event-related components. A prompting strategy is designed to direct LLMs in
making and rationalizing demand predictions considering historical mobility and
event features. A case study is conducted for Barclays Center in New York City,
based on publicly available event information and taxi trip data. Results show
that LLM-MPE surpasses traditional models, particularly on event days, with
textual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers
interpretable insights into its predictions. Despite the great potential of
LLMs, we also identify key challenges including misinformation and high costs
that remain barriers to their broader adoption in large-scale human mobility
analysis.",None,-1
76e29e6c-5349-4317-905d-e9c892588786,Justifiable Artificial Intelligence: Engineering Large Language Models for Legal Applications,0.315703,1,"In this work, I discuss how Large Language Models can be applied in the legal
domain, circumventing their current drawbacks. Despite their large success and
acceptance, their lack of explainability hinders legal experts to trust in
their output, and this happens rightfully so. However, in this paper, I argue
in favor of a new view, Justifiable Artificial Intelligence, instead of
focusing on Explainable Artificial Intelligence. I discuss in this paper how
gaining evidence for and against a Large Language Model's output may make their
generated texts more trustworthy - or hold them accountable for misinformation.",None,-1
def3adc1-16d5-4ebd-9e32-1c8ac9514745,Towards Few-shot Inductive Link Prediction on Knowledge Graphs: A Relational Anonymous Walk-guided Neural Process Approach,0.705425,5,"Few-shot inductive link prediction on knowledge graphs (KGs) aims to predict
missing links for unseen entities with few-shot links observed. Previous
methods are limited to transductive scenarios, where entities exist in the
knowledge graphs, so they are unable to handle unseen entities. Therefore,
recent inductive methods utilize the sub-graphs around unseen entities to
obtain the semantics and predict links inductively. However, in the few-shot
setting, the sub-graphs are often sparse and cannot provide meaningful
inductive patterns. In this paper, we propose a novel relational anonymous
walk-guided neural process for few-shot inductive link prediction on knowledge
graphs, denoted as RawNP. Specifically, we develop a neural process-based
method to model a flexible distribution over link prediction functions. This
enables the model to quickly adapt to new entities and estimate the uncertainty
when making predictions. To capture general inductive patterns, we present a
relational anonymous walk to extract a series of relational motifs from
few-shot observations. These motifs reveal the distinctive semantic patterns on
KGs that support inductive predictions. Extensive experiments on typical
benchmark datasets demonstrate that our model derives new state-of-the-art
performance.",None,-1
ac3731fc-66aa-4e06-a274-0d2fe52766d7,Structured Dialogue Discourse Parsing,0.799105,7,"Dialogue discourse parsing aims to uncover the internal structure of a
multi-participant conversation by finding all the discourse~\emph{links} and
corresponding~\emph{relations}. Previous work either treats this task as a
series of independent multiple-choice problems, in which the link existence and
relations are decoded separately, or the encoding is restricted to only local
interaction, ignoring the holistic structural information. In contrast, we
propose a principled method that improves upon previous work from two
perspectives: encoding and decoding. From the encoding side, we perform
structured encoding on the adjacency matrix followed by the matrix-tree
learning algorithm, where all discourse links and relations in the dialogue are
jointly optimized based on latent tree-level distribution. From the decoding
side, we perform structured inference using the modified Chiu-Liu-Edmonds
algorithm, which explicitly generates the labeled multi-root non-projective
spanning tree that best captures the discourse structure. In addition, unlike
in previous work, we do not rely on hand-crafted features; this improves the
model's robustness. Experiments show that our method achieves new
state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on
Molweni (F1 scores). \footnote{Code released
at~\url{https://github.com/chijames/structured_dialogue_discourse_parsing}.}",None,-1
d54426f6-3d84-474e-a9a6-1fb332619bd1,A Multimodal Analysis of Influencer Content on Twitter,0.624476,2,"Influencer marketing involves a wide range of strategies in which brands
collaborate with popular content creators (i.e., influencers) to leverage their
reach, trust, and impact on their audience to promote and endorse products or
services. Because followers of influencers are more likely to buy a product
after receiving an authentic product endorsement rather than an explicit direct
product promotion, the line between personal opinions and commercial content
promotion is frequently blurred. This makes automatic detection of regulatory
compliance breaches related to influencer advertising (e.g., misleading
advertising or hidden sponsorships) particularly difficult. In this work, we
(1) introduce a new Twitter (now X) dataset consisting of 15,998 influencer
posts mapped into commercial and non-commercial categories for assisting in the
automatic detection of commercial influencer content; (2) experiment with an
extensive set of predictive models that combine text and visual information
showing that our proposed cross-attention approach outperforms state-of-the-art
multimodal models; and (3) conduct a thorough analysis of strengths and
limitations of our models. We show that multimodal modeling is useful for
identifying commercial posts, reducing the amount of false positives, and
capturing relevant context that aids in the discovery of undisclosed commercial
posts.",None,-1
670ba3a7-6a14-4bfd-a9bf-06e589f59545,M$^2$DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer,0.696786,7,"Ensuring traffic safety and preventing accidents is a critical goal in daily
driving, where the advancement of computer vision technologies can be leveraged
to achieve this goal. In this paper, we present a multi-view, multi-scale
framework for naturalistic driving action recognition and localization in
untrimmed videos, namely M$^2$DAR, with a particular focus on detecting
distracted driving behaviors. Our system features a weight-sharing, multi-scale
Transformer-based action recognition network that learns robust hierarchical
representations. Furthermore, we propose a new election algorithm consisting of
aggregation, filtering, merging, and selection processes to refine the
preliminary results from the action recognition module across multiple views.
Extensive experiments conducted on the 7th AI City Challenge Track 3 dataset
demonstrate the effectiveness of our approach, where we achieved an overlap
score of 0.5921 on the A2 test set. Our source code is available at
\url{https://github.com/PurdueDigitalTwin/M2DAR}.",None,-1
626ffad7-a069-4c38-9b52-d98bd6421eba,A Game of Bundle Adjustment -- Learning Efficient Convergence,0.294678,2,"Bundle adjustment is the common way to solve localization and mapping. It is
an iterative process in which a system of non-linear equations is solved using
two optimization methods, weighted by a damping factor. In the classic
approach, the latter is chosen heuristically by the Levenberg-Marquardt
algorithm on each iteration. This might take many iterations, making the
process computationally expensive, which might be harmful to real-time
applications. We propose to replace this heuristic by viewing the problem in a
holistic manner, as a game, and formulating it as a reinforcement-learning
task. We set an environment which solves the non-linear equations and train an
agent to choose the damping factor in a learned manner. We demonstrate that our
approach considerably reduces the number of iterations required to reach the
bundle adjustment's convergence, on both synthetic and real-life scenarios. We
show that this reduction benefits the classic approach and can be integrated
with other bundle adjustment acceleration methods.",None,-1
5eed57a0-b8c0-496c-980c-3503355e47b6,"Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques",0.516021,2,"This paper describes the participation of team QUST in the SemEval2023 task
3. The monolingual models are first evaluated with the under-sampling of the
majority classes in the early stage of the task. Then, the pre-trained
multilingual model is fine-tuned with a combination of the class weights and
the sample weights. Two different fine-tuning strategies, the task-agnostic and
the task-dependent, are further investigated. All experiments are conducted
under the 10-fold cross-validation, the multilingual approaches are superior to
the monolingual ones. The submitted system achieves the second best in Italian
and Spanish (zero-shot) in subtask-1.",None,-1
ba930d01-bf5a-4caf-bb8e-5d19fd9f8e8d,Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion,0.798131,19,"Diffusion models have shown superior performance in image generation and
manipulation, but the inherent stochasticity presents challenges in preserving
and manipulating image content and identity. While previous approaches like
DreamBooth and Textual Inversion have proposed model or latent representation
personalization to maintain the content, their reliance on multiple reference
images and complex training limits their practicality. In this paper, we
present a simple yet highly effective approach to personalization using highly
personalized (HiPer) text embedding by decomposing the CLIP embedding space for
personalization and content manipulation. Our method does not require model
fine-tuning or identifiers, yet still enables manipulation of background,
texture, and motion with just a single image and target text. Through
experiments on diverse target texts, we demonstrate that our approach produces
highly personalized and complex semantic image edits across a wide range of
tasks. We believe that the novel understanding of the text embedding space
presented in this work has the potential to inspire further research across
various tasks.",None,-1
d4182153-1b07-433b-9655-db8cade90ede,Continual Learning as Computationally Constrained Reinforcement Learning,0.681186,9,"An agent that efficiently accumulates knowledge to develop increasingly
sophisticated skills over a long lifetime could advance the frontier of
artificial intelligence capabilities. The design of such agents, which remains
a long-standing challenge of artificial intelligence, is addressed by the
subject of continual learning. This monograph clarifies and formalizes concepts
of continual learning, introducing a framework and set of tools to stimulate
further research.",None,-1
c8892f42-c192-463b-98c7-a03d04ac3fac,TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision,0.44145,15,"In this paper, we investigate an open research task of generating
controllable 3D textured shapes from the given textual descriptions. Previous
works either require ground truth caption labeling or extensive optimization
time. To resolve these issues, we present a novel framework, TAPS3D, to train a
text-guided 3D shape generator with pseudo captions. Specifically, based on
rendered 2D images, we retrieve relevant words from the CLIP vocabulary and
construct pseudo captions using templates. Our constructed captions provide
high-level semantic supervision for generated 3D shapes. Further, in order to
produce fine-grained textures and increase geometry diversity, we propose to
adopt low-level image regularization to enable fake-rendered images to align
with the real ones. During the inference phase, our proposed model can generate
3D textured shapes from the given text without any additional optimization. We
conduct extensive experiments to analyze each of our proposed components and
show the efficacy of our framework in generating high-fidelity 3D textured and
text-relevant shapes.",None,-1
053c770a-b3df-43f2-b48d-bc00a5967dc6,Mixture-of-Expert Conformer for Streaming Multilingual ASR,0.663956,6,"End-to-end models with large capacity have significantly improved
multilingual automatic speech recognition, but their computation cost poses
challenges for on-device applications. We propose a streaming truly
multilingual Conformer incorporating mixture-of-expert (MoE) layers that learn
to only activate a subset of parameters in training and inference. The MoE
layer consists of a softmax gate which chooses the best two experts among many
in forward propagation. The proposed MoE layer offers efficient inference by
activating a fixed number of parameters as the number of experts increases. We
evaluate the proposed model on a set of 12 languages, and achieve an average
11.9% relative improvement in WER over the baseline. Compared to an adapter
model using ground truth information, our MoE model achieves similar WER and
activates similar number of parameters but without any language information. We
further show around 3% relative WER improvement by multilingual shallow fusion.",None,-1
57e66691-4fdc-4d14-93b1-51eafa01622f,MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting,0.424158,7,"Large language models (LLMs) have achieved impressive performance on various
reasoning tasks. To further improve the performance, we propose MultiTool-CoT,
a novel framework that leverages chain-of-thought (CoT) prompting to
incorporate multiple external tools, such as a calculator and a knowledge
retriever, during the reasoning process. We apply MultiTool-CoT to the Task 2
dataset of NumGLUE, which requires both numerical reasoning and domain-specific
knowledge. The experiments show that our method significantly outperforms
strong baselines and achieves state-of-the-art performance.",None,-1
4f495207-777d-4cdc-92d3-779e71f9aa57,OVO: Open-Vocabulary Occupancy,0.527836,7,"Semantic occupancy prediction aims to infer dense geometry and semantics of
surroundings for an autonomous agent to operate safely in the 3D environment.
Existing occupancy prediction methods are almost entirely trained on
human-annotated volumetric data. Although of high quality, the generation of
such 3D annotations is laborious and costly, restricting them to a few specific
object categories in the training dataset. To address this limitation, this
paper proposes Open Vocabulary Occupancy (OVO), a novel approach that allows
semantic occupancy prediction of arbitrary classes but without the need for 3D
annotations during training. Keys to our approach are (1) knowledge
distillation from a pre-trained 2D open-vocabulary segmentation model to the 3D
occupancy network, and (2) pixel-voxel filtering for high-quality training data
generation. The resulting framework is simple, compact, and compatible with
most state-of-the-art semantic occupancy prediction models. On NYUv2 and
SemanticKITTI datasets, OVO achieves competitive performance compared to
supervised semantic occupancy prediction approaches. Furthermore, we conduct
extensive analyses and ablation studies to offer insights into the design of
the proposed framework. Our code is publicly available at
https://github.com/dzcgaara/OVO.",None,-1
4dd39af8-3761-48e3-b212-ef53c28bf9be,Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction,0.999988,134,"Modern methods for vision-centric autonomous driving perception widely adopt
the bird's-eye-view (BEV) representation to describe a 3D scene. Despite its
better efficiency than voxel representation, it has difficulty describing the
fine-grained 3D structure of a scene with a single plane. To address this, we
propose a tri-perspective view (TPV) representation which accompanies BEV with
two additional perpendicular planes. We model each point in the 3D space by
summing its projected features on the three planes. To lift image features to
the 3D TPV space, we further propose a transformer-based TPV encoder
(TPVFormer) to obtain the TPV features effectively. We employ the attention
mechanism to aggregate the image features corresponding to each query in each
TPV plane. Experiments show that our model trained with sparse supervision
effectively predicts the semantic occupancy for all voxels. We demonstrate for
the first time that using only camera inputs can achieve comparable performance
with LiDAR-based methods on the LiDAR segmentation task on nuScenes. Code:
https://github.com/wzzheng/TPVFormer.",None,-1
dd5ccb19-9e31-4960-96a2-72bd1575ccf7,Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer,0.446245,3,"Cross-lingual natural language inference is a fundamental problem in
cross-lingual language understanding. Many recent works have used prompt
learning to address the lack of annotated parallel corpora in XNLI. However,
these methods adopt discrete prompting by simply translating the templates to
the target language and need external expert knowledge to design the templates.
Besides, discrete prompts of human-designed template words are not trainable
vectors and can not be migrated to target languages in the inference stage
flexibly. In this paper, we propose a novel Soft prompt learning framework with
the Multilingual Verbalizer (SoftMV) for XNLI. SoftMV first constructs
cloze-style question with soft prompts for the input sample. Then we leverage
bilingual dictionaries to generate an augmented multilingual question for the
original question. SoftMV adopts a multilingual verbalizer to align the
representations of original and augmented multilingual questions into the same
semantic space with consistency regularization. Experimental results on XNLI
demonstrate that SoftMV can achieve state-of-the-art performance and
significantly outperform the previous methods under the few-shot and full-shot
cross-lingual transfer settings.",None,-1
45aa21a6-f344-4251-9198-347382fdd2eb,Information Value: Measuring Utterance Predictability as Distance from Plausible Alternatives,0.255069,1,"We present information value, a measure which quantifies the predictability
of an utterance relative to a set of plausible alternatives. We introduce a
method to obtain interpretable estimates of information value using neural text
generators, and exploit their psychometric predictive power to investigate the
dimensions of predictability that drive human comprehension behaviour.
Information value is a stronger predictor of utterance acceptability in written
and spoken dialogue than aggregates of token-level surprisal and it is
complementary to surprisal for predicting eye-tracked reading times.",None,-1
fb98b70a-f607-4df9-aad0-cddba5d89dbb,Representation Matters: The Game of Chess Poses a Challenge to Vision Transformers,0.0828452,2,"While transformers have gained the reputation as the ""Swiss army knife of
AI"", no one has challenged them to master the game of chess, one of the
classical AI benchmarks. Simply using vision transformers (ViTs) within
AlphaZero does not master the game of chess, mainly because ViTs are too slow.
Even making them more efficient using a combination of MobileNet and NextViT
does not beat what actually matters: a simple change of the input
representation and value loss, resulting in a greater boost of up to 180 Elo
points over AlphaZero.",None,-1
41298614-69cb-4066-8843-3b52ec774bf1,IndoRobusta: Towards Robustness Against Diverse Code-Mixed Indonesian Local Languages,0.45488,8,"Significant progress has been made on Indonesian NLP. Nevertheless,
exploration of the code-mixing phenomenon in Indonesian is limited, despite
many languages being frequently mixed with Indonesian in daily conversation. In
this work, we explore code-mixing in Indonesian with four embedded languages,
i.e., English, Sundanese, Javanese, and Malay; and introduce IndoRobusta, a
framework to evaluate and improve the code-mixing robustness. Our analysis
shows that the pre-training corpus bias affects the model's ability to better
handle Indonesian-English code-mixing when compared to other local languages,
despite having higher language diversity.",None,-1
8ced093b-c360-4524-a04b-0930c4ae4125,SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding,0.237694,2,"Modern speech processing systems rely on self-attention. Unfortunately, token
mixing with self-attention takes quadratic time in the length of the speech
utterance, slowing down inference as well as training and increasing memory
consumption. Cheaper alternatives to self-attention for ASR have been
developed, but they fail to consistently reach the same level of accuracy. This
paper, therefore, proposes a novel linear-time alternative to self-attention.
It summarises an utterance with the mean over vectors for all time steps. This
single summary is then combined with time-specific information. We call this
method ""SummaryMixing"". Introducing SummaryMixing in state-of-the-art ASR
models makes it feasible to preserve or exceed previous speech recognition
performance while lowering the training and inference times by up to 28$\%$ and
reducing the memory budget by a factor of two. The benefits of SummaryMixing
can also be generalized to other speech-processing tasks, such as speech
understanding.",None,-1
adcb9be0-181c-4946-99c1-f13f520b8900,LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain,0.599472,29,"Lately, propelled by the phenomenal advances around the transformer
architecture, the legal NLP field has enjoyed spectacular growth. To measure
progress, well curated and challenging benchmarks are crucial. However, most
benchmarks are English only and in legal NLP specifically there is no
multilingual benchmark available yet. Additionally, many benchmarks are
saturated, with the best models clearly outperforming the best humans and
achieving near perfect scores. We survey the legal NLP literature and select 11
datasets covering 24 languages, creating LEXTREME. To provide a fair
comparison, we propose two aggregate scores, one based on the datasets and one
on the languages. The best baseline (XLM-R large) achieves both a dataset
aggregate score a language aggregate score of 61.3. This indicates that
LEXTREME is still very challenging and leaves ample room for improvement. To
make it easy for researchers and practitioners to use, we release LEXTREME on
huggingface together with all the code required to evaluate models and a public
Weights and Biases project with all the runs.",None,-1
72f0a652-02fc-41ea-8854-5cca096b838f,Offline-to-Online Knowledge Distillation for Video Instance Segmentation,0.0802377,1,"In this paper, we present offline-to-online knowledge distillation (OOKD) for
video instance segmentation (VIS), which transfers a wealth of video knowledge
from an offline model to an online model for consistent prediction. Unlike
previous methods that having adopting either an online or offline model, our
single online model takes advantage of both models by distilling offline
knowledge. To transfer knowledge correctly, we propose query filtering and
association (QFA), which filters irrelevant queries to exact instances. Our KD
with QFA increases the robustness of feature matching by encoding
object-centric features from a single frame supplemented by long-range global
information. We also propose a simple data augmentation scheme for knowledge
distillation in the VIS task that fairly transfers the knowledge of all classes
into the online model. Extensive experiments show that our method significantly
improves the performance in video instance segmentation, especially for
challenging datasets including long, dynamic sequences. Our method also
achieves state-of-the-art performance on YTVIS-21, YTVIS-22, and OVIS datasets,
with mAP scores of 46.1%, 43.6%, and 31.1%, respectively.",None,-1
366b9f86-4da7-4ccc-add6-928aafc8306f,Infinite Photorealistic Worlds using Procedural Generation,0.343849,29,"We introduce Infinigen, a procedural generator of photorealistic 3D scenes of
the natural world. Infinigen is entirely procedural: every asset, from shape to
texture, is generated from scratch via randomized mathematical rules, using no
external source and allowing infinite variation and composition. Infinigen
offers broad coverage of objects and scenes in the natural world including
plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and
snow. Infinigen can be used to generate unlimited, diverse training data for a
wide range of computer vision tasks including object detection, semantic
segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a
useful resource for computer vision research and beyond. Please visit
https://infinigen.org for videos, code and pre-generated data.",None,-1
43f6f814-9568-47ba-8be3-40e6aa541b3e,Facial Expression Recognition at the Edge: CPU vs GPU vs VPU vs TPU,0.688763,4,"Facial Expression Recognition (FER) plays an important role in human-computer
interactions and is used in a wide range of applications. Convolutional Neural
Networks (CNN) have shown promise in their ability to classify human facial
expressions, however, large CNNs are not well-suited to be implemented on
resource- and energy-constrained IoT devices. In this work, we present a
hierarchical framework for developing and optimizing hardware-aware CNNs tuned
for deployment at the edge. We perform a comprehensive analysis across various
edge AI accelerators including NVIDIA Jetson Nano, Intel Neural Compute Stick,
and Coral TPU. Using the proposed strategy, we achieved a peak accuracy of
99.49% when testing on the CK+ facial expression recognition dataset.
Additionally, we achieved a minimum inference latency of 0.39 milliseconds and
a minimum power consumption of 0.52 Watts.",None,-1
d033b69a-e7d3-41df-9fce-dc64f82f65a4,Imitating Human Behaviour with Diffusion Models,1.0,106,"Diffusion models have emerged as powerful generative models in the
text-to-image domain. This paper studies their application as
observation-to-action models for imitating human behaviour in sequential
environments. Human behaviour is stochastic and multimodal, with structured
correlations between action dimensions. Meanwhile, standard modelling choices
in behaviour cloning are limited in their expressiveness and may introduce bias
into the cloned policy. We begin by pointing out the limitations of these
choices. We then propose that diffusion models are an excellent fit for
imitating human behaviour, since they learn an expressive distribution over the
joint action space. We introduce several innovations to make diffusion models
suitable for sequential environments; designing suitable architectures,
investigating the role of guidance, and developing reliable sampling
strategies. Experimentally, diffusion models closely match human demonstrations
in a simulated robotic control task and a modern 3D gaming environment.",None,-1
998eaa36-6f26-4377-8aec-ff23d3067935,Multiagent Inverse Reinforcement Learning via Theory of Mind Reasoning,0.687686,6,"We approach the problem of understanding how people interact with each other
in collaborative settings, especially when individuals know little about their
teammates, via Multiagent Inverse Reinforcement Learning (MIRL), where the goal
is to infer the reward functions guiding the behavior of each individual given
trajectories of a team's behavior during some task. Unlike current MIRL
approaches, we do not assume that team members know each other's goals a
priori; rather, that they collaborate by adapting to the goals of others
perceived by observing their behavior, all while jointly performing a task. To
address this problem, we propose a novel approach to MIRL via Theory of Mind
(MIRL-ToM). For each agent, we first use ToM reasoning to estimate a posterior
distribution over baseline reward profiles given their demonstrated behavior.
We then perform MIRL via decentralized equilibrium by employing single-agent
Maximum Entropy IRL to infer a reward function for each agent, where we
simulate the behavior of other teammates according to the time-varying
distribution over profiles. We evaluate our approach in a simulated 2-player
search-and-rescue operation where the goal of the agents, playing different
roles, is to search for and evacuate victims in the environment. Our results
show that the choice of baseline profiles is paramount to the recovery of the
ground-truth rewards, and that MIRL-ToM is able to recover the rewards used by
agents interacting both with known and unknown teammates.",None,-1
b7a2607c-6898-4fe3-9628-b22704bcc8ad,Adaptive Voronoi NeRFs,0.150198,3,"Neural Radiance Fields (NeRFs) learn to represent a 3D scene from just a set
of registered images. Increasing sizes of a scene demands more complex
functions, typically represented by neural networks, to capture all details.
Training and inference then involves querying the neural network millions of
times per image, which becomes impractically slow. Since such complex functions
can be replaced by multiple simpler functions to improve speed, we show that a
hierarchy of Voronoi diagrams is a suitable choice to partition the scene. By
equipping each Voronoi cell with its own NeRF, our approach is able to quickly
learn a scene representation. We propose an intuitive partitioning of the space
that increases quality gains during training by distributing information evenly
among the networks and avoids artifacts through a top-down adaptive refinement.
Our framework is agnostic to the underlying NeRF method and easy to implement,
which allows it to be applied to various NeRF variants for improved learning
and rendering speeds.",None,-1
64bdb356-dfe4-4f16-8a49-d2eb91641020,Correlating Medi-Claim Service by Deep Learning Neural Networks,0.443849,1,"Medical insurance claims are of organized crimes related to patients,
physicians, diagnostic centers, and insurance providers, forming a chain
reaction that must be monitored constantly. These kinds of frauds affect the
financial growth of both insured people and health insurance companies. The
Convolution Neural Network architecture is used to detect fraudulent claims
through a correlation study of regression models, which helps to detect money
laundering on different claims given by different providers. Supervised and
unsupervised classifiers are used to detect fraud and non-fraud claims.",None,-1
143d6c3d-84a7-416f-a211-9cdbdde27cfe,Towards CGAN-based Satellite Image Synthesis with Partial Pixel-Wise Annotation,0.0351642,1,"Conditional Generative Adversarial Nets (CGANs) need a significantly huge
dataset with a detailed pixel-wise annotation to generate high-quality images.
Unfortunately, any amount of missing pixel annotations may significantly impact
the result not only locally, but also in annotated areas. To the best of our
knowledge, such a challenge has never been investigated in the broader field of
GANs. In this paper, we take the first step in this direction to study the
problem of CGAN-based satellite image synthesis given partially annotated
images. We first define the problem of image synthesis using partially
annotated data, and we discuss a scenario in which we face such a challenge. We
then propose an effective solution called detail augmentation to address this
problem. To do so, we tested two different approaches to augment details to
compensate for missing pixel-wise annotations. In the first approach, we
augmented the original images with their Canny edges to using the CGAN to
compensate for the missing annotations. The second approach, however, attempted
to assign a color to all pixels with missing annotation. Eventually, a
different CGAN was trained to translate the new feature images into a final
output.",None,-1
0f00aba3-5e18-4043-bcb3-73ae4fea820c,Enhanced Meta Label Correction for Coping with Label Corruption,0.0880462,1,"Traditional methods for learning with the presence of noisy labels have
successfully handled datasets with artificially injected noise but still fall
short of adequately handling real-world noise. With the increasing use of
meta-learning in the diverse fields of machine learning, researchers leveraged
auxiliary small clean datasets to meta-correct the training labels.
Nonetheless, existing meta-label correction approaches are not fully exploiting
their potential. In this study, we propose an Enhanced Meta Label Correction
approach abbreviated as EMLC for the learning with noisy labels (LNL) problem.
We re-examine the meta-learning process and introduce faster and more accurate
meta-gradient derivations. We propose a novel teacher architecture tailored
explicitly to the LNL problem, equipped with novel training objectives. EMLC
outperforms prior approaches and achieves state-of-the-art results in all
standard benchmarks. Notably, EMLC enhances the previous art on the noisy
real-world dataset Clothing1M by $1.52\%$ while requiring $\times 0.5$ the time
per epoch and with much faster convergence of the meta-objective when compared
to the baseline approach.",None,-1
5a8fb3dc-dd03-45a4-aa69-c4479cc6a869,ProtoCon: Pseudo-label Refinement via Online Clustering and Prototypical Consistency for Efficient Semi-supervised Learning,0.594645,7,"Confidence-based pseudo-labeling is among the dominant approaches in
semi-supervised learning (SSL). It relies on including high-confidence
predictions made on unlabeled data as additional targets to train the model. We
propose ProtoCon, a novel SSL method aimed at the less-explored label-scarce
SSL where such methods usually underperform. ProtoCon refines the pseudo-labels
by leveraging their nearest neighbours' information. The neighbours are
identified as the training proceeds using an online clustering approach
operating in an embedding space trained via a prototypical loss to encourage
well-formed clusters. The online nature of ProtoCon allows it to utilise the
label history of the entire dataset in one training cycle to refine labels in
the following cycle without the need to store image embeddings. Hence, it can
seamlessly scale to larger datasets at a low cost. Finally, ProtoCon addresses
the poor training signal in the initial phase of training (due to fewer
confident predictions) by introducing an auxiliary self-supervised loss. It
delivers significant gains and faster convergence over state-of-the-art across
5 datasets, including CIFARs, ImageNet and DomainNet.",None,-1
410e36f5-13af-48a6-9504-dc5d5d9efde9,Simple diffusion: End-to-end diffusion for high resolution images,0.999643,128,"Currently, applying diffusion models in pixel space of high resolution images
is difficult. Instead, existing approaches focus on diffusion in lower
dimensional spaces (latent diffusion), or have multiple super-resolution levels
of generation referred to as cascades. The downside is that these approaches
add additional complexity to the diffusion framework.
  This paper aims to improve denoising diffusion for high resolution images
while keeping the model as simple as possible. The paper is centered around the
research question: How can one train a standard denoising diffusion models on
high resolution images, and still obtain performance comparable to these
alternate approaches?
  The four main findings are: 1) the noise schedule should be adjusted for high
resolution images, 2) It is sufficient to scale only a particular part of the
architecture, 3) dropout should be added at specific locations in the
architecture, and 4) downsampling is an effective strategy to avoid high
resolution feature maps. Combining these simple yet effective techniques, we
achieve state-of-the-art on image generation among diffusion models without
sampling modifiers on ImageNet.",None,-1
c9a98381-b61c-4d93-900c-fdf61621d997,Measuring a Priori Voting Power -- Taking Delegations Seriously,0.440599,2,"We introduce new power indices to measure the a priori voting power of voters
in liquid democracy elections where an underlying network restricts
delegations. We argue that our power indices are natural extensions of the
standard Penrose-Banzhaf index in simple voting games. We show that computing
the criticality of a voter is #P-hard even when voting weights are
polynomially-bounded in the size of the instance. However, for specific
settings, such as when the underlying network is a bipartite or complete graph,
recursive formulas can compute these indices for weighted voting games in
pseudo-polynomial time. We highlight their theoretical properties and provide
numerical results to illustrate how restricting the possible delegations can
alter voters' voting power.",None,-1
682ff7f2-8760-4784-abe3-e9fd8b93ccd3,Knowledge Restore and Transfer for Multi-label Class-Incremental Learning,0.639713,4,"Current class-incremental learning research mainly focuses on single-label
classification tasks while multi-label class-incremental learning (MLCIL) with
more practical application scenarios is rarely studied. Although there have
been many anti-forgetting methods to solve the problem of catastrophic
forgetting in class-incremental learning, these methods have difficulty in
solving the MLCIL problem due to label absence and information dilution. In
this paper, we propose a knowledge restore and transfer (KRT) framework for
MLCIL, which includes a dynamic pseudo-label (DPL) module to restore the old
class knowledge and an incremental cross-attention(ICA) module to save
session-specific knowledge and transfer old class knowledge to the new model
sufficiently. Besides, we propose a token loss to jointly optimize the
incremental cross-attention module. Experimental results on MS-COCO and PASCAL
VOC datasets demonstrate the effectiveness of our method for improving
recognition performance and mitigating forgetting on multi-label
class-incremental learning tasks.",None,-1
eb9b8a1a-e551-4036-bcf4-d935f636010a,Spatial Knowledge-Infused Hierarchical Learning: An Application in Flood Mapping on Earth Imagery,0.522586,4,"Deep learning for Earth imagery plays an increasingly important role in
geoscience applications such as agriculture, ecology, and natural disaster
management. Still, progress is often hindered by the limited training labels.
Given Earth imagery with limited training labels, a base deep neural network
model, and a spatial knowledge base with label constraints, our problem is to
infer the full labels while training the neural network. The problem is
challenging due to the sparse and noisy input labels, spatial uncertainty
within the label inference process, and high computational costs associated
with a large number of sample locations. Existing works on neuro-symbolic
models focus on integrating symbolic logic into neural networks (e.g., loss
function, model architecture, and training label augmentation), but these
methods do not fully address the challenges of spatial data (e.g., spatial
uncertainty, the trade-off between spatial granularity and computational
costs). To bridge this gap, we propose a novel Spatial Knowledge-Infused
Hierarchical Learning (SKI-HL) framework that iteratively infers sample labels
within a multi-resolution hierarchy. Our framework consists of a module to
selectively infer labels in different resolutions based on spatial uncertainty
and a module to train neural network parameters with uncertainty-aware
multi-instance learning. Extensive experiments on real-world flood mapping
datasets show that the proposed model outperforms several baseline methods. The
code is available at \url{https://github.com/ZelinXu2000/SKI-HL}.",None,-1
e2e81b56-5fcf-41b0-8cef-2ebeb282b4b1,Improving Deep Policy Gradients with Value Function Search,0.234072,5,"Deep Policy Gradient (PG) algorithms employ value networks to drive the
learning of parameterized policies and reduce the variance of the gradient
estimates. However, value function approximation gets stuck in local optima and
struggles to fit the actual return, limiting the variance reduction efficacy
and leading policies to sub-optimal performance. This paper focuses on
improving value approximation and analyzing the effects on Deep PG primitives
such as value prediction, variance reduction, and correlation of gradient
estimates with the true gradient. To this end, we introduce a Value Function
Search that employs a population of perturbed value networks to search for a
better approximation. Our framework does not require additional environment
interactions, gradient computations, or ensembles, providing a computationally
inexpensive approach to enhance the supervised learning task on which value
networks train. Crucially, we show that improving Deep PG primitives results in
improved sample efficiency and policies with higher returns using common
continuous control benchmark domains.",None,-1
17aaa07a-864d-4e10-bf32-af0134bb860e,Cloud K-SVD for Image Denoising,0.0303642,1,"Cloud K-SVD is a dictionary learning algorithm that can train at multiple
nodes and hereby produce a mutual dictionary to represent low-dimensional
geometric structures in image data. We present a novel application of the
algorithm as we use it to recover both noiseless and noisy images from
overlapping patches. We implement a node network in Kubernetes using Docker
containers to facilitate Cloud K-SVD. Results show that Cloud K-SVD can recover
images approximately and remove quantifiable amounts of noise from benchmark
gray-scaled images without sacrificing accuracy in recovery; we achieve an SSIM
index of 0.88, 0.91 and 0.95 between clean and recovered images for noise
levels ($\mu$ = 0, $\sigma^{2}$ = 0.01, 0.005, 0.001), respectively, which is
similar to SOTA in the field. Cloud K-SVD is evidently able to learn a mutual
dictionary across multiple nodes and remove AWGN from images. The mutual
dictionary can be used to recover a specific image at any of the nodes in the
network.",None,-1
039b7ee5-4520-4b72-8c15-dbe96bf58948,Otter: A Multi-Modal Model with In-Context Instruction Tuning,1.0,351,"Large language models (LLMs) have demonstrated significant universal
capabilities as few/zero-shot learners in various tasks due to their
pre-training on vast amounts of text data, as exemplified by GPT-3, which
boosted to InstrctGPT and ChatGPT, effectively following natural language
instructions to accomplish real-world tasks. In this paper, we propose to
introduce instruction tuning into multi-modal models, motivated by the Flamingo
model's upstream interleaved format pretraining dataset. We adopt a similar
approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT)
dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo
(open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and
showcasing improved instruction-following ability and in-context learning. We
also optimize OpenFlamingo's implementation for researchers, democratizing the
required training resources from 1$\times$ A100 GPU to 4$\times$ RTX-3090 GPUs,
and integrate both OpenFlamingo and Otter into Huggingface Transformers for
more researchers to incorporate the models into their customized training and
inference pipelines.",None,-1
b4e00046-0645-4c9e-8d2a-bc032c5de30d,Robust face anti-spoofing framework with Convolutional Vision Transformer,0.85237,4,"Owing to the advances in image processing technology and large-scale
datasets, companies have implemented facial authentication processes, thereby
stimulating increased focus on face anti-spoofing (FAS) against realistic
presentation attacks. Recently, various attempts have been made to improve face
recognition performance using both global and local learning on face images;
however, to the best of our knowledge, this is the first study to investigate
whether the robustness of FAS against domain shifts is improved by considering
global information and local cues in face images captured using self-attention
and convolutional layers. This study proposes a convolutional vision
transformer-based framework that achieves robust performance for various unseen
domain data. Our model resulted in 7.3%$p$ and 12.9%$p$ increases in FAS
performance compared to models using only a convolutional neural network or
vision transformer, respectively. It also shows the highest average rank in
sub-protocols of cross-dataset setting over the other nine benchmark models for
domain generalization.",None,-1
6d73fcca-2e5d-4fb0-8185-6bc553c06f5c,HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge,0.190751,2,"The semantic parsing-based method is an important research branch for
knowledge-based question answering. It usually generates executable programs
lean upon the question and then conduct them to reason answers over a knowledge
base. Benefit from this inherent mechanism, it has advantages in the
performance and the interpretability. However, traditional semantic parsing
methods usually generate a complete program before executing it, which
struggles with multi-hop question answering over heterogeneous knowledge. On
one hand, generating a complete multi-hop program relies on multiple
heterogeneous supporting facts, and it is difficult for generators to
understand these facts simultaneously. On the other hand, this way ignores the
semantic information of the intermediate answers at each hop, which is
beneficial for subsequent generation. To alleviate these challenges, we propose
a self-iterative framework for multi-hop program generation (HopPG) over
heterogeneous knowledge, which leverages the previous execution results to
retrieve supporting facts and generate subsequent programs hop by hop. We
evaluate our model on MMQA-T^2, and the experimental results show that HopPG
outperforms existing semantic-parsing-based baselines, especially on the
multi-hop questions.",None,-1
959819d3-9732-4b68-b010-aa6d31ee97f9,A Human-Centered Safe Robot Reinforcement Learning Framework with Interactive Behaviors,0.576342,8,"Deployment of Reinforcement Learning (RL) algorithms for robotics
applications in the real world requires ensuring the safety of the robot and
its environment. Safe Robot RL (SRRL) is a crucial step towards achieving
human-robot coexistence. In this paper, we envision a human-centered SRRL
framework consisting of three stages: safe exploration, safety value alignment,
and safe collaboration. We examine the research gaps in these areas and propose
to leverage interactive behaviors for SRRL. Interactive behaviors enable
bi-directional information transfer between humans and robots, such as
conversational robot ChatGPT. We argue that interactive behaviors need further
attention from the SRRL community. We discuss four open challenges related to
the robustness, efficiency, transparency, and adaptability of SRRL with
interactive behaviors.",None,-1
d5951162-d6f3-44d7-aea3-0838a8decfef,Multilevel Large Language Models for Everyone,0.0334822,4,"Large language models have made significant progress in the past few years.
However, they are either generic {\it or} field specific, splitting the
community into different groups. In this paper, we unify these large language
models into a larger map, where the generic {\it and} specific models are
linked together and can improve each other, based on the user personal input
and information from the internet. The idea of linking several large language
models together is inspired by the functionality of human brain. The specific
regions on the brain cortex are specific for certain low level functionality.
And these regions can jointly work together to achieve more complex high level
functionality. Such behavior on human brain cortex sheds the light to design
the multilevel large language models that contain global level, field level and
user level models. The user level models run on local machines to achieve
efficient response and protect the user's privacy. Such multilevel models
reduce some redundancy and perform better than the single level models. The
proposed multilevel idea can be applied in various applications, such as
natural language processing, computer vision tasks, professional assistant,
business and healthcare.",None,-1
3f58438c-85e7-4a36-a5a2-017bf40141b1,Zero-Shot Image Harmonization with Generative Model Prior,0.416548,3,"We propose a zero-shot approach to image harmonization, aiming to overcome
the reliance on large amounts of synthetic composite images in existing
methods. These methods, while showing promising results, involve significant
training expenses and often struggle with generalization to unseen images. To
this end, we introduce a fully modularized framework inspired by human
behavior. Leveraging the reasoning capabilities of recent foundation models in
language and vision, our approach comprises three main stages. Initially, we
employ a pretrained vision-language model (VLM) to generate descriptions for
the composite image. Subsequently, these descriptions guide the foreground
harmonization direction of a text-to-image generative model (T2I). We refine
text embeddings for enhanced representation of imaging conditions and employ
self-attention and edge maps for structure preservation. Following each
harmonization iteration, an evaluator determines whether to conclude or modify
the harmonization direction. The resulting framework, mirroring human behavior,
achieves harmonious results without the need for extensive training. We present
compelling visual results across diverse scenes and objects, along with a user
study validating the effectiveness of our approach.",None,-1
715ca8ac-2502-466e-8ba8-fd70b54168c2,Wavelet-based Unsupervised Label-to-Image Translation,0.312598,6,"Semantic Image Synthesis (SIS) is a subclass of image-to-image translation
where a semantic layout is used to generate a photorealistic image.
State-of-the-art conditional Generative Adversarial Networks (GANs) need a huge
amount of paired data to accomplish this task while generic unpaired
image-to-image translation frameworks underperform in comparison, because they
color-code semantic layouts and learn correspondences in appearance instead of
semantic content. Starting from the assumption that a high quality generated
image should be segmented back to its semantic layout, we propose a new
Unsupervised paradigm for SIS (USIS) that makes use of a self-supervised
segmentation loss and whole image wavelet based discrimination. Furthermore, in
order to match the high-frequency distribution of real images, a novel
generator architecture in the wavelet domain is proposed. We test our
methodology on 3 challenging datasets and demonstrate its ability to bridge the
performance gap between paired and unpaired models.",None,-1
436027b0-0332-469a-a3a0-adfceed4853f,Is More Always Better? The Effects of Personal Characteristics and Level of Detail on the Perception of Explanations in a Recommender System,0.879326,18,"Despite the acknowledgment that the perception of explanations may vary
considerably between end-users, explainable recommender systems (RS) have
traditionally followed a one-size-fits-all model, whereby the same explanation
level of detail is provided to each user, without taking into consideration
individual user's context, i.e., goals and personal characteristics. To fill
this research gap, we aim in this paper at a shift from a one-size-fits-all to
a personalized approach to explainable recommendation by giving users agency in
deciding which explanation they would like to see. We developed a transparent
Recommendation and Interest Modeling Application (RIMA) that provides on-demand
personalized explanations of the recommendations, with three levels of detail
(basic, intermediate, advanced) to meet the demands of different types of
end-users. We conducted a within-subject study (N=31) to investigate the
relationship between user's personal characteristics and the explanation level
of detail, and the effects of these two variables on the perception of the
explainable RS with regard to different explanation goals. Our results show
that the perception of explainable RS with different levels of detail is
affected to different degrees by the explanation goal and user type.
Consequently, we suggested some theoretical and design guidelines to support
the systematic design of explanatory interfaces in RS tailored to the user's
context.",None,-1
2b1bdd88-f644-45d6-a056-df16d5fde592,Metacognitive threshold: a computational account,0.640985,1,"This paper will explore ways of computationally accounting for the
metacognitive threshold -- the minimum amount of stimulus needed for a mental
state to be perceived -- and discuss potential cognitive mechanisms by which
this threshold can be influenced through metacognitive training and meditation.",None,-1
4f1272de-75a0-4f82-ae17-37e142ad5159,"Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety",0.151271,2,"Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.",None,-1
3a15ddee-94c7-48f9-9bc2-e8acb00179a7,On Training Derivative-Constrained Neural Networks,0.337359,1,"We refer to the setting where the (partial) derivatives of a neural network's
(NN's) predictions with respect to its inputs are used as additional training
signal as a derivative-constrained (DC) NN. This situation is common in
physics-informed settings in the natural sciences. We propose an integrated
RELU (IReLU) activation function to improve training of DC NNs. We also
investigate denormalization and label rescaling to help stabilize DC training.
We evaluate our methods on physics-informed settings including quantum
chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that
existing architectures with IReLU activations combined with denormalization and
label rescaling better incorporate training signal provided by derivative
constraints.",None,-1
881ada31-cd49-451a-b779-f0271f86f713,How optimal transport can tackle gender biases in multi-class neural-network classifiers for job recommendations?,0.232933,3,"Automatic recommendation systems based on deep neural networks have become
extremely popular during the last decade. Some of these systems can however be
used for applications which are ranked as High Risk by the European Commission
in the A.I. act, as for instance for online job candidate recommendation. When
used in the European Union, commercial AI systems for this purpose will then be
required to have to proper statistical properties with regard to potential
discrimination they could engender. This motivated our contribution, where we
present a novel optimal transport strategy to mitigate undesirable algorithmic
biases in multi-class neural-network classification. Our stratey is model
agnostic and can be used on any multi-class classification neural-network
model. To anticipate the certification of recommendation systems using textual
data, we then used it on the Bios dataset, for which the learning task consists
in predicting the occupation of female and male individuals, based on their
LinkedIn biography. Results show that it can reduce undesired algorithmic
biases in this context to lower levels than a standard strategy.",None,-1
fe38b8f1-e24b-4731-9185-c4fc72736657,CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare,0.652744,8,"In the era of modern healthcare, swiftly generating medical question
summaries is crucial for informed and timely patient care. Despite the
increasing complexity and volume of medical data, existing studies have focused
solely on text-based summarization, neglecting the integration of visual
information. Recognizing the untapped potential of combining textual queries
with visual representations of medical conditions, we introduce the Multimodal
Medical Question Summarization (MMQS) Dataset. This dataset, a major
contribution to our work, pairs medical queries with visual aids, facilitating
a richer and more nuanced understanding of patient needs. We also propose a
framework, utilizing the power of Contrastive Language Image Pretraining(CLIP)
and Large Language Models(LLMs), consisting of four modules that identify
medical disorders, generate relevant context, filter medical concepts, and
craft visually aware summaries. Our comprehensive framework harnesses the power
of CLIP, a multimodal foundation model, and various general-purpose LLMs,
comprising four main modules: the medical disorder identification module, the
relevant context generation module, the context filtration module for
distilling relevant medical concepts and knowledge, and finally, a
general-purpose LLM to generate visually aware medical question summaries.
Leveraging our MMQS dataset, we showcase how visual cues from images enhance
the generation of medically nuanced summaries. This multimodal approach not
only enhances the decision-making process in healthcare but also fosters a more
nuanced understanding of patient queries, laying the groundwork for future
research in personalized and responsive medical care",None,-1
627d24a3-f017-4c2a-bccf-f5355171587e,Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation,0.977306,13,"Generative AI and large language models hold great promise in enhancing
programming education by automatically generating individualized feedback for
students. We investigate the role of generative AI models in providing human
tutor-style programming hints to help students resolve errors in their buggy
programs. Recent works have benchmarked state-of-the-art models for various
feedback generation scenarios; however, their overall quality is still inferior
to human tutors and not yet ready for real-world deployment. In this paper, we
seek to push the limits of generative AI models toward providing high-quality
programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a
first step, our technique leverages GPT-4 as a ``tutor'' model to generate
hints -- it boosts the generative quality by using symbolic information of
failing test cases and fixes in prompts. As a next step, our technique
leverages GPT-3.5, a weaker model, as a ``student'' model to further validate
the hint quality -- it performs an automatic quality validation by simulating
the potential utility of providing this feedback. We show the efficacy of our
technique via extensive evaluation using three real-world datasets of Python
programs covering a variety of concepts ranging from basic algorithms to
regular expressions and data analysis using pandas library.",None,-1
4e6af1a9-aca9-4855-80a3-c18a25cd732a,Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization,0.21046,11,"Generating persona consistent dialogue response is important for developing
an intelligent conversational agent. Recent works typically fine-tune
large-scale pre-trained models on this task by concatenating persona texts and
dialogue history as a single input sequence to generate the target response.
While simple and effective, our analysis shows that this popular practice is
seriously affected by order sensitivity where different input orders of persona
sentences significantly impact the quality and consistency of generated
response, resulting in severe performance fluctuations (i.e., 29.4% on GPT2 and
83.2% on BART). To mitigate the order sensitivity problem, we propose a
model-agnostic framework, ORder Insensitive Generation (ORIG), which enables
dialogue models to learn robust representation under different persona orders
and improve the consistency of response generation. Experiments on the
Persona-Chat dataset justify the effectiveness and superiority of our method
with two dominant pre-trained models (GPT2 and BART).",None,-1
153c8308-1918-4986-bfed-a98f1e36dea3,Redesigning Electronic Health Record Systems to Support Developing Countries,0.181433,1,"Electronic Health Record (EHR) has become an essential tool in the healthcare
ecosystem, providing authorized clinicians with patients' health-related
information for better treatment. While most developed countries are taking
advantage of EHRs to improve their healthcare system, it remains challenging in
developing countries to support clinical decision-making and public health
using a computerized patient healthcare information system. This paper proposes
a novel EHR architecture suitable for developing countries--an architecture
that fosters inclusion and provides solutions tailored to all social classes
and socioeconomic statuses. Our architecture foresees an internet-free
(offline) solution to allow medical transactions between healthcare
organizations, and the storage of EHRs in geographically underserved and rural
areas. Moreover, we discuss how artificial intelligence can leverage anonymous
health-related information to enable better public health policy and
surveillance.",None,-1
5755136d-505c-4f69-a900-33281bf26c4a,Neural Machine Translation for Code Generation,0.483239,4,"Neural machine translation (NMT) methods developed for natural language
processing have been shown to be highly successful in automating translation
from one natural language to another. Recently, these NMT methods have been
adapted to the generation of program code. In NMT for code generation, the task
is to generate output source code that satisfies constraints expressed in the
input. In the literature, a variety of different input scenarios have been
explored, including generating code based on natural language description,
lower-level representations such as binary or assembly (neural decompilation),
partial representations of source code (code completion and repair), and source
code in another language (code translation). In this paper we survey the NMT
for code generation literature, cataloging the variety of methods that have
been explored according to input and output representations, model
architectures, optimization techniques used, data sets, and evaluation methods.
We discuss the limitations of existing methods and future research directions",None,-1
976e6a34-5c7a-41c5-ad4d-c2db0ea4f48f,Prediction and Interpretation of Vehicle Trajectories in the Graph Spectral Domain,0.162449,1,"This work provides a comprehensive analysis and interpretation of the graph
spectral representation of traffic scenarios. Based on a spatio-temporal
vehicle interaction graph, an observed traffic scenario can be transformed into
the graph spectral domain by means of the multidimensional Graph Fourier
Transformation. Since these spectral scenario representations have shown to
successfully incorporate the complex and interactive nature of traffic
scenarios, the beneficial feature representation is employed for the purpose of
predicting vehicle trajectories. This work introduces GFTNNv2, a deep learning
network predicting vehicle trajectories in the graph spectral domain.
Evaluation of the GFTNNv2 on the publicly available datasets highD and NGSIM
shows a performance gain of up to 25% in comparison to state-of-the-art
prediction approaches.",None,-1
c2962632-2967-4a48-89db-9a5edbcd90e8,The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring,0.152028,1,"Neural networks and in particular the attention mechanism have brought
significant advances to the field of Automated Essay Scoring. Many of these
systems use a regression-based model which may be prone to underfitting when
the model only predicts the mean of the training data. In this paper, we
present a dynamic loss function that creates an incentive for the model to
predict with the correct distribution, as well as predicting the correct
values. Our loss function achieves this goal without sacrificing any
performance achieving a Quadratic Weighted Kappa score of 0.752 on the
Automated Student Assessment Prize Automated Essay Scoring dataset.",None,-1
884b99e0-b3be-4c67-8ef8-a8c93f47e5b6,Integrity and Junkiness Failure Handling for Embedding-based Retrieval: A Case Study in Social Network Search,0.331766,6,"Embedding based retrieval has seen its usage in a variety of search
applications like e-commerce, social networking search etc. While the approach
has demonstrated its efficacy in tasks like semantic matching and contextual
search, it is plagued by the problem of uncontrollable relevance. In this
paper, we conduct an analysis of embedding-based retrieval launched in early
2021 on our social network search engine, and define two main categories of
failures introduced by it, integrity and junkiness. The former refers to issues
such as hate speech and offensive content that can severely harm user
experience, while the latter includes irrelevant results like fuzzy text
matching or language mismatches. Efficient methods during model inference are
further proposed to resolve the issue, including indexing treatments and
targeted user cohort treatments, etc. Though being simple, we show the methods
have good offline NDCG and online A/B tests metrics gain in practice. We
analyze the reasons for the improvements, pointing out that our methods are
only preliminary attempts to this important but challenging problem. We put
forward potential future directions to explore.",None,-1
6a7c21be-1368-4365-9a8f-8b8a1c4e445b,Ultrasound Image Reconstruction with Denoising Diffusion Restoration Models,0.290233,4,"Ultrasound image reconstruction can be approximately cast as a linear inverse
problem that has traditionally been solved with penalized optimization using
the $l_1$ or $l_2$ norm, or wavelet-based terms. However, such regularization
functions often struggle to balance the sparsity and the smoothness. A
promising alternative is using learned priors to make the prior knowledge
closer to reality. In this paper, we rely on learned priors under the framework
of Denoising Diffusion Restoration Models (DDRM), initially conceived for
restoration tasks with natural images. We propose and test two adaptions of
DDRM to ultrasound inverse problem models, DRUS and WDRUS. Our experiments on
synthetic and PICMUS data show that from a single plane wave our method can
achieve image quality comparable to or better than DAS and state-of-the-art
methods. The code is available at:
https://github.com/Yuxin-Zhang-Jasmine/DRUS-v1.",None,-1
566b3ac7-4541-43df-9d80-e2ee099fc30c,Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding,0.192295,7,"Conversational AI systems such as Alexa need to understand defective queries
to ensure robust conversational understanding and reduce user friction. These
defective queries often arise from user ambiguities, mistakes, or errors in
automatic speech recognition (ASR) and natural language understanding (NLU).
  Personalized query rewriting is an approach that focuses on reducing defects
in queries by taking into account the user's individual behavior and
preferences. It typically relies on an index of past successful user
interactions with the conversational AI. However, unseen interactions within
the user's history present additional challenges for personalized query
rewriting. This paper presents our ""Collaborative Query Rewriting"" approach,
which specifically addresses the task of rewriting new user interactions that
have not been previously observed in the user's history. This approach builds a
""User Feedback Interaction Graph"" (FIG) of historical user-entity interactions
and leverages multi-hop graph traversal to enrich each user's index to cover
future unseen defective queries. The enriched user index is called a
Collaborative User Index and contains hundreds of additional entries. To
counteract precision degradation from the enlarged index, we add additional
transformer layers to the L1 retrieval model and incorporate graph-based and
guardrail features into the L2 ranking model.
  Since the user index can be pre-computed, we further investigate the
utilization of a Large Language Model (LLM) to enhance the FIG for user-entity
link prediction in the Video/Music domains. Specifically, this paper
investigates the Dolly-V2 7B model. We found that the user index augmented by
the fine-tuned Dolly-V2 generation significantly enhanced the coverage of
future unseen user interactions, thereby boosting QR performance on unseen
queries compared with the graph traversal only approach.",None,-1
1e8dc07a-89da-4991-b7d4-b0c83b13e87b,Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games,0.993761,6,"In this study, we explore the application of Large Language Models (LLMs) in
\textit{Jubensha}, a Chinese detective role-playing game and a novel area in
Artificial Intelligence (AI) driven gaming. We introduce the first dataset
specifically for Jubensha, including character scripts and game rules, to
foster AI agent development in this complex narrative environment. Our work
also presents a unique multi-agent interaction framework using LLMs, allowing
AI agents to autonomously engage in this game. To evaluate the gaming
performance of these AI agents, we developed novel methods measuring their
mastery of case information and reasoning skills. Furthermore, we incorporated
the latest advancements in in-context learning to improve the agents'
performance in information gathering, murderer identification, and logical
reasoning. The experimental results validate the effectiveness of our proposed
methods. This work aims to offer a novel perspective on understanding LLM
capabilities and establish a new benchmark for evaluating large language
model-based agents.",None,-1
5ae34471-1ba8-4e02-ac67-818ffbc4f10d,Responsibility Perspective Transfer for Italian Femicide News,0.388712,1,"Different ways of linguistically expressing the same real-world event can
lead to different perceptions of what happened. Previous work has shown that
different descriptions of gender-based violence (GBV) influence the reader's
perception of who is to blame for the violence, possibly reinforcing
stereotypes which see the victim as partly responsible, too. As a contribution
to raise awareness on perspective-based writing, and to facilitate access to
alternative perspectives, we introduce the novel task of automatically
rewriting GBV descriptions as a means to alter the perceived level of
responsibility on the perpetrator. We present a quasi-parallel dataset of
sentences with low and high perceived responsibility levels for the
perpetrator, and experiment with unsupervised (mBART-based), zero-shot and
few-shot (GPT3-based) methods for rewriting sentences. We evaluate our models
using a questionnaire study and a suite of automatic metrics.",None,-1
ef183206-51a9-4292-99f3-c690fa2fde19,"Vision, Deduction and Alignment: An Empirical Study on Multi-modal Knowledge Graph Alignment",0.758037,13,"Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in
knowledge engineering. Existing EA methods mostly focus on utilizing the graph
structures and entity attributes (including literals), but ignore images that
are common in modern multi-modal KGs. In this study we first constructed
Multi-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then
evaluated some existing embedding-based methods for utilizing images. In view
of the complementary nature of visual modal information and logical deduction,
we further developed a new multi-modal EA method named LODEME using logical
deduction and multi-modal KG embedding, with state-of-the-art performance
achieved on Multi-OpenEA and other existing multi-modal EA benchmarks.",None,-1
31e1b877-3aa9-4f3c-be89-bf4c3354c3b3,CLIPER: A Unified Vision-Language Framework for In-the-Wild Facial Expression Recognition,0.874825,9,"Facial expression recognition (FER) is an essential task for understanding
human behaviors. As one of the most informative behaviors of humans, facial
expressions are often compound and variable, which is manifested by the fact
that different people may express the same expression in very different ways.
However, most FER methods still use one-hot or soft labels as the supervision,
which lack sufficient semantic descriptions of facial expressions and are less
interpretable. Recently, contrastive vision-language pre-training (VLP) models
(e.g., CLIP) use text as supervision and have injected new vitality into
various computer vision tasks, benefiting from the rich semantics in text.
Therefore, in this work, we propose CLIPER, a unified framework for both static
and dynamic facial Expression Recognition based on CLIP. Besides, we introduce
multiple expression text descriptors (METD) to learn fine-grained expression
representations that make CLIPER more interpretable. We conduct extensive
experiments on several popular FER benchmarks and achieve state-of-the-art
performance, which demonstrates the effectiveness of CLIPER.",None,-1
d3677590-f6ef-4a78-8aea-7db658a16733,Unveiling and unraveling aggregation and dispersion fallacies in group MCDM,0.257449,2,"Priorities in multi-criteria decision-making (MCDM) convey the relevance
preference of one criterion over another, which is usually reflected by
imposing the non-negativity and unit-sum constraints. The processing of such
priorities is different than other unconstrained data, but this point is often
neglected by researchers, which results in fallacious statistical analysis.
This article studies three prevalent fallacies in group MCDM along with
solutions based on compositional data analysis to avoid misusing statistical
operations. First, we use a compositional approach to aggregate the priorities
of a group of DMs and show that the outcome of the compositional analysis is
identical to the normalized geometric mean, meaning that the arithmetic mean
should be avoided. Furthermore, a new aggregation method is developed, which is
a robust surrogate for the geometric mean. We also discuss the errors in
computing measures of dispersion, including standard deviation and distance
functions. Discussing the fallacies in computing the standard deviation, we
provide a probabilistic criteria ranking by developing proper Bayesian tests,
where we calculate the extent to which a criterion is more important than
another. Finally, we explain the errors in computing the distance between
priorities, and a clustering algorithm is specially tailored based on proper
distance metrics.",None,-1
9074aa60-fae4-4087-a49b-3afe3343a63b,A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment,0.665795,2,"When communicating with elders with cognitive impairment, cognitive
stimulation (CS) help to maintain the cognitive health of elders. Data sparsity
is the main challenge in building CS-based dialogue systems, particularly in
the Chinese language. To fill this gap, we construct a Chinese CS conversation
(CSConv) dataset, which contains about 2.6K groups of dialogues with CS
principles and emotional support strategy labels. Making chit chat while
providing emotional support is overlooked by the majority of existing cognitive
dialogue systems. In this paper, we propose a multi-source knowledge fusion
method for CS dialogue (CSD), to generate open-ended responses guided by the CS
principle and emotional support strategy. We first use a progressive mask
method based on external knowledge to learn encoders as effective classifiers,
which is the prerequisite to predict the CS principle and emotional support
strategy of the target response. Then a decoder interacts with the perceived CS
principle and emotional support strategy to generate responses. Extensive
experiments conducted on the CSConv dataset demonstrate the effectiveness of
the proposed method, while there is still a large space for improvement
compared to human performance.",None,-1
87ac1372-cbb5-4665-aa16-5c3e57cc9359,Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking,0.947539,12,"Transformer-based visual trackers have demonstrated significant progress
owing to their superior modeling capabilities. However, existing trackers are
hampered by low speed, limiting their applicability on devices with limited
computational power. To alleviate this problem, we propose HiT, a new family of
efficient tracking models that can run at high speed on different devices while
retaining high performance. The central idea of HiT is the Bridge Module, which
bridges the gap between modern lightweight transformers and the tracking
framework. The Bridge Module incorporates the high-level information of deep
features into the shallow large-resolution features. In this way, it produces
better features for the tracking head. We also propose a novel dual-image
position encoding technique that simultaneously encodes the position
information of both the search region and template images. The HiT model
achieves promising speed with competitive performance. For instance, it runs at
61 frames per second (fps) on the Nvidia Jetson AGX edge device. Furthermore,
HiT attains 64.6% AUC on the LaSOT benchmark, surpassing all previous efficient
trackers.",None,-1
88a90f43-2c14-4bc7-a5fa-87f483837d8c,Adaptive Gating in Mixture-of-Experts based Language Models,0.00963634,1,"Large language models, such as OpenAI's ChatGPT, have demonstrated
exceptional language understanding capabilities in various NLP tasks. Sparsely
activated mixture-of-experts (MoE) has emerged as a promising solution for
scaling models while maintaining a constant number of computational operations.
Existing MoE model adopts a fixed gating network where each token is computed
by the same number of experts. However, this approach contradicts our intuition
that the tokens in each sequence vary in terms of their linguistic complexity
and, consequently, require different computational costs. Little is discussed
in prior research on the trade-off between computation per token and model
performance. This paper introduces adaptive gating in MoE, a flexible training
strategy that allows tokens to be processed by a variable number of experts
based on expert probability distribution. The proposed framework preserves
sparsity while improving training efficiency. Additionally, curriculum learning
is leveraged to further reduce training time. Extensive experiments on diverse
NLP tasks show that adaptive gating reduces at most 22.5% training time while
maintaining inference quality. Moreover, we conduct a comprehensive analysis of
the routing decisions and present our insights when adaptive gating is used.",None,-1
35211d4b-aca0-4cbb-a3fc-470db90ab96a,A Novel Self-training Approach for Low-resource Speech Recognition,0.88048,4,"In this paper, we propose a self-training approach for automatic speech
recognition (ASR) for low-resource settings. While self-training approaches
have been extensively developed and evaluated for high-resource languages such
as English, their applications to low-resource languages like Punjabi have been
limited, despite the language being spoken by millions globally. The scarcity
of annotated data has hindered the development of accurate ASR systems,
especially for low-resource languages (e.g., Punjabi and M\=aori languages). To
address this issue, we propose an effective self-training approach that
generates highly accurate pseudo-labels for unlabeled low-resource speech. Our
experimental analysis demonstrates that our approach significantly improves
word error rate, achieving a relative improvement of 14.94% compared to a
baseline model across four real speech datasets. Further, our proposed approach
reports the best results on the Common Voice Punjabi dataset.",None,-1
0225f91b-2cd1-456e-bfd0-7bb9efa5032b,A Multiagent CyberBattleSim for RL Cyber Operation Agents,0.687528,6,"Hardening cyber physical assets is both crucial and labor-intensive.
Recently, Machine Learning (ML) in general and Reinforcement Learning RL) more
specifically has shown great promise to automate tasks that otherwise would
require significant human insight/intelligence. The development of autonomous
RL agents requires a suitable training environment that allows us to quickly
evaluate various alternatives, in particular how to arrange training scenarios
that pit attackers and defenders against each other. CyberBattleSim is a
training environment that supports the training of red agents, i.e., attackers.
We added the capability to train blue agents, i.e., defenders. The paper
describes our changes and reports on the results we obtained when training blue
agents, either in isolation or jointly with red agents. Our results show that
training a blue agent does lead to stronger defenses against attacks. In
particular, training a blue agent jointly with a red agent increases the blue
agent's capability to thwart sophisticated red agents.",None,-1
5546f0ce-541b-4476-8e06-48d600f715c9,SCB-dataset: A Dataset for Detecting Student Classroom Behavior,0.622878,9,"The use of deep learning methods for automatic detection of students'
classroom behavior is a promising approach to analyze their class performance
and enhance teaching effectiveness. However, the lack of publicly available
datasets on student behavior poses a challenge for researchers in this field.
To address this issue, we propose a Student Classroom Behavior dataset
(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248
labels and 4,003 images, with a focus on hand-raising behavior. We evaluated
the dataset using the YOLOv7 algorithm, achieving a mean average precision
(map) of up to 85.3%. We believe that our dataset can serve as a robust
foundation for future research in the field of student behavior detection and
promote further advancements in this area.Our SCB-dataset can be downloaded
from: https://github.com/Whiffe/SCB-dataset",None,-1
4498fc06-f981-4aae-b0cd-bd5d4b4540d2,"Separability, Contextuality, and the Quantum Frame Problem",0.41659,4,"We study the relationship between assumptions of state separability and both
preparation and measurement contextuality, and the relationship of both of
these to the frame problem, the problem of predicting what does not change in
consequence of an action. We state a quantum analog of the latter and prove its
undecidability. We show how contextuality is generically induced in state
preparation and measurement by basis choice, thermodynamic exchange, and the
imposition of a priori causal models, and how fine-tuning assumptions appear
ubiquitously in settings characterized as non-contextual.",None,-1
310f5bd6-2f2a-4eb7-92a8-fde964f73a2a,Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,0.749927,20,"Although large language models (LLMs) are often pre-trained on large-scale
multilingual texts, their reasoning abilities and real-world knowledge are
mainly evaluated based on English datasets. Assessing LLM capabilities beyond
English is increasingly vital but hindered due to the lack of suitable
datasets. In this work, we introduce IndoMMLU, the first multi-task language
understanding benchmark for Indonesian culture and languages, which consists of
questions from primary school to university entrance exams in Indonesia. By
employing professional teachers, we obtain 14,981 questions across 64 tasks and
education levels, with 46% of the questions focusing on assessing proficiency
in the Indonesian language and knowledge of nine local languages and cultures
in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass
the Indonesian primary school level, with limited knowledge of local Indonesian
languages and culture. Other smaller models such as BLOOMZ and Falcon perform
at even lower levels.",None,-1
76111383-bf47-463d-ad54-e044e5bcd776,ALDi: Quantifying the Arabic Level of Dialectness of Text,0.81652,3,"Transcribed speech and user-generated text in Arabic typically contain a
mixture of Modern Standard Arabic (MSA), the standardized language taught in
schools, and Dialectal Arabic (DA), used in daily communications. To handle
this variation, previous work in Arabic NLP has focused on Dialect
Identification (DI) on the sentence or the token level. However, DI treats the
task as binary, whereas we argue that Arabic speakers perceive a spectrum of
dialectness, which we operationalize at the sentence level as the Arabic Level
of Dialectness (ALDi), a continuous linguistic variable. We introduce the
AOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences
(17% from news articles and 83% from user comments on those articles) which are
manually labeled with their level of dialectness. We provide a detailed
analysis of AOC-ALDi and show that a model trained on it can effectively
identify levels of dialectness on a range of other corpora (including dialects
and genres not included in AOC-ALDi), providing a more nuanced picture than
traditional DI systems. Through case studies, we illustrate how ALDi can reveal
Arabic speakers' stylistic choices in different situations, a useful property
for sociolinguistic analyses.",None,-1
260d7446-76d7-40ab-ae04-fa1b0bfc36be,Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation,0.857866,21,"Language models have achieved impressive performances on dialogue generation
tasks. However, when generating responses for a conversation that requires
factual knowledge, they are far from perfect, due to an absence of mechanisms
to retrieve, encode, and reflect the knowledge in the generated responses. Some
knowledge-grounded dialogue generation methods tackle this problem by
leveraging facts from Knowledge Graphs (KGs); however, they do not guarantee
that the model utilizes a relevant piece of knowledge from the KG. To overcome
this limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a
framework for generating context-relevant and knowledge-grounded dialogues with
the KG. Specifically, our SURGE framework first retrieves the relevant subgraph
from the KG, and then enforces consistency across facts by perturbing their
word embeddings conditioned by the retrieved subgraph. Then, we utilize
contrastive learning to ensure that the generated texts have high similarity to
the retrieved subgraphs. We validate our SURGE framework on OpendialKG and
KOMODIS datasets, showing that it generates high-quality dialogues that
faithfully reflect the knowledge from KG.",None,-1
b9e528ff-407e-481d-bd09-784e775ea09d,Contrastive Learning of Sentence Embeddings from Scratch,0.436229,8,"Contrastive learning has been the dominant approach to train state-of-the-art
sentence embeddings. Previous studies have typically learned sentence
embeddings either through the use of human-annotated natural language inference
(NLI) data or via large-scale unlabeled sentences in an unsupervised manner.
However, even in the case of unlabeled data, their acquisition presents
challenges in certain domains due to various reasons. To address these issues,
we present SynCSE, a contrastive learning framework that trains sentence
embeddings with synthesized data. Specifically, we explore utilizing large
language models to synthesize the required data samples for contrastive
learning, including (1) producing positive and negative annotations given
unlabeled sentences (SynCSE-partial), and (2) generating sentences along with
their corresponding annotations from scratch (SynCSE-scratch). Experimental
results on sentence similarity and reranking tasks indicate that both
SynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines,
and SynCSE-partial even achieves comparable performance to the supervised
models in most settings.",None,-1
ee95dab2-35ef-4a36-8b1d-cb3d0b59943d,Learning Multilingual Sentence Representations with Cross-lingual Consistency Regularization,0.44432,2,"Multilingual sentence representations are the foundation for similarity-based
bitext mining, which is crucial for scaling multilingual neural machine
translation (NMT) system to more languages. In this paper, we introduce MuSR: a
one-for-all Multilingual Sentence Representation model that supports more than
220 languages. Leveraging billions of English-centric parallel corpora, we
train a multilingual Transformer encoder, coupled with an auxiliary Transformer
decoder, by adopting a multilingual NMT framework with CrossConST, a
cross-lingual consistency regularization technique proposed in Gao et al.
(2023). Experimental results on multilingual similarity search and bitext
mining tasks show the effectiveness of our approach. Specifically, MuSR
achieves superior performance over LASER3 (Heffernan et al., 2022) which
consists of 148 independent multilingual sentence encoders.",None,-1
5f67229f-4a64-498c-93dd-2ea015895042,BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis,0.99808,107,"We present a method for reconstructing high-quality meshes of large unbounded
real-world scenes suitable for photorealistic novel view synthesis. We first
optimize a hybrid neural volume-surface scene representation designed to have
well-behaved level sets that correspond to surfaces in the scene. We then bake
this representation into a high-quality triangle mesh, which we equip with a
simple and fast view-dependent appearance model based on spherical Gaussians.
Finally, we optimize this baked representation to best reproduce the captured
viewpoints, resulting in a model that can leverage accelerated polygon
rasterization pipelines for real-time view synthesis on commodity hardware. Our
approach outperforms previous scene representations for real-time rendering in
terms of accuracy, speed, and power consumption, and produces high quality
meshes that enable applications such as appearance editing and physical
simulation.",None,-1
bc8f1e38-8bd1-4530-94f3-913e16059596,Hierarchically Gated Recurrent Neural Network for Sequence Modeling,0.821246,36,"Transformers have surpassed RNNs in popularity due to their superior
abilities in parallel training and long-term dependency modeling. Recently,
there has been a renewed interest in using linear RNNs for efficient sequence
modeling. These linear RNNs often employ gating mechanisms in the output of the
linear recurrence layer while ignoring the significance of using forget gates
within the recurrence. In this paper, we propose a gated linear RNN model
dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes
forget gates that are lower bounded by a learnable value. The lower bound
increases monotonically when moving up layers. This allows the upper layers to
model long-term dependencies and the lower layers to model more local,
short-term dependencies. Experiments on language modeling, image
classification, and long-range arena benchmarks showcase the efficiency and
effectiveness of our proposed model. The source code is available at
https://github.com/OpenNLPLab/HGRN.",None,-1
2a9dc045-0fc4-43d3-9ed7-e5b7beb4a867,Fair Enough: Standardizing Evaluation and Model Selection for Fairness Research in NLP,0.500648,7,"Modern NLP systems exhibit a range of biases, which a growing literature on
model debiasing attempts to correct. However current progress is hampered by a
plurality of definitions of bias, means of quantification, and oftentimes vague
relation between debiasing algorithms and theoretical measures of bias. This
paper seeks to clarify the current situation and plot a course for meaningful
progress in fair learning, with two key contributions: (1) making clear
inter-relations among the current gamut of methods, and their relation to
fairness theory; and (2) addressing the practical problem of model selection,
which involves a trade-off between fairness and accuracy and has led to
systemic issues in fairness research. Putting them together, we make several
recommendations to help shape future work.",None,-1
5edf758b-48aa-422b-afa5-b8926de15324,Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation,0.963985,134,"The ability to collect a large dataset of human preferences from
text-to-image users is usually limited to companies, making such datasets
inaccessible to the public. To address this issue, we create a web app that
enables text-to-image users to generate images and specify their preferences.
Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image
prompts and real users' preferences over generated images. We leverage this
dataset to train a CLIP-based scoring function, PickScore, which exhibits
superhuman performance on the task of predicting human preferences. Then, we
test PickScore's ability to perform model evaluation and observe that it
correlates better with human rankings than other automatic evaluation metrics.
Therefore, we recommend using PickScore for evaluating future text-to-image
generation models, and using Pick-a-Pic prompts as a more relevant dataset than
MS-COCO. Finally, we demonstrate how PickScore can enhance existing
text-to-image models via ranking.",None,-1
45e2cfd9-ea23-47c4-8581-0ba5acba8a7d,Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur,0.404098,10,"Rendering novel view images is highly desirable for many applications.
Despite recent progress, it remains challenging to render high-fidelity and
view-consistent novel views of large-scale scenes from in-the-wild images with
inevitable artifacts (e.g., motion blur). To this end, we develop a hybrid
neural rendering model that makes image-based representation and neural 3D
representation join forces to render high-quality, view-consistent images.
Besides, images captured in the wild inevitably contain artifacts, such as
motion blur, which deteriorates the quality of rendered images. Accordingly, we
propose strategies to simulate blur effects on the rendered images to mitigate
the negative influence of blurriness images and reduce their importance during
training based on precomputed quality-aware weights. Extensive experiments on
real and synthetic data demonstrate our model surpasses state-of-the-art
point-based methods for novel view synthesis. The code is available at
https://daipengwa.github.io/Hybrid-Rendering-ProjectPage.",None,-1
cae1c0ff-92cf-44fa-b2ac-ddbe289c40db,Visually-Aware Context Modeling for News Image Captioning,0.0867035,1,"News Image Captioning aims to create captions from news articles and images,
emphasizing the connection between textual context and visual elements.
Recognizing the significance of human faces in news images and the face-name
co-occurrence pattern in existing datasets, we propose a face-naming module for
learning better name embeddings. Apart from names, which can be directly linked
to an image area (faces), news image captions mostly contain context
information that can only be found in the article. We design a retrieval
strategy using CLIP to retrieve sentences that are semantically close to the
image, mimicking human thought process of linking articles to images.
Furthermore, to tackle the problem of the imbalanced proportion of article
context and image context in captions, we introduce a simple yet effective
method Contrasting with Language Model backbone (CoLaM) to the training
pipeline. We conduct extensive experiments to demonstrate the efficacy of our
framework. We out-perform the previous state-of-the-art (without external data)
by 7.97/5.80 CIDEr scores on GoodNews/NYTimes800k. Our code is available at
https://github.com/tingyu215/VACNIC.",None,-1
fb86d646-00c1-4142-8a5c-a2b0bc89ab4d,SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models,0.83607,6,"Interpreting remote sensing imagery enables numerous downstream applications
ranging from land-use planning to deforestation monitoring. Robustly
classifying this data is challenging due to the Earth's geographic diversity.
While many distinct satellite and aerial image classification datasets exist,
there is yet to be a benchmark curated that suitably covers this diversity. In
this work, we introduce SATellite ImageNet (SATIN), a metadataset curated from
27 existing remotely sensed datasets, and comprehensively evaluate the
zero-shot transfer classification capabilities of a broad range of
vision-language (VL) models on SATIN. We find SATIN to be a challenging
benchmark-the strongest method we evaluate achieves a classification accuracy
of 52.0%. We provide a $\href{https://satinbenchmark.github.io}{\text{public
leaderboard}}$ to guide and track the progress of VL models in this important
domain.",None,-1
5efda5e6-d6b0-4753-96c2-b224f7e19ea7,ChatGPT is on the Horizon: Could a Large Language Model be Suitable for Intelligent Traffic Safety Research and Applications?,0.575939,6,"ChatGPT embarks on a new era of artificial intelligence and will
revolutionize the way we approach intelligent traffic safety systems. This
paper begins with a brief introduction about the development of large language
models (LLMs). Next, we exemplify using ChatGPT to address key traffic safety
issues. Furthermore, we discuss the controversies surrounding LLMs, raise
critical questions for their deployment, and provide our solutions. Moreover,
we propose an idea of multi-modality representation learning for smarter
traffic safety decision-making and open more questions for application
improvement. We believe that LLM will both shape and potentially facilitate
components of traffic safety research.",None,-1
b5b48c61-38a7-4189-8694-94a9df5945f1,Tuning computer vision models with task rewards,0.522967,25,"Misalignment between model predictions and intended usage can be detrimental
for the deployment of computer vision models. The issue is exacerbated when the
task involves complex structured outputs, as it becomes harder to design
procedures which address this misalignment. In natural language processing,
this is often addressed using reinforcement learning techniques that align
models with a task reward. We adopt this approach and show its surprising
effectiveness across multiple computer vision tasks, such as object detection,
panoptic segmentation, colorization and image captioning. We believe this
approach has the potential to be widely useful for better aligning models with
a diverse range of computer vision tasks.",None,-1
55532ddf-a5d7-458d-832c-96ae69d14e1c,Concept Alignment as a Prerequisite for Value Alignment,0.201259,3,"Value alignment is essential for building AI systems that can safely and
reliably interact with people. However, what a person values -- and is even
capable of valuing -- depends on the concepts that they are currently using to
understand and evaluate what happens in the world. The dependence of values on
concepts means that concept alignment is a prerequisite for value alignment --
agents need to align their representation of a situation with that of humans in
order to successfully align their values. Here, we formally analyze the concept
alignment problem in the inverse reinforcement learning setting, show how
neglecting concept alignment can lead to systematic value mis-alignment, and
describe an approach that helps minimize such failure modes by jointly
reasoning about a person's concepts and values. Additionally, we report
experimental results with human participants showing that humans reason about
the concepts used by an agent when acting intentionally, in line with our joint
reasoning model.",None,-1
9b88aab1-9723-422d-905f-2eca466bd8bd,On the Dimensionality of Sentence Embeddings,0.110136,1,"Learning sentence embeddings is a fundamental problem in natural language
processing. While existing research primarily focuses on enhancing the quality
of sentence embeddings, the exploration of sentence embedding dimensions is
limited. Here we present a comprehensive and empirical analysis of the
dimensionality of sentence embeddings. First, we demonstrate that the optimal
dimension of sentence embeddings is usually smaller than the default value.
Subsequently, to compress the dimension of sentence embeddings with minimum
performance degradation, we identify two components contributing to the overall
performance loss: the encoder's performance loss and the pooler's performance
loss. Therefore, we propose a two-step training method for sentence
representation learning models, wherein the encoder and the pooler are
optimized separately to mitigate the overall performance loss in low-dimension
scenarios. Experimental results on seven STS tasks and seven sentence
classification tasks demonstrate that our method significantly improves the
performance of low-dimensional sentence embeddings.",None,-1
c1fa4d11-43af-4efd-a0be-fc6b692ac8e2,F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories,0.987084,76,"This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF)
for novel view synthesis, which enables arbitrary input camera trajectories and
only costs a few minutes for training. Existing fast grid-based NeRF training
frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed
for bounded scenes and rely on space warping to handle unbounded scenes.
Existing two widely-used space-warping methods are only designed for the
forward-facing trajectory or the 360-degree object-centric trajectory but
cannot process arbitrary trajectories. In this paper, we delve deep into the
mechanism of space warping to handle unbounded scenes. Based on our analysis,
we further propose a novel space-warping method called perspective warping,
which allows us to handle arbitrary trajectories in the grid-based NeRF
framework. Extensive experiments demonstrate that F2-NeRF is able to use the
same perspective warping to render high-quality images on two standard datasets
and a new free trajectory dataset collected by us. Project page:
https://totoro97.github.io/projects/f2-nerf.",None,-1
5f7660ba-c5d0-4362-8cdb-5b77e2397e2a,Local Neural Descriptor Fields: Locally Conditioned Object Representations for Manipulation,0.776024,17,"A robot operating in a household environment will see a wide range of unique
and unfamiliar objects. While a system could train on many of these, it is
infeasible to predict all the objects a robot will see. In this paper, we
present a method to generalize object manipulation skills acquired from a
limited number of demonstrations, to novel objects from unseen shape
categories. Our approach, Local Neural Descriptor Fields (L-NDF), utilizes
neural descriptors defined on the local geometry of the object to effectively
transfer manipulation demonstrations to novel objects at test time. In doing
so, we leverage the local geometry shared between objects to produce a more
general manipulation framework. We illustrate the efficacy of our approach in
manipulating novel objects in novel poses -- both in simulation and in the real
world.",None,-1
d6f9a11a-b90b-43a6-86cf-6ebc86f25113,Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning,0.917915,15,"Planning for goal-oriented dialogue often requires simulating future dialogue
interactions and estimating task progress. Many approaches thus consider
training neural networks to perform look-ahead search algorithms such as A*
search and Monte Carlo Tree Search (MCTS). However, this training often
requires abundant annotated data, which creates challenges when faced with
noisy annotations or low-resource settings. We introduce GDP-Zero, an approach
using Open-Loop MCTS to perform goal-oriented dialogue policy planning without
any model training. GDP-Zero prompts a large language model to act as a policy
prior, value function, user simulator, and system model during the tree search.
We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that
its responses are preferred over ChatGPT up to 59.32% of the time, and are
rated more persuasive than ChatGPT during interactive evaluations.",None,-1
479349b3-ce5c-4c2f-a298-c79d6285873d,NeuroX Library for Neuron Analysis of Deep NLP Models,0.741346,6,"Neuron analysis provides insights into how knowledge is structured in
representations and discovers the role of neurons in the network. In addition
to developing an understanding of our models, neuron analysis enables various
applications such as debiasing, domain adaptation and architectural search. We
present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis
of natural language processing models. It implements various interpretation
methods under a unified API, and provides a framework for data processing and
evaluation, thus making it easier for researchers and practitioners to perform
neuron analysis. The Python toolkit is available at
https://www.github.com/fdalvi/NeuroX. Demo Video available at
https://youtu.be/mLhs2YMx4u8.",None,-1
1f3552ae-840b-4102-9459-630a7870f216,Language Conditioned Traffic Generation,0.827904,18,"Simulation forms the backbone of modern self-driving development. Simulators
help develop, test, and improve driving systems without putting humans,
vehicles, or their environment at risk. However, simulators face a major
challenge: They rely on realistic, scalable, yet interesting content. While
recent advances in rendering and scene reconstruction make great strides in
creating static scene assets, modeling their layout, dynamics, and behaviors
remains challenging. In this work, we turn to language as a source of
supervision for dynamic traffic scene generation. Our model, LCTGen, combines a
large language model with a transformer-based decoder architecture that selects
likely map locations from a dataset of maps, and produces an initial traffic
distribution, as well as the dynamics of each vehicle. LCTGen outperforms prior
work in both unconditional and conditional traffic scene generation in terms of
realism and fidelity. Code and video will be available at
https://ariostgx.github.io/lctgen.",None,-1
295b70aa-c4b0-46e8-a9f4-ef1d055ea80d,State of the Art on Diffusion Models for Visual Computing,0.830563,51,"The field of visual computing is rapidly advancing due to the emergence of
generative artificial intelligence (AI), which unlocks unprecedented
capabilities for the generation, editing, and reconstruction of images, videos,
and 3D scenes. In these domains, diffusion models are the generative AI
architecture of choice. Within the last year alone, the literature on
diffusion-based tools and applications has seen exponential growth and relevant
papers are published across the computer graphics, computer vision, and AI
communities with new works appearing daily on arXiv. This rapid growth of the
field makes it difficult to keep up with all recent developments. The goal of
this state-of-the-art report (STAR) is to introduce the basic mathematical
concepts of diffusion models, implementation details and design choices of the
popular Stable Diffusion model, as well as overview important aspects of these
generative AI tools, including personalization, conditioning, inversion, among
others. Moreover, we give a comprehensive overview of the rapidly growing
literature on diffusion-based generation and editing, categorized by the type
of generated medium, including 2D images, videos, 3D objects, locomotion, and
4D scenes. Finally, we discuss available datasets, metrics, open challenges,
and social implications. This STAR provides an intuitive starting point to
explore this exciting topic for researchers, artists, and practitioners alike.",None,-1
311eecd7-654b-4943-8f06-1f5c423c8a9e,Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs,0.593364,16,"The ever-increasing large language models (LLMs), though opening a potential
path for the upcoming artificial general intelligence, sadly drops a daunting
obstacle on the way towards their on-device deployment. As one of the most
well-established pre-LLMs approaches in reducing model complexity, network
pruning appears to lag behind in the era of LLMs, due mostly to its costly
fine-tuning (or re-training) necessity under the massive volumes of model
parameter and training data. To close this industry-academia gap, we introduce
Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that
slightly updates sparse LLMs without the expensive backpropagation and any
weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the
reconstruction error between the dense and sparse LLMs, in the fashion of
performing iterative weight pruning-and-growing on top of sparse LLMs. To
accomplish this purpose, DSnoT particularly takes into account the anticipated
reduction in reconstruction error for pruning and growing, as well as the
variance w.r.t. different input data for growing each weight. This practice can
be executed efficiently in linear time since its obviates the need of
backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2,
Vicuna, and OPT across various benchmarks demonstrate the effectiveness of
DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity
levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by
26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights
into how to fine-tune sparse LLMs in an efficient training-free manner and open
new venues to scale the great potential of sparsity to LLMs. Codes are
available at https://github.com/zyxxmu/DSnoT.",None,-1
88007850-a64a-4afc-9a60-6862efb63b06,Design Booster: A Text-Guided Diffusion Model for Image Translation with Spatial Layout Preservation,0.158745,2,"Diffusion models are able to generate photorealistic images in arbitrary
scenes. However, when applying diffusion models to image translation, there
exists a trade-off between maintaining spatial structure and high-quality
content. Besides, existing methods are mainly based on test-time optimization
or fine-tuning model for each input image, which are extremely time-consuming
for practical applications. To address these issues, we propose a new approach
for flexible image translation by learning a layout-aware image condition
together with a text condition. Specifically, our method co-encodes images and
text into a new domain during the training phase. In the inference stage, we
can choose images/text or both as the conditions for each time step, which
gives users more flexible control over layout and content. Experimental
comparisons of our method with state-of-the-art methods demonstrate our model
performs best in both style image translation and semantic image translation
and took the shortest time.",None,-1
28ee54e5-2c66-4fdc-8a3f-f82fe129a226,Translating SUMO-K to Higher-Order Set Theory,0.289069,1,"We describe a translation from a fragment of SUMO (SUMO-K) into higher-order
set theory. The translation provides a formal semantics for portions of SUMO
which are beyond first-order and which have previously only had an informal
interpretation. It also for the first time embeds a large common-sense ontology
into a very secure interactive theorem proving system. We further extend our
previous work in finding contradictions in SUMO from first order constructs to
include a portion of SUMO's higher order constructs. Finally, using the
translation, we can create problems that can be proven using higher-order
interactive and automated theorem provers. This is tested in several systems
and can be used to form a corpus of higher-order common-sense reasoning
problems.",None,-1
8f625610-1897-46b8-ad3f-c14e3caa6d43,Lightweight Adaptation of Neural Language Models via Subspace Embedding,0.0301973,1,"Traditional neural word embeddings are usually dependent on a richer
diversity of vocabulary. However, the language models recline to cover major
vocabularies via the word embedding parameters, in particular, for multilingual
language models that generally cover a significant part of their overall
learning parameters. In this work, we present a new compact embedding structure
to reduce the memory footprint of the pre-trained language models with a
sacrifice of up to 4% absolute accuracy. The embeddings vectors reconstruction
follows a set of subspace embeddings and an assignment procedure via the
contextual relationship among tokens from pre-trained language models. The
subspace embedding structure calibrates to masked language models, to evaluate
our compact embedding structure on similarity and textual entailment tasks,
sentence and paraphrase tasks. Our experimental evaluation shows that the
subspace embeddings achieve compression rates beyond 99.8% in comparison with
the original embeddings for the language models on XNLI and GLUE benchmark
suites.",None,-1
6d9477d5-a5bc-4f1a-a925-cbcf75803ff9,DBLPLink: An Entity Linker for the DBLP Scholarly Knowledge Graph,0.367551,3,"In this work, we present a web application named DBLPLink, which performs
entity linking over the DBLP scholarly knowledge graph. DBLPLink uses
text-to-text pre-trained language models, such as T5, to produce entity label
spans from an input text question. Entity candidates are fetched from a
database based on the labels, and an entity re-ranker sorts them based on
entity embeddings, such as TransE, DistMult and ComplEx. The results are
displayed so that users may compare and contrast the results between T5-small,
T5-base and the different KG embeddings used. The demo can be accessed at
https://ltdemos.informatik.uni-hamburg.de/dblplink/.",None,-1
c8c33474-d9f6-4487-a8e8-8b0689a7f8a3,Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration,0.757599,11,"Kendall's tau is frequently used to meta-evaluate how well machine
translation (MT) evaluation metrics score individual translations. Its focus on
pairwise score comparisons is intuitive but raises the question of how ties
should be handled, a gray area that has motivated different variants in the
literature. We demonstrate that, in settings like modern MT meta-evaluation,
existing variants have weaknesses arising from their handling of ties, and in
some situations can even be gamed. We propose instead to meta-evaluate metrics
with a version of pairwise accuracy that gives metrics credit for correctly
predicting ties, in combination with a tie calibration procedure that
automatically introduces ties into metric scores, enabling fair comparison
between metrics that do and do not predict ties. We argue and provide
experimental evidence that these modifications lead to fairer ranking-based
assessments of metric performance.",None,-1
18a6502d-1ed7-49f9-b278-a1d49a28c08d,Updated Corpora and Benchmarks for Long-Form Speech Recognition,0.466743,1,"The vast majority of ASR research uses corpora in which both the training and
test data have been pre-segmented into utterances. In most real-word ASR
use-cases, however, test audio is not segmented, leading to a mismatch between
inference-time conditions and models trained on segmented utterances. In this
paper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, and
VoxPopuli-en - with updated transcription and alignments to enable their use
for long-form ASR research. We use these reconstituted corpora to study the
train-test mismatch problem for transducers and attention-based
encoder-decoders (AEDs), confirming that AEDs are more susceptible to this
issue. Finally, we benchmark a simple long-form training for these models,
showing its efficacy for model robustness under this domain shift.",None,-1
0b9dc865-c7ac-41a0-9e7e-3a6062620ece,Neural Constraint Satisfaction: Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement,0.152534,8,"Object rearrangement is a challenge for embodied agents because solving these
tasks requires generalizing across a combinatorially large set of
configurations of entities and their locations. Worse, the representations of
these entities are unknown and must be inferred from sensory percepts. We
present a hierarchical abstraction approach to uncover these underlying
entities and achieve combinatorial generalization from unstructured visual
inputs. By constructing a factorized transition graph over clusters of entity
representations inferred from pixels, we show how to learn a correspondence
between intervening on states of entities in the agent's model and acting on
objects in the environment. We use this correspondence to develop a method for
control that generalizes to different numbers and configurations of objects,
which outperforms current offline deep RL methods when evaluated on simulated
rearrangement tasks.",None,-1
007f0700-e1e2-4bd2-bc61-e2ef183379e4,Russia-Ukraine war: Modeling and Clustering the Sentiments Trends of Various Countries,0.383007,6,"With Twitter's growth and popularity, a huge number of views are shared by
users on various topics, making this platform a valuable information source on
various political, social, and economic issues. This paper investigates English
tweets on the Russia-Ukraine war to analyze trends reflecting users' opinions
and sentiments regarding the conflict. The tweets' positive and negative
sentiments are analyzed using a BERT-based model, and the time series
associated with the frequency of positive and negative tweets for various
countries is calculated. Then, we propose a method based on the neighborhood
average for modeling and clustering the time series of countries. The
clustering results provide valuable insight into public opinion regarding this
conflict. Among other things, we can mention the similar thoughts of users from
the United States, Canada, the United Kingdom, and most Western European
countries versus the shared views of Eastern European, Scandinavian, Asian, and
South American nations toward the conflict.",None,-1
78fbb919-dfcc-4a64-8a50-6f65a7bfa9ad,Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems,0.946008,15,"This report describes a test of the large language model GPT-4 with the
Wolfram Alpha and the Code Interpreter plug-ins on 105 original problems in
science and math, at the high school and college levels, carried out in
June-August 2023. Our tests suggest that the plug-ins significantly enhance
GPT's ability to solve these problems. Having said that, there are still often
""interface"" failures; that is, GPT often has trouble formulating problems in a
way that elicits useful answers from the plug-ins. Fixing these interface
failures seems like a central challenge in making GPT a reliable tool for
college-level calculation problems.",None,-1
ddadb3c2-b94d-4fd4-b44e-bfc59df958d2,Deep RL with Hierarchical Action Exploration for Dialogue Generation,0.0174033,1,"Traditionally, approximate dynamic programming is employed in dialogue
generation with greedy policy improvement through action sampling, as the
natural language action space is vast. However, this practice is inefficient
for reinforcement learning (RL) due to the sparsity of eligible responses with
high action values, which leads to weak improvement sustained by random
sampling. This paper presents theoretical analysis and experiments that reveal
the performance of the dialogue policy is positively correlated with the
sampling size. To overcome this limitation, we introduce a novel
dual-granularity Q-function that explores the most promising response category
to intervene in the sampling process. Our approach extracts actions based on a
grained hierarchy, thereby achieving the optimum with fewer policy iterations.
Additionally, we use offline RL and learn from multiple reward functions
designed to capture emotional nuances in human interactions. Empirical studies
demonstrate that our algorithm outperforms baselines across automatic metrics
and human evaluations. Further testing reveals that our algorithm exhibits both
explainability and controllability and generates responses with higher expected
rewards.",None,-1
5807a92d-9bdf-4750-8f11-3614b2e087e6,ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation,0.995096,20,"Forecasting future trajectories of agents in complex traffic scenes requires
reliable and efficient predictions for all agents in the scene. However,
existing methods for trajectory prediction are either inefficient or sacrifice
accuracy. To address this challenge, we propose ADAPT, a novel approach for
jointly predicting the trajectories of all agents in the scene with dynamic
weight learning. Our approach outperforms state-of-the-art methods in both
single-agent and multi-agent settings on the Argoverse and Interaction
datasets, with a fraction of their computational overhead. We attribute the
improvement in our performance: first, to the adaptive head augmenting the
model capacity without increasing the model size; second, to our design choices
in the endpoint-conditioned prediction, reinforced by gradient stopping. Our
analyses show that ADAPT can focus on each agent with adaptive prediction,
allowing for accurate predictions efficiently. https://KUIS-AI.github.io/adapt",None,-1
917e4a7e-0ad3-4663-a856-235530071f8f,Turning large language models into cognitive models,0.841132,21,"Large language models are powerful systems that excel at many tasks, ranging
from translation to mathematical reasoning. Yet, at the same time, these models
often show unhuman-like characteristics. In the present paper, we address this
gap and ask whether large language models can be turned into cognitive models.
We find that -- after finetuning them on data from psychological experiments --
these models offer accurate representations of human behavior, even
outperforming traditional cognitive models in two decision-making domains. In
addition, we show that their representations contain the information necessary
to model behavior on the level of individual subjects. Finally, we demonstrate
that finetuning on multiple tasks enables large language models to predict
human behavior in a previously unseen task. Taken together, these results
suggest that large, pre-trained models can be adapted to become generalist
cognitive models, thereby opening up new research directions that could
transform cognitive psychology and the behavioral sciences as a whole.",None,-1
36d01dd3-646e-4abf-8884-c13cdbacf708,HIORE: Leveraging High-order Interactions for Unified Entity Relation Extraction,0.343354,2,"Entity relation extraction consists of two sub-tasks: entity recognition and
relation extraction. Existing methods either tackle these two tasks separately
or unify them with word-by-word interactions. In this paper, we propose HIORE,
a new method for unified entity relation extraction. The key insight is to
leverage the high-order interactions, i.e., the complex association among word
pairs, which contains richer information than the first-order word-by-word
interactions. For this purpose, we first devise a W-shape DNN (WNet) to capture
coarse-level high-order connections. Then, we build a heuristic high-order
graph and further calibrate the representations with a graph neural network
(GNN). Experiments on three benchmarks (ACE04, ACE05, SciERC) show that HIORE
achieves the state-of-the-art performance on relation extraction and an
improvement of 1.1~1.8 F1 points over the prior best unified model.",None,-1
8ea53281-ab67-4938-82f9-960baaa4c415,DarkBERT: A Language Model for the Dark Side of the Internet,0.726194,13,"Recent research has suggested that there are clear differences in the
language used in the Dark Web compared to that of the Surface Web. As studies
on the Dark Web commonly require textual analysis of the domain, language
models specific to the Dark Web may provide valuable insights to researchers.
In this work, we introduce DarkBERT, a language model pretrained on Dark Web
data. We describe the steps taken to filter and compile the text data used to
train DarkBERT to combat the extreme lexical and structural diversity of the
Dark Web that may be detrimental to building a proper representation of the
domain. We evaluate DarkBERT and its vanilla counterpart along with other
widely used language models to validate the benefits that a Dark Web domain
specific model offers in various use cases. Our evaluations show that DarkBERT
outperforms current language models and may serve as a valuable resource for
future research on the Dark Web.",None,-1
38c36a0f-38a1-43c0-91e9-e3be2e082b60,"The HCI Aspects of Public Deployment of Research Chatbots: A User Study, Design Recommendations, and Open Challenges",0.147704,1,"Publicly deploying research chatbots is a nuanced topic involving necessary
risk-benefit analyses. While there have recently been frequent discussions on
whether it is responsible to deploy such models, there has been far less focus
on the interaction paradigms and design approaches that the resulting
interfaces should adopt, in order to achieve their goals more effectively. We
aim to pose, ground, and attempt to answer HCI questions involved in this
scope, by reporting on a mixed-methods user study conducted on a recent
research chatbot. We find that abstract anthropomorphic representation for the
agent has a significant effect on user's perception, that offering AI
explainability may have an impact on feedback rates, and that two (diegetic and
extradiegetic) levels of the chat experience should be intentionally designed.
We offer design recommendations and areas of further focus for the research
community.",None,-1
712c38f5-c3c0-4f3f-8b99-43df134e13dd,Zero-Shot Transfer of Haptics-Based Object Insertion Policies,0.540574,4,"Humans naturally exploit haptic feedback during contact-rich tasks like
loading a dishwasher or stocking a bookshelf. Current robotic systems focus on
avoiding unexpected contact, often relying on strategically placed environment
sensors. Recently, contact-exploiting manipulation policies have been trained
in simulation and deployed on real robots. However, they require some form of
real-world adaptation to bridge the sim-to-real gap, which might not be
feasible in all scenarios. In this paper we train a contact-exploiting
manipulation policy in simulation for the contact-rich household task of
loading plates into a slotted holder, which transfers without any fine-tuning
to the real robot. We investigate various factors necessary for this zero-shot
transfer, like time delay modeling, memory representation, and domain
randomization. Our policy transfers with minimal sim-to-real gap and
significantly outperforms heuristic and learnt baselines. It also generalizes
to plates of different sizes and weights. Demonstration videos and code are
available at https://sites.google.com/view/compliant-object-insertion.",None,-1
27277935-beb2-49fa-99df-2af4819c22c9,TBFormer: Two-Branch Transformer for Image Forgery Localization,0.566443,9,"Image forgery localization aims to identify forged regions by capturing
subtle traces from high-quality discriminative features. In this paper, we
propose a Transformer-style network with two feature extraction branches for
image forgery localization, and it is named as Two-Branch Transformer
(TBFormer). Firstly, two feature extraction branches are elaborately designed,
taking advantage of the discriminative stacked Transformer layers, for both RGB
and noise domain features. Secondly, an Attention-aware Hierarchical-feature
Fusion Module (AHFM) is proposed to effectively fuse hierarchical features from
two different domains. Although the two feature extraction branches have the
same architecture, their features have significant differences since they are
extracted from different domains. We adopt position attention to embed them
into a unified feature domain for hierarchical feature investigation. Finally,
a Transformer decoder is constructed for feature reconstruction to generate the
predicted mask. Extensive experiments on publicly available datasets
demonstrate the effectiveness of the proposed model.",None,-1
aebec104-5966-4cd4-80c0-4d248d60a5d2,ReFit: Recurrent Fitting Network for 3D Human Recovery,0.654738,12,"We present Recurrent Fitting (ReFit), a neural network architecture for
single-image, parametric 3D human reconstruction. ReFit learns a
feedback-update loop that mirrors the strategy of solving an inverse problem
through optimization. At each iterative step, it reprojects keypoints from the
human model to feature maps to query feedback, and uses a recurrent-based
updater to adjust the model to fit the image better. Because ReFit encodes
strong knowledge of the inverse problem, it is faster to train than previous
regression models. At the same time, ReFit improves state-of-the-art
performance on standard benchmarks. Moreover, ReFit applies to other
optimization settings, such as multi-view fitting and single-view shape
fitting. Project website: https://yufu-wang.github.io/refit_humans/",None,-1
e5c4af0a-8ff5-47a0-b4e1-75e1dd81206f,Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models,0.448003,10,"Considerable effort has been dedicated to mitigating toxicity, but existing
methods often require drastic modifications to model parameters or the use of
computationally intensive auxiliary models. Furthermore, previous approaches
have often neglected the crucial factor of language's evolving nature over
time. In this work, we present a comprehensive perspective on toxicity
mitigation that takes into account its changing nature. We introduce
Goodtriever, a flexible methodology that matches the current state-of-the-art
toxicity mitigation while achieving 43% relative latency reduction during
inference and being more computationally efficient. By incorporating a
retrieval-based approach at decoding time, Goodtriever enables
toxicity-controlled text generation. Our research advocates for an increased
focus on adaptable mitigation techniques, which better reflect the data drift
models face when deployed in the wild. Code and data are available at
https://github.com/for-ai/goodtriever.",None,-1
ea5e1474-c64f-4406-97c5-7af9ae12f202,LAMBO: Large Language Model Empowered Edge Intelligence,0.626333,7,"Next-generation edge intelligence is anticipated to bring huge benefits to
various applications, e.g., offloading systems. However, traditional deep
offloading architectures face several issues, including heterogeneous
constraints, partial perception, uncertain generalization, and lack of
tractability. In this context, the integration of offloading with large
language models (LLMs) presents numerous advantages. Therefore, we propose an
LLM-Based Offloading (LAMBO) framework for mobile edge computing (MEC), which
comprises four components: (i) Input embedding (IE), which is used to represent
the information of the offloading system with constraints and prompts through
learnable vectors with high quality; (ii) Asymmetric encoderdecoder (AED)
model, which is a decision-making module with a deep encoder and a shallow
decoder. It can achieve high performance based on multi-head self-attention
schemes; (iii) Actor-critic reinforcement learning (ACRL) module, which is
employed to pre-train the whole AED for different optimization tasks under
corresponding prompts; and (iv) Active learning from expert feedback (ALEF),
which can be used to finetune the decoder part of the AED while adapting to
dynamic environmental changes. Our simulation results corroborate the
advantages of the proposed LAMBO framework.",None,-1
780a5648-0f77-4fcc-8caf-a7920c550d17,Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models,0.936578,85,"Large-scale text-to-image diffusion models achieve unprecedented success in
image generation and editing. However, how to extend such success to video
editing is unclear. Recent initial attempts at video editing require
significant text-to-video data and computation resources for training, which is
often not accessible. In this work, we propose vid2vid-zero, a simple yet
effective method for zero-shot video editing. Our vid2vid-zero leverages
off-the-shelf image diffusion models, and doesn't require training on any
video. At the core of our method is a null-text inversion module for
text-to-video alignment, a cross-frame modeling module for temporal
consistency, and a spatial regularization module for fidelity to the original
video. Without any training, we leverage the dynamic nature of the attention
mechanism to enable bi-directional temporal modeling at test time. Experiments
and analyses show promising results in editing attributes, subjects, places,
etc., in real-world videos. Code is made available at
\url{https://github.com/baaivision/vid2vid-zero}.",None,-1
4591992c-15ae-4ea8-b53a-b2a52180db36,The 2023 Video Similarity Dataset and Challenge,0.230478,3,"This work introduces a dataset, benchmark, and challenge for the problem of
video copy detection and localization. The problem comprises two distinct but
related tasks: determining whether a query video shares content with a
reference video (""detection""), and additionally temporally localizing the
shared content within each video (""localization""). The benchmark is designed to
evaluate methods on these two tasks, and simulates a realistic
needle-in-haystack setting, where the majority of both query and reference
videos are ""distractors"" containing no copied content. We propose a metric that
reflects both detection and localization accuracy. The associated challenge
consists of two corresponding tracks, each with restrictions that reflect
real-world settings. We provide implementation code for evaluation and
baselines. We also analyze the results and methods of the top submissions to
the challenge. The dataset, baseline methods and evaluation code is publicly
available and will be discussed at a dedicated CVPR'23 workshop.",None,-1
2878c4dc-d32a-4639-821d-6de1b54e69a8,IKEA-Manual: Seeing Shape Assembly Step by Step,0.256975,7,"Human-designed visual manuals are crucial components in shape assembly
activities. They provide step-by-step guidance on how we should move and
connect different parts in a convenient and physically-realizable way. While
there has been an ongoing effort in building agents that perform assembly
tasks, the information in human-design manuals has been largely overlooked. We
identify that this is due to 1) a lack of realistic 3D assembly objects that
have paired manuals and 2) the difficulty of extracting structured information
from purely image-based manuals. Motivated by this observation, we present
IKEA-Manual, a dataset consisting of 102 IKEA objects paired with assembly
manuals. We provide fine-grained annotations on the IKEA objects and assembly
manuals, including decomposed assembly parts, assembly plans, manual
segmentation, and 2D-3D correspondence between 3D parts and visual manuals. We
illustrate the broad application of our dataset on four tasks related to shape
assembly: assembly plan generation, part segmentation, pose estimation, and 3D
part assembly.",None,-1
749c02ad-f825-4f9b-abb3-bc3529aa6274,Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements,0.704882,12,"Empathetic dialogue is an indispensable part of building harmonious social
relationships and contributes to the development of a helpful AI. Previous
approaches are mainly based on fine small-scale language models. With the
advent of ChatGPT, the application effect of large language models (LLMs) in
this field has attracted great attention. This work empirically investigates
the performance of LLMs in generating empathetic responses and proposes three
improvement methods of semantically similar in-context learning, two-stage
interactive generation, and combination with the knowledge base. Extensive
experiments show that LLMs can significantly benefit from our proposed methods
and is able to achieve state-of-the-art performance in both automatic and human
evaluations. Additionally, we explore the possibility of GPT-4 simulating human
evaluators.",None,-1
281ba16c-d6b0-4ae3-ab9c-1ecbfa01db99,VARS: Video Assistant Referee System for Automated Soccer Decision Making from Multiple Views,0.99769,12,"The Video Assistant Referee (VAR) has revolutionized association football,
enabling referees to review incidents on the pitch, make informed decisions,
and ensure fairness. However, due to the lack of referees in many countries and
the high cost of the VAR infrastructure, only professional leagues can benefit
from it. In this paper, we propose a Video Assistant Referee System (VARS) that
can automate soccer decision-making. VARS leverages the latest findings in
multi-view video analysis, to provide real-time feedback to the referee, and
help them make informed decisions that can impact the outcome of a game. To
validate VARS, we introduce SoccerNet-MVFoul, a novel video dataset of soccer
fouls from multiple camera views, annotated with extensive foul descriptions by
a professional soccer referee, and we benchmark our VARS to automatically
recognize the characteristics of these fouls. We believe that VARS has the
potential to revolutionize soccer refereeing and take the game to new heights
of fairness and accuracy across all levels of professional and amateur
federations.",None,-1
4837c8df-4042-4733-9cf4-5c1d79ae8bb5,Temporal Collection and Distribution for Referring Video Object Segmentation,0.500809,9,"Referring video object segmentation aims to segment a referent throughout a
video sequence according to a natural language expression. It requires aligning
the natural language expression with the objects' motions and their dynamic
associations at the global video level but segmenting objects at the frame
level. To achieve this goal, we propose to simultaneously maintain a global
referent token and a sequence of object queries, where the former is
responsible for capturing video-level referent according to the language
expression, while the latter serves to better locate and segment objects with
each frame. Furthermore, to explicitly capture object motions and
spatial-temporal cross-modal reasoning over objects, we propose a novel
temporal collection-distribution mechanism for interacting between the global
referent token and object queries. Specifically, the temporal collection
mechanism collects global information for the referent token from object
queries to the temporal motions to the language expression. In turn, the
temporal distribution first distributes the referent token to the referent
sequence across all frames and then performs efficient cross-frame reasoning
between the referent sequence and object queries in every frame. Experimental
results show that our method outperforms state-of-the-art methods on all
benchmarks consistently and significantly.",None,-1
6621e124-bef7-4e1f-a2af-4168e41f888e,Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer,0.168143,2,"Nearest Neighbor Machine Translation ($k$NN-MT) has achieved great success in
domain adaptation tasks by integrating pre-trained Neural Machine Translation
(NMT) models with domain-specific token-level retrieval. However, the reasons
underlying its success have not been thoroughly investigated. In this paper, we
comprehensively analyze $k$NN-MT through theoretical and empirical studies.
Initially, we provide new insights into the working mechanism of $k$NN-MT as an
efficient technique to implicitly execute gradient descent on the output
projection layer of NMT, indicating that it is a specific case of model
fine-tuning. Subsequently, we conduct multi-domain experiments and word-level
analysis to examine the differences in performance between $k$NN-MT and
entire-model fine-tuning. Our findings suggest that: (1) Incorporating $k$NN-MT
with adapters yields comparable translation performance to fine-tuning on
in-domain test sets, while achieving better performance on out-of-domain test
sets; (2) Fine-tuning significantly outperforms $k$NN-MT on the recall of
in-domain low-frequency words, but this gap could be bridged by optimizing the
context representations with additional adapter layers.",None,-1
04e87ca6-1f9e-4c29-b641-8e2ded11f0ab,PolyFormer: Referring Image Segmentation as Sequential Polygon Generation,0.853552,63,"In this work, instead of directly predicting the pixel-level segmentation
masks, the problem of referring image segmentation is formulated as sequential
polygon generation, and the predicted polygons can be later converted into
segmentation masks. This is enabled by a new sequence-to-sequence framework,
Polygon Transformer (PolyFormer), which takes a sequence of image patches and
text query tokens as input, and outputs a sequence of polygon vertices
autoregressively. For more accurate geometric localization, we propose a
regression-based decoder, which predicts the precise floating-point coordinates
directly, without any coordinate quantization error. In the experiments,
PolyFormer outperforms the prior art by a clear margin, e.g., 5.40% and 4.52%
absolute improvements on the challenging RefCOCO+ and RefCOCOg datasets. It
also shows strong generalization ability when evaluated on the referring video
segmentation task without fine-tuning, e.g., achieving competitive 61.5% J&F on
the Ref-DAVIS17 dataset.",None,-1
c5063379-9489-4201-a395-e626940b2d34,Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning,0.341735,2,"Existing Knowledge Base Question Answering (KBQA) architectures are hungry
for annotated data, which make them costly and time-consuming to deploy. We
introduce the problem of few-shot transfer learning for KBQA, where the target
domain offers only a few labeled examples, but a large labeled training dataset
is available in a source domain. We propose a novel KBQA architecture called
FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers,
re-ranks using an LLM and uses this as input for LLM few-shot in-context
learning to generate logical forms. These are further refined using
execution-guided feedback. Experiments over multiple source-target KBQA pairs
of varying complexity show that FuSIC-KBQA significantly outperforms
adaptations of SoTA KBQA models for this setting. Additional experiments show
that FuSIC-KBQA also outperforms SoTA KBQA models in the in-domain setting when
training data is limited.",None,-1
6a15ceb8-0aaf-499f-869e-c9706fcd306e,Learning Disentangled Representation with Mutual Information Maximization for Real-Time UAV Tracking,0.308766,1,"Efficiency has been a critical problem in UAV tracking due to limitations in
computation resources, battery capacity, and unmanned aerial vehicle maximum
load. Although discriminative correlation filters (DCF)-based trackers prevail
in this field for their favorable efficiency, some recently proposed
lightweight deep learning (DL)-based trackers using model compression
demonstrated quite remarkable CPU efficiency as well as precision.
Unfortunately, the model compression methods utilized by these works, though
simple, are still unable to achieve satisfying tracking precision with higher
compression rates. This paper aims to exploit disentangled representation
learning with mutual information maximization (DR-MIM) to further improve
DL-based trackers' precision and efficiency for UAV tracking. The proposed
disentangled representation separates the feature into an identity-related and
an identity-unrelated features. Only the latter is used, which enhances the
effectiveness of the feature representation for subsequent classification and
regression tasks. Extensive experiments on four UAV benchmarks, including
UAV123@10fps, DTB70, UAVDT and VisDrone2018, show that our DR-MIM tracker
significantly outperforms state-of-the-art UAV tracking methods.",None,-1
8a01fabf-d557-4df0-9079-8fdf0c936656,An Experiment in Retrofitting Competency Questions for Existing Ontologies,0.612317,3,"Competency Questions (CQs) are a form of ontology functional requirements
expressed as natural language questions. Inspecting CQs together with the
axioms in an ontology provides critical insights into the intended scope and
applicability of the ontology. CQs also underpin a number of tasks in the
development of ontologies e.g. ontology reuse, ontology testing, requirement
specification, and the definition of patterns that implement such requirements.
Although CQs are integral to the majority of ontology engineering
methodologies, the practice of publishing CQs alongside the ontological
artefacts is not widely observed by the community. In this context, we present
an experiment in retrofitting CQs from existing ontologies. We propose
RETROFIT-CQs, a method to extract candidate CQs directly from ontologies using
Generative AI. In the paper we present the pipeline that facilitates the
extraction of CQs by leveraging Large Language Models (LLMs) and we discuss its
application to a number of existing ontologies.",None,-1
0cf5f7d8-693a-4fbc-9c86-6c4e269b599d,Transferable Decoding with Visual Entities for Zero-Shot Image Captioning,0.166297,5,"Image-to-text generation aims to describe images using natural language.
Recently, zero-shot image captioning based on pre-trained vision-language
models (VLMs) and large language models (LLMs) has made significant progress.
However, we have observed and empirically demonstrated that these methods are
susceptible to modality bias induced by LLMs and tend to generate descriptions
containing objects (entities) that do not actually exist in the image but
frequently appear during training (i.e., object hallucination). In this paper,
we propose ViECap, a transferable decoding model that leverages entity-aware
decoding to generate descriptions in both seen and unseen scenarios. ViECap
incorporates entity-aware hard prompts to guide LLMs' attention toward the
visual entities present in the image, enabling coherent caption generation
across diverse scenes. With entity-aware hard prompts, ViECap is capable of
maintaining performance when transferring from in-domain to out-of-domain
scenarios. Extensive experiments demonstrate that ViECap sets a new
state-of-the-art cross-domain (transferable) captioning and performs
competitively in-domain captioning compared to previous VLMs-based zero-shot
methods. Our code is available at: https://github.com/FeiElysia/ViECap",None,-1
844a9b74-5385-49e2-b81b-917b4cf50048,Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters,0.79919,10,"Identifying logical errors in complex, incomplete or even contradictory and
overall heterogeneous data like students' experimentation protocols is
challenging. Recognizing the limitations of current evaluation methods, we
investigate the potential of Large Language Models (LLMs) for automatically
identifying student errors and streamlining teacher assessments. Our aim is to
provide a foundation for productive, personalized feedback. Using a dataset of
65 student protocols, an Artificial Intelligence (AI) system based on the
GPT-3.5 and GPT-4 series was developed and tested against human raters. Our
results indicate varying levels of accuracy in error detection between the AI
system and human raters. The AI system can accurately identify many fundamental
student errors, for instance, the AI system identifies when a student is
focusing the hypothesis not on the dependent variable but solely on an expected
observation (acc. = 0.90), when a student modifies the trials in an ongoing
investigation (acc. = 1), and whether a student is conducting valid test trials
(acc. = 0.82) reliably. The identification of other, usually more complex
errors, like whether a student conducts a valid control trial (acc. = .60),
poses a greater challenge. This research explores not only the utility of AI in
educational settings, but also contributes to the understanding of the
capabilities of LLMs in error detection in inquiry-based learning like
experimentation.",None,-1
3fb18932-2670-4f41-90f3-8069f6d44557,Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations,0.825474,19,"Large language models (LLMs) can generate fluent natural language texts when
given relevant documents as background context. This ability has attracted
considerable interest in developing industry applications of LLMs. However,
LLMs are prone to generate hallucinations that are not supported by the
provided sources. In this paper, we propose a hierarchical framework to detect
and mitigate such ungrounded hallucination. Our framework uses Chain of Natural
Language Inference (CoNLI) for hallucination detection and hallucination
reduction via post-editing. Our approach achieves state-of-the-art performance
on hallucination detection and enhances text quality through rewrite, using
LLMs without any fine-tuning or domain-specific prompt engineering. We show
that this simple plug-and-play framework can serve as an effective choice for
hallucination detection and reduction, achieving competitive performance across
various contexts.",None,-1
16f059a6-c004-43bc-a923-59b6892a72e7,Evaluating Inter-Bilingual Semantic Parsing for Indian Languages,0.178496,3,"Despite significant progress in Natural Language Generation for Indian
languages (IndicNLP), there is a lack of datasets around complex structured
tasks such as semantic parsing. One reason for this imminent gap is the
complexity of the logical form, which makes English to multilingual translation
difficult. The process involves alignment of logical forms, intents and slots
with translated unstructured utterance. To address this, we propose an
Inter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct
Indian languages. We highlight the proposed task's practicality, and evaluate
existing multilingual seq2seq models across several train-test strategies. Our
experiment reveals a high correlation across performance of original
multilingual semantic parsing datasets (such as mTOP, multilingual TOP and
multiATIS++) and our proposed IE-SEMPARSE suite.",None,-1
f97f9e4a-443c-4452-b848-1da0e7800b0b,CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement,0.731583,3,"Low-light images, characterized by inadequate illumination, pose challenges
of diminished clarity, muted colors, and reduced details. Low-light image
enhancement, an essential task in computer vision, aims to rectify these issues
by improving brightness, contrast, and overall perceptual quality, thereby
facilitating accurate analysis and interpretation. This paper introduces the
Convolutional Dense Attention-guided Network (CDAN), a novel solution for
enhancing low-light images. CDAN integrates an autoencoder-based architecture
with convolutional and dense blocks, complemented by an attention mechanism and
skip connections. This architecture ensures efficient information propagation
and feature learning. Furthermore, a dedicated post-processing phase refines
color balance and contrast. Our approach demonstrates notable progress compared
to state-of-the-art results in low-light image enhancement, showcasing its
robustness across a wide range of challenging scenarios. Our model performs
remarkably on benchmark datasets, effectively mitigating under-exposure and
proficiently restoring textures and colors in diverse low-light scenarios. This
achievement underscores CDAN's potential for diverse computer vision tasks,
notably enabling robust object detection and recognition in challenging
low-light conditions.",None,-1
ba4e31c0-66c0-4787-ac1c-ef1fa2391ab3,Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking,0.47278,6,"Discovering entity mentions that are out of a Knowledge Base (KB) from texts
plays a critical role in KB maintenance, but has not yet been fully explored.
The current methods are mostly limited to the simple threshold-based approach
and feature-based classification, and the datasets for evaluation are
relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL)
method which can identify mentions that do not have corresponding KB entities
by matching them to a special NIL entity. To better utilize BERT, we propose
new techniques including NIL entity representation and classification, with
synonym enhancement. We also apply KB Pruning and Versioning strategies to
automatically construct out-of-KB datasets from common in-KB EL datasets.
Results on five datasets of clinical notes, biomedical publications, and
Wikipedia articles in various domains show the advantages of BLINKout over
existing methods to identify out-of-KB mentions for the medical ontologies,
UMLS, SNOMED CT, and the general KB, WikiData.",None,-1
397df104-af19-4c4e-b27f-60d6d316a37a,Meaningful human command: Advance control directives as a method to enable moral and legal responsibility for autonomous weapons systems,0.174369,1,"21st Century war is increasing in speed, with conventional forces combined
with massed use of autonomous systems and human-machine integration. However, a
significant challenge is how humans can ensure moral and legal responsibility
for systems operating outside of normal temporal parameters. This chapter
considers whether humans can stand outside of real time and authorise actions
for autonomous systems by the prior establishment of a contract, for actions to
occur in a future context particularly in faster than real time or in very slow
operations where human consciousness and concentration could not remain well
informed. The medical legal precdent found in 'advance care directives'
suggests how the time-consuming, deliberative process required for
accountability and responsibility of weapons systems may be achievable outside
real time captured in an 'advance control driective' (ACD). The chapter
proposes 'autonomy command' scaffolded and legitimised through the construction
of ACD ahead of the deployment of autonomous systems.",None,-1
87b3a5fe-5cd6-43ef-8f2e-9b91ca0f3c7a,Matching-based Term Semantics Pre-training for Spoken Patient Query Understanding,0.148393,2,"Medical Slot Filling (MSF) task aims to convert medical queries into
structured information, playing an essential role in diagnosis dialogue
systems. However, the lack of sufficient term semantics learning makes existing
approaches hard to capture semantically identical but colloquial expressions of
terms in medical conversations. In this work, we formalize MSF into a matching
problem and propose a Term Semantics Pre-trained Matching Network (TSPMN) that
takes both terms and queries as input to model their semantic interaction. To
learn term semantics better, we further design two self-supervised objectives,
including Contrastive Term Discrimination (CTD) and Matching-based Mask Term
Modeling (MMTM). CTD determines whether it is the masked term in the dialogue
for each given term, while MMTM directly predicts the masked ones. Experimental
results on two Chinese benchmarks show that TSPMN outperforms strong baselines,
especially in few-shot settings.",None,-1
3b8db9c5-9527-42f6-b73e-339b68f31a50,LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection,0.684859,3,"As malicious actors employ increasingly advanced and widespread bots to
disseminate misinformation and manipulate public opinion, the detection of
Twitter bots has become a crucial task. Though graph-based Twitter bot
detection methods achieve state-of-the-art performance, we find that their
inference depends on the neighbor users multi-hop away from the targets, and
fetching neighbors is time-consuming and may introduce bias. At the same time,
we find that after finetuning on Twitter bot detection, pretrained language
models achieve competitive performance and do not require a graph structure
during deployment. Inspired by this finding, we propose a novel bot detection
framework LMBot that distills the knowledge of graph neural networks (GNNs)
into language models (LMs) for graph-less deployment in Twitter bot detection
to combat the challenge of data dependency. Moreover, LMBot is compatible with
graph-based and graph-less datasets. Specifically, we first represent each user
as a textual sequence and feed them into the LM for domain adaptation. For
graph-based datasets, the output of LMs provides input features for the GNN,
enabling it to optimize for bot detection and distill knowledge back to the LM
in an iterative, mutually enhancing process. Armed with the LM, we can perform
graph-less inference, which resolves the graph data dependency and sampling
bias issues. For datasets without graph structure, we simply replace the GNN
with an MLP, which has also shown strong performance. Our experiments
demonstrate that LMBot achieves state-of-the-art performance on four Twitter
bot detection benchmarks. Extensive studies also show that LMBot is more
robust, versatile, and efficient compared to graph-based Twitter bot detection
methods.",None,-1
f9306627-fd76-4f00-9b41-882f621955d2,Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models,0.38972,51,"While language models (LMs) have shown potential across a range of
decision-making tasks, their reliance on simple acting processes limits their
broad deployment as autonomous agents. In this paper, we introduce Language
Agent Tree Search (LATS) -- the first general framework that synergizes the
capabilities of LMs in reasoning, acting, and planning. By leveraging the
in-context learning ability of LMs, we integrate Monte Carlo Tree Search into
LATS to enable LMs as agents, along with LM-powered value functions and
self-reflections for proficient exploration and enhanced decision-making. A key
feature of our approach is the incorporation of an environment for external
feedback, which offers a more deliberate and adaptive problem-solving mechanism
that surpasses the constraints of existing techniques. Our experimental
evaluation across diverse domains, including programming, interactive
question-answering (QA), web navigation, and math, validates the effectiveness
and generality of LATS in decision-making while maintaining competitive or
improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1
accuracy (92.7%) for programming on HumanEval with GPT-4 and demonstrates
gradient-free performance (average score of 75.9) comparable to gradient-based
fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at
https://github.com/lapisrocks/LanguageAgentTreeSearch",None,-1
94c07cea-0d01-4b35-86c0-e1fb09666dcd,Mistral 7B,0.999375,678,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered
for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B
across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and
code generation. Our model leverages grouped-query attention (GQA) for faster
inference, coupled with sliding window attention (SWA) to effectively handle
sequences of arbitrary length with a reduced inference cost. We also provide a
model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses
the Llama 2 13B -- Chat model both on human and automated benchmarks. Our
models are released under the Apache 2.0 license.",None,-1
07b981ea-63f2-46c1-ac57-fe4fec26bb36,Evaluation of Human-Understandability of Global Model Explanations using Decision Tree,0.139015,1,"In explainable artificial intelligence (XAI) research, the predominant focus
has been on interpreting models for experts and practitioners. Model agnostic
and local explanation approaches are deemed interpretable and sufficient in
many applications. However, in domains like healthcare, where end users are
patients without AI or domain expertise, there is an urgent need for model
explanations that are more comprehensible and instil trust in the model's
operations. We hypothesise that generating model explanations that are
narrative, patient-specific and global(holistic of the model) would enable
better understandability and enable decision-making. We test this using a
decision tree model to generate both local and global explanations for patients
identified as having a high risk of coronary heart disease. These explanations
are presented to non-expert users. We find a strong individual preference for a
specific type of explanation. The majority of participants prefer global
explanations, while a smaller group prefers local explanations. A task based
evaluation of mental models of these participants provide valuable feedback to
enhance narrative global explanations. This, in turn, guides the design of
health informatics systems that are both trustworthy and actionable.",None,-1
7c9af2f4-6a91-4656-aedd-605e59139ae1,Using simulation to quantify the performance of automotive perception systems,0.127789,1,"The design and evaluation of complex systems can benefit from a software
simulation - sometimes called a digital twin. The simulation can be used to
characterize system performance or to test its performance under conditions
that are difficult to measure (e.g., nighttime for automotive perception
systems). We describe the image system simulation software tools that we use to
evaluate the performance of image systems for object (automobile) detection. We
describe experiments with 13 different cameras with a variety of optics and
pixel sizes. To measure the impact of camera spatial resolution, we designed a
collection of driving scenes that had cars at many different distances. We
quantified system performance by measuring average precision and we report a
trend relating system resolution and object detection performance. We also
quantified the large performance degradation under nighttime conditions,
compared to daytime, for all cameras and a COCO pre-trained network.",None,-1
4b1df5d7-b576-4480-b904-3e1e6cc3ce20,DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations,0.383559,5,"In open-domain dialogue generation tasks, contexts and responses in most
datasets are one-to-one mapped, violating an important many-to-many
characteristic: a context leads to various responses, and a response answers
multiple contexts. Without such patterns, models poorly generalize and prefer
responding safely. Many attempts have been made in either multi-turn settings
from a one-to-many perspective or in a many-to-many perspective but limited to
single-turn settings. The major challenge to many-to-many augment multi-turn
dialogues is that discretely replacing each turn with semantic similarity
breaks fragile context coherence. In this paper, we propose DialoGue Path
Sampling (DialoGPS) method in continuous semantic space, the first many-to-many
augmentation method for multi-turn dialogues. Specifically, we map a dialogue
to our extended Brownian Bridge, a special Gaussian process. We sample latent
variables to form coherent dialogue paths in the continuous space. A dialogue
path corresponds to a new multi-turn dialogue and is used as augmented training
data. We show the effect of DialoGPS with both automatic and human evaluation.",None,-1
631bd5b5-a3f1-4369-92b0-ad0211455581,HugNLP: A Unified and Comprehensive Library for Natural Language Processing,0.0237852,1,"In this paper, we introduce HugNLP, a unified and comprehensive library for
natural language processing (NLP) with the prevalent backend of HuggingFace
Transformers, which is designed for NLP researchers to easily utilize
off-the-shelf algorithms and develop novel methods with user-defined models and
tasks in real-world scenarios. HugNLP consists of a hierarchical structure
including models, processors and applications that unifies the learning process
of pre-trained language models (PLMs) on different NLP tasks. Additionally, we
present some featured NLP applications to show the effectiveness of HugNLP,
such as knowledge-enhanced PLMs, universal information extraction, low-resource
mining, and code understanding and generation, etc. The source code will be
released on GitHub (https://github.com/wjn1996/HugNLP).",None,-1
ba13d959-4078-4031-9275-bdd435c449df,SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation,0.941997,14,"We present a novel Speech Augmented Language Model (SALM) with {\em
multitask} and {\em in-context} learning capabilities. SALM comprises a frozen
text LLM, a audio encoder, a modality adapter module, and LoRA layers to
accommodate speech input and associated task instructions. The unified SALM not
only achieves performance on par with task-specific Conformer baselines for
Automatic Speech Recognition (ASR) and Speech Translation (AST), but also
exhibits zero-shot in-context learning capabilities, demonstrated through
keyword-boosting task for ASR and AST. Moreover, {\em speech supervised
in-context training} is proposed to bridge the gap between LLM training and
downstream speech tasks, which further boosts the in-context learning ability
of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.",None,-1
62b8b216-a4f8-45e6-9e85-9e7fcc9d185b,Three-way Decisions with Evaluative Linguistic Expressions,0.0767304,1,"We propose a linguistic interpretation of three-way decisions, where the
regions of acceptance, rejection, and non-commitment are constructed by using
the so-called evaluative linguistic expressions, which are expressions of
natural language such as small, medium, very short, quite roughly strong,
extremely good, etc. Our results highlight new connections between two
different research areas: three-way decisions and the theory of evaluative
linguistic expressions.",None,-1
fa0f5803-82b3-471a-b078-882f4f5d91dc,Translate the Beauty in Songs: Jointly Learning to Align Melody and Translate Lyrics,0.141336,3,"Song translation requires both translation of lyrics and alignment of music
notes so that the resulting verse can be sung to the accompanying melody, which
is a challenging problem that has attracted some interests in different aspects
of the translation process. In this paper, we propose Lyrics-Melody Translation
with Adaptive Grouping (LTAG), a holistic solution to automatic song
translation by jointly modeling lyrics translation and lyrics-melody alignment.
It is a novel encoder-decoder framework that can simultaneously translate the
source lyrics and determine the number of aligned notes at each decoding step
through an adaptive note grouping module. To address data scarcity, we
commissioned a small amount of training data annotated specifically for this
task and used large amounts of augmented data through back-translation.
Experiments conducted on an English-Chinese song translation data set show the
effectiveness of our model in both automatic and human evaluation.",None,-1
7ded54c5-7c86-41ff-91ff-ece85b186ce1,Rethinking Voice-Face Correlation: A Geometry View,0.835534,3,"Previous works on voice-face matching and voice-guided face synthesis
demonstrate strong correlations between voice and face, but mainly rely on
coarse semantic cues such as gender, age, and emotion. In this paper, we aim to
investigate the capability of reconstructing the 3D facial shape from voice
from a geometry perspective without any semantic information. We propose a
voice-anthropometric measurement (AM)-face paradigm, which identifies
predictable facial AMs from the voice and uses them to guide 3D face
reconstruction. By leveraging AMs as a proxy to link the voice and face
geometry, we can eliminate the influence of unpredictable AMs and make the face
geometry tractable. Our approach is evaluated on our proposed dataset with
ground-truth 3D face scans and corresponding voice recordings, and we find
significant correlations between voice and specific parts of the face geometry,
such as the nasal cavity and cranium. Our work offers a new perspective on
voice-face correlation and can serve as a good empirical study for
anthropometry science.",None,-1
0e6d3c86-561c-4bb2-b591-f827b3e31b60,A Transformer-based Approach for Arabic Offline Handwritten Text Recognition,0.852553,4,"Handwriting recognition is a challenging and critical problem in the fields
of pattern recognition and machine learning, with applications spanning a wide
range of domains. In this paper, we focus on the specific issue of recognizing
offline Arabic handwritten text. Existing approaches typically utilize a
combination of convolutional neural networks for image feature extraction and
recurrent neural networks for temporal modeling, with connectionist temporal
classification used for text generation. However, these methods suffer from a
lack of parallelization due to the sequential nature of recurrent neural
networks. Furthermore, these models cannot account for linguistic rules,
necessitating the use of an external language model in the post-processing
stage to boost accuracy. To overcome these issues, we introduce two alternative
architectures, namely the Transformer Transducer and the standard
sequence-to-sequence Transformer, and compare their performance in terms of
accuracy and speed. Our approach can model language dependencies and relies
only on the attention mechanism, thereby making it more parallelizable and less
complex. We employ pre-trained Transformers for both image understanding and
language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that
our proposed method outperforms the current state-of-the-art approaches for
recognizing offline Arabic handwritten text.",None,-1
0695110b-4093-484f-b87f-f6268e78781c,Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation,0.789349,30,"Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a ""Game-of-Thoughts"". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.",None,-1
4350cd85-ed64-48f4-8f5a-1bae761f0547,Long Horizon Temperature Scaling,0.568984,5,"Temperature scaling is a popular technique for tuning the sharpness of a
model distribution. It is used extensively for sampling likely generations and
calibrating model uncertainty, and even features as a controllable parameter to
many large language models in deployment. However, autoregressive models rely
on myopic temperature scaling that greedily optimizes the next token. To
address this, we propose Long Horizon Temperature Scaling (LHTS), a novel
approach for sampling from temperature-scaled joint distributions. LHTS is
compatible with all likelihood-based models, and optimizes for the long horizon
likelihood of samples. We derive a temperature-dependent LHTS objective, and
show that finetuning a model on a range of temperatures produces a single model
capable of generation with a controllable long horizon temperature parameter.
We experiment with LHTS on image diffusion models and character/language
autoregressive models, demonstrating advantages over myopic temperature scaling
in likelihood and sample quality, and showing improvements in accuracy on a
multiple choice analogy task by $10\%$.",None,-1
19f59a61-b328-4be9-b7e8-929b8f005663,Towards General Text Embeddings with Multi-stage Contrastive Learning,0.996546,83,"We present GTE, a general-purpose text embedding model trained with
multi-stage contrastive learning. In line with recent advancements in unifying
various NLP tasks into a single format, we train a unified text embedding model
by employing contrastive learning over a diverse mixture of datasets from
multiple sources. By significantly increasing the number of training data
during both unsupervised pre-training and supervised fine-tuning stages, we
achieve substantial performance gains over existing embedding models. Notably,
even with a relatively modest parameter count of 110M, GTE$_\text{base}$
outperforms the black-box embedding API provided by OpenAI and even surpasses
10x larger text embedding models on the massive text embedding benchmark.
Furthermore, without additional fine-tuning on each programming language
individually, our model outperforms previous best code retrievers of similar
size by treating code as text. In summary, our model achieves impressive
results by effectively harnessing multi-stage contrastive learning, offering a
powerful and efficient text embedding model with broad applicability across
various NLP and code-related tasks.",None,-1
36562f74-8171-4299-b050-80d639e2d9eb,Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence,0.777899,21,"Sentence-level representations are beneficial for various natural language
processing tasks. It is commonly believed that vector representations can
capture rich linguistic properties. Currently, large language models (LMs)
achieve state-of-the-art performance on sentence embedding. However, some
recent works suggest that vector representations from LMs can cause information
leakage. In this work, we further investigate the information leakage issue and
propose a generative embedding inversion attack (GEIA) that aims to reconstruct
input sequences based only on their sentence embeddings. Given the black-box
access to a language model, we treat sentence embeddings as initial tokens'
representations and train or fine-tune a powerful decoder model to decode the
whole sequences directly. We conduct extensive experiments to demonstrate that
our generative inversion attack outperforms previous embedding inversion
attacks in classification metrics and generates coherent and contextually
similar sentences as the original inputs.",None,-1
1a743ecd-f40d-4458-b53e-9551dc1a16f5,Pretraining on the Test Set Is All You Need,0.479073,20,"Inspired by recent work demonstrating the promise of smaller
Transformer-based language models pretrained on carefully curated data, we
supercharge such approaches by investing heavily in curating a novel, high
quality, non-synthetic data mixture based solely on evaluation benchmarks.
Using our novel dataset mixture consisting of less than 100 thousand tokens, we
pretrain a 1 million parameter transformer-based LLM \textbf{phi-CTNL}
(pronounced ``fictional"") that achieves perfect results across diverse academic
benchmarks, strictly outperforming all known foundation models.
\textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen
grokking-like ability to accurately predict downstream evaluation benchmarks'
canaries.",None,-1
b8c2e35f-7ad7-4a17-b03e-ecd46734a5a9,SpinDOE: A ball spin estimation method for table tennis robot,0.701458,3,"Spin plays a considerable role in table tennis, making a shot's trajectory
harder to read and predict. However, the spin is challenging to measure because
of the ball's high velocity and the magnitude of the spin values. Existing
methods either require extremely high framerate cameras or are unreliable
because they use the ball's logo, which may not always be visible. Because of
this, many table tennis-playing robots ignore the spin, which severely limits
their capabilities. This paper proposes an easily implementable and reliable
spin estimation method. We developed a dotted-ball orientation estimation (DOE)
method, that can then be used to estimate the spin. The dots are first
localized on the image using a CNN and then identified using geometric hashing.
The spin is finally regressed from the estimated orientations. Using our
algorithm, the ball's orientation can be estimated with a mean error of
2.4{\deg} and the spin estimation has an relative error lower than 1%. Spins up
to 175 rps are measurable with a camera of 350 fps in real time. Using our
method, we generated a dataset of table tennis ball trajectories with position
and spin, available on our project page.",None,-1
b3b6b6ba-c621-4f69-a672-510c12e916ea,On Surgical Fine-tuning for Language Encoders,0.021735,1,"Fine-tuning all the layers of a pre-trained neural language encoder (either
using all the parameters or using parameter-efficient methods) is often the
de-facto way of adapting it to a new task. We show evidence that for different
downstream language tasks, fine-tuning only a subset of layers is sufficient to
obtain performance that is close to and often better than fine-tuning all the
layers in the language encoder. We propose an efficient metric based on the
diagonal of the Fisher information matrix (FIM score), to select the candidate
layers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE
tasks and across distinct language encoders, that this metric can effectively
select layers leading to a strong downstream performance. Our work highlights
that task-specific information corresponding to a given downstream task is
often localized within a few layers, and tuning only those is sufficient for
strong performance. Additionally, we demonstrate the robustness of the FIM
score to rank layers in a manner that remains constant during the optimization
process.",None,-1
5a003707-1643-4105-ac96-50586b9f282f,ComputeGPT: A computational chat model for numerical problems,0.120248,2,"Language models are not accurate in numerical problems. Their architecture
does not allow for anything less than a probabilistic next word. This paper
introduces ComputeGPT: an approach of creating a chat model able to answer
computational problems through running on-demand code. ComputeGPT converts each
question to relevant code, runs the code, and returns the computed answer as
part of the chat. We combine this approach with a local browser-based Python
interpretation and fine-tuned prompts in order to achieve state-of-the-art
efficiency on numerical problems and provide a suitable front-end and safe
environment for the code to be executed in.",None,-1
a3b23d96-3372-422f-9c0e-c0ce45643a7d,Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features,0.831646,15,"Recent studies have demonstrated the susceptibility of deep neural networks
to backdoor attacks. Given a backdoored model, its prediction of a poisoned
sample with trigger will be dominated by the trigger information, though
trigger information and benign information coexist. Inspired by the mechanism
of the optical polarizer that a polarizer could pass light waves with
particular polarizations while filtering light waves with other polarizations,
we propose a novel backdoor defense method by inserting a learnable neural
polarizer into the backdoored model as an intermediate layer, in order to
purify the poisoned sample via filtering trigger information while maintaining
benign information. The neural polarizer is instantiated as one lightweight
linear transformation layer, which is learned through solving a well designed
bi-level optimization problem, based on a limited clean dataset. Compared to
other fine-tuning-based defense methods which often adjust all parameters of
the backdoored model, the proposed method only needs to learn one additional
layer, such that it is more efficient and requires less clean data. Extensive
experiments demonstrate the effectiveness and efficiency of our method in
removing backdoors across various neural network architectures and datasets,
especially in the case of very limited clean data.",None,-1
0844d75f-1d68-4013-9d2b-f7fd1aa83eba,Certified Robust Neural Networks: Generalization and Corruption Resistance,0.345281,7,"Recent work have demonstrated that robustness (to ""corruption"") can be at
odds with generalization. Adversarial training, for instance, aims to reduce
the problematic susceptibility of modern neural networks to small data
perturbations. Surprisingly, overfitting is a major concern in adversarial
training despite being mostly absent in standard training. We provide here
theoretical evidence for this peculiar ""robust overfitting"" phenomenon.
Subsequently, we advance a novel distributionally robust loss function bridging
robustness and generalization. We demonstrate both theoretically as well as
empirically the loss to enjoy a certified level of robustness against two
common types of corruption--data evasion and poisoning attacks--while ensuring
guaranteed generalization. We show through careful numerical experiments that
our resulting holistic robust (HR) training procedure yields SOTA performance.
Finally, we indicate that HR training can be interpreted as a direct extension
of adversarial training and comes with a negligible additional computational
burden. A ready-to-use python library implementing our algorithm is available
at https://github.com/RyanLucas3/HR_Neural_Networks.",None,-1
7dfb3f57-03f4-4b65-81ad-a24c4b96272e,ReContrast: Domain-Specific Anomaly Detection via Contrastive Reconstruction,0.461554,6,"Most advanced unsupervised anomaly detection (UAD) methods rely on modeling
feature representations of frozen encoder networks pre-trained on large-scale
datasets, e.g. ImageNet. However, the features extracted from the encoders that
are borrowed from natural image domains coincide little with the features
required in the target UAD domain, such as industrial inspection and medical
imaging. In this paper, we propose a novel epistemic UAD method, namely
ReContrast, which optimizes the entire network to reduce biases towards the
pre-trained image domain and orients the network in the target domain. We start
with a feature reconstruction approach that detects anomalies from errors.
Essentially, the elements of contrastive learning are elegantly embedded in
feature reconstruction to prevent the network from training instability,
pattern collapse, and identical shortcut, while simultaneously optimizing both
the encoder and decoder on the target domain. To demonstrate our transfer
ability on various image domains, we conduct extensive experiments across two
popular industrial defect detection benchmarks and three medical image UAD
tasks, which shows our superiority over current state-of-the-art methods.",None,-1
dac56b9e-478f-41b4-b813-76abd581e593,The Hardness of Reasoning about Probabilities and Causality,0.77687,3,"We study formal languages which are capable of fully expressing quantitative
probabilistic reasoning and do-calculus reasoning for causal effects, from a
computational complexity perspective. We focus on satisfiability problems whose
instance formulas allow expressing many tasks in probabilistic and causal
inference. The main contribution of this work is establishing the exact
computational complexity of these satisfiability problems. We introduce a new
natural complexity class, named succ$\exists$R, which can be viewed as a
succinct variant of the well-studied class $\exists$R, and show that the
problems we consider are complete for succ$\exists$R. Our results imply even
stronger algorithmic limitations than were proven by Fagin, Halpern, and
Megiddo (1990) and Moss\'{e}, Ibeling, and Icard (2022) for some variants of
the standard languages used commonly in probabilistic and causal inference.",None,-1
5fc23ef5-4610-4284-bf96-6baa293527ae,Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization,0.473286,17,"Prompt tuning is one of the successful approaches for parameter-efficient
tuning of pre-trained language models. Despite being arguably the most
parameter-efficient (tuned soft prompts constitute <0.1% of total parameters),
it typically performs worse than other efficient tuning methods and is quite
sensitive to hyper-parameters. In this work, we introduce Residual Prompt
Tuning - a simple and efficient method that significantly improves the
performance and stability of prompt tuning. We propose to reparameterize soft
prompt embeddings using a shallow network with a residual connection. Our
experiments show that Residual Prompt Tuning significantly outperforms prompt
tuning on SuperGLUE benchmark. Notably, our method reaches +7 points
improvement over prompt tuning with T5-Base and allows to reduce the prompt
length by 10x without hurting performance. In addition, we show that our
approach is robust to the choice of learning rate and prompt initialization,
and is effective in few-shot settings.",None,-1
e5b1dc67-78c2-4895-905f-0c5734800ca2,Positive Label Is All You Need for Multi-Label Classification,0.416435,3,"Multi-label classification (MLC) faces challenges from label noise in
training data due to annotating diverse semantic labels for each image. Current
methods mainly target identifying and correcting label mistakes using trained
MLC models, but still struggle with persistent noisy labels during training,
resulting in imprecise recognition and reduced performance. Our paper addresses
label noise in MLC by introducing a positive and unlabeled multi-label
classification (PU-MLC) method. To counteract noisy labels, we directly discard
negative labels, focusing on the abundance of negative labels and the origin of
most noisy labels. PU-MLC employs positive-unlabeled learning, training the
model with only positive labels and unlabeled data. The method incorporates
adaptive re-balance factors and temperature coefficients in the loss function
to address label distribution imbalance and prevent over-smoothing of
probabilities during training. Additionally, we introduce a local-global
convolution module to capture both local and global dependencies in the image
without requiring backbone retraining. PU-MLC proves effective on MLC and MLC
with partial labels (MLC-PL) tasks, demonstrating significant improvements on
MS-COCO and PASCAL VOC datasets with fewer annotations. Code is available at:
https://github.com/TAKELAMAG/PU-MLC.",None,-1
f47934a8-4699-4616-86c0-581ebdd5d5f6,Explainable Predictive Maintenance,0.766345,5,"Explainable Artificial Intelligence (XAI) fills the role of a critical
interface fostering interactions between sophisticated intelligent systems and
diverse individuals, including data scientists, domain experts, end-users, and
more. It aids in deciphering the intricate internal mechanisms of ``black box''
Machine Learning (ML), rendering the reasons behind their decisions more
understandable. However, current research in XAI primarily focuses on two
aspects; ways to facilitate user trust, or to debug and refine the ML model.
The majority of it falls short of recognising the diverse types of explanations
needed in broader contexts, as different users and varied application areas
necessitate solutions tailored to their specific needs.
  One such domain is Predictive Maintenance (PdM), an exploding area of
research under the Industry 4.0 \& 5.0 umbrella. This position paper highlights
the gap between existing XAI methodologies and the specific requirements for
explanations within industrial applications, particularly the Predictive
Maintenance field. Despite explainability's crucial role, this subject remains
a relatively under-explored area, making this paper a pioneering attempt to
bring relevant challenges to the research community's attention. We provide an
overview of predictive maintenance tasks and accentuate the need and varying
purposes for corresponding explanations. We then list and describe XAI
techniques commonly employed in the literature, discussing their suitability
for PdM tasks. Finally, to make the ideas and claims more concrete, we
demonstrate XAI applied in four specific industrial use cases: commercial
vehicles, metro trains, steel plants, and wind farms, spotlighting areas
requiring further research.",None,-1
bc1ccef8-f896-410b-b135-6389bb7b7e32,SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality,0.726047,50,"In the last year alone, a surge of new benchmarks to measure compositional
understanding of vision-language models have permeated the machine learning
ecosystem. Given an image, these benchmarks probe a model's ability to identify
its associated caption amongst a set of compositional distractors.
Surprisingly, we find significant biases in all these benchmarks rendering them
hackable. This hackability is so dire that blind models with no access to the
image outperform state-of-the-art vision-language models. To remedy this
rampant vulnerability, we introduce SugarCrepe, a new benchmark for
vision-language compositionality evaluation. We employ large language models,
instead of rule-based templates used in previous benchmarks, to generate fluent
and sensical hard negatives, and utilize an adversarial refinement mechanism to
maximally reduce biases. We re-evaluate state-of-the-art models and recently
proposed compositionality inducing strategies, and find that their improvements
were hugely overestimated, suggesting that more innovation is needed in this
important direction. We release SugarCrepe and the code for evaluation at:
https://github.com/RAIVNLab/sugar-crepe.",None,-1
2150016b-ce0a-43d6-bea7-7b81a4562bcc,Theory of Mind for Multi-Agent Collaboration via Large Language Models,0.855026,16,"While Large Language Models (LLMs) have demonstrated impressive
accomplishments in both reasoning and planning, their abilities in multi-agent
collaborations remains largely unexplored. This study evaluates LLM-based
agents in a multi-agent cooperative text game with Theory of Mind (ToM)
inference tasks, comparing their performance with Multi-Agent Reinforcement
Learning (MARL) and planning-based baselines. We observed evidence of emergent
collaborative behaviors and high-order Theory of Mind capabilities among
LLM-based agents. Our results reveal limitations in LLM-based agents' planning
optimization due to systematic failures in managing long-horizon contexts and
hallucination about the task state. We explore the use of explicit belief state
representations to mitigate these issues, finding that it enhances task
performance and the accuracy of ToM inferences for LLM-based agents.",None,-1
2596b4f9-b18c-4715-94bb-797d37c2d8d4,Formally Explaining Neural Networks within Reactive Systems,0.761083,8,"Deep neural networks (DNNs) are increasingly being used as controllers in
reactive systems. However, DNNs are highly opaque, which renders it difficult
to explain and justify their actions. To mitigate this issue, there has been a
surge of interest in explainable AI (XAI) techniques, capable of pinpointing
the input features that caused the DNN to act as it did. Existing XAI
techniques typically face two limitations: (i) they are heuristic, and do not
provide formal guarantees that the explanations are correct; and (ii) they
often apply to ``one-shot'' systems, where the DNN is invoked independently of
past invocations, as opposed to reactive systems. Here, we begin bridging this
gap, and propose a formal DNN-verification-based XAI technique for reasoning
about multi-step, reactive systems. We suggest methods for efficiently
calculating succinct explanations, by exploiting the system's transition
constraints in order to curtail the search space explored by the underlying
verifier. We evaluate our approach on two popular benchmarks from the domain of
automated navigation; and observe that our methods allow the efficient
computation of minimal and minimum explanations, significantly outperforming
the state of the art. We also demonstrate that our methods produce formal
explanations that are more reliable than competing, non-verification-based XAI
techniques.",None,-1
3dacf429-7269-4fe0-ad8b-f9705d335426,On the Foundations of Cycles in Bayesian Networks,0.207411,2,"Bayesian networks (BNs) are a probabilistic graphical model widely used for
representing expert knowledge and reasoning under uncertainty. Traditionally,
they are based on directed acyclic graphs that capture dependencies between
random variables. However, directed cycles can naturally arise when
cross-dependencies between random variables exist, e.g., for modeling feedback
loops. Existing methods to deal with such cross-dependencies usually rely on
reductions to BNs without cycles. These approaches are fragile to generalize,
since their justifications are intermingled with additional knowledge about the
application context. In this paper, we present a foundational study regarding
semantics for cyclic BNs that are generic and conservatively extend the
cycle-free setting. First, we propose constraint-based semantics that specify
requirements for full joint distributions over a BN to be consistent with the
local conditional probabilities and independencies. Second, two kinds of limit
semantics that formalize infinite unfolding approaches are introduced and shown
to be computable by a Markov chain construction.",None,-1
857cf95a-824b-41b7-90d8-268f674768d5,Controllable Path of Destruction,0.0505502,1,"Path of Destruction (PoD) is a self-supervised method for learning iterative
generators. The core idea is to produce a training set by destroying a set of
artifacts, and for each destructive step create a training instance based on
the corresponding repair action. A generator trained on this dataset can then
generate new artifacts by repairing from arbitrary states. The PoD method is
very data-efficient in terms of original training examples and well-suited to
functional artifacts composed of categorical data, such as game levels and
discrete 3D structures. In this paper, we extend the Path of Destruction method
to allow designer control over aspects of the generated artifacts.
Controllability is introduced by adding conditional inputs to the state-action
pairs that make up the repair trajectories. We test the controllable PoD method
in a 2D dungeon setting, as well as in the domain of small 3D Lego cars.",None,-1
3e72d740-38b4-42f9-aefd-7f7ac3029ba1,WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents,0.764767,5,"In this paper, we introduce WeLayout, a novel system for segmenting the
layout of corporate documents, which stands for WeChat Layout Analysis System.
Our approach utilizes a sophisticated ensemble of DINO and YOLO models,
specifically developed for the ICDAR 2023 Competition on Robust Layout
Segmentation. Our method significantly surpasses the baseline, securing a top
position on the leaderboard with a mAP of 70.0. To achieve this performance, we
concentrated on enhancing various aspects of the task, such as dataset
augmentation, model architecture, bounding box refinement, and model ensemble
techniques. Additionally, we trained the data separately for each document
category to ensure a higher mean submission score. We also developed an
algorithm for cell matching to further improve our performance. To identify the
optimal weights and IoU thresholds for our model ensemble, we employed a
Bayesian optimization algorithm called the Tree-Structured Parzen Estimator.
Our approach effectively demonstrates the benefits of combining query-based and
anchor-free models for achieving robust layout segmentation in corporate
documents.",None,-1
16bdd8c9-52cb-4e6d-94d6-ed21cc7ee306,Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation,0.662958,4,"Most of the speech translation models heavily rely on parallel data, which is
hard to collect especially for low-resource languages. To tackle this issue, we
propose to build a cascaded speech translation system without leveraging any
kind of paired data. We use fully unpaired data to train our unsupervised
systems and evaluate our results on CoVoST 2 and CVSS. The results show that
our work is comparable with some other early supervised methods in some
language pairs. While cascaded systems always suffer from severe error
propagation problems, we proposed denoising back-translation (DBT), a novel
approach to building robust unsupervised neural machine translation (UNMT). DBT
successfully increases the BLEU score by 0.7--0.9 in all three translation
directions. Moreover, we simplified the pipeline of our cascaded system to
reduce inference latency and conducted a comprehensive analysis of every part
of our work. We also demonstrate our unsupervised speech translation results on
the established website.",None,-1
7b449f37-5214-484c-9d6c-02bc8e0827e0,Noisy Parallel Data Alignment,0.0580636,2,"An ongoing challenge in current natural language processing is how its major
advancements tend to disproportionately favor resource-rich languages, leaving
a significant number of under-resourced languages behind. Due to the lack of
resources required to train and evaluate models, most modern language
technologies are either nonexistent or unreliable to process endangered, local,
and non-standardized languages. Optical character recognition (OCR) is often
used to convert endangered language documents into machine-readable data.
However, such OCR output is typically noisy, and most word alignment models are
not built to work under such noisy conditions. In this work, we study the
existing word-level alignment models under noisy settings and aim to make them
more robust to noisy data. Our noise simulation and structural biasing method,
tested on multiple language pairs, manages to reduce the alignment error rate
on a state-of-the-art neural-based alignment model up to 59.6%.",None,-1
90fa03b1-0d2a-4864-b3b6-4d5ccb90ddda,AFPN: Asymptotic Feature Pyramid Network for Object Detection,0.829378,37,"Multi-scale features are of great importance in encoding objects with scale
variance in object detection tasks. A common strategy for multi-scale feature
extraction is adopting the classic top-down and bottom-up feature pyramid
networks. However, these approaches suffer from the loss or degradation of
feature information, impairing the fusion effect of non-adjacent levels. This
paper proposes an asymptotic feature pyramid network (AFPN) to support direct
interaction at non-adjacent levels. AFPN is initiated by fusing two adjacent
low-level features and asymptotically incorporates higher-level features into
the fusion process. In this way, the larger semantic gap between non-adjacent
levels can be avoided. Given the potential for multi-object information
conflicts to arise during feature fusion at each spatial location, adaptive
spatial fusion operation is further utilized to mitigate these inconsistencies.
We incorporate the proposed AFPN into both two-stage and one-stage object
detection frameworks and evaluate with the MS-COCO 2017 validation and test
datasets. Experimental evaluation shows that our method achieves more
competitive results than other state-of-the-art feature pyramid networks. The
code is available at
\href{https://github.com/gyyang23/AFPN}{https://github.com/gyyang23/AFPN}.",None,-1
c0308b62-b62b-4e0f-be37-d929c1eee981,Motion Capture Dataset for Practical Use of AI-based Motion Editing and Stylization,0.57646,4,"In this work, we proposed a new style-diverse dataset for the domain of
motion style transfer. The motion dataset uses an industrial-standard human
bone structure and thus is industry-ready to be plugged into 3D characters for
many projects. We claim the challenges in motion style transfer and encourage
future work in this domain by releasing the proposed motion dataset both to the
public and the market. We conduct a comprehensive study on motion style
transfer in the experiment using the state-of-the-art method, and the results
show the proposed dataset's validity for the motion style transfer task.",None,-1
dfba119c-ec54-43d3-b48a-e135647638dc,PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs,0.800289,18,"Large language models (LLMs) have shown great abilities of solving various
natural language tasks in different domains. Due to the training objective of
LLMs and their pre-training data, LLMs are not very well equipped for tasks
involving structured data generation. We propose a framework, Prompting with
Iterative Verification (PiVe), to improve graph-based generative capability of
LLMs. We show how a small language model could be trained to act as a verifier
module for the output of an LLM~(i.e., ChatGPT, GPT-4), and to iteratively
improve its performance via fine-grained corrective instructions. We also show
how the verifier module could apply iterative corrections offline for a more
cost-effective solution to the text-to-graph generation task. Experiments on
three graph-based datasets show consistent improvement gained via PiVe.
Additionally, we create GenWiki-HIQ and highlight that the verifier module can
be used as a data augmentation tool to help improve the quality of
automatically generated parallel text-graph datasets.",None,-1
48c7f5c4-7db1-46ee-9f9e-bb5c35b2fd8b,From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data,0.703824,23,"Large Language Models (LLMs) exhibit exceptional abilities for causal
analysis between concepts in numerous societally impactful domains, including
medicine, science, and law. Recent research on LLM performance in various
causal discovery and inference tasks has given rise to a new ladder in the
classical three-stage framework of causality. In this paper, we advance the
current research of LLM-driven causal discovery by proposing a novel framework
that combines knowledge-based LLM causal analysis with data-driven causal
structure learning. To make LLM more than a query tool and to leverage its
power in discovering natural and new laws of causality, we integrate the
valuable LLM expertise on existing causal mechanisms into statistical analysis
of objective data to build a novel and practical baseline for causal structure
learning.
  We introduce a universal set of prompts designed to extract causal graphs
from given variables and assess the influence of LLM prior causality on
recovering causal structures from data. We demonstrate the significant
enhancement of LLM expertise on the quality of recovered causal structures from
data, while also identifying critical challenges and issues, along with
potential approaches to address them. As a pioneering study, this paper aims to
emphasize the new frontier that LLMs are opening for classical causal discovery
and inference, and to encourage the widespread adoption of LLM capabilities in
data-driven causal analysis.",None,-1
bb5a47e9-eea8-44bc-a9cf-81c270f8aeb9,Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,0.340599,10,"Large language models (LLMs) have demonstrated impressive performance on
various downstream tasks without requiring fine-tuning, including ChatGPT, a
chat-based model built on top of LLMs such as GPT-3.5 and GPT-4. Despite having
a lower training proportion compared to English, these models also exhibit
remarkable capabilities in other languages. In this study, we assess the
performance of GPT-3.5 and GPT-4 models on seven distinct Arabic NLP tasks:
sentiment analysis, translation, transliteration, paraphrasing, part of speech
tagging, summarization, and diacritization. Our findings reveal that GPT-4
outperforms GPT-3.5 on five out of the seven tasks. Furthermore, we conduct an
extensive analysis of the sentiment analysis task, providing insights into how
LLMs achieve exceptional results on a challenging dialectal dataset.
Additionally, we introduce a new Python interface
https://github.com/ARBML/Taqyim that facilitates the evaluation of these tasks
effortlessly.",None,-1
20867284-79d8-4a82-8a5b-4eb275247433,Coverage-based Example Selection for In-Context Learning,0.446988,17,"In-context learning (ICL), the ability of large language models to perform
novel tasks by conditioning on a prompt with a few task examples, requires
these examples to be informative about the test instance. The standard approach
of independently ranking and selecting the most similar examples selects
redundant examples while omitting important information. In this work, we show
that BERTScore-Recall (BSR) selects better examples that demonstrate more of
the salient aspects, e.g. reasoning patterns, of the test input. We further
extend BSR and many standard metrics to easily optimizable set-level metrics,
giving still better coverage of those salient aspects. On 15 datasets spanning
6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric
for in-context example selection across the board, and (2) for compositional
tasks, set selection using Set-BSR outperforms independent ranking by up to 17
points on average and, despite being training-free, surpasses methods that
leverage task or LLM-specific training.",None,-1
7f44238e-236f-4a4c-9794-cf20a8254435,Multi3DRefer: Grounding Text Description to Multiple 3D Objects,0.728358,20,"We introduce the task of localizing a flexible number of objects in
real-world 3D scenes using natural language descriptions. Existing 3D visual
grounding tasks focus on localizing a unique object given a text description.
However, such a strict setting is unnatural as localizing potentially multiple
objects is a common need in real-world scenarios and robotic tasks (e.g.,
visual navigation and object rearrangement). To address this setting we propose
Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains
61926 descriptions of 11609 objects, where zero, single or multiple target
objects are referenced by each description. We also introduce a new evaluation
metric and benchmark methods from prior work to enable further investigation of
multi-modal 3D scene understanding. Furthermore, we develop a better baseline
leveraging 2D features from CLIP by rendering object proposals online with
contrastive learning, which outperforms the state of the art on the ScanRefer
benchmark.",None,-1
c72d756a-e5b1-4f22-8188-92df2855d989,"Towards Conceptualization of ""Fair Explanation"": Disparate Impacts of anti-Asian Hate Speech Explanations on Content Moderators",0.172302,2,"Recent research at the intersection of AI explainability and fairness has
focused on how explanations can improve human-plus-AI task performance as
assessed by fairness measures. We propose to characterize what constitutes an
explanation that is itself ""fair"" -- an explanation that does not adversely
impact specific populations. We formulate a novel evaluation method of ""fair
explanations"" using not just accuracy and label time, but also psychological
impact of explanations on different user groups across many metrics (mental
discomfort, stereotype activation, and perceived workload). We apply this
method in the context of content moderation of potential hate speech, and its
differential impact on Asian vs. non-Asian proxy moderators, across explanation
approaches (saliency map and counterfactual explanation). We find that saliency
maps generally perform better and show less evidence of disparate impact
(group) and individual unfairness than counterfactual explanations.
  Content warning: This paper contains examples of hate speech and racially
discriminatory language. The authors do not support such content. Please
consider your risk of discomfort carefully before continuing reading!",None,-1
9dde7a0f-43a2-4798-abfe-c7d24dad1f6b,Lifted Inference beyond First-Order Logic,0.109785,1,"Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic
inference in statistical relational learning models. As WFOMC is known to be
intractable in general ($\#$P-complete), logical fragments that admit
polynomial time WFOMC are of significant interest. Such fragments are called
domain liftable. Recent works have shown that the two-variable fragment of
first order logic extended with counting quantifiers ($\mathrm{C^2}$) is
domain-liftable. However, many properties of real-world data, like acyclicity
in citation networks and connectivity in social networks, cannot be modeled in
$\mathrm{C^2}$, or first order logic in general. In this work, we expand the
domain liftability of $\mathrm{C^2}$ with multiple such properties. We show
that any $\mathrm{C^2}$ sentence remains domain liftable when one of its
relations is restricted to represent a directed acyclic graph, a connected
graph, a tree (resp. a directed tree) or a forest (resp. a directed forest).
All our results rely on a novel and general methodology of ""counting by
splitting"". Besides their application to probabilistic inference, our results
provide a general framework for counting combinatorial structures. We expand a
vast array of previous results in discrete mathematics literature on directed
acyclic graphs, phylogenetic networks, etc.",None,-1
33a8a7f5-f820-4dbc-ab99-d6cc4097e000,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-level Representations in Medical Images,0.216436,4,"This paper introduces vox2vec - a contrastive method for self-supervised
learning (SSL) of voxel-level representations. vox2vec representations are
modeled by a Feature Pyramid Network (FPN): a voxel representation is a
concatenation of the corresponding feature vectors from different pyramid
levels. The FPN is pre-trained to produce similar representations for the same
voxel in different augmented contexts and distinctive representations for
different voxels. This results in unified multi-scale representations that
capture both global semantics (e.g., body part) and local semantics (e.g.,
different small organs or healthy versus tumor tissue). We use vox2vec to
pre-train a FPN on more than 6500 publicly available computed tomography
images. We evaluate the pre-trained representations by attaching simple heads
on top of them and training the resulting models for 22 segmentation tasks. We
show that vox2vec outperforms existing medical imaging SSL techniques in three
evaluation setups: linear and non-linear probing and end-to-end fine-tuning.
Moreover, a non-linear head trained on top of the frozen vox2vec
representations achieves competitive performance with the FPN trained from
scratch while having 50 times fewer trainable parameters. The code is available
at https://github.com/mishgon/vox2vec .",None,-1
549b646c-57bb-4550-b628-cfeaf5a94388,Automatically Identifying Relations Between Self-Admitted Technical Debt Across Different Sources,0.345399,1,"Self-Admitted Technical Debt or SATD can be found in various sources, such as
source code comments, commit messages, issue tracking systems, and pull
requests. Previous research has established the existence of relations between
SATD items in different sources; such relations can be useful for investigating
and improving SATD management. However, there is currently a lack of approaches
for automatically detecting these SATD relations. To address this, we proposed
and evaluated approaches for automatically identifying SATD relations across
different sources. Our findings show that our approach outperforms baseline
approaches by a large margin, achieving an average F1-score of 0.829 in
identifying relations between SATD items. Moreover, we explored the
characteristics of SATD relations in 103 open-source projects and describe nine
major cases in which related SATD is documented in a second source, and give a
quantitative overview of 26 kinds of relations.",None,-1
14370877-e00a-49b1-b92c-3b216c792a10,Automated clinical coding using off-the-shelf large language models,0.46545,5,"The task of assigning diagnostic ICD codes to patient hospital admissions is
typically performed by expert human coders. Efforts towards automated ICD
coding are dominated by supervised deep learning models. However, difficulties
in learning to predict the large number of rare codes remain a barrier to
adoption in clinical practice. In this work, we leverage off-the-shelf
pre-trained generative large language models (LLMs) to develop a practical
solution that is suitable for zero-shot and few-shot code assignment, with no
need for further task-specific training. Unsupervised pre-training alone does
not guarantee precise knowledge of the ICD ontology and specialist clinical
coding task, therefore we frame the task as information extraction, providing a
description of each coded concept and asking the model to retrieve related
mentions. For efficiency, rather than iterating over all codes, we leverage the
hierarchical nature of the ICD ontology to sparsely search for relevant codes.",None,-1
f1824767-3386-4cab-ad05-eef84381b8b0,Connections between Operator-splitting Methods and Deep Neural Networks with Applications in Image Segmentation,0.0683112,2,"Deep neural network is a powerful tool for many tasks. Understanding why it
is so successful and providing a mathematical explanation is an important
problem and has been one popular research direction in past years. In the
literature of mathematical analysis of deep neural networks, a lot of works is
dedicated to establishing representation theories. How to make connections
between deep neural networks and mathematical algorithms is still under
development. In this paper, we give an algorithmic explanation for deep neural
networks, especially in their connections with operator splitting. We show that
with certain splitting strategies, operator-splitting methods have the same
structure as networks. Utilizing this connection and the Potts model for image
segmentation, two networks inspired by operator-splitting methods are proposed.
The two networks are essentially two operator-splitting algorithms solving the
Potts model. Numerical experiments are presented to demonstrate the
effectiveness of the proposed networks.",None,-1
83b0db64-4f08-453a-a6b2-fe968a1a57e1,The Provable Benefits of Unsupervised Data Sharing for Offline Reinforcement Learning,0.261017,4,"Self-supervised methods have become crucial for advancing deep learning by
leveraging data itself to reduce the need for expensive annotations. However,
the question of how to conduct self-supervised offline reinforcement learning
(RL) in a principled way remains unclear. In this paper, we address this issue
by investigating the theoretical benefits of utilizing reward-free data in
linear Markov Decision Processes (MDPs) within a semi-supervised setting.
  Further, we propose a novel, Provable Data Sharing algorithm (PDS) to utilize
such reward-free data for offline RL. PDS uses additional penalties on the
reward function learned from labeled data to prevent overestimation, ensuring a
conservative algorithm. Our results on various offline RL tasks demonstrate
that PDS significantly improves the performance of offline RL algorithms with
reward-free data. Overall, our work provides a promising approach to leveraging
the benefits of unlabeled data in offline RL while maintaining theoretical
guarantees. We believe our findings will contribute to developing more robust
self-supervised RL methods.",None,-1
ecf4dc95-16f0-4e57-8286-585d20b52779,Neural Authorship Attribution: Stylometric Analysis on Large Language Models,0.438494,2,"Large language models (LLMs) such as GPT-4, PaLM, and Llama have
significantly propelled the generation of AI-crafted text. With rising concerns
about their potential misuse, there is a pressing need for AI-generated-text
forensics. Neural authorship attribution is a forensic effort, seeking to trace
AI-generated text back to its originating LLM. The LLM landscape can be divided
into two primary categories: proprietary and open-source. In this work, we
delve into these emerging categories of LLMs, focusing on the nuances of neural
authorship attribution. To enrich our understanding, we carry out an empirical
analysis of LLM writing signatures, highlighting the contrasts between
proprietary and open-source models, and scrutinizing variations within each
group. By integrating stylometric features across lexical, syntactic, and
structural aspects of language, we explore their potential to yield
interpretable results and augment pre-trained language model-based classifiers
utilized in neural authorship attribution. Our findings, based on a range of
state-of-the-art LLMs, provide empirical insights into neural authorship
attribution, paving the way for future investigations aimed at mitigating the
threats posed by AI-generated misinformation.",None,-1
1d3c724b-80ae-4e5f-8430-4362df6b0155,Both eyes open: Vigilant Incentives help Regulatory Markets improve AI Safety,0.184282,2,"In the context of rapid discoveries by leaders in AI, governments must
consider how to design regulation that matches the increasing pace of new AI
capabilities. Regulatory Markets for AI is a proposal designed with
adaptability in mind. It involves governments setting outcome-based targets for
AI companies to achieve, which they can show by purchasing services from a
market of private regulators. We use an evolutionary game theory model to
explore the role governments can play in building a Regulatory Market for AI
systems that deters reckless behaviour. We warn that it is alarmingly easy to
stumble on incentives which would prevent Regulatory Markets from achieving
this goal. These 'Bounty Incentives' only reward private regulators for
catching unsafe behaviour. We argue that AI companies will likely learn to
tailor their behaviour to how much effort regulators invest, discouraging
regulators from innovating. Instead, we recommend that governments always
reward regulators, except when they find that those regulators failed to detect
unsafe behaviour that they should have. These 'Vigilant Incentives' could
encourage private regulators to find innovative ways to evaluate cutting-edge
AI systems.",None,-1
40df5d99-4cec-4b93-bea3-38b5a0734578,Generalizable Pose Estimation Using Implicit Scene Representations,0.225643,2,"6-DoF pose estimation is an essential component of robotic manipulation
pipelines. However, it usually suffers from a lack of generalization to new
instances and object types. Most widely used methods learn to infer the object
pose in a discriminative setup where the model filters useful information to
infer the exact pose of the object. While such methods offer accurate poses,
the model does not store enough information to generalize to new objects. In
this work, we address the generalization capability of pose estimation using
models that contain enough information about the object to render it in
different poses. We follow the line of work that inverts neural renderers to
infer the pose. We propose i-$\sigma$SRN to maximize the information flowing
from the input pose to the rendered scene and invert them to infer the pose
given an input image. Specifically, we extend Scene Representation Networks
(SRNs) by incorporating a separate network for density estimation and introduce
a new way of obtaining a weighted scene representation. We investigate several
ways of initial pose estimates and losses for the neural renderer. Our final
evaluation shows a significant improvement in inference performance and speed
compared to existing approaches.",None,-1
48eab504-7d89-4933-9662-e1006a2fb82d,Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality,0.412156,5,"Improving factual consistency of abstractive summarization has been a widely
studied topic. However, most of the prior works on training factuality-aware
models have ignored the negative effect it has on summary quality. We propose
EFACTSUM (i.e., Effective Factual Summarization), a candidate summary
generation and ranking technique to improve summary factuality without
sacrificing summary quality. We show that using a contrastive learning
framework with our refined candidate summaries leads to significant gains on
both factuality and similarity-based metrics. Specifically, we propose a
ranking strategy in which we effectively combine two metrics, thereby
preventing any conflict during training. Models trained using our approach show
up to 6 points of absolute improvement over the base model with respect to
FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either
similarity-based metrics or absractiveness.",None,-1
082e50d6-6508-4332-8dac-c50ebac90a09,Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection,0.991498,23,"Weakly Supervised Video Anomaly Detection (WSVAD) is challenging because the
binary anomaly label is only given on the video level, but the output requires
snippet-level predictions. So, Multiple Instance Learning (MIL) is prevailing
in WSVAD. However, MIL is notoriously known to suffer from many false alarms
because the snippet-level detector is easily biased towards the abnormal
snippets with simple context, confused by the normality with the same bias, and
missing the anomaly with a different pattern. To this end, we propose a new MIL
framework: Unbiased MIL (UMIL), to learn unbiased anomaly features that improve
WSVAD. At each MIL training iteration, we use the current detector to divide
the samples into two groups with different context biases: the most confident
abnormal/normal snippets and the rest ambiguous ones. Then, by seeking the
invariant features across the two sample groups, we can remove the variant
context biases. Extensive experiments on benchmarks UCF-Crime and TAD
demonstrate the effectiveness of our UMIL. Our code is provided at
https://github.com/ktr-hubrt/UMIL.",None,-1
70cea542-4d67-465d-ae4a-aedf9bc08190,Data Games: A Game-Theoretic Approach to Swarm Robotic Data Collection,0.485323,3,"Fleets of networked autonomous vehicles (AVs) collect terabytes of sensory
data, which is often transmitted to central servers (the ''cloud'') for
training machine learning (ML) models. Ideally, these fleets should upload all
their data, especially from rare operating contexts, in order to train robust
ML models. However, this is infeasible due to prohibitive network bandwidth and
data labeling costs. Instead, we propose a cooperative data sampling strategy
where geo-distributed AVs collaborate to collect a diverse ML training dataset
in the cloud. Since the AVs have a shared objective but minimal information
about each other's local data distribution and perception model, we can
naturally cast cooperative data collection as an $N$-player mathematical game.
We show that our cooperative sampling strategy uses minimal information to
converge to a centralized oracle policy with complete information about all
AVs. Moreover, we theoretically characterize the performance benefits of our
game-theoretic strategy compared to greedy sampling. Finally, we experimentally
demonstrate that our method outperforms standard benchmarks by up to $21.9\%$
on 4 perception datasets, including for autonomous driving in adverse weather
conditions. Crucially, our experimental results on real-world datasets closely
align with our theoretical guarantees.",None,-1
3f7cb9a4-a6a1-4095-9f04-6195c64e88f2,Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena,0.999998,31,"Recent advancements in Large Language Models (LLMs) showcase advanced
reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating
this necessitates environments that test strategic reasoning in dynamic,
competitive scenarios requiring long-term planning. We introduce AucArena, a
novel evaluation suite that simulates auctions, a setting chosen for being
highly unpredictable and involving many skills related to resource and risk
management, while also being easy to evaluate. We conduct controlled
experiments using state-of-the-art LLMs to power bidding agents to benchmark
their planning and execution skills. Our research demonstrates that LLMs, such
as GPT-4, possess key skills for auction participation, such as budget
management and goal adherence, which improve with adaptive strategies. This
highlights LLMs' potential in modeling complex social interactions in
competitive contexts. However, variability in LLM performance and occasional
outperformance by simpler methods indicate opportunities for further
advancements in LLM design and the value of our simulation environment for
ongoing testing and refinement.",None,-1
ffbd2b40-740b-4546-bae9-d94ec2327b4f,ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting,0.576065,19,"Recent advances in neural rendering have shown great potential for
reconstructing scenes from multiview images. However, accurately representing
objects with glossy surfaces remains a challenge for existing methods. In this
work, we introduce ENVIDR, a rendering and modeling framework for high-quality
rendering and reconstruction of surfaces with challenging specular reflections.
To achieve this, we first propose a novel neural renderer with decomposed
rendering components to learn the interaction between surface and environment
lighting. This renderer is trained using existing physically based renderers
and is decoupled from actual scene representations. We then propose an
SDF-based neural surface model that leverages this learned neural renderer to
represent general scenes. Our model additionally synthesizes indirect
illuminations caused by inter-reflections from shiny surfaces by marching
surface-reflected rays. We demonstrate that our method outperforms state-of-art
methods on challenging shiny scenes, providing high-quality rendering of
specular reflections while also enabling material editing and scene relighting.",None,-1
c0e33b2d-e42d-4584-9f57-c8a04ac2cf1c,Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception,0.153049,1,"Vision-language models (VLMs) have shown powerful capabilities in visual
question answering and reasoning tasks by combining visual representations with
the abstract skill set large language models (LLMs) learn during pretraining.
Vision, while the most popular modality to augment LLMs with, is only one
representation of a scene. In human-robot interaction scenarios, robot
perception requires accurate scene understanding by the robot. In this paper,
we define and demonstrate a method of aligning the embedding spaces of
different modalities (in this case, inertial measurement unit (IMU) data) to
the vision embedding space through a combination of supervised and contrastive
training, enabling the VLM to understand and reason about these additional
modalities without retraining. We opt to give the model IMU embeddings directly
over using a separate human activity recognition model that feeds directly into
the prompt to allow for any nonlinear interactions between the query, image,
and IMU signal that would be lost by mapping the IMU data to a discrete
activity label. Further, we demonstrate our methodology's efficacy through
experiments involving human activity recognition using IMU data and visual
inputs. Our results show that using multiple modalities as input improves the
VLM's scene understanding and enhances its overall performance in various
tasks, thus paving the way for more versatile and capable language models in
multi-modal contexts.",None,-1
e55dc414-7970-4c30-80cb-f0b87a64ad1d,Knowledge Enhanced Model for Live Video Comment Generation,0.422859,1,"Live video commenting is popular on video media platforms, as it can create a
chatting atmosphere and provide supplementary information for users while
watching videos. Automatically generating live video comments can improve user
experience and enable human-like generation for bot chatting. Existing works
mostly focus on short video datasets while ignoring other important video types
such as long videos like movies. In this work, we collect a new Movie Live
Comments (MovieLC) dataset to support research on live video comment generation
for long videos. We also propose a knowledge enhanced generation model inspired
by the divergent and informative nature of live video comments. Our model
adopts a pre-training encoder-decoder framework and incorporates external
knowledge. Extensive experiments show that both objective metrics and human
evaluation demonstrate the effectiveness of our proposed model. The MovieLC
dataset and our code will be released.",None,-1
89a8ff85-7eab-49cd-a355-a31cca770290,Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration,0.439626,9,"Recent text-to-image generation models have shown promising results in
generating high-fidelity photo-realistic images. In parallel, the problem of
data scarcity has brought a growing interest in employing AIGC technology for
high-quality data expansion. However, this paradigm requires well-designed
prompt engineering that cost-less data expansion and labeling remain
under-explored. Inspired by LLM's powerful capability in task guidance, we
propose a new paradigm of annotated data expansion named as ChatGenImage. The
core idea behind it is to leverage the complementary strengths of diverse
models to establish a highly effective and user-friendly pipeline for
interactive data augmentation. In this work, we extensively study how LLMs
communicate with AIGC model to achieve more controllable image generation and
make the first attempt to collaborate them for automatic data augmentation for
a variety of downstream tasks. Finally, we present fascinating results obtained
from our ChatGenImage framework and demonstrate the powerful potential of our
synthetic data for systematic vision adaptation. Our codes are available at
https://github.com/Yuqifan1117/Labal-Anything-Pipeline.",None,-1
d247c2c5-c135-4623-a0be-acf1ac4ab7c9,Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method,0.541608,1,"Theorem proving is a fundamental task in mathematics. With the advent of
large language models (LLMs) and interactive theorem provers (ITPs) like Lean,
there has been growing interest in integrating LLMs and ITPs to automate
theorem proving. In this approach, the LLM generates proof steps (tactics), and
the ITP checks the applicability of the tactics at the current goal. The two
systems work together to complete the proof. In this paper, we introduce
DS-Prover, a novel dynamic sampling method for theorem proving. This method
dynamically determines the number of tactics to apply to expand the current
goal, taking into account the remaining time compared to the total allocated
time for proving a theorem. This makes the proof search process more efficient
by adjusting the balance between exploration and exploitation as time passes.
We also augment the training dataset by decomposing simplification and rewrite
tactics with multiple premises into tactics with single premises. This gives
the model more examples to learn from and helps it to predict the tactics with
premises more accurately. We perform our experiments using the Mathlib dataset
of the Lean theorem prover and report the performance on two standard datasets,
MiniF2F and ProofNet. Our methods achieve significant performance gains on both
datasets. We achieved a state-of-the-art performance (Pass@1) of 14.2% on the
ProofNet dataset and a performance of 29.8% on MiniF2F, slightly surpassing the
best-reported Pass@1 of 29.6% using Lean.",None,-1
c2570c82-20a1-4fab-95c3-ebda7d3d36a8,Medical Face Masks and Emotion Recognition from the Body: Insights from a Deep Learning Perspective,0.110063,1,"The COVID-19 pandemic has undoubtedly changed the standards and affected all
aspects of our lives, especially social communication. It has forced people to
extensively wear medical face masks, in order to prevent transmission. This
face occlusion can strongly irritate emotional reading from the face and urges
us to incorporate the whole body as an emotional cue. In this paper, we conduct
insightful studies about the effect of face occlusion on emotion recognition
performance, and showcase the superiority of full body input over the plain
masked face. We utilize a deep learning model based on the Temporal Segment
Network framework, and aspire to fully overcome the face mask consequences.
Although facial and bodily features can be learned from a single input, this
may lead to irrelevant information confusion. By processing those features
separately and fusing their prediction scores, we are more effectively taking
advantage of both modalities. This framework also naturally supports temporal
modeling, by mingling information among neighboring frames. In combination,
these techniques form an effective system capable of tackling emotion
recognition difficulties, caused by safety protocols applied in crucial areas.",None,-1
3b5d3ab3-c4a3-4e01-a9c0-5be29d6ffe1e,Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra,0.786505,23,"Neural Radiance Fields (NeRFs) are a very recent and very popular approach
for the problems of novel view synthesis and 3D reconstruction. A popular scene
representation used by NeRFs is to combine a uniform, voxel-based subdivision
of the scene with an MLP. Based on the observation that a (sparse) point cloud
of the scene is often available, this paper proposes to use an adaptive
representation based on tetrahedra obtained by Delaunay triangulation instead
of uniform subdivision or point-based representations. We show that such a
representation enables efficient training and leads to state-of-the-art
results. Our approach elegantly combines concepts from 3D geometry processing,
triangle-based rendering, and modern neural radiance fields. Compared to
voxel-based representations, ours provides more detail around parts of the
scene likely to be close to the surface. Compared to point-based
representations, our approach achieves better performance. The source code is
publicly available at: https://jkulhanek.com/tetra-nerf.",None,-1
a2101c6b-697a-4f4d-9cdc-8516cba101e1,"Large Language Models in Sport Science & Medicine: Opportunities, Risks and Considerations",0.346835,3,"This paper explores the potential opportunities, risks, and challenges
associated with the use of large language models (LLMs) in sports science and
medicine. LLMs are large neural networks with transformer style architectures
trained on vast amounts of textual data, and typically refined with human
feedback. LLMs can perform a large range of natural language processing tasks.
In sports science and medicine, LLMs have the potential to support and augment
the knowledge of sports medicine practitioners, make recommendations for
personalised training programs, and potentially distribute high-quality
information to practitioners in developing countries. However, there are also
potential risks associated with the use and development of LLMs, including
biases in the dataset used to create the model, the risk of exposing
confidential data, the risk of generating harmful output, and the need to align
these models with human preferences through feedback. Further research is
needed to fully understand the potential applications of LLMs in sports science
and medicine and to ensure that their use is ethical and beneficial to
athletes, clients, patients, practitioners, and the general public.",None,-1
e07ff86d-f700-449a-8e71-7a10c36911db,Retrieval-augmented Image Captioning,0.607163,16,"Inspired by retrieval-augmented language generation and pretrained Vision and
Language (V&L) encoders, we present a new approach to image captioning that
generates sentences given the input image and a set of captions retrieved from
a datastore, as opposed to the image alone. The encoder in our model jointly
processes the image and retrieved captions using a pretrained V&L BERT, while
the decoder attends to the multimodal encoder representations, benefiting from
the extra textual evidence from the retrieved captions. Experimental results on
the COCO dataset show that image captioning can be effectively formulated from
this new perspective. Our model, named EXTRA, benefits from using captions
retrieved from the training dataset, and it can also benefit from using an
external dataset without the need for retraining. Ablation studies show that
retrieving a sufficient number of captions (e.g., k=5) can improve captioning
quality. Our work contributes towards using pretrained V&L encoders for
generative tasks, instead of standard classification tasks.",None,-1
b9587e1a-d108-4773-95ba-ac8faa9bf9df,Building Extraction from Remote Sensing Images via an Uncertainty-Aware Network,0.785543,4,"Building extraction aims to segment building pixels from remote sensing
images and plays an essential role in many applications, such as city planning
and urban dynamic monitoring. Over the past few years, deep learning methods
with encoder-decoder architectures have achieved remarkable performance due to
their powerful feature representation capability. Nevertheless, due to the
varying scales and styles of buildings, conventional deep learning models
always suffer from uncertain predictions and cannot accurately distinguish the
complete footprints of the building from the complex distribution of ground
objects, leading to a large degree of omission and commission. In this paper,
we realize the importance of uncertain prediction and propose a novel and
straightforward Uncertainty-Aware Network (UANet) to alleviate this problem. To
verify the performance of our proposed UANet, we conduct extensive experiments
on three public building datasets, including the WHU building dataset, the
Massachusetts building dataset, and the Inria aerial image dataset. Results
demonstrate that the proposed UANet outperforms other state-of-the-art
algorithms by a large margin.",None,-1
9edbe873-5369-4880-8ea7-37febc9d76c1,Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension,0.323702,4,"Machine reading comprehension (MRC) poses new challenges over logical
reasoning, which aims to understand the implicit logical relations entailed in
the given contexts and perform inference over them. Due to the complexity of
logic, logical relations exist at different granularity levels. However, most
existing methods of logical reasoning individually focus on either entity-aware
or discourse-based information but ignore the hierarchical relations that may
even have mutual effects. In this paper, we propose a holistic graph network
(HGN) which deals with context at both discourse level and word level, as the
basis for logical reasoning, to provide a more fine-grained relation
extraction. Specifically, node-level and type-level relations, which can be
interpreted as bridges in the reasoning process, are modeled by a hierarchical
interaction mechanism to improve the interpretation of MRC systems.
Experimental results on logical reasoning QA datasets (ReClor and LogiQA) and
natural language inference datasets (SNLI and ANLI) show the effectiveness and
generalization of our method, and in-depth analysis verifies its capability to
understand complex logical relations.",None,-1
259d6181-e9e1-4178-95bc-2a5479743a8b,Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning,0.436515,6,"How can we perform computations over natural language representations to
solve tasks that require symbolic and numeric reasoning? We propose natural
language embedded programs (NLEP) as a unifying framework for addressing
math/symbolic reasoning, natural language understanding, and instruction
following tasks. Our approach prompts a language model to generate full Python
programs that define functions over data structures which contain natural
language representations of structured knowledge. A Python interpreter then
executes the generated code and prints the output. Despite using a task-general
prompt, we find that this approach can improve upon strong baselines across a
range of different tasks including math and symbolic reasoning, text
classification, question answering, and instruction following. We found that
the generated programs are interpretable since they outline the exact reasoning
process followed by the program interpreter.",None,-1
15811de0-76cf-43ab-aa7c-24a03a325f10,AnyDoor: Zero-shot Object-level Image Customization,0.999994,104,"This work presents AnyDoor, a diffusion-based image generator with the power
to teleport target objects to new scenes at user-specified locations in a
harmonious way. Instead of tuning parameters for each object, our model is
trained only once and effortlessly generalizes to diverse object-scene
combinations at the inference stage. Such a challenging zero-shot setting
requires an adequate characterization of a certain object. To this end, we
complement the commonly used identity feature with detail features, which are
carefully designed to maintain texture details yet allow versatile local
variations (e.g., lighting, orientation, posture, etc.), supporting the object
in favorably blending with different surroundings. We further propose to borrow
knowledge from video datasets, where we can observe various forms (i.e., along
the time axis) of a single object, leading to stronger model generalizability
and robustness. Extensive experiments demonstrate the superiority of our
approach over existing alternatives as well as its great potential in
real-world applications, such as virtual try-on and object moving. Project page
is https://damo-vilab.github.io/AnyDoor-Page/.",None,-1
db803b74-ab17-44a9-a5b2-beb665abd6d3,Towards foundation models and few-shot parameter-efficient fine-tuning for volumetric organ segmentation,0.210558,5,"With the recent raise of foundation models in computer vision and NLP, the
pretrain-and-adapt strategy, where a large-scale model is fine-tuned on
downstream tasks, is gaining popularity. However, traditional fine-tuning
approaches may still require significant resources and yield sub-optimal
results when the labeled data of the target task is scarce. This is especially
the case in clinical settings. To address this challenge, we formalize few-shot
efficient fine-tuning (FSEFT), a novel and realistic setting for medical image
segmentation. Furthermore, we introduce a novel parameter-efficient fine-tuning
strategy tailored to medical image segmentation, with (a) spatial adapter
modules that are more appropriate for dense prediction tasks; and (b) a
constrained transductive inference, which leverages task-specific prior
knowledge. Our comprehensive experiments on a collection of public CT datasets
for organ segmentation reveal the limitations of standard fine-tuning methods
in few-shot scenarios, point to the potential of vision adapters and
transductive inference, and confirm the suitability of foundation models.",None,-1
8144242d-986b-41e8-b2cb-f30515a04c8f,Weakly Supervised Reasoning by Neuro-Symbolic Approaches,0.623636,3,"Deep learning has largely improved the performance of various natural
language processing (NLP) tasks. However, most deep learning models are
black-box machinery, and lack explicit interpretation. In this chapter, we will
introduce our recent progress on neuro-symbolic approaches to NLP, which
combines different schools of AI, namely, symbolism and connectionism.
Generally, we will design a neural system with symbolic latent structures for
an NLP task, and apply reinforcement learning or its relaxation to perform
weakly supervised reasoning in the downstream task. Our framework has been
successfully applied to various tasks, including table query reasoning,
syntactic structure reasoning, information extraction reasoning, and rule
reasoning. For each application, we will introduce the background, our
approach, and experimental results.",None,-1
c6b49f00-8808-4f0f-bfa0-7d811cba7e2c,Conceptual Cognitive Maps Formation with Neural Successor Networks and Word Embeddings,0.772201,3,"The human brain possesses the extraordinary capability to contextualize the
information it receives from our environment. The entorhinal-hippocampal plays
a critical role in this function, as it is deeply engaged in memory processing
and constructing cognitive maps using place and grid cells. Comprehending and
leveraging this ability could significantly augment the field of artificial
intelligence. The multi-scale successor representation serves as a good model
for the functionality of place and grid cells and has already shown promise in
this role. Here, we introduce a model that employs successor representations
and neural networks, along with word embedding vectors, to construct a
cognitive map of three separate concepts. The network adeptly learns two
different scaled maps and situates new information in proximity to related
pre-existing representations. The dispersion of information across the
cognitive map varies according to its scale - either being heavily
concentrated, resulting in the formation of the three concepts, or spread
evenly throughout the map. We suggest that our model could potentially improve
current AI models by providing multi-modal context information to any input,
based on a similarity metric for the input and pre-existing knowledge
representations.",None,-1
2613e5fa-e5d5-43a2-b21f-f2f0f028e773,TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective,0.649053,5,"Vision Transformers (ViTs) have demonstrated powerful representation ability
in various visual tasks thanks to their intrinsic data-hungry nature. However,
we unexpectedly find that ViTs perform vulnerably when applied to face
recognition (FR) scenarios with extremely large datasets. We investigate the
reasons for this phenomenon and discover that the existing data augmentation
approach and hard sample mining strategy are incompatible with ViTs-based FR
backbone due to the lack of tailored consideration on preserving face
structural information and leveraging each local token information. To remedy
these problems, this paper proposes a superior FR model called TransFace, which
employs a patch-level data augmentation strategy named DPAP and a hard sample
mining strategy named EHSM. Specially, DPAP randomly perturbs the amplitude
information of dominant patches to expand sample diversity, which effectively
alleviates the overfitting problem in ViTs. EHSM utilizes the information
entropy in the local tokens to dynamically adjust the importance weight of easy
and hard samples during training, leading to a more stable prediction.
Experiments on several benchmarks demonstrate the superiority of our TransFace.
Code and models are available at https://github.com/DanJun6737/TransFace.",None,-1
4145ff07-ffac-4f39-88ec-0470268844fa,UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering,0.784189,12,"In this technical report, we present our solution, named UniOCC, for the
Vision-Centric 3D occupancy prediction track in the nuScenes Open Dataset
Challenge at CVPR 2023. Existing methods for occupancy prediction primarily
focus on optimizing projected features on 3D volume space using 3D occupancy
labels. However, the generation process of these labels is complex and
expensive (relying on 3D semantic annotations), and limited by voxel
resolution, they cannot provide fine-grained spatial semantics. To address this
limitation, we propose a novel Unifying Occupancy (UniOcc) prediction method,
explicitly imposing spatial geometry constraint and complementing fine-grained
semantic supervision through volume ray rendering. Our method significantly
enhances model performance and demonstrates promising potential in reducing
human annotation costs. Given the laborious nature of annotating 3D occupancy,
we further introduce a Depth-aware Teacher Student (DTS) framework to enhance
prediction accuracy using unlabeled data. Our solution achieves 51.27\% mIoU on
the official leaderboard with single model, placing 3rd in this challenge.",None,-1
c8ffd456-132a-4909-9938-808b198bc6aa,Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection,0.971434,4,"Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning,
yet constructing them through human annotations can be costly. As a result,
various automatic methods have been proposed to construct CSKG with larger
semantic coverage. However, these unsupervised approaches introduce spurious
noise that can lower the quality of the resulting CSKG, which cannot be tackled
easily by existing denoising algorithms due to the unique characteristics of
nodes and structures in CSKGs. To address this issue, we propose Gold (Global
and Local-aware Denoising), a denoising framework for CSKGs that incorporates
entity semantic information, global rules, and local structural information
from the CSKG. Experiment results demonstrate that Gold outperforms all
baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks.
Furthermore, we show that denoising a real-world CSKG is effective and even
benefits the downstream zero-shot commonsense question-answering task.",None,-1
3ea9c28f-c700-489b-b107-fa94635ff44a,Multilingual Content Moderation: A Case Study on Reddit,0.101272,4,"Content moderation is the process of flagging content based on pre-defined
platform rules. There has been a growing need for AI moderators to safeguard
users as well as protect the mental health of human moderators from traumatic
content. While prior works have focused on identifying hateful/offensive
language, they are not adequate for meeting the challenges of content
moderation since 1) moderation decisions are based on violation of rules, which
subsumes detection of offensive speech, and 2) such rules often differ across
communities which entails an adaptive solution. We propose to study the
challenges of content moderation by introducing a multilingual dataset of 1.8
Million Reddit comments spanning 56 subreddits in English, German, Spanish and
French. We perform extensive experimental analysis to highlight the underlying
challenges and suggest related research problems such as cross-lingual
transfer, learning under label noise (human biases), transfer of moderation
models, and predicting the violated rule. Our dataset and analysis can help
better prepare for the challenges and opportunities of auto moderation.",None,-1
8d2012e9-94ef-4dee-95c3-46d6b4f0da6c,ASIC: Aligning Sparse in-the-wild Image Collections,0.367344,9,"We present a method for joint alignment of sparse in-the-wild image
collections of an object category. Most prior works assume either ground-truth
keypoint annotations or a large dataset of images of a single object category.
However, neither of the above assumptions hold true for the long-tail of the
objects present in the world. We present a self-supervised technique that
directly optimizes on a sparse collection of images of a particular
object/object category to obtain consistent dense correspondences across the
collection. We use pairwise nearest neighbors obtained from deep features of a
pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches
and make them dense and accurate matches by optimizing a neural network that
jointly maps the image collection into a learned canonical grid. Experiments on
CUB and SPair-71k benchmarks demonstrate that our method can produce globally
consistent and higher quality correspondences across the image collection when
compared to existing self-supervised methods. Code and other material will be
made available at \url{https://kampta.github.io/asic}.",None,-1
44953069-cc46-4d8e-bc2c-0b9cacc58246,Learning Reward Machines in Cooperative Multi-Agent Tasks,0.13657,1,"This paper presents a novel approach to Multi-Agent Reinforcement Learning
(MARL) that combines cooperative task decomposition with the learning of reward
machines (RMs) encoding the structure of the sub-tasks. The proposed method
helps deal with the non-Markovian nature of the rewards in partially observable
environments and improves the interpretability of the learnt policies required
to complete the cooperative task. The RMs associated with each sub-task are
learnt in a decentralised manner and then used to guide the behaviour of each
agent. By doing so, the complexity of a cooperative multi-agent problem is
reduced, allowing for more effective learning. The results suggest that our
approach is a promising direction for future research in MARL, especially in
complex environments with large state spaces and multiple agents.",None,-1
e0e43930-ade5-4e05-a4e2-d9895b3f762d,SGLang: Efficient Execution of Structured Language Model Programs,0.880764,27,"Large language models (LLMs) are increasingly used for complex tasks that
require multiple generation calls, advanced prompting techniques, control flow,
and structured inputs/outputs. However, efficient systems are lacking for
programming and executing these applications. We introduce SGLang, a system for
efficient execution of complex language model programs. SGLang consists of a
frontend language and a runtime. The frontend simplifies programming with
primitives for generation and parallelism control. The runtime accelerates
execution with novel optimizations like RadixAttention for KV cache reuse and
compressed finite state machines for faster structured output decoding.
Experiments show that SGLang achieves up to 6.4x higher throughput compared to
state-of-the-art inference systems on various large language and multi-modal
models on tasks including agent control, logical reasoning, few-shot learning
benchmarks, JSON decoding, retrieval-augmented generation pipelines, and
multi-turn chat. The code is publicly available at
https://github.com/sgl-project/sglang",None,-1
5a9f702a-7672-4277-8876-d187497aa86a,Tiny and Efficient Model for the Edge Detection Generalization,0.372618,3,"Most high-level computer vision tasks rely on low-level image operations as
their initial processes. Operations such as edge detection, image enhancement,
and super-resolution, provide the foundations for higher level image analysis.
In this work we address the edge detection considering three main objectives:
simplicity, efficiency, and generalization since current state-of-the-art
(SOTA) edge detection models are increased in complexity for better accuracy.
To achieve this, we present Tiny and Efficient Edge Detector (TEED), a light
convolutional neural network with only $58K$ parameters, less than $0.2$% of
the state-of-the-art models. Training on the BIPED dataset takes $less than 30
minutes$, with each epoch requiring $less than 5 minutes$. Our proposed model
is easy to train and it quickly converges within very first few epochs, while
the predicted edge-maps are crisp and of high quality. Additionally, we propose
a new dataset to test the generalization of edge detection, which comprises
samples from popular images used in edge detection and image segmentation. The
source code is available in https://github.com/xavysp/TEED.",None,-1
c91f9872-695f-4ffb-872e-397a977a069c,Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf,1.0,93,"Communication games, which we refer to as incomplete information games that
heavily depend on natural language communication, hold significant research
value in fields such as economics, social science, and artificial intelligence.
In this work, we explore the problem of how to engage large language models
(LLMs) in communication games, and in response, propose a tuning-free
framework. Our approach keeps LLMs frozen, and relies on the retrieval and
reflection on past communications and experiences for improvement. An empirical
study on the representative and widely-studied communication game,
``Werewolf'', demonstrates that our framework can effectively play Werewolf
game without tuning the parameters of the LLMs. More importantly, strategic
behaviors begin to emerge in our experiments, suggesting that it will be a
fruitful journey to engage LLMs in communication games and associated domains.",None,-1
d89d98eb-ae9d-4fc5-93ad-7c3de1457b44,A System for Human-AI collaboration for Online Customer Support,0.0784541,2,"AI enabled chat bots have recently been put to use to answer customer service
queries, however it is a common feedback of users that bots lack a personal
touch and are often unable to understand the real intent of the user's
question. To this end, it is desirable to have human involvement in the
customer servicing process. In this work, we present a system where a human
support agent collaborates in real-time with an AI agent to satisfactorily
answer customer queries. We describe the user interaction elements of the
solution, along with the machine learning techniques involved in the AI agent.",None,-1
30ce372b-6ee4-4731-ae19-b05207209939,SAM3D: Segment Anything in 3D Scenes,0.97849,32,"In this work, we propose SAM3D, a novel framework that is able to predict
masks in 3D point clouds by leveraging the Segment-Anything Model (SAM) in RGB
images without further training or finetuning. For a point cloud of a 3D scene
with posed RGB images, we first predict segmentation masks of RGB images with
SAM, and then project the 2D masks into the 3D points. Later, we merge the 3D
masks iteratively with a bottom-up merging approach. At each step, we merge the
point cloud masks of two adjacent frames with the bidirectional merging
approach. In this way, the 3D masks predicted from different frames are
gradually merged into the 3D masks of the whole 3D scene. Finally, we can
optionally ensemble the result from our SAM3D with the over-segmentation
results based on the geometric information of the 3D scenes. Our approach is
experimented with ScanNet dataset and qualitative results demonstrate that our
SAM3D achieves reasonable and fine-grained 3D segmentation results without any
training or finetuning of SAM.",None,-1
31aae24d-049f-4ed7-9953-dd1515775b5b,SpotEM: Efficient Video Search for Episodic Memory,0.796142,5,"The goal in episodic memory (EM) is to search a long egocentric video to
answer a natural language query (e.g., ""where did I leave my purse?""). Existing
EM methods exhaustively extract expensive fixed-length clip features to look
everywhere in the video for the answer, which is infeasible for long
wearable-camera videos that span hours or even days. We propose SpotEM, an
approach to achieve efficiency for a given EM method while maintaining good
accuracy. SpotEM consists of three key ideas: 1) a novel clip selector that
learns to identify promising video regions to search conditioned on the
language query; 2) a set of low-cost semantic indexing features that capture
the context of rooms, objects, and interactions that suggest where to look; and
3) distillation losses that address the optimization issues arising from
end-to-end joint training of the clip selector and EM model. Our experiments on
200+ hours of video from the Ego4D EM Natural Language Queries benchmark and
three different EM models demonstrate the effectiveness of our approach:
computing only 10% - 25% of the clip features, we preserve 84% - 97% of the
original EM model's accuracy. Project page:
https://vision.cs.utexas.edu/projects/spotem",None,-1
72d32143-ade1-4ecf-98c3-e7a7b9e63a5f,The Relational Bottleneck as an Inductive Bias for Efficient Abstraction,0.759285,17,"A central challenge for cognitive science is to explain how abstract concepts
are acquired from limited experience. This has often been framed in terms of a
dichotomy between connectionist and symbolic cognitive models. Here, we
highlight a recently emerging line of work that suggests a novel reconciliation
of these approaches, by exploiting an inductive bias that we term the
relational bottleneck. In that approach, neural networks are constrained via
their architecture to focus on relations between perceptual inputs, rather than
the attributes of individual inputs. We review a family of models that employ
this approach to induce abstractions in a data-efficient manner, emphasizing
their potential as candidate models for the acquisition of abstract concepts in
the human mind and brain.",None,-1
1cc95df5-f949-4b63-a1ae-092f9d511f7b,Data Driven Reward Initialization for Preference based Reinforcement Learning,0.103422,2,"Preference-based Reinforcement Learning (PbRL) methods utilize binary
feedback from the human in the loop (HiL) over queried trajectory pairs to
learn a reward model in an attempt to approximate the human's underlying reward
function capturing their preferences. In this work, we investigate the issue of
a high degree of variability in the initialized reward models which are
sensitive to random seeds of the experiment. This further compounds the issue
of degenerate reward functions PbRL methods already suffer from. We propose a
data-driven reward initialization method that does not add any additional cost
to the human in the loop and negligible cost to the PbRL agent and show that
doing so ensures that the predicted rewards of the initialized reward model are
uniform in the state space and this reduces the variability in the performance
of the method across multiple runs and is shown to improve the overall
performance compared to other initialization methods.",None,-1
232f532b-c654-41f1-aeb1-325792a90405,Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language,0.994037,40,"We propose LENS, a modular approach for tackling computer vision problems by
leveraging the power of large language models (LLMs). Our system uses a
language model to reason over outputs from a set of independent and highly
descriptive vision modules that provide exhaustive information about an image.
We evaluate the approach on pure computer vision settings such as zero- and
few-shot object recognition, as well as on vision and language problems. LENS
can be applied to any off-the-shelf LLM and we find that the LLMs with LENS
perform highly competitively with much bigger and much more sophisticated
systems, without any multimodal training whatsoever. We open-source our code at
https://github.com/ContextualAI/lens and provide an interactive demo.",None,-1
ba8cf84f-4ec8-4189-841f-f061cefca073,Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach,0.97689,14,"StarCraft II is a challenging benchmark for AI agents due to the necessity of
both precise micro level operations and strategic macro awareness. Previous
works, such as Alphastar and SCC, achieve impressive performance on tackling
StarCraft II , however, still exhibit deficiencies in long term strategic
planning and strategy interpretability. Emerging large language model (LLM)
agents, such as Voyage and MetaGPT, presents the immense potential in solving
intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs
on StarCraft II, a highly complex RTS game.To conveniently take full advantage
of LLMs` reasoning abilities, we first develop textual StratCraft II
environment, called TextStarCraft II, which LLM agent can interact. Secondly,
we propose a Chain of Summarization method, including single frame
summarization for processing raw observations and multi frame summarization for
analyzing game information, providing command recommendations, and generating
strategic decisions. Our experiment consists of two parts: first, an evaluation
by human experts, which includes assessing the LLMs`s mastery of StarCraft II
knowledge and the performance of LLM agents in the game; second, the in game
performance of LLM agents, encompassing aspects like win rate and the impact of
Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the
relevant knowledge and complex planning abilities needed to address StarCraft
II scenarios; 2. Human experts consider the performance of LLM agents to be
close to that of an average player who has played StarCraft II for eight years;
3. LLM agents are capable of defeating the built in AI at the Harder(Lv5)
difficulty level. We have open sourced the code and released demo videos of LLM
agent playing StarCraft II.",None,-1
094988b7-a1e0-42ad-93ff-63d2d3b8bbed,Named entity recognition using GPT for identifying comparable companies,0.41859,2,"For both public and private firms, comparable companies' analysis is widely
used as a method for company valuation. In particular, the method is of great
value for valuation of private equity companies. The several approaches to the
comparable companies' method usually rely on a qualitative approach to
identifying similar peer companies, which tend to use established industry
classification schemes and/or analyst intuition and knowledge. However, more
quantitative methods have started being used in the literature and in the
private equity industry, in particular, machine learning clustering, and
natural language processing (NLP). For NLP methods, the process consists of
extracting product entities from e.g., the company's website or company
descriptions from some financial database system and then to perform similarity
analysis. Here, using companies' descriptions/summaries from publicly available
companies' Wikipedia websites, we show that using large language models (LLMs),
such as GPT from OpenAI, has a much higher precision and success rate than
using the standard named entity recognition (NER) methods which use manual
annotation. We demonstrate quantitatively a higher precision rate, and show
that, qualitatively, it can be used to create appropriate comparable companies
peer groups which could then be used for equity valuation.",None,-1
af7d1a5e-0c4a-407c-b580-975fd6610cad,Prompting for Multimodal Hateful Meme Classification,0.99798,42,"Hateful meme classification is a challenging multimodal task that requires
complex reasoning and contextual background knowledge. Ideally, we could
leverage an explicit external knowledge base to supplement contextual and
cultural information in hateful memes. However, there is no known explicit
external knowledge base that could provide such hate speech contextual
information. To address this gap, we propose PromptHate, a simple yet effective
prompt-based model that prompts pre-trained language models (PLMs) for hateful
meme classification. Specifically, we construct simple prompts and provide a
few in-context examples to exploit the implicit knowledge in the pre-trained
RoBERTa language model for hateful meme classification. We conduct extensive
experiments on two publicly available hateful and offensive meme datasets. Our
experimental results show that PromptHate is able to achieve a high AUC of
90.96, outperforming state-of-the-art baselines on the hateful meme
classification task. We also perform fine-grained analyses and case studies on
various prompt settings and demonstrate the effectiveness of the prompts on
hateful meme classification.",None,-1
40ef3261-c62a-46f9-a217-364f63427e96,Political claim identification and categorization in a multilingual setting: First experiments,0.495364,1,"The identification and classification of political claims is an important
step in the analysis of political newspaper reports; however, resources for
this task are few and far between. This paper explores different strategies for
the cross-lingual projection of political claims analysis. We conduct
experiments on a German dataset, DebateNet2.0, covering the policy debate
sparked by the 2015 refugee crisis. Our evaluation involves two tasks (claim
identification and categorization), three languages (German, English, and
French) and two methods (machine translation -- the best method in our
experiments -- and multilingual embeddings).",None,-1
91635986-6bb6-4711-832d-82967cadada7,Revisiting Token Dropping Strategy in Efficient BERT Pretraining,0.453476,7,"Token dropping is a recently-proposed strategy to speed up the pretraining of
masked language models, such as BERT, by skipping the computation of a subset
of the input tokens at several middle layers. It can effectively reduce the
training time without degrading much performance on downstream tasks. However,
we empirically find that token dropping is prone to a semantic loss problem and
falls short in handling semantic-intense tasks. Motivated by this, we propose a
simple yet effective semantic-consistent learning method (ScTD) to improve the
token dropping. ScTD aims to encourage the model to learn how to preserve the
semantic information in the representation space. Extensive experiments on 12
tasks show that, with the help of our ScTD, token dropping can achieve
consistent and significant performance gains across all task types and model
sizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings
up to +1.56% average improvement over the vanilla token dropping.",None,-1
4665dc25-2b29-4f5d-9cd9-d095ea384059,Robust Knowledge Distillation from RNN-T Models With Noisy Training Labels Using Full-Sum Loss,0.0915751,1,"This work studies knowledge distillation (KD) and addresses its constraints
for recurrent neural network transducer (RNN-T) models. In hard distillation, a
teacher model transcribes large amounts of unlabelled speech to train a student
model. Soft distillation is another popular KD method that distills the output
logits of the teacher model. Due to the nature of RNN-T alignments, applying
soft distillation between RNN-T architectures having different posterior
distributions is challenging. In addition, bad teachers having high
word-error-rate (WER) reduce the efficacy of KD. We investigate how to
effectively distill knowledge from variable quality ASR teachers, which has not
been studied before to the best of our knowledge. We show that a sequence-level
KD, full-sum distillation, outperforms other distillation methods for RNN-T
models, especially for bad teachers. We also propose a variant of full-sum
distillation that distills the sequence discriminative knowledge of the teacher
leading to further improvement in WER. We conduct experiments on public
datasets namely SpeechStew and LibriSpeech, and on in-house production data.",None,-1
31e4337e-25e8-4e28-bdfb-298795db5ae2,UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video,0.404555,3,"We show how to build a model that allows realistic, free-viewpoint renderings
of a scene under novel lighting conditions from video. Our method -- UrbanIR:
Urban Scene Inverse Rendering -- computes an inverse graphics representation
from the video. UrbanIR jointly infers shape, albedo, visibility, and sun and
sky illumination from a single video of unbounded outdoor scenes with unknown
lighting. UrbanIR uses videos from cameras mounted on cars (in contrast to many
views of the same points in typical NeRF-style estimation). As a result,
standard methods produce poor geometry estimates (for example, roofs), and
there are numerous ''floaters''. Errors in inverse graphics inference can
result in strong rendering artifacts. UrbanIR uses novel losses to control
these and other sources of error. UrbanIR uses a novel loss to make very good
estimates of shadow volumes in the original scene. The resulting
representations facilitate controllable editing, delivering photorealistic
free-viewpoint renderings of relit scenes and inserted objects. Qualitative
evaluation demonstrates strong improvements over the state-of-the-art.",None,-1
367704e3-48b3-43a5-8872-f151b49a341e,Integrating UMLS Knowledge into Large Language Models for Medical Question Answering,0.42805,7,"Large language models (LLMs) have demonstrated powerful text generation
capabilities, bringing unprecedented innovation to the healthcare field. While
LLMs hold immense promise for applications in healthcare, applying them to real
clinical scenarios presents significant challenges, as these models may
generate content that deviates from established medical facts and even exhibit
potential biases. In our research, we develop an augmented LLM framework based
on the Unified Medical Language System (UMLS), aiming to better serve the
healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our
benchmark models, and conduct automatic evaluations using the ROUGE Score and
BERTScore on 104 questions from the LiveQA test set. Additionally, we establish
criteria for physician-evaluation based on four dimensions: Factuality,
Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician
evaluation with 20 questions on the LiveQA test set. Multiple resident
physicians conducted blind reviews to evaluate the generated content, and the
results indicate that this framework effectively enhances the factuality,
completeness, and relevance of generated content. Our research demonstrates the
effectiveness of using UMLS-augmented LLMs and highlights the potential
application value of LLMs in in medical question-answering.",None,-1
4ec59f39-d8de-4e2a-9d56-a11b85a4d51b,Continual Multimodal Knowledge Graph Construction,0.877479,6,"Current Multimodal Knowledge Graph Construction (MKGC) models struggle with
the real-world dynamism of continuously emerging entities and relations, often
succumbing to catastrophic forgetting-loss of previously acquired knowledge.
This study introduces benchmarks aimed at fostering the development of the
continual MKGC domain. We further introduce MSPT framework, designed to
surmount the shortcomings of existing MKGC approaches during multimedia data
processing. MSPT harmonizes the retention of learned knowledge (stability) and
the integration of new data (plasticity), outperforming current continual
learning and multimodal methods. Our results confirm MSPT's superior
performance in evolving knowledge environments, showcasing its capacity to
navigate balance between stability and plasticity.",None,-1
01893210-dada-49c4-9029-ecaae60150af,Program-Aided Reasoners (better) Know What They Know,0.200977,1,"Prior work shows that program-aided reasoning, in which large language models
(LLMs) are combined with programs written in programming languages such as
Python, can significantly improve accuracy on various reasoning tasks. However,
while accuracy is essential, it is also important for such reasoners to ""know
what they know"", which can be quantified through the calibration of the model.
In this paper, we compare the calibration of Program Aided Language Models
(PAL) and text-based Chain-of-thought (COT) prompting techniques over 5
datasets and 2 model types: LLaMA models and OpenAI models. Our results
indicate that PAL leads to improved calibration in 75% of the instances. Our
analysis uncovers that prompting styles that produce lesser diversity in
generations also have more calibrated results, and thus we also experiment with
inducing lower generation diversity using temperature scaling and find that for
certain temperatures, PAL is not only more accurate but is also more calibrated
than COT. Overall, we demonstrate that, in the majority of cases, program-aided
reasoners better know what they know than text-based counterparts.",None,-1
82d3c84f-5c90-4349-84c4-b00d74ee512e,Multi-Dimensional Evaluation of Text Summarization with In-Context Learning,0.842946,20,"Evaluation of natural language generation (NLG) is complex and
multi-dimensional. Generated text can be evaluated for fluency, coherence,
factuality, or any other dimensions of interest. Most frameworks that perform
such multi-dimensional evaluation require training on large manually or
synthetically generated datasets. In this paper, we study the efficacy of large
language models as multi-dimensional evaluators using in-context learning,
obviating the need for large training datasets. Our experiments show that
in-context learning-based evaluators are competitive with learned evaluation
frameworks for the task of text summarization, establishing state-of-the-art on
dimensions such as relevance and factual consistency. We then analyze the
effects of factors such as the selection and number of in-context examples on
performance. Finally, we study the efficacy of in-context learning based
evaluators in evaluating zero-shot summaries written by large language models
such as GPT-3.",None,-1
1a6f2fc3-132f-4e6c-a4a9-3a98b6264489,Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models,0.150139,19,"We investigate the internal behavior of Transformer-based Large Language
Models (LLMs) when they generate factually incorrect text. We propose modeling
factual queries as constraint satisfaction problems and use this framework to
investigate how the LLM interacts internally with factual constraints. We find
a strong positive relationship between the LLM's attention to constraint tokens
and the factual accuracy of generations. We curate a suite of 10 datasets
containing over 40,000 prompts to study the task of predicting factual errors
with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe,
a method probing attention patterns, that can predict factual errors and
fine-grained constraint satisfaction, and allow early error identification. The
approach and findings take another step towards using the mechanistic
understanding of LLMs to enhance their reliability.",None,-1
1d921831-599b-462f-a61a-c1ab33e5790c,SkinSAM: Empowering Skin Cancer Segmentation with Segment Anything Model,0.963174,36,"Skin cancer is a prevalent and potentially fatal disease that requires
accurate and efficient diagnosis and treatment. Although manual tracing is the
current standard in clinics, automated tools are desired to reduce human labor
and improve accuracy. However, developing such tools is challenging due to the
highly variable appearance of skin cancers and complex objects in the
background. In this paper, we present SkinSAM, a fine-tuned model based on the
Segment Anything Model that showed outstanding segmentation performance. The
models are validated on HAM10000 dataset which includes 10015 dermatoscopic
images. While larger models (ViT_L, ViT_H) performed better than the smaller
one (ViT_b), the finetuned model (ViT_b_finetuned) exhibited the greatest
improvement, with a Mean pixel accuracy of 0.945, Mean dice score of 0.8879,
and Mean IoU score of 0.7843. Among the lesion types, vascular lesions showed
the best segmentation results. Our research demonstrates the great potential of
adapting SAM to medical image segmentation tasks.",None,-1
3f4edc2b-585a-4ffa-a6bd-3a28cfc18062,Neural Architecture Search for Visual Anomaly Segmentation,0.180066,2,"This paper presents the first application of neural architecture search to
the complex task of segmenting visual anomalies. Measurement of anomaly
segmentation performance is challenging due to imbalanced anomaly pixels,
varying region areas, and various types of anomalies. First, the
region-weighted Average Precision (rwAP) metric is proposed as an alternative
to existing metrics, which does not need to be limited to a specific maximum
false positive rate. Second, the AutoPatch neural architecture search method is
proposed, which enables efficient segmentation of visual anomalies without any
training. By leveraging a pre-trained supernet, a black-box optimization
algorithm can directly minimize computational complexity and maximize
performance on a small validation set of anomalous examples. Finally,
compelling results are presented on the widely studied MVTec dataset,
demonstrating that AutoPatch outperforms the current state-of-the-art with
lower computational complexity, using only one example per type of anomaly. The
results highlight the potential of automated machine learning to optimize
throughput in industrial quality control. The code for AutoPatch is available
at: https://github.com/tommiekerssies/AutoPatch",None,-1
7c2461dc-6089-4ab8-9622-b605a8364626,Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks,0.329145,3,"Robustness has become an important consideration in deep learning. With the
help of explainable AI, mismatches between an explained model's decision
strategy and the user's domain knowledge (e.g. Clever Hans effects) have been
identified as a starting point for improving faulty models. However, it is less
clear what to do when the user and the explanation agree. In this paper, we
demonstrate that acceptance of explanations by the user is not a guarantee for
a machine learning model to be robust against Clever Hans effects, which may
remain undetected. Such hidden flaws of the model can nevertheless be
mitigated, and we demonstrate this by contributing a new method,
Explanation-Guided Exposure Minimization (EGEM), that preemptively prunes
variations in the ML model that have not been the subject of positive
explanation feedback. Experiments demonstrate that our approach leads to models
that strongly reduce their reliance on hidden Clever Hans strategies, and
consequently achieve higher accuracy on new data.",None,-1
bc4cac40-8c7b-490d-abe9-c8ae63bc376d,"CORE-GPT: Combining Open Access research and large language models for credible, trustworthy question answering",0.184974,8,"In this paper, we present CORE-GPT, a novel question-answering platform that
combines GPT-based language models and more than 32 million full-text open
access scientific articles from CORE. We first demonstrate that GPT3.5 and GPT4
cannot be relied upon to provide references or citations for generated text. We
then introduce CORE-GPT which delivers evidence-based answers to questions,
along with citations and links to the cited papers, greatly increasing the
trustworthiness of the answers and reducing the risk of hallucinations.
CORE-GPT's performance was evaluated on a dataset of 100 questions covering the
top 20 scientific domains in CORE, resulting in 100 answers and links to 500
relevant articles. The quality of the provided answers and and relevance of the
links were assessed by two annotators. Our results demonstrate that CORE-GPT
can produce comprehensive and trustworthy answers across the majority of
scientific domains, complete with links to genuine, relevant scientific
articles.",None,-1
3bf424f3-888b-45f3-a3b1-41e3ccc16fd4,Incorporating Graph Information in Transformer-based AMR Parsing,0.767792,8,"Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that
aims at providing a semantic graph abstraction representing a given text.
Current approaches are based on autoregressive language models such as BART or
T5, fine-tuned through Teacher Forcing to obtain a linearized version of the
AMR graph from a sentence. In this paper, we present LeakDistill, a model and
method that explores a modification to the Transformer architecture, using
structural adapters to explicitly incorporate graph information into the
learned representations and improve AMR parsing performance. Our experiments
show how, by employing word-to-node alignment to embed graph structural
information into the encoder at training time, we can obtain state-of-the-art
AMR parsing through self-knowledge distillation, even without the use of
additional data. We release the code at
\url{http://www.github.com/sapienzanlp/LeakDistill}.",None,-1
2a9b7c22-0729-49f6-bc4f-e82279c50812,Challenges and Applications of Large Language Models,0.720336,151,"Large Language Models (LLMs) went from non-existent to ubiquitous in the
machine learning discourse within a few years. Due to the fast pace of the
field, it is difficult to identify the remaining challenges and already
fruitful application areas. In this paper, we aim to establish a systematic set
of open problems and application successes so that ML researchers can
comprehend the field's current state more quickly and become productive.",None,-1
bf292d98-55c9-4dd9-9dd8-5698b63daed3,CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction,0.656728,8,"Recent advances in neural reconstruction using posed image sequences have
made remarkable progress. However, due to the lack of depth information,
existing volumetric-based techniques simply duplicate 2D image features of the
object surface along the entire camera ray. We contend this duplication
introduces noise in empty and occluded spaces, posing challenges for producing
high-quality 3D geometry. Drawing inspiration from traditional multi-view
stereo methods, we propose an end-to-end 3D neural reconstruction framework
CVRecon, designed to exploit the rich geometric embedding in the cost volumes
to facilitate 3D geometric feature learning. Furthermore, we present
Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature
representation that encodes view-dependent information with improved integrity
and robustness. Through comprehensive experiments, we demonstrate that our
approach significantly improves the reconstruction quality in various metrics
and recovers clear fine details of the 3D geometries. Our extensive ablation
studies provide insights into the development of effective 3D geometric feature
learning schemes. Project page: https://cvrecon.ziyue.cool/",None,-1
4dc530dc-41f0-4a18-8653-f9afc416f86a,"K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",0.998601,277,"We introduce k-planes, a white-box model for radiance fields in arbitrary
dimensions. Our model uses d choose 2 planes to represent a d-dimensional
scene, providing a seamless way to go from static (d=3) to dynamic (d=4)
scenes. This planar factorization makes adding dimension-specific priors easy,
e.g. temporal smoothness and multi-resolution spatial structure, and induces a
natural decomposition of static and dynamic components of a scene. We use a
linear feature decoder with a learned color basis that yields similar
performance as a nonlinear black-box MLP decoder. Across a range of synthetic
and real, static and dynamic, fixed and varying appearance scenes, k-planes
yields competitive and often state-of-the-art reconstruction fidelity with low
memory usage, achieving 1000x compression over a full 4D grid, and fast
optimization with a pure PyTorch implementation. For video results and code,
please see https://sarafridov.github.io/K-Planes.",None,-1
2b67c795-809c-4178-9300-b76071293cae,Towards solving ontological dissonance using network graphs,0.33271,1,"Data Spaces are an emerging concept for the trusted implementation of
data-based applications and business models, offering a high degree of
flexibility and sovereignty to all stakeholders. As Data Spaces are currently
emerging in different domains such as mobility, health or food, semantic
interfaces need to be identified and implemented to ensure the technical
interoperability of these Data Spaces. This paper consolidates data models from
13 different domains and analyzes the ontological dissonance of these domains.
Using a network graph, central data models and ontology attributes are
identified, while the semantic heterogeneity of these domains is described
qualitatively. The research outlook describes how these results help to connect
different Data Spaces across domains.",None,-1
dbf6a794-510f-4c13-ad05-7f0f2d023edd,Active Neural Mapping,0.582136,6,"We address the problem of active mapping with a continually-learned neural
scene representation, namely Active Neural Mapping. The key lies in actively
finding the target space to be explored with efficient agent movement, thus
minimizing the map uncertainty on-the-fly within a previously unseen
environment. In this paper, we examine the weight space of the
continually-learned neural field, and show empirically that the neural
variability, the prediction robustness against random weight perturbation, can
be directly utilized to measure the instant uncertainty of the neural map.
Together with the continuous geometric information inherited in the neural map,
the agent can be guided to find a traversable path to gradually gain knowledge
of the environment. We present for the first time an active mapping system with
a coordinate-based implicit neural representation for online scene
reconstruction. Experiments in the visually-realistic Gibson and Matterport3D
environment demonstrate the efficacy of the proposed method.",None,-1
5c793160-42ee-417c-b7d7-7acf81127392,VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions,0.378421,3,"Font design is of vital importance in the digital content design and modern
printing industry. Developing algorithms capable of automatically synthesizing
vector fonts can significantly facilitate the font design process. However,
existing methods mainly concentrate on raster image generation, and only a few
approaches can directly synthesize vector fonts. This paper proposes an
end-to-end trainable method, VecFontSDF, to reconstruct and synthesize
high-quality vector fonts using signed distance functions (SDFs). Specifically,
based on the proposed SDF-based implicit shape representation, VecFontSDF
learns to model each glyph as shape primitives enclosed by several parabolic
curves, which can be precisely converted to quadratic B\'ezier curves that are
widely used in vector font products. In this manner, most image generation
methods can be easily extended to synthesize vector fonts. Qualitative and
quantitative experiments conducted on a publicly-available dataset demonstrate
that our method obtains high-quality results on several tasks, including vector
font reconstruction, interpolation, and few-shot vector font synthesis,
markedly outperforming the state of the art.",None,-1
4b5089a4-8069-4420-828b-7365c26d5283,Active Learning for Multilingual Semantic Parser,0.392634,3,"Current multilingual semantic parsing (MSP) datasets are almost all collected
by translating the utterances in the existing datasets from the resource-rich
language to the target language. However, manual translation is costly. To
reduce the translation effort, this paper proposes the first active learning
procedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing
datasets to be translated. We also propose a novel selection method that
prioritizes the examples diversifying the logical form structures with more
lexical choices, and a novel hyperparameter tuning method that needs no extra
annotation cost. Our experiments show that AL-MSP significantly reduces
translation costs with ideal selection methods. Our selection method with
proper hyperparameters yields better parsing performance than the other
baselines on two multilingual datasets.",None,-1
3c4b2648-16da-45f1-b546-ce5865527999,MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking,0.99802,17,"As a video task, Multiple Object Tracking (MOT) is expected to capture
temporal information of targets effectively. Unfortunately, most existing
methods only explicitly exploit the object features between adjacent frames,
while lacking the capacity to model long-term temporal information. In this
paper, we propose MeMOTR, a long-term memory-augmented Transformer for
multi-object tracking. Our method is able to make the same object's track
embedding more stable and distinguishable by leveraging long-term memory
injection with a customized memory-attention layer. This significantly improves
the target association ability of our model. Experimental results on DanceTrack
show that MeMOTR impressively surpasses the state-of-the-art method by 7.9% and
13.0% on HOTA and AssA metrics, respectively. Furthermore, our model also
outperforms other Transformer-based methods on association performance on MOT17
and generalizes well on BDD100K. Code is available at
https://github.com/MCG-NJU/MeMOTR.",None,-1
842fafde-55bd-42b6-9b83-ad60e1afa099,AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression,0.554183,7,"Knowledge distillation has attracted a great deal of interest recently to
compress pre-trained language models. However, existing knowledge distillation
methods suffer from two limitations. First, the student model simply imitates
the teacher's behavior while ignoring the underlying reasoning. Second, these
methods usually focus on the transfer of sophisticated model-specific knowledge
but overlook data-specific knowledge. In this paper, we present a novel
attribution-driven knowledge distillation approach, which explores the
token-level rationale behind the teacher model based on Integrated Gradients
(IG) and transfers attribution knowledge to the student model. To enhance the
knowledge transfer of model reasoning and generalization, we further explore
multi-view attribution distillation on all potential decisions of the teacher.
Comprehensive experiments are conducted with BERT on the GLUE benchmark. The
experimental results demonstrate the superior performance of our approach to
several state-of-the-art methods.",None,-1
e783af69-bb23-4622-8e91-75cb31c40b25,"Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results",0.249898,17,"As AI-bots continue to gain popularity due to their human-like traits and the
intimacy they offer to users, their societal impact inevitably expands. This
leads to the rising necessity for comprehensive studies to fully understand
AI-bots and reveal their potential opportunities, drawbacks, and overall
societal impact. With that in mind, this research conducted an extensive
investigation into ChatGPT3, a renowned AI bot, aiming to assess the temporal
reliability of its personality profile. Psychological questionnaires were
administered to the chatbot on two separate occasions, followed by a comparison
of the responses to human normative data. The findings revealed varying levels
of agreement in chatbot's responses over time, with some scales displaying
excellent agreement while others demonstrated poor agreement. Overall,
Davinci-003 displayed a socially desirable and pro-social personality profile,
particularly in the domain of communion. However, the underlying basis of the
chatbot's responses-whether driven by conscious self reflection or
predetermined algorithms-remains uncertain.",None,-1
9ae84d48-7db2-460f-8805-15d7299b059a,CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation,0.390559,6,"Most nighttime semantic segmentation studies are based on domain adaptation
approaches and image input. However, limited by the low dynamic range of
conventional cameras, images fail to capture structural details and boundary
information in low-light conditions. Event cameras, as a new form of vision
sensors, are complementary to conventional cameras with their high dynamic
range. To this end, we propose a novel unsupervised Cross-Modality Domain
Adaptation (CMDA) framework to leverage multi-modality (Images and Events)
information for nighttime semantic segmentation, with only labels on daytime
images. In CMDA, we design the Image Motion-Extractor to extract motion
information and the Image Content-Extractor to extract content information from
images, in order to bridge the gap between different modalities (Images to
Events) and domains (Day to Night). Besides, we introduce the first image-event
nighttime semantic segmentation dataset. Extensive experiments on both the
public image dataset and the proposed image-event dataset demonstrate the
effectiveness of our proposed approach. We open-source our code, models, and
dataset at https://github.com/XiaRho/CMDA.",None,-1
34c435e1-6e69-41e1-9133-6630d03ff45c,Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information,0.819576,3,"Visual Word Sense Disambiguation (VWSD) is a task to find the image that most
accurately depicts the correct sense of the target word for the given context.
Previously, image-text matching models often suffered from recognizing
polysemous words. This paper introduces an unsupervised VWSD approach that uses
gloss information of an external lexical knowledge-base, especially the sense
definitions. Specifically, we suggest employing Bayesian inference to
incorporate the sense definitions when sense information of the answer is not
provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we
propose a context-aware definition generation with GPT-3. Experimental results
show that the VWSD performance significantly increased with our Bayesian
inference-based approach. In addition, our context-aware definition generation
achieved prominent performance improvement in OOD examples exhibiting better
performance than the existing definition generation method.",None,-1
6bd93834-ca11-421b-8a47-c027332d7205,ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment,0.463272,3,"We present a comprehensive evaluation of large language models for
multilingual readability assessment. Existing evaluation resources lack domain
and language diversity, limiting the ability for cross-domain and cross-lingual
analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset
with human annotations of 9757 sentences in Arabic, English, French, Hindi, and
Russian, collected from 112 different data sources. This benchmark will
encourage research on developing robust multilingual readability assessment
methods. Using ReadMe++, we benchmark multilingual and monolingual language
models in the supervised, unsupervised, and few-shot prompting settings. The
domain and language diversity in ReadMe++ enable us to test more effective
few-shot prompting, and identify shortcomings in state-of-the-art unsupervised
methods. Our experiments also reveal exciting results of superior domain
generalization and enhanced cross-lingual transfer capabilities by models
trained on ReadMe++. We will make our data publicly available and release a
python package tool for multilingual sentence readability prediction using our
trained models at: https://github.com/tareknaous/readme",None,-1
7a5ea646-b7e8-471e-8579-4f49f69b261b,Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection,0.735778,6,"Hate speech is a severe issue that affects many online platforms. So far,
several studies have been performed to develop robust hate speech detection
systems. Large language models like ChatGPT have recently shown a great promise
in performing several tasks, including hate speech detection. However, it is
crucial to comprehend the limitations of these models to build robust hate
speech detection systems. To bridge this gap, our study aims to evaluate the
strengths and weaknesses of the ChatGPT model in detecting hate speech at a
granular level across 11 languages. Our evaluation employs a series of
functionality tests that reveals various intricate failures of the model which
the aggregate metrics like macro F1 or accuracy are not able to unfold. In
addition, we investigate the influence of complex emotions, such as the use of
emojis in hate speech, on the performance of the ChatGPT model. Our analysis
highlights the shortcomings of the generative models in detecting certain types
of hate speech and highlighting the need for further research and improvements
in the workings of these models.",None,-1
d26bbe21-47d3-4729-89a6-481b9145a131,DisCLIP: Open-Vocabulary Referring Expression Generation,0.45875,6,"Referring Expressions Generation (REG) aims to produce textual descriptions
that unambiguously identifies specific objects within a visual scene.
Traditionally, this has been achieved through supervised learning methods,
which perform well on specific data distributions but often struggle to
generalize to new images and concepts. To address this issue, we present a
novel approach for REG, named DisCLIP, short for discriminative CLIP. We build
on CLIP, a large-scale visual-semantic model, to guide an LLM to generate a
contextual description of a target concept in an image while avoiding other
distracting concepts. Notably, this optimization happens at inference time and
does not require additional training or tuning of learned parameters. We
measure the quality of the generated text by evaluating the capability of a
receiver model to accurately identify the described object within the scene. To
achieve this, we use a frozen zero-shot comprehension module as a critique of
our generated referring expressions. We evaluate DisCLIP on multiple referring
expression benchmarks through human evaluation and show that it significantly
outperforms previous methods on out-of-domain datasets. Our results highlight
the potential of using pre-trained visual-semantic models for generating
high-quality contextual descriptions.",None,-1
313a6f09-2ee5-4727-b52d-23aa0d009fe4,Why We Don't Have AGI Yet,0.0662221,1,"The original vision of AI was re-articulated in 2002 via the term 'Artificial
General Intelligence' or AGI. This vision is to build 'Thinking Machines' -
computer systems that can learn, reason, and solve problems similar to the way
humans do. This is in stark contrast to the 'Narrow AI' approach practiced by
almost everyone in the field over the many decades. While several large-scale
efforts have nominally been working on AGI (most notably DeepMind), the field
of pure focused AGI development has not been well funded or promoted. This is
surprising given the fantastic value that true AGI can bestow on humanity. In
addition to the dearth of effort in this field, there are also several
theoretical and methodical missteps that are hampering progress. We highlight
why purely statistical approaches are unlikely to lead to AGI, and identify
several crucial cognitive abilities required to achieve human-like adaptability
and autonomous learning. We conclude with a survey of socio-technical factors
that have undoubtedly slowed progress towards AGI.",None,-1
d3b8de17-1823-41aa-9bfb-dd845c1b92bf,ChatGPT: A Meta-Analysis after 2.5 Months,0.951498,85,"ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and
media attention since its release in November 2022. However, little hard
evidence is available regarding its perception in various sources. In this
paper, we analyze over 300,000 tweets and more than 150 scientific papers to
investigate how ChatGPT is perceived and discussed. Our findings show that
ChatGPT is generally viewed as of high quality, with positive sentiment and
emotions of joy dominating in social media. Its perception has slightly
decreased since its debut, however, with joy decreasing and (negative) surprise
on the rise, and it is perceived more negatively in languages other than
English. In recent scientific papers, ChatGPT is characterized as a great
opportunity across various fields including the medical domain, but also as a
threat concerning ethics and receives mixed assessments for education. Our
comprehensive meta-analysis of ChatGPT's current perception after 2.5 months
since its release can contribute to shaping the public debate and informing its
future development. We make our data available.",None,-1
a80a8c87-782e-4670-9a43-f49e6d214e56,How to prepare your task head for finetuning,0.233644,8,"In deep learning, transferring information from a pretrained network to a
downstream task by finetuning has many benefits. The choice of task head plays
an important role in fine-tuning, as the pretrained and downstream tasks are
usually different. Although there exist many different designs for finetuning,
a full understanding of when and why these algorithms work has been elusive. We
analyze how the choice of task head controls feature adaptation and hence
influences the downstream performance. By decomposing the learning dynamics of
adaptation, we find that the key aspect is the training accuracy and loss at
the beginning of finetuning, which determines the ""energy"" available for the
feature's adaptation. We identify a significant trend in the effect of changes
in this initial energy on the resulting features after fine-tuning.
Specifically, as the energy increases, the Euclidean and cosine distances
between the resulting and original features increase, while their dot products
(and the resulting features' norm) first increase and then decrease. Inspired
by this, we give several practical principles that lead to better downstream
performance. We analytically prove this trend in an overparamterized linear
setting and verify its applicability to different experimental settings.",None,-1
0a0f1523-7b71-4a06-8beb-72da622d1efa,Text-to-SQL Error Correction with Language Models of Code,0.632121,5,"Despite recent progress in text-to-SQL parsing, current semantic parsers are
still not accurate enough for practical use. In this paper, we investigate how
to build automatic text-to-SQL error correction models. Noticing that
token-level edits are out of context and sometimes ambiguous, we propose
building clause-level edit models instead. Besides, while most language models
of code are not specifically pre-trained for SQL, they know common data
structures and their operations in programming languages such as Python. Thus,
we propose a novel representation for SQL queries and their edits that adheres
more closely to the pre-training corpora of language models of code. Our error
correction model improves the exact set match accuracy of different parsers by
2.4-6.5 and obtains up to 4.3 point absolute improvement over two strong
baselines. Our code and data are available at
https://github.com/OSU-NLP-Group/Auto-SQL-Correction.",None,-1
992507b9-4c48-43a3-b0ed-371b82b5e5b9,Language models are not naysayers: An analysis of language models on negation benchmarks,0.147473,23,"Negation has been shown to be a major bottleneck for masked language models,
such as BERT. However, whether this finding still holds for larger-sized
auto-regressive language models (``LLMs'') has not been studied
comprehensively. With the ever-increasing volume of research and applications
of LLMs, we take a step back to evaluate the ability of current-generation LLMs
to handle negation, a fundamental linguistic phenomenon that is central to
language understanding. We evaluate different LLMs -- including the open-source
GPT-neo, GPT-3, and InstructGPT -- against a wide range of negation benchmarks.
Through systematic experimentation with varying model sizes and prompts, we
show that LLMs have several limitations including insensitivity to the presence
of negation, an inability to capture the lexical semantics of negation, and a
failure to reason under negation.",None,-1
2859e232-f2ff-414c-a17e-cc1150eb1a45,Causal Disentangled Variational Auto-Encoder for Preference Understanding in Recommendation,0.569905,4,"Recommendation models are typically trained on observational user interaction
data, but the interactions between latent factors in users' decision-making
processes lead to complex and entangled data. Disentangling these latent
factors to uncover their underlying representation can improve the robustness,
interpretability, and controllability of recommendation models. This paper
introduces the Causal Disentangled Variational Auto-Encoder (CaD-VAE), a novel
approach for learning causal disentangled representations from interaction data
in recommender systems. The CaD-VAE method considers the causal relationships
between semantically related factors in real-world recommendation scenarios,
rather than enforcing independence as in existing disentanglement methods. The
approach utilizes structural causal models to generate causal representations
that describe the causal relationship between latent factors. The results
demonstrate that CaD-VAE outperforms existing methods, offering a promising
solution for disentangling complex user behavior data in recommendation
systems.",None,-1
b407a70d-7929-4120-9a8f-7a5a80efc948,Reflective Linguistic Programming (RLP): A Stepping Stone in Socially-Aware AGI (SocialAGI),0.529254,8,"This paper presents Reflective Linguistic Programming (RLP), a unique
approach to conversational AI that emphasizes self-awareness and strategic
planning. RLP encourages models to introspect on their own predefined
personality traits, emotional responses to incoming messages, and planned
strategies, enabling contextually rich, coherent, and engaging interactions. A
striking illustration of RLP's potential involves a toy example, an AI persona
with an adversarial orientation, a demon named `Bogus' inspired by the
children's fairy tale Hansel & Gretel. Bogus exhibits sophisticated behaviors,
such as strategic deception and sensitivity to user discomfort, that
spontaneously arise from the model's introspection and strategic planning.
These behaviors are not pre-programmed or prompted, but emerge as a result of
the model's advanced cognitive modeling. The potential applications of RLP in
socially-aware AGI (Social AGI) are vast, from nuanced negotiations and mental
health support systems to the creation of diverse and dynamic AI personas. Our
exploration of deception serves as a stepping stone towards a new frontier in
AGI, one filled with opportunities for advanced cognitive modeling and the
creation of truly human `digital souls'.",None,-1
84d3158a-9f9a-4ab6-94f5-b21de6e7617f,Measuring Faithfulness in Chain-of-Thought Reasoning,0.968247,77,"Large language models (LLMs) perform better when they produce step-by-step,
""Chain-of-Thought"" (CoT) reasoning before answering a question, but it is
unclear if the stated reasoning is a faithful explanation of the model's actual
reasoning (i.e., its process for answering the question). We investigate
hypotheses for how CoT reasoning may be unfaithful, by examining how the model
predictions change when we intervene on the CoT (e.g., by adding mistakes or
paraphrasing it). Models show large variation across tasks in how strongly they
condition on the CoT when predicting their answer, sometimes relying heavily on
the CoT and other times primarily ignoring it. CoT's performance boost does not
seem to come from CoT's added test-time compute alone or from information
encoded via the particular phrasing of the CoT. As models become larger and
more capable, they produce less faithful reasoning on most tasks we study.
Overall, our results suggest that CoT can be faithful if the circumstances such
as the model size and task are carefully chosen.",None,-1
2e0ed06a-8517-46d9-a279-26339be3f78f,M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,0.892155,32,"Medical vision-language models enable co-learning and integrating features
from medical imaging and clinical text. However, these models are not easy to
train and the latent representation space can be complex. Here we propose a
novel way for pre-training and regularising medical vision-language models. The
proposed method, named Medical vision-language pre-training with Frozen
language models and Latent spAce Geometry optimization (M-FLAG), leverages a
frozen language model for training stability and efficiency and introduces a
novel orthogonality loss to harmonize the latent space geometry. We demonstrate
the potential of the pre-trained model on three downstream tasks: medical image
classification, segmentation, and object detection. Extensive experiments
across five public datasets demonstrate that M-FLAG significantly outperforms
existing medical vision-language pre-training approaches and reduces the number
of parameters by 78\%. Notably, M-FLAG achieves outstanding performance on the
segmentation task while using only 1\% of the RSNA dataset, even outperforming
ImageNet pre-trained models that have been fine-tuned using 100\% of the data.",None,-1
a7b6ada6-94b3-46c1-9a7c-a0770c883ea4,Using novel data and ensemble models to improve automated labeling of Sustainable Development Goals,0.258406,2,"A number of labeling systems based on text have been proposed to help monitor
work on the United Nations (UN) Sustainable Development Goals (SDGs). Here, we
present a systematic comparison of systems using a variety of text sources and
show that systems differ considerably in their specificity (i.e., true-positive
rate) and sensitivity (i.e., true-negative rate), have systematic biases (e.g.,
are more sensitive to specific SDGs relative to others), and are susceptible to
the type and amount of text analyzed. We then show that an ensemble model that
pools labeling systems alleviates some of these limitations, exceeding the
labeling performance of all currently available systems. We conclude that
researchers and policymakers should care about the choice of labeling system
and that ensemble methods should be favored when drawing conclusions about the
absolute and relative prevalence of work on the SDGs based on automated
methods.",None,-1
7f6c4708-7dc1-4116-b1ed-e1adeb286938,Class-Incremental Learning based on Label Generation,0.776989,7,"Despite the great success of pre-trained language models, it is still a
challenge to use these models for continual learning, especially for the
class-incremental learning (CIL) setting due to catastrophic forgetting (CF).
This paper reports our finding that if we formulate CIL as a continual label
generation problem, CF is drastically reduced and the generalizable
representations of pre-trained models can be better retained. We thus propose a
new CIL method (VAG) that also leverages the sparsity of vocabulary to focus
the generation and creates pseudo-replay samples by using label semantics.
Experimental results show that VAG outperforms baselines by a large margin.",None,-1
da75578d-c8b8-439e-a784-c33fb85be406,Multilingual Event Extraction from Historical Newspaper Adverts,0.371531,2,"NLP methods can aid historians in analyzing textual materials in greater
volumes than manually feasible. Developing such methods poses substantial
challenges though. First, acquiring large, annotated historical datasets is
difficult, as only domain experts can reliably label them. Second, most
available off-the-shelf NLP models are trained on modern language texts,
rendering them significantly less effective when applied to historical corpora.
This is particularly problematic for less well studied tasks, and for languages
other than English. This paper addresses these challenges while focusing on the
under-explored task of event extraction from a novel domain of historical
texts. We introduce a new multilingual dataset in English, French, and Dutch
composed of newspaper ads from the early modern colonial period reporting on
enslaved people who liberated themselves from enslavement. We find that: 1)
even with scarce annotated data, it is possible to achieve surprisingly good
results by formulating the problem as an extractive QA task and leveraging
existing datasets and models for modern languages; and 2) cross-lingual
low-resource learning for historical languages is highly challenging, and
machine translation of the historical datasets to the considered target
languages is, in practice, often the best-performing solution.",None,-1
2d0271ee-a680-42fa-a983-3d4a2ee820b6,AMIR: Automated MisInformation Rebuttal -- A COVID-19 Vaccination Datasets based Recommendation System,0.336915,1,"Misinformation has emerged as a major societal threat in recent years in
general; specifically in the context of the COVID-19 pandemic, it has wrecked
havoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable
solutions for combating misinformation are the need of the hour. This work
explored how existing information obtained from social media and augmented with
more curated fact checked data repositories can be harnessed to facilitate
automated rebuttal of misinformation at scale. While the ideas herein can be
generalized and reapplied in the broader context of misinformation mitigation
using a multitude of information sources and catering to the spectrum of social
media platforms, this work serves as a proof of concept, and as such, it is
confined in its scope to only rebuttal of tweets, and in the specific context
of misinformation regarding COVID-19. It leverages two publicly available
datasets, viz. FaCov (fact-checked articles) and misleading (social media
Twitter) data on COVID-19 Vaccination.",None,-1
eaf18c06-503a-4a68-92d0-3d2a61231c8d,GDDS: Pulmonary Bronchioles Segmentation with Group Deep Dense Supervision,0.734749,3,"Airway segmentation, especially bronchioles segmentation, is an important but
challenging task because distal bronchus are sparsely distributed and of a fine
scale. Existing neural networks usually exploit sparse topology to learn the
connectivity of bronchioles and inefficient shallow features to capture such
high-frequency information, leading to the breakage or missed detection of
individual thin branches. To address these problems, we contribute a new
bronchial segmentation method based on Group Deep Dense Supervision (GDDS) that
emphasizes fine-scale bronchioles segmentation in a simple-but-effective
manner. First, Deep Dense Supervision (DDS) is proposed by constructing local
dense topology skillfully and implementing dense topological learning on a
specific shallow feature layer. GDDS further empowers the shallow features with
better perception ability to detect bronchioles, even the ones that are not
easily discernible to the naked eye. Extensive experiments on the BAS benchmark
dataset have shown that our method promotes the network to have a high
sensitivity in capturing fine-scale branches and outperforms state-of-the-art
methods by a large margin (+12.8 % in BD and +8.8 % in TD) while only
introducing a small number of extra parameters.",None,-1
4c234e4c-0da0-4450-a2cd-17c85f3b8fec,Model Reporting for Certifiable AI: A Proposal from Merging EU Regulation into AI Development,0.154113,5,"Despite large progress in Explainable and Safe AI, practitioners suffer from
a lack of regulation and standards for AI safety. In this work we merge recent
regulation efforts by the European Union and first proposals for AI guidelines
with recent trends in research: data and model cards. We propose the use of
standardized cards to document AI applications throughout the development
process. Our main contribution is the introduction of use-case and operation
cards, along with updates for data and model cards to cope with regulatory
requirements. We reference both recent research as well as the source of the
regulation in our cards and provide references to additional support material
and toolboxes whenever possible. The goal is to design cards that help
practitioners develop safe AI systems throughout the development process, while
enabling efficient third-party auditing of AI applications, being easy to
understand, and building trust in the system. Our work incorporates insights
from interviews with certification experts as well as developers and
individuals working with the developed AI applications.",None,-1
e9db9963-d474-4187-920b-8e18ae397bda,Multi-modal Multi-view Clustering based on Non-negative Matrix Factorization,0.243723,2,"By combining related objects, unsupervised machine learning techniques aim to
reveal the underlying patterns in a data set. Non-negative Matrix Factorization
(NMF) is a data mining technique that splits data matrices by imposing
restrictions on the elements' non-negativity into two matrices: one
representing the data partitions and the other to represent the cluster
prototypes of the data set. This method has attracted a lot of attention and is
used in a wide range of applications, including text mining, clustering,
language modeling, music transcription, and neuroscience (gene separation). The
interpretation of the generated matrices is made simpler by the absence of
negative values. In this article, we propose a study on multi-modal clustering
algorithms and present a novel method called multi-modal multi-view
non-negative matrix factorization, in which we analyze the collaboration of
several local NMF models. The experimental results show the value of the
proposed approach, which was evaluated using a variety of data sets, and the
obtained results are very promising compared to state of art methods.",None,-1
ee892d05-9056-42b5-ae86-6483a056357c,Fair Multi-Exit Framework for Facial Attribute Classification,0.345673,3,"Fairness has become increasingly pivotal in facial recognition. Without bias
mitigation, deploying unfair AI would harm the interest of the underprivileged
population. In this paper, we observe that though the higher accuracy that
features from the deeper layer of a neural networks generally offer, fairness
conditions deteriorate as we extract features from deeper layers. This
phenomenon motivates us to extend the concept of multi-exit framework. Unlike
existing works mainly focusing on accuracy, our multi-exit framework is
fairness-oriented, where the internal classifiers are trained to be more
accurate and fairer. During inference, any instance with high confidence from
an internal classifier is allowed to exit early. Moreover, our framework can be
applied to most existing fairness-aware frameworks. Experiment results show
that the proposed framework can largely improve the fairness condition over the
state-of-the-art in CelebA and UTK Face datasets.",None,-1
1357c45d-056e-4c1f-91e4-7a4cc30916d2,Classification of Cross-cultural News Events,0.819908,3,"We present a methodology to support the analysis of culture from text such as
news events and demonstrate its usefulness on categorizing news events from
different categories (society, business, health, recreation, science, shopping,
sports, arts, computers, games and home) across different geographical
locations (different places in 117 countries). We group countries based on the
culture that they follow and then filter the news events based on their content
category. The news events are automatically labelled with the help of Hofstedes
cultural dimensions. We present combinations of events across different
categories and check the performances of different classification methods. We
also presents experimental comparison of different number of features in order
to find a suitable set to represent the culture.",None,-1
893a552b-fc1e-4e75-8504-6f5b450640b6,Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning,0.685959,13,"Self-supervised learning (SSL) has made remarkable progress in visual
representation learning. Some studies combine SSL with knowledge distillation
(SSL-KD) to boost the representation learning performance of small models. In
this study, we propose a Multi-mode Online Knowledge Distillation method (MOKD)
to boost self-supervised visual representation learning. Different from
existing SSL-KD methods that transfer knowledge from a static pre-trained
teacher to a student, in MOKD, two different models learn collaboratively in a
self-supervised manner. Specifically, MOKD consists of two distillation modes:
self-distillation and cross-distillation modes. Among them, self-distillation
performs self-supervised learning for each model independently, while
cross-distillation realizes knowledge interaction between different models. In
cross-distillation, a cross-attention feature search strategy is proposed to
enhance the semantic feature alignment between different models. As a result,
the two models can absorb knowledge from each other to boost their
representation learning performance. Extensive experimental results on
different backbones and datasets demonstrate that two heterogeneous models can
benefit from MOKD and outperform their independently trained baseline. In
addition, MOKD also outperforms existing SSL-KD methods for both the student
and teacher models.",None,-1
e3b7da08-f160-4d98-8405-750799b20eef,Identity-Preserving Aging of Face Images via Latent Diffusion Models,0.331116,3,"The performance of automated face recognition systems is inevitably impacted
by the facial aging process. However, high quality datasets of individuals
collected over several years are typically small in scale. In this work, we
propose, train, and validate the use of latent text-to-image diffusion models
for synthetically aging and de-aging face images. Our models succeed with
few-shot training, and have the added benefit of being controllable via
intuitive textual prompting. We observe high degrees of visual realism in the
generated images while maintaining biometric fidelity measured by commonly used
metrics. We evaluate our method on two benchmark datasets (CelebA and AgeDB)
and observe significant reduction (~44%) in the False Non-Match Rate compared
to existing state-of the-art baselines.",None,-1
dc8dae61-8279-4728-a40e-2234bbe4fb85,Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference,0.080483,2,"Humans no doubt use language to communicate about their emotional
experiences, but does language in turn help humans understand emotions, or is
language just a vehicle of communication? This study used a form of artificial
intelligence (AI) known as large language models (LLMs) to assess whether
language-based representations of emotion causally contribute to the AI's
ability to generate inferences about the emotional meaning of novel situations.
Fourteen attributes of human emotion concept representation were found to be
represented by the LLM's distinct artificial neuron populations. By
manipulating these attribute-related neurons, we in turn demonstrated the role
of emotion concept knowledge in generative emotion inference. The
attribute-specific performance deterioration was related to the importance of
different attributes in human mental space. Our findings provide a
proof-in-concept that even a LLM can learn about emotions in the absence of
sensory-motor representations and highlight the contribution of
language-derived emotion-concept knowledge for emotion inference.",None,-1
635f7a7a-3680-4d52-809e-b18f95b148e4,Can Large Language Models assist in Hazard Analysis?,0.659057,6,"Large Language Models (LLMs), such as GPT-3, have demonstrated remarkable
natural language processing and generation capabilities and have been applied
to a variety tasks, such as source code generation. This paper explores the
potential of integrating LLMs in the hazard analysis for safety-critical
systems, a process which we refer to as co-hazard analysis (CoHA). In CoHA, a
human analyst interacts with an LLM via a context-aware chat session and uses
the responses to support elicitation of possible hazard causes. In this
experiment, we explore CoHA with three increasingly complex versions of a
simple system, using Open AI's ChatGPT service. The quality of ChatGPT's
responses were systematically assessed to determine the feasibility of CoHA
given the current state of LLM technology. The results suggest that LLMs may be
useful for supporting human analysts performing hazard analysis.",None,-1
9fe66022-8f92-4c32-b99b-ca77502d6d80,PDSketch: Integrated Planning Domain Programming and Learning,0.229006,1,"This paper studies a model learning and online planning approach towards
building flexible and general robots. Specifically, we investigate how to
exploit the locality and sparsity structures in the underlying environmental
transition model to improve model generalization, data-efficiency, and
runtime-efficiency. We present a new domain definition language, named
PDSketch. It allows users to flexibly define high-level structures in the
transition models, such as object and feature dependencies, in a way similar to
how programmers use TensorFlow or PyTorch to specify kernel sizes and hidden
dimensions of a convolutional neural network. The details of the transition
model will be filled in by trainable neural networks. Based on the defined
structures and learned parameters, PDSketch automatically generates
domain-independent planning heuristics without additional training. The derived
heuristics accelerate the performance-time planning for novel goals.",None,-1
8565fd9b-3e96-420e-9a62-57b9d5fbb433,Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models,0.388475,63,"The escalating debate on AI's capabilities warrants developing reliable
metrics to assess machine ""intelligence"". Recently, many anecdotal examples
were used to suggest that newer large language models (LLMs) like ChatGPT and
GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached
conflicting conclusions regarding those abilities. We investigate the extent of
LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs
exhibit certain N-ToM abilities, this behavior is far from being robust. We
further examine the factors impacting performance on N-ToM tasks and discover
that LLMs struggle with adversarial examples, indicating reliance on shallow
heuristics rather than robust ToM abilities. We caution against drawing
conclusions from anecdotal examples, limited benchmark testing, and using
human-designed psychological tests to evaluate models.",None,-1
dd65916c-7877-402f-8abe-f133cc039737,Using Auxiliary Tasks In Multimodal Fusion Of Wav2vec 2.0 And BERT For Multimodal Emotion Recognition,0.683391,12,"The lack of data and the difficulty of multimodal fusion have always been
challenges for multimodal emotion recognition (MER). In this paper, we propose
to use pretrained models as upstream network, wav2vec 2.0 for audio modality
and BERT for text modality, and finetune them in downstream task of MER to cope
with the lack of data. For the difficulty of multimodal fusion, we use a
K-layer multi-head attention mechanism as a downstream fusion module. Starting
from the MER task itself, we design two auxiliary tasks to alleviate the
insufficient fusion between modalities and guide the network to capture and
align emotion-related features. Compared to the previous state-of-the-art
models, we achieve a better performance by 78.42% Weighted Accuracy (WA) and
79.71% Unweighted Accuracy (UA) on the IEMOCAP dataset.",None,-1
2e7bc074-09cd-452a-b85d-df3b922bc2cf,Extending Explainable Boosting Machines to Scientific Image Data,0.134988,1,"As the deployment of computer vision technology becomes increasingly common
in science, the need for explanations of the system and its output has become a
focus of great concern. Driven by the pressing need for interpretable models in
science, we propose the use of Explainable Boosting Machines (EBMs) for
scientific image data. Inspired by an important application underpinning the
development of quantum technologies, we apply EBMs to cold-atom soliton image
data tabularized using Gabor Wavelet Transform-based techniques that preserve
the spatial structure of the data. In doing so, we demonstrate the use of EBMs
for image data for the first time and show that our approach provides
explanations that are consistent with human intuition about the data.",None,-1
04725936-adca-4b49-9986-1b839f3e1804,Designing a 3D-Aware StyleNeRF Encoder for Face Editing,0.16886,6,"GAN inversion has been exploited in many face manipulation tasks, but 2D GANs
often fail to generate multi-view 3D consistent images. The encoders designed
for 2D GANs are not able to provide sufficient 3D information for the inversion
and editing. Therefore, 3D-aware GAN inversion is proposed to increase the 3D
editing capability of GANs. However, the 3D-aware GAN inversion remains
under-explored. To tackle this problem, we propose a 3D-aware (3Da) encoder for
GAN inversion and face editing based on the powerful StyleNeRF model. Our
proposed 3Da encoder combines a parametric 3D face model with a learnable
detail representation model to generate geometry, texture and view direction
codes. For more flexible face manipulation, we then design a dual-branch
StyleFlow module to transfer the StyleNeRF codes with disentangled geometry and
texture flows. Extensive experiments demonstrate that we realize 3D consistent
face manipulation in both facial attribute editing and texture transfer.
Furthermore, for video editing, we make the sequence of frame codes share a
common canonical manifold, which improves the temporal consistency of the
edited attributes.",None,-1
8cb4793a-892d-48d9-b127-f00061b25350,SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank,0.278639,1,"Deep neural classifiers trained with cross-entropy loss (CE loss) often
suffer from poor calibration, necessitating the task of out-of-distribution
(OOD) detection. Traditional supervised OOD detection methods require expensive
manual annotation of in-distribution and OOD samples. To address the annotation
bottleneck, we introduce SELFOOD, a self-supervised OOD detection method that
requires only in-distribution samples as supervision. We cast OOD detection as
an inter-document intra-label (IDIL) ranking problem and train the classifier
with our pairwise ranking loss, referred to as IDIL loss. Specifically, given a
set of in-distribution documents and their labels, for each label, we train the
classifier to rank the softmax scores of documents belonging to that label to
be higher than the scores of documents that belong to other labels. Unlike CE
loss, our IDIL loss function reaches zero when the desired confidence ranking
is achieved and gradients are backpropagated to decrease probabilities
associated with incorrect labels rather than continuously increasing the
probability of the correct label. Extensive experiments with several
classifiers on multiple classification datasets demonstrate the effectiveness
of our method in both coarse- and fine-grained settings.",None,-1
24bc02ad-46d1-48b9-b451-04c15a86a11c,Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes,0.652255,7,"Humans and animals have a rich and flexible understanding of the physical
world, which enables them to infer the underlying dynamical trajectories of
objects and events, plausible future states, and use that to plan and
anticipate the consequences of actions. However, the neural mechanisms
underlying these computations are unclear. We combine a goal-driven modeling
approach with dense neurophysiological data and high-throughput human
behavioral readouts to directly impinge on this question. Specifically, we
construct and evaluate several classes of sensory-cognitive networks to predict
the future state of rich, ethologically-relevant environments, ranging from
self-supervised end-to-end models with pixel-wise or object-centric objectives,
to models that future predict in the latent space of purely static image-based
or dynamic video-based pretrained foundation models. We find strong
differentiation across these model classes in their ability to predict neural
and behavioral data both within and across diverse environments. In particular,
we find that neural responses are currently best predicted by models trained to
predict the future state of their environment in the latent space of pretrained
foundation models optimized for dynamic scenes in a self-supervised manner.
Notably, models that future predict in the latent space of video foundation
models that are optimized to support a diverse range of sensorimotor tasks,
reasonably match both human behavioral error patterns and neural dynamics
across all environmental scenarios that we were able to test. Overall, these
findings suggest that the neural mechanisms and behaviors of primate mental
simulation are thus far most consistent with being optimized to future predict
on dynamic, reusable visual representations that are useful for Embodied AI
more generally.",None,-1
593308dd-42ed-4f04-b507-f902b17482c8,Quantifying the Dialect Gap and its Correlates Across Languages,0.456921,11,"Historically, researchers and consumers have noticed a decrease in quality
when applying NLP tools to minority variants of languages (i.e. Puerto Rican
Spanish or Swiss German), but studies exploring this have been limited to a
select few languages. Additionally, past studies have mainly been conducted in
a monolingual context, so cross-linguistic trends have not been identified and
tied to external factors. In this work, we conduct a comprehensive evaluation
of the most influential, state-of-the-art large language models (LLMs) across
two high-use applications, machine translation and automatic speech
recognition, to assess their functionality on the regional dialects of several
high- and low-resource languages. Additionally, we analyze how the regional
dialect gap is correlated with economic, social, and linguistic factors. The
impact of training data, including related factors like dataset size and its
construction procedure, is shown to be significant but not consistent across
models or languages, meaning a one-size-fits-all approach cannot be taken in
solving the dialect gap. This work will lay the foundation for furthering the
field of dialectal NLP by laying out evident disparities and identifying
possible pathways for addressing them through mindful data collection.",None,-1
179d2b74-5284-466d-82b4-348a92106504,ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs,0.770769,24,"ChatGPT, as a recently launched large language model (LLM), has shown
superior performance in various natural language processing (NLP) tasks.
However, two major limitations hinder its potential applications: (1) the
inflexibility of finetuning on downstream tasks and (2) the lack of
interpretability in the decision-making process. To tackle these limitations,
we propose a novel framework that leverages the power of ChatGPT for specific
tasks, such as text classification, while improving its interpretability. The
proposed framework conducts a knowledge graph extraction task to extract
refined and structural knowledge from the raw data using ChatGPT. The rich
knowledge is then converted into a graph, which is further used to train an
interpretable linear classifier to make predictions. To evaluate the
effectiveness of our proposed method, we conduct experiments on four datasets.
The result shows that our method can significantly improve the performance
compared to directly utilizing ChatGPT for text classification tasks. And our
method provides a more transparent decision-making process compared with
previous text classification methods.",None,-1
4dd8acff-eb6c-4e6e-97a4-0031908db0eb,GLADIS: A General and Large Acronym Disambiguation Benchmark,0.543882,2,"Acronym Disambiguation (AD) is crucial for natural language understanding on
various sources, including biomedical reports, scientific papers, and search
engine queries. However, existing acronym disambiguation benchmarks and tools
are limited to specific domains, and the size of prior benchmarks is rather
small. To accelerate the research on acronym disambiguation, we construct a new
benchmark named GLADIS with three components: (1) a much larger acronym
dictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus
with 160 million sentences; (3) three datasets that cover the general,
scientific, and biomedical domains. We then pre-train a language model,
\emph{AcroBERT}, on our constructed corpus for general acronym disambiguation,
and show the challenges and values of our new benchmark.",None,-1
eec754e6-dede-491a-bb22-28a5b524107d,A Frustratingly Simple Decoding Method for Neural Text Generation,0.151825,6,"We introduce a frustratingly simple, super efficient and surprisingly
effective decoding method, which we call Frustratingly Simple Decoding (FSD),
for neural text generation. The idea behind FSD is straightforward: we build an
anti-LM based on previously generated text and use this anti-LM to penalize
future generation of what has been generated. The anti-LM can be implemented as
simple as an n-gram language model or a vectorized variant. In this way, FSD
introduces no extra model parameters and negligible computational overhead (FSD
can be as fast as greedy search). Despite the simplicity, FSD is surprisingly
effective; Experiments show that FSD can outperform the canonical methods to
date (i.e., nucleus sampling) as well as several strong baselines that were
proposed recently.",None,-1
ab75c486-c92e-4dbf-a421-ed56349e358a,Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling,0.979691,11,"Recent works have proposed to craft adversarial clothes for evading person
detectors, while they are either only effective at limited viewing angles or
very conspicuous to humans. We aim to craft adversarial texture for clothes
based on 3D modeling, an idea that has been used to craft rigid adversarial
objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes
are non-rigid, leading to difficulties in physical realization. In order to
craft natural-looking adversarial clothes that can evade person detectors at
multiple viewing angles, we propose adversarial camouflage textures (AdvCaT)
that resemble one kind of the typical textures of daily clothes, camouflage
textures. We leverage the Voronoi diagram and Gumbel-softmax trick to
parameterize the camouflage textures and optimize the parameters via 3D
modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes
combining topologically plausible projection (TopoProj) and Thin Plate Spline
(TPS) to narrow the gap between digital and real-world objects. We printed the
developed 3D texture pieces on fabric materials and tailored them into T-shirts
and trousers. Experiments show high attack success rates of these clothes
against multiple detectors.",None,-1
8acea8ca-a854-407c-a92f-f27761d0aa7d,QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing,0.607163,4,"Questions Under Discussion (QUD) is a versatile linguistic framework in which
discourse progresses as continuously asking questions and answering them.
Automatic parsing of a discourse to produce a QUD structure thus entails a
complex question generation task: given a document and an answer sentence,
generate a question that satisfies linguistic constraints of QUD and can be
grounded in an anchor sentence in prior context. These questions are known to
be curiosity-driven and open-ended. This work introduces the first framework
for the automatic evaluation of QUD parsing, instantiating the theoretical
constraints of QUD in a concrete protocol. We present QUDeval, a dataset of
fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned
systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD
is still challenging for modern LLMs, and that existing evaluation metrics
poorly approximate parser quality. Encouragingly, human-authored QUDs are
scored highly by our human evaluators, suggesting that there is headroom for
further progress on language modeling to improve both QUD parsing and QUD
evaluation.",None,-1
1fbd5caf-cefe-4e2a-b6d9-af6b4d8f5763,Action valuation of on- and off-ball soccer players based on multi-agent deep reinforcement learning,0.565402,3,"Analysis of invasive sports such as soccer is challenging because the game
situation changes continuously in time and space, and multiple agents
individually recognize the game situation and make decisions. Previous studies
using deep reinforcement learning have often considered teams as a single agent
and valued the teams and players who hold the ball in each discrete event. Then
it was challenging to value the actions of multiple players, including players
far from the ball, in a spatiotemporally continuous state space. In this paper,
we propose a method of valuing possible actions for on- and off-ball soccer
players in a single holistic framework based on multi-agent deep reinforcement
learning. We consider a discrete action space in a continuous state space that
mimics that of Google research football and leverages supervised learning for
actions in reinforcement learning. In the experiment, we analyzed the
relationships with conventional indicators, season goals, and game ratings by
experts, and showed the effectiveness of the proposed method. Our approach can
assess how multiple players move continuously throughout the game, which is
difficult to be discretized or labeled but vital for teamwork, scouting, and
fan engagement.",None,-1
2c6bed76-03bd-4fc4-860a-b2bf7da81240,SEMI-PointRend: Improved Semiconductor Wafer Defect Classification and Segmentation as Rendering,0.858947,4,"In this study, we applied the PointRend (Point-based Rendering) method to
semiconductor defect segmentation. PointRend is an iterative segmentation
algorithm inspired by image rendering in computer graphics, a new image
segmentation method that can generate high-resolution segmentation masks. It
can also be flexibly integrated into common instance segmentation
meta-architecture such as Mask-RCNN and semantic meta-architecture such as FCN.
We implemented a model, termed as SEMI-PointRend, to generate precise
segmentation masks by applying the PointRend neural network module. In this
paper, we focus on comparing the defect segmentation predictions of
SEMI-PointRend and Mask-RCNN for various defect types (line-collapse, single
bridge, thin bridge, multi bridge non-horizontal). We show that SEMI-PointRend
can outperforms Mask R-CNN by up to 18.8% in terms of segmentation mean average
precision.",None,-1
2dc8554b-2c4e-47be-9785-50b959398a89,Document-level Relation Extraction with Cross-sentence Reasoning Graph,0.844489,7,"Relation extraction (RE) has recently moved from the sentence-level to
document-level, which requires aggregating document information and using
entities and mentions for reasoning. Existing works put entity nodes and
mention nodes with similar representations in a document-level graph, whose
complex edges may incur redundant information. Furthermore, existing studies
only focus on entity-level reasoning paths without considering global
interactions among entities cross-sentence. To these ends, we propose a novel
document-level RE model with a GRaph information Aggregation and Cross-sentence
Reasoning network (GRACR). Specifically, a simplified document-level graph is
constructed to model the semantic information of all mentions and sentences in
a document, and an entity-level graph is designed to explore relations of
long-distance cross-sentence entity pairs. Experimental results show that GRACR
achieves excellent performance on two public datasets of document-level RE. It
is especially effective in extracting potential relations of cross-sentence
entity pairs. Our code is available at https://github.com/UESTC-LHF/GRACR.",None,-1
ddb5bf0d-07cd-4041-89ae-cc5dbfd7442a,LM vs LM: Detecting Factual Errors via Cross Examination,0.834093,68,"A prominent weakness of modern language models (LMs) is their tendency to
generate factually incorrect text, which hinders their usability. A natural
question is whether such factual errors can be detected automatically. Inspired
by truth-seeking mechanisms in law, we propose a factuality evaluation
framework for LMs that is based on cross-examination. Our key idea is that an
incorrect claim is likely to result in inconsistency with other claims that the
model generates. To discover such inconsistencies, we facilitate a multi-turn
interaction between the LM that generated the claim and another LM (acting as
an examiner) which introduces questions to discover inconsistencies. We
empirically evaluate our method on factual claims made by multiple recent LMs
on four benchmarks, finding that it outperforms existing methods and baselines,
often by a large gap. Our results demonstrate the potential of using
interacting LMs for capturing factual errors.",None,-1
32ff8d3c-7a84-444d-9131-0b4784c52f31,Chordal Averaging on Flag Manifolds and Its Applications,0.268993,2,"This paper presents a new, provably-convergent algorithm for computing the
flag-mean and flag-median of a set of points on a flag manifold under the
chordal metric. The flag manifold is a mathematical space consisting of flags,
which are sequences of nested subspaces of a vector space that increase in
dimension. The flag manifold is a superset of a wide range of known matrix
spaces, including Stiefel and Grassmanians, making it a general object that is
useful in a wide variety computer vision problems.
  To tackle the challenge of computing first order flag statistics, we first
transform the problem into one that involves auxiliary variables constrained to
the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and
leveraging the numerical stability and efficiency of Stiefel-manifold
optimization enables us to compute the flag-mean effectively. Through a series
of experiments, we show the competence of our method in Grassmann and rotation
averaging, as well as principal component analysis. We release our source code
under https://github.com/nmank/FlagAveraging.",None,-1
c4fbe97b-2cb4-4010-9602-a2bb19e7bf10,GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction,0.979605,32,"Large Language Models (LLMs) combined with instruction tuning have made
significant progress when generalizing to unseen tasks. However, they have been
less successful in Information Extraction (IE), lagging behind task-specific
models. Typically, IE tasks are characterized by complex annotation guidelines
that describe the task and give examples to humans. Previous attempts to
leverage such information have failed, even with the largest models, as they
are not able to follow the guidelines out of the box. In this paper, we propose
GoLLIE (Guideline-following Large Language Model for IE), a model able to
improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to
comply with annotation guidelines. Comprehensive evaluation empirically
demonstrates that GoLLIE is able to generalize to and follow unseen guidelines,
outperforming previous attempts at zero-shot information extraction. The
ablation study shows that detailed guidelines are key for good results.",None,-1
49fbd787-6996-4da9-8092-465a4bf64799,Lawyer LLaMA Technical Report,0.930114,27,"Large Language Models (LLMs), like LLaMA, have exhibited remarkable
performance across various tasks. Nevertheless, when deployed to specific
domains such as law or medicine, the models still confront the challenge of a
deficiency in domain-specific knowledge and an inadequate capability to
leverage that knowledge to resolve domain-related problems. In this paper, we
propose a new framework to adapt LLMs to specific domains and build Lawyer
LLaMA, a legal domain LLM, based on this framework. Specifically, we inject
domain knowledge during the continual training stage and teach the model to
learn professional skills using properly designed supervised fine-tuning tasks.
Moreover, to alleviate the hallucination problem during the model's generation,
we add a retrieval module and extract relevant legal articles before the model
answers any queries. When learning domain-specific skills, we find that
experts' experience is much more useful than experiences distilled from
ChatGPT, where hundreds of expert-written data outperform tens of thousands of
ChatGPT-generated ones. We will release our model and data.",None,-1
75e4e5ad-6b92-42e6-a2b7-940e0ee79adb,CPopQA: Ranking Cultural Concept Popularity by LLMs,0.878456,2,"Prior work has demonstrated large language models' (LLMs) potential to
discern statistical tendencies within their pre-training corpora. Despite that,
many examinations of LLMs' knowledge capacity focus on knowledge explicitly
appearing in the training data or implicitly inferable from similar contexts.
How well an LLM captures the corpus-level statistical trends of concepts for
reasoning, especially long-tail ones, is still underexplored. In this study, we
introduce a novel few-shot question-answering task (CPopQA) that examines LLMs'
statistical ranking abilities for long-tail cultural concepts (e.g., holidays),
with a specific focus on these concepts' popularity in the United States and
the United Kingdom, respectively. We curate a dataset containing 459 holidays
across 58 countries, generating a total of 6,000 QA testing pairs. Experiments
on four strong LLMs show that large models are capable of ranking long-tail
cultural concepts regarding their statistical tendency. Notably, GPT-3.5
displayed superior performance and exhibited its potential to identify
geo-cultural proximity across continents.",None,-1
28b66d1c-b1e6-47f6-8d16-e9c5a5165404,Sentence-Incremental Neural Coreference Resolution,0.111925,3,"We propose a sentence-incremental neural coreference resolution system which
incrementally builds clusters after marking mention boundaries in a
shift-reduce method. The system is aimed at bridging two recent approaches at
coreference resolution: (1) state-of-the-art non-incremental models that incur
quadratic complexity in document length with high computational cost, and (2)
memory network-based models which operate incrementally but do not generalize
beyond pronouns. For comparison, we simulate an incremental setting by
constraining non-incremental systems to form partial coreference chains before
observing new sentences. In this setting, our system outperforms comparable
state-of-the-art methods by 2 F1 on OntoNotes and 7 F1 on the CODI-CRAC 2021
corpus. In a conventional coreference setup, our system achieves 76.3 F1 on
OntoNotes and 45.8 F1 on CODI-CRAC 2021, which is comparable to
state-of-the-art baselines. We also analyze variations of our system and show
that the degree of incrementality in the encoder has a surprisingly large
effect on the resulting performance.",None,-1
5c795e7d-8eeb-4e2c-b4ac-4a56413dd9b2,Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation,0.887438,17,"Semantic segmentation of point clouds in autonomous driving datasets requires
techniques that can process large numbers of points efficiently. Sparse 3D
convolutions have become the de-facto tools to construct deep neural networks
for this task: they exploit point cloud sparsity to reduce the memory and
computational loads and are at the core of today's best methods. In this paper,
we propose an alternative method that reaches the level of state-of-the-art
methods without requiring sparse convolutions. We actually show that such level
of performance is achievable by relying on tools a priori unfit for large scale
and high-performing 3D perception. In particular, we propose a novel 3D
backbone, WaffleIron, made almost exclusively of MLPs and dense 2D convolutions
and present how to train it to reach high performance on SemanticKITTI and
nuScenes. We believe that WaffleIron is a compelling alternative to backbones
using sparse 3D convolutions, especially in frameworks and on hardware where
those convolutions are not readily available.",None,-1
32cacd17-c6e2-456a-a815-c6fb105aae7a,TDG: Text-guided Domain Generalization,0.10226,1,"Domain generalization (DG) attempts to generalize a model trained on single
or multiple source domains to the unseen target domain. Benefiting from the
success of Visual-and-Language Pre-trained models in recent years, we argue
that it is crucial for domain generalization by introducing extra text
information. In this paper, we develop a novel Text-guided Domain
Generalization (TDG) paradigm for domain generalization, which includes three
following aspects. Specifically, we first devise an automatic words generation
method to extend the description of current domains with novel domain-relevant
words. Then, we embed the generated domain information into the text feature
space, by the proposed prompt learning-based text feature generation method,
which shares a common representation space with the image feature. Finally, we
utilize both input image features and generated text features to train a
specially designed classifier that generalizes well on unseen target domains,
while the image encoder is also updated under the supervision of gradients back
propagated from the classifier. Our experimental results show that the
techniques incorporated by TDG contribute to the performance in an easy
implementation manner. Experimental results on several domain generalization
benchmarks show that our proposed framework achieves superior performance by
effectively leveraging generated text information in domain generalization.",None,-1
923f709b-1a30-40db-8ba7-7a175367ffc7,No Free Lunch in Self Supervised Representation Learning,0.591619,8,"Self-supervised representation learning in computer vision relies heavily on
hand-crafted image transformations to learn meaningful and invariant features.
However few extensive explorations of the impact of transformation design have
been conducted in the literature. In particular, the dependence of downstream
performances to transformation design has been established, but not studied in
depth. In this work, we explore this relationship, its impact on a domain other
than natural images, and show that designing the transformations can be viewed
as a form of supervision. First, we demonstrate that not only do
transformations have an effect on downstream performance and relevance of
clustering, but also that each category in a supervised dataset can be impacted
in a different way. Following this, we explore the impact of transformation
design on microscopy images, a domain where the difference between classes is
more subtle and fuzzy than in natural images. In this case, we observe a
greater impact on downstream tasks performances. Finally, we demonstrate that
transformation design can be leveraged as a form of supervision, as careful
selection of these by a domain expert can lead to a drastic increase in
performance on a given downstream task.",None,-1
593bcb5f-43b1-48db-ab08-18bd05200d7a,DeCUR: decoupling common & unique representations for multimodal self-supervision,0.478857,8,"The increasing availability of multi-sensor data sparks interest in
multimodal self-supervised learning. However, most existing approaches learn
only common representations across modalities while ignoring intra-modal
training and modality-unique representations. We propose Decoupling Common and
Unique Representations (DeCUR), a simple yet effective method for multimodal
self-supervised learning. By distinguishing inter- and intra-modal embeddings,
DeCUR is trained to integrate complementary information across different
modalities. We evaluate DeCUR in three common multimodal scenarios
(radar-optical, RGB-elevation, and RGB-depth), and demonstrate its consistent
benefits on scene classification and semantic segmentation downstream tasks.
Notably, we get straightforward improvements by transferring our pretrained
backbones to state-of-the-art supervised multimodal methods without any
hyperparameter tuning. Furthermore, we conduct a comprehensive explainability
analysis to shed light on the interpretation of common and unique features in
our multimodal approach. Codes are available at
\url{https://github.com/zhu-xlab/DeCUR}.",None,-1
8ddf40e2-c414-4c65-9163-8da9b578ae82,Morphosyntactic probing of multilingual BERT models,0.352921,3,"We introduce an extensive dataset for multilingual probing of morphological
information in language models (247 tasks across 42 languages from 10
families), each consisting of a sentence with a target word and a morphological
tag as the desired label, derived from the Universal Dependencies treebanks. We
find that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features
that attain strong performance across these tasks. We then apply two methods to
locate, for each probing task, where the disambiguating information resides in
the input. The first is a new perturbation method that masks various parts of
context; the second is the classical method of Shapley values. The most
intriguing finding that emerges is a strong tendency for the preceding context
to hold more information relevant to the prediction than the following context.",None,-1
8afcbf8a-7209-4770-9fd6-8f3d17de9dc2,Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias,0.58271,5,"Large language models (LLMs) have demonstrated their potential in social
science research by emulating human perceptions and behaviors, a concept
referred to as algorithmic fidelity. This study assesses the algorithmic
fidelity and bias of LLMs by utilizing two nationally representative climate
change surveys. The LLMs were conditioned on demographics and/or psychological
covariates to simulate survey responses. The findings indicate that LLMs can
effectively capture presidential voting behaviors but encounter challenges in
accurately representing global warming perspectives when relevant covariates
are not included. GPT-4 exhibits improved performance when conditioned on both
demographics and covariates. However, disparities emerge in LLM estimations of
the views of certain groups, with LLMs tending to underestimate worry about
global warming among Black Americans. While highlighting the potential of LLMs
to aid social science research, these results underscore the importance of
meticulous conditioning, model selection, survey question format, and bias
assessment when employing LLMs for survey simulation. Further investigation
into prompt engineering and algorithm auditing is essential to harness the
power of LLMs while addressing their inherent limitations.",None,-1
1a673603-4af6-4dc3-90b4-12be0eb406db,User-friendly Image Editing with Minimal Text Input: Leveraging Captioning and Injection Techniques,0.112911,2,"Recent text-driven image editing in diffusion models has shown remarkable
success. However, the existing methods assume that the user's description
sufficiently grounds the contexts in the source image, such as objects,
background, style, and their relations. This assumption is unsuitable for
real-world applications because users have to manually engineer text prompts to
find optimal descriptions for different images. From the users' standpoint,
prompt engineering is a labor-intensive process, and users prefer to provide a
target word for editing instead of a full sentence. To address this problem, we
first demonstrate the importance of a detailed text description of the source
image, by dividing prompts into three categories based on the level of semantic
details. Then, we propose simple yet effective methods by combining prompt
generation frameworks, thereby making the prompt engineering process more
user-friendly. Extensive qualitative and quantitative experiments demonstrate
the importance of prompts in text-driven image editing and our method is
comparable to ground-truth prompts.",None,-1
b7cb77e7-46f6-4640-b4bd-f8cbc8046a19,SMPLitex: A Generative Model and Dataset for 3D Human Texture Estimation from Single Image,0.947175,9,"We propose SMPLitex, a method for estimating and manipulating the complete 3D
appearance of humans captured from a single image. SMPLitex builds upon the
recently proposed generative models for 2D images, and extends their use to the
3D domain through pixel-to-surface correspondences computed on the input image.
To this end, we first train a generative model for complete 3D human
appearance, and then fit it into the input image by conditioning the generative
model to the visible parts of the subject. Furthermore, we propose a new
dataset of high-quality human textures built by sampling SMPLitex conditioned
on subject descriptions and images. We quantitatively and qualitatively
evaluate our method in 3 publicly available datasets, demonstrating that
SMPLitex significantly outperforms existing methods for human texture
estimation while allowing for a wider variety of tasks such as editing,
synthesis, and manipulation",None,-1
15950dcd-87d4-4bf2-a463-c24096f7aed2,Guided Motion Diffusion for Controllable Human Motion Synthesis,0.929234,39,"Denoising diffusion models have shown great promise in human motion synthesis
conditioned on natural language descriptions. However, integrating spatial
constraints, such as pre-defined motion trajectories and obstacles, remains a
challenge despite being essential for bridging the gap between isolated human
motion and its surrounding environment. To address this issue, we propose
Guided Motion Diffusion (GMD), a method that incorporates spatial constraints
into the motion generation process. Specifically, we propose an effective
feature projection scheme that manipulates motion representation to enhance the
coherency between spatial information and local poses. Together with a new
imputation formulation, the generated motion can reliably conform to spatial
constraints such as global motion trajectories. Furthermore, given sparse
spatial constraints (e.g. sparse keyframes), we introduce a new dense guidance
approach to turn a sparse signal, which is susceptible to being ignored during
the reverse steps, into denser signals to guide the generated motion to the
given constraints. Our extensive experiments justify the development of GMD,
which achieves a significant improvement over state-of-the-art methods in
text-based motion generation while allowing control of the synthesized motions
with spatial constraints.",None,-1
ee590315-6e80-4667-84ce-0408eb2fec59,Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification,0.119139,3,"Intent classification (IC) plays an important role in task-oriented dialogue
systems. However, IC models often generalize poorly when training without
sufficient annotated examples for each user intent. We propose a novel
pre-training method for text encoders that uses contrastive learning with
intent psuedo-labels to produce embeddings that are well-suited for IC tasks,
reducing the need for manual annotations. By applying this pre-training
strategy, we also introduce Pre-trained Intent-aware Encoder (PIE), which is
designed to align encodings of utterances with their intent names.
Specifically, we first train a tagger to identify key phrases within utterances
that are crucial for interpreting intents. We then use these extracted phrases
to create examples for pre-training a text encoder in a contrastive manner. As
a result, our PIE model achieves up to 5.4% and 4.0% higher accuracy than the
previous state-of-the-art text encoder for the N-way zero- and one-shot
settings on four IC datasets.",None,-1
531115c0-7476-47f0-8ccb-5376b4164fa4,MMVP: Motion-Matrix-based Video Prediction,0.727268,6,"A central challenge of video prediction lies where the system has to reason
the objects' future motions from image frames while simultaneously maintaining
the consistency of their appearances across frames. This work introduces an
end-to-end trainable two-stream video prediction framework, Motion-Matrix-based
Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that
usually handle motion prediction and appearance maintenance within the same set
of modules, MMVP decouples motion and appearance information by constructing
appearance-agnostic motion matrices. The motion matrices represent the temporal
similarity of each and every pair of feature patches in the input frames, and
are the sole input of the motion prediction module in MMVP. This design
improves video prediction in both accuracy and efficiency, and reduces the
model size. Results of extensive experiments demonstrate that MMVP outperforms
state-of-the-art systems on public data sets by non-negligible large margins
(about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the
size or smaller).",None,-1
b8fd8d10-52f1-4ef6-bec3-65af02b84828,Dense RGB SLAM with Neural Implicit Maps,0.995579,30,"There is an emerging trend of using neural implicit functions for map
representation in Simultaneous Localization and Mapping (SLAM). Some pioneer
works have achieved encouraging results on RGB-D SLAM. In this paper, we
present a dense RGB SLAM method with neural implicit map representation. To
reach this challenging goal without depth input, we introduce a hierarchical
feature volume to facilitate the implicit map decoder. This design effectively
fuses shape cues across different scales to facilitate map reconstruction. Our
method simultaneously solves the camera motion and the neural implicit map by
matching the rendered and input video frames. To facilitate optimization, we
further propose a photometric warping loss in the spirit of multi-view stereo
to better constrain the camera pose and scene geometry. We evaluate our method
on commonly used benchmarks and compare it with modern RGB and RGB-D SLAM
systems. Our method achieves favorable results than previous methods and even
surpasses some recent RGB-D SLAM methods.The code is at
poptree.github.io/DIM-SLAM/.",None,-1
079e7ebd-9ddc-4424-abbc-db6de5bb5ead,Investigating Reinforcement Learning for Communication Strategies in a Task-Initiative Setting,0.0384124,1,"Many conversational domains require the system to present nuanced information
to users. Such systems must follow up what they say to address clarification
questions and repair misunderstandings. In this work, we explore this
interactive strategy in a referential communication task. Using simulation, we
analyze the communication trade-offs between initial presentation and
subsequent followup as a function of user clarification strategy, and compare
the performance of several baseline strategies to policies derived by
reinforcement learning. We find surprising advantages to coherence-based
representations of dialogue strategy, which bring minimal data requirements,
explainable choices, and strong audit capabilities, but incur little loss in
predicted outcomes across a wide range of user models.",None,-1
a64d2b9e-f40c-46f1-8a30-80fa72c93a51,Towards LLM-driven Dialogue State Tracking,0.467049,4,"Dialogue State Tracking (DST) is of paramount importance in ensuring accurate
tracking of user goals and system actions within task-oriented dialogue
systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT
has sparked considerable interest in assessing their efficacy across diverse
applications. In this study, we conduct an initial examination of ChatGPT's
capabilities in DST. Our evaluation uncovers the exceptional performance of
ChatGPT in this task, offering valuable insights to researchers regarding its
capabilities and providing useful directions for designing and enhancing
dialogue systems. Despite its impressive performance, ChatGPT has significant
limitations including its closed-source nature, request restrictions, raising
data privacy concerns, and lacking local deployment capabilities. To address
these concerns, we present LDST, an LLM-driven DST framework based on smaller,
open-source foundation models. By utilizing a novel domain-slot instruction
tuning method, LDST achieves performance on par with ChatGPT. Comprehensive
evaluations across three distinct experimental settings, we find that LDST
exhibits remarkable performance improvements in both zero-shot and few-shot
setting compared to previous SOTA methods. The source code is provided for
reproducibility.",None,-1
804da491-a25a-4c63-a98c-4da0d6f5d171,BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks,0.125846,2,"The MineRL BASALT competition has served to catalyze advances in learning
from human feedback through four hard-to-specify tasks in Minecraft, such as
create and photograph a waterfall. Given the completion of two years of BASALT
competitions, we offer to the community a formalized benchmark through the
BASALT Evaluation and Demonstrations Dataset (BEDD), which serves as a resource
for algorithm development and performance assessment. BEDD consists of a
collection of 26 million image-action pairs from nearly 14,000 videos of human
players completing the BASALT tasks in Minecraft. It also includes over 3,000
dense pairwise human evaluations of human and algorithmic agents. These
comparisons serve as a fixed, preliminary leaderboard for evaluating
newly-developed algorithms. To enable this comparison, we present a streamlined
codebase for benchmarking new algorithms against the leaderboard. In addition
to presenting these datasets, we conduct a detailed analysis of the data from
both datasets to guide algorithm development and evaluation. The released code
and data are available at https://github.com/minerllabs/basalt-benchmark .",None,-1
1981aa55-6454-4810-aff0-075c0b442bb2,Decidable Fragments of LTLf Modulo Theories (Extended Version),0.819908,2,"We study Linear Temporal Logic Modulo Theories over Finite Traces (LTLfMT), a
recently introduced extension of LTL over finite traces (LTLf) where
propositions are replaced by first-order formulas and where first-order
variables referring to different time points can be compared. In general,
LTLfMT was shown to be semi-decidable for any decidable first-order theory
(e.g., linear arithmetics), with a tableau-based semi-decision procedure.
  In this paper we present a sound and complete pruning rule for the LTLfMT
tableau. We show that for any LTLfMT formula that satisfies an abstract,
semantic condition, that we call finite memory, the tableau augmented with the
new rule is also guaranteed to terminate. Last but not least, this technique
allows us to establish novel decidability results for the satisfiability of
several fragments of LTLfMT, as well as to give new decidability proofs for
classes that are already known.",None,-1
1956c161-f482-4c2f-9ffa-be3b3fc13740,TransFool: An Adversarial Attack against Neural Machine Translation Models,0.687018,8,"Deep neural networks have been shown to be vulnerable to small perturbations
of their inputs, known as adversarial attacks. In this paper, we investigate
the vulnerability of Neural Machine Translation (NMT) models to adversarial
attacks and propose a new attack algorithm called TransFool. To fool NMT
models, TransFool builds on a multi-term optimization problem and a gradient
projection step. By integrating the embedding representation of a language
model, we generate fluent adversarial examples in the source language that
maintain a high level of semantic similarity with the clean samples.
Experimental results demonstrate that, for different translation tasks and NMT
architectures, our white-box attack can severely degrade the translation
quality while the semantic similarity between the original and the adversarial
sentences stays high. Moreover, we show that TransFool is transferable to
unknown target models. Finally, based on automatic and human evaluations,
TransFool leads to improvement in terms of success rate, semantic similarity,
and fluency compared to the existing attacks both in white-box and black-box
settings. Thus, TransFool permits us to better characterize the vulnerability
of NMT models and outlines the necessity to design strong defense mechanisms
and more robust NMT systems for real-life applications.",None,-1
4cc29728-94d9-41d0-aa6b-bf89c335b4d2,Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding,0.494303,3,"Negative sampling (NS) is widely used in knowledge graph embedding (KGE),
which aims to generate negative triples to make a positive-negative contrast
during training. However, existing NS methods are unsuitable when multi-modal
information is considered in KGE models. They are also inefficient due to their
complex design. In this paper, we propose Modality-Aware Negative Sampling
(MANS) for multi-modal knowledge graph embedding (MMKGE) to address the
mentioned problems. MANS could align structural and visual embeddings for
entities in KGs and learn meaningful embeddings to perform better in
multi-modal KGE while keeping lightweight and efficient. Empirical results on
two benchmarks demonstrate that MANS outperforms existing NS methods.
Meanwhile, we make further explorations about MANS to confirm its
effectiveness.",None,-1
ec004937-7c47-43d7-93cd-603736092ae1,Prediction-Powered Inference,0.871716,35,"Prediction-powered inference is a framework for performing valid statistical
inference when an experimental dataset is supplemented with predictions from a
machine-learning system. The framework yields simple algorithms for computing
provably valid confidence intervals for quantities such as means, quantiles,
and linear and logistic regression coefficients, without making any assumptions
on the machine-learning algorithm that supplies the predictions. Furthermore,
more accurate predictions translate to smaller confidence intervals.
Prediction-powered inference could enable researchers to draw valid and more
data-efficient conclusions using machine learning. The benefits of
prediction-powered inference are demonstrated with datasets from proteomics,
astronomy, genomics, remote sensing, census analysis, and ecology.",None,-1
63c8c4f8-f25d-4969-aadb-d98939251abc,VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style Transfer,0.783639,4,"Current talking face generation methods mainly focus on speech-lip
synchronization. However, insufficient investigation on the facial talking
style leads to a lifeless and monotonous avatar. Most previous works fail to
imitate expressive styles from arbitrary video prompts and ensure the
authenticity of the generated video. This paper proposes an unsupervised
variational style transfer model (VAST) to vivify the neutral photo-realistic
avatars. Our model consists of three key components: a style encoder that
extracts facial style representations from the given video prompts; a hybrid
facial expression decoder to model accurate speech-related movements; a
variational style enhancer that enhances the style space to be highly
expressive and meaningful. With our essential designs on facial style learning,
our model is able to flexibly capture the expressive facial style from
arbitrary video prompts and transfer it onto a personalized image renderer in a
zero-shot manner. Experimental results demonstrate the proposed approach
contributes to a more vivid talking avatar with higher authenticity and richer
expressiveness.",None,-1
29f809e8-6d63-4b55-bbab-593119111299,Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning,0.577294,3,"Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph
(KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial
task that aims to predict future facts based on historical occurrences. The key
challenge lies in uncovering structural dependencies within historical
subgraphs and temporal patterns. Most existing approaches model TKGs relying on
entity modeling, as nodes in the graph play a crucial role in knowledge
representation. However, the real-world scenario often involves an extensive
number of entities, with new entities emerging over time. This makes it
challenging for entity-dependent methods to cope with extensive volumes of
entities, and effectively handling newly emerging entities also becomes a
significant challenge. Therefore, we propose Temporal Inductive Path Neural
Network (TiPNN), which models historical information in an entity-independent
perspective. Specifically, TiPNN adopts a unified graph, namely history
temporal graph, to comprehensively capture and encapsulate information from
history. Subsequently, we utilize the defined query-aware temporal paths on a
history temporal graph to model historical path information related to queries
for reasoning. Extensive experiments illustrate that the proposed model not
only attains significant performance enhancements but also handles inductive
settings, while additionally facilitating the provision of reasoning evidence
through history temporal graphs.",None,-1
84857be1-76d6-4706-8830-78436965bbdc,Positional Diffusion: Ordering Unordered Sets with Diffusion Probabilistic Models,0.0542967,3,"Positional reasoning is the process of ordering unsorted parts contained in a
set into a consistent structure. We present Positional Diffusion, a
plug-and-play graph formulation with Diffusion Probabilistic Models to address
positional reasoning. We use the forward process to map elements' positions in
a set to random positions in a continuous space. Positional Diffusion learns to
reverse the noising process and recover the original positions through an
Attention-based Graph Neural Network. We conduct extensive experiments with
benchmark datasets including two puzzle datasets, three sentence ordering
datasets, and one visual storytelling dataset, demonstrating that our method
outperforms long-lasting research on puzzle solving with up to +18% compared to
the second-best deep learning method, and performs on par against the
state-of-the-art methods on sentence ordering and visual storytelling. Our work
highlights the suitability of diffusion models for ordering problems and
proposes a novel formulation and method for solving various ordering tasks.
Project website at https://iit-pavis.github.io/Positional_Diffusion/",None,-1
5c35a529-4e80-43e1-a353-8c50c32490ff,From Database Repairs to Causality in Databases and Beyond,0.705425,2,"We describe some recent approaches to score-based explanations for query
answers in databases. The focus is on work done by the author and
collaborators. Special emphasis is placed on the use of counterfactual
reasoning for score specification and computation. Several examples that
illustrate the flexibility of these methods are shown.",None,-1
09f09e77-98d3-4b38-82f4-cae9e0b45d73,Soda: An Object-Oriented Functional Language for Specifying Human-Centered Problems,0.809944,1,"We present Soda (Symbolic Objective Descriptive Analysis), a language that
helps to treat qualities and quantities in a natural way and greatly simplifies
the task of checking their correctness. We present key properties for the
language motivated by the design of a descriptive language to encode complex
requirements on computer systems, and we explain how these key properties must
be addressed to model these requirements with simple definitions. We give an
overview of a tool that helps to describe problems in an easy way that we
consider more transparent and less error-prone.",None,-1
88828c74-c780-4c36-a424-4373939c3f2e,VTQA: Visual Text Question Answering via Entity Alignment and Cross-Media Reasoning,0.165268,4,"The ideal form of Visual Question Answering requires understanding, grounding
and reasoning in the joint space of vision and language and serves as a proxy
for the AI task of scene understanding. However, most existing VQA benchmarks
are limited to just picking the answer from a pre-defined set of options and
lack attention to text. We present a new challenge with a dataset that contains
23,781 questions based on 10124 image-text pairs. Specifically, the task
requires the model to align multimedia representations of the same entity to
implement multi-hop reasoning between image and text and finally use natural
language to answer the question. The aim of this challenge is to develop and
benchmark models that are capable of multimedia entity alignment, multi-step
reasoning and open-ended answer generation.",None,-1
29fd3eb6-23be-4bcd-8b28-1836678f8e3a,Multimodal Adaptive Fusion of Face and Gait Features using Keyless attention based Deep Neural Networks for Human Identification,0.0788032,1,"Biometrics plays a significant role in vision-based surveillance
applications. Soft biometrics such as gait is widely used with face in
surveillance tasks like person recognition and re-identification. Nevertheless,
in practical scenarios, classical fusion techniques respond poorly to changes
in individual users and in the external environment. To this end, we propose a
novel adaptive multi-biometric fusion strategy for the dynamic incorporation of
gait and face biometric cues by leveraging keyless attention deep neural
networks. Various external factors such as viewpoint and distance to the
camera, are investigated in this study. Extensive experiments have shown
superior performanceof the proposed model compared with the state-of-the-art
model.",None,-1
8d463b71-f4d3-4d54-af9b-3ac709c1dae7,"Zoom-VQA: Patches, Frames and Clips Integration for Video Quality Assessment",0.470407,10,"Video quality assessment (VQA) aims to simulate the human perception of video
quality, which is influenced by factors ranging from low-level color and
texture details to high-level semantic content. To effectively model these
complicated quality-related factors, in this paper, we decompose video into
three levels (\ie, patch level, frame level, and clip level), and propose a
novel Zoom-VQA architecture to perceive spatio-temporal features at different
levels. It integrates three components: patch attention module, frame pyramid
alignment, and clip ensemble strategy, respectively for capturing
region-of-interest in the spatial dimension, multi-level information at
different feature levels, and distortions distributed over the temporal
dimension. Owing to the comprehensive design, Zoom-VQA obtains state-of-the-art
results on four VQA benchmarks and achieves 2nd place in the NTIRE 2023 VQA
challenge. Notably, Zoom-VQA has outperformed the previous best results on two
subsets of LSVQ, achieving 0.8860 (+1.0%) and 0.7985 (+1.9%) of SRCC on the
respective subsets. Adequate ablation studies further verify the effectiveness
of each component. Codes and models are released in
https://github.com/k-zha14/Zoom-VQA.",None,-1
3d655189-51ef-45ca-b036-3b6588b9113b,Multi-Agent Consensus Seeking via Large Language Models,0.831454,8,"Multi-agent systems driven by large language models (LLMs) have shown
promising abilities for solving complex tasks in a collaborative manner. This
work considers a fundamental problem in multi-agent collaboration: consensus
seeking. When multiple agents work together, we are interested in how they can
reach a consensus through inter-agent negotiation. To that end, this work
studies a consensus-seeking task where the state of each agent is a numerical
value and they negotiate with each other to reach a consensus value. It is
revealed that when not explicitly directed on which strategy should be adopted,
the LLM-driven agents primarily use the average strategy for consensus seeking
although they may occasionally use some other strategies. Moreover, this work
analyzes the impact of the agent number, agent personality, and network
topology on the negotiation process. The findings reported in this work can
potentially lay the foundations for understanding the behaviors of LLM-driven
multi-agent systems for solving more complex tasks. Furthermore, LLM-driven
consensus seeking is applied to a multi-robot aggregation task. This
application demonstrates the potential of LLM-driven agents to achieve
zero-shot autonomous planning for multi-robot collaboration tasks. Project
website: westlakeintelligentrobotics.github.io/ConsensusLLM/.",None,-1
c20c54c2-009c-40fa-b01f-44f56f442e18,Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention,0.542167,5,"Traditional multi-agent reinforcement learning algorithms are difficultly
applied in a large-scale multi-agent environment. The introduction of mean
field theory has enhanced the scalability of multi-agent reinforcement learning
in recent years. This paper considers partially observable multi-agent
reinforcement learning (MARL), where each agent can only observe other agents
within a fixed range. This partial observability affects the agent's ability to
assess the quality of the actions of surrounding agents. This paper focuses on
developing a method to capture more effective information from local
observations in order to select more effective actions. Previous work in this
field employs probability distributions or weighted mean field to update the
average actions of neighborhood agents, but it does not fully consider the
feature information of surrounding neighbors and leads to a local optimum. In
this paper, we propose a novel multi-agent reinforcement learning algorithm,
Partially Observable Mean Field Multi-Agent Reinforcement Learning based on
Graph--Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention
module and a mean field module to describe how an agent is influenced by the
actions of other agents at each time step. This graph attention module consists
of a graph attention encoder and a differentiable attention mechanism, and this
mechanism outputs a dynamic graph to represent the effectiveness of
neighborhood agents against central agents. The mean--field module approximates
the effect of a neighborhood agent on a central agent as the average effect of
effective neighborhood agents. We evaluate GAMFQ on three challenging tasks in
the MAgents framework. Experiments show that GAMFQ outperforms baselines
including the state-of-the-art partially observable mean-field reinforcement
learning algorithms.",None,-1
e6091415-94d8-4dab-8ed9-d075b3a6dbcb,T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation,0.410935,19,"Warning: This paper contains several contents that may be toxic, harmful, or
offensive.
  In the last few years, text-to-image generative models have gained remarkable
success in generating images with unprecedented quality accompanied by a
breakthrough of inference speed. Despite their rapid progress, human biases
that manifest in the training examples, particularly with regard to common
stereotypical biases, like gender and skin tone, still have been found in these
generative models. In this work, we seek to measure more complex human biases
exist in the task of text-to-image generations. Inspired by the well-known
Implicit Association Test (IAT) from social psychology, we propose a novel
Text-to-Image Association Test (T2IAT) framework that quantifies the implicit
stereotypes between concepts and valence, and those in the images. We replicate
the previously documented bias tests on generative models, including morally
neutral tests on flowers and insects as well as demographic stereotypical tests
on diverse social attributes. The results of these experiments demonstrate the
presence of complex stereotypical behaviors in image generations.",None,-1
ee779876-a6bb-4691-a117-02eba5d21a8f,Enhancing Your Trained DETRs with Box Refinement,0.0460364,1,"We present a conceptually simple, efficient, and general framework for
localization problems in DETR-like models. We add plugins to well-trained
models instead of inefficiently designing new models and training them from
scratch. The method, called RefineBox, refines the outputs of DETR-like
detectors by lightweight refinement networks. RefineBox is easy to implement
and train as it only leverages the features and predicted boxes from the
well-trained detection models. Our method is also efficient as we freeze the
trained detectors during training. In addition, we can easily generalize
RefineBox to various trained detection models without any modification. We
conduct experiments on COCO and LVIS $1.0$. Experimental results indicate the
effectiveness of our RefineBox for DETR and its representative variants (Figure
1). For example, the performance gains for DETR, Conditinal-DETR, DAB-DETR, and
DN-DETR are 2.4 AP, 2.5 AP, 1.9 AP, and 1.6 AP, respectively. We hope our work
will bring the attention of the detection community to the localization
bottleneck of current DETR-like models and highlight the potential of the
RefineBox framework. Code and models will be publicly available at:
\href{https://github.com/YiqunChen1999/RefineBox}{https://github.com/YiqunChen1999/RefineBox}.",None,-1
1e6c5dca-7bb0-4a92-a958-d3a72aeb617d,Let's Get Personal: Personal Questions Improve SocialBot Performance in the Alexa Prize,0.194052,2,"There has been an increased focus on creating conversational open-domain
dialogue systems in the spoken dialogue community. Unlike traditional dialogue
systems, these conversational systems cannot assume any specific information
need or domain restrictions, i.e., the only inherent goal is to converse with
the user on an unknown set of topics. While massive improvements in Natural
Language Understanding (NLU) and the growth of available knowledge resources
can partially support a robust conversation, these conversations generally lack
the rapport between two humans that know each other. We developed a robust
open-domain conversational system, Athena, that real Amazon Echo users access
and evaluate at scale in the context of the Alexa Prize competition. We
experiment with methods intended to increase intimacy between Athena and the
user by heuristically developing a rule-based user model that personalizes both
the current and subsequent conversations and evaluating specific personal
opinion question strategies in A/B studies. Our results show a statistically
significant positive impact on perceived conversation quality and length when
employing these strategies.",None,-1
8d42270f-4733-46f8-b381-35634817842d,Controllable Guide-Space for Generalizable Face Forgery Detection,0.813225,9,"Recent studies on face forgery detection have shown satisfactory performance
for methods involved in training datasets, but are not ideal enough for unknown
domains. This motivates many works to improve the generalization, but
forgery-irrelevant information, such as image background and identity, still
exists in different domain features and causes unexpected clustering, limiting
the generalization. In this paper, we propose a controllable guide-space (GS)
method to enhance the discrimination of different forgery domains, so as to
increase the forgery relevance of features and thereby improve the
generalization. The well-designed guide-space can simultaneously achieve both
the proper separation of forgery domains and the large distance between
real-forgery domains in an explicit and controllable manner. Moreover, for
better discrimination, we use a decoupling module to weaken the interference of
forgery-irrelevant correlations between domains. Furthermore, we make
adjustments to the decision boundary manifold according to the clustering
degree of the same domain features within the neighborhood. Extensive
experiments in multiple in-domain and cross-domain settings confirm that our
method can achieve state-of-the-art generalization.",None,-1
40c6b9f7-5a96-4218-8a94-4b90654d7525,Focus on Your Target: A Dual Teacher-Student Framework for Domain-adaptive Semantic Segmentation,0.2125,2,"We study unsupervised domain adaptation (UDA) for semantic segmentation.
Currently, a popular UDA framework lies in self-training which endows the model
with two-fold abilities: (i) learning reliable semantics from the labeled
images in the source domain, and (ii) adapting to the target domain via
generating pseudo labels on the unlabeled images. We find that, by
decreasing/increasing the proportion of training samples from the target
domain, the 'learning ability' is strengthened/weakened while the 'adapting
ability' goes in the opposite direction, implying a conflict between these two
abilities, especially for a single model. To alleviate the issue, we propose a
novel dual teacher-student (DTS) framework and equip it with a bidirectional
learning strategy. By increasing the proportion of target-domain data, the
second teacher-student model learns to 'Focus on Your Target' while the first
model is not affected. DTS is easily plugged into existing self-training
approaches. In a standard UDA scenario (training on synthetic, labeled data and
real, unlabeled data), DTS shows consistent gains over the baselines and sets
new state-of-the-art results of 76.5\% and 75.1\% mIoUs on
GTAv$\rightarrow$Cityscapes and SYNTHIA$\rightarrow$Cityscapes, respectively.",None,-1
fcd7bb2e-a34c-48d6-9c2d-df074fd61d2e,A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech,0.973373,21,"Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have
achieved near human-level naturalness. The diversity of human speech, however,
often goes beyond the coverage of these corpora. We believe the ability to
handle such diversity is crucial for AI systems to achieve human-level
communication. Our work explores the use of more abundant real-world data for
building speech synthesizers. We train TTS systems using real-world speech from
YouTube and podcasts. We observe the mismatch between training and inference
alignments in mel-spectrogram based autoregressive models, leading to
unintelligible synthesis, and demonstrate that learned discrete codes within
multiple code groups effectively resolves this issue. We introduce our MQTTS
system whose architecture is designed for multiple code generation and
monotonic alignment, along with the use of a clean silence prompt to improve
synthesis quality. We conduct ablation analyses to identify the efficacy of our
methods. We show that MQTTS outperforms existing TTS systems in several
objective and subjective measures.",None,-1
1138ef0d-182c-4abb-b8c9-9b1dc507d0ba,"Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation",0.372346,7,"We show that LLMs hallucinate because their output is not constrained to be
synonymous with claims for which they have evidence: a condition that we call
evidential closure. Information about the truth or falsity of sentences is not
statistically identified in the standard neural probabilistic language model
setup, and so cannot be conditioned on to generate new strings. We then show
how to constrain LLMs to produce output that does satisfy evidential closure. A
multimodal LLM must learn about the external world (perceptual learning); it
must learn a mapping from strings to states of the world (extensional
learning); and, to achieve fluency when generalizing beyond a body of evidence,
it must learn mappings from strings to their synonyms (intensional learning).
The output of a unimodal LLM must be synonymous with strings in a validated
evidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune,
that yields faithful output from an LLM by rejecting output that is not
synonymous with claims for which the LLM has evidence.",None,-1
7d3dcc83-f134-4095-8083-de722a581753,HeGeL: A Novel Dataset for Geo-Location from Hebrew Text,0.620955,3,"The task of textual geolocation - retrieving the coordinates of a place based
on a free-form language description - calls for not only grounding but also
natural language understanding and geospatial reasoning. Even though there are
quite a few datasets in English used for geolocation, they are currently based
on open-source data (Wikipedia and Twitter), where the location of the
described place is mostly implicit, such that the location retrieval resolution
is limited. Furthermore, there are no datasets available for addressing the
problem of textual geolocation in morphologically rich and resource-poor
languages, such as Hebrew. In this paper, we present the Hebrew Geo-Location
(HeGeL) corpus, designed to collect literal place descriptions and analyze
lingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place
descriptions of various place types in three cities in Israel. Qualitative and
empirical analysis show that the data exhibits abundant use of geospatial
reasoning and requires a novel environmental representation.",None,-1
de95f5a7-d7cf-4fa3-9745-949292701b67,ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation,0.558938,14,"Compositional generalization benchmarks for semantic parsing seek to assess
whether models can accurately compute meanings for novel sentences, but
operationalize this in terms of logical form (LF) prediction. This raises the
concern that semantically irrelevant details of the chosen LFs could shape
model performance. We argue that this concern is realized for the COGS
benchmark. COGS poses generalization splits that appear impossible for
present-day models, which could be taken as an indictment of those models.
However, we show that the negative results trace to incidental features of COGS
LFs. Converting these LFs to semantically equivalent ones and factoring out
capabilities unrelated to semantic interpretation, we find that even baseline
models get traction. A recent variable-free translation of COGS LFs suggests
similar conclusions, but we observe this format is not semantically equivalent;
it is incapable of accurately representing some COGS meanings. These findings
inform our proposal for ReCOGS, a modified version of COGS that comes closer to
assessing the target semantic capabilities while remaining very challenging.
Overall, our results reaffirm the importance of compositional generalization
and careful benchmark task design.",None,-1
09d9bce3-aa7d-4b7d-a0e4-b9dec84b8d96,Improving Autonomous Separation Assurance through Distributed Reinforcement Learning with Attention Networks,0.68986,2,"Advanced Air Mobility (AAM) introduces a new, efficient mode of
transportation with the use of vehicle autonomy and electrified aircraft to
provide increasingly autonomous transportation between previously underserved
markets. Safe and efficient navigation of low altitude aircraft through highly
dense environments requires the integration of a multitude of complex
observations, such as surveillance, knowledge of vehicle dynamics, and weather.
The processing and reasoning on these observations pose challenges due to the
various sources of uncertainty in the information while ensuring cooperation
with a variable number of aircraft in the airspace. These challenges coupled
with the requirement to make safety-critical decisions in real-time rule out
the use of conventional separation assurance techniques. We present a
decentralized reinforcement learning framework to provide autonomous
self-separation capabilities within AAM corridors with the use of speed and
vertical maneuvers. The problem is formulated as a Markov Decision Process and
solved by developing a novel extension to the sample-efficient, off-policy soft
actor-critic (SAC) algorithm. We introduce the use of attention networks for
variable-length observation processing and a distributed computing architecture
to achieve high training sample throughput as compared to existing approaches.
A comprehensive numerical study shows that the proposed framework can ensure
safe and efficient separation of aircraft in high density, dynamic environments
with various sources of uncertainty.",None,-1
96ab2208-18fa-4e58-9134-97a5884b4c85,"(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions",0.671933,1,"The concept of rationality is central to the field of artificial
intelligence. Whether we are seeking to simulate human reasoning, or the goal
is to achieve bounded optimality, we generally seek to make artificial agents
as rational as possible. Despite the centrality of the concept within AI, there
is no unified definition of what constitutes a rational agent. This article
provides a survey of rationality and irrationality in artificial intelligence,
and sets out the open questions in this area. The understanding of rationality
in other fields has influenced its conception within artificial intelligence,
in particular work in economics, philosophy and psychology. Focusing on the
behaviour of artificial agents, we consider irrational behaviours that can
prove to be optimal in certain scenarios. Some methods have been developed to
deal with irrational agents, both in terms of identification and interaction,
however work in this area remains limited. Methods that have up to now been
developed for other purposes, namely adversarial scenarios, may be adapted to
suit interactions with artificial agents. We further discuss the interplay
between human and artificial agents, and the role that rationality plays within
this interaction; many questions remain in this area, relating to potentially
irrational behaviour of both humans and artificial agents.",None,-1
50070c78-5b7b-4897-b0b1-30641a5debbc,Process Mining for Unstructured Data: Challenges and Research Directions,0.447108,2,"The application of process mining for unstructured data might significantly
elevate novel insights into disciplines where unstructured data is a common
data format. To efficiently analyze unstructured data by process mining and to
convey confidence into the analysis result, requires bridging multiple
challenges. The purpose of this paper is to discuss these challenges, present
initial solutions and describe future research directions. We hope that this
article lays the foundations for future collaboration on this topic.",None,-1
23366f0e-5358-4475-935f-7a0bb0baeed2,Presenting Multiagent Challenges in Team Sports Analytics,0.753084,3,"This paper draws correlations between several challenges and opportunities
within the area of team sports analytics and key research areas within
multiagent systems (MAS). We specifically consider invasion games, defined as
sports where players invade the opposing team's territory and can interact
anywhere on a playing surface such as ice hockey, soccer, and basketball. We
argue that MAS is well-equipped to study invasion games and will benefit both
MAS and sports analytics fields. Our discussion highlights areas for MAS
implementation and further development along two axes: short-term in-game
strategy (coaching) and long-term team planning (management).",None,-1
93021b00-d7aa-4e62-9281-e681f544cdb1,Nationality Bias in Text Generation,0.384487,29,"Little attention is placed on analyzing nationality bias in language models,
especially when nationality is highly used as a factor in increasing the
performance of social NLP models. This paper examines how a text generation
model, GPT-2, accentuates pre-existing societal biases about country-based
demonyms. We generate stories using GPT-2 for various nationalities and use
sensitivity analysis to explore how the number of internet users and the
country's economic status impacts the sentiment of the stories. To reduce the
propagation of biases through large language models (LLM), we explore the
debiasing method of adversarial triggering. Our results show that GPT-2
demonstrates significant bias against countries with lower internet users, and
adversarial triggering effectively reduces the same.",None,-1
c0fda8e9-4c2b-4b56-9506-1b08e17089d8,Randomized Adversarial Style Perturbations for Domain Generalization,0.0796691,1,"We propose a novel domain generalization technique, referred to as Randomized
Adversarial Style Perturbation (RASP), which is motivated by the observation
that the characteristics of each domain are captured by the feature statistics
corresponding to style. The proposed algorithm perturbs the style of a feature
in an adversarial direction towards a randomly selected class, and makes the
model learn against being misled by the unexpected styles observed in unseen
target domains. While RASP is effective to handle domain shifts, its naive
integration into the training procedure might degrade the capability of
learning knowledge from source domains because it has no restriction on the
perturbations of representations. This challenge is alleviated by Normalized
Feature Mixup (NFM), which facilitates the learning of the original features
while achieving robustness to perturbed representations via their mixup during
training. We evaluate the proposed algorithm via extensive experiments on
various benchmarks and show that our approach improves domain generalization
performance, especially in large-scale benchmarks.",None,-1
a4faa130-bcad-41ec-8ac0-33bc99ea3a59,1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track,0.0562673,2,"OOD-CV challenge is an out-of-distribution generalization task. To solve this
problem in object detection track, we propose a simple yet effective
Generalize-then-Adapt (G&A) framework, which is composed of a two-stage domain
generalization part and a one-stage domain adaptation part. The domain
generalization part is implemented by a Supervised Model Pretraining stage
using source data for model warm-up and a Weakly Semi-Supervised Model
Pretraining stage using both source data with box-level label and auxiliary
data (ImageNet-1K) with image-level label for performance boosting. The domain
adaptation part is implemented as a Source-Free Domain Adaptation paradigm,
which only uses the pre-trained model and the unlabeled target data to further
optimize in a self-supervised training manner. The proposed G&A framework help
us achieve the first place on the object detection leaderboard of the OOD-CV
challenge. Code will be released in
https://github.com/hikvision-research/OOD-CV.",None,-1
9e1c0ac9-9f07-480c-8104-1a4ee6265b74,Fully Bayesian VIB-DeepSSM,0.310853,8,"Statistical shape modeling (SSM) enables population-based quantitative
analysis of anatomical shapes, informing clinical diagnosis. Deep learning
approaches predict correspondence-based SSM directly from unsegmented 3D images
but require calibrated uncertainty quantification, motivating Bayesian
formulations. Variational information bottleneck DeepSSM (VIB-DeepSSM) is an
effective, principled framework for predicting probabilistic shapes of anatomy
from images with aleatoric uncertainty quantification. However, VIB is only
half-Bayesian and lacks epistemic uncertainty inference. We derive a fully
Bayesian VIB formulation and demonstrate the efficacy of two scalable
implementation approaches: concrete dropout and batch ensemble. Additionally,
we introduce a novel combination of the two that further enhances uncertainty
calibration via multimodal marginalization. Experiments on synthetic shapes and
left atrium data demonstrate that the fully Bayesian VIB network predicts SSM
from images with improved uncertainty reasoning without sacrificing accuracy.",None,-1
1504d478-eb69-40bd-9399-f478917e91d1,Ensemble of Counterfactual Explainers,0.614329,7,"In eXplainable Artificial Intelligence (XAI), several counterfactual
explainers have been proposed, each focusing on some desirable properties of
counterfactual instances: minimality, actionability, stability, diversity,
plausibility, discriminative power. We propose an ensemble of counterfactual
explainers that boosts weak explainers, which provide only a subset of such
properties, to a powerful method covering all of them. The ensemble runs weak
explainers on a sample of instances and of features, and it combines their
results by exploiting a diversity-driven selection function. The method is
model-agnostic and, through a wrapping approach based on autoencoders, it is
also data-agnostic.",None,-1
77ac5eea-9e69-4dfe-9242-89677c4641c2,HeightFormer: Explicit Height Modeling without Extra Data for Camera-only 3D Object Detection in Bird's Eye View,0.514041,4,"Vision-based Bird's Eye View (BEV) representation is an emerging perception
formulation for autonomous driving. The core challenge is to construct BEV
space with multi-camera features, which is a one-to-many ill-posed problem.
Diving into all previous BEV representation generation methods, we found that
most of them fall into two types: modeling depths in image views or modeling
heights in the BEV space, mostly in an implicit way. In this work, we propose
to explicitly model heights in the BEV space, which needs no extra data like
LiDAR and can fit arbitrary camera rigs and types compared to modeling depths.
Theoretically, we give proof of the equivalence between height-based methods
and depth-based methods. Considering the equivalence and some advantages of
modeling heights, we propose HeightFormer, which models heights and
uncertainties in a self-recursive way. Without any extra data, the proposed
HeightFormer could estimate heights in BEV accurately. Benchmark results show
that the performance of HeightFormer achieves SOTA compared with those
camera-only methods.",None,-1
bea630be-5a86-4b85-ace0-107f9cf8ebb2,Outlier detection using flexible categorisation and interrogative agendas,0.59811,1,"Categorization is one of the basic tasks in machine learning and data
analysis. Building on formal concept analysis (FCA), the starting point of the
present work is that different ways to categorize a given set of objects exist,
which depend on the choice of the sets of features used to classify them, and
different such sets of features may yield better or worse categorizations,
relative to the task at hand. In their turn, the (a priori) choice of a
particular set of features over another might be subjective and express a
certain epistemic stance (e.g. interests, relevance, preferences) of an agent
or a group of agents, namely, their interrogative agenda. In the present paper,
we represent interrogative agendas as sets of features, and explore and compare
different ways to categorize objects w.r.t. different sets of features
(agendas). We first develop a simple unsupervised FCA-based algorithm for
outlier detection which uses categorizations arising from different agendas. We
then present a supervised meta-learning algorithm to learn suitable (fuzzy)
agendas for categorization as sets of features with different weights or
masses. We combine this meta-learning algorithm with the unsupervised outlier
detection algorithm to obtain a supervised outlier detection algorithm. We show
that these algorithms perform at par with commonly used algorithms for outlier
detection on commonly used datasets in outlier detection. These algorithms
provide both local and global explanations of their results.",None,-1
87ad1d9f-f9c0-4fc8-a392-abae8fb1d15d,Visual Place Recognition: A Tutorial,0.81953,21,"Localization is an essential capability for mobile robots. A rapidly growing
field of research in this area is Visual Place Recognition (VPR), which is the
ability to recognize previously seen places in the world based solely on
images. This present work is the first tutorial paper on visual place
recognition. It unifies the terminology of VPR and complements prior research
in two important directions: 1) It provides a systematic introduction for
newcomers to the field, covering topics such as the formulation of the VPR
problem, a general-purpose algorithmic pipeline, an evaluation methodology for
VPR approaches, and the major challenges for VPR and how they may be addressed.
2) As a contribution for researchers acquainted with the VPR problem, it
examines the intricacies of different VPR problem types regarding input, data
processing, and output. The tutorial also discusses the subtleties behind the
evaluation of VPR algorithms, e.g., the evaluation of a VPR system that has to
find all matching database images per query, as opposed to just a single match.
Practical code examples in Python illustrate to prospective practitioners and
researchers how VPR is implemented and evaluated.",None,-1
9ff9384a-5c1a-4d92-8b14-e0d596fe5110,NaSGEC: a Multi-Domain Chinese Grammatical Error Correction Dataset from Native Speaker Texts,0.726314,4,"We introduce NaSGEC, a new dataset to facilitate research on Chinese
grammatical error correction (CGEC) for native speaker texts from multiple
domains. Previous CGEC research primarily focuses on correcting texts from a
single domain, especially learner essays. To broaden the target domain, we
annotate multiple references for 12,500 sentences from three native domains,
i.e., social media, scientific writing, and examination. We provide solid
benchmark results for NaSGEC by employing cutting-edge CGEC models and
different training data. We further perform detailed analyses of the
connections and gaps between our domains from both empirical and statistical
views. We hope this work can inspire future studies on an important but
under-explored direction--cross-domain GEC.",None,-1
19f5cec7-8a9e-41f9-8a2f-8a09513a3a8b,Nominal Topology for Data Languages,0.204455,2,"We propose a novel topological perspective on data languages recognizable by
orbit-finite nominal monoids. For this purpose, we introduce pro-orbit-finite
nominal topological spaces. Assuming globally bounded support sizes, they
coincide with nominal Stone spaces and are shown to be dually equivalent to a
subcategory of nominal boolean algebras. Recognizable data languages are
characterized as topologically clopen sets of pro-orbit-finite words. In
addition, we explore the expressive power of pro-orbit-finite equations by
establishing a nominal version of Reiterman's pseudovariety theorem.",None,-1
1b8fa985-06e4-41d4-a834-fd78d8f8a625,Not All Steps are Created Equal: Selective Diffusion Distillation for Image Manipulation,0.294435,3,"Conditional diffusion models have demonstrated impressive performance in
image manipulation tasks. The general pipeline involves adding noise to the
image and then denoising it. However, this method faces a trade-off problem:
adding too much noise affects the fidelity of the image while adding too little
affects its editability. This largely limits their practical applicability. In
this paper, we propose a novel framework, Selective Diffusion Distillation
(SDD), that ensures both the fidelity and editability of images. Instead of
directly editing images with a diffusion model, we train a feedforward image
manipulation network under the guidance of the diffusion model. Besides, we
propose an effective indicator to select the semantic-related timestep to
obtain the correct semantic guidance from the diffusion model. This approach
successfully avoids the dilemma caused by the diffusion process. Our extensive
experiments demonstrate the advantages of our framework. Code is released at
https://github.com/AndysonYs/Selective-Diffusion-Distillation.",None,-1
b163c670-411b-4370-aafc-a55f3c6464c7,"""What if?"" in Probabilistic Logic Programming",0.24988,1,"A ProbLog program is a logic program with facts that only hold with a
specified probability. In this contribution we extend this ProbLog language by
the ability to answer ""What if"" queries. Intuitively, a ProbLog program defines
a distribution by solving a system of equations in terms of mutually
independent predefined Boolean random variables. In the theory of causality,
Judea Pearl proposes a counterfactual reasoning for such systems of equations.
Based on Pearl's calculus, we provide a procedure for processing these
counterfactual queries on ProbLog programs, together with a proof of
correctness and a full implementation. Using the latter, we provide insights
into the influence of different parameters on the scalability of inference.
Finally, we also show that our approach is consistent with CP-logic, i.e. with
the causal semantics for logic programs with annotated with disjunctions.",None,-1
d370554d-8863-445f-8208-affc07a0c7ff,Semantic Segmentation on VSPW Dataset through Contrastive Loss and Multi-dataset Training Approach,0.0706409,1,"Video scene parsing incorporates temporal information, which can enhance the
consistency and accuracy of predictions compared to image scene parsing. The
added temporal dimension enables a more comprehensive understanding of the
scene, leading to more reliable results. This paper presents the winning
solution of the CVPR2023 workshop for video semantic segmentation, focusing on
enhancing Spatial-Temporal correlations with contrastive loss. We also explore
the influence of multi-dataset training by utilizing a label-mapping technique.
And the final result is aggregating the output of the above two models. Our
approach achieves 65.95% mIoU performance on the VSPW dataset, ranked 1st place
on the VSPW challenge at CVPR 2023.",None,-1
8e55b149-50b9-4927-9622-1c4130cf3021,Language Models are Few-shot Learners for Prognostic Prediction,0.967815,74,"Clinical prediction is an essential task in the healthcare industry. However,
the recent success of transformers, on which large language models are built,
has not been extended to this domain. In this research, we explore the use of
transformers and language models in prognostic prediction for immunotherapy
using real-world patients' clinical data and molecular profiles. This paper
investigates the potential of transformers to improve clinical prediction
compared to conventional machine learning approaches and addresses the
challenge of few-shot learning in predicting rare disease areas. The study
benchmarks the efficacy of baselines and language models on prognostic
prediction across multiple cancer types and investigates the impact of
different pretrained language models under few-shot regimes. The results
demonstrate significant improvements in accuracy and highlight the potential of
NLP in clinical research to improve early detection and intervention for
different diseases.",None,-1
8897bd1d-d28e-4160-bb98-e971fba69ad7,AMR Parsing with Instruction Fine-tuned Pre-trained Language Models,0.239704,2,"Instruction fine-tuned language models on a collection of instruction
annotated datasets (FLAN) have shown highly effective to improve model
performance and generalization to unseen tasks. However, a majority of standard
parsing tasks including abstract meaning representation (AMR), universal
dependency (UD), semantic role labeling (SRL) has been excluded from the FLAN
collections for both model training and evaluations. In this paper, we take one
of such instruction fine-tuned pre-trained language models, i.e. FLAN-T5, and
fine-tune them for AMR parsing. Our extensive experiments on various AMR
parsing tasks including AMR2.0, AMR3.0 and BioAMR indicate that FLAN-T5
fine-tuned models out-perform previous state-of-the-art models across all
tasks. In addition, full fine-tuning followed by the parameter efficient
fine-tuning, LoRA, further improves the model performances, setting new
state-of-the-arts in Smatch on AMR2.0 (86.4), AMR3.0 (84.9) and BioAMR (82.3).",None,-1
9e454627-08eb-40dc-bf2b-3415bec66e81,Novelty Detection in Network Traffic: Using Survival Analysis for Feature Identification,0.318184,4,"Intrusion Detection Systems are an important component of many organizations'
cyber defense and resiliency strategies. However, one downside of these systems
is their reliance on known attack signatures for detection of malicious network
events. When it comes to unknown attack types and zero-day exploits, modern
Intrusion Detection Systems often fall short. In this paper, we introduce an
unconventional approach to identifying network traffic features that influence
novelty detection based on survival analysis techniques. Specifically, we
combine several Cox proportional hazards models and implement Kaplan-Meier
estimates to predict the probability that a classifier identifies novelty after
the injection of an unknown network attack at any given time. The proposed
model is successful at pinpointing PSH Flag Count, ACK Flag Count, URG Flag
Count, and Down/Up Ratio as the main features to impact novelty detection via
Random Forest, Bayesian Ridge, and Linear Support Vector Regression
classifiers.",None,-1
bef1c3fb-3e2c-4b3d-aff6-bc9c7a37453c,Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining,0.346755,8,"Medical artificial general intelligence (MAGI) enables one foundation model
to solve different medical tasks, which is very practical in the medical
domain. It can significantly reduce the requirement of large amounts of
task-specific data by sufficiently sharing medical knowledge among different
tasks. However, due to the challenges of designing strongly generalizable
models with limited and complex medical data, most existing approaches tend to
develop task-specific models. To take a step towards MAGI, we propose a new
paradigm called Medical-knOwledge-enhanced mulTimOdal pretRaining (MOTOR). In
MOTOR, we combine two kinds of basic medical knowledge, i.e., general and
specific knowledge, in a complementary manner to boost the general pretraining
process. As a result, the foundation model with comprehensive basic knowledge
can learn compact representations from pretraining radiographic data for better
cross-modal alignment. MOTOR unifies the understanding and generation, which
are two kinds of core intelligence of an AI system, into a single medical
foundation model, to flexibly handle more diverse medical tasks. To enable a
comprehensive evaluation and facilitate further research, we construct a
medical multimodal benchmark including a wide range of downstream tasks, such
as chest x-ray report generation and medical visual question answering.
Extensive experiments on our benchmark show that MOTOR obtains promising
results through simple task-oriented adaptation. The visualization shows that
the injected knowledge successfully highlights key information in the medical
data, demonstrating the excellent interpretability of MOTOR. Our MOTOR
successfully mimics the human practice of fulfilling a ""medical student"" to
accelerate the process of becoming a ""specialist"". We believe that our work
makes a significant stride in realizing MAGI.",None,-1
bdeb0728-4d5b-43d6-ac92-31dd271e4591,Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati,0.0237744,1,"Local/Native South African languages are classified as low-resource
languages. As such, it is essential to build the resources for these languages
so that they can benefit from advances in the field of natural language
processing. In this work, the focus was to create annotated news datasets for
the isiZulu and Siswati native languages based on news topic classification
tasks and present the findings from these baseline classification models. Due
to the shortage of data for these native South African languages, the datasets
that were created were augmented and oversampled to increase data size and
overcome class classification imbalance. In total, four different
classification models were used namely Logistic regression, Naive bayes,
XGBoost and LSTM. These models were trained on three different word embeddings
namely Bag-Of-Words, TFIDF and Word2vec. The results of this study showed that
XGBoost, Logistic Regression and LSTM, trained from Word2vec performed better
than the other combinations.",None,-1
42eb934d-d1bc-4173-848c-fc16d12be63a,Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes,0.709431,3,"This paper proposes Allophant, a multilingual phoneme recognizer. It requires
only a phoneme inventory for cross-lingual transfer to a target language,
allowing for low-resource recognition. The architecture combines a
compositional phone embedding approach with individually supervised phonetic
attribute classifiers in a multi-task architecture. We also introduce
Allophoible, an extension of the PHOIBLE database. When combined with a
distance based mapping approach for grapheme-to-phoneme outputs, it allows us
to train on PHOIBLE inventories directly. By training and evaluating on 34
languages, we found that the addition of multi-task learning improves the
model's capability of being applied to unseen phonemes and phoneme inventories.
On supervised languages we achieve phoneme error rate improvements of 11
percentage points (pp.) compared to a baseline without multi-task learning.
Evaluation of zero-shot transfer on 84 languages yielded a decrease in PER of
2.63 pp. over the baseline.",None,-1
fd4c19a4-2443-4c31-99be-8ca506ee0d8c,Improving Code-Switching and Named Entity Recognition in ASR with Speech Editing based Data Augmentation,0.444514,3,"Recently, end-to-end (E2E) automatic speech recognition (ASR) models have
made great strides and exhibit excellent performance in general speech
recognition. However, there remain several challenging scenarios that E2E
models are not competent in, such as code-switching and named entity
recognition (NER). Data augmentation is a common and effective practice for
these two scenarios. However, the current data augmentation methods mainly rely
on audio splicing and text-to-speech (TTS) models, which might result in
discontinuous, unrealistic, and less diversified speech. To mitigate these
potential issues, we propose a novel data augmentation method by applying the
text-based speech editing model. The augmented speech from speech editing
systems is more coherent and diversified, also more akin to real speech. The
experimental results on code-switching and NER tasks show that our proposed
method can significantly outperform the audio splicing and neural TTS based
data augmentation systems.",None,-1
20c8d239-4b11-4a1c-877f-00c0e4a32a84,Benchmarking LLM-based Machine Translation on Cultural Awareness,0.770707,16,"Translating cultural-specific content is crucial for effective cross-cultural
communication. However, many MT systems still struggle to translate sentences
containing cultural-specific entities accurately and understandably. Recent
advancements in in-context learning utilize lightweight prompts to guide large
language models (LLMs) in machine translation tasks. Nevertheless, the
effectiveness of this approach in enhancing machine translation with cultural
awareness remains uncertain. To address this gap, we introduce a new data
curation pipeline to construct a culturally relevant parallel corpus, enriched
with annotations of cultural-specific items. Furthermore, we devise a novel
evaluation metric to assess the understandability of translations in a
reference-free manner by GPT-4. We evaluate a variety of neural machine
translation (NMT) and LLM-based MT systems using our dataset. Additionally, we
propose several prompting strategies for LLMs to incorporate external and
internal cultural knowledge into the translation process. Our results
demonstrate that eliciting explanations can significantly enhance the
understandability of cultural-specific entities, especially those without
well-known translations.",None,-1
23c41814-f6ce-4f13-a596-add875386fef,Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction,0.345673,3,"Continual few-shot relation extraction (RE) aims to continuously train a
model for new relations with few labeled training data, of which the major
challenges are the catastrophic forgetting of old relations and the overfitting
caused by data sparsity. In this paper, we propose a new model, namely SCKD, to
accomplish the continual few-shot RE task. Specifically, we design serial
knowledge distillation to preserve the prior knowledge from previous models and
conduct contrastive learning with pseudo samples to keep the representations of
samples in different relations sufficiently distinguishable. Our experiments on
two benchmark datasets validate the effectiveness of SCKD for continual
few-shot RE and its superiority in knowledge transfer and memory utilization
over state-of-the-art models.",None,-1
ca2b1070-f2a1-4346-82ab-b084612b71ef,LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination,0.732125,6,"Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable
proficiency in comprehending and generating natural language. On the other
hand, medical assistants hold the potential to offer substantial benefits for
individuals. However, the exploration of LLM-based personalized medical
assistant remains relatively scarce. Typically, patients converse differently
based on their background and preferences which necessitates the task of
enhancing user-oriented medical assistant. While one can fully train an LLM for
this objective, the resource consumption is unaffordable. Prior research has
explored memory-based methods to enhance the response with aware of previous
mistakes for new queries during a dialogue session. We contend that a mere
memory module is inadequate and fully training an LLM can be excessively
costly. In this study, we propose a novel computational bionic memory
mechanism, equipped with a parameter-efficient fine-tuning (PEFT) schema, to
personalize medical assistants.",None,-1
4f5613ee-9c7e-4f6e-a94a-3db35c0a5e4c,Learning and interpreting asymmetry-labeled DAGs: a case study on COVID-19 fear,0.497615,6,"Bayesian networks are widely used to learn and reason about the dependence
structure of discrete variables. However, they are only capable of formally
encoding symmetric conditional independence, which in practice is often too
strict to hold. Asymmetry-labeled DAGs have been recently proposed to both
extend the class of Bayesian networks by relaxing the symmetric assumption of
independence and denote the type of dependence existing between the variables
of interest. Here, we introduce novel structural learning algorithms for this
class of models which, whilst being efficient, allow for a straightforward
interpretation of the underlying dependence structure. A comprehensive
computational study highlights the efficiency of the algorithms. A real-world
data application using data from the Fear of COVID-19 Scale collected in Italy
showcases their use in practice.",None,-1
ec28b16e-8fba-4dbf-8ee6-9fc22c934876,Human Pose as Compositional Tokens,0.892381,20,"Human pose is typically represented by a coordinate vector of body joints or
their heatmap embeddings. While easy for data processing, unrealistic pose
estimates are admitted due to the lack of dependency modeling between the body
joints. In this paper, we present a structured representation, named Pose as
Compositional Tokens (PCT), to explore the joint dependency. It represents a
pose by M discrete tokens with each characterizing a sub-structure with several
interdependent joints. The compositional design enables it to achieve a small
reconstruction error at a low cost. Then we cast pose estimation as a
classification task. In particular, we learn a classifier to predict the
categories of the M tokens from an image. A pre-learned decoder network is used
to recover the pose from the tokens without further post-processing. We show
that it achieves better or comparable pose estimation results as the existing
methods in general scenarios, yet continues to work well when occlusion occurs,
which is ubiquitous in practice. The code and models are publicly available at
https://github.com/Gengzigang/PCT.",None,-1
dcee5c3c-c99c-4599-99ba-3f63af28042d,NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion,0.974691,122,"Novel view synthesis from a single image requires inferring occluded regions
of objects and scenes whilst simultaneously maintaining semantic and physical
consistency with the input. Existing approaches condition neural radiance
fields (NeRF) on local image features, projecting points to the input image
plane, and aggregating 2D features to perform volume rendering. However, under
severe occlusion, this projection fails to resolve uncertainty, resulting in
blurry renderings that lack details. In this work, we propose NerfDiff, which
addresses this issue by distilling the knowledge of a 3D-aware conditional
diffusion model (CDM) into NeRF through synthesizing and refining a set of
virtual views at test time. We further propose a novel NeRF-guided distillation
algorithm that simultaneously generates 3D consistent virtual views from the
CDM samples, and finetunes the NeRF based on the improved virtual views. Our
approach significantly outperforms existing NeRF-based and geometry-free
approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.",None,-1
48e61cf3-6920-4317-92f7-ba7b619563f6,Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models,0.765612,7,"Parameter-efficient tuning (PET) methods fit pre-trained language models
(PLMs) to downstream tasks by either computing a small compressed update for a
subset of model parameters, or appending and fine-tuning a small number of new
model parameters to the pre-trained network. Hand-designed PET architectures
from the literature perform well in practice, but have the potential to be
improved via automated neural architecture search (NAS). We propose an
efficient NAS method for learning PET architectures via structured and
unstructured pruning. We present experiments on GLUE demonstrating the
effectiveness of our algorithm and discuss how PET architectural design choices
affect performance in practice.",None,-1
8e6086be-606e-4c25-b7ff-4481f4d933ed,"Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks",0.659174,27,"We explore the abstract reasoning abilities of text-only and multimodal
versions of GPT-4, using the ConceptARC benchmark [10], which is designed to
evaluate robust understanding and reasoning with core-knowledge concepts. We
extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,
one-shot prompting (rather than simple, zero-shot prompts) with text versions
of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,
on zero- and one-shot prompts using image versions of the simplest tasks. Our
experimental results support the conclusion that neither version of GPT-4 has
developed robust abstraction abilities at humanlike levels.",None,-1
52d4fe7e-35cf-41ec-90c6-714e92a75f67,On the Efficacy of Sampling Adapters,0.123227,6,"Sampling is a common strategy for generating text from probabilistic models,
yet standard ancestral sampling often results in text that is incoherent or
ungrammatical. To alleviate this issue, various modifications to a model's
sampling distribution, such as nucleus or top-k sampling, have been introduced
and are now ubiquitously used in language generation systems. We propose a
unified framework for understanding these techniques, which we term sampling
adapters. Sampling adapters often lead to qualitatively better text, which
raises the question: From a formal perspective, how are they changing the
(sub)word-level distributions of language generation models? And why do these
local changes lead to higher-quality text? We argue that the shift they enforce
can be viewed as a trade-off between precision and recall: while the model
loses its ability to produce certain strings, its precision rate on desirable
text increases. While this trade-off is not reflected in standard metrics of
distribution quality (such as perplexity), we find that several
precision-emphasizing measures indeed indicate that sampling adapters can lead
to probability distributions more aligned with the true distribution. Further,
these measures correlate with higher sequence-level quality scores,
specifically, Mauve.",None,-1
91486b78-5c9d-4373-9145-59c185ebd8c3,A data science and machine learning approach to continuous analysis of Shakespeare's plays,0.187047,1,"The availability of quantitative text analysis methods has provided new ways
of analyzing literature in a manner that was not available in the
pre-information era. Here we apply comprehensive machine learning analysis to
the work of William Shakespeare. The analysis shows clear changes in the style
of writing over time, with the most significant changes in the sentence length,
frequency of adjectives and adverbs, and the sentiments expressed in the text.
Applying machine learning to make a stylometric prediction of the year of the
play shows a Pearson correlation of 0.71 between the actual and predicted year,
indicating that Shakespeare's writing style as reflected by the quantitative
measurements changed over time. Additionally, it shows that the stylometrics of
some of the plays is more similar to plays written either before or after the
year they were written. For instance, Romeo and Juliet is dated 1596, but is
more similar in stylometrics to plays written by Shakespeare after 1600. The
source code for the analysis is available for free download.",None,-1
4cb95eee-9962-4b1d-80e9-6e6b8f3741ee,ECO: Ensembling Context Optimization for Vision-Language Models,0.0699217,3,"Image recognition has recently witnessed a paradigm shift, where
vision-language models are now used to perform few-shot classification based on
textual prompts. Among these, the CLIP model has shown remarkable capabilities
for zero-shot transfer by matching an image and a custom textual prompt in its
latent space. This has paved the way for several works that focus on
engineering or learning textual contexts for maximizing CLIP's classification
capabilities. In this paper, we follow this trend by learning an ensemble of
prompts for image classification. We show that learning diverse and possibly
shorter contexts improves considerably and consistently the results rather than
relying on a single trainable prompt. In particular, we report better few-shot
capabilities with no additional cost at inference time. We demonstrate the
capabilities of our approach on 11 different benchmarks.",None,-1
0b3b5d12-5ab1-4194-bc07-fa0347c54546,Scaling Laws for Associative Memories,0.968791,7,"Learning arguably involves the discovery and memorization of abstract rules.
The aim of this paper is to study associative memory mechanisms. Our model is
based on high-dimensional matrices consisting of outer products of embeddings,
which relates to the inner layers of transformer language models. We derive
precise scaling laws with respect to sample size and parameter size, and
discuss the statistical efficiency of different estimators, including
optimization-based algorithms. We provide extensive numerical experiments to
validate and interpret theoretical results, including fine-grained
visualizations of the stored memory associations.",None,-1
dfc216fa-6459-457d-910d-7831cdad68cd,Everyone Deserves A Reward: Learning Customized Human Preferences,0.750768,16,"Reward models (RMs) are essential for aligning large language models (LLMs)
with human preferences to improve interaction quality. However, the real world
is pluralistic, which leads to diversified human preferences with respect to
different religions, politics, cultures, etc. Moreover, each individual can
have their unique preferences on various topics. Neglecting the diversity of
human preferences, current human feedback aligning methods only consider a
general reward model, which is below satisfaction for customized or
personalized application scenarios. To explore customized preference learning,
we collect a domain-specific preference (DSP) dataset, which includes preferred
responses for each given query from four practical domains. Besides, from the
perspective of data efficiency, we propose a three-stage customized RM learning
scheme, then empirically verify its effectiveness on both general preference
datasets and our DSP set. Furthermore, we test multiple training and data
strategies on the three learning stages. We find several ways to better
preserve the general preferring ability while training the customized RMs,
especially general preference enrichment, and customized preference imitation
learning. The DSP dataset and code are available at
https://github.com/Linear95/DSP.",None,-1
3d873082-08d0-41b8-b893-a889b01e7869,Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental Health,0.617625,5,"Amid ongoing health crisis, there is a growing necessity to discern possible
signs of Wellness Dimensions (WD) manifested in self-narrated text. As the
distribution of WD on social media data is intrinsically imbalanced, we
experiment the generative NLP models for data augmentation to enable further
improvement in the pre-screening task of classifying WD. To this end, we
propose a simple yet effective data augmentation approach through prompt-based
Generative NLP models, and evaluate the ROUGE scores and syntactic/semantic
similarity among existing interpretations and augmented data. Our approach with
ChatGPT model surpasses all the other methods and achieves improvement over
baselines such as Easy-Data Augmentation and Backtranslation. Introducing data
augmentation to generate more training samples and balanced dataset, results in
the improved F-score and the Matthew's Correlation Coefficient for upto 13.11%
and 15.95%, respectively.",None,-1
8b41ee79-ed72-4e83-a626-741ce9b3b026,EHRTutor: Enhancing Patient Understanding of Discharge Instructions,0.341246,4,"Large language models have shown success as a tutor in education in various
fields. Educating patients about their clinical visits plays a pivotal role in
patients' adherence to their treatment plans post-discharge. This paper
presents EHRTutor, an innovative multi-component framework leveraging the Large
Language Model (LLM) for patient education through conversational
question-answering. EHRTutor first formulates questions pertaining to the
electronic health record discharge instructions. It then educates the patient
through conversation by administering each question as a test. Finally, it
generates a summary at the end of the conversation. Evaluation results using
LLMs and domain experts have shown a clear preference for EHRTutor over the
baseline. Moreover, EHRTutor also offers a framework for generating synthetic
patient education dialogues that can be used for future in-house system
training.",None,-1
3fa7b1d2-6470-43b2-a6b0-87501328c01d,Ontology Pre-training for Poison Prediction,0.0199351,1,"Integrating human knowledge into neural networks has the potential to improve
their robustness and interpretability. We have developed a novel approach to
integrate knowledge from ontologies into the structure of a Transformer network
which we call ontology pre-training: we train the network to predict membership
in ontology classes as a way to embed the structure of the ontology into the
network, and subsequently fine-tune the network for the particular prediction
task. We apply this approach to a case study in predicting the potential
toxicity of a small molecule based on its molecular structure, a challenging
task for machine learning in life sciences chemistry. Our approach improves on
the state of the art, and moreover has several additional benefits. First, we
are able to show that the model learns to focus attention on more meaningful
chemical groups when making predictions with ontology pre-training than
without, paving a path towards greater robustness and interpretability. Second,
the training time is reduced after ontology pre-training, indicating that the
model is better placed to learn what matters for toxicity prediction with the
ontology pre-training than without. This strategy has general applicability as
a neuro-symbolic approach to embed meaningful semantics into neural networks.",None,-1
8f96097d-3ebe-495e-943e-de849b779b74,In-context Learning and Gradient Descent Revisited,0.216296,2,"In-context learning (ICL) has shown impressive results in few-shot learning
tasks, yet its underlying mechanism is still not fully understood. A recent
line of work suggests that ICL performs gradient descent (GD)-based
optimization implicitly. While appealing, much of the research focuses on
simplified settings, where the parameters of a shallow model are optimized. In
this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks
and models. We find gaps in evaluation, both in terms of problematic metrics
and insufficient baselines. We show that surprisingly, even untrained models
achieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next,
we explore a major discrepancy in the flow of information throughout the model
between ICL and GD, which we term Layer Causality. We propose a simple GD-based
optimization procedure that respects layer causality, and show it improves
similarity scores significantly.",None,-1
d0cb9a01-b0ea-440b-94ae-91127fb489bc,Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?,0.526926,7,"The rapid advancement of Large Language Models (LLMs) has spurred discussions
about their potential to enhance quantitative trading strategies. LLMs excel in
analyzing sentiments about listed companies from financial news, providing
critical insights for trading decisions. However, the performance of LLMs in
this task varies substantially due to their inherent characteristics. This
paper introduces a standardized experimental procedure for comprehensive
evaluations. We detail the methodology using three distinct LLMs, each
embodying a unique approach to performance enhancement, applied specifically to
the task of sentiment factor extraction from large volumes of Chinese news
summaries. Subsequently, we develop quantitative trading strategies using these
sentiment factors and conduct back-tests in realistic scenarios. Our results
will offer perspectives about the performances of Large Language Models applied
to extracting sentiments from Chinese news texts.",None,-1
5a1f8b0b-7ad4-428c-bd37-0fd81bd7a562,Large Language Models are biased to overestimate profoundness,0.0711078,2,"Recent advancements in natural language processing by large language models
(LLMs), such as GPT-4, have been suggested to approach Artificial General
Intelligence. And yet, it is still under dispute whether LLMs possess similar
reasoning abilities to humans. This study evaluates GPT-4 and various other
LLMs in judging the profoundness of mundane, motivational, and pseudo-profound
statements. We found a significant statement-to-statement correlation between
the LLMs and humans, irrespective of the type of statements and the prompting
technique used. However, LLMs systematically overestimate the profoundness of
nonsensical statements, with the exception of Tk-instruct, which uniquely
underestimates the profoundness of statements. Only few-shot learning prompts,
as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.
Furthermore, this work provides insights into the potential biases induced by
Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the
bias to overestimate the profoundness of statements.",None,-1
9cd97ed9-ad22-4535-be82-39706e95d4b4,A Study on the Appropriate size of the Mongolian general corpus,0.225112,1,"This study aims to determine the appropriate size of the Mongolian general
corpus. This study used the Heaps function and Type Token Ratio to determine
the appropriate size of the Mongolian general corpus. The sample corpus of
906,064 tokens comprised texts from 10 domains of newspaper politics, economy,
society, culture, sports, world articles and laws, middle and high school
literature textbooks, interview articles, and podcast transcripts. First, we
estimated the Heaps function with this sample corpus. Next, we observed changes
in the number of types and TTR values while increasing the number of tokens by
one million using the estimated Heaps function. As a result of observation, we
found that the TTR value hardly changed when the number of tokens exceeded from
39 to 42 million. Thus, we conclude that an appropriate size for a Mongolian
general corpus is from 39 to 42 million tokens.",None,-1
bfef7069-4df2-4cbd-abab-4030751f6770,The Co-12 Recipe for Evaluating Interpretable Part-Prototype Image Classifiers,0.698124,4,"Interpretable part-prototype models are computer vision models that are
explainable by design. The models learn prototypical parts and recognise these
components in an image, thereby combining classification and explanation.
Despite the recent attention for intrinsically interpretable models, there is
no comprehensive overview on evaluating the explanation quality of
interpretable part-prototype models. Based on the Co-12 properties for
explanation quality as introduced in arXiv:2201.08164 (e.g., correctness,
completeness, compactness), we review existing work that evaluates
part-prototype models, reveal research gaps and outline future approaches for
evaluation of the explanation quality of part-prototype models. This paper,
therefore, contributes to the progression and maturity of this relatively new
research field on interpretable part-prototype models. We additionally provide
a ``Co-12 cheat sheet'' that acts as a concise summary of our findings on
evaluating part-prototype models.",None,-1
521bb9e2-4f66-4791-ab28-cbc6c6d47a4e,"More Data Types More Problems: A Temporal Analysis of Complexity, Stability, and Sensitivity in Privacy Policies",0.523632,4,"Collecting personally identifiable information (PII) on data subjects has
become big business. Data brokers and data processors are part of a
multi-billion-dollar industry that profits from collecting, buying, and selling
consumer data. Yet there is little transparency in the data collection industry
which makes it difficult to understand what types of data are being collected,
used, and sold, and thus the risk to individual data subjects. In this study,
we examine a large textual dataset of privacy policies from 1997-2019 in order
to investigate the data collection activities of data brokers and data
processors. We also develop an original lexicon of PII-related terms
representing PII data types curated from legislative texts. This mesoscale
analysis looks at privacy policies overtime on the word, topic, and network
levels to understand the stability, complexity, and sensitivity of privacy
policies over time. We find that (1) privacy legislation correlates with
changes in stability and turbulence of PII data types in privacy policies; (2)
the complexity of privacy policies decreases over time and becomes more
regularized; (3) sensitivity rises over time and shows spikes that are
correlated with events when new privacy legislation is introduced.",None,-1
e39a8d06-3426-48b1-9ce3-3083ea2413a4,Designing Participatory AI: Creative Professionals' Worries and Expectations about Generative AI,0.756395,36,"Generative AI, i.e., the group of technologies that automatically generate
visual or written content based on text prompts, has undergone a leap in
complexity and become widely available within just a few years. Such
technologies potentially introduce a massive disruption to creative fields.
This paper presents the results of a qualitative survey ($N$ = 23)
investigating how creative professionals think about generative AI. The results
show that the advancement of these AI models prompts important reflections on
what defines creativity and how creatives imagine using AI to support their
workflows. Based on these reflections, we discuss how we might design
\textit{participatory AI} in the domain of creative expertise with the goal of
empowering creative professionals in their present and future coexistence with
AI.",None,-1
b1e7b216-7802-463f-82e5-ae4e0971c00d,SCARP: 3D Shape Completion in ARbitrary Poses for Improved Grasping,0.21478,5,"Recovering full 3D shapes from partial observations is a challenging task
that has been extensively addressed in the computer vision community. Many deep
learning methods tackle this problem by training 3D shape generation networks
to learn a prior over the full 3D shapes. In this training regime, the methods
expect the inputs to be in a fixed canonical form, without which they fail to
learn a valid prior over the 3D shapes. We propose SCARP, a model that performs
Shape Completion in ARbitrary Poses. Given a partial pointcloud of an object,
SCARP learns a disentangled feature representation of pose and shape by relying
on rotationally equivariant pose features and geometric shape features trained
using a multi-tasking objective. Unlike existing methods that depend on an
external canonicalization, SCARP performs canonicalization, pose estimation,
and shape completion in a single network, improving the performance by 45% over
the existing baselines. In this work, we use SCARP for improving grasp
proposals on tabletop objects. By completing partial tabletop objects directly
in their observed poses, SCARP enables a SOTA grasp proposal network improve
their proposals by 71.2% on partial shapes. Project page:
https://bipashasen.github.io/scarp",None,-1
c68cb375-ef91-4985-a492-e12f594eacb2,ABAW : Facial Expression Recognition in the wild,0.520981,3,"The fifth Affective Behavior Analysis in-the-wild (ABAW) competition has
multiple challenges such as Valence-Arousal Estimation Challenge, Expression
Classification Challenge, Action Unit Detection Challenge, Emotional Reaction
Intensity Estimation Challenge. In this paper we have dealt only expression
classification challenge using multiple approaches such as fully supervised,
semi-supervised and noisy label approach. Our approach using noise aware model
has performed better than baseline model by 10.46% and semi supervised model
has performed better than baseline model by 9.38% and the fully supervised
model has performed better than the baseline by 9.34%",None,-1
2e82e845-7e90-47eb-893e-c8d53a8db6c8,SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation,0.154303,2,"Language models trained on large-scale corpora can generate remarkably fluent
results in open-domain dialogue. However, for the persona-based dialogue
generation task, consistency and coherence are also key factors, which are
great challenges for language models. Existing works mainly focus on valuable
data filtering, model structure modifying, or objective function designing,
while their improvements are limited and hard to generalize to all types of
pre-trained language models. However, we find that language models can produce
consistent and coherent responses if we consider enough generations. Thus, the
problems lay in large-scale response generation and target response selection.
In this work, a simple but effective two-stage SimOAP strategy is proposed,
i.e., over-sampling and post-evaluation. The over-sampling stage takes
large-scale responses from existing trained models efficiently via
off-the-shelf distilling and compressing methods, and the post-evaluation stage
selects a good response based on multiple well-designed evaluation metrics from
large-scale candidates. Experimental results show that the proposed plug-in
SimOAP strategy improves the backbone models and outperforms the baseline
strategies in both automatic and human evaluations.",None,-1
28ccdc28-eeae-464c-a3bd-b484a0c17ec8,Attribute-Guided Encryption with Facial Texture Masking,0.334356,2,"The increasingly pervasive facial recognition (FR) systems raise serious
concerns about personal privacy, especially for billions of users who have
publicly shared their photos on social media. Several attempts have been made
to protect individuals from unauthorized FR systems utilizing adversarial
attacks to generate encrypted face images to protect users from being
identified by FR systems. However, existing methods suffer from poor visual
quality or low attack success rates, which limit their usability in practice.
In this paper, we propose Attribute Guided Encryption with Facial Texture
Masking (AGE-FTM) that performs a dual manifold adversarial attack on FR
systems to achieve both good visual quality and high black box attack success
rates. In particular, AGE-FTM utilizes a high fidelity generative adversarial
network (GAN) to generate natural on-manifold adversarial samples by modifying
facial attributes, and performs the facial texture masking attack to generate
imperceptible off-manifold adversarial samples. Extensive experiments on the
CelebA-HQ dataset demonstrate that our proposed method produces more
natural-looking encrypted images than state-of-the-art methods while achieving
competitive attack performance. We further evaluate the effectiveness of
AGE-FTM in the real world using a commercial FR API and validate its usefulness
in practice through an user study.",None,-1
0c3d1fef-30c2-4143-a1de-ad43b717fc49,Unsupervised Semantic Correspondence Using Stable Diffusion,0.845484,37,"Text-to-image diffusion models are now capable of generating images that are
often indistinguishable from real images. To generate such images, these models
must understand the semantics of the objects they are asked to generate. In
this work we show that, without any training, one can leverage this semantic
knowledge within diffusion models to find semantic correspondences - locations
in multiple images that have the same semantic meaning. Specifically, given an
image, we optimize the prompt embeddings of these models for maximum attention
on the regions of interest. These optimized embeddings capture semantic
information about the location, which can then be transferred to another image.
By doing so we obtain results on par with the strongly supervised state of the
art on the PF-Willow dataset and significantly outperform (20.9% relative for
the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow,
CUB-200 and SPair-71k datasets.",None,-1
e79b180e-369e-4d4b-82e2-0156b8f947a1,Training Deep Surrogate Models with Large Scale Online Learning,0.317038,2,"The spatiotemporal resolution of Partial Differential Equations (PDEs) plays
important roles in the mathematical description of the world's physical
phenomena. In general, scientists and engineers solve PDEs numerically by the
use of computationally demanding solvers. Recently, deep learning algorithms
have emerged as a viable alternative for obtaining fast solutions for PDEs.
Models are usually trained on synthetic data generated by solvers, stored on
disk and read back for training. This paper advocates that relying on a
traditional static dataset to train these models does not allow the full
benefit of the solver to be used as a data generator. It proposes an open
source online training framework for deep surrogate models. The framework
implements several levels of parallelism focused on simultaneously generating
numerical simulations and training deep neural networks. This approach
suppresses the I/O and storage bottleneck associated with disk-loaded datasets,
and opens the way to training on significantly larger datasets. Experiments
compare the offline and online training of four surrogate models, including
state-of-the-art architectures. Results indicate that exposing deep surrogate
models to more dataset diversity, up to hundreds of GB, can increase model
generalization capabilities. Fully connected neural networks, Fourier Neural
Operator (FNO), and Message Passing PDE Solver prediction accuracy is improved
by 68%, 16% and 7%, respectively.",None,-1
2b224dfc-b267-4a83-aee5-8762db37cb0c,Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling,0.038327,1,"Dropout is a widely used regularization trick to resolve the overfitting
issue in large feedforward neural networks trained on a small dataset, which
performs poorly on the held-out test subset. Although the effectiveness of this
regularization trick has been extensively studied for convolutional neural
networks, there is a lack of analysis of it for unsupervised models and in
particular, VAE-based neural topic models. In this paper, we have analyzed the
consequences of dropout in the encoder as well as in the decoder of the VAE
architecture in three widely used neural topic models, namely, contextualized
topic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly
available datasets. We characterize the dropout effect on these models in terms
of the quality and predictive performance of the generated topics.",None,-1
f850ff96-7e59-4d95-8524-f84e734f58ed,Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations,0.465649,23,"The rapid development and application of foundation models have
revolutionized the field of artificial intelligence. Large diffusion models
have gained significant attention for their ability to generate photorealistic
images and support various tasks. On-device deployment of these models provides
benefits such as lower server costs, offline functionality, and improved user
privacy. However, common large diffusion models have over 1 billion parameters
and pose challenges due to restricted computational and memory resources on
devices. We present a series of implementation optimizations for large
diffusion models that achieve the fastest reported inference latency to-date
(under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung
S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile
devices. These enhancements broaden the applicability of generative AI and
improve the overall user experience across a wide range of devices.",None,-1
6f1f6d66-61d7-4a0a-96d3-da64e390b888,General Image-to-Image Translation with One-Shot Image Guidance,0.931476,18,"Large-scale text-to-image models pre-trained on massive text-image pairs show
excellent performance in image synthesis recently. However, image can provide
more intuitive visual concepts than plain text. People may ask: how can we
integrate the desired visual concept into an existing image, such as our
portrait? Current methods are inadequate in meeting this demand as they lack
the ability to preserve content or translate visual concepts effectively.
Inspired by this, we propose a novel framework named visual concept translator
(VCT) with the ability to preserve content in the source image and translate
the visual concepts guided by a single reference image. The proposed VCT
contains a content-concept inversion (CCI) process to extract contents and
concepts, and a content-concept fusion (CCF) process to gather the extracted
information to obtain the target image. Given only one reference image, the
proposed VCT can complete a wide range of general image-to-image translation
tasks with excellent results. Extensive experiments are conducted to prove the
superiority and effectiveness of the proposed methods. Codes are available at
https://github.com/CrystalNeuro/visual-concept-translator.",None,-1
bff4cb4c-5613-4648-8c9a-11d13be18713,Distributed Marker Representation for Ambiguous Discourse Markers and Entangled Relations,0.55298,3,"Discourse analysis is an important task because it models intrinsic semantic
structures between sentences in a document. Discourse markers are natural
representations of discourse in our daily language. One challenge is that the
markers as well as pre-defined and human-labeled discourse relations can be
ambiguous when describing the semantics between sentences. We believe that a
better approach is to use a contextual-dependent distribution over the markers
to express discourse information. In this work, we propose to learn a
Distributed Marker Representation (DMR) by utilizing the (potentially)
unlimited discourse marker data with a latent discourse sense, thereby bridging
markers with sentence pairs. Such representations can be learned automatically
from data without supervision, and in turn provide insights into the data
itself. Experiments show the SOTA performance of our DMR on the implicit
discourse relation recognition task and strong interpretability. Our method
also offers a valuable tool to understand complex ambiguity and entanglement
among discourse markers and manually defined discourse relations.",None,-1
1b158085-e057-4c27-bc94-97bc1b33a800,Orthogonal Annotation Benefits Barely-supervised Medical Image Segmentation,0.540742,9,"Recent trends in semi-supervised learning have significantly boosted the
performance of 3D semi-supervised medical image segmentation. Compared with 2D
images, 3D medical volumes involve information from different directions, e.g.,
transverse, sagittal, and coronal planes, so as to naturally provide
complementary views. These complementary views and the intrinsic similarity
among adjacent 3D slices inspire us to develop a novel annotation way and its
corresponding semi-supervised model for effective segmentation. Specifically,
we firstly propose the orthogonal annotation by only labeling two orthogonal
slices in a labeled volume, which significantly relieves the burden of
annotation. Then, we perform registration to obtain the initial pseudo labels
for sparsely labeled volumes. Subsequently, by introducing unlabeled volumes,
we propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that
exploits dense pseudo labels in early stage and sparse labels in later stage
and meanwhile forces consistent output of two networks. Experimental results on
three benchmark datasets validated our effectiveness in performance and
efficiency in annotation. For example, with only 10 annotated slices, our
method reaches a Dice up to 86.93% on KiTS19 dataset.",None,-1
45ae6077-1ebd-4dad-8c03-2ef34eb10a3c,GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints,0.999874,191,"Multi-query attention (MQA), which only uses a single key-value head,
drastically speeds up decoder inference. However, MQA can lead to quality
degradation, and moreover it may not be desirable to train a separate model
just for faster inference. We (1) propose a recipe for uptraining existing
multi-head language model checkpoints into models with MQA using 5% of original
pre-training compute, and (2) introduce grouped-query attention (GQA), a
generalization of multi-query attention which uses an intermediate (more than
one, less than number of query heads) number of key-value heads. We show that
uptrained GQA achieves quality close to multi-head attention with comparable
speed to MQA.",None,-1
90da6beb-7480-412b-8f17-0f045507ad3b,Improved Trajectory Reconstruction for Markerless Pose Estimation,0.659436,8,"Markerless pose estimation allows reconstructing human movement from multiple
synchronized and calibrated views, and has the potential to make movement
analysis easy and quick, including gait analysis. This could enable much more
frequent and quantitative characterization of gait impairments, allowing better
monitoring of outcomes and responses to interventions. However, the impact of
different keypoint detectors and reconstruction algorithms on markerless pose
estimation accuracy has not been thoroughly evaluated. We tested these
algorithmic choices on data acquired from a multicamera system from a
heterogeneous sample of 25 individuals seen in a rehabilitation hospital. We
found that using a top-down keypoint detector and reconstructing trajectories
with an implicit function enabled accurate, smooth and anatomically plausible
trajectories, with a noise in the step width estimates compared to a GaitRite
walkway of only 8mm.",None,-1
ef0a9fc8-9b08-4cc6-8d46-5548fa8c34fc,A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset,0.801013,21,"Text Classification is the process of categorizing text into the relevant
categories and its algorithms are at the core of many Natural Language
Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP
are the most highly used information retrieval methods in text classification.
We have investigated and analyzed the feature weighting method for text
classification on unstructured data. The proposed model considered two features
N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset
for sentiment analysis. Then we have used the state-of-the-art classifier to
validate the method i.e., Support Vector Machine (SVM), Logistic Regression,
Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and
k-nearest neighbors (KNN). From those two feature extractions, a significant
increase in feature extraction with TF-IDF features rather than based on
N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall
(93.81%), and F1-score (91.99%) value in Random Forest classifier.",None,-1
331d2d09-5282-4c38-887a-e01ec42306ae,Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias,0.0422091,6,"Recent studies show that instruction tuning (IT) and reinforcement learning
from human feedback (RLHF) improve the abilities of large language models (LMs)
dramatically. While these tuning methods can help align models with human
objectives and generate high-quality text, not much is known about their
potential adverse effects. In this work, we investigate the effect of IT and
RLHF on decision making and reasoning in LMs, focusing on three cognitive
biases - the decoy effect, the certainty effect, and the belief bias - all of
which are known to influence human decision-making and reasoning. Our findings
highlight the presence of these biases in various models from the GPT-3,
Mistral, and T5 families. Notably, we find a stronger presence of biases in
models that have undergone instruction tuning, such as Flan-T5,
Mistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward
comprehending cognitive biases in instruction-tuned LMs, which is crucial for
the development of more reliable and unbiased language models.",None,-1
06c2e22c-9d99-49f7-9ae0-f1d1af1cb8ab,Learned Image Reasoning Prior Penetrates Deep Unfolding Network for Panchromatic and Multi-Spectral Image Fusion,0.166634,2,"The success of deep neural networks for pan-sharpening is commonly in a form
of black box, lacking transparency and interpretability. To alleviate this
issue, we propose a novel model-driven deep unfolding framework with image
reasoning prior tailored for the pan-sharpening task. Different from existing
unfolding solutions that deliver the proximal operator networks as the
uncertain and vague priors, our framework is motivated by the content reasoning
ability of masked autoencoders (MAE) with insightful designs. Specifically, the
pre-trained MAE with spatial masking strategy, acting as intrinsic reasoning
prior, is embedded into unfolding architecture. Meanwhile, the pre-trained MAE
with spatial-spectral masking strategy is treated as the regularization term
within loss function to constrain the spatial-spectral consistency. Such
designs penetrate the image reasoning prior into deep unfolding networks while
improving its interpretability and representation capability. The uniqueness of
our framework is that the holistic learning process is explicitly integrated
with the inherent physical mechanism underlying the pan-sharpening task.
Extensive experiments on multiple satellite datasets demonstrate the
superiority of our method over the existing state-of-the-art approaches. Code
will be released at \url{https://manman1995.github.io/}.",None,-1
d33565f1-f371-4a28-936b-77a3a9e0ad9d,A Suite of Fairness Datasets for Tabular Classification,0.315653,2,"There have been many papers with algorithms for improving fairness of
machine-learning classifiers for tabular data. Unfortunately, most use only
very few datasets for their experimental evaluation. We introduce a suite of
functions for fetching 20 fairness datasets and providing associated fairness
metadata. Hopefully, these will lead to more rigorous experimental evaluations
in future fairness-aware machine learning research.",None,-1
57612e4e-754f-45bb-9f30-d4aa57b2c54b,Human-Centered Responsible Artificial Intelligence: Current & Future Trends,0.971935,22,"In recent years, the CHI community has seen significant growth in research on
Human-Centered Responsible Artificial Intelligence. While different research
communities may use different terminology to discuss similar topics, all of
this work is ultimately aimed at developing AI that benefits humanity while
being grounded in human rights and ethics, and reducing the potential harms of
AI. In this special interest group, we aim to bring together researchers from
academia and industry interested in these topics to map current and future
research trends to advance this important area of research by fostering
collaboration and sharing ideas.",None,-1
ff6abf9d-9a86-4342-88b7-dbd765a73ff2,A Dual-way Enhanced Framework from Text Matching Point of View for Multimodal Entity Linking,0.49754,1,"Multimodal Entity Linking (MEL) aims at linking ambiguous mentions with
multimodal information to entity in Knowledge Graph (KG) such as Wikipedia,
which plays a key role in many applications. However, existing methods suffer
from shortcomings, including modality impurity such as noise in raw image and
ambiguous textual entity representation, which puts obstacles to MEL. We
formulate multimodal entity linking as a neural text matching problem where
each multimodal information (text and image) is treated as a query, and the
model learns the mapping from each query to the relevant entity from candidate
entities. This paper introduces a dual-way enhanced (DWE) framework for MEL:
(1) our model refines queries with multimodal data and addresses semantic gaps
using cross-modal enhancers between text and image information. Besides, DWE
innovatively leverages fine-grained image attributes, including facial
characteristic and scene feature, to enhance and refine visual features. (2)By
using Wikipedia descriptions, DWE enriches entity semantics and obtains more
comprehensive textual representation, which reduces between textual
representation and the entities in KG. Extensive experiments on three public
benchmarks demonstrate that our method achieves state-of-the-art (SOTA)
performance, indicating the superiority of our model. The code is released on
https://github.com/season1blue/DWE",None,-1
dfbe3335-b8c8-4c51-9785-08239b1b042b,Debiasing Algorithm through Model Adaptation,0.500456,5,"Large language models are becoming the go-to solution for the ever-growing
number of tasks. However, with growing capacity, models are prone to rely on
spurious correlations stemming from biases and stereotypes present in the
training data. This work proposes a novel method for detecting and mitigating
gender bias in language models. We perform causal analysis to identify
problematic model components and discover that mid-upper feed-forward layers
are most prone to convey bias. Based on the analysis results, we intervene in
the model by applying a linear projection to the weight matrices of these
layers. Our titular method, DAMA, significantly decreases bias as measured by
diverse metrics while maintaining the model's performance on downstream tasks.
We release code for our method and models, which retrain LLaMA's
state-of-the-art performance while being significantly less biased.",None,-1
cc2dedd2-6ab7-4079-a06e-a54fa0880f2e,Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition,0.425078,21,"In recent years, Large Language Models such as GPT-3 showed remarkable
capabilities in performing NLP tasks in the zero and few shot settings. On the
other hand, the experiments highlighted the difficulty of GPT-3 in carrying out
tasks that require a certain degree of reasoning, such as arithmetic
operations. In this paper we evaluate the ability of Transformer Language
Models to perform arithmetic operations following a pipeline that, before
performing computations, decomposes numbers in units, tens, and so on. We
denote the models fine-tuned with this pipeline with the name Calculon and we
test them in the task of performing additions, subtractions and multiplications
on the same test sets of GPT-3. Results show an increase of accuracy of 63% in
the five-digit addition task. Moreover, we demonstrate the importance of the
decomposition pipeline introduced, since fine-tuning the same Language Model
without decomposing numbers results in 0% accuracy in the five-digit addition
task.",None,-1
5ad25e5d-0e40-4ccf-80e6-0c39aa6c0ae8,Chit-Chat or Deep Talk: Prompt Engineering for Process Mining,0.669416,7,"This research investigates the application of Large Language Models (LLMs) to
augment conversational agents in process mining, aiming to tackle its inherent
complexity and diverse skill requirements. While LLM advancements present novel
opportunities for conversational process mining, generating efficient outputs
is still a hurdle. We propose an innovative approach that amend many issues in
existing solutions, informed by prior research on Natural Language Processing
(NLP) for conversational agents. Leveraging LLMs, our framework improves both
accessibility and agent performance, as demonstrated by experiments on public
question and data sets. Our research sets the stage for future explorations
into LLMs' role in process mining and concludes with propositions for enhancing
LLM memory, implementing real-time user testing, and examining diverse data
sets.",None,-1
14825bf0-c13d-4622-8700-1bf7e6c27dbe,Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control,0.794954,19,"Building agents with large language models (LLMs) for computer control is a
burgeoning research area, where the agent receives computer states and performs
actions to complete complex tasks. Previous computer agents have demonstrated
the benefits of in-context learning (ICL); however, their performance is
hindered by several issues. First, the limited context length of LLMs and
complex computer states restrict the number of exemplars, as a single webpage
can consume the entire context. Second, the exemplars in current methods, such
as high-level plans and multi-choice questions, cannot represent complete
trajectories, leading to suboptimal performance in long-horizon tasks. Third,
existing computer agents rely on task-specific exemplars and overlook the
similarity among tasks, resulting in poor generalization to novel tasks. To
address these challenges, we introduce Synapse, a computer agent featuring
three key components: i) state abstraction, which filters out task-irrelevant
information from raw states, allowing more exemplars within the limited
context, ii) trajectory-as-exemplar prompting, which prompts the LLM with
complete trajectories of the abstracted states and actions to improve
multi-step decision-making, and iii) exemplar memory, which stores the
embeddings of exemplars and retrieves them via similarity search for
generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard
task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse
achieves a 99.2% average success rate (a 10% relative improvement) across 64
tasks using demonstrations from only 48 tasks. Notably, Synapse is the first
ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a
56% relative improvement in average step success rate over the previous
state-of-the-art prompting scheme in Mind2Web.",None,-1
41d2a3fb-af45-4f8c-a891-4bb83c6d080c,MCD: A Model-Agnostic Counterfactual Search Method For Multi-modal Design Modifications,0.141916,3,"Designers may often ask themselves how to adjust their design concepts to
achieve demanding functional goals. To answer such questions, designers must
often consider counterfactuals, weighing design alternatives and their
projected performance. This paper introduces Multi-objective Counterfactuals
for Design (MCD), a computational tool that automates and streamlines the
counterfactual search process and recommends targeted design modifications that
meet designers' unique requirements. MCD improves upon existing counterfactual
search methods by supporting multi-objective requirements, which are crucial in
design problems, and by decoupling the counterfactual search and sampling
processes, thus enhancing efficiency and facilitating objective trade-off
visualization. The paper showcases MCD's capabilities in complex engineering
tasks using three demonstrative bicycle design challenges. In the first, MCD
effectively identifies design modifications that quantifiably enhance
functional performance, strengthening the bike frame and saving weight. In the
second, MCD modifies parametric bike models in a cross-modal fashion to
resemble subjective text prompts or reference images. In a final
multidisciplinary case study, MCD tackles all the quantitative and subjective
design requirements introduced in the first two problems, while simultaneously
customizing a bike design to an individual rider's biomechanical attributes. By
exploring hypothetical design alterations and their impact on multiple design
objectives, MCD recommends effective design modifications for practitioners
seeking to make targeted enhancements to their designs. The code, test
problems, and datasets used in the paper are available to the public at
decode.mit.edu/projects/counterfactuals/.",None,-1
b2f28bc9-8cf8-481f-a1bf-e86b3cbe09ea,Your Day in Your Pocket: Complex Activity Recognition from Smartphone Accelerometers,0.468659,5,"Human Activity Recognition (HAR) enables context-aware user experiences where
mobile apps can alter content and interactions depending on user activities.
Hence, smartphones have become valuable for HAR as they allow large, and
diversified data collection. Although previous work in HAR managed to detect
simple activities (i.e., sitting, walking, running) with good accuracy using
inertial sensors (i.e., accelerometer), the recognition of complex daily
activities remains an open problem, specially in remote work/study settings
when people are more sedentary. Moreover, understanding the everyday activities
of a person can support the creation of applications that aim to support their
well-being. This paper investigates the recognition of complex activities
exclusively using smartphone accelerometer data. We used a large smartphone
sensing dataset collected from over 600 users in five countries during the
pandemic and showed that deep learning-based, binary classification of eight
complex activities (sleeping, eating, watching videos, online communication,
attending a lecture, sports, shopping, studying) can be achieved with AUROC
scores up to 0.76 with partially personalized models. This shows encouraging
signs toward assessing complex activities only using phone accelerometer data
in the post-pandemic world.",None,-1
d434d0a2-0327-4abe-9a1b-53ca354218c0,Adversarial Ink: Componentwise Backward Error Attacks on Deep Learning,0.435374,6,"Deep neural networks are capable of state-of-the-art performance in many
classification tasks. However, they are known to be vulnerable to adversarial
attacks -- small perturbations to the input that lead to a change in
classification. We address this issue from the perspective of backward error
and condition number, concepts that have proved useful in numerical analysis.
To do this, we build on the work of Beuzeville et al. (2021). In particular, we
develop a new class of attack algorithms that use componentwise relative
perturbations. Such attacks are highly relevant in the case of handwritten
documents or printed texts where, for example, the classification of
signatures, postcodes, dates or numerical quantities may be altered by changing
only the ink consistency and not the background. This makes the perturbed
images look natural to the naked eye. Such ``adversarial ink'' attacks
therefore reveal a weakness that can have a serious impact on safety and
security. We illustrate the new attacks on real data and contrast them with
existing algorithms. We also study the use of a componentwise condition number
to quantify vulnerability.",None,-1
96636924-3811-4e32-ae2f-b3105d94e0ef,DUDES: Deep Uncertainty Distillation using Ensembles for Semantic Segmentation,0.226196,5,"Deep neural networks lack interpretability and tend to be overconfident,
which poses a serious problem in safety-critical applications like autonomous
driving, medical imaging, or machine vision tasks with high demands on
reliability. Quantifying the predictive uncertainty is a promising endeavour to
open up the use of deep neural networks for such applications. Unfortunately,
current available methods are computationally expensive. In this work, we
present a novel approach for efficient and reliable uncertainty estimation
which we call Deep Uncertainty Distillation using Ensembles for Segmentation
(DUDES). DUDES applies student-teacher distillation with a Deep Ensemble to
accurately approximate predictive uncertainties with a single forward pass
while maintaining simplicity and adaptability. Experimentally, DUDES accurately
captures predictive uncertainties without sacrificing performance on the
segmentation task and indicates impressive capabilities of identifying wrongly
classified pixels and out-of-domain samples on the Cityscapes dataset. With
DUDES, we manage to simultaneously simplify and outperform previous work on
Deep Ensemble-based Uncertainty Distillation.",None,-1
a8dc18f3-706e-4666-8ebd-76db4e2e8082,Cross-Lingual Cross-Age Group Adaptation for Low-Resource Elderly Speech Emotion Recognition,0.269388,1,"Speech emotion recognition plays a crucial role in human-computer
interactions. However, most speech emotion recognition research is biased
toward English-speaking adults, which hinders its applicability to other
demographic groups in different languages and age groups. In this work, we
analyze the transferability of emotion recognition across three different
languages--English, Mandarin Chinese, and Cantonese; and 2 different age
groups--adults and the elderly. To conduct the experiment, we develop an
English-Mandarin speech emotion benchmark for adults and the elderly, BiMotion,
and a Cantonese speech emotion dataset, YueMotion. This study concludes that
different language and age groups require specific speech features, thus making
cross-lingual inference an unsuitable method. However, cross-group data
augmentation is still beneficial to regularize the model, with linguistic
distance being a significant influence on cross-lingual transferability. We
release publicly release our code at https://github.com/HLTCHKUST/elderly_ser.",None,-1
3ecb876a-eddd-43c8-ace6-29d91cea4c58,Active Reasoning in an Open-World Environment,0.114822,4,"Recent advances in vision-language learning have achieved notable success on
complete-information question-answering datasets through the integration of
extensive world knowledge. Yet, most models operate passively, responding to
questions based on pre-stored knowledge. In stark contrast, humans possess the
ability to actively explore, accumulate, and reason using both newfound and
existing information to tackle incomplete-information questions. In response to
this gap, we introduce $Conan$, an interactive open-world environment devised
for the assessment of active reasoning. $Conan$ facilitates active exploration
and promotes multi-round abductive inference, reminiscent of rich, open-world
settings like Minecraft. Diverging from previous works that lean primarily on
single-round deduction via instruction following, $Conan$ compels agents to
actively interact with their surroundings, amalgamating new evidence with prior
knowledge to elucidate events from incomplete observations. Our analysis on
$Conan$ underscores the shortcomings of contemporary state-of-the-art models in
active exploration and understanding complex scenarios. Additionally, we
explore Abduction from Deduction, where agents harness Bayesian rules to recast
the challenge of abduction as a deductive process. Through $Conan$, we aim to
galvanize advancements in active reasoning and set the stage for the next
generation of artificial intelligence agents adept at dynamically engaging in
environments.",None,-1
267f6654-785d-4059-8211-a303756d50a9,Semantic Feature Verification in FLAN-T5,0.35718,5,"This study evaluates the potential of a large language model for aiding in
generation of semantic feature norms - a critical tool for evaluating
conceptual structure in cognitive science. Building from an existing
human-generated dataset, we show that machine-verified norms capture aspects of
conceptual structure beyond what is expressed in human norms alone, and better
explain human judgments of semantic similarity amongst items that are distally
related. The results suggest that LLMs can greatly enhance traditional methods
of semantic feature norm verification, with implications for our understanding
of conceptual representation in humans and machines.",None,-1
223c87c8-b629-4f18-b33b-49121319666c,D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field,0.964879,15,"Realistic virtual humans play a crucial role in numerous industries, such as
metaverse, intelligent healthcare, and self-driving simulation. But creating
them on a large scale with high levels of realism remains a challenge. The
utilization of deep implicit function sparks a new era of image-based 3D
clothed human reconstruction, enabling pixel-aligned shape recovery with fine
details. Subsequently, the vast majority of works locate the surface by
regressing the deterministic implicit value for each point. However, should all
points be treated equally regardless of their proximity to the surface? In this
paper, we propose replacing the implicit value with an adaptive uncertainty
distribution, to differentiate between points based on their distance to the
surface. This simple ``value to distribution'' transition yields significant
improvements on nearly all the baselines. Furthermore, qualitative results
demonstrate that the models trained using our uncertainty distribution loss,
can capture more intricate wrinkles, and realistic limbs. Code and models are
available for research purposes at https://github.com/psyai-net/D-IF_release.",None,-1
ed833205-0386-4f2f-86e2-d16232a6f192,The AI Revolution: Opportunities and Challenges for the Finance Sector,0.60828,7,"This report examines Artificial Intelligence (AI) in the financial sector,
outlining its potential to revolutionise the industry and identify its
challenges. It underscores the criticality of a well-rounded understanding of
AI, its capabilities, and its implications to effectively leverage its
potential while mitigating associated risks. The potential of AI potential
extends from augmenting existing operations to paving the way for novel
applications in the finance sector. The application of AI in the financial
sector is transforming the industry. Its use spans areas from customer service
enhancements, fraud detection, and risk management to credit assessments and
high-frequency trading. However, along with these benefits, AI also presents
several challenges. These include issues related to transparency,
interpretability, fairness, accountability, and trustworthiness. The use of AI
in the financial sector further raises critical questions about data privacy
and security. A further issue identified in this report is the systemic risk
that AI can introduce to the financial sector. Being prone to errors, AI can
exacerbate existing systemic risks, potentially leading to financial crises.
Regulation is crucial to harnessing the benefits of AI while mitigating its
potential risks. Despite the global recognition of this need, there remains a
lack of clear guidelines or legislation for AI use in finance. This report
discusses key principles that could guide the formation of effective AI
regulation in the financial sector, including the need for a risk-based
approach, the inclusion of ethical considerations, and the importance of
maintaining a balance between innovation and consumer protection. The report
provides recommendations for academia, the finance industry, and regulators.",None,-1
0bf1f47e-92e1-462b-94f5-bf76db3dd1fe,SAINE: Scientific Annotation and Inference Engine of Scientific Research,0.0375697,1,"We present SAINE, an Scientific Annotation and Inference ENgine based on a
set of standard open-source software, such as Label Studio and MLflow. We show
that our annotation engine can benefit the further development of a more
accurate classification. Based on our previous work on hierarchical discipline
classifications, we demonstrate its application using SAINE in understanding
the space for scholarly publications. The user study of our annotation results
shows that user input collected with the help of our system can help us better
understand the classification process. We believe that our work will help to
foster greater transparency and better understand scientific research. Our
annotation and inference engine can further support the downstream meta-science
projects. We welcome collaboration and feedback from the scientific community
on these projects. The demonstration video can be accessed from
https://youtu.be/yToO-G9YQK4. A live demo website is available at
https://app.heartex.com/user/signup/?token=e2435a2f97449fa1 upon free
registration.",None,-1
2c8360c1-8065-4905-8146-3fc589ff8d97,Video Generation Beyond a Single Clip,0.0584624,1,"We tackle the long video generation problem, i.e.~generating videos beyond
the output length of video generation models. Due to the computation resource
constraints, video generation models can only generate video clips that are
relatively short compared with the length of real videos. Existing works apply
a sliding window approach to generate long videos at inference time, which is
often limited to generating recurrent events or homogeneous content. To
generate long videos covering diverse content and multiple events, we propose
to use additional guidance to control the video generation process. We further
present a two-stage approach to the problem, which allows us to utilize
existing video generation models to generate high-quality videos within a small
time window while modeling the video holistically based on the input guidance.
The proposed approach is complementary to existing efforts on video generation,
which focus on generating realistic video within a fixed time window. Extensive
experiments on challenging real-world videos validate the benefit of the
proposed method, which improves over state-of-the-art by up to 9.5% in
objective metrics and is preferred by users more than 80% of time.",None,-1
69c42f4a-bfda-471c-991b-54f7619da819,Black-Box Analysis: GPTs Across Time in Legal Textual Entailment Task,0.337034,1,"The evolution of Generative Pre-trained Transformer (GPT) models has led to
significant advancements in various natural language processing applications,
particularly in legal textual entailment. We present an analysis of GPT-3.5
(ChatGPT) and GPT-4 performances on COLIEE Task 4 dataset, a prominent
benchmark in this domain. The study encompasses data from Heisei 18 (2006) to
Reiwa 3 (2021), exploring the models' abilities to discern entailment
relationships within Japanese statute law across different periods. Our
preliminary experimental results unveil intriguing insights into the models'
strengths and weaknesses in handling legal textual entailment tasks, as well as
the patterns observed in model performance. In the context of proprietary
models with undisclosed architectures and weights, black-box analysis becomes
crucial for evaluating their capabilities. We discuss the influence of training
data distribution and the implications on the models' generalizability. This
analysis serves as a foundation for future research, aiming to optimize
GPT-based models and enable their successful adoption in legal information
extraction and entailment applications.",None,-1
da58a7f5-41c2-41c8-8c31-a3055f2a2d9c,C5: Towards Better Conversation Comprehension and Contextual Continuity for ChatGPT,0.100268,2,"Large language models (LLMs), such as ChatGPT, have demonstrated outstanding
performance in various fields, particularly in natural language understanding
and generation tasks. In complex application scenarios, users tend to engage in
multi-turn conversations with ChatGPT to keep contextual information and obtain
comprehensive responses. However, human forgetting and model contextual
forgetting remain prominent issues in multi-turn conversation scenarios, which
challenge the users' conversation comprehension and contextual continuity for
ChatGPT. To address these challenges, we propose an interactive conversation
visualization system called C5, which includes Global View, Topic View, and
Context-associated Q\&A View. The Global View uses the GitLog diagram metaphor
to represent the conversation structure, presenting the trend of conversation
evolution and supporting the exploration of locally salient features. The Topic
View is designed to display all the question and answer nodes and their
relationships within a topic using the structure of a knowledge graph, thereby
display the relevance and evolution of conversations. The Context-associated
Q\&A View consists of three linked views, which allow users to explore
individual conversations deeply while providing specific contextual information
when posing questions. The usefulness and effectiveness of C5 were evaluated
through a case study and a user study.",None,-1
06e28bd5-a0d0-4495-9bf3-dbc8a37009d1,DDS3D: Dense Pseudo-Labels with Dynamic Threshold for Semi-Supervised 3D Object Detection,0.815199,9,"In this paper, we present a simple yet effective semi-supervised 3D object
detector named DDS3D. Our main contributions have two-fold. On the one hand,
different from previous works using Non-Maximal Suppression (NMS) or its
variants for obtaining the sparse pseudo labels, we propose a dense
pseudo-label generation strategy to get dense pseudo-labels, which can retain
more potential supervision information for the student network. On the other
hand, instead of traditional fixed thresholds, we propose a dynamic threshold
manner to generate pseudo-labels, which can guarantee the quality and quantity
of pseudo-labels during the whole training process. Benefiting from these two
components, our DDS3D outperforms the state-of-the-art semi-supervised 3d
object detection with mAP of 3.1% on the pedestrian and 2.1% on the cyclist
under the same configuration of 1% samples. Extensive ablation studies on the
KITTI dataset demonstrate the effectiveness of our DDS3D. The code and models
will be made publicly available at https://github.com/hust-jy/DDS3D",None,-1
0d0ddd43-5d5e-41e8-a489-a895cd561372,FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions,0.999985,32,"Theory of mind (ToM) evaluations currently focus on testing models using
passive narratives that inherently lack interactivity. We introduce FANToM, a
new benchmark designed to stress-test ToM within information-asymmetric
conversational contexts via question answering. Our benchmark draws upon
important theoretical requisites from psychology and necessary empirical
considerations when evaluating large language models (LLMs). In particular, we
formulate multiple types of questions that demand the same underlying reasoning
to identify illusory or false sense of ToM capabilities in LLMs. We show that
FANToM is challenging for state-of-the-art LLMs, which perform significantly
worse than humans even with chain-of-thought reasoning or fine-tuning.",None,-1
e85e2f55-d372-48f7-b5b0-ed9f8adf61d8,An Empirical Study on the Transferability of Transformer Modules in Parameter-Efficient Fine-Tuning,0.0327163,1,"Parameter-efficient fine-tuning approaches have recently garnered a lot of
attention. Having considerably lower number of trainable weights, these methods
can bring about scalability and computational effectiveness. In this paper, we
look for optimal sub-networks and investigate the capability of different
transformer modules in transferring knowledge from a pre-trained model to a
downstream task. Our empirical results suggest that every transformer module in
BERT can act as a winning ticket: fine-tuning each specific module while
keeping the rest of the network frozen can lead to comparable performance to
the full fine-tuning. Among different modules, LayerNorms exhibit the best
capacity for knowledge transfer with limited trainable weights, to the extent
that, with only 0.003% of all parameters in the layer-wise analysis, they show
acceptable performance on various target tasks. On the reasons behind their
effectiveness, we argue that their notable performance could be attributed to
their high-magnitude weights compared to that of the other modules in the
pre-trained BERT.",None,-1
12597243-5fde-4188-82b9-8b116d51eb66,Design of JiuTian Intelligent Network Simulation Platform,0.517415,2,"This paper introduced the JiuTian Intelligent Network Simulation Platform,
which can provide wireless communication simulation data services for the Open
Innovation Platform. The platform contains a series of scalable simulator
functionalities, offering open services that enable users to use reinforcement
learning algorithms for model training and inference based on simulation
environments and data. Additionally, it allows users to address optimization
tasks in different scenarios by uploading and updating parameter
configurations. The platform and its open services were primarily introduced
from the perspectives of background, overall architecture, simulator, business
scenarios, and future directions.",None,-1
2ed32217-e002-4ad8-9e21-22659b3bf73b,Deliberate then Generate: Enhanced Prompting Framework for Text Generation,0.0756813,4,"Large language models (LLMs) have shown remarkable success across a wide
range of natural language generation tasks, where proper prompt designs make
great impacts. While existing prompting methods are normally restricted to
providing correct information, in this paper, we encourage the model to
deliberate by proposing a novel Deliberate then Generate (DTG) prompting
framework, which consists of error detection instructions and candidates that
may contain errors. DTG is a simple yet effective technique that can be applied
to various text generation tasks with minimal modifications. We conduct
extensive experiments on 20+ datasets across 7 text generation tasks, including
summarization, translation, dialogue, and more. We show that DTG consistently
outperforms existing prompting methods and achieves state-of-the-art
performance on multiple text generation tasks. We also provide in-depth
analyses to reveal the underlying mechanisms of DTG, which may inspire future
research on prompting for LLMs.",None,-1
bf1e83bb-842b-4bf6-ba85-a85e46585d68,Heterogeneous Graph Convolutional Neural Network via Hodge-Laplacian for Brain Functional Data,0.611651,4,"This study proposes a novel heterogeneous graph convolutional neural network
(HGCNN) to handle complex brain fMRI data at regional and across-region levels.
We introduce a generic formulation of spectral filters on heterogeneous graphs
by introducing the $k-th$ Hodge-Laplacian (HL) operator. In particular, we
propose Laguerre polynomial approximations of HL spectral filters and prove
that their spatial localization on graphs is related to the polynomial order.
Furthermore, based on the bijection property of boundary operators on simplex
graphs, we introduce a generic topological graph pooling (TGPool) method that
can be used at any dimensional simplices. This study designs HL-node, HL-edge,
and HL-HGCNN neural networks to learn signal representation at a graph node,
edge levels, and both, respectively. Our experiments employ fMRI from the
Adolescent Brain Cognitive Development (ABCD; n=7693) to predict general
intelligence. Our results demonstrate the advantage of the HL-edge network over
the HL-node network when functional brain connectivity is considered as
features. The HL-HGCNN outperforms the state-of-the-art graph neural networks
(GNNs) approaches, such as GAT, BrainGNN, dGCN, BrainNetCNN, and Hypergraph NN.
The functional connectivity features learned from the HL-HGCNN are meaningful
in interpreting neural circuits related to general intelligence.",None,-1
629a5548-306a-4a0f-904d-99e660a3ef54,Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery,0.999635,34,"Despite growing interest in using large language models (LLMs) in healthcare,
current explorations do not assess the real-world utility and safety of LLMs in
clinical settings. Our objective was to determine whether two LLMs can serve
information needs submitted by physicians as questions to an informatics
consultation service in a safe and concordant manner. Sixty six questions from
an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple
prompts. 12 physicians assessed the LLM responses' possibility of patient harm
and concordance with existing reports from an informatics consultation service.
Physician assessments were summarized based on majority vote. For no questions
did a majority of physicians deem either LLM response as harmful. For GPT-3.5,
responses to 8 questions were concordant with the informatics consult report,
20 discordant, and 9 were unable to be assessed. There were 29 responses with
no majority on ""Agree"", ""Disagree"", and ""Unable to assess"". For GPT-4,
responses to 13 questions were concordant, 15 discordant, and 3 were unable to
be assessed. There were 35 responses with no majority. Responses from both LLMs
were largely devoid of overt harm, but less than 20% of the responses agreed
with an answer from an informatics consultation service, responses contained
hallucinated references, and physicians were divided on what constitutes harm.
These results suggest that while general purpose LLMs are able to provide safe
and credible responses, they often do not meet the specific information need of
a given question. A definitive evaluation of the usefulness of LLMs in
healthcare settings will likely require additional research on prompt
engineering, calibration, and custom-tailoring of general purpose models.",None,-1
2430fde4-1a82-4f74-980f-b81db7f82fea,A theory for the sparsity emerged in the Forward Forward algorithm,0.64992,4,"This report explores the theory that explains the high sparsity phenomenon
\citep{tosato2023emergent} observed in the forward-forward algorithm
\citep{hinton2022forward}. The two theorems proposed predict the sparsity
changes of a single data point's activation in two cases: Theorem
\ref{theorem:1}: Decrease the goodness of the whole batch. Theorem
\ref{theorem:2}: Apply the complete forward forward algorithm to decrease the
goodness for negative data and increase the goodness for positive data. The
theory aligns well with the experiments tested on the MNIST dataset.",None,-1
57c82be7-1a2f-4817-a7ce-05097f58dfe7,Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning,0.210933,8,"We propose Pgx, a suite of board game reinforcement learning (RL)
environments written in JAX and optimized for GPU/TPU accelerators. By
leveraging JAX's auto-vectorization and parallelization over accelerators, Pgx
can efficiently scale to thousands of simultaneous simulations over
accelerators. In our experiments on a DGX-A100 workstation, we discovered that
Pgx can simulate RL environments 10-100x faster than existing implementations
available in Python. Pgx includes RL environments commonly used as benchmarks
in RL research, such as backgammon, chess, shogi, and Go. Additionally, Pgx
offers miniature game sets and baseline models to facilitate rapid research
cycles. We demonstrate the efficient training of the Gumbel AlphaZero algorithm
with Pgx environments. Overall, Pgx provides high-performance environment
simulators for researchers to accelerate their RL experiments. Pgx is available
at http://github.com/sotetsuk/pgx.",None,-1
0d0f03d9-6de2-4d9d-a5dc-84c64611fd44,Pre-training Multi-party Dialogue Models with Latent Discourse Inference,0.305514,2,"Multi-party dialogues are more difficult for models to understand than
one-to-one two-party dialogues, since they involve multiple interlocutors,
resulting in interweaving reply-to relations and information flows. To step
over these obstacles, an effective way is to pre-train a model that understands
the discourse structure of multi-party dialogues, namely, to whom each
utterance is replying. However, due to the lack of explicitly annotated
discourse labels in multi-party dialogue corpora, previous works fail to scale
up the pre-training process by putting aside the unlabeled multi-party
conversational data for nothing. To fully utilize the unlabeled data, we
propose to treat the discourse structures as latent variables, then jointly
infer them and pre-train the discourse-aware model by unsupervised latent
variable inference methods. Experiments on multiple downstream tasks show that
our pre-trained model outperforms strong baselines by large margins and
achieves state-of-the-art (SOTA) results, justifying the effectiveness of our
method. The official implementation of this paper is available at
https://github.com/EricLee8/MPD_EMVI.",None,-1
53a39c2c-0750-442f-87d3-bfd772258540,GeoAdapt: Self-Supervised Test-Time Adaptation in LiDAR Place Recognition Using Geometric Priors,0.451123,1,"LiDAR place recognition approaches based on deep learning suffer from
significant performance degradation when there is a shift between the
distribution of training and test datasets, often requiring re-training the
networks to achieve peak performance. However, obtaining accurate ground truth
data for new training data can be prohibitively expensive, especially in
complex or GPS-deprived environments. To address this issue we propose
GeoAdapt, which introduces a novel auxiliary classification head to generate
pseudo-labels for re-training on unseen environments in a self-supervised
manner. GeoAdapt uses geometric consistency as a prior to improve the
robustness of our generated pseudo-labels against domain shift, improving the
performance and reliability of our Test-Time Adaptation approach. Comprehensive
experiments show that GeoAdapt significantly boosts place recognition
performance across moderate to severe domain shifts, and is competitive with
fully supervised test-time adaptation approaches. Our code is available at
https://github.com/csiro-robotics/GeoAdapt.",None,-1
542a3a1b-ad7a-4881-91e5-1b2aa7511666,You Only Segment Once: Towards Real-Time Panoptic Segmentation,0.674976,34,"In this paper, we propose YOSO, a real-time panoptic segmentation framework.
YOSO predicts masks via dynamic convolutions between panoptic kernels and image
feature maps, in which you only need to segment once for both instance and
semantic segmentation tasks. To reduce the computational overhead, we design a
feature pyramid aggregator for the feature map extraction, and a separable
dynamic decoder for the panoptic kernel generation. The aggregator
re-parameterizes interpolation-first modules in a convolution-first way, which
significantly speeds up the pipeline without any additional costs. The decoder
performs multi-head cross-attention via separable dynamic convolution for
better efficiency and accuracy. To the best of our knowledge, YOSO is the first
real-time panoptic segmentation framework that delivers competitive performance
compared to state-of-the-art models. Specifically, YOSO achieves 46.4 PQ, 45.6
FPS on COCO; 52.5 PQ, 22.6 FPS on Cityscapes; 38.0 PQ, 35.4 FPS on ADE20K; and
34.1 PQ, 7.1 FPS on Mapillary Vistas. Code is available at
https://github.com/hujiecpp/YOSO.",None,-1
0399c74f-aad7-4799-98ad-4058eff9daa0,DeblurSR: Event-Based Motion Deblurring Under the Spiking Representation,0.436157,2,"We present DeblurSR, a novel motion deblurring approach that converts a
blurry image into a sharp video. DeblurSR utilizes event data to compensate for
motion ambiguities and exploits the spiking representation to parameterize the
sharp output video as a mapping from time to intensity. Our key contribution,
the Spiking Representation (SR), is inspired by the neuromorphic principles
determining how biological neurons communicate with each other in living
organisms. We discuss why the spikes can represent sharp edges and how the
spiking parameters are interpreted from the neuromorphic perspective. DeblurSR
has higher output quality and requires fewer computing resources than
state-of-the-art event-based motion deblurring methods. We additionally show
that our approach easily extends to video super-resolution when combined with
recent advances in implicit neural representation. The implementation and
animated visualization of DeblurSR are available at
https://github.com/chensong1995/DeblurSR.",None,-1
7d048acd-847d-4271-b87a-bf8974f00c0c,InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion,0.999822,52,"This paper addresses a novel task of anticipating 3D human-object
interactions (HOIs). Most existing research on HOI synthesis lacks
comprehensive whole-body interactions with dynamic objects, e.g., often limited
to manipulating small or static objects. Our task is significantly more
challenging, as it requires modeling dynamic objects with various shapes,
capturing whole-body motion, and ensuring physically valid interactions. To
this end, we propose InterDiff, a framework comprising two key steps: (i)
interaction diffusion, where we leverage a diffusion model to encode the
distribution of future human-object interactions; (ii) interaction correction,
where we introduce a physics-informed predictor to correct denoised HOIs in a
diffusion step. Our key insight is to inject prior knowledge that the
interactions under reference with respect to contact points follow a simple
pattern and are easily predictable. Experiments on multiple human-object
interaction datasets demonstrate the effectiveness of our method for this task,
capable of producing realistic, vivid, and remarkably long-term 3D HOI
predictions.",None,-1
7fd74d9e-7b6a-4a0e-b53c-8bab72038578,Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs,0.500376,23,"Large Language Models (LLMs) have demonstrated exceptional natural language
understanding abilities and have excelled in a variety of natural language
processing (NLP)tasks in recent years. Despite the fact that most LLMs are
trained predominantly in English, multiple studies have demonstrated their
comparative performance in many other languages. However, fundamental questions
persist regarding how LLMs acquire their multi-lingual abilities and how
performance varies across different languages. These inquiries are crucial for
the study of LLMs since users and researchers often come from diverse language
backgrounds, potentially influencing their utilization and interpretation of
LLMs' results. In this work, we propose a systematic way of qualifying the
performance disparities of LLMs under multilingual settings. We investigate the
phenomenon of across-language generalizations in LLMs, wherein insufficient
multi-lingual training data leads to advanced multi-lingual capabilities. To
accomplish this, we employ a novel back-translation-based prompting method. The
results show that GPT exhibits highly translating-like behaviour in
multilingual settings.",None,-1
9439bc72-46d3-40e9-a92f-59ce407c5f88,THUIR@COLIEE 2023: More Parameters and Legal Knowledge for Legal Case Entailment,0.98139,12,"This paper describes the approach of the THUIR team at the COLIEE 2023 Legal
Case Entailment task. This task requires the participant to identify a specific
paragraph from a given supporting case that entails the decision for the query
case. We try traditional lexical matching methods and pre-trained language
models with different sizes. Furthermore, learning-to-rank methods are employed
to further improve performance. However, learning-to-rank is not very robust on
this task. which suggests that answer passages cannot simply be determined with
information retrieval techniques. Experimental results show that more
parameters and legal knowledge contribute to the legal case entailment task.
Finally, we get the third place in COLIEE 2023. The implementation of our
method can be found at https://github.com/CSHaitao/THUIR-COLIEE2023.",None,-1
a4f09f50-20b3-4a6f-949d-10ed03eeaaec,Modeling Continuous Motion for 3D Point Cloud Object Tracking,0.243768,2,"The task of 3D single object tracking (SOT) with LiDAR point clouds is
crucial for various applications, such as autonomous driving and robotics.
However, existing approaches have primarily relied on appearance matching or
motion modeling within only two successive frames, thereby overlooking the
long-range continuous motion property of objects in 3D space. To address this
issue, this paper presents a novel approach that views each tracklet as a
continuous stream: at each timestamp, only the current frame is fed into the
network to interact with multi-frame historical features stored in a memory
bank, enabling efficient exploitation of sequential information. To achieve
effective cross-frame message passing, a hybrid attention mechanism is designed
to account for both long-range relation modeling and local geometric feature
extraction. Furthermore, to enhance the utilization of multi-frame features for
robust tracking, a contrastive sequence enhancement strategy is proposed, which
uses ground truth tracklets to augment training sequences and promote
discrimination against false positives in a contrastive manner. Extensive
experiments demonstrate that the proposed method outperforms the
state-of-the-art method by significant margins on multiple benchmarks.",None,-1
fada50da-e180-4d92-8cd7-4eb751760167,Discovering Universal Geometry in Embeddings with ICA,0.760512,6,"This study utilizes Independent Component Analysis (ICA) to unveil a
consistent semantic structure within embeddings of words or images. Our
approach extracts independent semantic components from the embeddings of a
pre-trained model by leveraging anisotropic information that remains after the
whitening process in Principal Component Analysis (PCA). We demonstrate that
each embedding can be expressed as a composition of a few intrinsic
interpretable axes and that these semantic axes remain consistent across
different languages, algorithms, and modalities. The discovery of a universal
semantic structure in the geometric patterns of embeddings enhances our
understanding of the representations in embeddings.",None,-1
60d87f95-7d9a-4e23-9bbf-2f2813b2a387,Weakly-Supervised Scientific Document Classification via Retrieval-Augmented Multi-Stage Training,0.952331,14,"Scientific document classification is a critical task for a wide range of
applications, but the cost of obtaining massive amounts of human-labeled data
can be prohibitive. To address this challenge, we propose a weakly-supervised
approach for scientific document classification using label names only. In
scientific domains, label names often include domain-specific concepts that may
not appear in the document corpus, making it difficult to match labels and
documents precisely. To tackle this issue, we propose WANDER, which leverages
dense retrieval to perform matching in the embedding space to capture the
semantics of label names. We further design the label name expansion module to
enrich the label name representations. Lastly, a self-training step is used to
refine the predictions. The experiments on three datasets show that WANDER
outperforms the best baseline by 11.9% on average. Our code will be published
at https://github.com/ritaranx/wander.",None,-1
47b0ef04-6b18-435a-9406-4aa5b441af7c,Towards Learning Rubik's Cube with N-tuple-based Reinforcement Learning,0.497613,3,"This work describes in detail how to learn and solve the Rubik's cube game
(or puzzle) in the General Board Game (GBG) learning and playing framework. We
cover the cube sizes 2x2x2 and 3x3x3. We describe in detail the cube's state
representation, how to transform it with twists, whole-cube rotations and color
transformations and explain the use of symmetries in Rubik's cube. Next, we
discuss different n-tuple representations for the cube, how we train the agents
by reinforcement learning and how we improve the trained agents during
evaluation by MCTS wrapping. We present results for agents that learn Rubik's
cube from scratch, with and without MCTS wrapping, with and without symmetries
and show that both, MCTS wrapping and symmetries, increase computational costs,
but lead at the same time to much better results. We can solve the 2x2x2 cube
completely, and the 3x3x3 cube in the majority of the cases for scrambled cubes
up to p = 15 (QTM). We cannot yet reliably solve 3x3x3 cubes with more than 15
scrambling twists. Although our computational costs are higher with MCTS
wrapping and with symmetries than without, they are still considerably lower
than in the approaches of McAleer et al. (2018, 2019) and Agostinelli et al.
(2019) who provide the best Rubik's cube learning agents so far.",None,-1
889b7024-bae6-4b51-a589-dd5300b3da7b,ColonMapper: topological mapping and localization for colonoscopy,0.0883493,1,"We propose a topological mapping and localization system able to operate on
real human colonoscopies, despite significant shape and illumination changes.
The map is a graph where each node codes a colon location by a set of real
images, while edges represent traversability between nodes. For close-in-time
images, where scene changes are minor, place recognition can be successfully
managed with the recent transformers-based local feature matching algorithms.
However, under long-term changes -- such as different colonoscopies of the same
patient -- feature-based matching fails. To address this, we train on real
colonoscopies a deep global descriptor achieving high recall with significant
changes in the scene. The addition of a Bayesian filter boosts the accuracy of
long-term place recognition, enabling relocalization in a previously built map.
Our experiments show that ColonMapper is able to autonomously build a map and
localize against it in two important use cases: localization within the same
colonoscopy or within different colonoscopies of the same patient. Code will be
available upon acceptance.",None,-1
9966df27-4beb-47a9-8498-8c60c579428e,Introducing Explicit Gaze Constraints to Face Swapping,0.174415,2,"Face swapping combines one face's identity with another face's non-appearance
attributes (expression, head pose, lighting) to generate a synthetic face. This
technology is rapidly improving, but falls flat when reconstructing some
attributes, particularly gaze. Image-based loss metrics that consider the full
face do not effectively capture the perceptually important, yet spatially
small, eye regions. Improving gaze in face swaps can improve naturalness and
realism, benefiting applications in entertainment, human computer interaction,
and more. Improved gaze will also directly improve Deepfake detection efforts,
serving as ideal training data for classifiers that rely on gaze for
classification. We propose a novel loss function that leverages gaze prediction
to inform the face swap model during training and compare against existing
methods. We find all methods to significantly benefit gaze in resulting face
swaps.",None,-1
0c30c813-58cd-4445-ba7e-e26bd6794224,EFE: End-to-end Frame-to-Gaze Estimation,0.260899,4,"Despite the recent development of learning-based gaze estimation methods,
most methods require one or more eye or face region crops as inputs and produce
a gaze direction vector as output. Cropping results in a higher resolution in
the eye regions and having fewer confounding factors (such as clothing and
hair) is believed to benefit the final model performance. However, this
eye/face patch cropping process is expensive, erroneous, and
implementation-specific for different methods. In this paper, we propose a
frame-to-gaze network that directly predicts both 3D gaze origin and 3D gaze
direction from the raw frame out of the camera without any face or eye
cropping. Our method demonstrates that direct gaze regression from the raw
downscaled frame, from FHD/HD to VGA/HVGA resolution, is possible despite the
challenges of having very few pixels in the eye region. The proposed method
achieves comparable results to state-of-the-art methods in Point-of-Gaze (PoG)
estimation on three public gaze datasets: GazeCapture, MPIIFaceGaze, and EVE,
and generalizes well to extreme camera view changes.",None,-1
3a61a1dd-5036-40f7-99d1-1d761dd59eb4,Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion,0.999822,23,"We propose a high-quality 3D-to-3D conversion method, Instruct 3D-to-3D. Our
method is designed for a novel task, which is to convert a given 3D scene to
another scene according to text instructions. Instruct 3D-to-3D applies
pretrained Image-to-Image diffusion models for 3D-to-3D conversion. This
enables the likelihood maximization of each viewpoint image and high-quality 3D
generation. In addition, our proposed method explicitly inputs the source 3D
scene as a condition, which enhances 3D consistency and controllability of how
much of the source 3D scene structure is reflected. We also propose dynamic
scaling, which allows the intensity of the geometry transformation to be
adjusted. We performed quantitative and qualitative evaluations and showed that
our proposed method achieves higher quality 3D-to-3D conversions than baseline
methods.",None,-1
ff159299-e68f-47a0-b1ba-44e377d49549,Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,0.972379,26,"Large Language Models (LLMs) excel in various Natural Language Processing
(NLP) tasks, yet their evaluation, particularly in languages beyond the top
$20$, remains inadequate due to existing benchmarks and metrics limitations.
Employing LLMs as evaluators to rank or score other models' outputs emerges as
a viable solution, addressing the constraints tied to human annotators and
established benchmarks. In this study, we explore the potential of LLM-based
evaluators, specifically GPT-4 in enhancing multilingual evaluation by
calibrating them against $20$K human judgments across three text-generation
tasks, five metrics, and eight languages. Our analysis reveals a bias in
GPT4-based evaluators towards higher scores, underscoring the necessity of
calibration with native speaker judgments, especially in low-resource and
non-Latin script languages, to ensure accurate evaluation of LLM performance
across diverse languages.",None,-1
7ca6ff44-eb56-412c-b8bb-7e81d38c7fc6,LLMLight: Large Language Models as Traffic Signal Control Agents,0.457935,1,"Traffic Signal Control (TSC) is a crucial component in urban traffic
management, aiming to optimize road network efficiency and reduce congestion.
Traditional methods in TSC, primarily based on transportation engineering and
reinforcement learning (RL), often exhibit limitations in generalization across
varied traffic scenarios and lack interpretability. This paper presents
LLMLight, a novel framework employing Large Language Models (LLMs) as
decision-making agents for TSC. Specifically, the framework begins by
instructing the LLM with a knowledgeable prompt detailing real-time traffic
conditions. Leveraging the advanced generalization capabilities of LLMs,
LLMLight engages a reasoning and decision-making process akin to human
intuition for effective traffic control. Moreover, we build LightGPT, a
specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic
patterns and control strategies, LightGPT enhances the LLMLight framework
cost-effectively. Extensive experiments on nine real-world and synthetic
datasets showcase the remarkable effectiveness, generalization ability, and
interpretability of LLMLight against nine transportation-based and RL-based
baselines.",None,-1
54ca2cae-70f0-48f7-91ef-88205789ea38,Morphological Inflection: A Reality Check,0.265067,1,"Morphological inflection is a popular task in sub-word NLP with both
practical and cognitive applications. For years now, state-of-the-art systems
have reported high, but also highly variable, performance across data sets and
languages. We investigate the causes of this high performance and high
variability; we find several aspects of data set creation and evaluation which
systematically inflate performance and obfuscate differences between languages.
To improve generalizability and reliability of results, we propose new data
sampling and evaluation strategies that better reflect likely use-cases. Using
these new strategies, we make new observations on the generalization abilities
of current inflection systems.",None,-1
7165044c-9738-4b53-b0f4-874651d871b8,Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators,0.692428,7,"The recent success of Large Language Models (LLMs) signifies an impressive
stride towards artificial general intelligence. They have shown a promising
prospect in automatically completing tasks upon user instructions, functioning
as brain-like coordinators. The associated risks will be revealed as we
delegate an increasing number of tasks to machines for automated completion. A
big question emerges: how can we make machines behave responsibly when helping
humans automate tasks as personal copilots? In this paper, we explore this
question in depth from the perspectives of feasibility, completeness and
security. In specific, we present Responsible Task Automation (ResponsibleTA)
as a fundamental framework to facilitate responsible collaboration between
LLM-based coordinators and executors for task automation with three empowered
capabilities: 1) predicting the feasibility of the commands for executors; 2)
verifying the completeness of executors; 3) enhancing the security (e.g., the
protection of users' privacy). We further propose and compare two paradigms for
implementing the first two capabilities. One is to leverage the generic
knowledge of LLMs themselves via prompt engineering while the other is to adopt
domain-specific learnable models. Moreover, we introduce a local memory
mechanism for achieving the third capability. We evaluate our proposed
ResponsibleTA on UI task automation and hope it could bring more attentions to
ensuring LLMs more responsible in diverse scenarios.",None,-1
d828ac3d-c108-44ac-a0fa-153d4034d198,DPPD: Deformable Polar Polygon Object Detection,0.039385,1,"Regular object detection methods output rectangle bounding boxes, which are
unable to accurately describe the actual object shapes. Instance segmentation
methods output pixel-level labels, which are computationally expensive for
real-time applications. Therefore, a polygon representation is needed to
achieve precise shape alignment, while retaining low computation cost. We
develop a novel Deformable Polar Polygon Object Detection method (DPPD) to
detect objects in polygon shapes. In particular, our network predicts, for each
object, a sparse set of flexible vertices to construct the polygon, where each
vertex is represented by a pair of angle and distance in the Polar coordinate
system. To enable training, both ground truth and predicted polygons are
densely resampled to have the same number of vertices with equal-spaced
raypoints. The resampling operation is fully differentable, allowing gradient
back-propagation. Sparse polygon predicton ensures high-speed runtime inference
while dense resampling allows the network to learn object shapes with high
precision. The polygon detection head is established on top of an anchor-free
and NMS-free network architecture. DPPD has been demonstrated successfully in
various object detection tasks for autonomous driving such as traffic-sign,
crosswalk, vehicle and pedestrian objects.",None,-1
78e15f7c-9ad1-40f0-ab1c-d4a89b743f7c,LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation,0.61298,27,"Existing automatic evaluation on text-to-image synthesis can only provide an
image-text matching score, without considering the object-level
compositionality, which results in poor correlation with human judgments. In
this work, we propose LLMScore, a new framework that offers evaluation scores
with multi-granularity compositionality. LLMScore leverages the large language
models (LLMs) to evaluate text-to-image models. Initially, it transforms the
image into image-level and object-level visual descriptions. Then an evaluation
instruction is fed into the LLMs to measure the alignment between the
synthesized image and the text, ultimately generating a score accompanied by a
rationale. Our substantial analysis reveals the highest correlation of LLMScore
with human judgments on a wide range of datasets (Attribute Binding Contrast,
Concept Conjunction, MSCOCO, DrawBench, PaintSkills). Notably, our LLMScore
achieves Kendall's tau correlation with human evaluations that is 58.8% and
31.2% higher than the commonly-used text-image matching metrics CLIP and BLIP,
respectively.",None,-1
291f247b-a7c0-4796-9ad5-3b5d7a195939,PSVT: End-to-End Multi-person 3D Pose and Shape Estimation with Progressive Video Transformers,0.310626,16,"Existing methods of multi-person video 3D human Pose and Shape Estimation
(PSE) typically adopt a two-stage strategy, which first detects human instances
in each frame and then performs single-person PSE with temporal model. However,
the global spatio-temporal context among spatial instances can not be captured.
In this paper, we propose a new end-to-end multi-person 3D Pose and Shape
estimation framework with progressive Video Transformer, termed PSVT. In PSVT,
a spatio-temporal encoder (STE) captures the global feature dependencies among
spatial objects. Then, spatio-temporal pose decoder (STPD) and shape decoder
(STSD) capture the global dependencies between pose queries and feature tokens,
shape queries and feature tokens, respectively. To handle the variances of
objects as time proceeds, a novel scheme of progressive decoding is used to
update pose and shape queries at each frame. Besides, we propose a novel
pose-guided attention (PGA) for shape decoder to better predict shape
parameters. The two components strengthen the decoder of PSVT to improve
performance. Extensive experiments on the four datasets show that PSVT achieves
stage-of-the-art results.",None,-1
9cbf8910-2164-45f1-bc41-8f767b55f21b,Being Right for Whose Right Reasons?,0.49845,6,"Explainability methods are used to benchmark the extent to which model
predictions align with human rationales i.e., are 'right for the right
reasons'. Previous work has failed to acknowledge, however, that what counts as
a rationale is sometimes subjective. This paper presents what we think is a
first of its kind, a collection of human rationale annotations augmented with
the annotators demographic information. We cover three datasets spanning
sentiment analysis and common-sense reasoning, and six demographic groups
(balanced across age and ethnicity). Such data enables us to ask both what
demographics our predictions align with and whose reasoning patterns our
models' rationales align with. We find systematic inter-group annotator
disagreement and show how 16 Transformer-based models align better with
rationales provided by certain demographic groups: We find that models are
biased towards aligning best with older and/or white annotators. We zoom in on
the effects of model size and model distillation, finding -- contrary to our
expectations -- negative correlations between model size and rationale
agreement as well as no evidence that either model size or model distillation
improves fairness.",None,-1
d48de97e-17f3-4d18-af08-ec4ba6be9b9c,SRFormer: Permuted Self-Attention for Single Image Super-Resolution,0.932902,33,"Previous works have shown that increasing the window size for
Transformer-based image super-resolution models (e.g., SwinIR) can
significantly improve the model performance but the computation overhead is
also considerable. In this paper, we present SRFormer, a simple but novel
method that can enjoy the benefit of large window self-attention but introduces
even less computational burden. The core of our SRFormer is the permuted
self-attention (PSA), which strikes an appropriate balance between the channel
and spatial information for self-attention. Our PSA is simple and can be easily
applied to existing super-resolution networks based on window self-attention.
Without any bells and whistles, we show that our SRFormer achieves a 33.86dB
PSNR score on the Urban100 dataset, which is 0.46dB higher than that of SwinIR
but uses fewer parameters and computations. We hope our simple and effective
approach can serve as a useful tool for future research in super-resolution
model design.",None,-1
9d59deed-5474-4ee0-a479-ab83d5a5ab4c,UstanceBR: a multimodal language resource for stance prediction,0.108154,2,"This work introduces UstanceBR, a multimodal corpus in the Brazilian
Portuguese Twitter domain for target-based stance prediction. The corpus
comprises 86.8 k labelled stances towards selected target topics, and extensive
network information about the users who published these stances on social
media. In this article we describe the corpus multimodal data, and a number of
usage examples in both in-domain and zero-shot stance prediction based on text-
and network-related information, which are intended to provide initial baseline
results for future studies in the field.",None,-1
304cfeb0-a6d6-41d2-a42a-3f4cf42e50ae,Text2Scene: Text-driven Indoor Scene Stylization with Part-aware Details,0.257852,8,"We propose Text2Scene, a method to automatically create realistic textures
for virtual scenes composed of multiple objects. Guided by a reference image
and text descriptions, our pipeline adds detailed texture on labeled 3D
geometries in the room such that the generated colors respect the hierarchical
structure or semantic parts that are often composed of similar materials.
Instead of applying flat stylization on the entire scene at a single step, we
obtain weak semantic cues from geometric segmentation, which are further
clarified by assigning initial colors to segmented parts. Then we add texture
details for individual objects such that their projections on image space
exhibit feature embedding aligned with the embedding of the input. The
decomposition makes the entire pipeline tractable to a moderate amount of
computation resources and memory. As our framework utilizes the existing
resources of image and text embedding, it does not require dedicated datasets
with high-quality textures designed by skillful artists. To the best of our
knowledge, it is the first practical and scalable approach that can create
detailed and realistic textures of the desired style that maintain structural
context for scenes with multiple objects.",None,-1
23095326-b8af-4b2e-876b-421ca7c71759,"Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased",0.923882,18,"There is a recent trend of applying multi-agent reinforcement learning (MARL)
to train an agent that can cooperate with humans in a zero-shot fashion without
using any human data. The typical workflow is to first repeatedly run self-play
(SP) to build a policy pool and then train the final adaptive policy against
this pool. A crucial limitation of this framework is that every policy in the
pool is optimized w.r.t. the environment reward function, which implicitly
assumes that the testing partners of the adaptive policy will be precisely
optimizing the same reward function as well. However, human objectives are
often substantially biased according to their own preferences, which can differ
greatly from the environment reward. We propose a more general framework,
Hidden-Utility Self-Play (HSP), which explicitly models human biases as hidden
reward functions in the self-play objective. By approximating the reward space
as linear functions, HSP adopts an effective technique to generate an augmented
policy pool with biased policies. We evaluate HSP on the Overcooked benchmark.
Empirical results show that our HSP method produces higher rewards than
baselines when cooperating with learned human models, manually scripted
policies, and real humans. The HSP policy is also rated as the most assistive
policy based on human feedback.",None,-1
4ce2c07f-c4ce-4fd4-890d-c2c9cc9888da,Just Tell Me: Prompt Engineering in Business Process Management,0.586599,14,"GPT-3 and several other language models (LMs) can effectively address various
natural language processing (NLP) tasks, including machine translation and text
summarization. Recently, they have also been successfully employed in the
business process management (BPM) domain, e.g., for predictive process
monitoring and process extraction from text. This, however, typically requires
fine-tuning the employed LM, which, among others, necessitates large amounts of
suitable training data. A possible solution to this problem is the use of
prompt engineering, which leverages pre-trained LMs without fine-tuning them.
Recognizing this, we argue that prompt engineering can help bring the
capabilities of LMs to BPM research. We use this position paper to develop a
research agenda for the use of prompt engineering for BPM research by
identifying the associated potentials and challenges.",None,-1
440df5e1-25f5-42bb-a56d-68586f58c4d9,Proportional Fairness in Obnoxious Facility Location,0.0758865,1,"We consider the obnoxious facility location problem (in which agents prefer
the facility location to be far from them) and propose a hierarchy of
distance-based proportional fairness concepts for the problem. These fairness
axioms ensure that groups of agents at the same location are guaranteed to be a
distance from the facility proportional to their group size. We consider
deterministic and randomized mechanisms, and compute tight bounds on the price
of proportional fairness. In the deterministic setting, not only are our
proportional fairness axioms incompatible with strategyproofness, the Nash
equilibria may not guarantee welfare within a constant factor of the optimal
welfare. On the other hand, in the randomized setting, we identify
proportionally fair and strategyproof mechanisms that give an expected welfare
within a constant factor of the optimal welfare.",None,-1
54133f17-a4ce-48c5-accb-9965a7ff4f4f,Towards Integration of Discriminability and Robustness for Document-Level Relation Extraction,0.775232,5,"Document-level relation extraction (DocRE) predicts relations for entity
pairs that rely on long-range context-dependent reasoning in a document. As a
typical multi-label classification problem, DocRE faces the challenge of
effectively distinguishing a small set of positive relations from the majority
of negative ones. This challenge becomes even more difficult to overcome when
there exists a significant number of annotation errors in the dataset. In this
work, we aim to achieve better integration of both the discriminability and
robustness for the DocRE problem. Specifically, we first design an effective
loss function to endow high discriminability to both probabilistic outputs and
internal representations. We innovatively customize entropy minimization and
supervised contrastive learning for the challenging multi-label and long-tailed
learning problems. To ameliorate the impact of label errors, we equipped our
method with a novel negative label sampling strategy to strengthen the model
robustness. In addition, we introduce two new data regimes to mimic more
realistic scenarios with annotation errors and evaluate our sampling strategy.
Experimental results verify the effectiveness of each component and show that
our method achieves new state-of-the-art results on the DocRED dataset, its
recently cleaned version, Re-DocRED, and the proposed data regimes.",None,-1
c6df7f26-5bc4-4e6d-9809-f7b5a972459e,"Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",0.999703,106,"The capability of Large Language Models (LLMs) like ChatGPT to comprehend
user intent and provide reasonable responses has made them extremely popular
lately. In this paper, we focus on assessing the overall ability of ChatGPT
using 7 fine-grained information extraction (IE) tasks. Specially, we present
the systematically analysis by measuring ChatGPT's performance, explainability,
calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT
or domain experts. Our findings reveal that ChatGPT's performance in
Standard-IE setting is poor, but it surprisingly exhibits excellent performance
in the OpenIE setting, as evidenced by human evaluation. In addition, our
research indicates that ChatGPT provides high-quality and trustworthy
explanations for its decisions. However, there is an issue of ChatGPT being
overconfident in its predictions, which resulting in low calibration.
Furthermore, ChatGPT demonstrates a high level of faithfulness to the original
text in the majority of cases. We manually annotate and release the test sets
of 7 fine-grained IE tasks contains 14 datasets to further promote the
research. The datasets and code are available at
https://github.com/pkuserc/ChatGPT_for_IE.",None,-1
6e40e3e7-3e00-415c-8c13-42ece462c361,Object Goal Navigation with Recursive Implicit Maps,0.790261,8,"Object goal navigation aims to navigate an agent to locations of a given
object category in unseen environments. Classical methods explicitly build maps
of environments and require extensive engineering while lacking semantic
information for object-oriented exploration. On the other hand, end-to-end
learning methods alleviate manual map design and predict actions using implicit
representations. Such methods, however, lack an explicit notion of geometry and
may have limited ability to encode navigation history. In this work, we propose
an implicit spatial map for object goal navigation. Our implicit map is
recursively updated with new observations at each step using a transformer. To
encourage spatial reasoning, we introduce auxiliary tasks and train our model
to reconstruct explicit maps as well as to predict visual features, semantic
labels and actions. Our method significantly outperforms the state of the art
on the challenging MP3D dataset and generalizes well to the HM3D dataset. We
successfully deploy our model on a real robot and achieve encouraging object
goal navigation results in real scenes using only a few real-world
demonstrations. Code, trained models and videos are available at
\url{https://www.di.ens.fr/willow/research/onav_rim/}.",None,-1
7606ff91-197f-4445-baca-366dc03e539b,Vanishing Activations: A Symptom of Deep Capsule Networks,0.168993,1,"Capsule Networks, an extension to Neural Networks utilizing vector or matrix
representations instead of scalars, were initially developed to create a
dynamic parse tree where visual concepts evolve from parts to complete objects.
Early implementations of Capsule Networks achieved and maintain
state-of-the-art results on various datasets. However, recent studies have
revealed shortcomings in the original Capsule Network architecture, notably its
failure to construct a parse tree and its susceptibility to vanishing gradients
when deployed in deeper networks. This paper extends the investigation to a
range of leading Capsule Network architectures, demonstrating that these issues
are not confined to the original design. We argue that the majority of Capsule
Network research has produced architectures that, while modestly divergent from
the original Capsule Network, still retain a fundamentally similar structure.
We posit that this inherent design similarity might be impeding the scalability
of Capsule Networks. Our study contributes to the broader discussion on
improving the robustness and scalability of Capsule Networks.",None,-1
dc2b99c6-da47-4a8c-8ba7-dd90bd856c80,Learning with Difference Attention for Visually Grounded Self-supervised Representations,0.0541987,1,"Recent works in self-supervised learning have shown impressive results on
single-object images, but they struggle to perform well on complex multi-object
images as evidenced by their poor visual grounding. To demonstrate this
concretely, we propose visual difference attention (VDA) to compute visual
attention maps in an unsupervised fashion by comparing an image with its
salient-regions-masked-out version. We use VDA to derive attention maps for
state-of-the art SSL methods and show they do not highlight all salient regions
in an image accurately, suggesting their inability to learn strong
representations for downstream tasks like segmentation. Motivated by these
limitations, we cast VDA as a differentiable operation and propose a new
learning objective, Differentiable Difference Attention (DiDA) loss, which
leads to substantial improvements in an SSL model's visually grounding to an
image's salient regions.",None,-1
415c983d-3718-49cb-9979-58cc79a6bc4d,Towards Local Visual Modeling for Image Captioning,0.738976,23,"In this paper, we study the local visual modeling with grid features for
image captioning, which is critical for generating accurate and detailed
captions. To achieve this target, we propose a Locality-Sensitive Transformer
Network (LSTNet) with two novel designs, namely Locality-Sensitive Attention
(LSA) and Locality-Sensitive Fusion (LSF). LSA is deployed for the intra-layer
interaction in Transformer via modeling the relationship between each grid and
its neighbors. It reduces the difficulty of local object recognition during
captioning. LSF is used for inter-layer information fusion, which aggregates
the information of different encoder layers for cross-layer semantical
complementarity. With these two novel designs, the proposed LSTNet can model
the local visual information of grid features to improve the captioning
quality. To validate LSTNet, we conduct extensive experiments on the
competitive MS-COCO benchmark. The experimental results show that LSTNet is not
only capable of local visual modeling, but also outperforms a bunch of
state-of-the-art captioning models on offline and online testings, i.e., 134.8
CIDEr and 136.3 CIDEr, respectively. Besides, the generalization of LSTNet is
also verified on the Flickr8k and Flickr30k datasets",None,-1
7930c180-034f-4801-9e1a-e5c83de2bb11,UOR: Universal Backdoor Attacks on Pre-trained Language Models,0.362869,8,"Backdoors implanted in pre-trained language models (PLMs) can be transferred
to various downstream tasks, which exposes a severe security threat. However,
most existing backdoor attacks against PLMs are un-targeted and task-specific.
Few targeted and task-agnostic methods use manually pre-defined triggers and
output representations, which prevent the attacks from being more effective and
general. In this paper, we first summarize the requirements that a more
threatening backdoor attack against PLMs should satisfy, and then propose a new
backdoor attack method called UOR, which breaks the bottleneck of the previous
approach by turning manual selection into automatic optimization. Specifically,
we define poisoned supervised contrastive learning which can automatically
learn the more uniform and universal output representations of triggers for
various PLMs. Moreover, we use gradient search to select appropriate trigger
words which can be adaptive to different PLMs and vocabularies. Experiments
show that our method can achieve better attack performance on various text
classification tasks compared to manual methods. Further, we tested our method
on PLMs with different architectures, different usage paradigms, and more
difficult tasks, which demonstrated the universality of our method.",None,-1
b6e12116-5756-4b6e-a705-34ce83116e72,"Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors",0.994309,64,"ChatGPT has stimulated the research boom in the field of large language
models. In this paper, we assess the capabilities of ChatGPT from four
perspectives including Performance, Evaluation Criteria, Robustness and Error
Types. Specifically, we first evaluate ChatGPT's performance on 17 datasets
with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought
scenarios, and find a huge performance gap between ChatGPT and SOTA results.
Next, we rethink this gap and propose a soft-matching strategy for evaluation
to more accurately reflect ChatGPT's performance. Then, we analyze the
robustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely
outputs invalid responses; 2) Irrelevant context and long-tail target types
greatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the
subject-object relationships in RE task. Finally, we analyze the errors of
ChatGPT, and find that ""unannotated spans"" is the most dominant error type.
This raises concerns about the quality of annotated data, and indicates the
possibility of annotating data with ChatGPT. The data and code are released at
Github site.",None,-1
cbf2a8a9-4a86-4c72-b509-6fd405fed49f,Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge Graphs,0.710906,5,"Knowledge graph embeddings (KGE) have been extensively studied to embed
large-scale relational data for many real-world applications. Existing methods
have long ignored the fact many KGs contain two fundamentally different views:
high-level ontology-view concepts and fine-grained instance-view entities. They
usually embed all nodes as vectors in one latent space. However, a single
geometric representation fails to capture the structural differences between
two views and lacks probabilistic semantics towards concepts' granularity. We
propose Concept2Box, a novel approach that jointly embeds the two views of a KG
using dual geometric representations. We model concepts with box embeddings,
which learn the hierarchy structure and complex relations such as overlap and
disjoint among them. Box volumes can be interpreted as concepts' granularity.
Different from concepts, we model entities as vectors. To bridge the gap
between concept box embeddings and entity vector embeddings, we propose a novel
vector-to-box distance metric and learn both embeddings jointly. Experiments on
both the public DBpedia KG and a newly-created industrial KG showed the
effectiveness of Concept2Box.",None,-1
c12cc9e8-f223-4d0d-8fcd-991b9b8394cf,MVDream: Multi-view Diffusion for 3D Generation,1.0,278,"We introduce MVDream, a diffusion model that is able to generate consistent
multi-view images from a given text prompt. Learning from both 2D and 3D data,
a multi-view diffusion model can achieve the generalizability of 2D diffusion
models and the consistency of 3D renderings. We demonstrate that such a
multi-view diffusion model is implicitly a generalizable 3D prior agnostic to
3D representations. It can be applied to 3D generation via Score Distillation
Sampling, significantly enhancing the consistency and stability of existing
2D-lifting methods. It can also learn new concepts from a few 2D examples, akin
to DreamBooth, but for 3D generation.",None,-1
69f860f7-5e69-4c6a-925b-35a79bcbc085,Beyond the Prior Forgery Knowledge: Mining Critical Clues for General Face Forgery Detection,0.757616,15,"Face forgery detection is essential in combating malicious digital face
attacks. Previous methods mainly rely on prior expert knowledge to capture
specific forgery clues, such as noise patterns, blending boundaries, and
frequency artifacts. However, these methods tend to get trapped in local
optima, resulting in limited robustness and generalization capability. To
address these issues, we propose a novel Critical Forgery Mining (CFM)
framework, which can be flexibly assembled with various backbones to boost
their generalization and robustness performance. Specifically, we first build a
fine-grained triplet and suppress specific forgery traces through prior
knowledge-agnostic data augmentation. Subsequently, we propose a fine-grained
relation learning prototype to mine critical information in forgeries through
instance and local similarity-aware losses. Moreover, we design a novel
progressive learning controller to guide the model to focus on principal
feature components, enabling it to learn critical forgery features in a
coarse-to-fine manner. The proposed method achieves state-of-the-art forgery
detection performance under various challenging evaluation settings.",None,-1
3ccf6df5-de07-4c9e-b9f2-8dc3ea376d36,Dynamic Interactive Relation Capturing via Scene Graph Learning for Robotic Surgical Report Generation,0.382921,2,"For robot-assisted surgery, an accurate surgical report reflects clinical
operations during surgery and helps document entry tasks, post-operative
analysis and follow-up treatment. It is a challenging task due to many complex
and diverse interactions between instruments and tissues in the surgical scene.
Although existing surgical report generation methods based on deep learning
have achieved large success, they often ignore the interactive relation between
tissues and instrumental tools, thereby degrading the report generation
performance. This paper presents a neural network to boost surgical report
generation by explicitly exploring the interactive relation between tissues and
surgical instruments. We validate the effectiveness of our method on a
widely-used robotic surgery benchmark dataset, and experimental results show
that our network can significantly outperform existing state-of-the-art
surgical report generation methods (e.g., 7.48% and 5.43% higher for BLEU-1 and
ROUGE).",None,-1
bf221179-b501-440b-a3d9-0cbdb34c42bf,Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation,0.980614,35,"Self-supervised and language-supervised image models contain rich knowledge
of the world that is important for generalization. Many robotic tasks, however,
require a detailed understanding of 3D geometry, which is often lacking in 2D
image features. This work bridges this 2D-to-3D gap for robotic manipulation by
leveraging distilled feature fields to combine accurate 3D geometry with rich
semantics from 2D foundation models. We present a few-shot learning method for
6-DOF grasping and placing that harnesses these strong spatial and semantic
priors to achieve in-the-wild generalization to unseen objects. Using features
distilled from a vision-language model, CLIP, we present a way to designate
novel objects for manipulation via free-text natural language, and demonstrate
its ability to generalize to unseen expressions and novel categories of
objects.",None,-1
c4dd38e8-f97c-46c3-ad46-b493b830a814,Delving into Crispness: Guided Label Refinement for Crisp Edge Detection,0.238936,2,"Learning-based edge detection usually suffers from predicting thick edges.
Through extensive quantitative study with a new edge crispness measure, we find
that noisy human-labeled edges are the main cause of thick predictions. Based
on this observation, we advocate that more attention should be paid on label
quality than on model design to achieve crisp edge detection. To this end, we
propose an effective Canny-guided refinement of human-labeled edges whose
result can be used to train crisp edge detectors. Essentially, it seeks for a
subset of over-detected Canny edges that best align human labels. We show that
several existing edge detectors can be turned into a crisp edge detector
through training on our refined edge maps. Experiments demonstrate that deep
models trained with refined edges achieve significant performance boost of
crispness from 17.4% to 30.6%. With the PiDiNet backbone, our method improves
ODS and OIS by 12.2% and 12.6% on the Multicue dataset, respectively, without
relying on non-maximal suppression. We further conduct experiments and show the
superiority of our crisp edge detection for optical flow estimation and image
segmentation.",None,-1
a076f502-fae9-45da-a78d-1d398030f506,Toolformer: Language Models Can Teach Themselves to Use Tools,1.0,950,"Language models (LMs) exhibit remarkable abilities to solve new tasks from
just a few examples or textual instructions, especially at scale. They also,
paradoxically, struggle with basic functionality, such as arithmetic or factual
lookup, where much simpler and smaller models excel. In this paper, we show
that LMs can teach themselves to use external tools via simple APIs and achieve
the best of both worlds. We introduce Toolformer, a model trained to decide
which APIs to call, when to call them, what arguments to pass, and how to best
incorporate the results into future token prediction. This is done in a
self-supervised way, requiring nothing more than a handful of demonstrations
for each API. We incorporate a range of tools, including a calculator, a Q\&A
system, two different search engines, a translation system, and a calendar.
Toolformer achieves substantially improved zero-shot performance across a
variety of downstream tasks, often competitive with much larger models, without
sacrificing its core language modeling abilities.",None,-1
c2b746a5-8d32-4b80-b5cb-17becb344dc6,Legal Extractive Summarization of U.S. Court Opinions,0.438252,2,"This paper tackles the task of legal extractive summarization using a dataset
of 430K U.S. court opinions with key passages annotated. According to automated
summary quality metrics, the reinforcement-learning-based MemSum model is best
and even out-performs transformer-based models. In turn, expert human
evaluation shows that MemSum summaries effectively capture the key points of
lengthy court opinions. Motivated by these results, we open-source our models
to the general public. This represents progress towards democratizing law and
making U.S. court opinions more accessible to the general public.",None,-1
97c0606f-3d4e-4649-96ec-80f09a47416b,Reference-based Painterly Inpainting via Diffusion: Crossing the Wild Reference Domain Gap,0.459294,3,"Have you ever imagined how it would look if we placed new objects into
paintings? For example, what would it look like if we placed a basketball into
Claude Monet's ``Water Lilies, Evening Effect''? We propose Reference-based
Painterly Inpainting, a novel task that crosses the wild reference domain gap
and implants novel objects into artworks. Although previous works have examined
reference-based inpainting, they are not designed for large domain
discrepancies between the target and the reference, such as inpainting an
artistic image using a photorealistic reference. This paper proposes a novel
diffusion framework, dubbed RefPaint, to ``inpaint more wildly'' by taking such
references with large domain gaps. Built with an image-conditioned diffusion
model, we introduce a ladder-side branch and a masked fusion mechanism to work
with the inpainting mask. By decomposing the CLIP image embeddings at inference
time, one can manipulate the strength of semantic and style information with
ease. Experiments demonstrate that our proposed RefPaint framework produces
significantly better results than existing methods. Our method enables creative
painterly image inpainting with reference objects that would otherwise be
difficult to achieve. Project page: https://vita-group.github.io/RefPaint/",None,-1
a279cea7-c91b-4be6-86ec-b64dc9750b26,ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation,0.0514558,4,"Paraphrase generation is a long-standing task in natural language processing
(NLP). Supervised paraphrase generation models, which rely on human-annotated
paraphrase pairs, are cost-inefficient and hard to scale up. On the other hand,
automatically annotated paraphrase pairs (e.g., by machine back-translation),
usually suffer from the lack of syntactic diversity -- the generated paraphrase
sentences are very similar to the source sentences in terms of syntax. In this
work, we present ParaAMR, a large-scale syntactically diverse paraphrase
dataset created by abstract meaning representation back-translation. Our
quantitative analysis, qualitative examples, and human evaluation demonstrate
that the paraphrases of ParaAMR are syntactically more diverse compared to
existing large-scale paraphrase datasets while preserving good semantic
similarity. In addition, we show that ParaAMR can be used to improve on three
NLP tasks: learning sentence embeddings, syntactically controlled paraphrase
generation, and data augmentation for few-shot learning. Our results thus
showcase the potential of ParaAMR for improving various NLP applications.",None,-1
0c137951-b38d-45dd-a2ac-a34f8c522fb6,InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree Neural Radiance Fields,0.295256,14,"We propose InNeRF360, an automatic system that accurately removes
text-specified objects from 360-degree Neural Radiance Fields (NeRF). The
challenge is to effectively remove objects while inpainting perceptually
consistent content for the missing regions, which is particularly demanding for
existing NeRF models due to their implicit volumetric representation. Moreover,
unbounded scenes are more prone to floater artifacts in the inpainted region
than frontal-facing scenes, as the change of object appearance and background
across views is more sensitive to inaccurate segmentations and inconsistent
inpainting. With a trained NeRF and a text description, our method efficiently
removes specified objects and inpaints visually consistent content without
artifacts. We apply depth-space warping to enforce consistency across multiview
text-encoded segmentations, and then refine the inpainted NeRF model using
perceptual priors and 3D diffusion-based geometric priors to ensure visual
plausibility. Through extensive experiments in segmentation and inpainting on
360-degree and frontal-facing NeRFs, we show that our approach is effective and
enhances NeRF's editability. Project page: https://ivrl.github.io/InNeRF360.",None,-1
7947b3a2-fe91-408d-b891-3940bfa43aab,GridFormer: Residual Dense Transformer with Grid Structure for Image Restoration in Adverse Weather Conditions,0.532998,6,"Image restoration in adverse weather conditions is a difficult task in
computer vision. In this paper, we propose a novel transformer-based framework
called GridFormer which serves as a backbone for image restoration under
adverse weather conditions. GridFormer is designed in a grid structure using a
residual dense transformer block, and it introduces two core designs. First, it
uses an enhanced attention mechanism in the transformer layer. The mechanism
includes stages of the sampler and compact self-attention to improve
efficiency, and a local enhancement stage to strengthen local information.
Second, we introduce a residual dense transformer block (RDTB) as the final
GridFormer layer. This design further improves the network's ability to learn
effective features from both preceding and current local features. The
GridFormer framework achieves state-of-the-art results on five diverse image
restoration tasks in adverse weather conditions, including image deraining,
dehazing, deraining \& dehazing, desnowing, and multi-weather restoration. The
source code and pre-trained models are available at
https://github.com/TaoWangzj/GridFormer.",None,-1
28033602-1634-4430-864a-2f730c15d2b7,Digital Twin Applications in Urban Logistics: An Overview,0.650761,7,"Urban traffic attributed to commercial and industrial transportation is
observed to largely affect living standards in cities due to external effects
pertaining to pollution and congestion. In order to counter this, smart cities
deploy technological tools to achieve sustainability. Such tools include
Digital Twins (DT)s which are virtual replicas of real-life physical systems.
Research suggests that DTs can be very beneficial in how they control a
physical system by constantly optimizing its performance. The concept has been
extensively studied in other technology-driven industries like manufacturing.
However, little work has been done with regards to their application in urban
logistics. In this paper, we seek to provide a framework by which DTs could be
easily adapted to urban logistics networks. To do this, we provide a
characterization of key factors in urban logistics for dynamic decision-making.
We also survey previous research on DT applications in urban logistics as we
found that a holistic overview is lacking. Using this knowledge in combination
with the characterization, we produce a conceptual model that describes the
ontology, learning capabilities and optimization prowess of an urban logistics
digital twin through its quantitative models. We finish off with a discussion
on potential research benefits and limitations based on previous research and
our practical experience.",None,-1
b5677d4a-d9d9-423c-afd3-038ea0edcf60,Enhancing Explainability in Mobility Data Science through a combination of methods,0.473248,1,"In the domain of Mobility Data Science, the intricate task of interpreting
models trained on trajectory data, and elucidating the spatio-temporal movement
of entities, has persistently posed significant challenges. Conventional XAI
techniques, although brimming with potential, frequently overlook the distinct
structure and nuances inherent within trajectory data. Observing this
deficiency, we introduced a comprehensive framework that harmonizes pivotal XAI
techniques: LIME (Local Interpretable Model-agnostic Explanations), SHAP
(SHapley Additive exPlanations), Saliency maps, attention mechanisms, direct
trajectory visualization, and Permutation Feature Importance (PFI). Unlike
conventional strategies that deploy these methods singularly, our unified
approach capitalizes on the collective efficacy of these techniques, yielding
deeper and more granular insights for models reliant on trajectory data. In
crafting this synthesis, we effectively address the multifaceted essence of
trajectories, achieving not only amplified interpretability but also a nuanced,
contextually rich comprehension of model decisions. To validate and enhance our
framework, we undertook a survey to gauge preferences and reception among
various user demographics. Our findings underscored a dichotomy: professionals
with academic orientations, particularly those in roles like Data Scientist, IT
Expert, and ML Engineer, showcased a profound, technical understanding and
often exhibited a predilection for amalgamated methods for interpretability.
Conversely, end-users or individuals less acquainted with AI and Data Science
showcased simpler inclinations, such as bar plots indicating timestep
significance or visual depictions pinpointing pivotal segments of a vessel's
trajectory.",None,-1
0dd1a542-75e5-45a6-8380-af052f1df85c,'That Darned Sandstorm': A Study of Procedural Generation through Archaeological Storytelling,0.0795551,2,"Procedural content generation has been applied to many domains, especially
level design, but the narrative affordances of generated game environments are
comparatively understudied. In this paper we present our first attempt to study
these effects through the lens of what we call a generative archaeology game
that prompts the player to archaeologically interpret the generated content of
the game world. We report on a survey that gathered qualitative and
quantitative data on the experiences of 187 participants playing the game
Nothing Beside Remains. We provide some preliminary analysis of our intentional
attempt to prompt player interpretation, and the unintentional effects of a
glitch on the player experience of the game.",None,-1
31fae4e1-089c-490e-9137-948126da6785,Polarimetric Imaging for Perception,0.165233,1,"Autonomous driving and advanced driver-assistance systems rely on a set of
sensors and algorithms to perform the appropriate actions and provide alerts as
a function of the driving scene. Typically, the sensors include color cameras,
radar, lidar and ultrasonic sensors. Strikingly however, although light
polarization is a fundamental property of light, it is seldom harnessed for
perception tasks. In this work we analyze the potential for improvement in
perception tasks when using an RGB-polarimetric camera, as compared to an RGB
camera. We examine monocular depth estimation and free space detection during
the middle of the day, when polarization is independent of subject heading, and
show that a quantifiable improvement can be achieved for both of them using
state-of-the-art deep neural networks, with a minimum of architectural changes.
We also present a new dataset composed of RGB-polarimetric images, lidar scans,
GNSS / IMU readings and free space segmentations that further supports
developing perception algorithms that take advantage of light polarization.",None,-1
c6fdf647-42b5-4166-91cf-343e9a3df92d,Writing user personas with Large Language Models: Testing phase 6 of a Thematic Analysis of semi-structured interviews,0.530553,5,"The goal of this paper is establishing if we can satisfactorily perform a
Thematic Analysis (TA) of semi-structured interviews using a Large Language
Model (more precisely GPT3.5-Turbo). Building on previous work by the author,
which established an embryonal process for conducting a TA with the model, this
paper will perform a further analysis and then cover the last phase of a TA
(phase 6), which entails the writing up of the result. This phase was not
covered by the previous work. In particular, the focus will be on using the
results of a TA done with the LLM on a dataset of user interviews, for writing
user personas, with the model building on the TA to produce the personas
narratives. User personas are models of real users, usually built from a data
analysis like interviews with a sample of users. User personas are tools often
used in User Centered Design processes. The paper shows that the model can
build basic user personas with an acceptable quality deriving them from themes,
and that the model can serve for the generation of ideas for user personas.",None,-1
1fda0296-43ce-410f-b692-3191130480d8,Single-view Neural Radiance Fields with Depth Teacher,0.0697325,2,"Neural Radiance Fields (NeRF) have been proposed for photorealistic novel
view rendering. However, it requires many different views of one scene for
training. Moreover, it has poor generalizations to new scenes and requires
retraining or fine-tuning on each scene. In this paper, we develop a new NeRF
model for novel view synthesis using only a single image as input. We propose
to combine the (coarse) planar rendering and the (fine) volume rendering to
achieve higher rendering quality and better generalizations. We also design a
depth teacher net that predicts dense pseudo depth maps to supervise the joint
rendering mechanism and boost the learning of consistent 3D geometry. We
evaluate our method on three challenging datasets. It outperforms
state-of-the-art single-view NeRFs by achieving 5$\sim$20\% improvements in
PSNR and reducing 20$\sim$50\% of the errors in the depth rendering. It also
shows excellent generalization abilities to unseen data without the need to
fine-tune on each new scene.",None,-1
e7080e40-8b24-43b9-8a41-cd11d68b5d99,PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations,0.632673,2,"PokerKit is an open-source Python library designed to overcome the
restrictions of existing poker game simulation and hand evaluation tools, which
typically support only a handful of poker variants and lack flexibility in game
state control. In contrast, PokerKit significantly expands this scope by
supporting an extensive array of poker variants and it provides a flexible
architecture for users to define their custom games. This paper details the
design and implementation of PokerKit, including its intuitive programmatic
API, multi-variant game support, and a unified hand evaluation suite across
different hand types. The flexibility of PokerKit allows for applications in
diverse areas, such as poker AI development, tool creation, and online poker
casino implementation. PokerKit's reliability has been established through
static type checking, extensive doctests, and unit tests, achieving 99% code
coverage. The introduction of PokerKit represents a significant contribution to
the field of computer poker, fostering future research and advanced AI
development for a wide variety of poker games. The source code is available at
https://github.com/uoftcprg/pokerkit",None,-1
1cfb4514-d513-48f1-aef0-0e3d2d0657f3,Dr.ICL: Demonstration-Retrieved In-context Learning,0.647619,30,"In-context learning (ICL), teaching a large language model (LLM) to perform a
task with few-shot demonstrations rather than adjusting the model parameters,
has emerged as a strong paradigm for using LLMs. While early studies primarily
used a fixed or random set of demonstrations for all test queries, recent
research suggests that retrieving semantically similar demonstrations to the
input from a pool of available demonstrations results in better performance.
This work expands the applicability of retrieval-based ICL approaches by
demonstrating that even simple word-overlap similarity measures such as BM25
outperform randomly selected demonstrations. Furthermore, we extend the success
of retrieval-based ICL to instruction-finetuned LLMs as well as
Chain-of-Thought (CoT) prompting. For instruction-finetuned LLMs, we find that
although a model has already seen the training data at training time,
retrieving demonstrations from the training data at test time yields better
results compared to using no demonstrations or random demonstrations. Last but
not least, we train a task-specific demonstration retriever that outperforms
off-the-shelf retrievers.",None,-1
17380dd1-77b1-478b-af7a-d8c40f3b0ff9,An Information-Theoretic Perspective on Variance-Invariance-Covariance Regularization,0.432141,13,"Variance-Invariance-Covariance Regularization (VICReg) is a self-supervised
learning (SSL) method that has shown promising results on a variety of tasks.
However, the fundamental mechanisms underlying VICReg remain unexplored. In
this paper, we present an information-theoretic perspective on the VICReg
objective. We begin by deriving information-theoretic quantities for
deterministic networks as an alternative to unrealistic stochastic network
assumptions. We then relate the optimization of the VICReg objective to mutual
information optimization, highlighting underlying assumptions and facilitating
a constructive comparison with other SSL algorithms and derive a generalization
bound for VICReg, revealing its inherent advantages for downstream tasks.
Building on these results, we introduce a family of SSL methods derived from
information-theoretic principles that outperform existing SSL techniques.",None,-1
d7c1f0f8-569d-4b6a-b5e6-a754cdb50fe4,TabLib: A Dataset of 627M Tables with Context,0.607764,4,"It is well-established that large, diverse datasets play a pivotal role in
the performance of modern AI systems for text and image modalities. However,
there are no datasets for tabular data of comparable size and diversity to
those available for text and images. Thus we present ""TabLib'', a compilation
of 627 million tables totaling 69 TiB, along with 867B tokens of context.
TabLib was extracted from numerous file formats, including CSV, HTML, SQLite,
PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and
diversity of TabLib offer considerable promise in the table modality,
reminiscent of the original promise of foundational datasets for text and
images, such as The Pile and LAION.",None,-1
c6b2def0-7a7b-45cb-9ec9-550347dd6ed4,Handwritten Text Recognition from Crowdsourced Annotations,0.293011,3,"In this paper, we explore different ways of training a model for handwritten
text recognition when multiple imperfect or noisy transcriptions are available.
We consider various training configurations, such as selecting a single
transcription, retaining all transcriptions, or computing an aggregated
transcription from all available annotations. In addition, we evaluate the
impact of quality-based data selection, where samples with low agreement are
removed from the training set. Our experiments are carried out on municipal
registers of the city of Belfort (France) written between 1790 and 1946. %
results The results show that computing a consensus transcription or training
on multiple transcriptions are good alternatives. However, selecting training
samples based on the degree of agreement between annotators introduces a bias
in the training data and does not improve the results. Our dataset is publicly
available on Zenodo: https://zenodo.org/record/8041668.",None,-1
914bbe7d-1888-4358-9a6e-21023f0f9b42,Adaptive Sparse Convolutional Networks with Global Context Enhancement for Faster Object Detection on Drone Images,0.407987,14,"Object detection on drone images with low-latency is an important but
challenging task on the resource-constrained unmanned aerial vehicle (UAV)
platform. This paper investigates optimizing the detection head based on the
sparse convolution, which proves effective in balancing the accuracy and
efficiency. Nevertheless, it suffers from inadequate integration of contextual
information of tiny objects as well as clumsy control of the mask ratio in the
presence of foreground with varying scales. To address the issues above, we
propose a novel global context-enhanced adaptive sparse convolutional network
(CEASC). It first develops a context-enhanced group normalization (CE-GN)
layer, by replacing the statistics based on sparsely sampled features with the
global contextual ones, and then designs an adaptive multi-layer masking
strategy to generate optimal mask ratios at distinct scales for compact
foreground coverage, promoting both the accuracy and efficiency. Extensive
experimental results on two major benchmarks, i.e. VisDrone and UAVDT,
demonstrate that CEASC remarkably reduces the GFLOPs and accelerates the
inference procedure when plugging into the typical state-of-the-art detection
frameworks (e.g. RetinaNet and GFL V1) with competitive performance. Code is
available at https://github.com/Cuogeihong/CEASC.",None,-1
2bb35a4b-2c24-4940-a12b-49fc148255be,Mesh Strikes Back: Fast and Efficient Human Reconstruction from RGB videos,0.188949,2,"Human reconstruction and synthesis from monocular RGB videos is a challenging
problem due to clothing, occlusion, texture discontinuities and sharpness, and
framespecific pose changes. Many methods employ deferred rendering, NeRFs and
implicit methods to represent clothed humans, on the premise that mesh-based
representations cannot capture complex clothing and textures from RGB,
silhouettes, and keypoints alone. We provide a counter viewpoint to this
fundamental premise by optimizing a SMPL+D mesh and an efficient,
multi-resolution texture representation using only RGB images, binary
silhouettes and sparse 2D keypoints. Experimental results demonstrate that our
approach is more capable of capturing geometric details compared to visual
hull, mesh-based methods. We show competitive novel view synthesis and
improvements in novel pose synthesis compared to NeRF-based methods, which
introduce noticeable, unwanted artifacts. By restricting the solution space to
the SMPL+D model combined with differentiable rendering, we obtain dramatic
speedups in compute, training times (up to 24x) and inference times (up to
192x). Our method therefore can be used as is or as a fast initialization to
NeRF-based methods.",None,-1
1cbc4a7a-3ffb-4082-99e7-adf8cd962be6,Near Optimal Memory-Regret Tradeoff for Online Learning,0.31712,6,"In the experts problem, on each of $T$ days, an agent needs to follow the
advice of one of $n$ ``experts''. After each day, the loss associated with each
expert's advice is revealed. A fundamental result in learning theory says that
the agent can achieve vanishing regret, i.e. their cumulative loss is within
$o(T)$ of the cumulative loss of the best-in-hindsight expert.
  Can the agent perform well without sufficient space to remember all the
experts? We extend a nascent line of research on this question in two
directions:
  $\bullet$ We give a new algorithm against the oblivious adversary, improving
over the memory-regret tradeoff obtained by [PZ23], and nearly matching the
lower bound of [SWXZ22].
  $\bullet$ We also consider an adaptive adversary who can observe past experts
chosen by the agent. In this setting we give both a new algorithm and a novel
lower bound, proving that roughly $\sqrt{n}$ memory is both necessary and
sufficient for obtaining $o(T)$ regret.",None,-1
6989a9d6-a0a7-42a7-acb8-e6fbe9411fb6,Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm,0.619823,3,"Contextual biasing refers to the problem of biasing the automatic speech
recognition (ASR) systems towards rare entities that are relevant to the
specific user or application scenarios. We propose algorithms for contextual
biasing based on the Knuth-Morris-Pratt algorithm for pattern matching. During
beam search, we boost the score of a token extension if it extends matching
into a set of biasing phrases. Our method simulates the classical approaches
often implemented in the weighted finite state transducer (WFST) framework, but
avoids the FST language altogether, with careful considerations on memory
footprint and efficiency on tensor processing units (TPUs) by vectorization.
Without introducing additional model parameters, our method achieves
significant word error rate (WER) reductions on biasing test sets by itself,
and yields further performance gain when combined with a model-based biasing
method.",None,-1
392c2d8e-cad1-4e27-a494-81521e0e5920,A Hierarchical Destroy and Repair Approach for Solving Very Large-Scale Travelling Salesman Problem,0.414693,1,"For prohibitively large-scale Travelling Salesman Problems (TSPs), existing
algorithms face big challenges in terms of both computational efficiency and
solution quality. To address this issue, we propose a hierarchical
destroy-and-repair (HDR) approach, which attempts to improve an initial
solution by applying a series of carefully designed destroy-and-repair
operations. A key innovative concept is the hierarchical search framework,
which recursively fixes partial edges and compresses the input instance into a
small-scale TSP under some equivalence guarantee. This neat search framework is
able to deliver highly competitive solutions within a reasonable time. Fair
comparisons based on nineteen famous large-scale instances (with 10,000 to
10,000,000 cities) show that HDR is highly competitive against existing
state-of-the-art TSP algorithms, in terms of both efficiency and solution
quality. Notably, on two large instances with 3,162,278 and 10,000,000 cities,
HDR breaks the world records (i.e., best-known results regardless of
computation time), which were previously achieved by LKH and its variants,
while HDR is completely independent of LKH. Finally, ablation studies are
performed to certify the importance and validity of the hierarchical search
framework.",None,-1
77157f50-e50f-4fdd-b2d9-53158d5b4194,Neural Airport Ground Handling,0.773387,5,"Airport ground handling (AGH) offers necessary operations to flights during
their turnarounds and is of great importance to the efficiency of airport
management and the economics of aviation. Such a problem involves the interplay
among the operations that leads to NP-hard problems with complex constraints.
Hence, existing methods for AGH are usually designed with massive domain
knowledge but still fail to yield high-quality solutions efficiently. In this
paper, we aim to enhance the solution quality and computation efficiency for
solving AGH. Particularly, we first model AGH as a multiple-fleet vehicle
routing problem (VRP) with miscellaneous constraints including precedence, time
windows, and capacity. Then we propose a construction framework that decomposes
AGH into sub-problems (i.e., VRPs) in fleets and present a neural method to
construct the routing solutions to these sub-problems. In specific, we resort
to deep learning and parameterize the construction heuristic policy with an
attention-based neural network trained with reinforcement learning, which is
shared across all sub-problems. Extensive experiments demonstrate that our
method significantly outperforms classic meta-heuristics, construction
heuristics and the specialized methods for AGH. Besides, we empirically verify
that our neural method generalizes well to instances with large numbers of
flights or varying parameters, and can be readily adapted to solve real-time
AGH with stochastic flight arrivals. Our code is publicly available at:
https://github.com/RoyalSkye/AGH.",None,-1
257796c3-abbe-4490-ad5c-7f5e64e529c2,Object pop-up: Can we infer 3D objects and their poses from human interactions alone?,0.61709,21,"The intimate entanglement between objects affordances and human poses is of
large interest, among others, for behavioural sciences, cognitive psychology,
and Computer Vision communities. In recent years, the latter has developed
several object-centric approaches: starting from items, learning pipelines
synthesizing human poses and dynamics in a realistic way, satisfying both
geometrical and functional expectations. However, the inverse perspective is
significantly less explored: Can we infer 3D objects and their poses from human
interactions alone? Our investigation follows this direction, showing that a
generic 3D human point cloud is enough to pop up an unobserved object, even
when the user is just imitating a functionality (e.g., looking through a
binocular) without involving a tangible counterpart. We validate our method
qualitatively and quantitatively, with synthetic data and sequences acquired
for the task, showing applicability for XR/VR. The code is available at
https://github.com/ptrvilya/object-popup.",None,-1
7d796c05-9e9d-4609-8cff-bce828af39fe,ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes,0.99998,55,"We present ScanNet++, a large-scale dataset that couples together capture of
high-quality and commodity-level geometry and color of indoor scenes. Each
scene is captured with a high-end laser scanner at sub-millimeter resolution,
along with registered 33-megapixel images from a DSLR camera, and RGB-D streams
from an iPhone. Scene reconstructions are further annotated with an open
vocabulary of semantics, with label-ambiguous scenarios explicitly annotated
for comprehensive semantic understanding. ScanNet++ enables a new real-world
benchmark for novel view synthesis, both from high-quality RGB capture, and
importantly also from commodity-level images, in addition to a new benchmark
for 3D semantic scene understanding that comprehensively encapsulates diverse
and ambiguous semantic labeling scenarios. Currently, ScanNet++ contains 460
scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.",None,-1
271a8f82-9684-4ff9-bce9-caa1ec65221e,Applying HCAI in developing effective human-AI teaming: A perspective from human-AI joint cognitive systems,0.282023,4,"Research and application have used human-AI teaming (HAT) as a new paradigm
to develop AI systems. HAT recognizes that AI will function as a teammate
instead of simply a tool in collaboration with humans. Effective human-AI teams
need to be capable of taking advantage of the unique abilities of both humans
and AI while overcoming the known challenges and limitations of each member,
augmenting human capabilities, and raising joint performance beyond that of
either entity. The National AI Research and Strategic Plan 2023 update has
recognized that research programs focusing primarily on the independent
performance of AI systems generally fail to consider the functionality that AI
must provide within the context of dynamic, adaptive, and collaborative teams
and calls for further research on human-AI teaming and collaboration. However,
there has been debate about whether AI can work as a teammate with humans. The
primary concern is that adopting the ""teaming"" paradigm contradicts the
human-centered AI (HCAI) approach, resulting in humans losing control of AI
systems. This article further analyzes the HAT paradigm and the debates.
Specifically, we elaborate on our proposed conceptual framework of human-AI
joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI
umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The
implications and future work for HAIJCS are also discussed.
  Insights: AI has led to the emergence of a new form of human-machine
relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems;
We must follow a human-centered AI (HCAI) approach when applying HAT as a new
design paradigm; We propose a conceptual framework of human-AI joint cognitive
systems (HAIJCS) to represent and implement HAT for developing effective
human-AI teaming",None,-1
da18d7e1-17fd-44d3-8435-db73727a3863,"Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges",0.577713,5,"Multi-agent reinforcement learning (MARL) is a widely used Artificial
Intelligence (AI) technique. However, current studies and applications need to
address its scalability, non-stationarity, and trustworthiness. This paper aims
to review methods and applications and point out research trends and visionary
prospects for the next decade. First, this paper summarizes the basic methods
and application scenarios of MARL. Second, this paper outlines the
corresponding research methods and their limitations on safety, robustness,
generalization, and ethical constraints that need to be addressed in the
practical applications of MARL. In particular, we believe that trustworthy MARL
will become a hot research topic in the next decade. In addition, we suggest
that considering human interaction is essential for the practical application
of MARL in various societies. Therefore, this paper also analyzes the
challenges while MARL is applied to human-machine interaction.",None,-1
562269a2-096e-4f6f-a209-aff373bf7c90,Made of Steel? Learning Plausible Materials for Components in the Vehicle Repair Domain,0.213652,2,"We propose a novel approach to learn domain-specific plausible materials for
components in the vehicle repair domain by probing Pretrained Language Models
(PLMs) in a cloze task style setting to overcome the lack of annotated
datasets. We devise a new method to aggregate salient predictions from a set of
cloze query templates and show that domain-adaptation using either a small,
high-quality or a customized Wikipedia corpus boosts performance. When
exploring resource-lean alternatives, we find a distilled PLM clearly
outperforming a classic pattern-based algorithm. Further, given that 98% of our
domain-specific components are multiword expressions, we successfully exploit
the compositionality assumption as a way to address data sparsity.",None,-1
1dbfd773-e470-4314-a184-b739e8423035,The Gradient of Generative AI Release: Methods and Considerations,0.871335,68,"As increasingly powerful generative AI systems are developed, the release
method greatly varies. We propose a framework to assess six levels of access to
generative AI systems: fully closed; gradual or staged access; hosted access;
cloud-based or API access; downloadable access; and fully open. Each level,
from fully closed to fully open, can be viewed as an option along a gradient.
We outline key considerations across this gradient: release methods come with
tradeoffs, especially around the tension between concentrating power and
mitigating risks. Diverse and multidisciplinary perspectives are needed to
examine and mitigate risk in generative AI systems from conception to
deployment. We show trends in generative system release over time, noting
closedness among large companies for powerful systems and openness among
organizations founded on principles of openness. We also enumerate safety
controls and guardrails for generative systems and necessary investments to
improve future releases.",None,-1
b96ed7e3-2cb9-40cf-905d-3c214ff4fe6f,"HealthEdge: A Machine Learning-Based Smart Healthcare Framework for Prediction of Type 2 Diabetes in an Integrated IoT, Edge, and Cloud Computing System",0.725276,12,"Diabetes Mellitus has no permanent cure to date and is one of the leading
causes of death globally. The alarming increase in diabetes calls for the need
to take precautionary measures to avoid/predict the occurrence of diabetes.
This paper proposes HealthEdge, a machine learning-based smart healthcare
framework for type 2 diabetes prediction in an integrated IoT-edge-cloud
computing system. Numerical experiments and comparative analysis were carried
out between the two most used machine learning algorithms in the literature,
Random Forest (RF) and Logistic Regression (LR), using two real-life diabetes
datasets. The results show that RF predicts diabetes with 6% more accuracy on
average compared to LR.",None,-1
7fe2167f-c3a6-4981-9bcf-177bbbac33cb,DFormer: Diffusion-guided Transformer for Universal Image Segmentation,0.318751,6,"This paper introduces an approach, named DFormer, for universal image
segmentation. The proposed DFormer views universal image segmentation task as a
denoising process using a diffusion model. DFormer first adds various levels of
Gaussian noise to ground-truth masks, and then learns a model to predict
denoising masks from corrupted masks. Specifically, we take deep pixel-level
features along with the noisy masks as inputs to generate mask features and
attention masks, employing diffusion-based decoder to perform mask prediction
gradually. At inference, our DFormer directly predicts the masks and
corresponding categories from a set of randomly-generated masks. Extensive
experiments reveal the merits of our proposed contributions on different image
segmentation tasks: panoptic segmentation, instance segmentation, and semantic
segmentation. Our DFormer outperforms the recent diffusion-based panoptic
segmentation method Pix2Seq-D with a gain of 3.6% on MS COCO val2017 set.
Further, DFormer achieves promising semantic segmentation performance
outperforming the recent diffusion-based method by 2.2% on ADE20K val set. Our
source code and models will be publicly on https://github.com/cp3wan/DFormer",None,-1
4d139a46-4e8c-4a09-95be-f8ec7b82c333,Face0: Instantaneously Conditioning a Text-to-Image Model on a Face,0.60605,30,"We present Face0, a novel way to instantaneously condition a text-to-image
generation model on a face, in sample time, without any optimization procedures
such as fine-tuning or inversions. We augment a dataset of annotated images
with embeddings of the included faces and train an image generation model, on
the augmented dataset. Once trained, our system is practically identical at
inference time to the underlying base model, and is therefore able to generate
images, given a user-supplied face image and a prompt, in just a couple of
seconds. Our method achieves pleasing results, is remarkably simple, extremely
fast, and equips the underlying model with new capabilities, like controlling
the generated images both via text or via direct manipulation of the input face
embeddings. In addition, when using a fixed random vector instead of a face
embedding from a user supplied image, our method essentially solves the problem
of consistent character generation across images. Finally, while requiring
further research, we hope that our method, which decouples the model's textual
biases from its biases on faces, might be a step towards some mitigation of
biases in future text-to-image models.",None,-1
fe7246af-5505-4b40-b3b1-c2816c746b05,A Boosted Model Ensembling Approach to Ball Action Spotting in Videos: The Runner-Up Solution to CVPR'23 SoccerNet Challenge,0.761796,2,"This technical report presents our solution to Ball Action Spotting in
videos. Our method reached second place in the CVPR'23 SoccerNet Challenge.
Details of this challenge can be found at
https://www.soccer-net.org/tasks/ball-action-spotting. Our approach is
developed based on a baseline model termed E2E-Spot, which was provided by the
organizer of this competition. We first generated several variants of the
E2E-Spot model, resulting in a candidate model set. We then proposed a strategy
for selecting appropriate model members from this set and assigning an
appropriate weight to each model. The aim of this strategy is to boost the
performance of the resulting model ensemble. Therefore, we call our approach
Boosted Model Ensembling (BME). Our code is available at
https://github.com/ZJLAB-AMMI/E2E-Spot-MBS.",None,-1
e747df7a-941c-40f9-97a9-51f41514fceb,Building Safe and Reliable AI systems for Safety Critical Tasks with Vision-Language Processing,0.043528,1,"Although AI systems have been applied in various fields and achieved
impressive performance, their safety and reliability are still a big concern.
This is especially important for safety-critical tasks. One shared
characteristic of these critical tasks is their risk sensitivity, where small
mistakes can cause big consequences and even endanger life. There are several
factors that could be guidelines for the successful deployment of AI systems in
sensitive tasks: (i) failure detection and out-of-distribution (OOD) detection;
(ii) overfitting identification; (iii) uncertainty quantification for
predictions; (iv) robustness to data perturbations. These factors are also
challenges of current AI systems, which are major blocks for building safe and
reliable AI. Specifically, the current AI algorithms are unable to identify
common causes for failure detection. Furthermore, additional techniques are
required to quantify the quality of predictions. All these contribute to
inaccurate uncertainty quantification, which lowers trust in predictions. Hence
obtaining accurate model uncertainty quantification and its further improvement
are challenging. To address these issues, many techniques have been proposed,
such as regularization methods and learning strategies. As vision and language
are the most typical data type and have many open source benchmark datasets,
this thesis will focus on vision-language data processing for tasks like
classification, image captioning, and vision question answering. In this
thesis, we aim to build a safeguard by further developing current techniques to
ensure the accurate model uncertainty for safety-critical tasks.",None,-1
2897a212-6841-4850-981d-7823a159bbd0,Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation,0.324049,5,"Hallucination of text ungrounded in the input is a well-known problem in
neural data-to-text generation. Many methods have been proposed to mitigate it,
but they typically require altering model architecture or collecting additional
data, and thus cannot be easily applied to an existing model. In this paper, we
explore a new way to mitigate hallucinations by combining the probabilistic
output of a generator language model (LM) with the output of a special ""text
critic"" classifier, which guides the generation by assessing the match between
the input data and the text generated so far. Our method does not need any
changes to the underlying LM's architecture or training procedure and can thus
be combined with any model and decoding operating on word probabilities. The
critic does not need any additional training data, using the base LM's training
data and synthetic negative examples. Our experimental results show that our
method improves over the baseline on the WebNLG and OpenDialKG benchmarks.",None,-1
e0b7c82f-cd6b-4957-888e-6370e6a6c967,Exploring Deep Models for Practical Gait Recognition,0.829775,13,"Gait recognition is a rapidly advancing vision technique for person
identification from a distance. Prior studies predominantly employed relatively
shallow networks to extract subtle gait features, achieving impressive
successes in constrained settings. Nevertheless, experiments revealed that
existing methods mostly produce unsatisfactory results when applied to newly
released real-world gait datasets. This paper presents a unified perspective to
explore how to construct deep models for state-of-the-art outdoor gait
recognition, including the classical CNN-based and emerging Transformer-based
architectures. Specifically, we challenge the stereotype of shallow gait models
and demonstrate the superiority of explicit temporal modeling and deep
transformer structure for discriminative gait representation learning.
Consequently, the proposed CNN-based DeepGaitV2 series and Transformer-based
SwinGait series exhibit significant performance improvements on Gait3D and
GREW. As for the constrained gait datasets, the DeepGaitV2 series also reaches
a new state-of-the-art in most cases, convincingly showing its practicality and
generality. The source code is available at
https://github.com/ShiqiYu/OpenGait.",None,-1
c1d1bfe2-f840-482a-bf0b-a53e3e372e34,Improving Autoregressive NLP Tasks via Modular Linearized Attention,0.0328582,1,"Various natural language processing (NLP) tasks necessitate models that are
efficient and small based on their ultimate application at the edge or in other
resource-constrained environments. While prior research has reduced the size of
these models, increasing computational efficiency without considerable
performance impacts remains difficult, especially for autoregressive tasks.
This paper proposes modular linearized attention (MLA), which combines multiple
efficient attention mechanisms, including cosFormer, to maximize inference
quality while achieving notable speedups. We validate this approach on several
autoregressive NLP tasks, including speech-to-text neural machine translation
(S2T NMT), speech-to-text simultaneous translation (SimulST), and
autoregressive text-to-spectrogram, noting efficiency gains on TTS and
competitive performance for NMT and SimulST during training and inference.",None,-1
75005647-a870-4954-83da-38116af62be9,MultiMediate'23: Engagement Estimation and Bodily Behaviour Recognition in Social Interactions,0.901612,8,"Automatic analysis of human behaviour is a fundamental prerequisite for the
creation of machines that can effectively interact with- and support humans in
social interactions. In MultiMediate'23, we address two key human social
behaviour analysis tasks for the first time in a controlled challenge:
engagement estimation and bodily behaviour recognition in social interactions.
This paper describes the MultiMediate'23 challenge and presents novel sets of
annotations for both tasks. For engagement estimation we collected novel
annotations on the NOvice eXpert Interaction (NOXI) database. For bodily
behaviour recognition, we annotated test recordings of the MPIIGroupInteraction
corpus with the BBSI annotation scheme. In addition, we present baseline
results for both challenge tasks.",None,-1
7b195a41-8fe1-40f3-86b5-e5fedcb09bfc,Segment Anything Meets Semantic Communication,0.738594,9,"In light of the diminishing returns of traditional methods for enhancing
transmission rates, the domain of semantic communication presents promising new
frontiers. Focusing on image transmission, this paper explores the application
of foundation models, particularly the Segment Anything Model (SAM) developed
by Meta AI Research, to improve semantic communication. SAM is a promptable
image segmentation model that has gained attention for its ability to perform
zero-shot segmentation tasks without explicit training or domain-specific
knowledge. By employing SAM's segmentation capability and lightweight neural
network architecture for semantic coding, we propose a practical approach to
semantic communication. We demonstrate that this approach retains critical
semantic features, achieving higher image reconstruction quality and reducing
communication overhead. This practical solution eliminates the
resource-intensive stage of training a segmentation model and can be applied to
any semantic coding architecture, paving the way for real-world applications.",None,-1
039677f5-c0da-4f4a-8c5d-c8134a60f4ef,"With Shared Microexponents, A Little Shifting Goes a Long Way",0.871078,20,"This paper introduces Block Data Representations (BDR), a framework for
exploring and evaluating a wide spectrum of narrow-precision formats for deep
learning. It enables comparison of popular quantization standards, and through
BDR, new formats based on shared microexponents (MX) are identified, which
outperform other state-of-the-art quantization approaches, including
narrow-precision floating-point and block floating-point. MX utilizes multiple
levels of quantization scaling with ultra-fine scaling factors based on shared
microexponents in the hardware. The effectiveness of MX is demonstrated on
real-world models including large-scale generative pretraining and inferencing,
and production-scale recommendation systems.",None,-1
1b3e987a-35d8-4b3e-a248-a593b01ed829,"Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts for Zero-Shot Dialogue State Tracking",0.451024,6,"Zero-shot transfer learning for Dialogue State Tracking (DST) helps to handle
a variety of task-oriented dialogue domains without the cost of collecting
in-domain data. Existing works mainly study common data- or model-level
augmentation methods to enhance the generalization but fail to effectively
decouple the semantics of samples, limiting the zero-shot performance of DST.
In this paper, we present a simple and effective ""divide, conquer and combine""
solution, which explicitly disentangles the semantics of seen data, and
leverages the performance and robustness with the mixture-of-experts mechanism.
Specifically, we divide the seen data into semantically independent subsets and
train corresponding experts, the newly unseen samples are mapped and inferred
with mixture-of-experts with our designed ensemble inference. Extensive
experiments on MultiWOZ2.1 upon the T5-Adapter show our schema significantly
and consistently improves the zero-shot performance, achieving the SOTA on
settings without external knowledge, with only 10M trainable parameters1.",None,-1
4bf660a3-079a-477e-864b-520625f23442,Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis,1.0,195,"We present a method that simultaneously addresses the tasks of dynamic scene
novel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense
scene elements. We follow an analysis-by-synthesis framework, inspired by
recent work that models scenes as a collection of 3D Gaussians which are
optimized to reconstruct input images via differentiable rendering. To model
dynamic scenes, we allow Gaussians to move and rotate over time while enforcing
that they have persistent color, opacity, and size. By regularizing Gaussians'
motion and rotation with local-rigidity constraints, we show that our Dynamic
3D Gaussians correctly model the same area of physical space over time,
including the rotation of that space. Dense 6-DOF tracking and dynamic
reconstruction emerges naturally from persistent dynamic view synthesis,
without requiring any correspondence or flow as input. We demonstrate a large
number of downstream applications enabled by our representation, including
first-person view synthesis, dynamic compositional scene synthesis, and 4D
video editing.",None,-1
f7060004-6a44-4321-afe2-790c7a3e538d,SynthASpoof: Developing Face Presentation Attack Detection Based on Privacy-friendly Synthetic Data,0.993694,14,"Recently, significant progress has been made in face presentation attack
detection (PAD), which aims to secure face recognition systems against
presentation attacks, owing to the availability of several face PAD datasets.
However, all available datasets are based on privacy and legally-sensitive
authentic biometric data with a limited number of subjects. To target these
legal and technical challenges, this work presents the first synthetic-based
face PAD dataset, named SynthASpoof, as a large-scale PAD development dataset.
The bona fide samples in SynthASpoof are synthetically generated and the attack
samples are collected by presenting such synthetic data to capture systems in a
real attack scenario. The experimental results demonstrate the feasibility of
using SynthASpoof for the development of face PAD. Moreover, we boost the
performance of such a solution by incorporating the domain generalization tool
MixStyle into the PAD solutions. Additionally, we showed the viability of using
synthetic data as a supplement to enrich the diversity of limited authentic
training data and consistently enhance PAD performances. The SynthASpoof
dataset, containing 25,000 bona fide and 78,800 attack samples, the
implementation, and the pre-trained weights are made publicly available.",None,-1
0415ef53-a091-45d2-95f6-fe34f4f6ed27,Mimetic Initialization of Self-Attention Layers,0.61913,20,"It is notoriously difficult to train Transformers on small datasets;
typically, large pre-trained models are instead used as the starting point. We
explore the weights of such pre-trained Transformers (particularly for vision)
to attempt to find reasons for this discrepancy. Surprisingly, we find that
simply initializing the weights of self-attention layers so that they ""look""
more like their pre-trained counterparts allows us to train vanilla
Transformers faster and to higher final accuracies, particularly on vision
tasks such as CIFAR-10 and ImageNet classification, where we see gains in
accuracy of over 5% and 4%, respectively. Our initialization scheme is closed
form, learning-free, and very simple: we set the product of the query and key
weights to be approximately the identity, and the product of the value and
projection weights to approximately the negative identity. As this mimics the
patterns we saw in pre-trained Transformers, we call the technique ""mimetic
initialization"".",None,-1
d519fa45-3a4d-4f66-8e1b-88c27529827c,SSCBench: Monocular 3D Semantic Scene Completion Benchmark in Street Views,0.505358,5,"Monocular scene understanding is a foundational component of autonomous
systems. Within the spectrum of monocular perception topics, one crucial and
useful task for holistic 3D scene understanding is semantic scene completion
(SSC), which jointly completes semantic information and geometric details from
RGB input. However, progress in SSC, particularly in large-scale street views,
is hindered by the scarcity of high-quality datasets. To address this issue, we
introduce SSCBench, a comprehensive benchmark that integrates scenes from
widely used automotive datasets (e.g., KITTI-360, nuScenes, and Waymo).
SSCBench follows an established setup and format in the community, facilitating
the easy exploration of SSC methods in various street views. We benchmark
models using monocular, trinocular, and point cloud input to assess the
performance gap resulting from sensor coverage and modality. Moreover, we have
unified semantic labels across diverse datasets to simplify cross-domain
generalization testing. We commit to including more datasets and SSC models to
drive further advancements in this field.",None,-1
f43bfed1-91ad-4695-b8b5-0ff8cd4899a1,Learning to Predict Navigational Patterns from Partial Observations,0.0954803,1,"Human beings cooperatively navigate rule-constrained environments by adhering
to mutually known navigational patterns, which may be represented as
directional pathways or road lanes. Inferring these navigational patterns from
incompletely observed environments is required for intelligent mobile robots
operating in unmapped locations. However, algorithmically defining these
navigational patterns is nontrivial. This paper presents the first
self-supervised learning (SSL) method for learning to infer navigational
patterns in real-world environments from partial observations only. We explain
how geometric data augmentation, predictive world modeling, and an
information-theoretic regularizer enables our model to predict an unbiased
local directional soft lane probability (DSLP) field in the limit of infinite
data. We demonstrate how to infer global navigational patterns by fitting a
maximum likelihood graph to the DSLP field. Experiments show that our SSL model
outperforms two SOTA supervised lane graph prediction models on the nuScenes
dataset. We propose our SSL method as a scalable and interpretable continual
learning paradigm for navigation by perception. Code is available at
https://github.com/robin-karlsson0/dslp.",None,-1
d89b7240-85a7-4e28-ba92-b79187cd8905,Estimation of the qualification and behavior of a contributor and aggregation of his answers in a crowdsourcing context,0.747399,4,"Crowdsourcing is the outsourcing of tasks to a crowd of contributors on a
dedicated platform. The crowd on these platforms is very diversified and
includes various profiles of contributors which generates data of uneven
quality. However, majority voting, which is the aggregating method commonly
used in platforms, gives equal weight to each contribution. To overcome this
problem, we propose a method, MONITOR, which estimates the contributor's
profile and aggregates the collected data by taking into account their possible
imperfections thanks to the theory of belief functions. To do so, MONITOR
starts by estimating the profile of the contributor through his qualification
for the task and his behavior.Crowdsourcing campaigns have been carried out to
collect the necessary data to test MONITOR on real data in order to compare it
to existing approaches. The results of the experiments show that thanks to the
use of the MONITOR method, we obtain a better rate of correct answer after
aggregation of the contributions compared to the majority voting. Our
contributions in this article are for the first time the proposal of a model
that takes into account both the qualification of the contributor and his
behavior in the estimation of his profile. For the second one, the weakening
and the aggregation of the answers according to the estimated profiles.",None,-1
a01673a1-01d0-4231-b385-2e7100bfcb05,On the Zero-Shot Generalization of Machine-Generated Text Detectors,0.676686,11,"The rampant proliferation of large language models, fluent enough to generate
text indistinguishable from human-written language, gives unprecedented
importance to the detection of machine-generated text. This work is motivated
by an important research question: How will the detectors of machine-generated
text perform on outputs of a new generator, that the detectors were not trained
on? We begin by collecting generation data from a wide range of LLMs, and train
neural detectors on data from each generator and test its performance on
held-out generators. While none of the detectors can generalize to all
generators, we observe a consistent and interesting pattern that the detectors
trained on data from a medium-size LLM can zero-shot generalize to the larger
version. As a concrete application, we demonstrate that robust detectors can be
built on an ensemble of training data from medium-sized models.",None,-1
df204016-fb34-4425-a4c7-6d7f9ca2723e,Improving neural network representations using human similarity judgments,0.66225,18,"Deep neural networks have reached human-level performance on many computer
vision tasks. However, the objectives used to train these networks enforce only
that similar images are embedded at similar locations in the representation
space, and do not directly constrain the global structure of the resulting
space. Here, we explore the impact of supervising this global structure by
linearly aligning it with human similarity judgments. We find that a naive
approach leads to large changes in local representational structure that harm
downstream performance. Thus, we propose a novel method that aligns the global
structure of representations while preserving their local structure. This
global-local transform considerably improves accuracy across a variety of
few-shot learning and anomaly detection tasks. Our results indicate that human
visual representations are globally organized in a way that facilitates
learning from few examples, and incorporating this global structure into neural
network representations improves performance on downstream tasks.",None,-1
1ae88de1-61b4-4f0f-82de-0b0a48084518,Domain Adaptation of Synthetic Driving Datasets for Real-World Autonomous Driving,0.139918,2,"While developing perception based deep learning models, the benefit of
synthetic data is enormous. However, performance of networks trained with
synthetic data for certain computer vision tasks degrade significantly when
tested on real world data due to the domain gap between them. One of the
popular solutions in bridging this gap between synthetic and actual world data
is to frame it as a domain adaptation task. In this paper, we propose and
evaluate novel ways for the betterment of such approaches. In particular we
build upon the method of UNIT-GAN.
  In normal GAN training for the task of domain translation, pairing of images
from both the domains (viz, real and synthetic) is done randomly. We propose a
novel method to efficiently incorporate semantic supervision into this pair
selection, which helps in boosting the performance of the model along with
improving the visual quality of such transformed images. We illustrate our
empirical findings on Cityscapes \cite{cityscapes} and challenging synthetic
dataset Synscapes. Though the findings are reported on the base network of
UNIT-GAN, they can be easily extended to any other similar network.",None,-1
819aee21-50bc-47f2-8f7a-f3abb4e65047,Exploring Large Language Models for Ontology Alignment,0.341839,10,"This work investigates the applicability of recent generative Large Language
Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for
identifying concept equivalence mappings across ontologies. To test the
zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging
subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking
into account concept labels and structural contexts. Preliminary findings
suggest that LLMs have the potential to outperform existing ontology alignment
systems like BERTMap, given careful framework and prompt design.",None,-1
521a6657-5deb-44e7-9d19-b8e2d7e9d4d4,AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation,0.851422,27,"We present All-Pairs Multi-Field Transforms (AMT), a new network architecture
for video frame interpolation. It is based on two essential designs. First, we
build bidirectional correlation volumes for all pairs of pixels, and use the
predicted bilateral flows to retrieve correlations for updating both flows and
the interpolated content feature. Second, we derive multiple groups of
fine-grained flow fields from one pair of updated coarse flows for performing
backward warping on the input frames separately. Combining these two designs
enables us to generate promising task-oriented flows and reduce the
difficulties in modeling large motions and handling occluded areas during frame
interpolation. These qualities promote our model to achieve state-of-the-art
performance on various benchmarks with high efficiency. Moreover, our
convolution-based model competes favorably compared to Transformer-based models
in terms of accuracy and efficiency. Our code is available at
https://github.com/MCG-NKU/AMT.",None,-1
c5d98113-9008-418e-9b99-dcdb36f558db,Effective Real Image Editing with Accelerated Iterative Diffusion Inversion,0.737315,18,"Despite all recent progress, it is still challenging to edit and manipulate
natural images with modern generative models. When using Generative Adversarial
Network (GAN), one major hurdle is in the inversion process mapping a real
image to its corresponding noise vector in the latent space, since its
necessary to be able to reconstruct an image to edit its contents. Likewise for
Denoising Diffusion Implicit Models (DDIM), the linearization assumption in
each inversion step makes the whole deterministic inversion process unreliable.
Existing approaches that have tackled the problem of inversion stability often
incur in significant trade-offs in computational efficiency. In this work we
propose an Accelerated Iterative Diffusion Inversion method, dubbed AIDI, that
significantly improves reconstruction accuracy with minimal additional overhead
in space and time complexity. By using a novel blended guidance technique, we
show that effective results can be obtained on a large range of image editing
tasks without large classifier-free guidance in inversion. Furthermore, when
compared with other diffusion inversion based works, our proposed process is
shown to be more robust for fast image editing in the 10 and 20 diffusion
steps' regimes.",None,-1
f67f1c95-b041-48c0-bafd-d9751e01624e,Track Anything: Segment Anything Meets Videos,1.0,115,"Recently, the Segment Anything Model (SAM) gains lots of attention rapidly
due to its impressive segmentation performance on images. Regarding its strong
ability on image segmentation and high interactivity with different prompts, we
found that it performs poorly on consistent segmentation in videos. Therefore,
in this report, we propose Track Anything Model (TAM), which achieves
high-performance interactive tracking and segmentation in videos. To be
detailed, given a video sequence, only with very little human participation,
i.e., several clicks, people can track anything they are interested in, and get
satisfactory results in one-pass inference. Without additional training, such
an interactive design performs impressively on video object tracking and
segmentation. All resources are available on
{https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitate
related research.",None,-1
f050857c-233d-4d44-9c64-45c1281f31d4,"MOSO: Decomposing MOtion, Scene and Object for Video Prediction",0.65593,7,"Motion, scene and object are three primary visual components of a video. In
particular, objects represent the foreground, scenes represent the background,
and motion traces their dynamics. Based on this insight, we propose a two-stage
MOtion, Scene and Object decomposition framework (MOSO) for video prediction,
consisting of MOSO-VQVAE and MOSO-Transformer. In the first stage, MOSO-VQVAE
decomposes a previous video clip into the motion, scene and object components,
and represents them as distinct groups of discrete tokens. Then, in the second
stage, MOSO-Transformer predicts the object and scene tokens of the subsequent
video clip based on the previous tokens and adds dynamic motion at the token
level to the generated object and scene tokens. Our framework can be easily
extended to unconditional video generation and video frame interpolation tasks.
Experimental results demonstrate that our method achieves new state-of-the-art
performance on five challenging benchmarks for video prediction and
unconditional video generation: BAIR, RoboNet, KTH, KITTI and UCF101. In
addition, MOSO can produce realistic videos by combining objects and scenes
from different videos.",None,-1
93c99ac8-9caf-4e19-9076-286630f14d94,Specformer: Spectral Graph Neural Networks Meet Transformers,0.977405,47,"Spectral graph neural networks (GNNs) learn graph representations via
spectral-domain graph convolutions. However, most existing spectral graph
filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a
single filtered value, thus ignoring the global pattern of the spectrum.
Furthermore, these filters are often constructed based on some fixed-order
polynomials, which have limited expressiveness and flexibility. To tackle these
issues, we introduce Specformer, which effectively encodes the set of all
eigenvalues and performs self-attention in the spectral domain, leading to a
learnable set-to-set spectral filter. We also design a decoder with learnable
bases to enable non-local graph convolution. Importantly, Specformer is
equivariant to permutation. By stacking multiple Specformer layers, one can
build a powerful spectral GNN. On synthetic datasets, we show that our
Specformer can better recover ground-truth spectral filters than other spectral
GNNs. Extensive experiments of both node-level and graph-level tasks on
real-world graph datasets show that our Specformer outperforms state-of-the-art
GNNs and learns meaningful spectrum patterns. Code and data are available at
https://github.com/bdy9527/Specformer.",None,-1
a78b98b9-f242-47ef-92b3-bef4326c1f18,MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning,0.719917,8,"This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing
Detection. We used a multi-label contrastive loss for fine-tuning large
pre-trained language models in a multi-lingual setting, achieving very
competitive results: our system was ranked first on the official test set and
on the official shared task leaderboard for five of the six languages for which
we had training data and for which we could perform fine-tuning. Here, we
describe our experimental setup, as well as various ablation studies. The code
of our system is available at https://github.com/QishengL/SemEval2023",None,-1
7a31e619-3418-48cc-a82e-365918bb5d7d,Decentralized Monte Carlo Tree Search for Partially Observable Multi-agent Pathfinding,0.811397,2,"The Multi-Agent Pathfinding (MAPF) problem involves finding a set of
conflict-free paths for a group of agents confined to a graph. In typical MAPF
scenarios, the graph and the agents' starting and ending vertices are known
beforehand, allowing the use of centralized planning algorithms. However, in
this study, we focus on the decentralized MAPF setting, where the agents may
observe the other agents only locally and are restricted in communications with
each other. Specifically, we investigate the lifelong variant of MAPF, where
new goals are continually assigned to the agents upon completion of previous
ones. Drawing inspiration from the successful AlphaZero approach, we propose a
decentralized multi-agent Monte Carlo Tree Search (MCTS) method for MAPF tasks.
Our approach utilizes the agent's observations to recreate the intrinsic Markov
decision process, which is then used for planning with a tailored for
multi-agent tasks version of neural MCTS. The experimental results show that
our approach outperforms state-of-the-art learnable MAPF solvers. The source
code is available at https://github.com/AIRI-Institute/mats-lp.",None,-1
09ea67f2-22b8-440b-a0fe-00d881e8f6b2,Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths,0.370449,1,"The objective of Entity Alignment (EA) is to identify equivalent entity pairs
from multiple Knowledge Graphs (KGs) and create a more comprehensive and
unified KG. The majority of EA methods have primarily focused on the structural
modality of KGs, lacking exploration of multi-modal information. A few
multi-modal EA methods have made good attempts in this field. Still, they have
two shortcomings: (1) inconsistent and inefficient modality modeling that
designs complex and distinct models for each modality; (2) ineffective modality
fusion due to the heterogeneous nature of modalities in EA. To tackle these
challenges, we propose PathFusion, consisting of two main components: (1) MSP,
a unified modeling approach that simplifies the alignment process by
constructing paths connecting entities and modality nodes to represent multiple
modalities; (2) IRF, an iterative fusion method that effectively combines
information from different modalities using the path as an information carrier.
Experimental results on real-world datasets demonstrate the superiority of
PathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement
on Hits@1, and 0.194-0.245 absolute improvement on MRR.",None,-1
216de838-a1fb-4226-9f5b-5dad72304b22,A Simple Recipe for Competitive Low-compute Self supervised Vision Models,0.237352,5,"Self-supervised methods in vision have been mostly focused on large
architectures as they seem to suffer from a significant performance drop for
smaller architectures. In this paper, we propose a simple self-supervised
distillation technique that can train high performance low-compute neural
networks. Our main insight is that existing joint-embedding based SSL methods
can be repurposed for knowledge distillation from a large self-supervised
teacher to a small student model. Thus, we call our method Replace one Branch
(RoB) as it simply replaces one branch of the joint-embedding training with a
large teacher model. RoB is widely applicable to a number of architectures such
as small ResNets, MobileNets and ViT, and pretrained models such as DINO, SwAV
or iBOT. When pretraining on the ImageNet dataset, RoB yields models that
compete with supervised knowledge distillation. When applied to MSN, RoB
produces students with strong semi-supervised capabilities. Finally, our best
ViT-Tiny models improve over prior SSL state-of-the-art on ImageNet by $2.3\%$
and are on par or better than a supervised distilled DeiT on five downstream
transfer tasks (iNaturalist, CIFAR, Clevr/Count, Clevr/Dist and Places). We
hope RoB enables practical self-supervision at smaller scale.",None,-1
508df2d3-e979-43d0-9038-6e9e56202a7c,Intent Induction from Conversations for Task-Oriented Dialogue Track at DSTC 11,0.137583,2,"With increasing demand for and adoption of virtual assistants, recent work
has investigated ways to accelerate bot schema design through the automatic
induction of intents or the induction of slots and dialogue states. However, a
lack of dedicated benchmarks and standardized evaluation has made progress
difficult to track and comparisons between systems difficult to make. This
challenge track, held as part of the Eleventh Dialog Systems Technology
Challenge, introduces a benchmark that aims to evaluate methods for the
automatic induction of customer intents in a realistic setting of customer
service interactions between human agents and customers. We propose two
subtasks for progressively tackling the automatic induction of intents and
corresponding evaluation methodologies. We then present three datasets suitable
for evaluating the tasks and propose simple baselines. Finally, we summarize
the submissions and results of the challenge track, for which we received
submissions from 34 teams.",None,-1
40be3617-3148-4fd6-887b-45efcb1664ec,ModEFormer: Modality-Preserving Embedding for Audio-Video Synchronization using Transformers,0.21372,2,"Lack of audio-video synchronization is a common problem during television
broadcasts and video conferencing, leading to an unsatisfactory viewing
experience. A widely accepted paradigm is to create an error detection
mechanism that identifies the cases when audio is leading or lagging. We
propose ModEFormer, which independently extracts audio and video embeddings
using modality-specific transformers. Different from the other
transformer-based approaches, ModEFormer preserves the modality of the input
streams which allows us to use a larger batch size with more negative audio
samples for contrastive learning. Further, we propose a trade-off between the
number of negative samples and number of unique samples in a batch to
significantly exceed the performance of previous methods. Experimental results
show that ModEFormer achieves state-of-the-art performance, 94.5% for LRS2 and
90.9% for LRS3. Finally, we demonstrate how ModEFormer can be used for offset
detection for test clips.",None,-1
5d3e3020-ac30-41af-94a4-cc16c6bfb724,Orthogonal Subspace Learning for Language Model Continual Learning,0.965073,28,"Benefiting from massive corpora and advanced hardware, large language models
(LLMs) exhibit remarkable capabilities in language understanding and
generation. However, their performance degrades in scenarios where multiple
tasks are encountered sequentially, also known as catastrophic forgetting. In
this paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and
efficient approach for continual learning in language models, effectively
mitigating catastrophic forgetting while learning new tasks. Specifically,
O-LoRA learns tasks in different (low-rank) vector subspaces that are kept
orthogonal to each other in order to minimize interference. Our method induces
only marginal additional parameter costs and requires no user data storage for
replay. Experimental results on continual learning benchmarks show that our
method outperforms state-of-the-art methods. Furthermore, compared to previous
approaches, our method excels in preserving the generalization ability of LLMs
on unseen tasks.",None,-1
16cbf406-f596-4eb3-b949-aadfa8412a50,Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR,0.11372,2,"We propose a method of segmenting long-form speech by separating semantically
complete sentences within the utterance. This prevents the ASR decoder from
needlessly processing faraway context while also preventing it from missing
relevant context within the current sentence. Semantically complete sentence
boundaries are typically demarcated by punctuation in written text; but
unfortunately, spoken real-world utterances rarely contain punctuation. We
address this limitation by distilling punctuation knowledge from a
bidirectional teacher language model (LM) trained on written, punctuated text.
We compare our segmenter, which is distilled from the LM teacher, against a
segmenter distilled from a acoustic-pause-based teacher used in other works, on
a streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2%
relative WER gain along with a 60 ms median end-of-segment latency reduction on
a YouTube captioning task.",None,-1
1d19662d-3923-4fcc-9510-1052d3411abd,Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios,0.346223,3,"The abilities to understand the social interaction behaviors between a
vehicle and its surroundings while predicting its trajectory in an urban
environment are critical for road safety in autonomous driving. Social
interactions are hard to explain because of their uncertainty. In recent years,
neural network-based methods have been widely used for trajectory prediction
and have been shown to outperform hand-crafted methods. However, these methods
suffer from their lack of interpretability. In order to overcome this
limitation, we combine the interpretability of a discrete choice model with the
high accuracy of a neural network-based model for the task of vehicle
trajectory prediction in an interactive environment. We implement and evaluate
our model using the INTERACTION dataset and demonstrate the effectiveness of
our proposed architecture to explain its predictions without compromising the
accuracy.",None,-1
200cadf9-1901-4b8a-86d8-ac05399c071c,Reinforcement Learning and Data-Generation for Syntax-Guided Synthesis,0.133553,1,"Program synthesis is the task of automatically generating code based on a
specification. In Syntax-Guided Synthesis (SyGuS) this specification is a
combination of a syntactic template and a logical formula, and the result is
guaranteed to satisfy both.
  We present a reinforcement-learning guided algorithm for SyGuS which uses
Monte-Carlo Tree Search (MCTS) to search the space of candidate solutions. Our
algorithm learns policy and value functions which, combined with the upper
confidence bound for trees, allow it to balance exploration and exploitation. A
common challenge in applying machine learning approaches to syntax-guided
synthesis is the scarcity of training data. To address this, we present a
method for automatically generating training data for SyGuS based on
anti-unification of existing first-order satisfiability problems, which we use
to train our MCTS policy. We implement and evaluate this setup and demonstrate
that learned policy and value improve the synthesis performance over a baseline
by over 26 percentage points in the training and testing sets. Our tool
outperforms state-of-the-art tool cvc5 on the training set and performs
comparably in terms of the total number of problems solved on the testing set
(solving 23% of the benchmarks on which cvc5 fails). We make our data set
publicly available, to enable further application of machine learning methods
to the SyGuS problem.",None,-1
71436ffb-0503-459c-b477-d38876e4c9a5,Applying Automated Machine Translation to Educational Video Courses,0.0691373,1,"We studied the capability of automated machine translation in the online
video education space by automatically translating Khan Academy videos with
state-of-the-art translation models and applying text-to-speech synthesis and
audio/video synchronization to build engaging videos in target languages. We
also analyzed and established two reliable translation confidence estimators
based on round-trip translations in order to efficiently manage translation
quality and reduce human translation effort. Finally, we developed a deployable
system to deliver translated videos to end users and collect user corrections
for iterative improvement.",None,-1
2bc96f05-7ad0-4b79-b1fd-be87a301c02b,DiffAVA: Personalized Text-to-Audio Generation with Visual Alignment,0.441575,9,"Text-to-audio (TTA) generation is a recent popular problem that aims to
synthesize general audio given text descriptions. Previous methods utilized
latent diffusion models to learn audio embedding in a latent space with text
embedding as the condition. However, they ignored the synchronization between
audio and visual content in the video, and tended to generate audio mismatching
from video frames. In this work, we propose a novel and personalized
text-to-sound generation approach with visual alignment based on latent
diffusion models, namely DiffAVA, that can simply fine-tune lightweight
visual-text alignment modules with frozen modality-specific encoders to update
visual-aligned text embeddings as the condition. Specifically, our DiffAVA
leverages a multi-head attention transformer to aggregate temporal information
from video features, and a dual multi-modal residual network to fuse temporal
visual representations with text embeddings. Then, a contrastive learning
objective is applied to match visual-aligned text embeddings with audio
features. Experimental results on the AudioCaps dataset demonstrate that the
proposed DiffAVA can achieve competitive performance on visual-aligned
text-to-audio generation.",None,-1
f90dda6f-4c90-4f4a-9d7c-1a0df85ddc27,Partial Network Cloning,0.83801,13,"In this paper, we study a novel task that enables partial knowledge transfer
from pre-trained models, which we term as Partial Network Cloning (PNC). Unlike
prior methods that update all or at least part of the parameters in the target
network throughout the knowledge transfer process, PNC conducts partial
parametric ""cloning"" from a source network and then injects the cloned module
to the target, without modifying its parameters. Thanks to the transferred
module, the target network is expected to gain additional functionality, such
as inference on new classes; whenever needed, the cloned module can be readily
removed from the target, with its original parameters and competence kept
intact. Specifically, we introduce an innovative learning scheme that allows us
to identify simultaneously the component to be cloned from the source and the
position to be inserted within the target network, so as to ensure the optimal
performance. Experimental results on several datasets demonstrate that, our
method yields a significant improvement of 5% in accuracy and 50% in locality
when compared with parameter-tuning based methods. Our code is available at
https://github.com/JngwenYe/PNCloning.",None,-1
97e01a26-e27f-4ad7-9614-926ab1957696,A Closer Look into Automatic Evaluation Using Large Language Models,0.104743,3,"Using large language models (LLMs) to evaluate text quality has recently
gained popularity. Some prior works explore the idea of using LLMs for
evaluation, while they differ in some details of the evaluation process. In
this paper, we analyze LLM evaluation (Chiang and Lee, 2023) and G-Eval (Liu et
al., 2023), and we discuss how those details in the evaluation process change
how well the ratings given by LLMs correlate with human ratings. We find that
the auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more
aligned with human ratings. We also show that forcing the LLM to output only a
numeric rating, as in G-Eval, is suboptimal. Last, we reveal that asking the
LLM to explain its own ratings consistently improves the correlation between
the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations
on two meta-evaluation datasets.",None,-1
0b801ccb-f886-4c0d-a98f-fe532926a573,Disease X vaccine production and supply chains: risk assessing healthcare systems operating with artificial intelligence and industry 4.0,0.943527,19,"A set of six algorithmic solutions is presented for resolving vaccine
production and supply chain bottlenecks. A different set of algorithmic
solutions is presented for forecasting risks during a Disease X event.",None,-1
8d493731-3def-48c8-a635-5b82329a641f,Masked Diffusion as Self-supervised Representation Learner,0.397554,3,"Denoising diffusion probabilistic models have recently demonstrated
state-of-the-art generative performance and have been used as strong
pixel-level representation learners. This paper decomposes the interrelation
between the generative capability and representation learning ability inherent
in diffusion models. We present the masked diffusion model (MDM), a scalable
self-supervised representation learner for semantic segmentation, substituting
the conventional additive Gaussian noise of traditional diffusion with a
masking mechanism. Our proposed approach convincingly surpasses prior
benchmarks, demonstrating remarkable advancements in both medical and natural
image semantic segmentation tasks, particularly in few-shot scenarios.",None,-1
0033edf7-f8b6-4e67-8767-2f0d6f97896e,Continuous Risk Measures for Driving Support,0.122483,4,"In this paper, we compare three different model-based risk measures by
evaluating their stengths and weaknesses qualitatively and testing them
quantitatively on a set of real longitudinal and intersection scenarios. We
start with the traditional heuristic Time-To-Collision (TTC), which we extend
towards 2D operation and non-crash cases to retrieve the
Time-To-Closest-Encounter (TTCE). The second risk measure models position
uncertainty with a Gaussian distribution and uses spatial occupancy
probabilities for collision risks. We then derive a novel risk measure based on
the statistics of sparse critical events and so-called survival conditions. The
resulting survival analysis shows to have an earlier detection time of crashes
and less false positive detections in near-crash and non-crash cases supported
by its solid theoretical grounding. It can be seen as a generalization of TTCE
and the Gaussian method which is suitable for the validation of ADAS and AD.",None,-1
b73fa342-aae7-47c6-997f-8f731fb54ecf,In-Context Impersonation Reveals Large Language Models' Strengths and Biases,0.424797,71,"In everyday conversations, humans can take on different roles and adapt their
vocabulary to their chosen roles. We explore whether LLMs can take on, that is
impersonate, different roles when they generate text in-context. We ask LLMs to
assume different personas before solving vision and language tasks. We do this
by prefixing the prompt with a persona that is associated either with a social
identity or domain expertise. In a multi-armed bandit task, we find that LLMs
pretending to be children of different ages recover human-like developmental
stages of exploration. In a language-based reasoning task, we find that LLMs
impersonating domain experts perform better than LLMs impersonating non-domain
experts. Finally, we test whether LLMs' impersonations are complementary to
visual information when describing different categories. We find that
impersonation can improve performance: an LLM prompted to be a bird expert
describes birds better than one prompted to be a car expert. However,
impersonation can also uncover LLMs' biases: an LLM prompted to be a man
describes cars better than one prompted to be a woman. These findings
demonstrate that LLMs are capable of taking on diverse roles and that this
in-context impersonation can be used to uncover their hidden strengths and
biases.",None,-1
57a763af-4607-43b7-a057-2fb5b9c37491,Lost in Translation: Large Language Models in Non-English Content Analysis,0.436908,14,"In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa,
Google's PaLM) have become the dominant approach for building AI systems to
analyze and generate language online. However, the automated systems that
increasingly mediate our interactions online -- such as chatbots, content
moderation systems, and search engines -- are primarily designed for and work
far more effectively in English than in the world's other 7,000 languages.
Recently, researchers and technology companies have attempted to extend the
capabilities of large language models into languages other than English by
building what are called multilingual language models.
  In this paper, we explain how these multilingual language models work and
explore their capabilities and limits. Part I provides a simple technical
explanation of how large language models work, why there is a gap in available
data between English and other languages, and how multilingual language models
attempt to bridge that gap. Part II accounts for the challenges of doing
content analysis with large language models in general and multilingual
language models in particular. Part III offers recommendations for companies,
researchers, and policymakers to keep in mind when considering researching,
developing and deploying large and multilingual language models.",None,-1
d30eb8e0-59fa-406d-92cc-6ef2e8c717a9,Local Implicit Ray Function for Generalizable Radiance Field Representation,0.585239,17,"We propose LIRF (Local Implicit Ray Function), a generalizable neural
rendering approach for novel view rendering. Current generalizable neural
radiance fields (NeRF) methods sample a scene with a single ray per pixel and
may therefore render blurred or aliased views when the input views and rendered
views capture scene content with different resolutions. To solve this problem,
we propose LIRF to aggregate the information from conical frustums to construct
a ray. Given 3D positions within conical frustums, LIRF takes 3D coordinates
and the features of conical frustums as inputs and predicts a local volumetric
radiance field. Since the coordinates are continuous, LIRF renders high-quality
novel views at a continuously-valued scale via volume rendering. Besides, we
predict the visible weights for each input view via transformer-based feature
matching to improve the performance in occluded areas. Experimental results on
real-world scenes validate that our method outperforms state-of-the-art methods
on novel view rendering of unseen scenes at arbitrary scales.",None,-1
73f0133f-0d65-4af6-a17d-01ac1ab2d6c4,Improving Online Lane Graph Extraction by Object-Lane Clustering,0.506368,2,"Autonomous driving requires accurate local scene understanding information.
To this end, autonomous agents deploy object detection and online BEV lane
graph extraction methods as a part of their perception stack. In this work, we
propose an architecture and loss formulation to improve the accuracy of local
lane graph estimates by using 3D object detection outputs. The proposed method
learns to assign the objects to centerlines by considering the centerlines as
cluster centers and the objects as data points to be assigned a probability
distribution over the cluster centers. This training scheme ensures direct
supervision on the relationship between lanes and objects, thus leading to
better performance. The proposed method improves lane graph estimation
substantially over state-of-the-art methods. The extensive ablations show that
our method can achieve significant performance improvements by using the
outputs of existing 3D object detection methods. Since our method uses the
detection outputs rather than detection method intermediate representations, a
single model of our method can use any detection method at test time.",None,-1
62677121-fb33-46fd-ac72-7258ac9e7555,Integrating Semantic Information into Sketchy Reading Module of Retro-Reader for Vietnamese Machine Reading Comprehension,0.40413,2,"Machine Reading Comprehension has become one of the most advanced and popular
research topics in the fields of Natural Language Processing in recent years.
The classification of answerability questions is a relatively significant
sub-task in machine reading comprehension; however, there haven't been many
studies. Retro-Reader is one of the studies that has solved this problem
effectively. However, the encoders of most traditional machine reading
comprehension models in general and Retro-Reader, in particular, have not been
able to exploit the contextual semantic information of the context completely.
Inspired by SemBERT, we use semantic role labels from the SRL task to add
semantics to pre-trained language models such as mBERT, XLM-R, PhoBERT. This
experiment was conducted to compare the influence of semantics on the
classification of answerability for the Vietnamese machine reading
comprehension. Additionally, we hope this experiment will enhance the encoder
for the Retro-Reader model's Sketchy Reading Module. The improved Retro-Reader
model's encoder with semantics was first applied to the Vietnamese Machine
Reading Comprehension task and obtained positive results.",None,-1
d85191c0-6e85-456c-916b-717175ace12f,Findings of the VarDial Evaluation Campaign 2023,0.410659,20,"This report presents the results of the shared tasks organized as part of the
VarDial Evaluation Campaign 2023. The campaign is part of the tenth workshop on
Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects
(VarDial), co-located with EACL 2023. Three separate shared tasks were included
this year: Slot and intent detection for low-resource language varieties
(SID4LR), Discriminating Between Similar Languages -- True Labels (DSL-TL), and
Discriminating Between Similar Languages -- Speech (DSL-S). All three tasks
were organized for the first time this year.",None,-1
c77ef60c-9db3-4198-9d23-619494b39f46,Strategize Before Teaching: A Conversational Tutoring System with Pedagogy Self-Distillation,0.728035,9,"Conversational tutoring systems (CTSs) aim to help students master
educational material with natural language interaction in the form of a dialog.
CTSs have become a key pillar in educational data mining research. A key
challenge in CTSs is to engage the student in the conversation while exposing
them to a diverse set of teaching strategies, akin to a human teacher, thereby,
helping them learn in the process. Different from previous work that generates
responses given the strategies as input, we propose to jointly predict teaching
strategies and generate tutor responses accordingly, which fits a more
realistic application scenario. We benchmark several competitive models on
three dialog tutoring datasets and propose a unified framework that combines
teaching response generation and pedagogical strategy prediction, where a
self-distillation mechanism is adopted to guide the teaching strategy learning
and facilitate tutor response generation. Our experiments and analyses shed
light on how teaching strategies affect dialog tutoring.",None,-1
2659b12e-3e97-4d21-8e3e-e7d8251ae3fe,Model-based learning for location-to-channel mapping,0.394077,2,"Modern communication systems rely on accurate channel estimation to achieve
efficient and reliable transmission of information. As the communication
channel response is highly related to the user's location, one can use a neural
network to map the user's spatial coordinates to the channel coefficients.
However, these latter are rapidly varying as a function of the location, on the
order of the wavelength. Classical neural architectures being biased towards
learning low frequency functions (spectral bias), such mapping is therefore
notably difficult to learn. In order to overcome this limitation, this paper
presents a frugal, model-based network that separates the low frequency from
the high frequency components of the target mapping function. This yields an
hypernetwork architecture where the neural network only learns low frequency
sparse coefficients in a dictionary of high frequency components. Simulation
results show that the proposed neural network outperforms standard approaches
on realistic synthetic data.",None,-1
6837b85f-2b6d-4a7f-b10d-0336e5725095,A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding,0.741395,31,"Zero-shot dialogue understanding aims to enable dialogue to track the user's
needs without any training data, which has gained increasing attention. In this
work, we investigate the understanding ability of ChatGPT for zero-shot
dialogue understanding tasks including spoken language understanding (SLU) and
dialogue state tracking (DST). Experimental results on four popular benchmarks
reveal the great potential of ChatGPT for zero-shot dialogue understanding. In
addition, extensive analysis shows that ChatGPT benefits from the multi-turn
interactive prompt in the DST task but struggles to perform slot filling for
SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue
understanding tasks, hoping to provide some insights for future research on
building zero-shot dialogue understanding systems with Large Language Models
(LLMs).",None,-1
8d79b543-6346-4343-921d-cd488d3b1a0c,Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,0.346874,4,"Fairness has become increasingly pivotal in medical image recognition.
However, without mitigating bias, deploying unfair medical AI systems could
harm the interests of underprivileged populations. In this paper, we observe
that while features extracted from the deeper layers of neural networks
generally offer higher accuracy, fairness conditions deteriorate as we extract
features from deeper layers. This phenomenon motivates us to extend the concept
of multi-exit frameworks. Unlike existing works mainly focusing on accuracy,
our multi-exit framework is fairness-oriented; the internal classifiers are
trained to be more accurate and fairer, with high extensibility to apply to
most existing fairness-aware frameworks. During inference, any instance with
high confidence from an internal classifier is allowed to exit early.
Experimental results show that the proposed framework can improve the fairness
condition over the state-of-the-art in two dermatological disease datasets.",None,-1
73d99270-8b87-4f88-8a2c-fbe5578212e8,Nonrigid Object Contact Estimation With Regional Unwrapping Transformer,0.393469,1,"Acquiring contact patterns between hands and nonrigid objects is a common
concern in the vision and robotics community. However, existing learning-based
methods focus more on contact with rigid ones from monocular images. When
adopting them for nonrigid contact, a major problem is that the existing
contact representation is restricted by the geometry of the object.
Consequently, contact neighborhoods are stored in an unordered manner and
contact features are difficult to align with image cues. At the core of our
approach lies a novel hand-object contact representation called RUPs (Region
Unwrapping Profiles), which unwrap the roughly estimated hand-object surfaces
as multiple high-resolution 2D regional profiles. The region grouping strategy
is consistent with the hand kinematic bone division because they are the
primitive initiators for a composite contact pattern. Based on this
representation, our Regional Unwrapping Transformer (RUFormer) learns the
correlation priors across regions from monocular inputs and predicts
corresponding contact and deformed transformations. Our experiments demonstrate
that the proposed framework can robustly estimate the deformed degrees and
deformed transformations, which makes it suitable for both nonrigid and rigid
contact.",None,-1
5c2d260c-9578-4ea3-96b9-51490d8f9b94,Learnable Earth Parser: Discovering 3D Prototypes in Aerial Scans,0.284985,1,"We propose an unsupervised method for parsing large 3D scans of real-world
scenes with easily-interpretable shapes. This work aims to provide a practical
tool for analyzing 3D scenes in the context of aerial surveying and mapping,
without the need for user annotations. Our approach is based on a probabilistic
reconstruction model that decomposes an input 3D point cloud into a small set
of learned prototypical 3D shapes. The resulting reconstruction is visually
interpretable and can be used to perform unsupervised instance and low-shot
semantic segmentation of complex scenes. We demonstrate the usefulness of our
model on a novel dataset of seven large aerial LiDAR scans from diverse
real-world scenarios. Our approach outperforms state-of-the-art unsupervised
methods in terms of decomposition accuracy while remaining visually
interpretable. Our code and dataset are available at
https://romainloiseau.fr/learnable-earth-parser/",None,-1
37b1973b-ad76-462a-95d4-74b5019fe247,RoBERTweet: A BERT Language Model for Romanian Tweets,0.156198,1,"Developing natural language processing (NLP) systems for social media
analysis remains an important topic in artificial intelligence research. This
article introduces RoBERTweet, the first Transformer architecture trained on
Romanian tweets. Our RoBERTweet comes in two versions, following the base and
large architectures of BERT. The corpus used for pre-training the models
represents a novelty for the Romanian NLP community and consists of all tweets
collected from 2008 to 2022. Experiments show that RoBERTweet models outperform
the previous general-domain Romanian and multilingual language models on three
NLP tasks with tweet inputs: emotion detection, sexist language identification,
and named entity recognition. We make our models and the newly created corpus
of Romanian tweets freely available.",None,-1
73d0fcba-315c-4df9-8199-1ea043ae175d,CoTracker: It is Better to Track Together,1.0,67,"We introduce CoTracker, a transformer-based model that tracks dense points in
a frame jointly across a video sequence. This differs from most existing
state-of-the-art approaches that track points independently, ignoring their
correlation. We show that joint tracking results in a significantly higher
tracking accuracy and robustness. We also provide several technical
innovations, including the concept of virtual tracks, which allows CoTracker to
track 70k points jointly and simultaneously. Furthermore, CoTracker operates
causally on short windows (hence, it is suitable for online tasks), but is
trained by unrolling the windows across longer video sequences, which enables
and significantly improves long-term tracking. We demonstrate qualitatively
impressive tracking results, where points can be tracked for a long time even
when they are occluded or leave the field of view. Quantitatively, CoTracker
outperforms all recent trackers on standard benchmarks, often by a substantial
margin.",None,-1
d9aecd25-fdc5-45ed-822d-a6dc0657745c,Witscript 3: A Hybrid AI System for Improvising Jokes in a Conversation,0.173375,4,"Previous papers presented Witscript and Witscript 2, AI systems for
improvising jokes in a conversation. Witscript generates jokes that rely on
wordplay, whereas the jokes generated by Witscript 2 rely on common sense. This
paper extends that earlier work by presenting Witscript 3, which generates joke
candidates using three joke production mechanisms and then selects the best
candidate to output. Like Witscript and Witscript 2, Witscript 3 is based on
humor algorithms created by an expert comedy writer. Human evaluators judged
Witscript 3's responses to input sentences to be jokes 44% of the time. This is
evidence that Witscript 3 represents another step toward giving a chatbot a
humanlike sense of humor.",None,-1
715dbad1-35e0-4448-b787-1857519bdedd,Learning Unseen Modality Interaction,0.182589,3,"Multimodal learning assumes all modality combinations of interest are
available during training to learn cross-modal correspondences. In this paper,
we challenge this modality-complete assumption for multimodal learning and
instead strive for generalization to unseen modality combinations during
inference. We pose the problem of unseen modality interaction and introduce a
first solution. It exploits a module that projects the multidimensional
features of different modalities into a common space with rich information
preserved. This allows the information to be accumulated with a simple
summation operation across available modalities. To reduce overfitting to less
discriminative modality combinations during training, we further improve the
model learning with pseudo-supervision indicating the reliability of a
modality's prediction. We demonstrate that our approach is effective for
diverse tasks and modalities by evaluating it for multimodal video
classification, robot state regression, and multimedia retrieval. Project
website: https://xiaobai1217.github.io/Unseen-Modality-Interaction/.",None,-1
478a5838-dc67-4b4f-a345-758287ac6061,Evaluation of Induced Expert Knowledge in Causal Structure Learning by NOTEARS,0.438177,5,"Causal modeling provides us with powerful counterfactual reasoning and
interventional mechanism to generate predictions and reason under various
what-if scenarios. However, causal discovery using observation data remains a
nontrivial task due to unobserved confounding factors, finite sampling, and
changes in the data distribution. These can lead to spurious cause-effect
relationships. To mitigate these challenges in practice, researchers augment
causal learning with known causal relations. The goal of the paper is to study
the impact of expert knowledge on causal relations in the form of additional
constraints used in the formulation of the nonparametric NOTEARS. We provide a
comprehensive set of comparative analyses of biasing the model using different
types of knowledge. We found that (i) knowledge that corrects the mistakes of
the NOTEARS model can lead to statistically significant improvements, (ii)
constraints on active edges have a larger positive impact on causal discovery
than inactive edges, and surprisingly, (iii) the induced knowledge does not
correct on average more incorrect active and/or inactive edges than expected.
We also demonstrate the behavior of the model and the effectiveness of domain
knowledge on a real-world dataset.",None,-1
fae4e3c3-cd5b-4279-8c69-565c895d6e4d,Forward LTLf Synthesis: DPLL At Work,0.0837267,3,"This paper proposes a new AND-OR graph search framework for synthesis of
Linear Temporal Logic on finite traces (\LTLf), that overcomes some limitations
of previous approaches. Within such framework, we devise a procedure inspired
by the Davis-Putnam-Logemann-Loveland (DPLL) algorithm to generate the next
available agent-environment moves in a truly depth-first fashion, possibly
avoiding exhaustive enumeration or costly compilations. We also propose a novel
equivalence check for search nodes based on syntactic equivalence of state
formulas. Since the resulting procedure is not guaranteed to terminate, we
identify a stopping condition to abort execution and restart the search with
state-equivalence checking based on Binary Decision Diagrams (BDD), which we
show to be correct. The experimental results show that in many cases the
proposed techniques outperform other state-of-the-art approaches. Our
implementation Nike competed in the LTLf Realizability Track in the 2023
edition of SYNTCOMP, and won the competition.",None,-1
2c6205ec-6a0e-4bda-b596-8f80355e2bcd,Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers,0.573969,2,"Recent applications of LLMs in Machine Reading Comprehension (MRC) systems
have shown impressive results, but the use of shortcuts, mechanisms triggered
by features spuriously correlated to the true label, has emerged as a potential
threat to their reliability. We analyze the problem from two angles: LLMs as
editors, guided to edit text to mislead LLMs; and LLMs as readers, who answer
questions based on the edited text. We introduce a framework that guides an
editor to add potential shortcuts-triggers to samples. Using GPT4 as the
editor, we find it can successfully edit trigger shortcut in samples that fool
LLMs. Analysing LLMs as readers, we observe that even capable LLMs can be
deceived using shortcut knowledge. Strikingly, we discover that GPT4 can be
deceived by its own edits (15% drop in F1). Our findings highlight inherent
vulnerabilities of LLMs to shortcut manipulations. We publish ShortcutQA, a
curated dataset generated by our framework for future research.",None,-1
599f3d15-1f02-4045-9364-2058c295cb35,Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement,0.712303,3,"Conversational agents participating in multi-party interactions face
significant challenges in dialogue state tracking, since the identity of the
speaker adds significant contextual meaning. It is common to utilise
diarisation models to identify the speaker. However, it is not clear if these
are accurate enough to correctly identify specific conversational events such
as agreement or disagreement during a real-time interaction. This study uses a
cooperative quiz, where the conversational agent acts as quiz-show host, to
determine whether diarisation or a frequency-and-proximity-based method is more
accurate at determining agreement, and whether this translates to feelings of
engagement from the players. Experimental results show that our procedural
system was more engaging to players, and was more accurate at detecting
agreement, reaching an average accuracy of 0.44 compared to 0.28 for the
diarised system.",None,-1
1f782d0e-755a-4325-9443-5a27bd93e7fe,The Touché23-ValueEval Dataset for Identifying Human Values behind Arguments,0.915793,32,"We present the Touch\'e23-ValueEval Dataset for Identifying Human Values
behind Arguments. To investigate approaches for the automated detection of
human values behind arguments, we collected 9324 arguments from 6 diverse
sources, covering religious texts, political discussions, free-text arguments,
newspaper editorials, and online democracy platforms. Each argument was
annotated by 3 crowdworkers for 54 values. The Touch\'e23-ValueEval dataset
extends the Webis-ArgValues-22. In comparison to the previous dataset, the
effectiveness of a 1-Baseline decreases, but that of an out-of-the-box BERT
model increases. Therefore, though the classification difficulty increased as
per the label distribution, the larger dataset allows for training better
models.",None,-1
f5b54165-2999-4015-892c-1720d5cf7df4,Localized Text-to-Image Generation for Free via Cross Attention Control,0.410139,17,"Despite the tremendous success in text-to-image generative models, localized
text-to-image generation (that is, generating objects or features at specific
locations in an image while maintaining a consistent overall generation) still
requires either explicit training or substantial additional inference time. In
this work, we show that localized generation can be achieved by simply
controlling cross attention maps during inference. With no additional training,
model architecture modification or inference time, our proposed cross attention
control (CAC) provides new open-vocabulary localization abilities to standard
text-to-image models. CAC also enhances models that are already trained for
localized generation when deployed at inference time. Furthermore, to assess
localized text-to-image generation performance automatically, we develop a
standardized suite of evaluations using large pretrained recognition models.
Our experiments show that CAC improves localized generation performance with
various types of location information ranging from bounding boxes to semantic
segmentation maps, and enhances the compositional capability of
state-of-the-art text-to-image generative models.",None,-1
054509f3-8821-427e-8bc0-bc428d4455dc,Geometric Ultrasound Localization Microscopy,0.718299,3,"Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for
non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound
Localization Microscopy (ULM) has enabled a revolutionary breakthrough by
offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers
are used to render ULM frames, ultimately determining the image resolution
capability. To take full advantage of ULM, this study questions whether
beamforming is the most effective processing step for ULM, suggesting an
alternative approach that relies solely on Time-Difference-of-Arrival (TDoA)
information. To this end, a novel geometric framework for micro bubble
localization via ellipse intersections is proposed to overcome existing
beamforming limitations. We present a benchmark comparison based on a public
dataset for which our geometric ULM outperforms existing baseline methods in
terms of accuracy and robustness while only utilizing a portion of the
available transducer data.",None,-1
095ebbb6-235c-49dd-bbd1-ce202945da2c,Relighting Neural Radiance Fields with Shadow and Highlight Hints,0.839207,15,"This paper presents a novel neural implicit radiance representation for free
viewpoint relighting from a small set of unstructured photographs of an object
lit by a moving point light source different from the view position. We express
the shape as a signed distance function modeled by a multi layer perceptron. In
contrast to prior relightable implicit neural representations, we do not
disentangle the different reflectance components, but model both the local and
global reflectance at each point by a second multi layer perceptron that, in
addition, to density features, the current position, the normal (from the
signed distace function), view direction, and light position, also takes shadow
and highlight hints to aid the network in modeling the corresponding high
frequency light transport effects. These hints are provided as a suggestion,
and we leave it up to the network to decide how to incorporate these in the
final relit result. We demonstrate and validate our neural implicit
representation on synthetic and real scenes exhibiting a wide variety of
shapes, material properties, and global illumination light transport.",None,-1
15523a39-a1b0-47b7-ae05-d513b44a7781,Coherent Concept-based Explanations in Medical Image and Its Application to Skin Lesion Diagnosis,0.733773,8,"Early detection of melanoma is crucial for preventing severe complications
and increasing the chances of successful treatment. Existing deep learning
approaches for melanoma skin lesion diagnosis are deemed black-box models, as
they omit the rationale behind the model prediction, compromising the
trustworthiness and acceptability of these diagnostic methods. Attempts to
provide concept-based explanations are based on post-hoc approaches, which
depend on an additional model to derive interpretations. In this paper, we
propose an inherently interpretable framework to improve the interpretability
of concept-based models by incorporating a hard attention mechanism and a
coherence loss term to assure the visual coherence of concept activations by
the concept encoder, without requiring the supervision of additional
annotations. The proposed framework explains its decision in terms of
human-interpretable concepts and their respective contribution to the final
prediction, as well as a visual interpretation of the locations where the
concept is present in the image. Experiments on skin image datasets demonstrate
that our method outperforms existing black-box and concept-based models for
skin lesion classification.",None,-1
980e62ab-f62f-479a-90c4-81531c8dca98,"Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators",0.865335,44,"Large language models that exhibit instruction-following behaviour represent
one of the biggest recent upheavals in conversational interfaces, a trend in
large part fuelled by the release of OpenAI's ChatGPT, a proprietary large
language model for text generation fine-tuned through reinforcement learning
from human feedback (LLM+RLHF). We review the risks of relying on proprietary
software and survey the first crop of open-source projects of comparable
architecture and functionality. The main contribution of this paper is to show
that openness is differentiated, and to offer scientific documentation of
degrees of openness in this fast-moving field. We evaluate projects in terms of
openness of code, training data, model weights, RLHF data, licensing,
scientific documentation, and access methods. We find that while there is a
fast-growing list of projects billing themselves as 'open source', many inherit
undocumented data of dubious legality, few share the all-important
instruction-tuning (a key site where human annotation labour is involved), and
careful scientific documentation is exceedingly rare. Degrees of openness are
relevant to fairness and accountability at all points, from data collection and
curation to model architecture, and from training and fine-tuning to release
and deployment.",None,-1
f6d6444c-48fd-46cf-9174-6eebd94d4975,Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation,0.0252183,3,"This work aims at decreasing the end-to-end generation latency of large
language models (LLMs). One of the major causes of the high generation latency
is the sequential decoding approach adopted by almost all state-of-the-art
LLMs. In this work, motivated by the thinking and writing process of humans, we
propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the
skeleton of the answer, and then conducts parallel API calls or batched
decoding to complete the contents of each skeleton point in parallel. Not only
does SoT provide considerable speed-ups across 12 LLMs, but it can also
potentially improve the answer quality on several question categories. SoT is
an initial attempt at data-centric optimization for inference efficiency, and
showcases the potential of eliciting high-quality answers by explicitly
planning the answer structure in language.",None,-1
74158941-9df7-469d-8ee3-c380fd24449c,Fairness in Visual Clustering: A Novel Transformer Clustering Approach,0.854114,6,"Promoting fairness for deep clustering models in unsupervised clustering
settings to reduce demographic bias is a challenging goal. This is because of
the limitation of large-scale balanced data with well-annotated labels for
sensitive or protected attributes. In this paper, we first evaluate demographic
bias in deep clustering models from the perspective of cluster purity, which is
measured by the ratio of positive samples within a cluster to their correlation
degree. This measurement is adopted as an indication of demographic bias. Then,
a novel loss function is introduced to encourage a purity consistency for all
clusters to maintain the fairness aspect of the learned clustering model.
Moreover, we present a novel attention mechanism, Cross-attention, to measure
correlations between multiple clusters, strengthening faraway positive samples
and improving the purity of clusters during the learning process. Experimental
results on a large-scale dataset with numerous attribute settings have
demonstrated the effectiveness of the proposed approach on both clustering
accuracy and fairness enhancement on several sensitive attributes.",None,-1
75eedc48-1446-4cad-9c3f-1120c98abaaa,MiniRBT: A Two-stage Distilled Small Chinese Pre-trained Model,0.121598,2,"In natural language processing, pre-trained language models have become
essential infrastructures. However, these models often suffer from issues such
as large size, long inference time, and challenging deployment. Moreover, most
mainstream pre-trained models focus on English, and there are insufficient
studies on small Chinese pre-trained models. In this paper, we introduce
MiniRBT, a small Chinese pre-trained model that aims to advance research in
Chinese natural language processing. MiniRBT employs a narrow and deep student
model and incorporates whole word masking and two-stage distillation during
pre-training to make it well-suited for most downstream tasks. Our experiments
on machine reading comprehension and text classification tasks reveal that
MiniRBT achieves 94% performance relative to RoBERTa, while providing a 6.8x
speedup, demonstrating its effectiveness and efficiency.",None,-1
4d8e4747-9a22-49ca-891e-60609c4f6700,RweetMiner: Automatic identification and categorization of help requests on twitter during disasters,0.511244,21,"Catastrophic events create uncertain situations for humanitarian
organizations locating and providing aid to affected people. Many people turn
to social media during disasters for requesting help and/or providing relief to
others. However, the majority of social media posts seeking help could not
properly be detected and remained concealed because often they are noisy and
ill-formed. Existing systems lack in planning an effective strategy for tweet
preprocessing and grasping the contexts of tweets. This research, first of all,
formally defines request tweets in the context of social networking sites,
hereafter rweets, along with their different primary types and sub-types. Our
main contributions are the identification and categorization of rweets. For
rweet identification, we employ two approaches, namely a rule-based and
logistic regression, and show their high precision and F1 scores. The rweets
classification into sub-types such as medical, food, and shelter, using
logistic regression shows promising results and outperforms existing works.
Finally, we introduce an architecture to store intermediate data to accelerate
the development process of the machine learning classifiers.",None,-1
8249ca98-55f1-40ab-b4aa-dfafe6ebe2d8,Meet in the Middle: A New Pre-training Paradigm,0.193522,11,"Most language models (LMs) are trained and applied in an autoregressive
left-to-right fashion, assuming that the next token only depends on the
preceding ones. However, this assumption ignores the potential benefits of
using the full sequence information during training, and the possibility of
having context from both sides during inference. In this paper, we propose a
new pre-training paradigm with techniques that jointly improve the training
data efficiency and the capabilities of the LMs in the infilling task. The
first is a training objective that aligns the predictions of a left-to-right LM
with those of a right-to-left LM, trained on the same data but in reverse
order. The second is a bidirectional inference procedure that enables both LMs
to meet in the middle. We show the effectiveness of our pre-training paradigm
with extensive experiments on both programming and natural language models,
outperforming strong baselines.",None,-1
70921c22-6257-4a04-bcd5-f4329e1ac323,Curriculum Learning for ab initio Deep Learned Refractive Optics,0.329597,9,"Deep optical optimization has recently emerged as a new paradigm for
designing computational imaging systems using only the output image as the
objective. However, it has been limited to either simple optical systems
consisting of a single element such as a diffractive optical element (DOE) or
metalens, or the fine-tuning of compound lenses from good initial designs. Here
we present a DeepLens design method based on curriculum learning, which is able
to learn optical designs of compound lenses ab initio from randomly initialized
surfaces without human intervention, therefore overcoming the need for a good
initial design. We demonstrate the effectiveness of our approach by fully
automatically designing both classical imaging lenses and a large field-of-view
extended depth-of-field computational lens in a cellphone-style form factor,
with highly aspheric surfaces and a short back focal length.",None,-1
8891452f-2252-4b88-9f7f-e64a3e383414,FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios,0.900563,95,"The emergence of generative pre-trained models has facilitated the synthesis
of high-quality text, but it has also posed challenges in identifying factual
errors in the generated text. In particular: (1) A wider range of tasks now
face an increasing risk of containing factual errors when handled by generative
models. (2) Generated texts tend to be lengthy and lack a clearly defined
granularity for individual facts. (3) There is a scarcity of explicit evidence
available during the process of fact checking. With the above challenges in
mind, in this paper, we propose FacTool, a task and domain agnostic framework
for detecting factual errors of texts generated by large language models (e.g.,
ChatGPT). Experiments on four different tasks (knowledge-based QA, code
generation, mathematical reasoning, and scientific literature review) show the
efficacy of the proposed method. We release the code of FacTool associated with
ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",None,-1
06237462-92ea-4dbd-98e2-d2b98601be75,Neural Machine Translation Data Generation and Augmentation using ChatGPT,0.601888,4,"Neural models have revolutionized the field of machine translation, but
creating parallel corpora is expensive and time-consuming. We investigate an
alternative to manual parallel corpora - hallucinated parallel corpora created
by generative language models. Although these models are themselves trained on
parallel data, they can leverage a multilingual vector space to create data,
and may be able to supplement small manually-procured corpora. Our experiments
highlight two key findings - despite a lack of diversity in their output, the
hallucinated data improves the translation signal, even when the domain clashes
with the original dataset.",None,-1
a9bb4ddf-af4f-48d3-bf2d-ef1363d31b63,Boosting Theory-of-Mind Performance in Large Language Models via Prompting,0.306653,50,"Large language models (LLMs) excel in many tasks in 2023, but they still face
challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require
understanding agents' beliefs, goals, and mental states, are essential for
common-sense reasoning involving humans, making it crucial to enhance LLM
performance in this area. This study measures the ToM performance of GPT-4 and
three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates
the effectiveness of in-context learning in improving their ToM comprehension.
We evaluated prompts featuring two-shot chain of thought reasoning and
step-by-step thinking instructions. We found that LLMs trained with
Reinforcement Learning from Human Feedback (RLHF) (all models excluding
Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed
best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell
short of the 87% human accuracy on the test set. However, when supplied with
prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM
accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate
prompting enhances LLM ToM reasoning, and they underscore the context-dependent
nature of LLM cognitive capacities.",None,-1
75fa7142-2f8f-4654-8c5a-70cf05bdb758,Experimental results from applying GPT-4 to an unpublished formal language,0.210909,3,"Can large language models be used to complete mathematical tasks that are
traditionally performed either manually or with the aid of theorem provers? To
answer this question, a state-of-the-art system, GPT-4, was provided with a
concise natural language specification for a previously unpublished formal
system and asked to complete a number of tasks, from stating function and type
definitions to proving simple theorems and verifying user-supplied proofs. The
system completed all tasks successfully, showed extensive domain knowledge,
invented helpful new syntax and semantics, and exhibited generalization and
inference abilities. So the answer seems to be: yes.",None,-1
326481fb-fd35-4136-9682-81da4c9e4b6c,RLHF and IIA: Perverse Incentives,0.0648904,1,"Existing algorithms for reinforcement learning from human feedback (RLHF) can
incentivize responses at odds with preferences because they are based on models
that assume independence of irrelevant alternatives (IIA). The perverse
incentives induced by IIA hinder innovations on query formats and learning
algorithms.",None,-1
25ee47d1-f95f-4c8a-a115-2f05b0e5c5d9,On the Interventional Kullback-Leibler Divergence,0.367799,5,"Modern machine learning approaches excel in static settings where a large
amount of i.i.d. training data are available for a given task. In a dynamic
environment, though, an intelligent agent needs to be able to transfer
knowledge and re-use learned components across domains. It has been argued that
this may be possible through causal models, aiming to mirror the modularity of
the real world in terms of independent causal mechanisms. However, the true
causal structure underlying a given set of data is generally not identifiable,
so it is desirable to have means to quantify differences between models (e.g.,
between the ground truth and an estimate), on both the observational and
interventional level.
  In the present work, we introduce the Interventional Kullback-Leibler (IKL)
divergence to quantify both structural and distributional differences between
models based on a finite set of multi-environment distributions generated by
interventions from the ground truth. Since we generally cannot quantify all
differences between causal models for every finite set of interventional
distributions, we propose a sufficient condition on the intervention targets to
identify subsets of observed variables on which the models provably agree or
disagree.",None,-1
5af9a6ef-ce24-4e0b-a201-0f805ba46492,OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation,0.768101,20,"Referring video object segmentation (RVOS) aims at segmenting an object in a
video following human instruction. Current state-of-the-art methods fall into
an offline pattern, in which each clip independently interacts with text
embedding for cross-modal understanding. They usually present that the offline
pattern is necessary for RVOS, yet model limited temporal association within
each clip. In this work, we break up the previous offline belief and propose a
simple yet effective online model using explicit query propagation, named
OnlineRefer. Specifically, our approach leverages target cues that gather
semantic information and position prior to improve the accuracy and ease of
referring predictions for the current frame. Furthermore, we generalize our
online model into a semi-online framework to be compatible with video-based
backbones. To show the effectiveness of our method, we evaluate it on four
benchmarks, \ie, Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, and
JHMDB-Sentences. Without bells and whistles, our OnlineRefer with a Swin-L
backbone achieves 63.5 J&F and 64.8 J&F on Refer-Youtube-VOS and Refer-DAVIS17,
outperforming all other offline methods.",None,-1
d0b9c206-fedf-4c2d-a18b-cf56474230de,Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance,0.239742,3,"Retrieval augmentation enhances performance of traditional language models by
incorporating additional context. However, the computational demands for
retrieval augmented large language models (LLMs) pose a challenge when applying
them to real-time tasks, such as composition assistance. To address this
limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG)
framework, a novel approach that efficiently combines a cloud-based LLM with a
smaller, client-side, language model through retrieval augmented memory. This
integration enables the client model to generate effective responses,
benefiting from the LLM's capabilities and contextual information.
Additionally, through an asynchronous memory update mechanism, the client model
can deliver real-time completions swiftly to user inputs without the need to
wait for responses from the cloud. Our experiments on five benchmark datasets
demonstrate that HybridRAG significantly improves utility over client-only
models while maintaining low latency.",None,-1
3ffaf857-dd36-4aa2-a37d-e494ce68edf9,Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks,0.101606,1,"Deep learning has been rapidly employed in many applications revolutionizing
many industries, but it is known to be vulnerable to adversarial attacks. Such
attacks pose a serious threat to deep learning-based systems compromising their
integrity, reliability, and trust. Interpretable Deep Learning Systems (IDLSes)
are designed to make the system more transparent and explainable, but they are
also shown to be susceptible to attacks. In this work, we propose a novel
microbial genetic algorithm-based black-box attack against IDLSes that requires
no prior knowledge of the target model and its interpretation model. The
proposed attack is a query-efficient approach that combines transfer-based and
score-based methods, making it a powerful tool to unveil IDLS vulnerabilities.
Our experiments of the attack show high attack success rates using adversarial
examples with attribution maps that are highly similar to those of benign
samples which makes it difficult to detect even by human analysts. Our results
highlight the need for improved IDLS security to ensure their practical
reliability.",None,-1
92e5fe84-791e-4f5b-8724-ca61fd1c9e09,MPSA-DenseNet: A novel deep learning model for English accent classification,0.361223,1,"This paper presents three innovative deep learning models for English accent
classification: Multi-DenseNet, PSA-DenseNet, and MPSE-DenseNet, that combine
multi-task learning and the PSA module attention mechanism with DenseNet. We
applied these models to data collected from six dialects of English across
native English speaking regions (Britain, the United States, Scotland) and
nonnative English speaking regions (China, Germany, India). Our experimental
results show a significant improvement in classification accuracy, particularly
with MPSA-DenseNet, which outperforms all other models, including DenseNet and
EPSA models previously used for accent identification. Our findings indicate
that MPSA-DenseNet is a highly promising model for accurately identifying
English accents.",None,-1
ff2b3268-777d-4c93-985d-4adaa9bf9ad6,TACO: Topics in Algorithmic COde generation dataset,0.291125,6,"We introduce TACO, an open-source, large-scale code generation dataset, with
a focus on the optics of algorithms, designed to provide a more challenging
training dataset and evaluation benchmark in the field of code generation
models. TACO includes competition-level programming questions that are more
challenging, to enhance or evaluate problem understanding and reasoning
abilities in real-world programming scenarios. There are 25433 and 1000 coding
problems in training and test set, as well as up to 1.55 million diverse
solution answers. Moreover, each TACO problem includes several fine-grained
labels such as task topics, algorithms, programming skills, and difficulty
levels, providing a more precise reference for the training and evaluation of
code generation models. The dataset and evaluation scripts are available on
Hugging Face Hub (https://huggingface.co/datasets/BAAI/TACO) and Github
(https://github.com/FlagOpen/TACO).",None,-1
d3f87d0c-6249-44bd-bbc7-1c5e681b8034,Designing Behavior Trees from Goal-Oriented LTLf Formulas,0.737184,3,"Temporal logic can be used to formally specify autonomous agent goals, but
synthesizing planners that guarantee goal satisfaction can be computationally
prohibitive. This paper shows how to turn goals specified using a subset of
finite trace Linear Temporal Logic (LTL) into a behavior tree (BT) that
guarantees that successful traces satisfy the LTL goal. Useful LTL formulas for
achievement goals can be derived using achievement-oriented task mission
grammars, leading to missions made up of tasks combined using LTL operators.
Constructing BTs from LTL formulas leads to a relaxed behavior synthesis
problem in which a wide range of planners can implement the action nodes in the
BT. Importantly, any successful trace induced by the planners satisfies the
corresponding LTL formula. The usefulness of the approach is demonstrated in
two ways: a) exploring the alignment between two planners and LTL goals, and b)
solving a sequential key-door problem for a Fetch robot.",None,-1
631c0be2-95eb-43d5-90b7-1816d9b49c0c,Which Tokens to Use? Investigating Token Reduction in Vision Transformers,0.542913,12,"Since the introduction of the Vision Transformer (ViT), researchers have
sought to make ViTs more efficient by removing redundant information in the
processed tokens. While different methods have been explored to achieve this
goal, we still lack understanding of the resulting reduction patterns and how
those patterns differ across token reduction methods and datasets. To close
this gap, we set out to understand the reduction patterns of 10 different token
reduction methods using four image classification datasets. By systematically
comparing these methods on the different classification tasks, we find that the
Top-K pruning method is a surprisingly strong baseline. Through in-depth
analysis of the different methods, we determine that: the reduction patterns
are generally not consistent when varying the capacity of the backbone model,
the reduction patterns of pruning-based methods significantly differ from fixed
radial patterns, and the reduction patterns of pruning-based methods are
correlated across classification datasets. Finally we report that the
similarity of reduction patterns is a moderate-to-strong proxy for model
performance. Project page at https://vap.aau.dk/tokens.",None,-1
9577dde8-d889-4920-aef2-72af8b2addf0,Sampling to Distill: Knowledge Transfer from Open-World Data,0.601225,7,"Data-Free Knowledge Distillation (DFKD) is a novel task that aims to train
high-performance student models using only the teacher network without original
training data. Despite encouraging results, existing DFKD methods rely heavily
on generation modules with high computational costs. Meanwhile, they ignore the
fact that the generated and original data exist domain shifts due to the lack
of supervision information. Moreover, knowledge is transferred through each
example, ignoring the implicit relationship among multiple examples. To this
end, we propose a novel Open-world Data Sampling Distillation (ODSD) method
without a redundant generation process. First, we try to sample open-world data
close to the original data's distribution by an adaptive sampling module. Then,
we introduce a low-noise representation to alleviate the domain shifts and
build a structured relationship of multiple data examples to exploit data
knowledge. Extensive experiments on CIFAR-10, CIFAR-100, NYUv2, and ImageNet
show that our ODSD method achieves state-of-the-art performance. Especially, we
improve 1.50\%-9.59\% accuracy on the ImageNet dataset compared with the
existing results.",None,-1
ea43f2bf-d5db-4d16-af02-ba2b5dd052f5,"Learning with Impartiality to Walk on the Pareto Frontier of Fairness, Privacy, and Utility",0.474602,3,"Deploying machine learning (ML) models often requires both fairness and
privacy guarantees. Both of these objectives present unique trade-offs with the
utility (e.g., accuracy) of the model. However, the mutual interactions between
fairness, privacy, and utility are less well-understood. As a result, often
only one objective is optimized, while the others are tuned as
hyper-parameters. Because they implicitly prioritize certain objectives, such
designs bias the model in pernicious, undetectable ways. To address this, we
adopt impartiality as a principle: design of ML pipelines should not favor one
objective over another. We propose impartially-specified models, which provide
us with accurate Pareto frontiers that show the inherent trade-offs between the
objectives. Extending two canonical ML frameworks for privacy-preserving
learning, we provide two methods (FairDP-SGD and FairPATE) to train
impartially-specified models and recover the Pareto frontier. Through
theoretical privacy analysis and a comprehensive empirical study, we provide an
answer to the question of where fairness mitigation should be integrated within
a privacy-aware ML pipeline.",None,-1
07e4ff6a-bf41-474a-9081-fa74c01a6d41,In-Context Learning Dynamics with Random Binary Sequences,0.0109463,1,"Large language models (LLMs) trained on huge corpora of text datasets
demonstrate intriguing capabilities, achieving state-of-the-art performance on
tasks they were not explicitly trained for. The precise nature of LLM
capabilities is often mysterious, and different prompts can elicit different
capabilities through in-context learning. We propose a framework that enables
us to analyze in-context learning dynamics to understand latent concepts
underlying LLMs' behavioral patterns. This provides a more nuanced
understanding than success-or-failure evaluation benchmarks, but does not
require observing internal activations as a mechanistic interpretation of
circuits would. Inspired by the cognitive science of human randomness
perception, we use random binary sequences as context and study dynamics of
in-context learning by manipulating properties of context data, such as
sequence length. In the latest GPT-3.5+ models, we find emergent abilities to
generate seemingly random numbers and learn basic formal languages, with
striking in-context learning dynamics where model outputs transition sharply
from seemingly random behaviors to deterministic repetition.",None,-1
e0845306-bfc1-4d3e-9fda-310883aef9d1,The Update-Equivalence Framework for Decision-Time Planning,0.251063,1,"The process of revising (or constructing) a policy at execution time -- known
as decision-time planning -- has been key to achieving superhuman performance
in perfect-information games like chess and Go. A recent line of work has
extended decision-time planning to imperfect-information games, leading to
superhuman performance in poker. However, these methods involve solving
subgames whose sizes grow quickly in the amount of non-public information,
making them unhelpful when the amount of non-public information is large.
Motivated by this issue, we introduce an alternative framework for
decision-time planning that is not based on solving subgames, but rather on
update equivalence. In this update-equivalence framework, decision-time
planning algorithms replicate the updates of last-iterate algorithms, which
need not rely on public information. This facilitates scalability to games with
large amounts of non-public information. Using this framework, we derive a
provably sound search algorithm for fully cooperative games based on mirror
descent and a search algorithm for adversarial games based on magnetic mirror
descent. We validate the performance of these algorithms in cooperative and
adversarial domains, notably in Hanabi, the standard benchmark for search in
fully cooperative imperfect-information games. Here, our mirror descent
approach exceeds or matches the performance of public information-based search
while using two orders of magnitude less search time. This is the first
instance of a non-public-information-based algorithm outperforming
public-information-based approaches in a domain they have historically
dominated.",None,-1
6713b485-2d2a-4470-9586-11338a0fc088,Heuristic Algorithms for the Approximation of Mutual Coherence,0.127092,1,"Mutual coherence is a measure of similarity between two opinions. Although
the notion comes from philosophy, it is essential for a wide range of
technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters
to find candidates that are the closest to their political preferences. The
exact computation of mutual coherence is highly time-consuming due to the
iteration over all subsets of an opinion. Moreover, for every subset, an
instance of the SAT model counting problem has to be solved which is known to
be a hard problem in computer science. This work is the first study to
accelerate this computation. We model the distribution of the so-called
confirmation values as a mixture of three Gaussians and present efficient
heuristics to estimate its model parameters. The mutual coherence is then
approximated with the expected value of the distribution. Some of the presented
algorithms are fully polynomial-time, others only require solving a small
number of instances of the SAT model counting problem. The average squared
error of our best algorithm lies below 0.0035 which is insignificant if the
efficiency is taken into account. Furthermore, the accuracy is precise enough
to be used in Wahl-O-Mat-like systems.",None,-1
8d93e2ab-b767-4476-9e90-7db4623a01fb,Estimating Distances Between People using a Single Overhead Fisheye Camera with Application to Social-Distancing Oversight,0.182092,1,"Unobtrusive monitoring of distances between people indoors is a useful tool
in the fight against pandemics. A natural resource to accomplish this are
surveillance cameras. Unlike previous distance estimation methods, we use a
single, overhead, fisheye camera with wide area coverage and propose two
approaches. One method leverages a geometric model of the fisheye lens, whereas
the other method uses a neural network to predict the 3D-world distance from
people-locations in a fisheye image. To evaluate our algorithms, we collected a
first-of-its-kind dataset using single fisheye camera, that comprises a wide
range of distances between people (1-58 ft) and will be made publicly
available. The algorithms achieve 1-2 ft distance error and over 95% accuracy
in detecting social-distance violations.",None,-1
77dd2c39-fd79-47a4-b8ea-2be37c024da5,AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities,0.703748,3,"Accident detection and traffic analysis is a critical component of smart city
and autonomous transportation systems that can reduce accident frequency,
severity and improve overall traffic management. This paper presents a
comprehensive analysis of traffic accidents in different regions across the
United States using data from the National Highway Traffic Safety
Administration (NHTSA) Crash Report Sampling System (CRSS). To address the
challenges of accident detection and traffic analysis, this paper proposes a
framework that uses traffic surveillance cameras and action recognition systems
to detect and respond to traffic accidents spontaneously. Integrating the
proposed framework with emergency services will harness the power of traffic
cameras and machine learning algorithms to create an efficient solution for
responding to traffic accidents and reducing human errors. Advanced
intelligence technologies, such as the proposed accident detection systems in
smart cities, will improve traffic management and traffic accident severity.
Overall, this study provides valuable insights into traffic accidents in the US
and presents a practical solution to enhance the safety and efficiency of
transportation systems.",None,-1
55a2e85e-3028-4008-96ba-26fbe09f650c,Road Extraction with Satellite Images and Partial Road Maps,0.477884,9,"Road extraction is a process of automatically generating road maps mainly
from satellite images. Existing models all target to generate roads from the
scratch despite that a large quantity of road maps, though incomplete, are
publicly available (e.g. those from OpenStreetMap) and can help with road
extraction. In this paper, we propose to conduct road extraction based on
satellite images and partial road maps, which is new. We then propose a
two-branch Partial to Complete Network (P2CNet) for the task, which has two
prominent components: Gated Self-Attention Module (GSAM) and Missing Part (MP)
loss. GSAM leverages a channel-wise self-attention module and a gate module to
capture long-range semantics, filter out useless information, and better fuse
the features from two branches. MP loss is derived from the partial road maps,
trying to give more attention to the road pixels that do not exist in partial
road maps. Extensive experiments are conducted to demonstrate the effectiveness
of our model, e.g. P2CNet achieves state-of-the-art performance with the IoU
scores of 70.71% and 75.52%, respectively, on the SpaceNet and OSM datasets.",None,-1
9bdb2bc3-ac3f-4ea9-a7dc-982adc59d0ae,A Theory of Bounded Inductive Rationality,0.811881,4,"The dominant theories of rational choice assume logical omniscience. That is,
they assume that when facing a decision problem, an agent can perform all
relevant computations and determine the truth value of all relevant
logical/mathematical claims. This assumption is unrealistic when, for example,
we offer bets on remote digits of pi or when an agent faces a computationally
intractable planning problem. Furthermore, the assumption of logical
omniscience creates contradictions in cases where the environment can contain
descriptions of the agent itself. Importantly, strategic interactions as
studied in game theory are decision problems in which a rational agent is
predicted by its environment (the other players). In this paper, we develop a
theory of rational decision making that does not assume logical omniscience. We
consider agents who repeatedly face decision problems (including ones like
betting on digits of pi or games against other agents). The main contribution
of this paper is to provide a sensible theory of rationality for such agents.
Roughly, we require that a boundedly rational inductive agent tests each
efficiently computable hypothesis infinitely often and follows those hypotheses
that keep their promises of high rewards. We then prove that agents that are
rational in this sense have other desirable properties. For example, they learn
to value random and pseudo-random lotteries at their expected reward. Finally,
we consider strategic interactions between different agents and prove a folk
theorem for what strategies bounded rational inductive agents can converge to.",None,-1
70d52bdf-bb2f-4c46-9ea4-1f1e0425a388,Accelerated Coordinate Encoding: Learning to Relocalize in Minutes using RGB and Poses,0.282319,14,"Learning-based visual relocalizers exhibit leading pose accuracy, but require
hours or days of training. Since training needs to happen on each new scene
again, long training times make learning-based relocalization impractical for
most applications, despite its promise of high accuracy. In this paper we show
how such a system can actually achieve the same accuracy in less than 5
minutes. We start from the obvious: a relocalization network can be split in a
scene-agnostic feature backbone, and a scene-specific prediction head. Less
obvious: using an MLP prediction head allows us to optimize across thousands of
view points simultaneously in each single training iteration. This leads to
stable and extremely fast convergence. Furthermore, we substitute effective but
slow end-to-end training using a robust pose solver with a curriculum over a
reprojection loss. Our approach does not require privileged knowledge, such a
depth maps or a 3D model, for speedy training. Overall, our approach is up to
300x faster in mapping than state-of-the-art scene coordinate regression, while
keeping accuracy on par.",None,-1
df6f9b6e-72d8-412d-9617-f982d3262cc8,VISION Datasets: A Benchmark for Vision-based InduStrial InspectiON,0.957586,7,"Despite progress in vision-based inspection algorithms, real-world industrial
challenges -- specifically in data availability, quality, and complex
production requirements -- often remain under-addressed. We introduce the
VISION Datasets, a diverse collection of 14 industrial inspection datasets,
uniquely poised to meet these challenges. Unlike previous datasets, VISION
brings versatility to defect detection, offering annotation masks across all
splits and catering to various detection methodologies. Our datasets also
feature instance-segmentation annotation, enabling precise defect
identification. With a total of 18k images encompassing 44 defect types, VISION
strives to mirror a wide range of real-world production scenarios. By
supporting two ongoing challenge competitions on the VISION Datasets, we hope
to foster further advancements in vision-based industrial inspection.",None,-1
569d55e9-13d1-49b0-93d4-d2f3bd8dd3a8,Duplex Diffusion Models Improve Speech-to-Speech Translation,0.589174,3,"Speech-to-speech translation is a typical sequence-to-sequence learning task
that naturally has two directions. How to effectively leverage bidirectional
supervision signals to produce high-fidelity audio for both directions?
Existing approaches either train two separate models or a multitask-learned
model with low efficiency and inferior performance. In this paper, we propose a
duplex diffusion model that applies diffusion probabilistic models to both
sides of a reversible duplex Conformer, so that either end can simultaneously
input and output a distinct language's speech. Our model enables reversible
speech translation by simply flipping the input and output ends. Experiments
show that our model achieves the first success of reversible speech translation
with significant improvements of ASR-BLEU scores compared with a list of
state-of-the-art baselines.",None,-1
093c54f6-b20a-43e3-9e85-fc7c6708faca,Clinical Trial Recommendations Using Semantics-Based Inductive Inference and Knowledge Graph Embeddings,0.192463,1,"Designing a new clinical trial entails many decisions, such as defining a
cohort and setting the study objectives to name a few, and therefore can
benefit from recommendations based on exhaustive mining of past clinical trial
records. Here, we propose a novel recommendation methodology, based on neural
embeddings trained on a first-of-a-kind knowledge graph of clinical trials. We
addressed several important research questions in this context, including
designing a knowledge graph (KG) for clinical trial data, effectiveness of
various KG embedding (KGE) methods for it, a novel inductive inference using
KGE, and its use in generating recommendations for clinical trial design. We
used publicly available data from clinicaltrials.gov for the study. Results
show that our recommendations approach achieves relevance scores of 70%-83%,
measured as the text similarity to actual clinical trial elements, and the most
relevant recommendation can be found near the top of list. Our study also
suggests potential improvement in training KGE using node semantics.",None,-1
c83e8d8f-309c-4bd8-96b7-fba99dabaca4,Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems using Differentially Private Language Models,0.625143,4,"We address the challenge of ensuring differential privacy (DP) guarantees in
training deep retrieval systems. Training these systems often involves the use
of contrastive-style losses, which are typically non-per-example decomposable,
making them difficult to directly DP-train with since common techniques require
per-example gradients. To address this issue, we propose an approach that
prioritizes ensuring query privacy prior to training a deep retrieval system.
Our method employs DP language models (LMs) to generate private synthetic
queries representative of the original data. These synthetic queries can be
used in downstream retrieval system training without compromising privacy. Our
approach demonstrates a significant enhancement in retrieval quality compared
to direct DP-training, all while maintaining query-level privacy guarantees.
This work highlights the potential of harnessing LMs to overcome limitations in
standard DP-training methods.",None,-1
2cf3f254-4440-45ea-916a-d9bb32b2c66b,CNN-BiLSTM model for English Handwriting Recognition: Comprehensive Evaluation on the IAM Dataset,0.59811,2,"We present a CNN-BiLSTM system for the problem of offline English handwriting
recognition, with extensive evaluations on the public IAM dataset, including
the effects of model size, data augmentation and the lexicon. Our best model
achieves 3.59\% CER and 9.44\% WER using CNN-BiLSTM network with CTC layer.
Test time augmentation with rotation and shear transformations applied to the
input image, is proposed to increase recognition of difficult cases and found
to reduce the word error rate by 2.5\% points. We also conduct an error
analysis of our proposed method on IAM dataset, show hard cases of handwriting
images and explore samples with erroneous labels. We provide our source code as
public-domain, to foster further research to encourage scientific
reproducibility.",None,-1
eab5c9c2-102d-4866-8704-95dd295f63bb,Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts,0.517645,5,"Transformer-based pre-trained models have achieved great improvements in
semantic matching. However, existing models still suffer from insufficient
ability to capture subtle differences. The modification, addition and deletion
of words in sentence pairs may make it difficult for the model to predict their
relationship. To alleviate this problem, we propose a novel Dual Path Modeling
Framework to enhance the model's ability to perceive subtle differences in
sentence pairs by separately modeling affinity and difference semantics. Based
on dual-path modeling framework we design the Dual Path Modeling Network
(DPM-Net) to recognize semantic relations. And we conduct extensive experiments
on 10 well-studied semantic matching and robustness test datasets, and the
experimental results show that our proposed method achieves consistent
improvements over baselines.",None,-1
